[{"id": "1712.00032", "submitter": "Xavier Roynard", "authors": "Xavier Roynard and Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette", "title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud\n  dataset for automatic segmentation and classification", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new Urban Point Cloud Dataset for Automatic\nSegmentation and Classification acquired by Mobile Laser Scanning (MLS). We\ndescribe how the dataset is obtained from acquisition to post-processing and\nlabeling. This dataset can be used to learn classification algorithm, however,\ngiven that a great attention has been paid to the split between the different\nobjects, this dataset can also be used to learn the segmentation. The dataset\nconsists of around 2km of MLS point cloud acquired in two cities. The number of\npoints and range of classes make us consider that it can be used to train\nDeep-Learning methods. Besides we show some results of automatic segmentation\nand classification. The dataset is available at:\nhttp://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:08:52 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 15:53:58 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Roynard", "Xavier", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "1712.00075", "submitter": "Shuo Liu", "authors": "Shuo Liu and Zheng Liu", "title": "Multi-Channel CNN-based Object Detection for Enhanced Situation\n  Awareness", "comments": "Published at the Sensors & Electronics Technology (SET) panel\n  Symposium SET-241 on 9th NATO Military Sensing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Detection is critical for automatic military operations. However, the\nperformance of current object detection algorithms is deficient in terms of the\nrequirements in military scenarios. This is mainly because the object presence\nis hard to detect due to the indistinguishable appearance and dramatic changes\nof object's size which is determined by the distance to the detection sensors.\nRecent advances in deep learning have achieved promising results in many\nchallenging tasks. The state-of-the-art in object detection is represented by\nconvolutional neural networks (CNNs), such as the fast R-CNN algorithm. These\nCNN-based methods improve the detection performance significantly on several\npublic generic object detection datasets. However, their performance on\ndetecting small objects or undistinguishable objects in visible spectrum images\nis still insufficient. In this study, we propose a novel detection algorithm\nfor military objects by fusing multi-channel CNNs. We combine spatial, temporal\nand thermal information by generating a three-channel image, and they will be\nfused as CNN feature maps in an unsupervised manner. The backbone of our object\ndetection framework is from the fast R-CNN algorithm, and we utilize\ncross-domain transfer learning technique to fine-tune the CNN model on\ngenerated multi-channel images. In the experiments, we validated the proposed\nmethod with the images from SENSIAC (Military Sensing Information Analysis\nCentre) database and compared it with the state-of-the-art. The experimental\nresults demonstrated the effectiveness of the proposed method on both accuracy\nand computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 20:54:49 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Liu", "Shuo", ""], ["Liu", "Zheng", ""]]}, {"id": "1712.00080", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik\n  Learned-Miller, Jan Kautz", "title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for\n  Video Interpolation", "comments": "CVPR 2018 version with minor revisions and supplementary material\n  included. Project page http://jianghz.me/projects/superslomo", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two consecutive frames, video interpolation aims at generating\nintermediate frame(s) to form both spatially and temporally coherent video\nsequences. While most existing methods focus on single-frame interpolation, we\npropose an end-to-end convolutional neural network for variable-length\nmulti-frame video interpolation, where the motion interpretation and occlusion\nreasoning are jointly modeled. We start by computing bi-directional optical\nflow between the input images using a U-Net architecture. These flows are then\nlinearly combined at each time step to approximate the intermediate\nbi-directional optical flows. These approximate flows, however, only work well\nin locally smooth regions and produce artifacts around motion boundaries. To\naddress this shortcoming, we employ another U-Net to refine the approximated\nflow and also predict soft visibility maps. Finally, the two input images are\nwarped and linearly fused to form each intermediate frame. By applying the\nvisibility maps to the warped images before fusion, we exclude the contribution\nof occluded pixels to the interpolated intermediate frame to avoid artifacts.\nSince none of our learned network parameters are time-dependent, our approach\nis able to produce as many intermediate frames as needed. We use 1,132 video\nclips with 240-fps, containing 300K individual video frames, to train our\nnetwork. Experimental results on several datasets, predicting different numbers\nof interpolated frames, demonstrate that our approach performs consistently\nbetter than existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:05:15 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 15:16:36 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Jiang", "Huaizu", ""], ["Sun", "Deqing", ""], ["Jampani", "Varun", ""], ["Yang", "Ming-Hsuan", ""], ["Learned-Miller", "Erik", ""], ["Kautz", "Jan", ""]]}, {"id": "1712.00097", "submitter": "Xiaodong Yang", "authors": "Behrooz Mahasseni, Xiaodong Yang, Pavlo Molchanov, Jan Kautz", "title": "Budget-Aware Activity Detection with A Recurrent Policy Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging problem of efficient temporal\nactivity detection in untrimmed long videos. While most recent work has focused\nand advanced the detection accuracy, the inference time can take seconds to\nminutes in processing each single video, which is too slow to be useful in\nreal-world settings. This motivates the proposed budget-aware framework, which\nlearns to perform activity detection by intelligently selecting a small subset\nof frames according to a specified time budget. We formulate this problem as a\nMarkov decision process, and adopt a recurrent network to model the frame\nselection policy. We derive a recurrent policy gradient based approach to\napproximate the gradient of the non-decomposable and non-differentiable\nobjective defined in our problem. In the extensive experiments, we achieve\ncompetitive detection accuracy, and more importantly, our approach is able to\nsubstantially reduce computation time and detect multiple activities with only\n0.35s for each untrimmed long video.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:52:27 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 04:55:57 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Mahasseni", "Behrooz", ""], ["Yang", "Xiaodong", ""], ["Molchanov", "Pavlo", ""], ["Kautz", "Jan", ""]]}, {"id": "1712.00108", "submitter": "Zelun Luo", "authors": "Zelun Luo, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei", "title": "Graph Distillation for Action Detection with Privileged Modalities", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique that tackles action detection in multimodal videos\nunder a realistic and challenging condition in which only limited training data\nand partially observed modalities are available. Common methods in transfer\nlearning do not take advantage of the extra modalities potentially available in\nthe source domain. On the other hand, previous work on multimodal learning only\nfocuses on a single domain or task and does not handle the modality discrepancy\nbetween training and testing. In this work, we propose a method termed graph\ndistillation that incorporates rich privileged information from a large-scale\nmultimodal dataset in the source domain, and improves the learning in the\ntarget domain where training data and modalities are scarce. We evaluate our\napproach on action classification and detection tasks in multimodal videos, and\nshow that our model outperforms the state-of-the-art by a large margin on the\nNTU RGB+D and PKU-MMD benchmarks. The code is released at\nhttp://alan.vision/eccv18_graph/.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 22:40:59 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 22:03:03 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Luo", "Zelun", ""], ["Hsieh", "Jun-Ting", ""], ["Jiang", "Lu", ""], ["Niebles", "Juan Carlos", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1712.00110", "submitter": "Rui Zhu", "authors": "Rui Zhu, Chaoyang Wang, Chen-Hsuan Lin, Ziyan Wang, Simon Lucey", "title": "Semantic Photometric Bundle Adjustment on Natural Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of obtaining dense reconstruction of an object in a natural\nsequence of images has been long studied in computer vision. Classically this\nproblem has been solved through the application of bundle adjustment (BA). More\nrecently, excellent results have been attained through the application of\nphotometric bundle adjustment (PBA) methods -- which directly minimize the\nphotometric error across frames. A fundamental drawback to BA & PBA, however,\nis: (i) their reliance on having to view all points on the object, and (ii) for\nthe object surface to be well textured. To circumvent these limitations we\npropose semantic PBA which incorporates a 3D object prior, obtained through\ndeep learning, within the photometric bundle adjustment problem. We demonstrate\nstate of the art performance in comparison to leading methods for object\nreconstruction across numerous natural sequences.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 22:45:59 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Zhu", "Rui", ""], ["Wang", "Chaoyang", ""], ["Lin", "Chen-Hsuan", ""], ["Wang", "Ziyan", ""], ["Lucey", "Simon", ""]]}, {"id": "1712.00111", "submitter": "Yanjun Li", "authors": "Yanjun Li, Kiryung Lee, Yoram Bresler", "title": "Blind Gain and Phase Calibration via Sparse Spectral Methods", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind gain and phase calibration (BGPC) is a bilinear inverse problem\ninvolving the determination of unknown gains and phases of the sensing system,\nand the unknown signal, jointly. BGPC arises in numerous applications, e.g.,\nblind albedo estimation in inverse rendering, synthetic aperture radar\nautofocus, and sensor array auto-calibration. In some cases, sparse structure\nin the unknown signal alleviates the ill-posedness of BGPC. Recently there has\nbeen renewed interest in solutions to BGPC with careful analysis of error\nbounds. In this paper, we formulate BGPC as an eigenvalue/eigenvector problem,\nand propose to solve it via power iteration, or in the sparsity or joint\nsparsity case, via truncated power iteration. Under certain assumptions, the\nunknown gains, phases, and the unknown signal can be recovered simultaneously.\nNumerical experiments show that power iteration algorithms work not only in the\nregime predicted by our main results, but also in regimes where theoretical\nanalysis is limited. We also show that our power iteration algorithms for BGPC\ncompare favorably with competing algorithms in adversarial conditions, e.g.,\nwith noisy measurement or with a bad initial estimate.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 22:46:14 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Li", "Yanjun", ""], ["Lee", "Kiryung", ""], ["Bresler", "Yoram", ""]]}, {"id": "1712.00123", "submitter": "Zelun Luo", "authors": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei", "title": "Label Efficient Learning of Transferable Representations across Domains\n  and Tasks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that learns a representation transferable across\ndifferent domains and tasks in a label efficient manner. Our approach battles\ndomain shift with a domain adversarial loss, and generalizes the embedding to\nnovel task using a metric learning-based approach. Our model is simultaneously\noptimized on labeled source data and unlabeled or sparsely labeled data in the\ntarget domain. Our method shows compelling results on novel classes within a\nnew domain even when only a few labeled examples per class are available,\noutperforming the prevalent fine-tuning approach. In addition, we demonstrate\nthe effectiveness of our framework on the transfer learning task from image\nobject recognition to video action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:31:28 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Luo", "Zelun", ""], ["Zou", "Yuliang", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1712.00133", "submitter": "Yajiao Dong", "authors": "Yj Dong and JG Li", "title": "Video retrieval based on deep convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the enormous growth of online videos, fast video retrieval\nresearch has received increasing attention. As an extension of image hashing\ntechniques, traditional video hashing methods mainly depend on hand-crafted\nfeatures and transform the real-valued features into binary hash codes. As\nvideos provide far more diverse and complex visual information than images,\nextracting features from videos is much more challenging than that from images.\nTherefore, high-level semantic features to represent videos are needed rather\nthan low-level hand-crafted methods. In this paper, a deep convolutional neural\nnetwork is proposed to extract high-level semantic features and a binary hash\nfunction is then integrated into this framework to achieve an end-to-end\noptimization. Particularly, our approach also combines triplet loss function\nwhich preserves the relative similarity and difference of videos and\nclassification loss function as the optimization objective. Experiments have\nbeen performed on two public datasets and the results demonstrate the\nsuperiority of our proposed method compared with other state-of-the-art video\nretrieval methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 00:07:47 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dong", "Yj", ""], ["Li", "JG", ""]]}, {"id": "1712.00158", "submitter": "Yeong-Jun Cho", "authors": "Yeong-Jun Cho, Kuk-Jin Yoon", "title": "Distance-based Camera Network Topology Inference for Person\n  Re-identification", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel distance-based camera network topology\ninference method for efficient person re-identification. To this end, we first\ncalibrate each camera and estimate relative scales between cameras. Using the\ncalibration results of multiple cameras, we calculate the speed of each person\nand infer the distance between cameras to generate distance-based camera\nnetwork topology. The proposed distance-based topology can be applied\nadaptively to each person according to its speed and handle diverse transition\ntime of people between non-overlapping cameras. To validate the proposed\nmethod, we tested the proposed method using an open person re-identification\ndataset and compared to state-of-the-art methods. The experimental results show\nthat the proposed method is effective for person re-identification in the\nlarge-scale camera network with various people transition time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 02:21:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Cho", "Yeong-Jun", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1712.00175", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey", "title": "Learning Depth from Monocular Videos using Direct Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict depth from a single image - using recent advances in\nCNNs - is of increasing interest to the vision community. Unsupervised\nstrategies to learning are particularly appealing as they can utilize much\nlarger and varied monocular video datasets during learning without the need for\nground truth depth or stereo. In previous works, separate pose and depth CNN\npredictors had to be determined such that their joint outputs minimized the\nphotometric error. Inspired by recent advances in direct visual odometry (DVO),\nwe argue that the depth CNN predictor can be learned without a pose CNN\npredictor. Further, we demonstrate empirically that incorporation of a\ndifferentiable implementation of DVO, along with a novel depth normalization\nstrategy - substantially improves performance over state of the art that use\nmonocular videos for training.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 03:37:18 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Wang", "Chaoyang", ""], ["Buenaposada", "Jose Miguel", ""], ["Zhu", "Rui", ""], ["Lucey", "Simon", ""]]}, {"id": "1712.00184", "submitter": "Chang-Ryeol Lee", "authors": "Chang-Ryeol Lee, Kuk-Jin Yoon", "title": "Inertial-aided Rolling Shutter Relative Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative pose estimation is a fundamental problem in computer vision and it\nhas been studied for conventional global shutter cameras for decades. However,\nrecently, a rolling shutter camera has been widely used due to its low cost\nimaging capability and, since the rolling shutter camera captures the image\nline-by-line, the relative pose estimation of a rolling shutter camera is more\ndifficult than that of a global shutter camera. In this paper, we propose to\nexploit inertial measurements (gravity and angular velocity) for the rolling\nshutter relative pose estimation problem. The inertial measurements provide\ninformation about the partial relative rotation between two views (cameras) and\nthe instantaneous motion that causes the rolling shutter distortion. Based on\nthis information, we simplify the rolling shutter relative pose estimation\nproblem and propose effective methods to solve it. Unlike the previous methods,\nwhich require 44 (linear) or 17 (nonlinear) points with the uniform rolling\nshutter camera model, the proposed methods require at most 9 or 11 points to\nestimate the relative pose between the rolling shutter cameras. Experimental\nresults on synthetic data and the public PennCOSYVIO dataset show that the\nproposed methods outperform the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:16:36 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lee", "Chang-Ryeol", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1712.00185", "submitter": "Seung-Hwan Bae", "authors": "Seung-Hwan Bae, Youngwan Lee, Youngjoo Jo, Yuseok Bae, Joong-won Hwang", "title": "Rank of Experts: Detection Network Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances of convolutional detectors show impressive performance\nimprovement for large scale object detection. However, in general, the\ndetection performance usually decreases as the object classes to be detected\nincreases, and it is a practically challenging problem to train a dominant\nmodel for all classes due to the limitations of detection models and datasets.\nIn most cases, therefore, there are distinct performance differences of the\nmodern convolutional detectors for each object class detection. In this paper,\nin order to build an ensemble detector for large scale object detection, we\npresent a conceptually simple but very effective class-wise ensemble detection\nwhich is named as Rank of Experts. We first decompose an intractable problem of\nfinding the best detections for all object classes into small subproblems of\nfinding the best ones for each object class. We then solve the detection\nproblem by ranking detectors in order of the average precision rate for each\nclass, and then aggregate the responses of the top ranked detectors (i.e.\nexperts) for class-wise ensemble detection. The main benefit of our method is\neasy to implement and does not require any joint training of experts for\nensemble. Based on the proposed Rank of Experts, we won the 2nd place in the\nILSVRC 2017 object detection competition.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:27:20 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Bae", "Seung-Hwan", ""], ["Lee", "Youngwan", ""], ["Jo", "Youngjoo", ""], ["Bae", "Yuseok", ""], ["Hwang", "Joong-won", ""]]}, {"id": "1712.00192", "submitter": "Alican Bozkurt", "authors": "Alican Bozkurt, Kivanc Kose, Jaume Coll-Font, Christi Alessi-Fox, Dana\n  H. Brooks, Jennifer G. Dy, Milind Rajadhyaksha", "title": "Delineation of Skin Strata in Reflectance Confocal Microscopy Images\n  using Recurrent Convolutional Networks with Toeplitz Attention", "comments": "Accepted for ML4H Workshop at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflectance confocal microscopy (RCM) is an effective, non-invasive\npre-screening tool for skin cancer diagnosis, but it requires extensive\ntraining and experience to assess accurately. There are few quantitative tools\navailable to standardize image acquisition and analysis, and the ones that are\navailable are not interpretable. In this study, we use a recurrent neural\nnetwork with attention on convolutional network features. We apply it to\ndelineate skin strata in vertically-oriented stacks of transverse RCM image\nslices in an interpretable manner. We introduce a new attention mechanism\ncalled Toeplitz attention, which constrains the attention map to have a\nToeplitz structure. Testing our model on an expert labeled dataset of 504 RCM\nstacks, we achieve 88.17% image-wise classification accuracy, which is the\ncurrent state-of-art.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:59:25 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Bozkurt", "Alican", ""], ["Kose", "Kivanc", ""], ["Coll-Font", "Jaume", ""], ["Alessi-Fox", "Christi", ""], ["Brooks", "Dana H.", ""], ["Dy", "Jennifer G.", ""], ["Rajadhyaksha", "Milind", ""]]}, {"id": "1712.00193", "submitter": "Hee Jung Ryu", "authors": "Hee Jung Ryu, Hartwig Adam, Margaret Mitchell", "title": "InclusiveFaceNet: Improving Face Attribute Detection with Race and\n  Gender Diversity", "comments": "Presented as a talk at the 2018 Workshop on Fairness, Accountability,\n  and Transparency in Machine Learning (FAT/ML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an approach to face attribute detection that retains or\nimproves attribute detection accuracy across gender and race subgroups by\nlearning demographic information prior to learning the attribute detection\ntask. The system, which we call InclusiveFaceNet, detects face attributes by\ntransferring race and gender representations learned from a held-out dataset of\npublic race and gender identities. Leveraging learned demographic\nrepresentations while withholding demographic inference from the downstream\nface attribute detection task preserves potential users' demographic privacy\nwhile resulting in some of the best reported numbers to date on attribute\ndetection in the Faces of the World and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 05:00:16 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 20:52:18 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 12:29:08 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ryu", "Hee Jung", ""], ["Adam", "Hartwig", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1712.00195", "submitter": "Irwandi Hipiny", "authors": "N. Hussain, H. Ujir, I. Hipiny, J-L Minoi", "title": "3D Facial Action Units Recognition for Emotional Expression", "comments": "To be published in Advanced Science Letters Volume 24 (ICCSE2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The muscular activities caused the activation of certain AUs for every facial\nexpression at the certain duration of time throughout the facial expression.\nThis paper presents the methods to recognise facial Action Unit (AU) using\nfacial distance of the facial features which activates the muscles. The seven\nfacial action units involved are AU1, AU4, AU6, AU12, AU15, AU17 and AU25 that\ncharacterises happy and sad expression. The recognition is performed on each AU\naccording to rules defined based on the distance of each facial points. The\nfacial distances chosen are extracted from twelve facial features. Then the\nfacial distances are trained using Support Vector Machine (SVM) and Neural\nNetwork (NN). Classification result using SVM is presented with several\ndifferent SVM kernels while result using NN is presented for each training,\nvalidation and testing phase.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 05:03:17 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hussain", "N.", ""], ["Ujir", "H.", ""], ["Hipiny", "I.", ""], ["Minoi", "J-L", ""]]}, {"id": "1712.00201", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Yingda Xia, Wei Shen, Elliot K. Fishman, Alan L. Yuille", "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation", "comments": "9 pages, 4 figures, Accepted to 3DV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adopt 3D Convolutional Neural Networks to segment\nvolumetric medical images. Although deep neural networks have been proven to be\nvery effective on many 2D vision tasks, it is still challenging to apply them\nto 3D tasks due to the limited amount of annotated 3D data and limited\ncomputational resources. We propose a novel 3D-based coarse-to-fine framework\nto effectively and efficiently tackle these challenges. The proposed 3D-based\nframework outperforms the 2D counterpart to a large margin since it can\nleverage the rich spatial infor- mation along all three axes. We conduct\nexperiments on two datasets which include healthy and pathological pancreases\nrespectively, and achieve the current state-of-the-art in terms of\nDice-S{\\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,\nwe outperform the previous best by an average of over 2%, and the worst case is\nimproved by 7% to reach almost 70%, which indicates the reliability of our\nframework in clinical applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 05:57:19 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 02:30:38 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Xia", "Yingda", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1712.00202", "submitter": "Kai Fan", "authors": "Kai Fan, Qi Wei, Wenlin Wang, Amit Chakraborty, Katherine Heller", "title": "InverseNet: Solving Inverse Problems with Splitting Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method that uses deep learning techniques to solve the\ninverse problems. The inverse problem is cast in the form of learning an\nend-to-end mapping from observed data to the ground-truth. Inspired by the\nsplitting strategy widely used in regularized iterative algorithm to tackle\ninverse problems, the mapping is decomposed into two networks, with one\nhandling the inversion of the physical forward model associated with the data\nterm and one handling the denoising of the output from the former network,\ni.e., the inverted version, associated with the prior/regularization term. The\ntwo networks are trained jointly to learn the end-to-end mapping, getting rid\nof a two-step training. The training is annealing as the intermediate variable\nbetween these two networks bridges the gap between the input (the degraded\nversion of output) and output and progressively approaches to the ground-truth.\nThe proposed network, referred to as InverseNet, is flexible in the sense that\nmost of the existing end-to-end network structure can be leveraged in the first\nnetwork and most of the existing denoising network structure can be used in the\nsecond one. Extensive experiments on both synthetic data and real datasets on\nthe tasks, motion deblurring, super-resolution, and colorization, demonstrate\nthe efficiency and accuracy of the proposed method compared with other image\nprocessing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 06:04:05 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Fan", "Kai", ""], ["Wei", "Qi", ""], ["Wang", "Wenlin", ""], ["Chakraborty", "Amit", ""], ["Heller", "Katherine", ""]]}, {"id": "1712.00213", "submitter": "Chunhua Shen", "authors": "Zifeng Wu, Chunhua Shen, Anton van den Hengel", "title": "Real-time Semantic Image Segmentation via Spatial Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to semantic (image) segmentation that reduces the\ncomputational costs by a factor of 25 with limited impact on the quality of\nresults. Semantic segmentation has a number of practical applications, and for\nmost such applications the computational costs are critical. The method follows\na typical two-column network structure, where one column accepts an input\nimage, while the other accepts a half-resolution version of that image. By\nidentifying specific regions in the full-resolution image that can be safely\nignored, as well as carefully tailoring the network structure, we can process\napproximately 15 highresolution Cityscapes images (1024x2048) per second using\na single GTX 980 video card, while achieving a mean intersection-over-union\nscore of 72.9% on the Cityscapes test set.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 07:15:28 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Wu", "Zifeng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1712.00238", "submitter": "Morad Behandish", "authors": "Morad Behandish and Horea T. Ilies", "title": "Shape Complementarity Analysis for Objects of Arbitrary Shape", "comments": "Technical Report, University of Connecticut, 2014", "journal-ref": null, "doi": null, "report-no": "CDL-TR-14-01", "categories": "cs.CG cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic problem of shape complementarity analysis appears fundamental to\napplications as diverse as mechanical design, assembly automation, robot motion\nplanning, micro- and nano-fabrication, protein-ligand binding, and rational\ndrug design. However, the current challenge lies in the lack of a general\nmathematical formulation that applies to objects of arbitrary shape. We propose\nthat a measure of shape complementarity can be obtained from the extent of\napproximate overlap between shape skeletons. A space-continuous implicit\ngeneralization of the skeleton, called the skeletal density function (SDF) is\ndefined over the Euclidean space that contains the individual assembly\npartners. The SDF shape descriptors capture the essential features that are\nrelevant to proper contact alignment, and are considerably more robust than the\nconventional explicit skeletal representations. We express the shape\ncomplementarity score as a convolution of the individual SDFs. The problem then\nbreaks down to a global optimization of the score over the configuration space\nof spatial relations, which can be efficiently implemented using fast Fourier\ntransforms (FFTs) on nonequispaced samples. We demonstrate the effectiveness of\nthe scoring approach for several examples from 2D peg-in-hole alignment to more\ncomplex 3D examples in mechanical assembly and protein docking. We show that\nthe proposed method is reliable, inherently robust against small perturbations,\nand effective in steering gradient-based optimization.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 09:07:14 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""]]}, {"id": "1712.00244", "submitter": "Thanh Hai Nguyen", "authors": "Thanh Hai Nguyen, Yann Chevaleyre, Edi Prifti, Nataliya Sokolovska and\n  Jean-Daniel Zucker", "title": "Deep Learning for Metagenomic Data: using 2D Embeddings and\n  Convolutional Neural Networks", "comments": "Accepted at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/); In Proceedings of the NIPS ML4H 2017\n  Workshop in Long Beach, CA, USA;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) techniques have had unprecedented success when applied to\nimages, waveforms, and texts to cite a few. In general, when the sample size\n(N) is much greater than the number of features (d), DL outperforms previous\nmachine learning (ML) techniques, often through the use of convolution neural\nnetworks (CNNs). However, in many bioinformatics ML tasks, we encounter the\nopposite situation where d is greater than N. In these situations, applying DL\ntechniques (such as feed-forward networks) would lead to severe overfitting.\nThus, sparse ML techniques (such as LASSO e.g.) usually yield the best results\non these tasks. In this paper, we show how to apply CNNs on data which do not\nhave originally an image structure (in particular on metagenomic data). Our\nfirst contribution is to show how to map metagenomic data in a meaningful way\nto 1D or 2D images. Based on this representation, we then apply a CNN, with the\naim of predicting various diseases. The proposed approach is applied on six\ndifferent datasets including in total over 1000 samples from various diseases.\nThis approach could be a promising one for prediction tasks in the\nbioinformatics field.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 09:18:04 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Nguyen", "Thanh Hai", ""], ["Chevaleyre", "Yann", ""], ["Prifti", "Edi", ""], ["Sokolovska", "Nataliya", ""], ["Zucker", "Jean-Daniel", ""]]}, {"id": "1712.00250", "submitter": "Sebastian Sudholt", "authors": "Neha Gurjar, Sebastian Sudholt, Gernot A. Fink", "title": "Learning Deep Representations for Word Spotting Under Weak Supervision", "comments": "submitted to DAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have made their mark in various fields of\ncomputer vision in recent years. They have achieved state-of-the-art\nperformance in the field of document analysis as well. However, CNNs require a\nlarge amount of annotated training data and, hence, great manual effort. In our\napproach, we introduce a method to drastically reduce the manual annotation\neffort while retaining the high performance of a CNN for word spotting in\nhandwritten documents. The model is learned with weak supervision using a\ncombination of synthetically generated training data and a small subset of the\ntraining partition of the handwritten data set. We show that the network\nachieves results highly competitive to the state-of-the-art in word spotting\nwith shorter training times and a fraction of the annotation effort.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 09:41:13 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 09:56:58 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 10:42:16 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Gurjar", "Neha", ""], ["Sudholt", "Sebastian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1712.00268", "submitter": "Or Litany", "authors": "Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia", "title": "Deformable Shape Completion with Graph Convolutional Autoencoders", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of affordable and portable depth sensors has made scanning\nobjects and people simpler than ever. However, dealing with occlusions and\nmissing parts is still a significant challenge. The problem of reconstructing a\n(possibly non-rigidly moving) 3D object from a single or multiple partial scans\nhas received increasing attention in recent years. In this work, we propose a\nnovel learning-based method for the completion of partial shapes. Unlike the\nmajority of existing approaches, our method focuses on objects that can undergo\nnon-rigid deformations. The core of our method is a variational autoencoder\nwith graph convolutional operations that learns a latent space for complete\nrealistic shapes. At inference, we optimize to find the representation in this\nlatent space that best fits the generated shape to the known partial input. The\ncompleted shape exhibits a realistic appearance on the unknown part. We show\npromising results towards the completion of synthetic and real scans of human\nbody and face meshes exhibiting different styles of articulation and\npartiality.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 10:34:19 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 13:35:54 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 13:28:52 GMT"}, {"version": "v4", "created": "Tue, 3 Apr 2018 18:14:47 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Litany", "Or", ""], ["Bronstein", "Alex", ""], ["Bronstein", "Michael", ""], ["Makadia", "Ameesh", ""]]}, {"id": "1712.00269", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, Calvin Seward", "title": "GANosaic: Mosaic Creation with Generative Texture Manifolds", "comments": "31st Conference on Neural Information Processing Systems (NIPS 2017),\n  Long Beach, CA, USA. Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for generating texture mosaics with\nconvolutional neural networks. Our method is called GANosaic and performs\noptimization in the latent noise space of a generative texture model, which\nallows the transformation of a content image into a mosaic exhibiting the\nvisual properties of the underlying texture manifold. To represent that\nmanifold, we use a state-of-the-art generative adversarial method for texture\nsynthesis, which can learn expressive texture representations from data and\nproduce mosaic images with very high resolution. This fully convolutional model\ngenerates smooth (without any visible borders) mosaic images which morph and\nblend different textures locally. In addition, we develop a new type of\ndifferentiable statistical regularization appropriate for optimization over the\nprior noise space of the PSGAN model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 10:35:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Seward", "Calvin", ""]]}, {"id": "1712.00282", "submitter": "Abhinav Kumar", "authors": "Abhinav Kumar, Shantanu Gupta, Vladimir Kozitsky and Sriganesh\n  Madhvanath", "title": "Neural Signatures for Licence Plate Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of vehicle licence plate re-identification is generally\nconsidered as a one-shot image retrieval problem. The objective of this task is\nto learn a feature representation (called a \"signature\") for licence plates.\nIncoming licence plate images are converted to signatures and matched to a\npreviously collected template database through a distance measure. Then, the\ninput image is recognized as the template whose signature is \"nearest\" to the\ninput signature. The template database is restricted to contain only a single\nsignature per unique licence plate for our problem.\n  We measure the performance of deep convolutional net-based features adapted\nfrom face recognition on this task. In addition, we also test a hybrid approach\ncombining the Fisher vector with a neural network-based embedding called \"f2nn\"\ntrained with the Triplet loss function. We find that the hybrid approach\nperforms comparably while providing computational benefits. The signature\ngenerated by the hybrid approach also shows higher generalizability to datasets\nmore dissimilar to the training corpus.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 11:36:15 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Kumar", "Abhinav", ""], ["Gupta", "Shantanu", ""], ["Kozitsky", "Vladimir", ""], ["Madhvanath", "Sriganesh", ""]]}, {"id": "1712.00311", "submitter": "Marc Oliu", "authors": "Marc Oliu, Javier Selva, Sergio Escalera", "title": "Folded Recurrent Neural Networks for Future Video Prediction", "comments": "Submitted to European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future video prediction is an ill-posed Computer Vision problem that recently\nreceived much attention. Its main challenges are the high variability in video\ncontent, the propagation of errors through time, and the non-specificity of the\nfuture frames: given a sequence of past frames there is a continuous\ndistribution of possible futures. This work introduces bijective Gated\nRecurrent Units, a double mapping between the input and output of a GRU layer.\nThis allows for recurrent auto-encoders with state sharing between encoder and\ndecoder, stratifying the sequence representation and helping to prevent\ncapacity problems. We show how with this topology only the encoder or decoder\nneeds to be applied for input encoding and prediction, respectively. This\nreduces the computational cost and avoids re-encoding the predictions when\ngenerating a sequence of frames, mitigating the propagation of errors.\nFurthermore, it is possible to remove layers from an already trained model,\ngiving an insight to the role performed by each layer and making the model more\nexplainable. We evaluate our approach on three video datasets, outperforming\nstate of the art prediction results on MMNIST and UCF101, and obtaining\ncompetitive results on KTH with 2 and 3 times less memory usage and\ncomputational cost than the best scored approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 13:31:56 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 15:15:24 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Oliu", "Marc", ""], ["Selva", "Javier", ""], ["Escalera", "Sergio", ""]]}, {"id": "1712.00321", "submitter": "Sebastian Raschka SR", "authors": "Vahid Mirjalili, Sebastian Raschka, Anoop Namboodiri, and Arun Ross", "title": "Semi-Adversarial Networks: Convolutional Autoencoders for Imparting\n  Privacy to Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design and evaluate a convolutional autoencoder that\nperturbs an input face image to impart privacy to a subject. Specifically, the\nproposed autoencoder transforms an input face image such that the transformed\nimage can be successfully used for face recognition but not for gender\nclassification. In order to train this autoencoder, we propose a novel training\nscheme, referred to as semi-adversarial training in this work. The training is\nfacilitated by attaching a semi-adversarial module consisting of a pseudo\ngender classifier and a pseudo face matcher to the autoencoder. The objective\nfunction utilized for training this network has three terms: one to ensure that\nthe perturbed image is a realistic face image; another to ensure that the\ngender attributes of the face are confounded; and a third to ensure that\nbiometric recognition performance due to the perturbed image is not impacted.\nExtensive experiments confirm the efficacy of the proposed architecture in\nextending gender privacy to face images.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 14:05:50 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 16:19:05 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 03:09:02 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Mirjalili", "Vahid", ""], ["Raschka", "Sebastian", ""], ["Namboodiri", "Anoop", ""], ["Ross", "Arun", ""]]}, {"id": "1712.00358", "submitter": "Yuxin Peng", "authors": "Jian Zhang, Yuxin Peng, and Mingkuan Yuan", "title": "Unsupervised Generative Adversarial Cross-modal Hashing", "comments": "8 pages, accepted by 32th AAAI Conference on Artificial Intelligence\n  (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Unsupervised cross-modal hashing is more flexible and applicable\nthan supervised methods, since no intensive labeling work is involved. However,\nexisting unsupervised methods learn hashing functions by preserving inter and\nintra correlations, while ignoring the underlying manifold structure across\ndifferent modalities, which is extremely helpful to capture meaningful nearest\nneighbors of different modalities for cross-modal retrieval. To address the\nabove problem, in this paper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full use of GAN's ability for\nunsupervised representation learning to exploit the underlying manifold\nstructure of cross-modal data. The main contributions can be summarized as\nfollows: (1) We propose a generative adversarial network to model cross-modal\nhashing in an unsupervised fashion. In the proposed UGACH, given a data of one\nmodality, the generative model tries to fit the distribution over the manifold\nstructure, and select informative data of another modality to challenge the\ndiscriminative model. The discriminative model learns to distinguish the\ngenerated data and the true positive data sampled from correlation graph to\nachieve better retrieval accuracy. These two models are trained in an\nadversarial way to improve each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to capture the underlying\nmanifold structure across different modalities, so that data of different\nmodalities but within the same manifold can have smaller Hamming distance and\npromote retrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods verify the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:20:51 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Zhang", "Jian", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1712.00368", "submitter": "Adrien Lagrange", "authors": "Adrien Lagrange, Mathieu Fauvel, St\\'ephane May and Nicolas Dobigeon", "title": "Hierarchical Bayesian image analysis: from low-level modeling to robust\n  supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a supervised classification framework, labeled data are used to learn\nclassifier parameters. Prior to that, it is generally required to perform\ndimensionality reduction via feature extraction. These preprocessing steps have\nmotivated numerous research works aiming at recovering latent variables in an\nunsupervised context. This paper proposes a unified framework to perform\nclassification and low-level modeling jointly. The main objective is to use the\nestimated latent variables as features for classification and to incorporate\nsimultaneously supervised information to help latent variable extraction. The\nproposed hierarchical Bayesian model is divided into three stages: a first\nlow-level modeling stage to estimate latent variables, a second stage\nclustering these features into statistically homogeneous groups and a last\nclassification stage exploiting the (possibly badly) labeled data. Performance\nof the model is assessed in the specific context of hyperspectral image\ninterpretation, unifying two standard analysis techniques, namely unmixing and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:32:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lagrange", "Adrien", ""], ["Fauvel", "Mathieu", ""], ["May", "St\u00e9phane", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1712.00374", "submitter": "Andreas Maier", "authors": "Andreas Maier, Frank Schebesch, Christopher Syben, Tobias W\\\"urfl,\n  Stefan Steidl, Jang-Hwan Choi, Rebecca Fahrig", "title": "Precision Learning: Towards Use of Known Operators in Neural Networks", "comments": "accepted on ICPR 2018", "journal-ref": "A. Maier, F. Schebesch, C. Syben, T. W\\\"urfl, S. Steidl, J.-H.\n  Choi, R. Fahrig, Precision Learning: Towards Use of Known Operators in Neural\n  Networks, in: 24rd International Conference on Pattern Recognition (ICPR),\n  2018, pp. 183-188", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the use of prior knowledge within neural networks.\nIn particular, we investigate the effect of a known transform within the\nmapping from input data space to the output domain. We demonstrate that use of\nknown transforms is able to change maximal error bounds.\n  In order to explore the effect further, we consider the problem of X-ray\nmaterial decomposition as an example to incorporate additional prior knowledge.\nWe demonstrate that inclusion of a non-linear function known from the physical\nproperties of the system is able to reduce prediction errors therewith\nimproving prediction quality from SSIM values of 0.54 to 0.88.\n  This approach is applicable to a wide set of applications in physics and\nsignal processing that provide prior knowledge on such transforms. Also maximal\nerror estimation and network understanding could be facilitated within the\ncontext of precision learning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:44:15 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 10:20:24 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 22:52:58 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 08:09:28 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Maier", "Andreas", ""], ["Schebesch", "Frank", ""], ["Syben", "Christopher", ""], ["W\u00fcrfl", "Tobias", ""], ["Steidl", "Stefan", ""], ["Choi", "Jang-Hwan", ""], ["Fahrig", "Rebecca", ""]]}, {"id": "1712.00377", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual\n  Question Answering", "comments": "15 pages, 10 figures. To appear in IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of studies have found that today's Visual Question Answering (VQA)\nmodels are heavily driven by superficial correlations in the training data and\nlack sufficient image grounding. To encourage development of models geared\ntowards the latter, we propose a new setting for VQA where for every question\ntype, train and test sets have different prior distributions of answers.\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\nrespectively). First, we evaluate several existing VQA models under this new\nsetting and show that their performance degrades significantly compared to the\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\nAnswering model (GVQA) that contains inductive biases and restrictions in the\narchitecture specifically designed to prevent the model from 'cheating' by\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\ndisentangles the recognition of visual concepts present in the image from the\nidentification of plausible answer space for a given question, enabling the\nmodel to more robustly generalize across different distributions of answers.\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\nseveral cases. GVQA offers strengths complementary to SAN when trained and\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\ntransparent and interpretable than existing VQA models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:48:50 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 15:32:06 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1712.00386", "submitter": "Michael Figurnov", "authors": "Michael Figurnov, Artem Sobolev, Dmitry Vetrov", "title": "Probabilistic Adaptive Computation Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model with discrete latent variables that control\nthe computation time in deep learning models such as ResNets and LSTMs. A prior\non the latent variables expresses the preference for faster computation. The\namount of computation for an input is determined via amortized maximum a\nposteriori (MAP) inference. MAP inference is performed using a novel stochastic\nvariational optimization method. The recently proposed Adaptive Computation\nTime mechanism can be seen as an ad-hoc relaxation of this model. We\ndemonstrate training using the general-purpose Concrete relaxation of discrete\nvariables. Evaluation on ResNet shows that our method matches the\nspeed-accuracy trade-off of Adaptive Computation Time, while allowing for\nevaluation with a simple deterministic procedure that has a lower memory\nfootprint.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 16:09:26 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Figurnov", "Michael", ""], ["Sobolev", "Artem", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1712.00427", "submitter": "Alejandro Frery", "authors": "Debanshu Ratha, Avik Bhattacharya, Alejandro C. Frery", "title": "Unsupervised Classification of PolSAR Data Using a Scattering Similarity\n  Measure Derived from a Geodesic Distance", "comments": "Accepted for publication at IEEE Geoscience and Remote Sensing\n  Letters", "journal-ref": null, "doi": "10.1109/LGRS.2017.2778749", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose a novel technique for obtaining scattering\ncomponents from Polarimetric Synthetic Aperture Radar (PolSAR) data using the\ngeodesic distance on the unit sphere. This geodesic distance is obtained\nbetween an elementary target and the observed Kennaugh matrix, and it is\nfurther utilized to compute a similarity measure between scattering mechanisms.\nThe normalized similarity measure for each elementary target is then modulated\nwith the total scattering power (Span). This measure is used to categorize\npixels into three categories i.e. odd-bounce, double-bounce and volume,\ndepending on which of the above scattering mechanisms dominate. Then the\nmaximum likelihood classifier of [J.-S. Lee, M. R. Grunes, E. Pottier, and L.\nFerro-Famil, Unsupervised terrain classification preserving polarimetric\nscattering characteristics, IEEE Trans. Geos. Rem. Sens., vol. 42, no. 4, pp.\n722731, April 2004.] based on the complex Wishart distribution is iteratively\nused for each category. Dominant scattering mechanisms are thus preserved in\nthis classification scheme. We show results for L-band AIRSAR and ALOS-2\ndatasets acquired over San Francisco and Mumbai, respectively. The scattering\nmechanisms are better preserved using the proposed methodology than the\nunsupervised classification results using the Freeman-Durden scattering powers\non an orientation angle (OA) corrected PolSAR image. Furthermore, (1) the\nscattering similarity is a completely non-negative quantity unlike the negative\npowers that might occur in double- bounce and odd-bounce scattering component\nunder Freeman Durden decomposition (FDD), and (2) the methodology can be\nextended to more canonical targets as well as for bistatic scattering.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 17:58:42 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Ratha", "Debanshu", ""], ["Bhattacharya", "Avik", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1712.00433", "submitter": "Zhishuai Zhang", "authors": "Zhishuai Zhang, Siyuan Qiao, Cihang Xie, Wei Shen, Bo Wang, Alan L.\n  Yuille", "title": "Single-Shot Object Detection with Enriched Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel single shot object detection network named Detection with\nEnriched Semantics (DES). Our motivation is to enrich the semantics of object\ndetection features within a typical deep detector, by a semantic segmentation\nbranch and a global activation module. The segmentation branch is supervised by\nweak segmentation ground-truth, i.e., no extra annotation is required. In\nconjunction with that, we employ a global activation module which learns\nrelationship between channels and object classes in a self-supervised manner.\nComprehensive experimental results on both PASCAL VOC and MS COCO detection\ndatasets demonstrate the effectiveness of the proposed method. In particular,\nwith a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of\n32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on\na Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on\nVOC2007 with an inference speed of 13.0 milliseconds per image.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 18:18:42 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 01:01:25 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zhang", "Zhishuai", ""], ["Qiao", "Siyuan", ""], ["Xie", "Cihang", ""], ["Shen", "Wei", ""], ["Wang", "Bo", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1712.00436", "submitter": "Nikola Bani\\'c", "authors": "Nikola Bani\\'c, Karlo Ko\\v{s}\\v{c}evi\\'c, and Sven Lon\\v{c}ari\\'c", "title": "Unsupervised Learning for Color Constancy", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most digital camera pipelines use color constancy methods to reduce the\ninfluence of illumination and camera sensor on the colors of scene objects. The\nhighest accuracy of color correction is obtained with learning-based color\nconstancy methods, but they require a significant amount of calibrated training\nimages with known ground-truth illumination. Such calibration is time\nconsuming, preferably done for each sensor individually, and therefore a major\nbottleneck in acquiring high color constancy accuracy. Statistics-based methods\ndo not require calibrated training images, but they are less accurate. In this\npaper an unsupervised learning-based method is proposed that learns its\nparameter values after approximating the unknown ground-truth illumination of\nthe training images, thus avoiding calibration. In terms of accuracy the\nproposed method outperforms all statistics-based and many learning-based\nmethods. An extension of the method is also proposed, which learns the needed\nparameters from non-calibrated images taken with one sensor and which can then\nbe successfully applied to images taken with another sensor. This effectively\nenables inter-camera unsupervised learning for color constancy. Additionally, a\nnew high quality color constancy benchmark dataset with 1707 calibrated images\nis created, used for testing, and made publicly available. The results are\npresented and discussed. The source code and the dataset are available at\nhttp://www.fer.unizg.hr/ipg/resources/color_constancy/.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 18:38:36 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 16:26:53 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 14:30:10 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2019 09:58:28 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Bani\u0107", "Nikola", ""], ["Ko\u0161\u010devi\u0107", "Karlo", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "1712.00479", "submitter": "Zachary Murez", "authors": "Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, Kyungnam\n  Kim", "title": "Image to Image Translation for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for unsupervised domain adaptation, which\nallows deep neural networks trained on a source domain to be tested on a\ndifferent target domain without requiring any training annotations in the\ntarget domain. This is achieved by adding extra networks and losses that help\nregularize the features extracted by the backbone encoder network. To this end\nwe propose the novel use of the recently proposed unpaired image-toimage\ntranslation framework to constrain the features extracted by the encoder\nnetwork. Specifically, we require that the features extracted are able to\nreconstruct the images in both domains. In addition we require that the\ndistribution of features extracted from images in the two domains are\nindistinguishable. Many recent works can be seen as specific cases of our\ngeneral framework. We apply our method for domain adaptation between MNIST,\nUSPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in\nclassification tasks, and also between GTA5 and Cityscapes datasets for a\nsegmentation task. We demonstrate state of the art performance on each of these\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 20:15:20 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Murez", "Zak", ""], ["Kolouri", "Soheil", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""], ["Kim", "Kyungnam", ""]]}, {"id": "1712.00489", "submitter": "Abhinav Gupta", "authors": "Abhinav Gupta, Yajie Miao, Leonardo Neves, Florian Metze", "title": "Visual Features for Context-Aware Speech Recognition", "comments": "5 pages and 3 figures", "journal-ref": "IEEE Xplore (ICASSP) (2017) 5020-5024", "doi": "10.1109/ICASSP.2017.7953112", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic transcriptions of consumer-generated multi-media content such as\n\"Youtube\" videos still exhibit high word error rates. Such data typically\noccupies a very broad domain, has been recorded in challenging conditions, with\ncheap hardware and a focus on the visual modality, and may have been\npost-processed or edited. In this paper, we extend our earlier work on adapting\nthe acoustic model of a DNN-based speech recognition system to an RNN language\nmodel and show how both can be adapted to the objects and scenes that can be\nautomatically detected in the video. We are working on a corpus of \"how-to\"\nvideos from the web, and the idea is that an object that can be seen (\"car\"),\nor a scene that is being detected (\"kitchen\") can be used to condition both\nmodels on the \"context\" of the recording, thereby reducing perplexity and\nimproving transcription. We achieve good improvements in both cases and compare\nand analyze the respective reductions in word error rate. We expect that our\nresults can be used for any type of speech processing in which \"context\"\ninformation is available, for example in robotics, man-machine interaction, or\nwhen indexing large audio-visual archives, and should ultimately help to bring\ntogether the \"video-to-text\" and \"speech-to-text\" communities.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 20:56:31 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Gupta", "Abhinav", ""], ["Miao", "Yajie", ""], ["Neves", "Leonardo", ""], ["Metze", "Florian", ""]]}, {"id": "1712.00497", "submitter": "Onur Ozdemir", "authors": "Onur Ozdemir, Benjamin Woodward, Andrew A. Berlin", "title": "Propagating Uncertainty in Multi-Stage Bayesian Convolutional Neural\n  Networks with Application to Pulmonary Nodule Detection", "comments": "NIPS Workshop on Bayesian Deep Learning, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of computer-aided detection (CAD) of pulmonary\nnodules, we introduce methods to propagate and fuse uncertainty information in\na multi-stage Bayesian convolutional neural network (CNN) architecture. The\nquestion we seek to answer is \"can we take advantage of the model uncertainty\nprovided by one deep learning model to improve the performance of the\nsubsequent deep learning models and ultimately of the overall performance in a\nmulti-stage Bayesian deep learning architecture?\". Our experiments show that\npropagating uncertainty through the pipeline enables us to improve the overall\nperformance in terms of both final prediction accuracy and model confidence.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 21:21:35 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ozdemir", "Onur", ""], ["Woodward", "Benjamin", ""], ["Berlin", "Andrew A.", ""]]}, {"id": "1712.00512", "submitter": "Jumana Dakka", "authors": "Jumana Dakka, Pouya Bashivan, Mina Gheiratmand, Irina Rish, Shantenu\n  Jha, Russell Greiner", "title": "Learning Neural Markers of Schizophrenia Disorder Using Recurrent Neural\n  Networks", "comments": "To be published as a workshop paper at NIPS 2017 Machine Learning for\n  Health (ML4H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart systems that can accurately diagnose patients with mental disorders and\nidentify effective treatments based on brain functional imaging data are of\ngreat applicability and are gaining much attention. Most previous machine\nlearning studies use hand-designed features, such as functional connectivity,\nwhich does not maintain the potential useful information in the spatial\nrelationship between brain regions and the temporal profile of the signal in\neach region. Here we propose a new method based on recurrent-convolutional\nneural networks to automatically learn useful representations from segments of\n4-D fMRI recordings. Our goal is to exploit both spatial and temporal\ninformation in the functional MRI movie (at the whole-brain voxel level) for\nidentifying patients with schizophrenia.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 22:57:15 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Dakka", "Jumana", ""], ["Bashivan", "Pouya", ""], ["Gheiratmand", "Mina", ""], ["Rish", "Irina", ""], ["Jha", "Shantenu", ""], ["Greiner", "Russell", ""]]}, {"id": "1712.00516", "submitter": "Samaneh Azadi", "authors": "Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli\n  Shechtman, Trevor Darrell", "title": "Multi-Content GAN for Few-Shot Font Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on the challenge of taking partial observations of\nhighly-stylized text and generalizing the observations to generate unobserved\nglyphs in the ornamented typeface. To generate a set of multi-content images\nfollowing a consistent style from very few examples, we propose an end-to-end\nstacked conditional GAN model considering content along channels and style\nalong network layers. Our proposed network transfers the style of given glyphs\nto the contents of unseen ones, capturing highly stylized fonts found in the\nreal-world such as those on movie posters or infographics. We seek to transfer\nboth the typographic stylization (ex. serifs and ears) as well as the textual\nstylization (ex. color gradients and effects.) We base our experiments on our\ncollected data set including 10,000 fonts with different styles and demonstrate\neffective generalization from a very small number of observed glyphs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 23:12:58 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Azadi", "Samaneh", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir", ""], ["Wang", "Zhaowen", ""], ["Shechtman", "Eli", ""], ["Darrell", "Trevor", ""]]}, {"id": "1712.00523", "submitter": "Stanislav Fort", "authors": "Stanislav Fort", "title": "Towards understanding feedback from supermassive black holes using\n  convolutional neural networks", "comments": "5 pages, 5 figures, accepted at Workshop on Deep Learning for\n  Physical Sciences (DLPS 2017), NIPS 2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supermassive black holes at centers of clusters of galaxies strongly interact\nwith their host environment via AGN feedback. Key tracers of such activity are\nX-ray cavities -- regions of lower X-ray brightness within the cluster. We\npresent an automatic method for detecting, and characterizing X-ray cavities in\nnoisy, low-resolution X-ray images. We simulate clusters of galaxies, insert\ncavities into them, and produce realistic low-quality images comparable to\nobservations at high redshifts. We then train a custom-built convolutional\nneural network to generate pixel-wise analysis of presence of cavities in a\ncluster. A ResNet architecture is then used to decode radii of cavities from\nthe pixel-wise predictions. We surpass the accuracy, stability, and speed of\ncurrent visual inspection based methods on simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 00:05:16 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fort", "Stanislav", ""]]}, {"id": "1712.00542", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Shunxing Bao, Camilo Bermudez, Andrew J.\n  Plassard, Jiaqi Liu, Yuang Yao, Albert Assad, Richard G. Abramson, Bennett A.\n  Landman", "title": "Splenomegaly Segmentation using Global Convolutional Kernels and\n  Conditional Generative Adversarial Networks", "comments": "SPIE Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spleen volume estimation using automated image segmentation technique may be\nused to detect splenomegaly (abnormally enlarged spleen) on Magnetic Resonance\nImaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN)\nsegmentation methods have demonstrated advantages for abdominal organ\nsegmentation. However, variations in both size and shape of the spleen on MRI\nimages may result in large false positive and false negative labeling when\ndeploying DCNN based methods. In this paper, we propose the Splenomegaly\nSegmentation Network (SSNet) to address spatial variations when segmenting\nextraordinarily large spleens. SSNet was designed based on the framework of\nimage-to-image conditional generative adversarial networks (cGAN).\nSpecifically, the Global Convolutional Network (GCN) was used as the generator\nto reduce false negatives, while the Markovian discriminator (PatchGAN) was\nused to alleviate false positives. A cohort of clinically acquired 3D MRI scans\n(both T1 weighted and T2 weighted) from patients with splenomegaly were used to\ntrain and test the networks. The experimental results demonstrated that a mean\nDice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNet\non independently tested MRI volumes of patients with splenomegaly.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 03:47:37 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Bao", "Shunxing", ""], ["Bermudez", "Camilo", ""], ["Plassard", "Andrew J.", ""], ["Liu", "Jiaqi", ""], ["Yao", "Yuang", ""], ["Assad", "Albert", ""], ["Abramson", "Richard G.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1712.00543", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Shunxing Bao, Prasanna Parvathaneni, Bennett A. Landman", "title": "Improved Stability of Whole Brain Surface Parcellation with Multi-Atlas\n  Segmentation", "comments": "SPIE Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation and cortical surface parcellation are essential in\nunderstanding the anatomical-functional relationships of the brain. Multi-atlas\nsegmentation has been regarded as one of the leading segmentation methods for\nthe whole brain segmentation. In our recent work, the multi-atlas technique has\nbeen adapted to surface reconstruction using a method called Multi-atlas CRUISE\n(MaCRUISE). The MaCRUISE method not only performed consistent volume-surface\nanalyses but also showed advantages on robustness compared with the FreeSurfer\nmethod. However, a detailed surface parcellation was not provided by MaCRUISE,\nwhich hindered the region of interest (ROI) based analyses on surfaces. Herein,\nthe MaCRUISE surface parcellation (MaCRUISEsp) method is proposed to perform\nthe surface parcellation upon the inner, central and outer surfaces that are\nreconstructed from MaCRUISE. MaCRUISEsp parcellates inner, central and outer\nsurfaces with 98 cortical labels respectively using a volume segmentation based\nsurface parcellation (VSBSP), following a topological correction step. To\nvalidate the performance of MaCRUISEsp, 21 scan-rescan magnetic resonance\nimaging (MRI) T1 volume pairs from the Kirby21 dataset were used to perform a\nreproducibility analyses. MaCRUISEsp achieved 0.948 on median Dice Similarity\nCoefficient (DSC) for central surfaces. Meanwhile, FreeSurfer achieved 0.905\nDSC for inner surfaces and 0.881 DSC for outer surfaces, while the proposed\nmethod achieved 0.929 DSC for inner surfaces and 0.835 DSC for outer surfaces.\nQualitatively, the results are encouraging, but are not directly comparable as\nthe two approaches use different definitions of cortical labels.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 03:48:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Huo", "Yuankai", ""], ["Bao", "Shunxing", ""], ["Parvathaneni", "Prasanna", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1712.00559", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua,\n  Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy", "title": "Progressive Neural Architecture Search", "comments": "To appear in ECCV 2018 as oral. The code and checkpoint for PNASNet-5\n  trained on ImageNet (both Mobile and Large) can now be downloaded from\n  https://github.com/tensorflow/models/tree/master/research/slim#Pretrained.\n  Also see https://github.com/chenxi116/PNASNet.TF for refactored and\n  simplified TensorFlow code; see https://github.com/chenxi116/PNASNet.pytorch\n  for exact conversion to PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 06:23:16 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 00:39:27 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 19:51:26 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Liu", "Chenxi", ""], ["Zoph", "Barret", ""], ["Neumann", "Maxim", ""], ["Shlens", "Jonathon", ""], ["Hua", "Wei", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""], ["Yuille", "Alan", ""], ["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1712.00575", "submitter": "Xi Zhang", "authors": "Di Ma, Xi Zhang, Xu Ouyang, Gady Agam", "title": "Lecture video indexing using boosted margin maximizing neural networks", "comments": "Accepted by ICMLA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for lecture video indexing using a\nboosted deep convolutional neural network system. The indexing is performed by\nmatching high quality slide images, for which text is either known or\nextracted, to lower resolution video frames with possible noise, perspective\ndistortion, and occlusions. We propose a deep neural network integrated with a\nboosting framework composed of two sub-networks targeting feature extraction\nand similarity determination to perform the matching. The trained network is\ngiven as input a pair of slide image and a candidate video frame image and\nproduces the similarity between them. A boosting framework is integrated into\nour proposed network during the training process. Experimental results show\nthat the proposed approach is much more capable of handling occlusion, spatial\ntransformations, and other types of noises when compared with known approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 09:08:05 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Xi", ""], ["Ouyang", "Xu", ""], ["Agam", "Gady", ""]]}, {"id": "1712.00580", "submitter": "Mihai Oltean", "authors": "Horea Mure\\c{s}an, Mihai Oltean", "title": "Fruit recognition from images using deep learning", "comments": "38 pages", "journal-ref": "Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42,\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new, high-quality, dataset of images containing\nfruits. We also present the results of some numerical experiment for training a\nneural network to detect fruits. We discuss the reason why we chose to use\nfruits in this project by proposing a few applications that could use this kind\nof neural network.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 09:48:37 GMT"}, {"version": "v10", "created": "Sun, 24 Jan 2021 11:19:36 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 07:08:33 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 14:34:09 GMT"}, {"version": "v4", "created": "Sat, 21 Jul 2018 08:12:45 GMT"}, {"version": "v5", "created": "Sun, 2 Sep 2018 19:28:53 GMT"}, {"version": "v6", "created": "Fri, 14 Sep 2018 12:03:30 GMT"}, {"version": "v7", "created": "Thu, 18 Oct 2018 08:27:42 GMT"}, {"version": "v8", "created": "Wed, 26 Dec 2018 20:29:37 GMT"}, {"version": "v9", "created": "Fri, 4 Jan 2019 13:18:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mure\u015fan", "Horea", ""], ["Oltean", "Mihai", ""]]}, {"id": "1712.00598", "submitter": "Elias Vansteenkiste", "authors": "Elias Vansteenkiste and Patrick Kern", "title": "Taming Adversarial Domain Transfer with Structural Constraints for Image\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to improve images of traffic scenes that are\ndegraded by natural causes such as fog, rain and limited visibility during the\nnight. For these applications, it is next to impossible to get pixel perfect\npairs of the same scene, with and without the degrading conditions. This makes\nit unsuitable for conventional supervised learning approaches, however, it is\neasy to collect unpaired images of the scenes in a perfect and in a degraded\ncondition. To enhance the images taken in a poor visibility condition, domain\ntransfer models can be trained to transform an image from the degraded to the\nclear domain. A well-known concept for unsupervised domain transfer are\ncycle-consistent generative adversarial models. Unfortunately, the resulting\ngenerators often change the structure of the scene. This causes an undesirable\nchange in the semantics. We propose three ways to cope with this problem\ndepending on the type of degradation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 12:37:34 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 12:42:16 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 14:24:36 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Vansteenkiste", "Elias", ""], ["Kern", "Patrick", ""]]}, {"id": "1712.00617", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Amaia Salvador, Miriam Bellver, Victor Campos, Manel Baradad, Ferran\n  Marques, Jordi Torres, Xavier Giro-i-Nieto", "title": "Recurrent Neural Networks for Semantic Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a recurrent model for semantic instance segmentation that\nsequentially generates binary masks and their associated class probabilities\nfor every object in an image. Our proposed system is trainable end-to-end from\nan input image to a sequence of labeled masks and, compared to methods relying\non object proposals, does not require post-processing steps on its output. We\nstudy the suitability of our recurrent model on three different instance\nsegmentation benchmarks, namely Pascal VOC 2012, CVPPP Plant Leaf Segmentation\nand Cityscapes. Further, we analyze the object sorting patterns generated by\nour model and observe that it learns to follow a consistent pattern, which\ncorrelates with the activations learned in the encoder part of our network.\nSource code and models are available at https://imatge-upc.github.io/rsis/\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 15:01:27 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 12:34:22 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 15:07:52 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2019 14:02:32 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Salvador", "Amaia", ""], ["Bellver", "Miriam", ""], ["Campos", "Victor", ""], ["Baradad", "Manel", ""], ["Marques", "Ferran", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1712.00621", "submitter": "Chongyi Li", "authors": "Chongyi Li, Jichang Guo, Fatih Porikli, Chunle Guo, Huzhu Fu, Xi Li", "title": "DR-Net: Transmission Steered Single Image Dehazing Network with Weakly\n  Supervised Refinement", "comments": "8 pages, 8 figures, submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress in image dehazing, several problems remain\nlargely unsolved such as robustness for varying scenes, the visual quality of\nreconstructed images, and effectiveness and flexibility for applications. To\ntackle these problems, we propose a new deep network architecture for single\nimage dehazing called DR-Net. Our model consists of three main subnetworks: a\ntransmission prediction network that predicts transmission map for the input\nimage, a haze removal network that reconstructs latent image steered by the\ntransmission map, and a refinement network that enhances the details and color\nproperties of the dehazed result via weakly supervised learning. Compared to\nprevious methods, our method advances in three aspects: (i) pure data-driven\nmodel; (ii) the end-to-end system; (iii) superior robustness, accuracy, and\napplicability. Extensive experiments demonstrate that our DR-Net outperforms\nthe state-of-the-art methods on both synthetic and real images in qualitative\nand quantitative metrics. Additionally, the utility of DR-Net has been\nillustrated by its potential usage in several important computer vision tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 15:23:43 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Jichang", ""], ["Porikli", "Fatih", ""], ["Guo", "Chunle", ""], ["Fu", "Huzhu", ""], ["Li", "Xi", ""]]}, {"id": "1712.00636", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R. Manmatha, Alexander J.\n  Smola, Philipp Kr\\\"ahenb\\\"uhl", "title": "Compressed Video Action Recognition", "comments": "CVPR 2018 (Selected for spotlight presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training robust deep video representations has proven to be much more\nchallenging than learning deep image representations. This is in part due to\nthe enormous size of raw video streams and the high temporal redundancy; the\ntrue and interesting signal is often drowned in too much irrelevant data.\nMotivated by that the superfluous information can be reduced by up to two\norders of magnitude by video compression (using H.264, HEVC, etc.), we propose\nto train a deep network directly on the compressed video.\n  This representation has a higher information density, and we found the\ntraining to be easier. In addition, the signals in a compressed video provide\nfree, albeit noisy, motion information. We propose novel techniques to use them\neffectively. Our approach is about 4.6 times faster than Res3D and 2.7 times\nfaster than ResNet-152. On the task of action recognition, our approach\noutperforms all the other methods on the UCF-101, HMDB-51, and Charades\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 16:47:41 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 18:14:00 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Zaheer", "Manzil", ""], ["Hu", "Hexiang", ""], ["Manmatha", "R.", ""], ["Smola", "Alexander J.", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1712.00661", "submitter": "Ziwei Liu", "authors": "Xiaohang Zhan, Ziwei Liu, Ping Luo, Xiaoou Tang, Chen Change Loy", "title": "Mix-and-Match Tuning for Self-Supervised Semantic Segmentation", "comments": "To appear in AAAI 2018 as a spotlight paper. More details at the\n  project page: http://mmlab.ie.cuhk.edu.hk/projects/M%26M/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks for semantic image segmentation typically require\nlarge-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training.\nTo reduce annotation efforts, self-supervised semantic segmentation is recently\nproposed to pre-train a network without any human-provided labels. The key of\nthis new form of learning is to design a proxy task (e.g. image colorization),\nfrom which a discriminative loss can be formulated on unlabeled data. Many\nproxy tasks, however, lack the critical supervision signals that could induce\ndiscriminative representation for the target image segmentation task. Thus\nself-supervision's performance is still far from that of supervised\npre-training. In this study, we overcome this limitation by incorporating a\n\"mix-and-match\" (M&M) tuning stage in the self-supervision pipeline. The\nproposed approach is readily pluggable to many self-supervision methods and\ndoes not use more annotated samples than the original process. Yet, it is\ncapable of boosting the performance of target image segmentation task to\nsurpass fully-supervised pre-trained counterpart. The improvement is made\npossible by better harnessing the limited pixel-wise annotations in the target\ndataset. Specifically, we first introduce the \"mix\" stage, which sparsely\nsamples and mixes patches from the target set to reflect rich and diverse local\npatch statistics of target images. A \"match\" stage then forms a class-wise\nconnected graph, which can be used to derive a strong triplet-based\ndiscriminative loss for fine-tuning the network. Our paradigm follows the\nstandard practice in existing self-supervised studies and no extra data or\nlabel is required. With the proposed M&M approach, for the first time, a\nself-supervision method can achieve comparable or even better performance\ncompared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset\nand CityScapes dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 20:25:37 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 06:53:27 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 00:16:05 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhan", "Xiaohang", ""], ["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1712.00684", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Linh Tran, Yannis Panagakis, Maja Pantic", "title": "GAGAN: Geometry-Aware Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models learned through adversarial training have become\nincreasingly popular for their ability to generate naturalistic image textures.\nHowever, aside from their texture, the visual appearance of objects is\nsignificantly influenced by their shape geometry; information which is not\ntaken into account by existing generative models. This paper introduces the\nGeometry-Aware Generative Adversarial Networks (GAGAN) for incorporating\ngeometric information into the image generation process. Specifically, in GAGAN\nthe generator samples latent variables from the probability space of a\nstatistical shape model. By mapping the output of the generator to a canonical\ncoordinate frame through a differentiable geometric transformation, we enforce\nthe geometry of the objects and add an implicit connection from the prior to\nthe generated object. Experimental results on face generation indicate that the\nGAGAN can generate realistic images of faces with arbitrary facial attributes\nsuch as facial expression, pose, and morphology, that are of better quality\nthan current GAN-based methods. Our method can be used to augment any existing\nGAN architecture and improve the quality of the images generated.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 00:12:41 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 17:01:20 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 22:11:56 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kossaifi", "Jean", ""], ["Tran", "Linh", ""], ["Panagakis", "Yannis", ""], ["Pantic", "Maja", ""]]}, {"id": "1712.00704", "submitter": "Shengke Xue", "authors": "Shengke Xue, Wenyuan Qiu, Fan Liu, and Xinyu Jin", "title": "Low-Rank Tensor Completion by Truncated Nuclear Norm Regularization", "comments": "Accepted as a poster presentation at the 24th International\n  Conference on Pattern Recognition in 20-24 August 2018, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, low-rank tensor completion has gained cumulative attention in\nrecovering incomplete visual data whose partial elements are missing. By taking\na color image or video as a three-dimensional (3D) tensor, previous studies\nhave suggested several definitions of tensor nuclear norm. However, they have\nlimitations and may not properly approximate the real rank of a tensor.\nBesides, they do not explicitly use the low-rank property in optimization. It\nis proved that the recently proposed truncated nuclear norm (TNN) can replace\nthe traditional nuclear norm, as a better estimation to the rank of a matrix.\nThus, this paper presents a new method called the tensor truncated nuclear norm\n(T-TNN), which proposes a new definition of tensor nuclear norm and extends the\ntruncated nuclear norm from the matrix case to the tensor case. Beneficial from\nthe low rankness of TNN, our approach improves the efficacy of tensor\ncompletion. We exploit the previously proposed tensor singular value\ndecomposition and the alternating direction method of multipliers in\noptimization. Extensive experiments on real-world videos and images demonstrate\nthat the performance of our approach is superior to those of existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 03:40:08 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 13:51:19 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 01:58:16 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 14:09:08 GMT"}, {"version": "v5", "created": "Mon, 28 May 2018 03:03:14 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Xue", "Shengke", ""], ["Qiu", "Wenyuan", ""], ["Liu", "Fan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1712.00712", "submitter": "Wellington Pinheiro dos Santos", "authors": "Wellington Pinheiro dos Santos, Ricardo Emmanuel de Souza, Pl\\'inio B.\n  dos Santos Filho", "title": "Evaluation of Alzheimer's Disease by Analysis of MR Images using\n  Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the\n  ADC Maps", "comments": "29th Annual Conference of the IEEE Engineering in Medicine and\n  Biology Society - EMBC 2007", "journal-ref": null, "doi": "10.1109/IEMBS.2007.4352740", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common cause of dementia, yet hard to\ndiagnose precisely without invasive techniques, particularly at the onset of\nthe disease. This work approaches image analysis and classification of\nsynthetic multispectral images composed by diffusion-weighted magnetic\nresonance (MR) cerebral images for the evaluation of cerebrospinal fluid area\nand measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging\nsystem was used to acquire all images presented. The classification methods are\nbased on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We\nassume the classes of interest can be separated by hyperquadrics. Therefore, a\n2-degree polynomial network is used to classify the original image, generating\nthe ground truth image. The classification results are used to improve the\nusual analysis of the apparent diffusion coefficient map.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 05:54:40 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Santos", "Wellington Pinheiro dos", ""], ["de Souza", "Ricardo Emmanuel", ""], ["Filho", "Pl\u00ednio B. dos Santos", ""]]}, {"id": "1712.00714", "submitter": "Nader Akoury", "authors": "Nader Akoury and Anh Nguyen", "title": "Spatial PixelCNN: Generating Images from Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose Spatial PixelCNN, a conditional autoregressive model\nthat generates images from small patches. By conditioning on a grid of pixel\ncoordinates and global features extracted from a Variational Autoencoder (VAE),\nwe are able to train on patches of images, and reproduce the full-sized image.\nWe show that it not only allows for generating high quality samples at the same\nresolution as the underlying dataset, but is also capable of upscaling images\nto arbitrary resolutions (tested at resolutions up to $50\\times$) on the MNIST\ndataset. Compared to a PixelCNN++ baseline, Spatial PixelCNN quantitatively and\nqualitatively achieves similar performance on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:02:23 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Akoury", "Nader", ""], ["Nguyen", "Anh", ""]]}, {"id": "1712.00720", "submitter": "Huichao Hong", "authors": "Huichao Hong, Lixin Zheng, Jianqing Zhu, Shuwan Pan, Kaiting Zhou", "title": "Automatic Recognition of Coal and Gangue based on Convolution Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a gangue sorting system,and built a convolutional neural network\nmodel based on AlexNet. Data enhancement and transfer learning are used to\nsolve the problem which the convolution neural network has insufficient\ntraining data in the training stage. An object detection and region clipping\nalgorithm is proposed to adjust the training image data to the optimum size.\nCompared with traditional neural network and SVM algorithm, this algorithm has\nhigher recognition rate for coal and coal gangue, and provides important\nreference for identification and separation of coal and gangue.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:37:15 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hong", "Huichao", ""], ["Zheng", "Lixin", ""], ["Zhu", "Jianqing", ""], ["Pan", "Shuwan", ""], ["Zhou", "Kaiting", ""]]}, {"id": "1712.00721", "submitter": "Jialiang Zhang", "authors": "Jialiang Zhang, Xiongwei Wu, Jianke Zhu, Steven C.H. Hoi", "title": "Feature Agglomeration Networks for Single Stage Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed promising results of face detection using deep\nlearning. Despite making remarkable progresses, face detection in the wild\nremains an open research challenge especially when detecting faces at vastly\ndifferent scales and characteristics. In this paper, we propose a novel simple\nyet effective framework of \"Feature Agglomeration Networks\" (FANet) to build a\nnew single stage face detector, which not only achieves state-of-the-art\nperformance but also runs efficiently. As inspired by Feature Pyramid Networks\n(FPN), the key idea of our framework is to exploit inherent multi-scale\nfeatures of a single convolutional neural network by aggregating higher-level\nsemantic feature maps of different scales as contextual cues to augment\nlower-level feature maps via a hierarchical agglomeration manner at marginal\nextra computation cost. We further propose a Hierarchical Loss to effectively\ntrain the FANet model. We evaluate the proposed FANet detector on several\npublic face detection benchmarks, including PASCAL face, FDDB and WIDER FACE\ndatasets and achieved state-of-the-art results. Our detector can run in real\ntime for VGA-resolution images on GPU.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:38:09 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 05:42:16 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhang", "Jialiang", ""], ["Wu", "Xiongwei", ""], ["Zhu", "Jianke", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1712.00725", "submitter": "Abhinav Gupta", "authors": "Laura Graesser, Abhinav Gupta, Lakshay Sharma, Evelina Bakhturina", "title": "Sentiment Classification using Images and Label Embeddings", "comments": "13 pages, 3 figures, 9 tables. Technical report for Statistical\n  Natural Language Processing Project (NYU CS - Fall 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we analysed how much semantic information images carry, and\nhow much value image data can add to sentiment analysis of the text associated\nwith the images. To better understand the contribution from images, we compared\nmodels which only made use of image data, models which only made use of text\ndata, and models which combined both data types. We also analysed if this\napproach could help sentiment classifiers generalize to unknown sentiments.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 07:20:15 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Graesser", "Laura", ""], ["Gupta", "Abhinav", ""], ["Sharma", "Lakshay", ""], ["Bakhturina", "Evelina", ""]]}, {"id": "1712.00726", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Nuno Vasconcelos", "title": "Cascade R-CNN: Delving into High Quality Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, an intersection over union (IoU) threshold is required\nto define positives and negatives. An object detector, trained with low IoU\nthreshold, e.g. 0.5, usually produces noisy detections. However, detection\nperformance tends to degrade with increasing the IoU thresholds. Two main\nfactors are responsible for this: 1) overfitting during training, due to\nexponentially vanishing positive samples, and 2) inference-time mismatch\nbetween the IoUs for which the detector is optimal and those of the input\nhypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is\nproposed to address these problems. It consists of a sequence of detectors\ntrained with increasing IoU thresholds, to be sequentially more selective\nagainst close false positives. The detectors are trained stage by stage,\nleveraging the observation that the output of a detector is a good distribution\nfor training the next higher quality detector. The resampling of progressively\nimproved hypotheses guarantees that all detectors have a positive set of\nexamples of equivalent size, reducing the overfitting problem. The same cascade\nprocedure is applied at inference, enabling a closer match between the\nhypotheses and the detector quality of each stage. A simple implementation of\nthe Cascade R-CNN is shown to surpass all single-model object detectors on the\nchallenging COCO dataset. Experiments also show that the Cascade R-CNN is\nwidely applicable across detector architectures, achieving consistent gains\nindependently of the baseline detector strength. The code will be made\navailable at https://github.com/zhaoweicai/cascade-rcnn.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 07:24:45 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Cai", "Zhaowei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1712.00733", "submitter": "Guohao Li", "authors": "Guohao Li, Hang Su, Wenwu Zhu", "title": "Incorporating External Knowledge to Answer Open-Domain Visual Questions\n  with Dynamic Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has attracted much attention since it offers\ninsight into the relationships between the multi-modal analysis of images and\nnatural language. Most of the current algorithms are incapable of answering\nopen-domain questions that require to perform reasoning beyond the image\ncontents. To address this issue, we propose a novel framework which endows the\nmodel capabilities in answering more complex questions by leveraging massive\nexternal knowledge with dynamic memory networks. Specifically, the questions\nalong with the corresponding images trigger a process to retrieve the relevant\ninformation in external knowledge bases, which are embedded into a continuous\nvector space by preserving the entity-relation structures. Afterwards, we\nemploy dynamic memory networks to attend to the large body of facts in the\nknowledge graph and images, and then perform reasoning over these facts to\ngenerate corresponding answers. Extensive experiments demonstrate that our\nmodel not only achieves the state-of-the-art performance in the visual question\nanswering task, but can also answer open-domain questions effectively by\nleveraging the external knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 08:41:35 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Guohao", ""], ["Su", "Hang", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1712.00736", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Junzhi Yu, Shihan Kong, Zhengxing Wu, Xi Fang, Li Wen", "title": "Towards Real-Time Advancement of Underwater Visual Quality with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low visual quality has prevented underwater robotic vision from a wide range\nof applications. Although several algorithms have been developed, real-time and\nadaptive methods are deficient for real-world tasks. In this paper, we address\nthis difficulty based on generative adversarial networks (GAN), and propose a\nGAN-based restoration scheme (GAN-RS). In particular, we develop a multi-branch\ndiscriminator including an adversarial branch and a critic branch for the\npurpose of simultaneously preserving image content and removing underwater\nnoise. In addition to adversarial learning, a novel dark channel prior loss\nalso promotes the generator to produce realistic vision. More specifically, an\nunderwater index is investigated to describe underwater properties, and a loss\nfunction based on the underwater index is designed to train the critic branch\nfor underwater noise suppression. Through extensive comparisons on visual\nquality and feature restoration, we confirm the superiority of the proposed\napproach. Consequently, the GAN-RS can adaptively improve underwater visual\nquality in real time and induce an overall superior restoration performance.\nFinally, a real-world experiment is conducted on the seabed for grasping marine\nproducts, and the results are quite promising. The source code is publicly\navailable at https://github.com/SeanChenxy/GAN_RS.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 09:39:29 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 15:12:50 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 09:14:26 GMT"}, {"version": "v4", "created": "Wed, 25 Mar 2020 05:32:04 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Chen", "Xingyu", ""], ["Yu", "Junzhi", ""], ["Kong", "Shihan", ""], ["Wu", "Zhengxing", ""], ["Fang", "Xi", ""], ["Wen", "Li", ""]]}, {"id": "1712.00779", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, Aarti Singh", "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of\n  Spurious Local Minima", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a one-hidden-layer neural network with\nnon-overlapping convolutional layer and ReLU activation, i.e., $f(\\mathbf{Z},\n\\mathbf{w}, \\mathbf{a}) = \\sum_j a_j\\sigma(\\mathbf{w}^T\\mathbf{Z}_j)$, in which\nboth the convolutional weights $\\mathbf{w}$ and the output weights $\\mathbf{a}$\nare parameters to be learned. When the labels are the outputs from a teacher\nnetwork of the same architecture with fixed weights $(\\mathbf{w}^*,\n\\mathbf{a}^*)$, we prove that with Gaussian input $\\mathbf{Z}$, there is a\nspurious local minimizer. Surprisingly, in the presence of the spurious local\nminimizer, gradient descent with weight normalization from randomly initialized\nweights can still be proven to recover the true parameters with constant\nprobability, which can be boosted to probability $1$ with multiple restarts. We\nalso show that with constant probability, the same procedure could also\nconverge to the spurious local minimum, showing that the local minimum plays a\nnon-trivial role in the dynamics of gradient descent. Furthermore, a\nquantitative analysis shows that the gradient descent dynamics has two phases:\nit starts off slow, but converges much faster after several iterations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 15:00:35 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 00:41:03 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Tian", "Yuandong", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""]]}, {"id": "1712.00796", "submitter": "Giorgos Bouritsas", "authors": "Giorgos Bouritsas, Petros Koutras, Athanasia Zlatintsi and Petros\n  Maragos", "title": "Multimodal Visual Concept Learning with Weakly Supervised Techniques", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the availability of a huge amount of video data accompanied by\ndescriptive texts, it is not always easy to exploit the information contained\nin natural language in order to automatically recognize video concepts. Towards\nthis goal, in this paper we use textual cues as means of supervision,\nintroducing two weakly supervised techniques that extend the Multiple Instance\nLearning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and\nthe Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes\nthe spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets,\nwhile the latter models different interpretations of each description's\nsemantics with Probabilistic Labels, both formulated through a convex\noptimization algorithm. In addition, we provide a novel technique to extract\nweak labels in the presence of complex semantics, that consists of semantic\nsimilarity computations. We evaluate our methods on two distinct problems,\nnamely face and action recognition, in the challenging and realistic setting of\nmovies accompanied by their screenplays, contained in the COGNIMUSE database.\nWe show that, on both tasks, our method considerably outperforms a\nstate-of-the-art weakly supervised approach, as well as other baselines.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 16:51:56 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 11:30:40 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 18:29:11 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Bouritsas", "Giorgos", ""], ["Koutras", "Petros", ""], ["Zlatintsi", "Athanasia", ""], ["Maragos", "Petros", ""]]}, {"id": "1712.00818", "submitter": "Sudipta Sinha", "authors": "Daniel Scharstein, Tatsunori Taniai, Sudipta N. Sinha", "title": "Semi-Global Stereo Matching with Surface Orientation Priors", "comments": "extended draft of 3DV 2017 (spotlight) paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Global Matching (SGM) is a widely-used efficient stereo matching\ntechnique. It works well for textured scenes, but fails on untextured slanted\nsurfaces due to its fronto-parallel smoothness assumption. To remedy this\nproblem, we propose a simple extension, termed SGM-P, to utilize precomputed\nsurface orientation priors. Such priors favor different surface slants in\ndifferent 2D image regions or 3D scene regions and can be derived in various\nways. In this paper we evaluate plane orientation priors derived from stereo\nmatching at a coarser resolution and show that such priors can yield\nsignificant performance gains for difficult weakly-textured scenes. We also\nexplore surface normal priors derived from Manhattan-world assumptions, and we\nanalyze the potential performance gains using oracle priors derived from\nground-truth data. SGM-P only adds a minor computational overhead to SGM and is\nan attractive alternative to more complex methods employing higher-order\nsmoothness terms.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 18:59:48 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Scharstein", "Daniel", ""], ["Taniai", "Tatsunori", ""], ["Sinha", "Sudipta N.", ""]]}, {"id": "1712.00840", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Przemys{\\l}aw Wa{\\l}\\k{e}ga and Carl\n  Schultz", "title": "Visual Explanation by High-Level Abduction: On Answer-Set Programming\n  Driven Reasoning about Moving Objects", "comments": "Preprint of final publication published as part of AAAI 2018: J.\n  Suchan., M. Bhatt, Wa{\\l}\\k{e}ga, P., Schultz, C. (2018). Visual Explanation\n  by High-Level Abduction: On Answer-Set Programming Driven Reasoning about\n  Moving Objects. In AAAI 2018: Proceedings of the Thirty-Second AAAI\n  Conference on Artificial Intelligence, February 2-7, 2018, New Orleans, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid architecture for systematically computing robust visual\nexplanation(s) encompassing hypothesis formation, belief revision, and default\nreasoning with video data. The architecture consists of two tightly integrated\nsynergistic components: (1) (functional) answer set programming based abductive\nreasoning with space-time tracklets as native entities; and (2) a visual\nprocessing pipeline for detection based object tracking and motion analysis.\n  We present the formal framework, its general implementation as a\n(declarative) method in answer set programming, and an example application and\nevaluation based on two diverse video datasets: the MOTChallenge benchmark\ndeveloped by the vision community, and a recently developed Movie Dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 21:17:07 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Wa\u0142\u0119ga", "Przemys\u0142aw", ""], ["Schultz", "Carl", ""]]}, {"id": "1712.00863", "submitter": "Yueru Chen", "authors": "Yueru Chen, Pranav Aggarwal, Jongmoo Choi, and C.-C. Jay Kuo", "title": "A Deep Learning Approach to Drone Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A drone monitoring system that integrates deep-learning-based detection and\ntracking modules is proposed in this work. The biggest challenge in adopting\ndeep learning methods for drone detection is the limited amount of training\ndrone images. To address this issue, we develop a model-based drone\naugmentation technique that automatically generates drone images with a\nbounding box label on drone's location. To track a small flying drone, we\nutilize the residual information between consecutive image frames. Finally, we\npresent an integrated detection and tracking system that outperforms the\nperformance of each individual module containing detection or tracking only.\nThe experiments show that, even being trained on synthetic data, the proposed\nsystem performs well on real world drone images with complex background. The\nUSC drone detection and tracking dataset with user labeled bounding boxes is\navailable to the public.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 00:30:58 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Chen", "Yueru", ""], ["Aggarwal", "Pranav", ""], ["Choi", "Jongmoo", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1712.00886", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Honghui Shi and Jiahui Yu and Hai Phan and Rogerio\n  Feris and Liangliang Cao and Ding Liu and Xinchao Wang and Thomas Huang and\n  Marios Savvides", "title": "Improving Object Detection from Scratch via Gated Feature Reuse", "comments": "Accepted in BMVC 2019. Code: https://github.com/szq0214/GFR-DSOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple and parameter-efficient drop-in module for\none-stage object detectors like SSD when learning from scratch (i.e., without\npre-trained models). We call our module GFR (Gated Feature Reuse), which\nexhibits two main advantages. First, we introduce a novel gate-controlled\nprediction strategy enabled by Squeeze-and-Excitation to adaptively enhance or\nattenuate supervision at different scales based on the input object size. As a\nresult, our model is more effective in detecting diverse sizes of objects.\nSecond, we propose a feature-pyramids structure to squeeze rich spatial and\nsemantic features into a single prediction layer, which strengthens feature\nrepresentation and reduces the number of parameters to learn. We apply the\nproposed structure on DSOD and SSD detection frameworks, and evaluate the\nperformance on PASCAL VOC 2007, 2012 and COCO datasets. With fewer model\nparameters, GFR-DSOD outperforms the baseline DSOD by 1.4%, 1.1%, 1.7% and\n0.6%, respectively. GFR-SSD also outperforms the original SSD and SSD with\ndense prediction by 3.6% and 2.8% on VOC 2007 dataset. Code is available at:\nhttps://github.com/szq0214/GFR-DSOD .\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 03:03:53 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 16:37:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Shi", "Honghui", ""], ["Yu", "Jiahui", ""], ["Phan", "Hai", ""], ["Feris", "Rogerio", ""], ["Cao", "Liangliang", ""], ["Liu", "Ding", ""], ["Wang", "Xinchao", ""], ["Huang", "Thomas", ""], ["Savvides", "Marios", ""]]}, {"id": "1712.00891", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Data Dropout in Arbitrary Basis for Deep Network Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in training deep networks with high capacity is to\nensure that the trained network works well when presented with new inputs\noutside the training dataset. Dropout is an effective regularization technique\nto boost the network generalization in which a random subset of the elements of\nthe given data and the extracted features are set to zero during the training\nprocess. In this paper, a new randomized regularization technique in which we\nwithhold a random part of the data without necessarily turning off the\nneurons/data-elements is proposed. In the proposed method, of which the\nconventional dropout is shown to be a special case, random data dropout is\nperformed in an arbitrary basis, hence the designation Generalized Dropout. We\nalso present a framework whereby the proposed technique can be applied\nefficiently to convolutional neural networks. The presented numerical\nexperiments demonstrate that the proposed technique yields notable performance\ngain. Generalized Dropout provides new insight into the idea of dropout, shows\nthat we can achieve different performance gains by using different bases\nmatrices, and opens up a new research question as of how to choose optimal\nbases matrices that achieve maximal performance gain.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 03:29:38 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 02:55:21 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1712.00899", "submitter": "Fei Gao", "authors": "Jun Yu, Xingxin Xu, Fei Gao, Shengjie Shi, Meng Wang, Dacheng Tao, and\n  Qingming Huang", "title": "Towards Realistic Face Photo-Sketch Synthesis via Composition-Aided GANs", "comments": "10 pages, 8 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face photo-sketch synthesis aims at generating a facial sketch/photo\nconditioned on a given photo/sketch. It is of wide applications including\ndigital entertainment and law enforcement. Precisely depicting face\nphotos/sketches remains challenging due to the restrictions on structural\nrealism and textural consistency. While existing methods achieve compelling\nresults, they mostly yield blurred effects and great deformation over various\nfacial components, leading to the unrealistic feeling of synthesized images. To\ntackle this challenge, in this work, we propose to use the facial composition\ninformation to help the synthesis of face sketch/photo. Specially, we propose a\nnovel composition-aided generative adversarial network (CA-GAN) for face\nphoto-sketch synthesis. In CA-GAN, we utilize paired inputs including a face\nphoto/sketch and the corresponding pixel-wise face labels for generating a\nsketch/photo. In addition, to focus training on hard-generated components and\ndelicate facial structures, we propose a compositional reconstruction loss.\nFinally, we use stacked CA-GANs (SCA-GAN) to further rectify defects and add\ncompelling details. Experimental results show that our method is capable of\ngenerating both visually comfortable and identity-preserving face\nsketches/photos over a wide range of challenging data. Our method achieves the\nstate-of-the-art quality, reducing best previous Frechet Inception distance\n(FID) by a large margin. Besides, we demonstrate that the proposed method is of\nconsiderable generalization ability. We have made our code and results publicly\navailable: https://fei-hdu.github.io/ca-gan/.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 04:24:19 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 06:20:40 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 08:12:47 GMT"}, {"version": "v4", "created": "Thu, 9 Jan 2020 03:35:56 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Yu", "Jun", ""], ["Xu", "Xingxin", ""], ["Gao", "Fei", ""], ["Shi", "Shengjie", ""], ["Wang", "Meng", ""], ["Tao", "Dacheng", ""], ["Huang", "Qingming", ""]]}, {"id": "1712.00912", "submitter": "Jong Chul Ye", "authors": "Jaejun Yoo, Sohail Sabir, Duchang Heo, Kee Hyun Kim, Abdul Wahab,\n  Yoonseok Choi, Seul-I Lee, Eun Young Chae, Hak Hee Kim, Young Min Bae,\n  Young-wook Choi, Seungryong Cho, and Jong Chul Ye", "title": "Deep Learning Diffuse Optical Tomography", "comments": "Accepted for IEEE Trans. on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2936522", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffuse optical tomography (DOT) has been investigated as an alternative\nimaging modality for breast cancer detection thanks to its excellent contrast\nto hemoglobin oxidization level. However, due to the complicated non-linear\nphoton scattering physics and ill-posedness, the conventional reconstruction\nalgorithms are sensitive to imaging parameters such as boundary conditions. To\naddress this, here we propose a novel deep learning approach that learns\nnon-linear photon scattering physics and obtains an accurate three dimensional\n(3D) distribution of optical anomalies. In contrast to the traditional\nblack-box deep learning approaches, our deep network is designed to invert the\nLippman-Schwinger integral equation using the recent mathematical theory of\ndeep convolutional framelets. As an example of clinical relevance, we applied\nthe method to our prototype DOT system. We show that our deep neural network,\ntrained with only simulation data, can accurately recover the location of\nanomalies within biomimetic phantoms and live animals without the use of an\nexogenous contrast agent.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 05:47:10 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 03:46:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yoo", "Jaejun", ""], ["Sabir", "Sohail", ""], ["Heo", "Duchang", ""], ["Kim", "Kee Hyun", ""], ["Wahab", "Abdul", ""], ["Choi", "Yoonseok", ""], ["Lee", "Seul-I", ""], ["Chae", "Eun Young", ""], ["Kim", "Hak Hee", ""], ["Bae", "Young Min", ""], ["Choi", "Young-wook", ""], ["Cho", "Seungryong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1712.00926", "submitter": "Bolun Cai", "authors": "Bolun Cai, Xiangmin Xu, Kailing Guo, Kui Jia, Dacheng Tao", "title": "Deep Sampling Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks achieve excellent image up-sampling\nperformance. However, CNN-based methods tend to restore high-resolution results\nhighly depending on traditional interpolations (e.g. bicubic). In this paper,\nwe present a deep sampling network (DSN) for down-sampling and up-sampling\nwithout any cheap interpolation. First, the down-sampling subnetwork is trained\nwithout supervision, thereby preserving more information and producing better\nvisual effects in the low-resolution image. Second, the up-sampling subnetwork\nlearns a sub-pixel residual with dense connections to accelerate convergence\nand improve performance. DSN's down-sampling subnetwork can be used to generate\nphoto-realistic low-resolution images and replace traditional down-sampling\nmethod in image processing. With the powerful down-sampling process, the\nco-training DSN set a new state-of-the-art performance for image\nsuper-resolution. Moreover, DSN is compatible with existing image codecs to\nimprove image compression.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 06:46:41 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 02:30:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cai", "Bolun", ""], ["Xu", "Xiangmin", ""], ["Guo", "Kailing", ""], ["Jia", "Kui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1712.00955", "submitter": "Jingdong Wang", "authors": "Jingdong Wang and Ting Zhang", "title": "Composite Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the compact coding approach to approximate nearest\nneighbor search. We introduce a composite quantization framework. It uses the\ncomposition of several ($M$) elements, each of which is selected from a\ndifferent dictionary, to accurately approximate a $D$-dimensional vector, thus\nyielding accurate search, and represents the data vector by a short code\ncomposed of the indices of the selected elements in the corresponding\ndictionaries. Our key contribution lies in introducing a near-orthogonality\nconstraint, which makes the search efficiency is guaranteed as the cost of the\ndistance computation is reduced to $O(M)$ from $O(D)$ through a distance table\nlookup scheme. The resulting approach is called near-orthogonal composite\nquantization. We theoretically justify the equivalence between near-orthogonal\ncomposite quantization and minimizing an upper bound of a function formed by\njointly considering the quantization error and the search cost according to a\ngeneralized triangle inequality. We empirically show the efficacy of the\nproposed approach over several benchmark datasets. In addition, we demonstrate\nthe superior performances in other three applications: combination with\ninverted multi-index, quantizing the query for mobile search, and inner-product\nsimilarity search.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 08:48:44 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wang", "Jingdong", ""], ["Zhang", "Ting", ""]]}, {"id": "1712.00960", "submitter": "Zuoxin Li", "authors": "Zuoxin Li and Fuqiang Zhou", "title": "FSSD: Feature Fusion Single Shot Multibox Detector", "comments": "add project code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  SSD (Single Shot Multibox Detector) is one of the best object detection\nalgorithms with both high accuracy and fast speed. However, SSD's feature\npyramid detection method makes it hard to fuse the features from different\nscales. In this paper, we proposed FSSD (Feature Fusion Single Shot Multibox\nDetector), an enhanced SSD with a novel and lightweight feature fusion module\nwhich can improve the performance significantly over SSD with just a little\nspeed drop. In the feature fusion module, features from different layers with\ndifferent scales are concatenated together, followed by some down-sampling\nblocks to generate new feature pyramid, which will be fed to multibox detectors\nto predict the final detection results. On the Pascal VOC 2007 test, our\nnetwork can achieve 82.7 mAP (mean average precision) at the speed of 65.8 FPS\n(frame per second) with the input size 300$\\times$300 using a single Nvidia\n1080Ti GPU. In addition, our result on COCO is also better than the\nconventional SSD with a large margin. Our FSSD outperforms a lot of\nstate-of-the-art object detection algorithms in both aspects of accuracy and\nspeed. Code is available at https://github.com/lzx1413/CAFFE_SSD/tree/fssd.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 09:05:55 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 02:27:48 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 03:47:18 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Li", "Zuoxin", ""], ["Zhou", "Fuqiang", ""]]}, {"id": "1712.00967", "submitter": "Christoph Wick", "authors": "Christoph Wick and Frank Puppe", "title": "Leaf Identification Using a Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have become popular especially in\ncomputer vision in the last few years because they achieved outstanding\nperformance on different tasks, such as image classifications. We propose a\nnine-layer CNN for leaf identification using the famous Flavia and Foliage\ndatasets. Usually the supervised learning of deep CNNs requires huge datasets\nfor training. However, the used datasets contain only a few examples per plant\nspecies. Therefore, we apply data augmentation and transfer learning to prevent\nour network from overfitting. The trained CNNs achieve recognition rates above\n99% on the Flavia and Foliage datasets, and slightly outperform current methods\nfor leaf classification.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 09:17:53 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wick", "Christoph", ""], ["Puppe", "Frank", ""]]}, {"id": "1712.00971", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Bernhard Kratzwald, Danda Pani Paudel, Jiqing Wu, Luc Van\n  Gool", "title": "Face Translation between Images and Videos using Identity-aware CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new problem of unpaired face translation between images\nand videos, which can be applied to facial video prediction and enhancement. In\nthis problem there exist two major technical challenges: 1) designing a robust\ntranslation model between static images and dynamic videos, and 2) preserving\nfacial identity during image-video translation. To address such two problems,\nwe generalize the state-of-the-art image-to-image translation network\n(Cycle-Consistent Adversarial Networks) to the image-to-video/video-to-image\ntranslation context by exploiting a image-video translation model and an\nidentity preservation model. In particular, we apply the state-of-the-art\nWasserstein GAN technique to the setting of image-video translation for better\nconvergence, and we meanwhile introduce a face verificator to ensure the\nidentity. Experiments on standard image/video face datasets demonstrate the\neffectiveness of the proposed model in both terms of qualitative and\nquantitative evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 09:27:14 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Huang", "Zhiwu", ""], ["Kratzwald", "Bernhard", ""], ["Paudel", "Danda Pani", ""], ["Wu", "Jiqing", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.00981", "submitter": "Yongqin Xian", "authors": "Yongqin Xian, Tobias Lorenz, Bernt Schiele, Zeynep Akata", "title": "Feature Generating Networks for Zero-Shot Learning", "comments": "2018 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffering from the extreme training data imbalance between seen and unseen\nclasses, most of existing state-of-the-art approaches fail to achieve\nsatisfactory results for the challenging generalized zero-shot learning task.\nTo circumvent the need for labeled examples of unseen classes, we propose a\nnovel generative adversarial network (GAN) that synthesizes CNN features\nconditioned on class-level semantic information, offering a shortcut directly\nfrom a semantic descriptor of a class to a class-conditional feature\ndistribution. Our proposed approach, pairing a Wasserstein GAN with a\nclassification loss, is able to generate sufficiently discriminative CNN\nfeatures to train softmax classifiers or any multimodal embedding method. Our\nexperimental results demonstrate a significant boost in accuracy over the state\nof the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet --\nin both the zero-shot learning and generalized zero-shot learning settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 10:00:40 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 12:17:47 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Xian", "Yongqin", ""], ["Lorenz", "Tobias", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""]]}, {"id": "1712.00996", "submitter": "Giovanni Montana", "authors": "Emanuele Pesce, Petros-Pavlos Ypsilantis, Samuel Withey, Robert\n  Bakewell, Vicky Goh, Giovanni Montana", "title": "Learning to detect chest radiographs containing lung nodules using\n  visual attention networks", "comments": null, "journal-ref": "Medical Image Analysis, Vol. 53, pag. 26-38, 2019", "doi": "10.1016/j.media.2018.12.007", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches hold great potential for the automated detection\nof lung nodules in chest radiographs, but training the algorithms requires vary\nlarge amounts of manually annotated images, which are difficult to obtain. Weak\nlabels indicating whether a radiograph is likely to contain pulmonary nodules\nare typically easier to obtain at scale by parsing historical free-text\nradiological reports associated to the radiographs. Using a repositotory of\nover 700,000 chest radiographs, in this study we demonstrate that promising\nnodule detection performance can be achieved using weak labels through\nconvolutional neural networks for radiograph classification. We propose two\nnetwork architectures for the classification of images likely to contain\npulmonary nodules using both weak labels and manually-delineated bounding\nboxes, when these are available. Annotated nodules are used at training time to\ndeliver a visual attention mechanism informing the model about its localisation\nperformance. The first architecture extracts saliency maps from high-level\nconvolutional layers and compares the estimated position of a nodule against\nthe ground truth, when this is available. A corresponding localisation error is\nthen back-propagated along with the softmax classification error. The second\napproach consists of a recurrent attention model that learns to observe a short\nsequence of smaller image portions through reinforcement learning. When a\nnodule annotation is available at training time, the reward function is\nmodified accordingly so that exploring portions of the radiographs away from a\nnodule incurs a larger penalty. Our empirical results demonstrate the potential\nadvantages of these architectures in comparison to competing methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 10:44:32 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 19:48:02 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 10:52:39 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Pesce", "Emanuele", ""], ["Ypsilantis", "Petros-Pavlos", ""], ["Withey", "Samuel", ""], ["Bakewell", "Robert", ""], ["Goh", "Vicky", ""], ["Montana", "Giovanni", ""]]}, {"id": "1712.01026", "submitter": "Jiqing Wu", "authors": "Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, Luc Van Gool", "title": "Wasserstein Divergence for GANs", "comments": "accepted by eccv_2018, correct minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains of computer vision, generative adversarial networks (GANs)\nhave achieved great success, among which the family of Wasserstein GANs (WGANs)\nis considered to be state-of-the-art due to the theoretical contributions and\ncompetitive qualitative performance. However, it is very challenging to\napproximate the $k$-Lipschitz constraint required by the Wasserstein-1\nmetric~(W-met). In this paper, we propose a novel Wasserstein\ndivergence~(W-div), which is a relaxed version of W-met and does not require\nthe $k$-Lipschitz constraint. As a concrete application, we introduce a\nWasserstein divergence objective for GANs~(WGAN-div), which can faithfully\napproximate W-div through optimization. Under various settings, including\nprogressive growing training, we demonstrate the stability of the proposed\nWGAN-div owing to its theoretical and practical advantages over WGANs. Also, we\nstudy the quantitative and visual performance of WGAN-div on standard image\nsynthesis benchmarks of computer vision, showing the superior performance of\nWGAN-div compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 11:57:28 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 07:28:58 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 08:54:14 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 12:41:21 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Wu", "Jiqing", ""], ["Huang", "Zhiwu", ""], ["Thoma", "Janine", ""], ["Acharya", "Dinesh", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.01034", "submitter": "Peihua Li", "authors": "Peihua Li, Jiangtao Xie, Qilong Wang, Zilin Gao", "title": "Towards Faster Training of Global Covariance Pooling Networks by\n  Iterative Matrix Square Root Normalization", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global covariance pooling in convolutional neural networks has achieved\nimpressive improvement over the classical first-order pooling. Recent works\nhave shown matrix square root normalization plays a central role in achieving\nstate-of-the-art performance. However, existing methods depend heavily on\neigendecomposition (EIG) or singular value decomposition (SVD), suffering from\ninefficient training due to limited support of EIG and SVD on GPU. Towards\naddressing this problem, we propose an iterative matrix square root\nnormalization method for fast end-to-end training of global covariance pooling\nnetworks. At the core of our method is a meta-layer designed with loop-embedded\ndirected graph structure. The meta-layer consists of three consecutive\nnonlinear structured layers, which perform pre-normalization, coupled matrix\niteration and post-compensation, respectively. Our method is much faster than\nEIG or SVD based ones, since it involves only matrix multiplications, suitable\nfor parallel implementation on GPU. Moreover, the proposed network with ResNet\narchitecture can converge in much less epochs, further accelerating network\ntraining. On large-scale ImageNet, we achieve competitive performance superior\nto existing counterparts. By finetuning our models pre-trained on ImageNet, we\nestablish state-of-the-art results on three challenging fine-grained\nbenchmarks. The source code and network models will be available at\nhttp://www.peihuali.org/iSQRT-COV\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:22:42 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 22:22:19 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Li", "Peihua", ""], ["Xie", "Jiangtao", ""], ["Wang", "Qilong", ""], ["Gao", "Zilin", ""]]}, {"id": "1712.01039", "submitter": "Zhengfa Liang", "authors": "Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei Chen, Linbo\n  Qiao, Li Zhou, Jianfeng Zhang", "title": "Learning for Disparity Estimation through Feature Constancy", "comments": "Accepted by CVPR 2018, 10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching algorithms usually consist of four steps, including matching\ncost calculation, matching cost aggregation, disparity calculation, and\ndisparity refinement. Existing CNN-based methods only adopt CNN to solve parts\nof the four steps, or use different networks to deal with different steps,\nmaking them difficult to obtain the overall optimal solution. In this paper, we\npropose a network architecture to incorporate all steps of stereo matching. The\nnetwork consists of three parts. The first part calculates the multi-scale\nshared features. The second part performs matching cost calculation, matching\ncost aggregation and disparity calculation to estimate the initial disparity\nusing shared features. The initial disparity and the shared features are used\nto calculate the feature constancy that measures correctness of the\ncorrespondence between two input images. The initial disparity and the feature\nconstancy are then fed to a sub-network to refine the initial disparity. The\nproposed method has been evaluated on the Scene Flow and KITTI datasets. It\nachieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015\nbenchmarks while maintaining a very fast running time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:35:38 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 15:25:44 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Liang", "Zhengfa", ""], ["Feng", "Yiliu", ""], ["Guo", "Yulan", ""], ["Liu", "Hengzhu", ""], ["Chen", "Wei", ""], ["Qiao", "Linbo", ""], ["Zhou", "Li", ""], ["Zhang", "Jianfeng", ""]]}, {"id": "1712.01056", "submitter": "Anil Baslamisli", "authors": "Anil S. Baslamisli, Hoang-An Le, Theo Gevers", "title": "CNN based Learning using Reflection and Retinex Models for Intrinsic\n  Image Decomposition", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the traditional work on intrinsic image decomposition rely on\nderiving priors about scene characteristics. On the other hand, recent research\nuse deep learning models as in-and-out black box and do not consider the\nwell-established, traditional image formation process as the basis of their\nintrinsic learning process. As a consequence, although current deep learning\napproaches show superior performance when considering quantitative benchmark\nresults, traditional approaches are still dominant in achieving high\nqualitative results. In this paper, the aim is to exploit the best of the two\nworlds. A method is proposed that (1) is empowered by deep learning\ncapabilities, (2) considers a physics-based reflection model to steer the\nlearning process, and (3) exploits the traditional approach to obtain intrinsic\nimages by exploiting reflectance and shading gradient information. The proposed\nmodel is fast to compute and allows for the integration of all intrinsic\ncomponents. To train the new model, an object centered large-scale datasets\nwith intrinsic ground-truth images are created. The evaluation results\ndemonstrate that the new model outperforms existing methods. Visual inspection\nshows that the image formation loss function augments color reproduction and\nthe use of gradient information produces sharper edges. Datasets, models and\nhigher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:16:33 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 12:43:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Baslamisli", "Anil S.", ""], ["Le", "Hoang-An", ""], ["Gevers", "Theo", ""]]}, {"id": "1712.01057", "submitter": "Franziska Mueller", "authors": "Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant\n  Mehta, Srinath Sridhar, Dan Casas, Christian Theobalt", "title": "GANerated Hands for Real-time 3D Hand Tracking from Monocular RGB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the highly challenging problem of real-time 3D hand tracking based\non a monocular RGB-only sequence. Our tracking method combines a convolutional\nneural network with a kinematic 3D hand model, such that it generalizes well to\nunseen data, is robust to occlusions and varying camera viewpoints, and leads\nto anatomically plausible as well as temporally smooth hand motions. For\ntraining our CNN we propose a novel approach for the synthetic generation of\ntraining data that is based on a geometrically consistent image-to-image\ntranslation network. To be more specific, we use a neural network that\ntranslates synthetic images to \"real\" images, such that the so-generated images\nfollow the same statistical distribution as real-world hand images. For\ntraining this translation network we combine an adversarial loss and a\ncycle-consistency loss with a geometric consistency loss in order to preserve\ngeometric properties (such as hand pose) during translation. We demonstrate\nthat our hand tracking system outperforms the current state-of-the-art on\nchallenging RGB-only footage.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:20:25 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Mueller", "Franziska", ""], ["Bernard", "Florian", ""], ["Sotnychenko", "Oleksandr", ""], ["Mehta", "Dushyant", ""], ["Sridhar", "Srinath", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "1712.01059", "submitter": "Qizheng He", "authors": "Qizheng He, Jianan Wu, Gang Yu, Chi Zhang", "title": "SOT for MOT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a robust tracker to solve the multiple object\ntracking (MOT) problem, under the framework of tracking-by-detection. As the\nfirst contribution, we innovatively combine single object tracking (SOT)\nalgorithms with multiple object tracking algorithms, and our results show that\nSOT is a general way to strongly reduce the number of false negatives,\nregardless of the quality of detection. Another contribution is that we show\nwith a deep learning based appearance model, it is easy to associate detections\nof the same object efficiently and also with high accuracy. This appearance\nmodel plays an important role in our MOT algorithm to correctly associate\ndetections into long trajectories, and also in our SOT algorithm to discover\nnew detections mistakenly missed by the detector. The deep neural network based\nmodel ensures the robustness of our tracking algorithm, which can perform data\nassociation in a wide variety of scenes. We ran comprehensive experiments on a\nlarge-scale and challenging dataset, the MOT16 benchmark, and results showed\nthat our tracker achieved state-of-the-art performance based on both public and\nprivate detections.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:22:31 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["He", "Qizheng", ""], ["Wu", "Jianan", ""], ["Yu", "Gang", ""], ["Zhang", "Chi", ""]]}, {"id": "1712.01066", "submitter": "Tribhuvanesh Orekondy", "authors": "Tribhuvanesh Orekondy, Mario Fritz, Bernt Schiele", "title": "Connecting Pixels to Privacy and Utility: Automatic Redaction of Private\n  Information in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images convey a broad spectrum of personal information. If such images are\nshared on social media platforms, this personal information is leaked which\nconflicts with the privacy of depicted persons. Therefore, we aim for automated\napproaches to redact such private information and thereby protect privacy of\nthe individual. By conducting a user study we find that obfuscating the image\nregions related to the private information leads to privacy while retaining\nutility of the images. Moreover, by varying the size of the regions different\nprivacy-utility trade-offs can be achieved. Our findings argue for a \"redaction\nby segmentation\" paradigm. Hence, we propose the first sizable dataset of\nprivate images \"in the wild\" annotated with pixel and instance level labels\nacross a broad range of privacy classes. We present the first model for\nautomatic redaction of diverse private information.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:35:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Orekondy", "Tribhuvanesh", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1712.01073", "submitter": "Shivin Yadav", "authors": "Shivin Yadav, Karthik Gopinath, Jayanthi Sivaswamy", "title": "A Generalized Motion Pattern and FCN based approach for retinal fluid\n  detection and segmentation", "comments": "8 pages, 4th MICCAI Workshop on Ophthalmic Medical Image Analysis\n  (OMIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SD-OCT is a non-invasive cross-sectional imaging modality used for diagnosis\nof macular defects. Efficient detection and segmentation of the abnormalities\nseen as biomarkers in OCT can help in analyzing the progression of the disease\nand advising effective treatment for the associated disease. In this work, we\npropose a fully automated Generalized Motion Pattern(GMP) based segmentation\nmethod using a cascade of fully convolutional networks for detection and\nsegmentation of retinal fluids from SD-OCT scans. General methods for\nsegmentation depend on domain knowledge-based feature extraction, whereas we\npropose a method based on Generalized Motion Pattern (GMP) which is derived by\ninducing motion to an image to suppress the background.The proposed method is\nparallelizable and handles inter-scanner variability efficiently. Our method\nachieves a mean Dice score of 0.61,0.70 and 0.73 during segmentation and a mean\nAUC of 0.85,0.84 and 0.87 during detection for the 3 types of fluids IRF, SRF\nand PDE respectively.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:49:57 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yadav", "Shivin", ""], ["Gopinath", "Karthik", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1712.01090", "submitter": "Chen Chen", "authors": "Mengyuan Liu, Hong Liu, Chen Chen", "title": "Robust 3D Action Recognition through Sampling Local Appearances and\n  Global Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition has broad applications in human-computer interaction\nand intelligent surveillance. However, recognizing similar actions remains\nchallenging since previous literature fails to capture motion and shape cues\neffectively from noisy depth data. In this paper, we propose a novel two-layer\nBag-of-Visual-Words (BoVW) model, which suppresses the noise disturbances and\njointly encodes both motion and shape cues. First, background clutter is\nremoved by a background modeling method that is designed for depth data. Then,\nmotion and shape cues are jointly used to generate robust and distinctive\nspatial-temporal interest points (STIPs): motion-based STIPs and shape-based\nSTIPs. In the first layer of our model, a multi-scale 3D local steering kernel\n(M3DLSK) descriptor is proposed to describe local appearances of cuboids around\nmotion-based STIPs. In the second layer, a spatial-temporal vector (STV)\ndescriptor is proposed to describe the spatial-temporal distributions of\nshape-based STIPs. Using the Bag-of-Visual-Words (BoVW) model, motion and shape\ncues are combined to form a fused action representation. Our model performs\nfavorably compared with common STIP detection and description methods. Thorough\nexperiments verify that our model is effective in distinguishing similar\nactions and robust to background clutter, partial occlusions and pepper noise.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 14:31:42 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 15:21:15 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Liu", "Mengyuan", ""], ["Liu", "Hong", ""], ["Chen", "Chen", ""]]}, {"id": "1712.01111", "submitter": "Chen Chen", "authors": "Rui Hou, Chen Chen, Mubarak Shah", "title": "An End-to-end 3D Convolutional Neural Network for Action Detection and\n  Segmentation in Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.10664", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end 3D CNN for action detection and\nsegmentation in videos. The proposed architecture is a unified deep network\nthat is able to recognize and localize action based on 3D convolution features.\nA video is first divided into equal length clips and next for each clip a set\nof tube proposals are generated based on 3D CNN features. Finally, the tube\nproposals of different clips are linked together and spatio-temporal action\ndetection is performed using these linked video proposals. This top-down action\ndetection approach explicitly relies on a set of good tube proposals to perform\nwell and training the bounding box regression usually requires a large number\nof annotated samples. To remedy this, we further extend the 3D CNN to an\nencoder-decoder structure and formulate the localization problem as action\nsegmentation. The foreground regions (i.e. action regions) for each frame are\nsegmented first then the segmented foreground maps are used to generate the\nbounding boxes. This bottom-up approach effectively avoids tube proposal\ngeneration by leveraging the pixel-wise annotations of segmentation. The\nsegmentation framework also can be readily applied to a general problem of\nvideo object segmentation. Extensive experiments on several video datasets\ndemonstrate the superior performance of our approach for action detection and\nvideo object segmentation compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:26:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hou", "Rui", ""], ["Chen", "Chen", ""], ["Shah", "Mubarak", ""]]}, {"id": "1712.01127", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Cordelia Schmid, Karteek Alahari", "title": "Learning to Segment Moving Objects", "comments": "arXiv admin note: text overlap with arXiv:1704.05737,\n  arXiv:1612.07217", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of segmenting moving objects in unconstrained videos.\nGiven a video, the task is to segment all the objects that exhibit independent\nmotion in at least one frame. We formulate this as a learning problem and\ndesign our framework with three cues: (i) independent object motion between a\npair of frames, which complements object recognition, (ii) object appearance,\nwhich helps to correct errors in motion estimation, and (iii) temporal\nconsistency, which imposes additional constraints on the segmentation. The\nframework is a two-stream neural network with an explicit memory module. The\ntwo streams encode appearance and motion cues in a video sequence respectively,\nwhile the memory module captures the evolution of objects over time, exploiting\nthe temporal consistency. The motion stream is a convolutional neural network\ntrained on synthetic videos to segment independently moving objects in the\noptical flow field. The module to build a 'visual memory' in video, i.e., a\njoint representation of all the video frames, is realized with a convolutional\nrecurrent unit learned from a small number of training video sequences.\n  For every pixel in a frame of a test video, our approach assigns an object or\nbackground label based on the learned spatio-temporal features as well as the\n'visual memory' specific to the video. We evaluate our method extensively on\nthree benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset and\nSegTrack. In addition, we provide an extensive ablation study to investigate\nboth the choice of the training data and the influence of each component in the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 16:50:26 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Schmid", "Cordelia", ""], ["Alahari", "Karteek", ""]]}, {"id": "1712.01195", "submitter": "Kunal Swami", "authors": "Kunal Swami, Pranav P. Deshpande, Gaurav Khandelwal and Ajay\n  Vijayvargiya", "title": "Why my photos look sideways or upside down? Detecting Canonical\n  Orientation of Images using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW.2017.8026216", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image orientation detection requires high-level scene understanding. Humans\nuse object recognition and contextual scene information to correctly orient\nimages. In literature, the problem of image orientation detection is mostly\nconfronted by using low-level vision features, while some approaches\nincorporate few easily detectable semantic cues to gain minor improvements. The\nvast amount of semantic content in images makes orientation detection\nchallenging, and therefore there is a large semantic gap between existing\nmethods and human behavior. Also, existing methods in literature report highly\ndiscrepant detection rates, which is mainly due to large differences in\ndatasets and limited variety of test images used for evaluation. In this work,\nfor the first time, we leverage the power of deep learning and adapt\npre-trained convolutional neural networks using largest training dataset\nto-date for the image orientation detection task. An extensive evaluation of\nour model on different public datasets shows that it remarkably generalizes to\ncorrectly orient a large set of unconstrained images; it also significantly\noutperforms the state-of-the-art and achieves accuracy very close to that of\nhumans.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 16:59:34 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Swami", "Kunal", ""], ["Deshpande", "Pranav P.", ""], ["Khandelwal", "Gaurav", ""], ["Vijayvargiya", "Ajay", ""]]}, {"id": "1712.01217", "submitter": "Carles Ventura", "authors": "Carles Ventura, Jordi Pont-Tuset, Sergi Caelles, Kevis-Kokitsi\n  Maninis, Luc Van Gool", "title": "Iterative Deep Learning for Network Topology Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the task of estimating the topology of filamentary\nnetworks such as retinal vessels and road networks. Building on top of a global\nmodel that performs a dense semantical classification of the pixels of the\nimage, we design a Convolutional Neural Network (CNN) that predicts the local\nconnectivity between the central pixel of an input patch and its border points.\nBy iterating this local connectivity we sweep the whole image and infer the\nglobal topology of the filamentary network, inspired by a human delineating a\ncomplex network with the tip of their finger.\n  We perform an extensive and comprehensive qualitative and quantitative\nevaluation on two tasks: retinal veins and arteries topology extraction and\nroad network estimation. In both cases, represented by two publicly available\ndatasets (DRIVE and Massachusetts Roads), we show superior performance to very\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:45:55 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Ventura", "Carles", ""], ["Pont-Tuset", "Jordi", ""], ["Caelles", "Sergi", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.01238", "submitter": "Ishan Misra", "authors": "Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta,\n  Laurens van der Maaten", "title": "Learning by Asking Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an interactive learning framework for the development and\ntesting of intelligent visual systems, called learning-by-asking (LBA). We\nexplore LBA in context of the Visual Question Answering (VQA) task. LBA differs\nfrom standard VQA training in that most questions are not observed during\ntraining time, and the learner must ask questions it wants answers to. Thus,\nLBA more closely mimics natural learning and has the potential to be more\ndata-efficient than the traditional VQA setting. We present a model that\nperforms LBA on the CLEVR dataset, and show that it automatically discovers an\neasy-to-hard curriculum when learning interactively from an oracle. Our LBA\ngenerated data consistently matches or outperforms the CLEVR train data and is\nmore sample efficient. We also show that our model asks questions that\ngeneralize to state-of-the-art VQA models and to novel test time distributions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:23:19 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Misra", "Ishan", ""], ["Girshick", "Ross", ""], ["Fergus", "Rob", ""], ["Hebert", "Martial", ""], ["Gupta", "Abhinav", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1712.01259", "submitter": "Yannick Hold-Geoffroy", "authors": "Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisenmann, Matt\n  Fisher, Emiliano Gambaretto, Sunil Hadap, Jean-Fran\\c{c}ois Lalonde", "title": "A Perceptual Measure for Deep Single Image Camera Calibration", "comments": "Published at CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current single image camera calibration methods rely on specific image\nfeatures or user input, and cannot be applied to natural images captured in\nuncontrolled settings. We propose directly inferring camera calibration\nparameters from a single image using a deep convolutional neural network. This\nnetwork is trained using automatically generated samples from a large-scale\npanorama dataset, and considerably outperforms other methods, including recent\ndeep learning-based approaches, in terms of standard L2 error. However, we\nargue that in many cases it is more important to consider how humans perceive\nerrors in camera estimation. To this end, we conduct a large-scale human\nperception study where we ask users to judge the realism of 3D objects\ncomposited with and without ground truth camera calibration. Based on this\nstudy, we develop a new perceptual measure for camera calibration, and\ndemonstrate that our deep calibration network outperforms other methods on this\nmeasure. Finally, we demonstrate the use of our calibration network for a\nnumber of applications including virtual object insertion, image retrieval and\ncompositing.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 00:21:58 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 14:15:33 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 16:12:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Hold-Geoffroy", "Yannick", ""], ["Sunkavalli", "Kalyan", ""], ["Eisenmann", "Jonathan", ""], ["Fisher", "Matt", ""], ["Gambaretto", "Emiliano", ""], ["Hadap", "Sunil", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1712.01261", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, David Jacobs", "title": "SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild", "comments": "Accepted to CVPR 2018 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SfSNet, an end-to-end learning framework for producing an accurate\ndecomposition of an unconstrained human face image into shape, reflectance and\nilluminance. SfSNet is designed to reflect a physical lambertian rendering\nmodel. SfSNet learns from a mixture of labeled synthetic and unlabeled real\nworld images. This allows the network to capture low frequency variations from\nsynthetic and high frequency details from real images through the photometric\nreconstruction loss. SfSNet consists of a new decomposition architecture with\nresidual blocks that learns a complete separation of albedo and normal. This is\nused along with the original image to predict lighting. SfSNet produces\nsignificantly better quantitative and qualitative results than state-of-the-art\nmethods for inverse rendering and independent normal and illumination\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 04:08:22 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 04:56:39 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Kanazawa", "Angjoo", ""], ["Castillo", "Carlos D.", ""], ["Jacobs", "David", ""]]}, {"id": "1712.01262", "submitter": "Yong-Siang Shih", "authors": "Yong-Siang Shih, Kai-Yueh Chang, Hsuan-Tien Lin, Min Sun", "title": "Compatibility Family Learning for Item Recommendation and Generation", "comments": "9 pages, accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compatibility between items, such as clothes and shoes, is a major factor\namong customer's purchasing decisions. However, learning \"compatibility\" is\nchallenging due to (1) broader notions of compatibility than those of\nsimilarity, (2) the asymmetric nature of compatibility, and (3) only a small\nset of compatible and incompatible items are observed. We propose an end-to-end\ntrainable system to embed each item into a latent vector and project a query\nitem into K compatible prototypes in the same space. These prototypes reflect\nthe broad notions of compatibility. We refer to both the embedding and\nprototypes as \"Compatibility Family\". In our learned space, we introduce a\nnovel Projected Compatibility Distance (PCD) function which is differentiable\nand ensures diversity by aiming for at least one prototype to be close to a\ncompatible item, whereas none of the prototypes are close to an incompatible\nitem. We evaluate our system on a toy dataset, two Amazon product datasets, and\nPolyvore outfit dataset. Our method consistently achieves state-of-the-art\nperformance. Finally, we show that we can visualize the candidate compatible\nprototypes using a Metric-regularized Conditional Generative Adversarial\nNetwork (MrCGAN), where the input is a projected prototype and the output is a\ngenerated image of a compatible item. We ask human evaluators to judge the\nrelative compatibility between our generated images and images generated by\nCGANs conditioned directly on query items. Our generated images are\nsignificantly preferred, with roughly twice the number of votes as others.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 04:22:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Shih", "Yong-Siang", ""], ["Chang", "Kai-Yueh", ""], ["Lin", "Hsuan-Tien", ""], ["Sun", "Min", ""]]}, {"id": "1712.01329", "submitter": "Dana Kianfar", "authors": "Mircea Mironenco, Dana Kianfar, Ke Tran, Evangelos Kanoulas,\n  Efstratios Gavves", "title": "Examining Cooperation in Visual Dialog Models", "comments": "9 pages, 5 figures, 2 tables, code at\n  http://github.com/danakianfar/Examining-Cooperation-in-VDM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a blackbox intervention method for visual dialog\nmodels, with the aim of assessing the contribution of individual linguistic or\nvisual components. Concretely, we conduct structured or randomized\ninterventions that aim to impair an individual component of the model, and\nobserve changes in task performance. We reproduce a state-of-the-art visual\ndialog model and demonstrate that our methodology yields surprising insights,\nnamely that both dialog and image information have minimal contributions to\ntask performance. The intervention method presented here can be applied as a\nsanity check for the strength and robustness of each component in visual dialog\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 20:16:52 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Mironenco", "Mircea", ""], ["Kianfar", "Dana", ""], ["Tran", "Ke", ""], ["Kanoulas", "Evangelos", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1712.01337", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung, Hsiao-Wei Tung, Ersin Yumer, Katerina Fragkiadaki", "title": "Self-supervised Learning of Motion Capture", "comments": "Neural Information Processing Systems (NIPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art solutions for motion capture from a single camera\nare optimization driven: they optimize the parameters of a 3D human model so\nthat its re-projection matches measurements in the video (e.g. person\nsegmentation, optical flow, keypoint detections etc.). Optimization models are\nsusceptible to local minima. This has been the bottleneck that forced using\nclean green-screen like backgrounds at capture time, manual initialization, or\nswitching to multiple cameras as input resource. In this work, we propose a\nlearning based motion capture model for single camera input. Instead of\noptimizing mesh and skeleton parameters directly, our model optimizes neural\nnetwork weights that predict 3D shape and skeleton configurations given a\nmonocular RGB video. Our model is trained using a combination of strong\nsupervision from synthetic data, and self-supervision from differentiable\nrendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c)\nhuman-background segmentation, in an end-to-end framework. Empirically we show\nour model combines the best of both worlds of supervised learning and test-time\noptimization: supervised learning initializes the model parameters in the right\nregime, ensuring good pose and surface initialization at test time, without\nmanual effort. Self-supervision by back-propagating through differentiable\nrendering allows (unsupervised) adaptation of the model to the test data, and\noffers much tighter fit than a pretrained fixed model. We show that the\nproposed model improves with experience and converges to low-error solutions\nwhere previous optimization methods fail.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 20:25:47 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Tung", "Hsiao-Wei", ""], ["Yumer", "Ersin", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1712.01358", "submitter": "Abhinav Moudgil", "authors": "Abhinav Moudgil, Vineet Gandhi", "title": "Long-Term Visual Object Tracking Benchmark", "comments": "ACCV 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new long video dataset (called Track Long and Prosper - TLP) and\nbenchmark for single object tracking. The dataset consists of 50 HD videos from\nreal world scenarios, encompassing a duration of over 400 minutes (676K\nframes), making it more than 20 folds larger in average duration per sequence\nand more than 8 folds larger in terms of total covered duration, as compared to\nexisting generic datasets for visual tracking. The proposed dataset paves a way\nto suitably assess long term tracking performance and train better deep\nlearning architectures (avoiding/reducing augmentation, which may not reflect\nreal world behaviour). We benchmark the dataset on 17 state of the art trackers\nand rank them according to tracking accuracy and run time speeds. We further\npresent thorough qualitative and quantitative evaluation highlighting the\nimportance of long term aspect of tracking. Our most interesting observations\nare (a) existing short sequence benchmarks fail to bring out the inherent\ndifferences in tracking algorithms which widen up while tracking on long\nsequences and (b) the accuracy of trackers abruptly drops on challenging long\nsequences, suggesting the potential need of research efforts in the direction\nof long-term tracking.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:02:24 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 07:38:33 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 11:58:18 GMT"}, {"version": "v4", "created": "Tue, 1 Jan 2019 10:21:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Moudgil", "Abhinav", ""], ["Gandhi", "Vineet", ""]]}, {"id": "1712.01359", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Ziwei Li, Hyun Soo Park", "title": "3D Semantic Trajectory Reconstruction from 3D Pixel Continuum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to reconstruct dense semantic trajectory stream\nof human interactions in 3D from synchronized multiple videos. The interactions\ninherently introduce self-occlusion and illumination/appearance/shape changes,\nresulting in highly fragmented trajectory reconstruction with noisy and coarse\nsemantic labels. Our conjecture is that among many views, there exists a set of\nviews that can confidently recognize the visual semantic label of a 3D\ntrajectory. We introduce a new representation called 3D semantic map---a\nprobability distribution over the semantic labels per trajectory. We construct\nthe 3D semantic map by reasoning about visibility and 2D recognition confidence\nbased on view-pooling, i.e., finding the view that best represents the\nsemantics of the trajectory. Using the 3D semantic map, we precisely infer all\ntrajectory labels jointly by considering the affinity between long range\ntrajectories via estimating their local rigid transformations. This inference\nquantitatively outperforms the baseline approaches in terms of predictive\nvalidity, representation robustness, and affinity effectiveness. We demonstrate\nthat our algorithm can robustly compute the semantic labels of a large scale\ntrajectory set involving real-world human interactions with object, scenes, and\npeople.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:03:12 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Li", "Ziwei", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1712.01361", "submitter": "Hieu Le", "authors": "Hieu Le, Tomas F. Yago Vicente, Vu Nguyen, Minh Hoai, Dimitris Samaras", "title": "A+D Net: Training a Shadow Detector with Adversarial Shadow Attenuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel GAN-based framework for detecting shadows in images, in\nwhich a shadow detection network (D-Net) is trained together with a shadow\nattenuation network (A-Net) that generates adversarial training examples. The\nA-Net modifies the original training images constrained by a simplified\nphysical shadow model and is focused on fooling the D-Net's shadow predictions.\nHence, it is effectively augmenting the training data for D-Net with\nhard-to-predict cases. The D-Net is trained to predict shadows in both original\nimages and generated images from the A-Net. Our experimental results show that\nthe additional training data from A-Net significantly improves the shadow\ndetection accuracy of D-Net. Our method outperforms the state-of-the-art\nmethods on the most challenging shadow detection benchmark (SBU) and also\nobtains state-of-the-art results on a cross-dataset task, testing on UCF.\nFurthermore, the proposed method achieves accurate real-time shadow detection\nat 45 frames per second.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:14:32 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 19:44:31 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Le", "Hieu", ""], ["Vicente", "Tomas F. Yago", ""], ["Nguyen", "Vu", ""], ["Hoai", "Minh", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1712.01381", "submitter": "Yizhe Zhu", "authors": "Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng, Ahmed Elgammal", "title": "A Generative Adversarial Approach for Zero-Shot Learning from Noisy\n  Texts", "comments": "To appear in CVPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing zero-shot learning methods consider the problem as a visual\nsemantic embedding one. Given the demonstrated capability of Generative\nAdversarial Networks(GANs) to generate images, we instead leverage GANs to\nimagine unseen categories from text descriptions and hence recognize novel\nclasses with no examples being seen. Specifically, we propose a simple yet\neffective generative model that takes as input noisy text descriptions about an\nunseen class (e.g.Wikipedia articles) and generates synthesized visual features\nfor this class. With added pseudo data, zero-shot learning is naturally\nconverted to a traditional classification problem. Additionally, to preserve\nthe inter-class discrimination of the generated features, a visual pivot\nregularization is proposed as an explicit supervision. Unlike previous methods\nusing complex engineered regularizers, our approach can suppress the noise well\nwithout additional regularization. Empirically, we show that our method\nconsistently outperforms the state of the art on the largest available\nbenchmarks on Text-based Zero-shot Learning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:40:10 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 16:24:07 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 01:18:30 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhu", "Yizhe", ""], ["Elhoseiny", "Mohamed", ""], ["Liu", "Bingchen", ""], ["Peng", "Xi", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1712.01393", "submitter": "Yipin Zhou", "authors": "Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui and Tamara L. Berg", "title": "Visual to Sound: Generating Natural Sound for Videos in the Wild", "comments": "Project page:\n  http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As two of the five traditional human senses (sight, hearing, taste, smell,\nand touch), vision and sound are basic sources through which humans understand\nthe world. Often correlated during natural events, these two modalities combine\nto jointly affect human perception. In this paper, we pose the task of\ngenerating sound given visual input. Such capabilities could help enable\napplications in virtual reality (generating sound for virtual scenes\nautomatically) or provide additional accessibility to images or videos for\npeople with visual impairments. As a first step in this direction, we apply\nlearning-based methods to generate raw waveform samples given input video\nframes. We evaluate our models on a dataset of videos containing a variety of\nsounds (such as ambient sounds and sounds from people/animals). Our experiments\nshow that the generated sounds are fairly realistic and have good temporal\nsynchronization with the visual inputs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 22:24:29 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 06:40:49 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Zhou", "Yipin", ""], ["Wang", "Zhaowen", ""], ["Fang", "Chen", ""], ["Bui", "Trung", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1712.01397", "submitter": "Chawin Sitawarin", "authors": "Mark Martinez, Chawin Sitawarin, Kevin Finch, Lennart Meincke, Alex\n  Yablonski, Alain Kornhauser", "title": "Beyond Grand Theft Auto V for Training, Testing and Enhancing Deep\n  Learning in Self Driving Cars", "comments": "15 pages, 4 figures, under review by TRB 2018 Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an initial assessment, over 480,000 labeled virtual images of normal\nhighway driving were readily generated in Grand Theft Auto V's virtual\nenvironment. Using these images, a CNN was trained to detect following distance\nto cars/objects ahead, lane markings, and driving angle (angular heading\nrelative to lane centerline): all variables necessary for basic autonomous\ndriving. Encouraging results were obtained when tested on over 50,000 labeled\nvirtual images from substantially different GTA-V driving environments. This\ninitial assessment begins to define both the range and scope of the labeled\nimages needed for training as well as the range and scope of labeled images\nneeded for testing the definition of boundaries and limitations of trained\nnetworks. It is the efficacy and flexibility of a \"GTA-V\"-like virtual\nenvironment that is expected to provide an efficient well-defined foundation\nfor the training and testing of Convolutional Neural Networks for safe driving.\nAdditionally, described is the Princeton Virtual Environment (PVE) for the\ntraining, testing and enhancement of safe driving AI, which is being developed\nusing the video-game engine Unity. PVE is being developed to recreate rare but\ncritical corner cases that can be used in re-training and enhancing machine\nlearning models and understanding the limitations of current self driving\nmodels. The Florida Tesla crash is being used as an initial reference.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 22:41:46 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Martinez", "Mark", ""], ["Sitawarin", "Chawin", ""], ["Finch", "Kevin", ""], ["Meincke", "Lennart", ""], ["Yablonski", "Alex", ""], ["Kornhauser", "Alain", ""]]}, {"id": "1712.01429", "submitter": "Ot\\'avio Penatti", "authors": "Ot\\'avio A. B. Penatti and Milton F. S. Santos", "title": "Human activity recognition from mobile inertial sensors using recurrence\n  plots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial sensors are present in most mobile devices nowadays and such devices\nare used by people during most of their daily activities. In this paper, we\npresent an approach for human activity recognition based on inertial sensors by\nemploying recurrence plots (RP) and visual descriptors. The pipeline of the\nproposed approach is the following: compute RPs from sensor data, compute\nvisual features from RPs and use them in a machine learning protocol. As RPs\ngenerate texture visual patterns, we transform the problem of sensor data\nclassification to a problem of texture classification. Experiments for\nclassifying human activities based on accelerometer data showed that the\nproposed approach obtains the highest accuracies, outperforming time- and\nfrequency-domain features directly extracted from sensor data. The best results\nare obtained when using RGB RPs, in which each RGB channel corresponds to the\nRP of an independent accelerometer axis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 00:49:07 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Penatti", "Ot\u00e1vio A. B.", ""], ["Santos", "Milton F. S.", ""]]}, {"id": "1712.01432", "submitter": "Yihang Lou", "authors": "Lingyu Duan, Yihang Lou, Shiqi Wang, Wen Gao, Yong Rui", "title": "AI Oriented Large-Scale Video Management for Smart City: Technologies,\n  Standards and Beyond", "comments": "8 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved substantial success in a series of tasks in\ncomputer vision. Intelligent video analysis, which can be broadly applied to\nvideo surveillance in various smart city applications, can also be driven by\nsuch powerful deep learning engines. To practically facilitate deep neural\nnetwork models in the large-scale video analysis, there are still unprecedented\nchallenges for the large-scale video data management. Deep feature coding,\ninstead of video coding, provides a practical solution for handling the\nlarge-scale video surveillance data. To enable interoperability in the context\nof deep feature coding, standardization is urgent and important. However, due\nto the explosion of deep learning algorithms and the particularity of feature\ncoding, there are numerous remaining problems in the standardization process.\nThis paper envisions the future deep feature coding standard for the AI\noriented large-scale video management, and discusses existing techniques,\nstandards and possible solutions for these open problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 01:02:33 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Duan", "Lingyu", ""], ["Lou", "Yihang", ""], ["Wang", "Shiqi", ""], ["Gao", "Wen", ""], ["Rui", "Yong", ""]]}, {"id": "1712.01434", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Partha Pratim Roy, Umapada Pal", "title": "Zone-based Keyword Spotting in Bangla and Devanagari Documents", "comments": "Preprint Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 01:12:25 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1712.01443", "submitter": "Shiyang Cheng", "authors": "Shiyang Cheng, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou", "title": "4DFAB: A Large Scale 4D Facial Expression Database for Biometric\n  Applications", "comments": "The original paper is published in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress we are currently witnessing in many computer vision\napplications, including automatic face analysis, would not be made possible\nwithout tremendous efforts in collecting and annotating large scale visual\ndatabases. To this end, we propose 4DFAB, a new large scale database of dynamic\nhigh-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings\nof 180 subjects captured in four different sessions spanning over a five-year\nperiod. It contains 4D videos of subjects displaying both spontaneous and posed\nfacial behaviours. The database can be used for both face and facial expression\nrecognition, as well as behavioural biometrics. It can also be used to learn\nvery powerful blendshapes for parametrising facial behaviour. In this paper, we\nconduct several experiments and demonstrate the usefulness of the database for\nvarious applications. The database will be made publicly available for research\npurposes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 02:13:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 10:32:17 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Cheng", "Shiyang", ""], ["Kotsia", "Irene", ""], ["Pantic", "Maja", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1712.01455", "submitter": "Zhiqian Chen", "authors": "Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo, Jing Dai and\n  Chang-Tien Lu", "title": "Multimodal Storytelling via Generative Adversarial Imitation Learning", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving event storylines is an effective summarization method to succinctly\norganize extensive information, which can significantly alleviate the pain of\ninformation overload. The critical challenge is the lack of widely recognized\ndefinition of storyline metric. Prior studies have developed various approaches\nbased on different assumptions about users' interests. These works can extract\ninteresting patterns, but their assumptions do not guarantee that the derived\npatterns will match users' preference. On the other hand, their exclusiveness\nof single modality source misses cross-modality information. This paper\nproposes a method, multimodal imitation learning via generative adversarial\nnetworks(MIL-GAN), to directly model users' interests as reflected by various\ndata. In particular, the proposed model addresses the critical challenge by\nimitating users' demonstrated storylines. Our proposed model is designed to\nlearn the reward patterns given user-provided storylines and then applies the\nlearned policy to unseen data. The proposed approach is demonstrated to be\ncapable of acquiring the user's implicit intent and outperforming competing\nmethods by a substantial margin with a user study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 02:51:35 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Chen", "Zhiqian", ""], ["Zhang", "Xuchao", ""], ["Boedihardjo", "Arnold P.", ""], ["Dai", "Jing", ""], ["Lu", "Chang-Tien", ""]]}, {"id": "1712.01493", "submitter": "Zhou Yin", "authors": "Zhou Yin, Wei-Shi Zheng, Ancong Wu, Hong-Xing Yu, Hai Wan, Xiaowei\n  Guo, Feiyue Huang, Jianhuang Lai", "title": "Adversarial Attribute-Image Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While attributes have been widely used for person re-identification (Re-ID)\nwhich aims at matching the same person images across disjoint camera views,\nthey are used either as extra features or for performing multi-task learning to\nassist the image-image matching task. However, how to find a set of person\nimages according to a given attribute description, which is very practical in\nmany surveillance applications, remains a rarely investigated cross-modality\nmatching problem in person Re-ID. In this work, we present this challenge and\nformulate this task as a joint space learning problem. By imposing an\nattribute-guided attention mechanism for images and a semantic consistent\nadversary strategy for attributes, each modality, i.e., images and attributes,\nsuccessfully learns semantically correlated concepts under the guidance of the\nother. We conducted extensive experiments on three attribute datasets and\ndemonstrated that the proposed joint space learning method is so far the most\neffective method for the attribute-image cross-modality person Re-ID problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 06:06:32 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 07:57:07 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:49:39 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Yin", "Zhou", ""], ["Zheng", "Wei-Shi", ""], ["Wu", "Ancong", ""], ["Yu", "Hong-Xing", ""], ["Wan", "Hai", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1712.01509", "submitter": "Guoyan Zheng", "authors": "Rens Janssens, Guodong Zeng and Guoyan Zheng", "title": "Fully Automatic Segmentation of Lumbar Vertebrae from CT Images using\n  Cascaded 3D Fully Convolutional Networks", "comments": "5 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to address the challenging problem of segmentation of\nlumbar vertebrae from CT images acquired with varying fields of view. Our\nmethod is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting\nof a localization FCN and a segmentation FCN. More specifically, in the first\nstep we train a regression 3D FCN (we call it \"LocalizationNet\") to find the\nbounding box of the lumbar region. After that, a 3D U-net like FCN (we call it\n\"SegmentationNet\") is then developed, which after training, can perform a\npixel-wise multi-class segmentation to map a cropped lumber region volumetric\ndata to its volume-wise labels. Evaluated on publicly available datasets, our\nmethod achieved an average Dice coefficient of 95.77 $\\pm$ 0.81% and an average\nsymmetric surface distance of 0.37 $\\pm$ 0.06 mm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 07:24:57 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Janssens", "Rens", ""], ["Zeng", "Guodong", ""], ["Zheng", "Guoyan", ""]]}, {"id": "1712.01511", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Patrick Virtue, Stella X. Yu", "title": "Successive Embedding and Classification Loss for Aerial Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can be effective means to automatically classify aerial\nimages but is easy to overfit to the training data. It is critical for trained\nneural networks to be robust to variations that exist between training and test\nenvironments. To address the overfitting problem in aerial image\nclassification, we consider the neural network as successive transformations of\nan input image into embedded feature representations and ultimately into a\nsemantic class label, and train neural networks to optimize image\nrepresentations in the embedded space in addition to optimizing the final\nclassification score. We demonstrate that networks trained with this dual\nembedding and classification loss outperform networks with classification loss\nonly. %We also study placing the embedding loss on different network layers. We\nalso find that moving the embedding loss from commonly-used feature space to\nthe classifier space, which is the space just before softmax nonlinearization,\nleads to the best classification performance for aerial images. Visualizations\nof the network's embedded representations reveal that the embedding loss\nencourages greater separation between target class clusters for both training\nand testing partitions of two aerial image classification benchmark datasets,\nMSTAR and AID. Our code is publicly available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 07:45:18 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 05:05:33 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 05:57:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Wang", "Jiayun", ""], ["Virtue", "Patrick", ""], ["Yu", "Stella X.", ""]]}, {"id": "1712.01537", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, Xin Tong", "title": "O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3072959.3073608", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D\nshape analysis. Built upon the octree representation of 3D shapes, our method\ntakes the average normal vectors of a 3D model sampled in the finest leaf\noctants as input and performs 3D CNN operations on the octants occupied by the\n3D shape surface. We design a novel octree data structure to efficiently store\nthe octant information and CNN features into the graphics memory and execute\nthe entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN\nstructures and works for 3D shapes in different representations. By restraining\nthe computations on the octants occupied by 3D surfaces, the memory and\ncomputational costs of the O-CNN grow quadratically as the depth of the octree\nincreases, which makes the 3D CNN feasible for high-resolution 3D models. We\ncompare the performance of the O-CNN with other existing 3D CNN solutions and\ndemonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks,\nincluding object classification, shape retrieval, and shape segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 09:25:19 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Liu", "Yang", ""], ["Guo", "Yu-Xiao", ""], ["Sun", "Chun-Yu", ""], ["Tong", "Xin", ""]]}, {"id": "1712.01551", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "title": "Manifold-valued Image Generation with Wasserstein Generative Adversarial\n  Nets", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling over natural images is one of the most fundamental\nmachine learning problems. However, few modern generative models, including\nWasserstein Generative Adversarial Nets (WGANs), are studied on manifold-valued\nimages that are frequently encountered in real-world applications. To fill the\ngap, this paper first formulates the problem of generating manifold-valued\nimages and exploits three typical instances: hue-saturation-value (HSV) color\nimage generation, chromaticity-brightness (CB) color image generation, and\ndiffusion-tensor (DT) image generation. For the proposed generative modeling\nproblem, we then introduce a theorem of optimal transport to derive a new\nWasserstein distance of data distributions on complete manifolds, enabling us\nto achieve a tractable objective under the WGAN framework. In addition, we\nrecommend three benchmark datasets that are CIFAR-10 HSV/CB color images,\nImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we\nexperimentally demonstrate the proposed manifold-aware WGAN model can generate\nmore plausible manifold-valued images than its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 10:02:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 16:09:47 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wu", "Jiqing", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.01600", "submitter": "Alexandre Benoit", "authors": "A Hamida, A. Beno\\^it (IPNL), P. Lambert (LISTIC), L Klein, C Amar, N.\n  Audebert (ONERA), S. Lef\\`evre (VALORIA)", "title": "Deep learning for semantic segmentation of remote sensing images with\n  rich spectral content", "comments": "IEEE International Geoscience and Remote Sensing Symposium, Jul 2017,\n  Fort Worth, United States. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Remote Sensing acquisition techniques, there is\na need to scale and improve processing tools to cope with the observed increase\nof both data volume and richness. Among popular techniques in remote sensing,\nDeep Learning gains increasing interest but depends on the quality of the\ntraining data. Therefore, this paper presents recent Deep Learning approaches\nfor fine or coarse land cover semantic segmentation estimation. Various 2D\narchitectures are tested and a new 3D model is introduced in order to jointly\nprocess the spatial and spectral dimensions of the data. Such a set of networks\nenables the comparison of the different spectral fusion schemes. Besides, we\nalso assess the use of a \" noisy ground truth \" (i.e. outdated and low spatial\nresolution labels) for training and testing the networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 12:25:43 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Hamida", "A", "", "IPNL"], ["Beno\u00eet", "A.", "", "IPNL"], ["Lambert", "P.", "", "LISTIC"], ["Klein", "L", "", "ONERA"], ["Amar", "C", "", "ONERA"], ["Audebert", "N.", "", "ONERA"], ["Lef\u00e8vre", "S.", "", "VALORIA"]]}, {"id": "1712.01606", "submitter": "Alexandre Benoit", "authors": "Rizl\\`ene Raoui-Outach (LISTIC), C\\'ecile Million-Rousseau (LISTIC),\n  Alexandre Benoit (LISTIC), Patrick Lambert (LISTIC)", "title": "Deep Learning for automatic sale receipt understanding", "comments": "International Conference on Image Processing Theory, Tools and\n  Applications, Nov 2017, Montreal, Canada. 2017,\n  http://www.ipta-conference.com/ipta17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a general rule, data analytics are now mandatory for companies. Scanned\ndocument analysis brings additional challenges introduced by paper damages and\nscanning quality.In an industrial context, this work focuses on the automatic\nunderstanding of sale receipts which enable access to essential and accurate\nconsumption statistics. Given an image acquired with a smart-phone, the\nproposed work mainly focuses on the first steps of the full tool chain which\naims at providing essential information such as the store brand, purchased\nproducts and related prices with the highest possible confidence. To get this\nhigh confidence level, even if scanning is not perfectly controlled, we propose\na double check processing tool-chain using Deep Convolutional Neural Networks\n(DCNNs) on one hand and more classical image and text processings on another\nhand.The originality of this work relates in this double check processing and\nin the joint use of DCNNs for different applications and text analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 12:40:20 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Raoui-Outach", "Rizl\u00e8ne", "", "LISTIC"], ["Million-Rousseau", "C\u00e9cile", "", "LISTIC"], ["Benoit", "Alexandre", "", "LISTIC"], ["Lambert", "Patrick", "", "LISTIC"]]}, {"id": "1712.01619", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig,\n  Andreas Morel-Forster, Thomas Vetter", "title": "Empirically Analyzing the Effect of Dataset Biases on Deep Face\n  Recognition Systems", "comments": "Accepted to CVPR 2018 Workshop on Analysis and Modeling of Faces and\n  Gestures (AMFG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is unknown what kind of biases modern in the wild face datasets have\nbecause of their lack of annotation. A direct consequence of this is that total\nrecognition rates alone only provide limited insight about the generalization\nability of a Deep Convolutional Neural Networks (DCNNs). We propose to\nempirically study the effect of different types of dataset biases on the\ngeneralization ability of DCNNs. Using synthetically generated face images, we\nstudy the face recognition rate as a function of interpretable parameters such\nas face pose and light. The proposed method allows valuable details about the\ngeneralization performance of different DCNN architectures to be observed and\ncompared. In our experiments, we find that: 1) Indeed, dataset bias has a\nsignificant influence on the generalization performance of DCNNs. 2) DCNNs can\ngeneralize surprisingly well to unseen illumination conditions and large\nsampling gaps in the pose variation. 3) Using the presented methodology we\nreveal that the VGG-16 architecture outperforms the AlexNet architecture at\nface recognition tasks because it can much better generalize to unseen face\nposes, although it has significantly more parameters. 4) We uncover a main\nlimitation of current DCNN architectures, which is the difficulty to generalize\nwhen different identities to not share the same pose variation. 5) We\ndemonstrate that our findings on synthetic data also apply when learning from\nreal-world data. Our face image generator is publicly available to enable the\ncommunity to benchmark other DCNN architectures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 13:52:42 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 06:42:52 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 19:28:40 GMT"}, {"version": "v4", "created": "Thu, 19 Apr 2018 15:05:39 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Kortylewski", "Adam", ""], ["Egger", "Bernhard", ""], ["Schneider", "Andreas", ""], ["Gerig", "Thomas", ""], ["Morel-Forster", "Andreas", ""], ["Vetter", "Thomas", ""]]}, {"id": "1712.01628", "submitter": "Vaneet Aggarwal", "authors": "Morteza Ashraphijuo and Vaneet Aggarwal and Xiaodong Wang", "title": "On Deterministic Sampling Patterns for Robust Low-Rank Matrix Completion", "comments": "Accepted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2780983", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we study the deterministic sampling patterns for the\ncompletion of low rank matrix, when corrupted with a sparse noise, also known\nas robust matrix completion. We extend the recent results on the deterministic\nsampling patterns in the absence of noise based on the geometric analysis on\nthe Grassmannian manifold. A special case where each column has a certain\nnumber of noisy entries is considered, where our probabilistic analysis\nperforms very efficiently. Furthermore, assuming that the rank of the original\nmatrix is not given, we provide an analysis to determine if the rank of a valid\ncompletion is indeed the actual rank of the data corrupted with sparse noise by\nverifying some conditions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 14:06:44 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Aggarwal", "Vaneet", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1712.01635", "submitter": "Yasser El-Sonbaty", "authors": "Ibrahim Abdelkader, Yasser El-Sonbaty and Mohamed El-Habrouk", "title": "Keypoint-based object tracking and localization using networks of\n  low-power embedded smart cameras", "comments": null, "journal-ref": "International Conferences Computer Graphics, Visualization,\n  Computer Vision and Image Processing 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking and localization is a complex task that typically requires\nprocessing power beyond the capabilities of low-power embedded cameras. This\npaper presents a new approach to real-time object tracking and localization\nusing multi-view binary keypoints descriptor. The proposed approach offers a\ncompromise between processing power, accuracy and networking bandwidth and has\nbeen tested using multiple distributed low-power smart cameras. Additionally,\nmultiple optimization techniques are presented to improve the performance of\nthe keypoints descriptor for low-power embedded systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 07:54:54 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Abdelkader", "Ibrahim", ""], ["El-Sonbaty", "Yasser", ""], ["El-Habrouk", "Mohamed", ""]]}, {"id": "1712.01636", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, Joseph\n  Barfett", "title": "Generalization of Deep Neural Networks for Chest Pathology\n  Classification in X-Rays Using Generative Adversarial Networks", "comments": "This paper is accepted for presentation at IEEE International\n  Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical datasets are often highly imbalanced with over-representation of\ncommon medical problems and a paucity of data from rare conditions. We propose\nsimulation of pathology in images to overcome the above limitations. Using\nchest X-rays as a model medical image, we implement a generative adversarial\nnetwork (GAN) to create artificial images based upon a modest sized labeled\ndataset. We employ a combination of real and artificial images to train a deep\nconvolutional neural network (DCNN) to detect pathology across five classes of\nchest X-rays. Furthermore, we demonstrate that augmenting the original\nimbalanced dataset with GAN generated images improves performance of chest\npathology classification using the proposed DCNN in comparison to the same DCNN\ntrained with the original dataset alone. This improved performance is largely\nattributed to balancing of the dataset using GAN generated images, where image\nclasses that are lacking in example images are preferentially augmented.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:26:01 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 18:25:28 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""], ["Dowdell", "Tim", ""], ["Colak", "Errol", ""], ["Barfett", "Joseph", ""]]}, {"id": "1712.01639", "submitter": "Yu Li", "authors": "Yu Li, Hu Wang, Juanjuan Liu", "title": "Can CNN Construct Highly Accurate Models Efficiently for\n  High-Dimensional Problems in Complex Product Designs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of the nonlinearity and dimension, it is difficult for the\npresent popular metamodeling techniques to construct reliable metamodels. To\naddress this problem, Convolutional Neural Network (CNN) is introduced to\nconstruct a highly accurate metamodel efficiently. Considering the inherent\ncharacteristics of the CNN, it is a potential modeling tool to handle highly\nnonlinear and dimensional problems (hundreds-dimensional problems) with the\nlimited training samples. In order to evaluate the proposed CNN metamodel for\nhundreds-dimensional and strong nonlinear problems, CNN is compared with other\nmetamodeling techniques. Furthermore, several high-dimensional analytical\nfunctions are also employed to test the CNN metamodel. Testing and comparisons\nconfirm the efficiency and capability of the CNN metamodel for\nhundreds-dimensional and strong nonlinear problems. Moreover, the proposed CNN\nmetamodel is also applied to IsoGeometric Analysis (IGA)-based optimization\nsuccessfully.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:06:54 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 00:49:01 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 01:04:46 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Li", "Yu", ""], ["Wang", "Hu", ""], ["Liu", "Juanjuan", ""]]}, {"id": "1712.01640", "submitter": "Dawit Mureja Argaw", "authors": "Malinda Vania and Dawit Mureja and Deukhee Lee", "title": "Automatic Spine Segmentation using Convolutional Neural Network via\n  Redundant Generation of Class Labels for 3D Spine Modeling", "comments": "18 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a significant increase from 2010 to 2016 in the number of\npeople suffering from spine problems. The automatic image segmentation of the\nspine obtained from a computed tomography (CT) image is important for\ndiagnosing spine conditions and for performing surgery with computer-assisted\nsurgery systems. The spine has a complex anatomy that consists of 33 vertebrae,\n23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the\nspinal surgeon is faced with the challenge of needing a robust algorithm to\nsegment and create a model of the spine. In this study, we developed an\nautomatic segmentation method to segment the spine, and we compared our\nsegmentation results with reference segmentations obtained by experts. We\ndeveloped a fully automatic approach for spine segmentation from CT based on a\nhybrid method. This method combines the convolutional neural network (CNN) and\nfully convolutional network (FCN), and utilizes class redundancy as a soft\nconstraint to greatly improve the segmentation results. The proposed method was\nfound to significantly enhance the accuracy of the segmentation results and the\nsystem processing time. Our comparison was based on 12 measurements: the Dice\ncoefficient (94%), Jaccard index (93%), volumetric similarity (96%),\nsensitivity (97%), specificity (99%), precision (over segmentation; 8.3 and\nunder segmentation 2.6), accuracy (99%), Matthews correlation coefficient\n(0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and\nglobal consistency error (0.02). We experimented with CT images from 32\npatients, and the experimental results demonstrated the efficiency of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:19:07 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Vania", "Malinda", ""], ["Mureja", "Dawit", ""], ["Lee", "Deukhee", ""]]}, {"id": "1712.01641", "submitter": "Xuemei Xie", "authors": "Jiang Du, Xuemei Xie, Chenye Wang, Guangming Shi, Xun Xu, Yuxiang Wang", "title": "Fully Convolutional Measurement Network for Compressive Sensing Image\n  Reconstruction", "comments": "Accepted by neurocomputing in 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning methods have made a significant improvement in\ncompressive sensing image reconstruction task. In the existing methods, the\nscene is measured block by block due to the high computational complexity. This\nresults in block-effect of the recovered images. In this paper, we propose a\nfully convolutional measurement network, where the scene is measured as a\nwhole. The proposed method powerfully removes the block-effect since the\nstructure information of scene images is preserved. To make the measure more\nflexible, the measurement and the recovery parts are jointly trained. From the\nexperiments, it is shown that the results by the proposed method outperforms\nthose by the existing methods in PSNR, SSIM, and visual effect.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 04:27:23 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 04:20:25 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Du", "Jiang", ""], ["Xie", "Xuemei", ""], ["Wang", "Chenye", ""], ["Shi", "Guangming", ""], ["Xu", "Xun", ""], ["Wang", "Yuxiang", ""]]}, {"id": "1712.01642", "submitter": "Qingxiang Feng", "authors": "Qingxiang Feng and Yicong Zhou", "title": "Color Face Recognition using High-Dimension Quaternion-based Adaptive\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, quaternion collaborative representation-based classification (QCRC)\nand quaternion sparse representation-based classification (QSRC) have been\nproposed for color face recognition. They can obtain correlation information\namong different color channels. However, their performance is unstable in\ndifferent conditions. For example, QSRC performs better than than QCRC on some\nsituations but worse on other situations. To benefit from quaternion-based\n$e_2$-norm minimization in QCRC and quaternion-based $e_1$-norm minimization in\nQSRC, we propose the quaternion-based adaptive representation (QAR) that uses a\nquaternion-based $e_p$-norm minimization ($1 \\le p \\le 2$) for color face\nrecognition. To obtain the high dimension correlation information among\ndifferent color channels, we further propose the high-dimension\nquaternion-based adaptive representation (HD-QAR). The experimental results\ndemonstrate that the proposed QAR and HD-QAR achieve better recognition rates\nthan QCRC, QSRC and several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 06:15:59 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Feng", "Qingxiang", ""], ["Zhou", "Yicong", ""]]}, {"id": "1712.01643", "submitter": "Qingxiang Feng", "authors": "Qingxiang Feng and Yicong Zhou", "title": "Discriminant Projection Representation-based Classification for Vision\n  Recognition", "comments": "Accepted by the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation-based classification methods such as sparse\nrepresentation-based classification (SRC) and linear regression classification\n(LRC) have attracted a lot of attentions. In order to obtain the better\nrepresentation, a novel method called projection representation-based\nclassification (PRC) is proposed for image recognition in this paper. PRC is\nbased on a new mathematical model. This model denotes that the 'ideal\nprojection' of a sample point $x$ on the hyper-space $H$ may be gained by\niteratively computing the projection of $x$ on a line of hyper-space $H$ with\nthe proper strategy. Therefore, PRC is able to iteratively approximate the\n'ideal representation' of each subject for classification. Moreover, the\ndiscriminant PRC (DPRC) is further proposed, which obtains the discriminant\ninformation by maximizing the ratio of the between-class reconstruction error\nover the within-class reconstruction error. Experimental results on five\ntypical databases show that the proposed PRC and DPRC are effective and\noutperform other state-of-the-art methods on several vision recognition tasks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 06:25:17 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Feng", "Qingxiang", ""], ["Zhou", "Yicong", ""]]}, {"id": "1712.01645", "submitter": "Qingxiang Feng", "authors": "Qingxiang Feng and Yicong Zhou", "title": "Vision Recognition using Discriminant Sparse Optimization Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better select the correct training sample and obtain the robust\nrepresentation of the query sample, this paper proposes a discriminant-based\nsparse optimization learning model. This learning model integrates discriminant\nand sparsity together. Based on this model, we then propose a classifier called\nlocality-based discriminant sparse representation (LDSR). Because discriminant\ncan help to increase the difference of samples in different classes and to\ndecrease the difference of samples within the same class, LDSR can obtain\nbetter sparse coefficients and constitute a better sparse representation for\nclassification. In order to take advantages of kernel techniques, discriminant\nand sparsity, we further propose a nonlinear classifier called kernel\nlocality-based discriminant sparse representation (KLDSR). Experiments on\nseveral well-known databases prove that the performance of LDSR and KLDSR is\nbetter than that of several state-of-the-art methods including deep learning\nbased methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 13:39:49 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Feng", "Qingxiang", ""], ["Zhou", "Yicong", ""]]}, {"id": "1712.01651", "submitter": "Shun Miao", "authors": "Shun Miao, Sebastien Piat, Peter Fischer, Ahmet Tuysuzoglu, Philip\n  Mewes, Tommaso Mansi, Rui Liao", "title": "Dilated FCN for Multi-Agent 2D/3D Medical Image Registration", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D/3D image registration to align a 3D volume and 2D X-ray images is a\nchallenging problem due to its ill-posed nature and various artifacts presented\nin 2D X-ray images. In this paper, we propose a multi-agent system with an auto\nattention mechanism for robust and efficient 2D/3D image registration.\nSpecifically, an individual agent is trained with dilated Fully Convolutional\nNetwork (FCN) to perform registration in a Markov Decision Process (MDP) by\nobserving a local region, and the final action is then taken based on the\nproposals from multiple agents and weighted by their corresponding confidence\nlevels. The contributions of this paper are threefold. First, we formulate\n2D/3D registration as a MDP with observations, actions, and rewards properly\ndefined with respect to X-ray imaging systems. Second, to handle various\nartifacts in 2D X-ray images, multiple local agents are employed efficiently\nvia FCN-based structures, and an auto attention mechanism is proposed to favor\nthe proposals from regions with more reliable visual cues. Third, a dilated\nFCN-based training mechanism is proposed to significantly reduce the Degree of\nFreedom in the simulation of registration environment, and drastically improve\ntraining efficiency by an order of magnitude compared to standard CNN-based\ntraining method. We demonstrate that the proposed method achieves high\nrobustness on both spine cone beam Computed Tomography data with a low\nsignal-to-noise ratio and data from minimally invasive spine surgery where\nsevere image artifacts and occlusions are presented due to metal screws and\nguide wires, outperforming other state-of-the-art methods (single agent-based\nand optimization-based) by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 03:22:17 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Miao", "Shun", ""], ["Piat", "Sebastien", ""], ["Fischer", "Peter", ""], ["Tuysuzoglu", "Ahmet", ""], ["Mewes", "Philip", ""], ["Mansi", "Tommaso", ""], ["Liao", "Rui", ""]]}, {"id": "1712.01652", "submitter": "Zeng Yu", "authors": "Zeng Yu, Tianrui Li, Ning Yu, Xun Gong, Ke Chen, Yi Pan", "title": "Three-Stream Convolutional Networks for Video-based Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to develop a new architecture that can make full use of the\nfeature maps of convolutional networks. To this end, we study a number of\nmethods for video-based person re-identification and make the following\nfindings: 1) Max-pooling only focuses on the maximum value of a receptive\nfield, wasting a lot of information. 2) Networks with different streams even\nincluding the one with the worst performance work better than networks with\nsame streams, where each one has the best performance alone. 3) A full\nconnection layer at the end of convolutional networks is not necessary. Based\non these studies, we propose a new convolutional architecture termed\nThree-Stream Convolutional Networks (TSCN). It first uses different streams to\nlearn different aspects of feature maps for attentive spatio-temporal fusion of\nvideo, and then merges them together to study some union features. To further\nutilize the feature maps, two architectures are designed by using the\nstrategies of multi-scale and upsampling. Comparative experiments on iLIDS-VID,\nPRID-2011 and MARS datasets illustrate that the proposed architectures are\nsignificantly better for feature extraction than the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:05:58 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Yu", "Zeng", ""], ["Li", "Tianrui", ""], ["Yu", "Ning", ""], ["Gong", "Xun", ""], ["Chen", "Ke", ""], ["Pan", "Yi", ""]]}, {"id": "1712.01653", "submitter": "Ignacio Garcia Dorado", "authors": "Aysegul Dundar and Ignacio Garcia-Dorado", "title": "Context Augmentation for Convolutional Neural Networks", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent enhancements of deep convolutional neural networks (ConvNets)\nempowered by enormous amounts of labeled data have closed the gap with human\nperformance for many object recognition tasks. These impressive results have\ngenerated interest in understanding and visualization of ConvNets. In this\nwork, we study the effect of background in the task of image classification.\nOur results show that changing the backgrounds of the training datasets can\nhave drastic effects on testing accuracies. Furthermore, we enhance existing\naugmentation techniques with the foreground segmented objects. The findings of\nthis work are important in increasing the accuracies when only a small dataset\nis available, in creating datasets, and creating synthetic images.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 23:53:47 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 01:11:35 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Dundar", "Aysegul", ""], ["Garcia-Dorado", "Ignacio", ""]]}, {"id": "1712.01655", "submitter": "Michele Alberti", "authors": "Michele Alberti, Mathias Seuret, Rolf Ingold, Marcus Liwicki", "title": "A Pitfall of Unsupervised Pre-Training", "comments": "This submission has been withdrawn by the author, it is a duplicate\n  of arXiv:1703.04332", "journal-ref": "Conference on Neural Information Processing Systems, Deep\n  Learning: Bridging Theory and Practice, December 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point of this paper is to question typical assumptions in deep learning\nand suggest alternatives. A particular contribution is to prove that even if a\nStacked Convolutional Auto-Encoder is good at reconstructing pictures, it is\nnot necessarily good at discriminating their classes. When using Auto-Encoders,\nintuitively one assumes that features which are good for reconstruction will\nalso lead to high classification accuracy. Indeed, it became research practice\nand is a suggested strategy by introductory books. However, we prove that this\nis not always the case. We thoroughly investigate the quality of features\nproduced by Stacked Convolutional Auto-Encoders when trained to reconstruct\ntheir input. In particular, we analyze the relation between the reconstruction\nand classification capabilities of the network, if we were to use the same\nfeatures for both tasks. Experimental results suggest that in fact, there is no\ncorrelation between the reconstruction score and the quality of features for a\nclassification task. This means, more formally, that the sub-dimension\nrepresentation space learned from the Stacked Convolutional Auto-Encoder (while\nbeing trained for input reconstruction) is not necessarily better separable\nthan the initial input space. Furthermore, we show that the reconstruction\nerror is not a good metric to assess the quality of features, because it is\nbiased by the decoder quality. We do not question the usefulness of\npre-training, but we conclude that aiming for the lowest reconstruction error\nis not necessarily a good idea if afterwards one performs a classification\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:54:18 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 11:51:44 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 20:23:24 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Alberti", "Michele", ""], ["Seuret", "Mathias", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1712.01656", "submitter": "Michele Alberti", "authors": "Michele Alberti, Manuel Bouillon, Rolf Ingold, Marcus Liwicki", "title": "Open Evaluation Tool for Layout Analysis of Document Images", "comments": "The 14th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), HIP: 4th International Workshop on Historical Document\n  Imaging and Processing, Kyoto, Japan, 2017", "journal-ref": "ICDAR-OST 2017", "doi": "10.1109/ICDAR.2017.311", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an open tool for standardizing the evaluation process of\nthe layout analysis task of document images at pixel level. We introduce a new\nevaluation tool that is both available as a standalone Java application and as\na RESTful web service. This evaluation tool is free and open-source in order to\nbe a common tool that anyone can use and contribute to. It aims at providing as\nmany metrics as possible to investigate layout analysis predictions, and also\nprovide an easy way of visualizing the results. This tool evaluates document\nsegmentation at pixel level, and support multi-labeled pixel ground truth.\nFinally, this tool has been successfully used for the ICDAR2017 competition on\nLayout Analysis for Challenging Medieval Manuscripts.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:18:04 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Alberti", "Michele", ""], ["Bouillon", "Manuel", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1712.01657", "submitter": "Danping Liao", "authors": "Danping Liao, Yuntao Qian and Yuan Yan Tang", "title": "Constrained Manifold Learning for Hyperspectral Imagery Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying the large number of bands in a hyper- spectral image (HSI) on a\ntrichromatic monitor is important for HSI processing and analysis system. The\nvisualized image shall convey as much information as possible from the original\nHSI and meanwhile facilitate image interpretation. However, most existing\nmethods display HSIs in false color, which contradicts with user experience and\nexpectation. In this paper, we propose a visualization approach based on\nconstrained manifold learning, whose goal is to learn a visualized image that\nnot only preserves the manifold structure of the HSI but also has natural\ncolors. Manifold learning preserves the image structure by forcing pixels with\nsimilar signatures to be displayed with similar colors. A composite kernel is\napplied in manifold learning to incorporate both the spatial and spectral\ninformation of HSI in the embedded space. The colors of the output image are\nconstrained by a corresponding natural-looking RGB image, which can either be\ngenerated from the HSI itself (e.g., band selection from the visible\nwavelength) or be captured by a separate device. Our method can be done at\ninstance-level and feature-level. Instance-level learning directly obtains the\nRGB coordinates for the pixels in the HSI while feature-level learning learns\nan explicit mapping function from the high dimensional spectral space to the\nRGB space. Experimental results demonstrate the advantage of the proposed\nmethod in information preservation and natural color visualization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 07:04:04 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Liao", "Danping", ""], ["Qian", "Yuntao", ""], ["Tang", "Yuan Yan", ""]]}, {"id": "1712.01661", "submitter": "Avirup Bhattacharyya", "authors": "Avirup Bhattacharyya, Rajkumar Saini, Partha Pratim Roy, Debi Prosad\n  Dogra, Samarjit Kar", "title": "Recognizing Gender from Human Facial Regions using Genetic Algorithm", "comments": "Preprint Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recognition of gender from facial images has gained a lot of\nimportance. There exist a handful of research work that focus on feature\nextraction to obtain gender specific information from facial images. However,\nanalyzing different facial regions and their fusion help in deciding the gender\nof a person from facial images. In this paper, we propose a new approach to\nidentify gender from frontal facial images that is robust to background,\nillumination, intensity, and facial expression. In our framework, first the\nfrontal face image is divided into a number of distinct regions based on facial\nlandmark points that are obtained by the Chehra model proposed by Asthana et\nal. The model provides 49 facial landmark points covering different regions of\nthe face, e.g. forehead, left eye, right eye, lips. Next, a face image is\nsegmented into facial regions using landmark points and features are extracted\nfrom each region. The Compass LBP feature, a variant of LBP feature, has been\nused in our framework to obtain discriminative gender-specific information.\nFollowing this, a Support Vector Machine based classifier has been used to\ncompute the probability scores from each facial region. Finally, the\nclassification scores obtained from individual regions are combined with a\ngenetic algorithm based learning to improve the overall classification\naccuracy. The experiments have been performed on popular face image datasets\nsuch as Adience, cFERET (color FERET), LFW and two sketch datasets, namely CUFS\nand CUFSF. Through experiments, we have observed that, the proposed method\noutperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 14:33:57 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Bhattacharyya", "Avirup", ""], ["Saini", "Rajkumar", ""], ["Roy", "Partha Pratim", ""], ["Dogra", "Debi Prosad", ""], ["Kar", "Samarjit", ""]]}, {"id": "1712.01662", "submitter": "Ryan Renslow", "authors": "Jamie R. Nu\\~nez, Christopher R. Anderton, Ryan S. Renslow", "title": "Optimizing colormaps with consideration for color vision deficiency to\n  enable accurate interpretation of scientific data", "comments": null, "journal-ref": "J. R. Nu\\~nez, C. R. Anderton, and R. S. Renslow, \"Optimizing\n  colormaps with consideration for color vision deficiency to enable accurate\n  interpretation of scientific data,\" PLOS ONE, 2018. 13(7): p. e0199239", "doi": "10.1371/journal.pone.0199239", "report-no": null, "categories": "cs.CV q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color vision deficiency (CVD) affects more than 4% of the population and\nleads to a different visual perception of colors. Though this has been known\nfor decades, colormaps with many colors across the visual spectra are often\nused to represent data, leading to the potential for misinterpretation or\ndifficulty with interpretation by someone with this deficiency. Until the\ncreation of the module presented here, there were no colormaps mathematically\noptimized for CVD using modern color appearance models. While there have been\nsome attempts to make aesthetically pleasing or subjectively tolerable\ncolormaps for those with CVD, our goal was to make optimized colormaps for the\nmost accurate perception of scientific data by as many viewers as possible. We\ndeveloped a Python module, cmaputil, to create CVD-optimized colormaps, which\nimports colormaps and modifies them to be perceptually uniform in CVD-safe\ncolorspace while linearizing and maximizing the brightness range. The module is\nmade available to the science community to enable others to easily create their\nown CVDoptimized colormaps. Here, we present an example CVD-optimized colormap\ncreated with this module that is optimized for viewing by those without a CVD\nas well as those with redgreen colorblindness. This colormap, cividis, enables\nnearly-identical visual-data interpretation to both groups, is perceptually\nuniform in hue and brightness, and increases in brightness linearly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 20:24:15 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 18:14:25 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 19:17:41 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Nu\u00f1ez", "Jamie R.", ""], ["Anderton", "Christopher R.", ""], ["Renslow", "Ryan S.", ""]]}, {"id": "1712.01668", "submitter": "Siyu Yu", "authors": "Siyu Yu, Nanning Zheng, Yongqiang Ma, Hao Wu, Badong Chen", "title": "A Novel Brain Decoding Method: a Correlation Network Framework for\n  Revealing Brain Connections", "comments": "10 pages, 8 figures, IEEE Transactions on Cognitive and Developmental\n  Systems(TCDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding is a hot spot in cognitive science, which focuses on\nreconstructing perceptual images from brain activities. Analyzing the\ncorrelations of collected data from human brain activities and representing\nactivity patterns are two problems in brain decoding based on functional\nmagnetic resonance imaging (fMRI) signals. However, existing correlation\nanalysis methods mainly focus on the strength information of voxel, which\nreveals functional connectivity in the cerebral cortex. They tend to neglect\nthe structural information that implies the intracortical or intrinsic\nconnections; that is, structural connectivity. Hence, the effective\nconnectivity inferred by these methods is relatively unilateral. Therefore, we\nproposed a correlation network (CorrNet) framework that could be flexibly\ncombined with diverse pattern representation models. In the CorrNet framework,\nthe topological correlation was introduced to reveal structural information.\nRich correlations were obtained, which contributed to specifying the underlying\neffective connectivity. We also combined the CorrNet framework with a linear\nsupport vector machine (SVM) and a dynamic evolving spike neuron network (SNN)\nfor pattern representation separately, thus providing a novel method for\ndecoding cognitive activity patterns. Experimental results verified the\nreliability and robustness of our CorrNet framework and demonstrated that the\nnew method achieved significant improvement in brain decoding over comparable\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 11:24:54 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Yu", "Siyu", ""], ["Zheng", "Nanning", ""], ["Ma", "Yongqiang", ""], ["Wu", "Hao", ""], ["Chen", "Badong", ""]]}, {"id": "1712.01670", "submitter": "Mengwei Xu", "authors": "Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xuanzhe Liu", "title": "DeepCache: Principled Cache for Mobile Deep Vision", "comments": "Accepted for publication in the MobiCom 2018, copyright the ACM,\n  posted with permission", "journal-ref": null, "doi": "10.1145/3241539.3241563", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepCache, a principled cache design for deep learning inference\nin continuous mobile vision. DeepCache benefits model execution efficiency by\nexploiting temporal locality in input video streams. It addresses a key\nchallenge raised by mobile vision: the cache must operate under video scene\nvariation, while trading off among cacheability, overhead, and loss in model\naccuracy. At the input of a model, DeepCache discovers video temporal locality\nby exploiting the video's internal structure, for which it borrows proven\nheuristics from video compression; into the model, DeepCache propagates regions\nof reusable results by exploiting the model's internal structure. Notably,\nDeepCache eschews applying video heuristics to model internals which are not\npixels but high-dimensional, difficult-to-interpret data. Our implementation of\nDeepCache works with unmodified deep learning models, requires zero developer's\nmanual effort, and is therefore immediately deployable on off-the-shelf mobile\ndevices. Our experiments show that DeepCache saves inference execution time by\n18% on average and up to 47%. DeepCache reduces system energy consumption by\n20% on average.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 16:52:04 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 15:13:47 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 01:46:42 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 04:02:42 GMT"}, {"version": "v5", "created": "Mon, 30 Mar 2020 04:16:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Xu", "Mengwei", ""], ["Zhu", "Mengze", ""], ["Liu", "Yunxin", ""], ["Lin", "Felix Xiaozhu", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "1712.01675", "submitter": "Jyoti Islam", "authors": "Jyoti Islam, Yanqing Zhang", "title": "An Ensemble of Deep Convolutional Neural Networks for Alzheimer's\n  Disease Detection and Classification", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's Disease destroys brain cells causing people to lose their memory,\nmental functions and ability to continue daily activities. It is a severe\nneurological brain disorder which is not curable, but earlier detection of\nAlzheimer's Disease can help for proper treatment and to prevent brain tissue\ndamage. Detection and classification of Alzheimer's Disease (AD) is challenging\nbecause sometimes the signs that distinguish Alzheimer's Disease MRI data can\nbe found in normal healthy brain MRI data of older people. Moreover, there are\nrelatively small amount of dataset available to train the automated Alzheimer's\nDisease detection and classification model. In this paper, we present a novel\nAlzheimer's Disease detection and classification model using brain MRI data\nanalysis. We develop an ensemble of deep convolutional neural networks and\ndemonstrate superior performance on the Open Access Series of Imaging Studies\n(OASIS) dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 03:13:52 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 19:17:09 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Islam", "Jyoti", ""], ["Zhang", "Yanqing", ""]]}, {"id": "1712.01694", "submitter": "Wellington Pinheiro dos Santos", "authors": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo\n  Emmanuel de Souza, Priscilla B. Mendes, Henrique S. S. Monteiro, Havana Diogo\n  Alves", "title": "Fuzzy-Based Dialectical Non-Supervised Image Classification and\n  Clustering", "comments": null, "journal-ref": "International Journal of Hybrid Intelligent Systems, v. 7, p.\n  115-124, 2010", "doi": "10.3233/HIS-2010-0108", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The materialist dialectical method is a philosophical investigative method to\nanalyze aspects of reality. These aspects are viewed as complex processes\ncomposed by basic units named poles, which interact with each other. Dialectics\nhas experienced considerable progress in the 19th century, with Hegel's\ndialectics and, in the 20th century, with the works of Marx, Engels, and\nGramsci, in Philosophy and Economics. The movement of poles through their\ncontradictions is viewed as a dynamic process with intertwined phases of\nevolution and revolutionary crisis. In order to build a computational process\nbased on dialectics, the interaction between poles can be modeled using fuzzy\nmembership functions. Based on this assumption, we introduce the Objective\nDialectical Classifier (ODC), a non-supervised map for classification based on\nmaterialist dialectics and designed as an extension of fuzzy c-means\nclassifier. As a case study, we used ODC to classify 181 magnetic resonance\nsynthetic multispectral images composed by proton density, $T_1$- and\n$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means,\nand Kohonen's self-organized maps, concerning with image fidelity indexes as\nestimatives of quantization distortion, we proved that ODC can reach almost the\nsame quantization performance as optimal non-supervised classifiers like\nKohonen's self-organized maps.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 17:56:15 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Santos", "Wellington Pinheiro dos", ""], ["de Assis", "Francisco Marcos", ""], ["de Souza", "Ricardo Emmanuel", ""], ["Mendes", "Priscilla B.", ""], ["Monteiro", "Henrique S. S.", ""], ["Alves", "Havana Diogo", ""]]}, {"id": "1712.01695", "submitter": "Wellington Pinheiro dos Santos", "authors": "Higor Neto Lima, Wellington Pinheiro dos Santos, M\\^euser Jorge Silva\n  Valen\\c{c}a", "title": "Triagem virtual de imagens de imuno-histoqu\\'imica usando redes neurais\n  artificiais e espectro de padr\\~oes", "comments": "in Portuguese", "journal-ref": "Learning and Nonlinear Models, v. 8, p. 202-215, 2010", "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of organizing medical images according to their nature,\napplication and relevance is increasing. Furhermore, a previous selection of\nmedical images can be useful to accelerate the task of analysis by\npathologists. Herein this work we propose an image classifier to integrate a\nCBIR (Content-Based Image Retrieval) selection system. This classifier is based\non pattern spectra and neural networks. Feature selection is performed using\npattern spectra and principal component analysis, whilst image classification\nis based on multilayer perceptrons and a composition of self-organizing maps\nand learning vector quantization. These methods were applied for content\nselection of immunohistochemical images of placenta and newdeads lungs. Results\ndemonstrated that this approach can reach reasonable classification\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 18:06:22 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Lima", "Higor Neto", ""], ["Santos", "Wellington Pinheiro dos", ""], ["Valen\u00e7a", "M\u00eauser Jorge Silva", ""]]}, {"id": "1712.01696", "submitter": "Wellington Pinheiro dos Santos", "authors": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis", "title": "Avalia\\c{c}\\~ao do m\\'etodo dial\\'etico na quantiza\\c{c}\\~ao de imagens\n  multiespectrais", "comments": "in Portuguese", "journal-ref": "Learning and Nonlinear Models, v. 8, p. 174-201, 2010", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised classification has a very important role in the analysis of\nmultispectral images, given its ability to assist the extraction of a priori\nknowledge of images. Algorithms like k-means and fuzzy c-means has long been\nused in this task. Computational Intelligence has proven to be an important\nfield to assist in building classifiers optimized according to the quality of\nthe grouping of classes and the evaluation of the quality of vector\nquantization. Several studies have shown that Philosophy, especially the\nDialectical Method, has served as an important inspiration for the construction\nof new computational methods. This paper presents an evaluation of four methods\nbased on the Dialectics: the Objective Dialectical Classifier and the\nDialectical Optimization Method adapted to build a version of k-means with\noptimal quality indices; each of them is presented in two versions: a canonical\nversion and another version obtained by applying the Principle of Maximum\nEntropy. These methods were compared to k-means, fuzzy c-means and Kohonen's\nself-organizing maps. The results showed that the methods based on Dialectics\nare robust to noise, and quantization can achieve results as good as those\nobtained with the Kohonen map, considered an optimal quantizer.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 18:13:27 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Santos", "Wellington Pinheiro dos", ""], ["de Assis", "Francisco Marcos", ""]]}, {"id": "1712.01697", "submitter": "Wellington Pinheiro dos Santos", "authors": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo\n  Emmanuel de Souza, Pl\\'inio Batista dos Santos Filho, Fernando Buarque de\n  Lima Neto", "title": "Dialectical Multispectral Classification of Diffusion-Weighted Magnetic\n  Resonance Images as an Alternative to Apparent Diffusion Coefficients Maps to\n  Perform Anatomical Analysis", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics, v. 33, p. 442-460, 2009", "doi": "10.1016/j.compmedimag.2009.04.004", "report-no": null, "categories": "cs.CV cs.GR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral image analysis is a relatively promising field of research with\napplications in several areas, such as medical imaging and satellite\nmonitoring. A considerable number of current methods of analysis are based on\nparametric statistics. Alternatively, some methods in Computational\nIntelligence are inspired by biology and other sciences. Here we claim that\nPhilosophy can be also considered as a source of inspiration. This work\nproposes the Objective Dialectical Method (ODM): a method for classification\nbased on the Philosophy of Praxis. ODM is instrumental in assembling evolvable\nmathematical tools to analyze multispectral images. In the case study described\nin this paper, multispectral images are composed of diffusion-weighted (DW)\nmagnetic resonance (MR) images. The results are compared to ground-truth images\nproduced by polynomial networks using a morphological similarity index. The\nclassification results are used to improve the usual analysis of the apparent\ndiffusion coefficient map. Such results proved that gray and white matter can\nbe distinguished in DW-MR multispectral analysis and, consequently, DW-MR\nimages can also be used to furnish anatomical information.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 18:23:33 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Santos", "Wellington Pinheiro dos", ""], ["de Assis", "Francisco Marcos", ""], ["de Souza", "Ricardo Emmanuel", ""], ["Filho", "Pl\u00ednio Batista dos Santos", ""], ["Neto", "Fernando Buarque de Lima", ""]]}, {"id": "1712.01700", "submitter": "Wellington Pinheiro dos Santos", "authors": "Wellington Pinheiro dos Santos, Ricardo Emmanuel de Souza, Ascendino\n  Fl\\'avio Dias e Silva, Pl\\'inio Batista dos Santos Filho", "title": "Avalia\\c{c}\\~ao da doen\\c{c}a de Alzheimer pela an\\'alise multiespectral\n  de imagens DW-MR por redes RBF como alternativa aos mapas ADC", "comments": "in Portuguese", "journal-ref": "Learning and Nonlinear Models, v. 4, p. 43-53, 2008", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common cause of dementia, yet difficult to\naccurately diagnose without the use of invasive techniques, particularly at the\nbeginning of the disease. This work addresses the classification and analysis\nof multispectral synthetic images composed by diffusion-weighted magnetic\nresonance brain volumes for evaluation of the area of cerebrospinal fluid and\nits correlation with the progression of Alzheimer's disease. A 1.5 T MR imaging\nsystem was used to acquire all the images presented. The classification methods\nare based on multilayer perceptrons and classifiers of radial basis function\nnetworks. It is assumed that the classes of interest can be separated by\nhyperquadrics. A polynomial network of degree 2 is used to classify the\noriginal volumes, generating a ground-truth volume. The classification results\nare used to improve the usual analysis by the map of apparent diffusion\ncoefficients.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 19:02:00 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Santos", "Wellington Pinheiro dos", ""], ["de Souza", "Ricardo Emmanuel", ""], ["Silva", "Ascendino Fl\u00e1vio Dias e", ""], ["Filho", "Pl\u00ednio Batista dos Santos", ""]]}, {"id": "1712.01707", "submitter": "Chaolu Feng", "authors": "Chaolu Feng", "title": "IEOPF: An Active Contour Model for Image Segmentation with\n  Inhomogeneities Estimated by Orthogonal Primary Functions", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is still an open problem especially when intensities of\nthe interested objects are overlapped due to the presence of intensity\ninhomogeneity (also known as bias field). To segment images with intensity\ninhomogeneities, a bias correction embedded level set model is proposed where\nInhomogeneities are Estimated by Orthogonal Primary Functions (IEOPF). In the\nproposed model, the smoothly varying bias is estimated by a linear combination\nof a given set of orthogonal primary functions. An inhomogeneous intensity\nclustering energy is then defined and membership functions of the clusters\ndescribed by the level set function are introduced to rewrite the energy as a\ndata term of the proposed model. Similar to popular level set methods, a\nregularization term and an arc length term are also included to regularize and\nsmooth the level set function, respectively. The proposed model is then\nextended to multichannel and multiphase patterns to segment colourful images\nand images with multiple objects, respectively. It has been extensively tested\non both synthetic and real images that are widely used in the literature and\npublic BrainWeb and IBSR datasets. Experimental results and comparison with\nstate-of-the-art methods demonstrate that advantages of the proposed model in\nterms of bias correction and segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 15:19:54 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 06:59:07 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 06:27:22 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 03:18:13 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Feng", "Chaolu", ""]]}, {"id": "1712.01721", "submitter": "Franco Manessi", "authors": "Franco Manessi, Alessandro Rozza, Simone Bianco, Paolo Napoletano,\n  Raimondo Schettini", "title": "Automated Pruning for Deep Neural Network Compression", "comments": "8 pages, 5 figures. Published as a conference paper at ICPR 2018", "journal-ref": "2018 24th International Conference on Pattern Recognition (ICPR),\n  Beijing, 2018, pp. 657-664", "doi": "10.1109/ICPR.2018.8546129", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method to improve the pruning step of the current\nstate-of-the-art methodology to compress neural networks. The novelty of the\nproposed pruning technique is in its differentiability, which allows pruning to\nbe performed during the backpropagation phase of the network training. This\nenables an end-to-end learning and strongly reduces the training time. The\ntechnique is based on a family of differentiable pruning functions and a new\nregularizer specifically designed to enforce pruning. The experimental results\nshow that the joint optimization of both the thresholds and the network weights\npermits to reach a higher compression rate, reducing the number of weights of\nthe pruned network by a further 14% to 33% compared to the current\nstate-of-the-art. Furthermore, we believe that this is the first study where\nthe generalization capabilities in transfer learning tasks of the features\nextracted by a pruned network are analyzed. To achieve this goal, we show that\nthe representations learned using the proposed pruning methodology maintain the\nsame effectiveness and generality of those learned by the corresponding\nnon-compressed network on a set of different recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 15:58:44 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 14:19:12 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Manessi", "Franco", ""], ["Rozza", "Alessandro", ""], ["Bianco", "Simone", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1712.01727", "submitter": "Jos\\'e Lezama", "authors": "Jos\\'e Lezama, Qiang Qiu, Pablo Mus\\'e, Guillermo Sapiro", "title": "OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained using a softmax layer at the top and the\ncross-entropy loss are ubiquitous tools for image classification. Yet, this\ndoes not naturally enforce intra-class similarity nor inter-class margin of the\nlearned deep representations. To simultaneously achieve these two goals,\ndifferent solutions have been proposed in the literature, such as the pairwise\nor triplet losses. However, such solutions carry the extra task of selecting\npairs or triplets, and the extra computational burden of computing and learning\nfor many combinations of them. In this paper, we propose a plug-and-play loss\nterm for deep networks that explicitly reduces intra-class variance and\nenforces inter-class margin simultaneously, in a simple and elegant geometric\nmanner. For each class, the deep features are collapsed into a learned linear\nsubspace, or union of them, and inter-class subspaces are pushed to be as\northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does\nnot require carefully crafting pairs or triplets of samples for training, and\nworks standalone as a classification loss, being the first reported deep metric\nlearning framework of its kind. Because of the improved margin between features\nof different classes, the resulting deep networks generalize better, are more\ndiscriminative, and more robust. We demonstrate improved classification\nperformance in general object recognition, plugging the proposed loss term into\nexisting off-the-shelf architectures. In particular, we show the advantage of\nthe proposed loss in the small data/model scenario, and we significantly\nadvance the state-of-the-art on the Stanford STL-10 benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:03:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Lezama", "Jos\u00e9", ""], ["Qiu", "Qiang", ""], ["Mus\u00e9", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1712.01743", "submitter": "Lukas Cavigelli", "authors": "Manuele Rusci, Lukas Cavigelli, Luca Benini", "title": "Design Automation for Binarized Neural Networks: A Quantum Leap\n  Opportunity?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AR cs.CV cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design automation in general, and in particular logic synthesis, can play a\nkey role in enabling the design of application-specific Binarized Neural\nNetworks (BNN). This paper presents the hardware design and synthesis of a\npurely combinational BNN for ultra-low power near-sensor processing. We\nleverage the major opportunities raised by BNN models, which consist mostly of\nlogical bit-wise operations and integer counting and comparisons, for pushing\nultra-low power deep learning circuits close to the sensor and coupling it with\nbinarized mixed-signal image sensor data. We analyze area, power and energy\nmetrics of BNNs synthesized as combinational networks. Our synthesis results in\nGlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for\nimplementing a combinational BNN with 32x32 binary input sensor receptive field\nand weight parameters fixed at design time. This is 2.2x smaller than a\nsynthesized network with re-configurable parameters. With respect to other\ncomparable techniques for deep learning near-sensor processing, our approach\nfeatures a 10x higher energy efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 09:54:37 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Rusci", "Manuele", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1712.01751", "submitter": "Chen Qin", "authors": "Chen Qin, Jo Schlemper, Jose Caballero, Anthony Price, Joseph V.\n  Hajnal and Daniel Rueckert", "title": "Convolutional Recurrent Neural Networks for Dynamic MR Image\n  Reconstruction", "comments": "Published in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating the data acquisition of dynamic magnetic resonance imaging (MRI)\nleads to a challenging ill-posed inverse problem, which has received great\ninterest from both the signal processing and machine learning community over\nthe last decades. The key ingredient to the problem is how to exploit the\ntemporal correlation of the MR sequence to resolve the aliasing artefact.\nTraditionally, such observation led to a formulation of a non-convex\noptimisation problem, which were solved using iterative algorithms. Recently,\nhowever, deep learning based-approaches have gained significant popularity due\nto its ability to solve general inversion problems. In this work, we propose a\nunique, novel convolutional recurrent neural network (CRNN) architecture which\nreconstructs high quality cardiac MR images from highly undersampled k-space\ndata by jointly exploiting the dependencies of the temporal sequences as well\nas the iterative nature of the traditional optimisation algorithms. In\nparticular, the proposed architecture embeds the structure of the traditional\niterative algorithms, efficiently modelling the recurrence of the iterative\nreconstruction stages by using recurrent hidden connections over such\niterations. In addition, spatiotemporal dependencies are simultaneously learnt\nby exploiting bidirectional recurrent hidden connections across time sequences.\nThe proposed algorithm is able to learn both the temporal dependency and the\niterative reconstruction process effectively with only a very small number of\nparameters, while outperforming current MR reconstruction methods in terms of\ncomputational complexity, reconstruction accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:49:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 17:51:56 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 21:14:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Qin", "Chen", ""], ["Schlemper", "Jo", ""], ["Caballero", "Jose", ""], ["Price", "Anthony", ""], ["Hajnal", "Joseph V.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1712.01770", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira\n  Bermudez, C\\'edric Richard", "title": "Tech Report: A Fast Multiscale Spatial Regularization for Sparse\n  Hyperspectral Unmixing", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2018.2878394", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse hyperspectral unmixing from large spectral libraries has been\nconsidered to circumvent limitations of endmember extraction algorithms in many\napplications. This strategy often leads to ill-posed inverse problems, which\ncan benefit from spatial regularization strategies. While existing spatial\nregularization methods improve the problem conditioning and promote piecewise\nsmooth solutions, they lead to large nonsmooth optimization problems. Thus,\nefficiently introducing spatial context in the unmixing problem remains a\nchallenge, and a necessity for many real world applications. In this paper, a\nnovel multiscale spatial regularization approach for sparse unmixing is\nproposed. The method uses a signal-adaptive spatial multiscale decomposition\nbased on superpixels to decompose the unmixing problem into two simpler\nproblems, one in the approximation domain and another in the original domain.\nSimulation results using both synthetic and real data indicate that the\nproposed method can outperform state-of-the-art Total Variation-based\nalgorithms with a computation time comparable to that of their unregularized\ncounterparts.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:24:54 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 03:28:50 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 22:21:08 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1712.01785", "submitter": "Kexin Pei", "authors": "Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana", "title": "Towards Practical Verification of Machine Learning: The Case of Computer\n  Vision Systems", "comments": "16 pages, 11 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing usage of machine learning (ML) techniques in security-\nand safety-critical domains, such as autonomous systems and medical diagnosis,\nensuring correct behavior of ML systems, especially for different corner cases,\nis of growing importance. In this paper, we propose a generic framework for\nevaluating security and robustness of ML systems using different real-world\nsafety properties. We further design, implement and evaluate VeriVis, a\nscalable methodology that can verify a diverse set of safety properties for\nstate-of-the-art computer vision systems with only blackbox access. VeriVis\nleverage different input space reduction techniques for efficient verification\nof different safety properties. VeriVis is able to find thousands of safety\nviolations in fifteen state-of-the-art computer vision systems including ten\nDeep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving\nsystem with thousands of neurons as well as five commercial third-party vision\nAPIs including Google vision and Clarifai for twelve different safety\nproperties. Furthermore, VeriVis can successfully verify local safety\nproperties, on average, for around 31.7% of the test images. VeriVis finds up\nto 64.8x more violations than existing gradient-based methods that, unlike\nVeriVis, cannot ensure non-existence of any violations. Finally, we show that\nretraining using the safety violations detected by VeriVis can reduce the\naverage number of violations up to 60.2%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:49:18 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 21:30:44 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 00:30:53 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Pei", "Kexin", ""], ["Cao", "Yinzhi", ""], ["Yang", "Junfeng", ""], ["Jana", "Suman", ""]]}, {"id": "1712.01802", "submitter": "Bharat Singh", "authors": "Bharat Singh, Hengduo Li, Abhishek Sharma, Larry S. Davis", "title": "R-FCN-3000 at 30fps: Decoupling Detection and Classification", "comments": "CVPR 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present R-FCN-3000, a large-scale real-time object detector in which\nobjectness detection and classification are decoupled. To obtain the detection\nscore for an RoI, we multiply the objectness score with the fine-grained\nclassification score. Our approach is a modification of the R-FCN architecture\nin which position-sensitive filters are shared across different object classes\nfor performing localization. For fine-grained classification, these\nposition-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9%\non the ImageNet detection dataset and outperforms YOLO-9000 by 18% while\nprocessing 30 images per second. We also show that the objectness learned by\nR-FCN-3000 generalizes to novel classes and the performance increases with the\nnumber of training object classes - supporting the hypothesis that it is\npossible to learn a universal objectness detector. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:30:19 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Singh", "Bharat", ""], ["Li", "Hengduo", ""], ["Sharma", "Abhishek", ""], ["Davis", "Larry S.", ""]]}, {"id": "1712.01812", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A. Efros,\n  Jitendra Malik", "title": "Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene", "comments": "Project url with code: https://shubhtuls.github.io/factored3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to take a single 2D image of a scene and recover\nthe 3D structure in terms of a small set of factors: a layout representing the\nenclosing surfaces as well as a set of objects represented in terms of shape\nand pose. We propose a convolutional neural network-based approach to predict\nthis representation and benchmark it on a large dataset of indoor scenes. Our\nexperiments evaluate a number of practical design questions, demonstrate that\nwe can infer this representation, and quantitatively and qualitatively\ndemonstrate its merits compared to alternate representations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:42:13 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:34:15 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Gupta", "Saurabh", ""], ["Fouhey", "David", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1712.01833", "submitter": "Sihao Ding", "authors": "Sihao Ding, Andreas Wallin", "title": "Towards Recovery of Conditional Vectors from Conditional Generative\n  Adversarial Networks", "comments": "Under consideration for Pattern Recognition Letters, 11 pages", "journal-ref": "Pattern Recognition Letters, vol. 122, pp. 66-72, 1 May 2019", "doi": "10.1016/j.patrec.2019.02.020", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional Generative Adversarial Network allows for generating samples\nconditioned on certain external information. Being able to recover latent and\nconditional vectors from a condi- tional GAN can be potentially valuable in\nvarious applications, ranging from image manipulation for entertaining purposes\nto diagnosis of the neural networks for security purposes. In this work, we\nshow that it is possible to recover both latent and conditional vectors from\ngenerated images given the generator of a conditional generative adversarial\nnetwork. Such a recovery is not trivial due to the often multi-layered\nnon-linearity of deep neural networks. Furthermore, the effect of such recovery\napplied on real natural images are investigated. We discovered that there\nexists a gap between the recovery performance on generated and real images,\nwhich we believe comes from the difference between generated data distribution\nand real data distribution. Experiments are conducted to evaluate the recovered\nconditional vectors and the reconstructed images from these recovered vectors\nquantitatively and qualitatively, showing promising results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 03:40:31 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ding", "Sihao", ""], ["Wallin", "Andreas", ""]]}, {"id": "1712.01867", "submitter": "Jonghyun Choi", "authors": "Jonghyun Choi and Jayant Krishnamurthy and Aniruddha Kembhavi and Ali\n  Farhadi", "title": "Structured Set Matching Networks for One-Shot Part Labeling", "comments": "one shot part labeling. CVPR 2018 accepted as spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrams often depict complex phenomena and serve as a good test bed for\nvisual and textual reasoning. However, understanding diagrams using natural\nimage understanding approaches requires large training datasets of diagrams,\nwhich are very hard to obtain. Instead, this can be addressed as a matching\nproblem either between labeled diagrams, images or both. This problem is very\nchallenging since the absence of significant color and texture renders local\ncues ambiguous and requires global reasoning. We consider the problem of\none-shot part labeling: labeling multiple parts of an object in a target image\ngiven only a single source image of that category. For this set-to-set matching\nproblem, we introduce the Structured Set Matching Network (SSMN), a structured\nprediction model that incorporates convolutional neural networks. The SSMN is\ntrained using global normalization to maximize local match scores between\ncorresponding elements and a global consistency score among all matched\nelements, while also enforcing a matching constraint between the two sets. The\nSSMN significantly outperforms several strong baselines on three label transfer\nscenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200\ncategories; image-to-image, evaluated on a dataset built on top of the Pascal\nPart Dataset; and image-to-diagram, evaluated on transferring labels across\nthese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:03:08 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 20:49:05 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Choi", "Jonghyun", ""], ["Krishnamurthy", "Jayant", ""], ["Kembhavi", "Aniruddha", ""], ["Farhadi", "Ali", ""]]}, {"id": "1712.01887", "submitter": "Song Han", "authors": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for\n  Distributed Training", "comments": "we find 99.9% of the gradient exchange in distributed SGD is\n  redundant; we reduce the communication bandwidth by two orders of magnitude\n  without losing accuracy. Code is available at:\n  https://github.com/synxlin/deep-gradient-compression", "journal-ref": "ICLR 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile. Code is available at:\nhttps://github.com/synxlin/deep-gradient-compression.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:48:11 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 19:38:39 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 03:28:30 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lin", "Yujun", ""], ["Han", "Song", ""], ["Mao", "Huizi", ""], ["Wang", "Yu", ""], ["Dally", "William J.", ""]]}, {"id": "1712.01892", "submitter": "Hanwang Zhang", "authors": "Hanwang Zhang, Yulei Niu, Shih-Fu Chang", "title": "Grounding Referring Expressions in Images by Variational Context", "comments": "in 2018 Conference on Computer Vision and Pattern Recognition\n  (CVPR'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on grounding (i.e., localizing or linking) referring expressions in\nimages, e.g., \"largest elephant standing behind baby elephant\". This is a\ngeneral yet challenging vision-language task since it does not only require the\nlocalization of objects, but also the multimodal comprehension of context ---\nvisual attributes (e.g., \"largest\", \"baby\") and relationships (e.g., \"behind\")\nthat help to distinguish the referent from other objects, especially those of\nthe same category. Due to the exponential complexity involved in modeling the\ncontext associated with multiple image regions, existing work oversimplifies\nthis task to pairwise region modeling by multiple instance learning. In this\npaper, we propose a variational Bayesian method, called Variational Context, to\nsolve the problem of complex context modeling in referring expression\ngrounding. Our model exploits the reciprocal relation between the referent and\ncontext, i.e., either of them influences the estimation of the posterior\ndistribution of the other, and thereby the search space of context can be\ngreatly reduced, resulting in better localization of referent. We develop a\nnovel cue-specific language-vision embedding network that learns this\nreciprocity model end-to-end. We also extend the model to the unsupervised\nsetting where no annotation for the referent is available. Extensive\nexperiments on various benchmarks show consistent improvement over\nstate-of-the-art methods in both supervised and unsupervised settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:57:52 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 04:54:08 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zhang", "Hanwang", ""], ["Niu", "Yulei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1712.01893", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Matthias Wilms, and Heinz Handels", "title": "Population-based Respiratory 4D Motion Atlas Construction and its\n  Application for VR Simulations of Liver Punctures", "comments": "7 pages, 4 figures, 1 movie, Proc. SPIE Medical Imaging: Image\n  Processing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) training simulators of liver needle insertion in the\nhepatic area of breathing virtual patients currently need 4D data acquisitions\nas a prerequisite. Here, first a population-based breathing virtual patient 4D\natlas can be built and second the requirement of a dose-relevant or expensive\nacquisition of a 4D data set for a new static 3D patient can be mitigated by\nwarping the mean atlas motion. The breakthrough contribution of this work is\nthe construction and reuse of population-based learned 4D motion models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:59:20 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 13:54:10 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Wilms", "Matthias", ""], ["Handels", "Heinz", ""]]}, {"id": "1712.01907", "submitter": "Junsik Kim", "authors": "Junsik Kim, Seokju Lee, Tae-Hyun Oh, In So Kweon", "title": "Co-domain Embedding using Deep Quadruplet Networks for Unseen Traffic\n  Sign Recognition", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in visual recognition show overarching success by virtue of\nlarge amounts of supervised data. However,the acquisition of a large supervised\ndataset is often challenging. This is also true for intelligent transportation\napplications, i.e., traffic sign recognition. For example, a model trained with\ndata of one country may not be easily generalized to another country without\nmuch data. We propose a novel feature embedding scheme for unseen class\nclassification when the representative class template is given. Traffic signs,\nunlike other objects, have official images. We perform co-domain embedding\nusing a quadruple relationship from real and synthetic domains. Our quadruplet\nnetwork fully utilizes the explicit pairwise similarity relationships among\nsamples from different domains. We validate our method on three datasets with\ntwo experiments involving one-shot classification and feature generalization.\nThe results show that the proposed method outperforms competing approaches on\nboth seen and unseen classes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 20:24:18 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Kim", "Junsik", ""], ["Lee", "Seokju", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1712.01924", "submitter": "Omid Hosseini Jafari", "authors": "Omid Hosseini Jafari, Siva Karthik Mustikovela, Karl Pertsch, Eric\n  Brachmann, Carsten Rother", "title": "iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of 6D pose estimation of known rigid objects from single\ninput images in scenarios where the objects are partly occluded. Recent\nRGB-D-based methods are robust to moderate degrees of occlusion. For RGB\ninputs, no previous method works well for partly occluded objects. Our main\ncontribution is to present the first deep learning-based system that estimates\naccurate poses for partly occluded objects from RGB-D and RGB input. We achieve\nthis with a new instance-aware pipeline that decomposes 6D object pose\nestimation into a sequence of simpler steps, where each step removes specific\naspects of the problem. The first step localizes all known objects in the image\nusing an instance segmentation network, and hence eliminates surrounding\nclutter and occluders. The second step densely maps pixels to 3D object surface\npositions, so called object coordinates, using an encoder-decoder network, and\nhence eliminates object appearance. The third, and final, step predicts the 6D\npose using geometric optimization. We demonstrate that we significantly\noutperform the state-of-the-art for pose estimation of partly occluded objects\nfor both RGB and RGB-D input.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:01:24 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 23:21:41 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 11:08:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jafari", "Omid Hosseini", ""], ["Mustikovela", "Siva Karthik", ""], ["Pertsch", "Karl", ""], ["Brachmann", "Eric", ""], ["Rother", "Carsten", ""]]}, {"id": "1712.01928", "submitter": "Long Chen", "authors": "Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, Shih-Fu Chang", "title": "Zero-Shot Visual Recognition using Semantics-Preserving Adversarial\n  Embedding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework called Semantics-Preserving Adversarial\nEmbedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test\nimages and their classes are both unseen during training. SP-AEN aims to tackle\nthe inherent problem --- semantic loss --- in the prevailing family of\nembedding-based ZSL, where some semantics would be discarded during training if\nthey are non-discriminative for training classes, but could become critical for\nrecognizing test classes. Specifically, SP-AEN prevents the semantic loss by\nintroducing an independent visual-to-semantic space embedder which disentangles\nthe semantic space into two subspaces for the two arguably conflicting\nobjectives: classification and reconstruction. Through adversarial learning of\nthe two subspaces, SP-AEN can transfer the semantics from the reconstructive\nsubspace to the discriminative one, accomplishing the improved zero-shot\nrecognition of unseen classes. Comparing with prior works, SP-AEN can not only\nimprove classification but also generate photo-realistic images, demonstrating\nthe effectiveness of semantic preservation. On four popular benchmarks: CUB,\nAWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art\nmethods by an absolute performance difference of 12.2\\%, 9.3\\%, 4.0\\%, and\n3.6\\% in terms of harmonic mean values\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:16:52 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 06:43:52 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chen", "Long", ""], ["Zhang", "Hanwang", ""], ["Xiao", "Jun", ""], ["Liu", "Wei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1712.01937", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi, Yuelong Li, and Vishal Monga", "title": "Blind Image Deblurring Using Row-Column Sparse Representations", "comments": "Accepted to IEEE Signal Processing Letters, December 2017", "journal-ref": null, "doi": "10.1109/LSP.2017.2782570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring is a particularly challenging inverse problem where\nthe blur kernel is unknown and must be estimated en route to recover the\ndeblurred image. The problem is of strong practical relevance since many\nimaging devices such as cellphone cameras, must rely on deblurring algorithms\nto yield satisfactory image quality. Despite significant research effort,\nhandling large motions remains an open problem. In this paper, we develop a new\nmethod called Blind Image Deblurring using Row-Column Sparsity (BD-RCS) to\naddress this issue. Specifically, we model the outer product of kernel and\nimage coefficients in certain transformation domains as a rank-one matrix, and\nrecover it by solving a rank minimization problem. Our central contribution\nthen includes solving {\\em two new} optimization problems involving row and\ncolumn sparsity to automatically determine blur kernel and image support\nsequentially. The kernel and image can then be recovered through a singular\nvalue decomposition (SVD). Experimental results on linear motion deblurring\ndemonstrate that BD-RCS can yield better results than state of the art,\nparticularly when the blur is caused by large motion. This is confirmed both\nvisually and through quantitative measures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:39:29 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Li", "Yuelong", ""], ["Monga", "Vishal", ""]]}, {"id": "1712.01938", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Michael S. Ryoo", "title": "Learning Latent Super-Events to Detect Multiple Activities in Videos", "comments": "CVPR 2018", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of learning latent super-events from\nactivity videos, and present how it benefits activity detection in continuous\nvideos. We define a super-event as a set of multiple events occurring together\nin videos with a particular temporal organization; it is the opposite concept\nof sub-events. Real-world videos contain multiple activities and are rarely\nsegmented (e.g., surveillance videos), and learning latent super-events allows\nthe model to capture how the events are temporally related in videos. We design\ntemporal structure filters that enable the model to focus on particular\nsub-intervals of the videos, and use them together with a soft attention\nmechanism to learn representations of latent super-events. Super-event\nrepresentations are combined with per-frame or per-segment CNNs to provide\nframe-level annotations. Our approach is designed to be fully differentiable,\nenabling end-to-end learning of latent super-event representations jointly with\nthe activity detector using them. Our experiments with multiple public video\ndatasets confirm that the proposed concept of latent super-event learning\nsignificantly benefits activity detection, advancing the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:40:09 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 05:12:22 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1712.01955", "submitter": "Mengyao Zhai", "authors": "Mengyao Zhai, Jiacheng Chen, Ruizhi Deng, Lei Chen, Ligeng Zhu, Greg\n  Mori", "title": "Learning to Forecast Videos of Human Activity with Multi-granularity\n  Models and Adaptive Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for forecasting video of complex human activity\ninvolving multiple people. Direct pixel-level prediction is too simple to\nhandle the appearance variability in complex activities. Hence, we develop\nnovel intermediate representations. An architecture combining a hierarchical\ntemporal model for predicting human poses and encoder-decoder convolutional\nneural networks for rendering target appearances is proposed. Our hierarchical\nmodel captures interactions among people by adopting a dynamic group-based\ninteraction mechanism. Next, our appearance rendering network encodes the\ntargets' appearances by learning adaptive appearance filters using a fully\nconvolutional network. Finally, these filters are placed in encoder-decoder\nneural networks to complete the rendering. We demonstrate that our model can\ngenerate videos that are superior to state-of-the-art methods, and can handle\ncomplex human activity scenarios in video forecasting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 22:26:43 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Zhai", "Mengyao", ""], ["Chen", "Jiacheng", ""], ["Deng", "Ruizhi", ""], ["Chen", "Lei", ""], ["Zhu", "Ligeng", ""], ["Mori", "Greg", ""]]}, {"id": "1712.01970", "submitter": "Amina Hussein", "authors": "Amina E. Hussein", "title": "What's in my closet?: Image classification using fuzzy logic", "comments": "12 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fuzzy system was created in MATLAB to identify an item of clothing as a\ndress, shirt, or pair of pants from a series of input images. The system was\ninitialized using a high-contrast vector-image of each item of clothing as the\nstate closest to a direct solution. Nine other user-input images (three of each\nitem) were also used to determine the characteristic function of each item and\nrecognize each pattern. Mamdani inference systems were used for edge location\nand identification of characteristic regions of interest for each item of\nclothing. Based on these non-dimensional trends, a second Mamdani fuzzy\ninference system was used to characterize each image as containing a shirt, a\ndress, or a pair of pants. An outline of the fuzzy inference system and image\nprocessing techniques used for creating an image pattern recognition system are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 23:41:47 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Hussein", "Amina E.", ""]]}, {"id": "1712.02029", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with Stochastic Gradient Descent, or its\nvariants, requires careful choice of both learning rate and batch size. While\nsmaller batch sizes generally converge in fewer training epochs, larger batch\nsizes offer more parallelism and hence better computational efficiency. We have\ndeveloped a new training approach that, rather than statically choosing a\nsingle batch size for all epochs, adaptively increases the batch size during\nthe training process. Our method delivers the convergence rate of small batch\nsizes while achieving performance similar to large batch sizes. We analyse our\napproach using the standard AlexNet, ResNet, and VGG networks operating on the\npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate\nthat learning with adaptive batch sizes can improve performance by factors of\nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1%\nrelative to training with fixed batch sizes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:19:14 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 04:26:45 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Naumov", "Maxim", ""], ["Garland", "Michael", ""]]}, {"id": "1712.02036", "submitter": "Yan Huang", "authors": "Yan Huang, Qi Wu, Liang Wang", "title": "Learning Semantic Concepts and Order for Image and Sentence Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and sentence matching has made great progress recently, but it remains\nchallenging due to the large visual-semantic discrepancy. This mainly arises\nfrom that the representation of pixel-level image usually lacks of high-level\nsemantic information as in its matched sentence. In this work, we propose a\nsemantic-enhanced image and sentence matching model, which can improve the\nimage representation by learning semantic concepts and then organizing them in\na correct semantic order. Given an image, we first use a multi-regional\nmulti-label CNN to predict its semantic concepts, including objects,\nproperties, actions, etc. Then, considering that different orders of semantic\nconcepts lead to diverse semantic meanings, we use a context-gated sentence\ngeneration scheme for semantic order learning. It simultaneously uses the image\nglobal context containing concept relations as reference and the groundtruth\nsemantic order in the matched sentence as supervision. After obtaining the\nimproved image representation, we learn the sentence representation with a\nconventional LSTM, and then jointly perform image and sentence matching and\nsentence generation for model learning. Extensive experiments demonstrate the\neffectiveness of our learned semantic concepts and order, by achieving the\nstate-of-the-art results on two public benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:36:40 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Huang", "Yan", ""], ["Wu", "Qi", ""], ["Wang", "Liang", ""]]}, {"id": "1712.02048", "submitter": "Shivanthan Yohanandan", "authors": "Shivanthan A.C. Yohanandan, Adrian G. Dyer, Dacheng Tao, and Andy Song", "title": "Saliency Preservation in Low-Resolution Grayscale Images", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual salience detection originated over 500 million years ago and is one of\nnature's most efficient mechanisms. In contrast, many state-of-the-art\ncomputational saliency models are complex and inefficient. Most saliency models\nprocess high-resolution color (HC) images; however, insights into the\nevolutionary origins of visual salience detection suggest that achromatic\nlow-resolution vision is essential to its speed and efficiency. Previous\nstudies showed that low-resolution color and high-resolution grayscale images\npreserve saliency information. However, to our knowledge, no one has\ninvestigated whether saliency is preserved in low-resolution grayscale (LG)\nimages. In this study, we explain the biological and computational motivation\nfor LG, and show, through a range of human eye-tracking and computational\nmodeling experiments, that saliency information is preserved in LG images.\nMoreover, we show that using LG images leads to significant speedups in model\ntraining and detection times and conclude by proposing LG images for fast and\nefficient salience detection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 05:39:13 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 05:54:39 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 11:06:31 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yohanandan", "Shivanthan A. C.", ""], ["Dyer", "Adrian G.", ""], ["Tao", "Dacheng", ""], ["Song", "Andy", ""]]}, {"id": "1712.02050", "submitter": "Le Hui", "authors": "Le Hui and Xiang Li and Jiaxin Chen and Hongliang He and Chen gong and\n  Jian Yang", "title": "Unsupervised Multi-Domain Image Translation with Domain-Specific\n  Encoders/Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Image-to-Image Translation achieves spectacularly advanced\ndevelopments nowadays. However, recent approaches mainly focus on one model\nwith two domains, which may face heavy burdens with large cost of $O(n^2)$\ntraining time and model parameters, under such a requirement that $n$ domains\nare freely transferred to each other in a general setting. To address this\nproblem, we propose a novel and unified framework named Domain-Bank, which\nconsists of a global shared auto-encoder and $n$ domain-specific\nencoders/decoders, assuming that a universal shared-latent sapce can be\nprojected. Thus, we yield $O(n)$ complexity in model parameters along with a\nhuge reduction of the time budgets. Besides the high efficiency, we show the\ncomparable (or even better) image translation results over state-of-the-arts on\nvarious challenging unsupervised image translation tasks, including face image\ntranslation, fashion-clothes translation and painting style translation. We\nalso apply the proposed framework to domain adaptation and achieve\nstate-of-the-art performance on digit benchmark datasets. Further, thanks to\nthe explicit representation of the domain-specific decoders as well as the\nuniversal shared-latent space, it also enables us to conduct incremental\nlearning to add a new domain encoder/decoder. Linear combination of different\ndomains' representations is also obtained by fusing the corresponding decoders.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 06:08:31 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Hui", "Le", ""], ["Li", "Xiang", ""], ["Chen", "Jiaxin", ""], ["He", "Hongliang", ""], ["gong", "Chen", ""], ["Yang", "Jian", ""]]}, {"id": "1712.02051", "submitter": "Hongge Chen", "authors": "Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh", "title": "Attacking Visual Language Grounding with Adversarial Examples: A Case\n  Study on Neural Image Captioning", "comments": "Accepted by 56th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2018). Hongge Chen and Huan Zhang contribute equally to this\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual language grounding is widely studied in modern neural image captioning\nsystems, which typically adopts an encoder-decoder framework consisting of two\nprincipal components: a convolutional neural network (CNN) for image feature\nextraction and a recurrent neural network (RNN) for language caption\ngeneration. To study the robustness of language grounding to adversarial\nperturbations in machine vision and perception, we propose Show-and-Fool, a\nnovel algorithm for crafting adversarial examples in neural image captioning.\nThe proposed algorithm provides two evaluation approaches, which check whether\nneural image captioning systems can be mislead to output some randomly chosen\ncaptions or keywords. Our extensive experiments show that our algorithm can\nsuccessfully craft visually-similar adversarial examples with randomly targeted\ncaptions or keywords, and the adversarial examples can be made highly\ntransferable to other image captioning systems. Consequently, our approach\nleads to new robustness implications of neural image captioning and novel\ninsights in visual language grounding.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 06:08:59 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 01:56:51 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Chen", "Hongge", ""], ["Zhang", "Huan", ""], ["Chen", "Pin-Yu", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1712.02066", "submitter": "Varghese Alex Kollerathu Mr.", "authors": "Varghese Alex, Mohammed Safwan and Ganapathy Krishnamurthi", "title": "Automatic Segmentation and Overall Survival Prediction in Gliomas using\n  Fully Convolutional Neural Network and Texture Analysis", "comments": "10 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use a fully convolutional neural network (FCNN) for the\nsegmentation of gliomas from Magnetic Resonance Images (MRI). A fully\nautomatic, voxel based classification was achieved by training a 23 layer deep\nFCNN on 2-D slices extracted from patient volumes. The network was trained on\nslices extracted from 130 patients and validated on 50 patients. For the task\nof survival prediction, texture and shape based features were extracted from T1\npost contrast volume to train an XGBoost regressor. On BraTS 2017 validation\nset, the proposed scheme achieved a mean whole tumor, tumor core and active\ndice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the\noverall survival prediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 07:45:38 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Alex", "Varghese", ""], ["Safwan", "Mohammed", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1712.02099", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Orazio Gallo, Jinwei Gu, Jan Kautz", "title": "Separating Reflection and Transmission Images in the Wild", "comments": "accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reflections caused by common semi-reflectors, such as glass windows, can\nimpact the performance of computer vision algorithms. State-of-the-art methods\ncan remove reflections on synthetic data and in controlled scenarios. However,\nthey are based on strong assumptions and do not generalize well to real-world\nimages. Contrary to a common misconception, real-world images are challenging\neven when polarization information is used. We present a deep learning approach\nto separate the reflected and the transmitted components of the recorded\nirradiance, which explicitly uses the polarization properties of light. To\ntrain it, we introduce an accurate synthetic data generation pipeline, which\nsimulates realistic reflections, including those generated by curved and\nnon-ideal surfaces, non-static scenes, and high-dynamic-range scenes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 09:31:27 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 14:12:06 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Gallo", "Orazio", ""], ["Gu", "Jinwei", ""], ["Kautz", "Jan", ""]]}, {"id": "1712.02154", "submitter": "Sebastian Stabinger MSc", "authors": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Guided Labeling using Convolutional Neural Networks", "comments": "Under review for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last couple of years, deep learning and especially convolutional\nneural networks have become one of the work horses of computer vision. One\nlimiting factor for the applicability of supervised deep learning to more areas\nis the need for large, manually labeled datasets. In this paper we propose an\neasy to implement method we call guided labeling, which automatically\ndetermines which samples from an unlabeled dataset should be labeled. We show\nthat using this procedure, the amount of samples that need to be labeled is\nreduced considerably in comparison to labeling images arbitrarily.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 12:18:24 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "1712.02170", "submitter": "Yuliang Liu", "authors": "Liu Yuliang, Jin Lianwen, Zhang Shuaitao and Zhang Sheng", "title": "Detecting Curve Text in the Wild: New Dataset and New Solution", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection has been made great progress in recent years. The\ndetection manners are evolving from axis-aligned rectangle to rotated rectangle\nand further to quadrangle. However, current datasets contain very little curve\ntext, which can be widely observed in scene images such as signboard, product\nname and so on. To raise the concerns of reading curve text in the wild, in\nthis paper, we construct a curve text dataset named CTW1500, which includes\nover 10k text annotations in 1,500 images (1000 for training and 500 for\ntesting). Based on this dataset, we pioneering propose a polygon based curve\ntext detector (CTD) which can directly detect curve text without empirical\ncombination. Moreover, by seamlessly integrating the recurrent transverse and\nlongitudinal offset connection (TLOC), the proposed method can be end-to-end\ntrainable to learn the inherent connection among the position offsets. This\nallows the CTD to explore context information instead of predicting points\nindependently, resulting in more smooth and accurate detection. We also propose\ntwo simple but effective post-processing methods named non-polygon suppress\n(NPS) and polygonal non-maximum suppression (PNMS) to further improve the\ndetection accuracy. Furthermore, the proposed approach in this paper is\ndesigned in an universal manner, which can also be trained with rectangular or\nquadrilateral bounding boxes without extra efforts. Experimental results on\nCTW-1500 demonstrate our method with only a light backbone can outperform\nstate-of-the-art methods with a large margin. By evaluating only in the curve\nor non-curve subset, the CTD + TLOC can still achieve the best results. Code is\navailable at https://github.com/Yuliang-Liu/Curve-Text-Detector.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 13:02:43 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Yuliang", "Liu", ""], ["Lianwen", "Jin", ""], ["Shuaitao", "Zhang", ""], ["Sheng", "Zhang", ""]]}, {"id": "1712.02190", "submitter": "Agata Mosinska", "authors": "Agata Mosinska, Pablo Marquez-Neila, Mateusz Kozinski, Pascal Fua", "title": "Beyond the Pixel-Wise Loss for Topology-Aware Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineation of curvilinear structures is an important problem in Computer\nVision with multiple practical applications. With the advent of Deep Learning,\nmany current approaches on automatic delineation have focused on finding more\npowerful deep architectures, but have continued using the habitual pixel-wise\nlosses such as binary cross-entropy. In this paper we claim that pixel-wise\nlosses alone are unsuitable for this problem because of their inability to\nreflect the topological impact of mistakes in the final prediction. We propose\na new loss term that is aware of the higher-order topological features of\nlinear structures. We also introduce a refinement pipeline that iteratively\napplies the same model over the previous delineation to refine the predictions\nat each step while keeping the number of parameters and the complexity of the\nmodel constant.\n  When combined with the standard pixel-wise loss, both our new loss term and\nour iterative refinement boost the quality of the predicted delineations, in\nsome cases almost doubling the accuracy as compared to the same classifier\ntrained with the binary cross-entropy alone. We show that our approach\noutperforms state-of-the-art methods on a wide range of data, from microscopy\nto aerial images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 14:03:51 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Mosinska", "Agata", ""], ["Marquez-Neila", "Pablo", ""], ["Kozinski", "Mateusz", ""], ["Fua", "Pascal", ""]]}, {"id": "1712.02198", "submitter": "Masaharu Sakamoto", "authors": "Masaharu Sakamoto, Hiroki Nakano, Kun Zhao and Taro Sekiyama", "title": "Lung Nodule Classification by the Combination of Fusion Classifier and\n  Cascaded Convolutional Neural Networks", "comments": "Draft of ISBI2018. arXiv admin note: text overlap with\n  arXiv:1703.00311", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodule classification is a class imbalanced problem, as nodules are\nfound with much lower frequency than non-nodules. In the class imbalanced\nproblem, conventional classifiers tend to be overwhelmed by the majority class\nand ignore the minority class. We showed that cascaded convolutional neural\nnetworks can classify the nodule candidates precisely for a class imbalanced\nnodule candidate data set in our previous study. In this paper, we propose\nFusion classifier in conjunction with the cascaded convolutional neural network\nmodels. To fuse the models, nodule probabilities are calculated by using the\nconvolutional neural network models at first. Then, Fusion classifier is\ntrained and tested by the nodule probabilities. The proposed method achieved\nthe sensitivity of 94.4% and 95.9% at 4 and 8 false positives per scan in Free\nReceiver Operating Characteristics (FROC) curve analysis, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 06:22:20 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 01:37:37 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Sakamoto", "Masaharu", ""], ["Nakano", "Hiroki", ""], ["Zhao", "Kun", ""], ["Sekiyama", "Taro", ""]]}, {"id": "1712.02225", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu,\n  Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (re-id) faces two major challenges: the lack of\ncross-view paired training data and learning discriminative identity-sensitive\nand view-invariant features in the presence of large pose variations. In this\nwork, we address both problems by proposing a novel deep person image\ngeneration model for synthesizing realistic person images conditional on the\npose. The model is based on a generative adversarial network (GAN) designed\nspecifically for pose normalization in re-id, thus termed pose-normalization\nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep\nre-id feature free of the influence of pose variations. We show that this\nfeature is strong on its own and complementary to features learned with the\noriginal images. Importantly, under the transfer learning setting, we show that\nour model generalizes well to any new re-id dataset without the need for\ncollecting any training data for model fine-tuning. The model thus has the\npotential to make re-id model truly scalable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 15:18:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 04:55:01 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 00:28:00 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 06:59:45 GMT"}, {"version": "v5", "created": "Tue, 13 Feb 2018 06:22:12 GMT"}, {"version": "v6", "created": "Wed, 25 Apr 2018 05:57:05 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Qian", "Xuelin", ""], ["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Wang", "Wenxuan", ""], ["Qiu", "Jie", ""], ["Wu", "Yang", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1712.02249", "submitter": "Aritra Dutta", "authors": "Aritra Dutta and Peter Richtarik", "title": "Online and Batch Supervised Background Estimation via L1 Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a surprisingly simple model for supervised video background\nestimation. Our model is based on $\\ell_1$ regression. As existing methods for\n$\\ell_1$ regression do not scale to high-resolution videos, we propose several\nsimple and scalable methods for solving the problem, including iteratively\nreweighted least squares, a homotopy method, and stochastic gradient descent.\nWe show through extensive experiments that our model and methods match or\noutperform the state-of-the-art online and batch methods in virtually all\nquantitative and qualitative measures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:34:20 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Dutta", "Aritra", ""], ["Richtarik", "Peter", ""]]}, {"id": "1712.02286", "submitter": "Yunhan Zhao", "authors": "Yunhan Zhao, Haider Ali, Rene Vidal", "title": "Stretching Domain Adaptation: How far is too far?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has led to significant advances in visual recognition\nover the past few years, such advances often require a lot of annotated data.\nUnsupervised domain adaptation has emerged as an alternative approach that does\nnot require as much annotated data, prior evaluations of domain adaptation\napproaches have been limited to relatively similar datasets, e.g source and\ntarget domains are samples captured by different cameras. A new data suite is\nproposed that comprehensively evaluates cross-modality domain adaptation\nproblems. This work pushes the limit of unsupervised domain adaptation through\nan in-depth evaluation of several state of the art methods on benchmark\ndatasets and the new dataset suite. We also propose a new domain adaptation\nnetwork called \"Deep MagNet\" that effectively transfers knowledge for\ncross-modality domain adaptation problems. Deep Magnet achieves state of the\nart performance on two benchmark datasets. More importantly, the proposed\nmethod shows consistent improvements in performance on the newly proposed\ndataset suite.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 17:03:07 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 20:53:17 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhao", "Yunhan", ""], ["Ali", "Haider", ""], ["Vidal", "Rene", ""]]}, {"id": "1712.02294", "submitter": "Ali Harakeh", "authors": "Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, Steven\n  Waslander", "title": "Joint 3D Proposal Generation and Object Detection from View Aggregation", "comments": "For any inquiries contact aharakeh(at)uwaterloo(dot)ca", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present AVOD, an Aggregate View Object Detection network for autonomous\ndriving scenarios. The proposed neural network architecture uses LIDAR point\nclouds and RGB images to generate features that are shared by two subnetworks:\na region proposal network (RPN) and a second stage detector network. The\nproposed RPN uses a novel architecture capable of performing multimodal feature\nfusion on high resolution feature maps to generate reliable 3D object proposals\nfor multiple object classes in road scenes. Using these proposals, the second\nstage detection network performs accurate oriented 3D bounding box regression\nand category classification to predict the extents, orientation, and\nclassification of objects in 3D space. Our proposed architecture is shown to\nproduce state of the art results on the KITTI 3D object detection benchmark\nwhile running in real time with a low memory footprint, making it a suitable\ncandidate for deployment on autonomous vehicles. Code is at:\nhttps://github.com/kujason/avod\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 17:20:21 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 16:34:19 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 16:50:01 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 14:11:40 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ku", "Jason", ""], ["Mozifian", "Melissa", ""], ["Lee", "Jungwook", ""], ["Harakeh", "Ali", ""], ["Waslander", "Steven", ""]]}, {"id": "1712.02310", "submitter": "David Fouhey", "authors": "David F. Fouhey and Wei-cheng Kuo and Alexei A. Efros and Jitendra\n  Malik", "title": "From Lifestyle Vlogs to Everyday Interactions", "comments": "Project page at: http://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major stumbling block to progress in understanding basic human\ninteractions, such as getting out of bed or opening a refrigerator, is lack of\ngood training data. Most past efforts have gathered this data explicitly:\nstarting with a laundry list of action labels, and then querying search engines\nfor videos tagged with each label. In this work, we do the reverse and search\nimplicitly: we start with a large collection of interaction-rich video data and\nthen annotate and analyze it. We use Internet Lifestyle Vlogs as the source of\nsurprisingly large and diverse interaction data. We show that by collecting the\ndata first, we are able to achieve greater scale and far greater diversity in\nterms of actions and actors. Additionally, our data exposes biases built into\ncommon explicitly gathered data. We make sense of our data by analyzing the\ncentral component of interaction -- hands. We benchmark two tasks: identifying\nsemantic object contact at the video level and non-semantic contact state at\nthe frame level. We additionally demonstrate future prediction of hands.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:07:57 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Fouhey", "David F.", ""], ["Kuo", "Wei-cheng", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1712.02327", "submitter": "Ben Mildenhall", "authors": "Ben Mildenhall, Jonathan T. Barron, Jiawen Chen, Dillon Sharlet, Ren\n  Ng, Robert Carroll", "title": "Burst Denoising with Kernel Prediction Networks", "comments": "To appear in CVPR 2018 (spotlight). Project page:\n  http://people.eecs.berkeley.edu/~bmild/kpn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for jointly denoising bursts of images taken from a\nhandheld camera. In particular, we propose a convolutional neural network\narchitecture for predicting spatially varying kernels that can both align and\ndenoise frames, a synthetic data generation approach based on a realistic noise\nformation model, and an optimization guided by an annealed loss function to\navoid undesirable local minima. Our model matches or outperforms the\nstate-of-the-art across a wide range of noise levels on both real and synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:50:28 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:56:32 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Mildenhall", "Ben", ""], ["Barron", "Jonathan T.", ""], ["Chen", "Jiawen", ""], ["Sharlet", "Dillon", ""], ["Ng", "Ren", ""], ["Carroll", "Robert", ""]]}, {"id": "1712.02328", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie", "title": "Generative Adversarial Perturbations", "comments": "CVPR 2018, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:52:12 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 01:18:08 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 06:50:03 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Poursaeed", "Omid", ""], ["Katsman", "Isay", ""], ["Gao", "Bicheng", ""], ["Belongie", "Serge", ""]]}, {"id": "1712.02400", "submitter": "Zhiwei Jia", "authors": "Zhiwei Jia, Haoshen Hong, Siyang Wang, Kwonjoon Lee, Zhuowen Tu", "title": "Controllable Top-down Feature Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the intrinsic transformation of feature maps across convolutional\nnetwork layers with explicit top-down control. To this end, we develop top-down\nfeature transformer (TFT), under controllable parameters, that are able to\naccount for the hidden layer transformation while maintaining the overall\nconsistency across layers. The learned generators capture the underlying\nfeature transformation processes that are independent of particular training\nimages. Our proposed TFT framework brings insights to and helps the\nunderstanding of, an important problem of studying the CNN internal feature\nrepresentation and transformation under the top-down processes. In the case of\nspatial transformations, we demonstrate the significant advantage of TFT over\nexisting data-driven approaches in building data-independent transformations.\nWe also show that it can be adopted in other applications such as data\naugmentation and image style transfer.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 20:20:27 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 03:12:33 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 06:41:54 GMT"}, {"version": "v4", "created": "Sun, 4 Nov 2018 01:31:30 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jia", "Zhiwei", ""], ["Hong", "Haoshen", ""], ["Wang", "Siyang", ""], ["Lee", "Kwonjoon", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1712.02408", "submitter": "Hongyu Xu", "authors": "Hongyu Xu, Xutao Lv, Xiaoyu Wang, Zhou Ren, Navaneeth Bodla and Rama\n  Chellappa", "title": "Deep Regionlets for Object Detection", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel object detection framework named \"Deep\nRegionlets\" by establishing a bridge between deep neural networks and\nconventional detection schema for accurate generic object detection. Motivated\nby the abilities of regionlets for modeling object deformation and multiple\naspect ratios, we incorporate regionlets into an end-to-end trainable deep\nlearning framework. The deep regionlets framework consists of a region\nselection network and a deep regionlet learning module. Specifically, given a\ndetection bounding box proposal, the region selection network provides guidance\non where to select regions to learn the features from. The regionlet learning\nmodule focuses on local feature selection and transformation to alleviate local\nvariations. To this end, we first realize non-rectangular region selection\nwithin the detection framework to accommodate variations in object appearance.\nMoreover, we design a \"gating network\" within the regionlet leaning module to\nenable soft regionlet selection and pooling. The Deep Regionlets framework is\ntrained end-to-end without additional efforts. We perform ablation studies and\nconduct extensive experiments on the PASCAL VOC and Microsoft COCO datasets.\nThe proposed framework outperforms state-of-the-art algorithms, such as\nRetinaNet and Mask R-CNN, even without additional segmentation labels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 21:05:21 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 21:15:25 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 01:35:53 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Xu", "Hongyu", ""], ["Lv", "Xutao", ""], ["Wang", "Xiaoyu", ""], ["Ren", "Zhou", ""], ["Bodla", "Navaneeth", ""], ["Chellappa", "Rama", ""]]}, {"id": "1712.02423", "submitter": "Ritwick Chaudhry", "authors": "Preeti Gopal, Ritwick Chaudhry, Sharat Chandran, Imants Svalbe, Ajit\n  Rajwade", "title": "Tomographic Reconstruction using Global Statistical Prior", "comments": "Published in The International Conference on Digital Image Computing:\n  Techniques and Applications (DICTA), Sydney, Australia, 2017. The conference\n  proceedings are not out yet. But the result can be seen here:\n  http://dicta2017.dictaconference.org/fullprogram.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in tomographic reconstruction is motivated by the need to\nefficiently recover detailed anatomy from limited measurements. One of the ways\nto compensate for the increasingly sparse sets of measurements is to exploit\nthe information from templates, i.e., prior data available in the form of\nalready reconstructed, structurally similar images. Towards this, previous work\nhas exploited using a set of global and patch based dictionary priors. In this\npaper, we propose a global prior to improve both the speed and quality of\ntomographic reconstruction within a Compressive Sensing framework.\n  We choose a set of potential representative 2D images referred to as\ntemplates, to build an eigenspace; this is subsequently used to guide the\niterative reconstruction of a similar slice from sparse acquisition data. Our\nexperiments across a diverse range of datasets show that reconstruction using\nan appropriate global prior, apart from being faster, gives a much lower\nreconstruction error when compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 22:15:48 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Gopal", "Preeti", ""], ["Chaudhry", "Ritwick", ""], ["Chandran", "Sharat", ""], ["Svalbe", "Imants", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1712.02463", "submitter": "Dogancan Temel", "authors": "Dogancan Temel, Gukyeong Kwon, Mohit Prabhushankar, Ghassan AlRegib", "title": "CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign\n  Recognition", "comments": "31st Conference on Neural Information Processing Systems (NIPS),\n  Machine Learning for Intelligent Transportation Systems Workshop, Long Beach,\n  CA, USA, 2017", "journal-ref": "D. Temel, G. Kwon*, M. Prabhushankar*, and G. AlRegib, \"CURE-TSR:\n  Challenging unreal and real environments for traffic sign recognition,\"\n  Neural Information Processing Systems (NIPS) MLITSW, December 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the robustness of traffic sign recognition\nalgorithms under challenging conditions. Existing datasets are limited in terms\nof their size and challenging condition coverage, which motivated us to\ngenerate the Challenging Unreal and Real Environments for Traffic Sign\nRecognition (CURE-TSR) dataset. It includes more than two million traffic sign\nimages that are based on real-world and simulator data. We benchmark the\nperformance of existing solutions in real-world scenarios and analyze the\nperformance variation with respect to challenging conditions. We show that\nchallenging conditions can decrease the performance of baseline methods\nsignificantly, especially if these challenging conditions result in loss or\nmisplacement of spatial information. We also investigate the effect of data\naugmentation and show that utilization of simulator data along with real-world\ndata enhance the average recognition performance in real-world scenarios. The\ndataset is publicly available at https://ghassanalregib.com/cure-tsr/.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 01:09:23 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 16:33:52 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "Dogancan", ""], ["Kwon", "Gukyeong", ""], ["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1712.02478", "submitter": "Le Hui", "authors": "Jifeng Wang and Xiang Li and Le Hui and Jian Yang", "title": "Stacked Conditional Generative Adversarial Networks for Jointly Learning\n  Shadow Detection and Shadow Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding shadows from a single image spontaneously derives into two\ntypes of task in previous studies, containing shadow detection and shadow\nremoval. In this paper, we present a multi-task perspective, which is not\nembraced by any existing work, to jointly learn both detection and removal in\nan end-to-end fashion that aims at enjoying the mutually improved benefits from\neach other. Our framework is based on a novel STacked Conditional Generative\nAdversarial Network (ST-CGAN), which is composed of two stacked CGANs, each\nwith a generator and a discriminator. Specifically, a shadow image is fed into\nthe first generator which produces a shadow detection mask. That shadow image,\nconcatenated with its predicted mask, goes through the second generator in\norder to recover its shadow-free image consequently. In addition, the two\ncorresponding discriminators are very likely to model higher level\nrelationships and global scene characteristics for the detected shadow region\nand reconstruction via removing shadows, respectively. More importantly, for\nmulti-task learning, our design of stacked paradigm provides a novel view which\nis notably different from the commonly used one as the multi-branch version. To\nfully evaluate the performance of our proposed framework, we construct the\nfirst large-scale benchmark with 1870 image triplets (shadow image, shadow mask\nimage, and shadow-free image) under 135 scenes. Extensive experimental results\nconsistently show the advantages of ST-CGAN over several representative\nstate-of-the-art methods on two large-scale publicly available datasets and our\nnewly released one.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 02:57:38 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Jifeng", ""], ["Li", "Xiang", ""], ["Hui", "Le", ""], ["Yang", "Jian", ""]]}, {"id": "1712.02494", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Hussein Sibai, Evan Fabry", "title": "Adversarial Examples that Fool Detectors", "comments": "Follow up paper for adversarial stop signs. Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversarial example is an example that has been adjusted to produce a\nwrong label when presented to a system at test time. To date, adversarial\nexample constructions have been demonstrated for classifiers, but not for\ndetectors. If adversarial examples that could fool a detector exist, they could\nbe used to (for example) maliciously create security hazards on roads populated\nwith smart vehicles. In this paper, we demonstrate a construction that\nsuccessfully fools two standard detectors, Faster RCNN and YOLO. The existence\nof such examples is surprising, as attacking a classifier is very different\nfrom attacking a detector, and that the structure of detectors - which must\nsearch for their own bounding box, and which cannot estimate that box very\naccurately - makes it quite likely that adversarial patterns are strongly\ndisrupted. We show that our construction produces adversarial examples that\ngeneralize well across sequences digitally, even though large perturbations are\nneeded. We also show that our construction yields physical objects that are\nadversarial.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 05:13:54 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Lu", "Jiajun", ""], ["Sibai", "Hussein", ""], ["Fabry", "Evan", ""]]}, {"id": "1712.02501", "submitter": "Chen Huang", "authors": "Chen Huang, Chen Kong, and Simon Lucey", "title": "CNNs are Globally Optimal Given Multi-Layer Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is the central workhorse for training\nmodern CNNs. Although giving impressive empirical performance it can be slow to\nconverge. In this paper we explore a novel strategy for training a CNN using an\nalternation strategy that offers substantial speedups during training. We make\nthe following contributions: (i) replace the ReLU non-linearity within a CNN\nwith positive hard-thresholding, (ii) reinterpret this non-linearity as a\nbinary state vector making the entire CNN linear if the multi-layer support is\nknown, and (iii) demonstrate that under certain conditions a global optima to\nthe CNN can be found through local descent. We then employ a novel alternation\nstrategy (between weights and support) for CNN training that leads to\nsubstantially faster convergence rates, nice theoretical properties, and\nachieving state of the art results across large scale datasets (e.g. ImageNet)\nas well as other standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 06:06:52 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 14:21:43 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Huang", "Chen", ""], ["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1712.02502", "submitter": "Chen Kong", "authors": "Chen Kong and Simon Lucey", "title": "Take it in your stride: Do we need striding in CNNs?", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their inception, CNNs have utilized some type of striding operator to\nreduce the overlap of receptive fields and spatial dimensions. Although having\nclear heuristic motivations (i.e. lowering the number of parameters to learn)\nthe mathematical role of striding within CNN learning remains unclear. This\npaper offers a novel and mathematical rigorous perspective on the role of the\nstriding operator within modern CNNs. Specifically, we demonstrate\ntheoretically that one can always represent a CNN that incorporates striding\nwith an equivalent non-striding CNN which has more filters and smaller size.\nThrough this equivalence we are then able to characterize striding as an\nadditional mechanism for parameter sharing among channels, thus reducing\ntraining complexity. Finally, the framework presented in this paper offers a\nnew mathematical perspective on the role of striding which we hope shall\nfacilitate and simplify the future theoretical analysis of CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 06:07:22 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1712.02514", "submitter": "Arnold Wiliem", "authors": "Teng Zhang, Arnold Wiliem, Siqi Yang, Brian C. Lovell", "title": "TV-GAN: Generative Adversarial Network Based Thermal to Visible Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the face recognition task on images captured using thermal\ncamera sensors which can operate in the non-light environment. While it can\ngreatly increase the scope and benefits of the current security surveillance\nsystems, performing such a task using thermal images is a challenging problem\ncompared to face recognition task in the Visible Light Domain (VLD). This is\npartly due to the much smaller amount number of thermal imagery data collected\ncompared to the VLD data. Unfortunately, direct application of the existing\nvery strong face recognition models trained using VLD data into the thermal\nimagery data will not produce a satisfactory performance. This is due to the\nexistence of the domain gap between the thermal and VLD images. To this end, we\npropose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is\nable to transform thermal face images into their corresponding VLD images\nwhilst maintaining identity information which is sufficient enough for the\nexisting VLD face recognition models to perform recognition. Some examples are\npresented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an\nexplicit closed-set face recognition loss to regularize the discriminator\nnetwork training. This information will then be conveyed into the generator\nnetwork in the forms of gradient loss. In the experiment, we show that by using\nthis additional explicit regularization for the discriminator network, the\nTV-GAN is able to preserve more identity information when translating a thermal\nimage of a person which is not seen before by the TV-GAN.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 07:06:39 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Zhang", "Teng", ""], ["Wiliem", "Arnold", ""], ["Yang", "Siqi", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1712.02517", "submitter": "Simyung Chang", "authors": "Simyung Chang, John Yang, Seonguk Park, Nojun Kwak", "title": "Broadcasting Convolutional Network for Visual Relational Reasoning", "comments": "Accepted paper at ECCV 2018. 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Broadcasting Convolutional Network (BCN) that\nextracts key object features from the global field of an entire input image and\nrecognizes their relationship with local features. BCN is a simple network\nmodule that collects effective spatial features, embeds location information\nand broadcasts them to the entire feature maps. We further introduce the\nMulti-Relational Network (multiRN) that improves the existing Relation Network\n(RN) by utilizing the BCN module. In pixel-based relation reasoning problems,\nwith the help of BCN, multiRN extends the concept of `pairwise relations' in\nconventional RNs to `multiwise relations' by relating each object with multiple\nobjects at once. This yields in O(n) complexity for n objects, which is a vast\ncomputational gain from RNs that take O(n^2). Through experiments, multiRN has\nachieved a state-of-the-art performance on CLEVR dataset, which proves the\nusability of BCN on relation reasoning problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 07:21:04 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 07:36:48 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 08:31:11 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Chang", "Simyung", ""], ["Yang", "John", ""], ["Park", "Seonguk", ""], ["Kwak", "Nojun", ""]]}, {"id": "1712.02527", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Jingwei Zhuo, Jun Zhu", "title": "Learning Random Fourier Features by Hybrid Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel embedding algorithm is an important component for adapting kernel\nmethods to large datasets. Since the algorithm consumes a major computation\ncost in the testing phase, we propose a novel teacher-learner framework of\nlearning computation-efficient kernel embeddings from specific data. In the\nframework, the high-precision embeddings (teacher) transfer the data\ninformation to the computation-efficient kernel embeddings (learner). We\njointly select informative embedding functions and pursue an orthogonal\ntransformation between two embeddings. We propose a novel approach of\nconstrained variational expectation maximization (CVEM), where the alternate\ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the\nmaximization step. We also propose two specific formulations based on the\nprevalent Random Fourier Feature (RFF), the masked and blocked version of\nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block\nstructure on the transformation matrix. By empirical studies of several\napplications on different real-world datasets, we demonstrate that the CERF\nsignificantly improves the performance of kernel methods upon the RFF, under\ncertain arithmetic operation requirements, and suitable for structured matrix\nmultiplication in Fastfood type algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 08:07:26 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Zhuo", "Jingwei", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.02560", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku and Tatsuya Harada", "title": "Maximum Classifier Discrepancy for Unsupervised Domain Adaptation", "comments": "Accepted to CVPR2018 Oral, Code is available at\n  https://github.com/mil-tokyo/MCD_DA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a method for unsupervised domain adaptation. Many\nadversarial learning methods train domain classifier networks to distinguish\nthe features as either a source or target and train a feature generator network\nto mimic the discriminator. Two problems exist with these methods. First, the\ndomain classifier only tries to distinguish the features as a source or target\nand thus does not consider task-specific decision boundaries between classes.\nTherefore, a trained generator can generate ambiguous features near class\nboundaries. Second, these methods aim to completely match the feature\ndistributions between different domains, which is difficult because of each\ndomain's characteristics.\n  To solve these problems, we introduce a new approach that attempts to align\ndistributions of source and target by utilizing the task-specific decision\nboundaries. We propose to maximize the discrepancy between two classifiers'\noutputs to detect target samples that are far from the support of the source. A\nfeature generator learns to generate target features near the support to\nminimize the discrepancy. Our method outperforms other methods on several\ndatasets of image classification and semantic segmentation. The codes are\navailable at \\url{https://github.com/mil-tokyo/MCD_DA}\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 10:49:33 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 00:38:53 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 10:51:11 GMT"}, {"version": "v4", "created": "Tue, 3 Apr 2018 08:44:29 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Saito", "Kuniaki", ""], ["Watanabe", "Kohei", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1712.02575", "submitter": "Han-Mu Park", "authors": "Han-Mu Park and Kuk-Jin Yoon", "title": "Consistent Multiple Graph Matching with Multi-layer Random Walks\n  Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the correspondence search problem among multiple graphs with\ncomplex properties while considering the matching consistency. We describe each\npair of graphs by combining multiple attributes, then jointly match them in a\nunified framework. The main contribution of this paper is twofold. First, we\nformulate the global correspondence search problem of multi-attributed graphs\nby utilizing a set of multi-layer structures. The proposed formulation\ndescribes each pair of graphs as a multi-layer structure, and jointly considers\nwhole matching pairs. Second, we propose a robust multiple graph matching\nmethod based on the multi-layer random walks framework. The proposed framework\nsynchronizes movements of random walkers, and leads them to consistent matching\ncandidates. In our extensive experiments, the proposed method exhibits robust\nand accurate performance over the state-of-the-art multiple graph matching\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 11:59:54 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 02:24:48 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Park", "Han-Mu", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1712.02609", "submitter": "Carles Riera Molina", "authors": "Carles Roger Riera Molina and Oriol Pujol Vila", "title": "Solving internal covariate shift in deep learning with linked neurons", "comments": "Submitted to CVPR 2018. Code available at\n  https://github.com/blauigris/linked_neurons", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel solution to the problem of internal covariate\nshift and dying neurons using the concept of linked neurons. We define the\nneuron linkage in terms of two constraints: first, all neuron activations in\nthe linkage must have the same operating point. That is to say, all of them\nshare input weights. Secondly, a set of neurons is linked if and only if there\nis at least one member of the linkage that has a non-zero gradient in regard to\nthe input of the activation function. This means that for any input in the\nactivation function, there is at least one member of the linkage that operates\nin a non-flat and non-zero area. This simple change has profound implications\nin the network learning dynamics. In this article we explore the consequences\nof this proposal and show that by using this kind of units, internal covariate\nshift is implicitly solved. As a result of this, the use of linked neurons\nallows to train arbitrarily large networks without any architectural or\nalgorithmic trick, effectively removing the need of using re-normalization\nschemes such as Batch Normalization, which leads to halving the required\ntraining time. It also solves the problem of the need for standarized input\ndata. Results show that the units using the linkage not only do effectively\nsolve the aforementioned problems, but are also a competitive alternative with\nrespect to state-of-the-art with very promising results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 13:26:26 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Molina", "Carles Roger Riera", ""], ["Vila", "Oriol Pujol", ""]]}, {"id": "1712.02616", "submitter": "Samuel Rota Bul\\`o", "authors": "Samuel Rota Bul\\`o, Lorenzo Porzi, Peter Kontschieder", "title": "In-Place Activated BatchNorm for Memory-Optimized Training of DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present In-Place Activated Batch Normalization (InPlace-ABN)\n- a novel approach to drastically reduce the training memory footprint of\nmodern deep neural networks in a computationally efficient way. Our solution\nsubstitutes the conventionally used succession of BatchNorm + Activation layers\nwith a single plugin layer, hence avoiding invasive framework surgery while\nproviding straightforward applicability for existing deep learning frameworks.\nWe obtain memory savings of up to 50% by dropping intermediate results and by\nrecovering required information during the backward pass through the inversion\nof stored forward results, with only minor increase (0.8-2%) in computation\ntime. Also, we demonstrate how frequently used checkpointing approaches can be\nmade computationally as efficient as InPlace-ABN. In our experiments on image\nclassification, we demonstrate on-par results on ImageNet-1k with\nstate-of-the-art approaches. On the memory-demanding task of semantic\nsegmentation, we report results for COCO-Stuff, Cityscapes and Mapillary\nVistas, obtaining new state-of-the-art results on the latter without additional\ntraining data but in a single-scale and -model scenario. Code can be found at\nhttps://github.com/mapillary/inplace_abn .\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 13:43:45 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 15:51:04 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 07:46:48 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Bul\u00f2", "Samuel Rota", ""], ["Porzi", "Lorenzo", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1712.02621", "submitter": "Qianru Sun", "authors": "Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt\n  Schiele, Mario Fritz", "title": "Disentangled Person Image Generation", "comments": "Published at CVPR'18 (Spotlight). Corresponding author is Qianru Sun", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating novel, yet realistic, images of persons is a challenging task due\nto the complex interplay between the different image factors, such as the\nforeground, background and pose information. In this work, we aim at generating\nsuch images based on a novel, two-stage reconstruction pipeline that learns a\ndisentangled representation of the aforementioned image factors and generates\nnovel person images at the same time. First, a multi-branched reconstruction\nnetwork is proposed to disentangle and encode the three factors into embedding\nfeatures, which are then combined to re-compose the input image itself. Second,\nthree corresponding mapping functions are learned in an adversarial manner in\norder to map Gaussian noise to the learned embedding feature space, for each\nfactor respectively. Using the proposed framework, we can manipulate the\nforeground, background and pose of the input image, and also sample new\nembedding features to generate such targeted manipulations, that provide more\ncontrol over the generation process. Experiments on Market-1501 and Deepfashion\ndatasets show that our model does not only generate realistic person images\nwith new foregrounds, backgrounds and poses, but also manipulates the generated\nfactors and interpolates the in-between states. Another set of experiments on\nMarket-1501 shows that our model can also be beneficial for the person\nre-identification task.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 14:03:12 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 20:27:49 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 12:34:17 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 04:48:49 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Ma", "Liqian", ""], ["Sun", "Qianru", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1712.02658", "submitter": "Ga\\\"elle Bonnet Loosli", "authors": "Ga\\\"elle Loosli and Hattoibe Aboubacar", "title": "Using SVDD in SimpleMKL for 3D-Shapes Filtering", "comments": "9 pages, 6 figures, conference : https://cap2014.sciencesconf.org/", "journal-ref": "CAp, conference d'apprentissage, July 2014, Saint-Etienne, France,\n  pp.84-92", "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the adaptation of Support Vector Data Description (SVDD)\nto the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a\nvariant called Slim-MK-SVDD that is able to produce a tighter frontier around\nthe data. For the sake of comparison, the equivalent methods are also developed\nfor One-Class SVM, known to be very similar to SVDD for certain shapes of\nkernels.\n  Those algorithms are illustrated in the context of 3D-shapes filtering and\noutliers detection. For the 3D-shapes problem, the objective is to be able to\nselect a sub-category of 3D-shapes, each sub-category being learned with our\nalgorithm in order to create a filter. For outliers detection, we apply the\nproposed algorithms for unsupervised outliers detection as well as for the\nsupervised case.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 14:56:53 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Loosli", "Ga\u00eblle", ""], ["Aboubacar", "Hattoibe", ""]]}, {"id": "1712.02662", "submitter": "Wei-Lin Hsiao", "authors": "Wei-Lin Hsiao, Kristen Grauman", "title": "Creating Capsule Wardrobes from Fashion Images", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to automatically create capsule wardrobes. Given an inventory of\ncandidate garments and accessories, the algorithm must assemble a minimal set\nof items that provides maximal mix-and-match outfits. We pose the task as a\nsubset selection problem. To permit efficient subset selection over the space\nof all outfit combinations, we develop submodular objective functions capturing\nthe key ingredients of visual compatibility, versatility, and user-specific\npreference. Since adding garments to a capsule only expands its possible\noutfits, we devise an iterative approach to allow near-optimal submodular\nfunction maximization. Finally, we present an unsupervised approach to learn\nvisual compatibility from \"in the wild\" full body outfit photos; the\ncompatibility metric translates well to cleaner catalog photos and improves\nover existing methods. Our results on thousands of pieces from popular fashion\nwebsites show that automatic capsule creation has potential to mimic skilled\nfashionistas in assembling flexible wardrobes, while being significantly more\nscalable.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:06:26 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 17:02:40 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Hsiao", "Wei-Lin", ""], ["Grauman", "Kristen", ""]]}, {"id": "1712.02719", "submitter": "Syed Shakib Sarwar", "authors": "Syed Shakib Sarwar, Aayush Ankit and Kaushik Roy", "title": "Incremental Learning in Deep Convolutional Neural Networks Using Partial\n  Network Sharing", "comments": "18 pages, 13 figures. IEEE Access 2019", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2963056", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (DCNN) based supervised learning is a\nwidely practiced approach for large-scale image classification. However,\nretraining these large networks to accommodate new, previously unseen data\ndemands high computational time and energy requirements. Also, previously seen\ntraining samples may not be available at the time of retraining. We propose an\nefficient training methodology and incrementally growing DCNN to learn new\ntasks while sharing part of the base network. Our proposed methodology is\ninspired by transfer learning techniques, although it does not forget\npreviously learned tasks. An updated network for learning new set of classes is\nformed using previously learned convolutional layers (shared from initial part\nof base network) with addition of few newly added convolutional kernels\nincluded in the later layers of the network. We employed a `clone-and-branch'\ntechnique which allows the network to learn new tasks one after another without\nany performance loss in old tasks. We evaluated the proposed scheme on several\nrecognition applications. The classification accuracy achieved by our approach\nis comparable to the regular incremental learning approach (where networks are\nupdated with new training samples only, without any network sharing), while\nachieving energy efficiency, reduction in storage requirements, memory access\nand training time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:05:54 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 18:10:52 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 07:22:16 GMT"}, {"version": "v4", "created": "Thu, 2 May 2019 06:20:19 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Sarwar", "Syed Shakib", ""], ["Ankit", "Aayush", ""], ["Roy", "Kaushik", ""]]}, {"id": "1712.02734", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas", "title": "Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\n  Transferable Chemical Property Prediction", "comments": "Submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With access to large datasets, deep neural networks (DNN) have achieved\nhuman-level accuracy in image and speech recognition tasks. However, in\nchemistry, data is inherently small and fragmented. In this work, we develop an\napproach of using rule-based knowledge for training ChemNet, a transferable and\ngeneralizable deep neural network for chemical property prediction that learns\nin a weak-supervised manner from large unlabeled chemical databases. When\ncoupled with transfer learning approaches to predict other smaller datasets for\nchemical properties that it was not originally trained on, we show that\nChemNet's accuracy outperforms contemporary DNN models that were trained using\nconventional supervised learning. Furthermore, we demonstrate that the ChemNet\npre-training approach is equally effective on both CNN (Chemception) and RNN\n(SMILES2vec) models, indicating that this approach is network architecture\nagnostic and is effective across multiple data modalities. Our results indicate\na pre-trained ChemNet that incorporates chemistry domain knowledge, enables the\ndevelopment of generalizable neural networks for more accurate prediction of\nnovel chemical properties.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:25:48 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:50:02 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "1712.02743", "submitter": "Thomas Hehn", "authors": "Thomas Hehn and Fred A. Hamprecht", "title": "End-to-end Learning of Deterministic Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional decision trees have a number of favorable properties, including\ninterpretability, a small computational footprint and the ability to learn from\nlittle training data. However, they lack a key quality that has helped fuel the\ndeep learning revolution: that of being end-to-end trainable, and to learn from\nscratch those features that best allow to solve a given supervised learning\nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the\ncost of losing a main attractive trait of decision trees: the fact that each\nsample is routed along a small subset of tree nodes only. We here propose a\nmodel and Expectation-Maximization training scheme for decision trees that are\nfully probabilistic at train time, but after a deterministic annealing process\nbecome deterministic at test time. We also analyze the learned oblique split\nparameters on image datasets and show that Neural Networks can be trained at\neach split node. In summary, we present the first end-to-end learning scheme\nfor deterministic decision trees and present results on par with or superior to\npublished standard oblique decision tree algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:40:25 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Hehn", "Thomas", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1712.02754", "submitter": "Adrian Galdran", "authors": "Adrian Galdran and Aitor Alvarez-Gila and Alessandro Bria and Javier\n  Vazquez-Corral and Marcelo Bertalmio", "title": "On the Duality Between Retinex and Image Dehazing", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing deals with the removal of undesired loss of visibility in\noutdoor images due to the presence of fog. Retinex is a color vision model\nmimicking the ability of the Human Visual System to robustly discount varying\nilluminations when observing a scene under different spectral lighting\nconditions. Retinex has been widely explored in the computer vision literature\nfor image enhancement and other related tasks. While these two problems are\napparently unrelated, the goal of this work is to show that they can be\nconnected by a simple linear relationship. Specifically, most Retinex-based\nalgorithms have the characteristic feature of always increasing image\nbrightness, which turns them into ideal candidates for effective image dehazing\nby directly applying Retinex to a hazy image whose intensities have been\ninverted. In this paper, we give theoretical proof that Retinex on inverted\nintensities is a solution to the image dehazing problem. Comprehensive\nqualitative and quantitative results indicate that several classical and modern\nimplementations of Retinex can be transformed into competing image dehazing\nalgorithms performing on pair with more complex fog removal methods, and can\novercome some of the main challenges associated with this problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 18:03:19 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 16:48:15 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Galdran", "Adrian", ""], ["Alvarez-Gila", "Aitor", ""], ["Bria", "Alessandro", ""], ["Vazquez-Corral", "Javier", ""], ["Bertalmio", "Marcelo", ""]]}, {"id": "1712.02765", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Super-FAN: Integrated facial landmark localization and super-resolution\n  of real-world low resolution faces in arbitrary poses with GANs", "comments": "CVPR 2018 SPOTLIGHT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses 2 challenging tasks: improving the quality of low\nresolution facial images and accurately locating the facial landmarks on such\npoor resolution images. To this end, we make the following 5 contributions: (a)\nwe propose Super-FAN: the very first end-to-end system that addresses both\ntasks simultaneously, i.e. both improves face resolution and detects the facial\nlandmarks. The novelty or Super-FAN lies in incorporating structural\ninformation in a GAN-based super-resolution algorithm via integrating a\nsub-network for face alignment through heatmap regression and optimizing a\nnovel heatmap loss. (b) We illustrate the benefit of training the two networks\njointly by reporting good results not only on frontal images (as in prior work)\nbut on the whole spectrum of facial poses, and not only on synthetic low\nresolution images (as in prior work) but also on real-world images. (c) We\nimprove upon the state-of-the-art in face super-resolution by proposing a new\nresidual-based architecture. (d) Quantitatively, we show large improvement over\nthe state-of-the-art for both face super-resolution and alignment. (e)\nQualitatively, we show for the first time good results on real-world low\nresolution images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 18:24:48 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:38:27 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1712.02779", "submitter": "Dimitris Tsipras", "authors": "Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt,\n  Aleksander Madry", "title": "Exploring the Landscape of Spatial Robustness", "comments": "ICML 2019. Presented in NIPS 2017 Workshop on Machine Learning and\n  Computer Security as \"A Rotation and a Translation Suffice: Fooling CNNs with\n  Simple Transformations.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 18:53:52 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 12:00:50 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 18:33:22 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 04:38:13 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Engstrom", "Logan", ""], ["Tran", "Brandon", ""], ["Tsipras", "Dimitris", ""], ["Schmidt", "Ludwig", ""], ["Madry", "Aleksander", ""]]}, {"id": "1712.02781", "submitter": "Kian Ahrabian", "authors": "Kian Ahrabian, Bagher Babaali", "title": "On Usage of Autoencoders and Siamese Networks for Online Handwritten\n  Signature Verification", "comments": "13 pages, 10 figures, Submitted to Neural Computing and Applications\n  journal", "journal-ref": null, "doi": "10.1007/s00521-018-3844-z", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel writer-independent global feature\nextraction framework for the task of automatic signature verification which\naims to make robust systems for automatically distinguishing negative and\npositive samples. Our method consists of an autoencoder for modeling the sample\nspace into a fixed length latent space and a Siamese Network for classifying\nthe fixed-length samples obtained from the autoencoder based on the reference\nsamples of a subject as being \"Genuine\" or \"Forged.\" During our experiments,\nusage of Attention Mechanism and applying Downsampling significantly improved\nthe accuracy of the proposed framework. We evaluated our proposed framework\nusing SigWiComp2013 Japanese and GPDSsyntheticOnLineOffLineSignature datasets.\nOn the SigWiComp2013 Japanese dataset, we achieved 8.65% EER that means 1.2%\nrelative improvement compared to the best-reported result. Furthermore, on the\nGPDSsyntheticOnLineOffLineSignature dataset, we achieved average EERs of 0.13%,\n0.12%, 0.21% and 0.25% respectively for 150, 300, 1000 and 2000 test subjects\nwhich indicates improvement of relative EER on the best-reported result by\n95.67%, 95.26%, 92.9% and 91.52% respectively. Apart from the accuracy gain,\nbecause of the nature of our proposed framework which is based on neural\nnetworks and consequently is as simple as some consecutive matrix\nmultiplications, it has less computational cost than conventional methods such\nas DTW and could be used concurrently on devices such as GPU, TPU, etc.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 18:55:42 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 07:12:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ahrabian", "Kian", ""], ["Babaali", "Bagher", ""]]}, {"id": "1712.02822", "submitter": "Alex Levinshtein", "authors": "Alex Levinshtein (1), Edmund Phung (1), Parham Aarabi (1 and 2) ((1)\n  ModiFace Inc, (2) University of Toronto)", "title": "Hybrid eye center localization using cascaded regression and\n  hand-crafted model fitting", "comments": "12 pages, 5 figures, submitted to Journal of Image and Vision\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new cascaded regressor for eye center detection. Previous\nmethods start from a face or an eye detector and use either advanced features\nor powerful regressors for eye center localization, but not both. Instead, we\ndetect the eyes more accurately using an existing facial feature alignment\nmethod. We improve the robustness of localization by using both advanced\nfeatures and powerful regression machinery. Unlike most other methods that do\nnot refine the regression results, we make the localization more accurate by\nadding a robust circle fitting post-processing step. Finally, using a simple\nhand-crafted method for eye center localization, we show how to train the\ncascaded regressor without the need for manually annotated training data. We\nevaluate our new approach and show that it achieves state-of-the-art\nperformance on the BioID, GI4E, and the TalkingFace datasets. At an average\nnormalized error of e < 0.05, the regressor trained on manually annotated data\nyields an accuracy of 95.07% (BioID), 99.27% (GI4E), and 95.68% (TalkingFace).\nThe automatically trained regressor is nearly as good, yielding an accuracy of\n93.9% (BioID), 99.27% (GI4E), and 95.46% (TalkingFace).\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 19:21:19 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Levinshtein", "Alex", "", "1 and 2"], ["Phung", "Edmund", "", "1 and 2"], ["Aarabi", "Parham", "", "1 and 2"]]}, {"id": "1712.02824", "submitter": "Lu\\'is Alexandre", "authors": "Ricardo Gamelas Sousa and Jorge M. Santos and Lu\\'is M. Silva and\n  Lu\\'is A. Alexandre and Tiago Esteves and Sara Rocha and Paulo Monjardino and\n  Joaquim Marques de S\\'a and Francisco Figueiredo and Pedro Quelhas", "title": "Stacked Denoising Autoencoders and Transfer Learning for Immunogold\n  Particles Detection and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a system for the detection of immunogold particles\nand a Transfer Learning (TL) framework for the recognition of these immunogold\nparticles. Immunogold particles are part of a high-magnification method for the\nselective localization of biological molecules at the subcellular level only\nvisible through Electron Microscopy. The number of immunogold particles in the\ncell walls allows the assessment of the differences in their compositions\nproviding a tool to analise the quality of different plants. For its\nquantization one requires a laborious manual labeling (or annotation) of images\ncontaining hundreds of particles. The system that is proposed in this paper can\nleverage significantly the burden of this manual task.\n  For particle detection we use a LoG filter coupled with a SDA. In order to\nimprove the recognition, we also study the applicability of TL settings for\nimmunogold recognition. TL reuses the learning model of a source problem on\nother datasets (target problems) containing particles of different sizes. The\nproposed system was developed to solve a particular problem on maize cells,\nnamely to determine the composition of cell wall ingrowths in endosperm\ntransfer cells. This novel dataset as well as the code for reproducing our\nexperiments is made publicly available.\n  We determined that the LoG detector alone attained more than 84\\% of accuracy\nwith the F-measure. Developing immunogold recognition with TL also provided\nsuperior performance when compared with the baseline models augmenting the\naccuracy rates by 10\\%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 19:28:05 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Sousa", "Ricardo Gamelas", ""], ["Santos", "Jorge M.", ""], ["Silva", "Lu\u00eds M.", ""], ["Alexandre", "Lu\u00eds A.", ""], ["Esteves", "Tiago", ""], ["Rocha", "Sara", ""], ["Monjardino", "Paulo", ""], ["de S\u00e1", "Joaquim Marques", ""], ["Figueiredo", "Francisco", ""], ["Quelhas", "Pedro", ""]]}, {"id": "1712.02854", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic reconstruction of an oolitic limestone by generative\n  adversarial networks", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic image reconstruction is a key part of modern digital rock physics\nand materials analysis that aims to create numerous representative samples of\nmaterial micro-structures for upscaling, numerical computation of effective\nproperties and uncertainty quantification. We present a method of\nthree-dimensional stochastic image reconstruction based on generative\nadversarial neural networks (GANs). GANs represent a framework of unsupervised\nlearning methods that require no a priori inference of the probability\ndistribution associated with the training data. Using a fully convolutional\nneural network allows fast sampling of large volumetric images.We apply a GAN\nbased workflow of network training and image generation to an oolitic Ketton\nlimestone micro-CT dataset. Minkowski functionals, effective permeability as\nwell as velocity distributions of simulated flow within the acquired images are\ncompared with the synthetic reconstructions generated by the deep neural\nnetwork. While our results show that GANs allow a fast and accurate\nreconstruction of the evaluated image dataset, we address a number of open\nquestions and challenges involved in the evaluation of generative network-based\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 20:21:01 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1712.02859", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Michael Zollh\\\"ofer, Pablo Garrido, Florian Bernard,\n  Hyeongwoo Kim, Patrick P\\'erez, Christian Theobalt", "title": "Self-supervised Multi-level Face Model Learning for Monocular\n  Reconstruction at over 250 Hz", "comments": "CVPR 2018 (Oral). Project webpage:\n  https://gvv.mpi-inf.mpg.de/projects/FML/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of dense 3D models of face geometry and appearance from a\nsingle image is highly challenging and ill-posed. To constrain the problem,\nmany approaches rely on strong priors, such as parametric face models learned\nfrom limited 3D scan data. However, prior models restrict generalization of the\ntrue diversity in facial geometry, skin reflectance and illumination. To\nalleviate this problem, we present the first approach that jointly learns 1) a\nregressor for face shape, expression, reflectance and illumination on the basis\nof 2) a concurrently learned parametric face model. Our multi-level face model\ncombines the advantage of 3D Morphable Models for regularization with the\nout-of-space generalization of a learned corrective space. We train end-to-end\non in-the-wild images without dense annotations by fusing a convolutional\nencoder with a differentiable expert-designed renderer and a self-supervised\ntraining loss, both defined at multiple detail levels. Our approach compares\nfavorably to the state-of-the-art in terms of reconstruction quality, better\ngeneralizes to real world faces, and runs at over 250 Hz.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:04:51 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 18:41:32 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Garrido", "Pablo", ""], ["Bernard", "Florian", ""], ["Kim", "Hyeongwoo", ""], ["P\u00e9rez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "1712.02861", "submitter": "Aditya Ganeshan Master", "authors": "Aditya Ganeshan", "title": "Per-Pixel Feedback for improving Semantic Segmentation", "comments": "33 pages,18 figures,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is the task of assigning a label to each pixel in the\nimage.In recent years, deep convolutional neural networks have been driving\nadvances in multiple tasks related to cognition. Although, DCNNs have resulted\nin unprecedented visual recognition performances, they offer little\ntransparency. To understand how DCNN based models work at the task of semantic\nsegmentation, we try to analyze the DCNN models in semantic segmentation. We\ntry to find the importance of global image information for labeling pixels.\n  Based on the experiments on discriminative regions, and modeling of\nfixations, we propose a set of new training loss functions for fine-tuning DCNN\nbased models. The proposed training regime has shown improvement in performance\nof DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset.\nHowever, further test remains to conclusively evaluate the benefits due to the\nproposed loss functions across models, and data-sets.\n  Submitted in part fulfillment of the requirements for the degree of\nIntegrated Masters of Science in Applied Mathematics.\n  Update: Further Experiment showed minimal benefits.\n  Code Available [here](https://github.com/BardOfCodes/Seg-Unravel).\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:12:53 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Ganeshan", "Aditya", ""]]}, {"id": "1712.02862", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal, Merry P. Mani, Mathews Jacob", "title": "MoDL: Model Based Deep Learning Architecture for Inverse Problems", "comments": "published in IEEE Transaction on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2865356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-based image reconstruction framework with a convolution\nneural network (CNN) based regularization prior. The proposed formulation\nprovides a systematic approach for deriving deep architectures for inverse\nproblems with the arbitrary structure. Since the forward model is explicitly\naccounted for, a smaller network with fewer parameters is sufficient to capture\nthe image information compared to black-box deep learning approaches, thus\nreducing the demand for training data and training time. Since we rely on\nend-to-end training, the CNN weights are customized to the forward model, thus\noffering improved performance over approaches that rely on pre-trained\ndenoisers. The main difference of the framework from existing end-to-end\ntraining strategies is the sharing of the network weights across iterations and\nchannels. Our experiments show that the decoupling of the number of iterations\nfrom the network complexity offered by this approach provides benefits\nincluding lower demand for training data, reduced risk of overfitting, and\nimplementations with significantly reduced memory footprint. We propose to\nenforce data-consistency by using numerical optimization blocks such as\nconjugate gradients algorithm within the network; this approach offers faster\nconvergence per iteration, compared to methods that rely on proximal gradients\nsteps to enforce data consistency. Our experiments show that the faster\nconvergence translates to improved performance, especially when the available\nGPU memory restricts the number of iterations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:13:13 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 20:24:37 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 18:41:30 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 16:31:11 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Mani", "Merry P.", ""], ["Jacob", "Mathews", ""]]}, {"id": "1712.02864", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Peyman Milanfar", "title": "Learned Perceptual Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a typical image enhancement pipeline involves minimization of a loss\nfunction between enhanced and reference images. While L1 and L2 losses are\nperhaps the most widely used functions for this purpose, they do not\nnecessarily lead to perceptually compelling results. In this paper, we show\nthat adding a learned no-reference image quality metric to the loss can\nsignificantly improve enhancement operators. This metric is implemented using a\nCNN (convolutional neural network) trained on a large-scale dataset labelled\nwith aesthetic preferences of human raters. This loss allows us to conveniently\nperform back-propagation in our learning framework to simultaneously optimize\nfor similarity to a given ground truth reference and perceptual quality. This\nperceptual loss is only used to train parameters of image processing operators,\nand does not impose any extra complexity at inference time. Our experiments\ndemonstrate that this loss can be effective for tuning a variety of operators\nsuch as local tone mapping and dehazing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:23:12 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Talebi", "Hossein", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1712.02874", "submitter": "Zhe Hu", "authors": "Zhe Hu, Yinglan Ma, Lizhuang Ma", "title": "Multi-Scale Video Frame-Synthesis Network with Transitive Consistency\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional approaches to interpolate/extrapolate frames in a video sequence\nrequire accurate pixel correspondences between images, e.g., using optical\nflow. Their results stem on the accuracy of optical flow estimation, and could\ngenerate heavy artifacts when flow estimation failed. Recently methods using\nauto-encoder has shown impressive progress, however they are usually trained\nfor specific interpolation/extrapolation settings and lack of flexibility and\nIn order to reduce these limitations, we propose a unified network to\nparameterize the interest frame position and therefore infer\ninterpolate/extrapolate frames within the same framework. To achieve this, we\nintroduce a transitive consistency loss to better regularize the network. We\nadopt a multi-scale structure for the network so that the parameters can be\nshared across multi-layers. Our approach avoids expensive global optimization\nof optical flow methods, and is efficient and flexible for video\ninterpolation/extrapolation applications. Experimental results have shown that\nour method performs favorably against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 22:01:00 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 21:38:12 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Hu", "Zhe", ""], ["Ma", "Yinglan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1712.02877", "submitter": "Shabab Bazrafkan", "authors": "Shabab Bazrafkan, Shejin Thavalengal, Peter Corcoran", "title": "An End to End Deep Neural Network for Iris Segmentation in Unconstraint\n  Scenarios", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.06.011", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing imaging and processing capabilities of today's mobile\ndevices, user authentication using iris biometrics has become feasible.\nHowever, as the acquisition conditions become more unconstrained and as image\nquality is typically lower than dedicated iris acquisition systems, the\naccurate segmentation of iris regions is crucial for these devices. In this\nwork, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is\nproposed to perform the iris segmentation task for lower-quality iris images.\nThe network design process is explained in detail, and the resulting network is\ntrained and tuned using several large public iris datasets. A set of methods to\ngenerate and augment suitable lower quality iris images from the high-quality\npublic databases are provided. The network is trained on Near InfraRed (NIR)\nimages initially and later tuned on additional datasets derived from visible\nimages. Comprehensive inter-database comparisons are provided together with\nresults from a selection of experiments detailing the effects of different\ntunings of the network. Finally, the proposed model is compared with\nSegNet-basic, and a near-optimal tuning of the network is compared to a\nselection of other state-of-art iris segmentation algorithms. The results show\nvery promising performance from the optimized Deep Neural Networks design when\ncompared with state-of-art techniques applied to the same lower quality\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 22:18:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Bazrafkan", "Shabab", ""], ["Thavalengal", "Shejin", ""], ["Corcoran", "Peter", ""]]}, {"id": "1712.02890", "submitter": "Hiroshi Kuwajima", "authors": "Hiroshi Kuwajima and Masayuki Tanaka", "title": "Network Analysis for Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety critical systems strongly require the quality aspects of artificial\nintelligence including explainability. In this paper, we analyzed a trained\nnetwork to extract features which mainly contribute the inference. Based on the\nanalysis, we developed a simple solution to generate explanations of the\ninference processes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 23:35:11 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Kuwajima", "Hiroshi", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "1712.02893", "submitter": "Kaiyue Lu", "authors": "Kaiyue Lu, Shaodi You, Nick Barnes", "title": "Deep Texture and Structure Aware Filtering Network for Image Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing is a fundamental task in computer vision, that aims to retain\nsalient structures and remove insignificant textures. In this paper, we aim to\naddress the fundamental shortcomings of existing image smoothing methods, which\ncannot properly distinguish textures and structures with similar low-level\nappearance. While deep learning approaches have started to explore the\npreservation of structure through image smoothing, existing work does not yet\nproperly address textures. To this end, we generate a large dataset by blending\nnatural textures with clean structure-only images, and then build a texture\nprediction network (TPN) that predicts the location and magnitude of textures.\nWe then combine the TPN with a semantic structure prediction network (SPN) so\nthat the final texture and structure aware filtering network (TSAFN) is able to\nidentify the textures to remove (\"texture-awareness\") and the structures to\npreserve (\"structure-awareness\"). The proposed model is easy to understand and\nimplement, and shows excellent performance on real images in the wild as well\nas our generated dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 23:54:26 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:51:15 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lu", "Kaiyue", ""], ["You", "Shaodi", ""], ["Barnes", "Nick", ""]]}, {"id": "1712.02896", "submitter": "Eric Chu", "authors": "Eric Chu and Deb Roy", "title": "Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies", "comments": "Data Mining (ICDM), 2017 IEEE 17th International Conference on", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stories can have tremendous power -- not only useful for entertainment, they\ncan activate our interests and mobilize our actions. The degree to which a\nstory resonates with its audience may be in part reflected in the emotional\njourney it takes the audience upon. In this paper, we use machine learning\nmethods to construct emotional arcs in movies, calculate families of arcs, and\ndemonstrate the ability for certain arcs to predict audience engagement. The\nsystem is applied to Hollywood films and high quality shorts found on the web.\nWe begin by using deep convolutional neural networks for audio and visual\nsentiment analysis. These models are trained on both new and existing\nlarge-scale datasets, after which they can be used to compute separate audio\nand visual emotional arcs. We then crowdsource annotations for 30-second video\nclips extracted from highs and lows in the arcs in order to assess the\nmicro-level precision of the system, with precision measured in terms of\nagreement in polarity between the system's predictions and annotators' ratings.\nThese annotations are also used to combine the audio and visual predictions.\nNext, we look at macro-level characterizations of movies by investigating\nwhether there exist `universal shapes' of emotional arcs. In particular, we\ndevelop a clustering approach to discover distinct classes of emotional arcs.\nFinally, we show on a sample corpus of short web videos that certain emotional\narcs are statistically significant predictors of the number of comments a video\nreceives. These results suggest that the emotional arcs learned by our approach\nsuccessfully represent macroscopic aspects of a video story that drive audience\nengagement. Such machine understanding could be used to predict audience\nreactions to video stories, ultimately improving our ability as storytellers to\ncommunicate with each other.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 00:27:08 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chu", "Eric", ""], ["Roy", "Deb", ""]]}, {"id": "1712.02898", "submitter": "Alexei Koulakov", "authors": "Sergey Shuvaev, Hamza Giaffar, and Alexei A. Koulakov", "title": "Representations of Sound in Deep Learning of Audio Features from Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work of a single musician, group or composer can vary widely in terms of\nmusical style. Indeed, different stylistic elements, from performance medium\nand rhythm to harmony and texture, are typically exploited and developed across\nan artist's lifetime. Yet, there is often a discernable character to the work\nof, for instance, individual composers at the perceptual level - an experienced\nlistener can often pick up on subtle clues in the music to identify the\ncomposer or performer. Here we suggest that a convolutional network may learn\nthese subtle clues or features given an appropriate representation of the\nmusic. In this paper, we apply a deep convolutional neural network to a large\naudio dataset and empirically evaluate its performance on audio classification\ntasks. Our trained network demonstrates accurate performance on such\nclassification tasks when presented with 5 s examples of music obtained by\nsimple transformations of the raw audio waveform. A particularly interesting\nexample is the spectral representation of music obtained by application of a\nlogarithmically spaced filter bank, mirroring the early stages of auditory\nsignal transduction in mammals. The most successful representation of music to\nfacilitate discrimination was obtained via a random matrix transform (RMT).\nNetworks based on logarithmic filter banks and RMT were able to correctly guess\nthe one composer out of 31 possibilities in 68 and 84 percent of cases\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 00:37:23 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Shuvaev", "Sergey", ""], ["Giaffar", "Hamza", ""], ["Koulakov", "Alexei A.", ""]]}, {"id": "1712.02912", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e", "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search", "comments": "PhD Thesis, 123 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:14:17 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Andr\u00e9", "Fabien", ""]]}, {"id": "1712.02933", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Cong Phouc Huynh and Fatih Porikli", "title": "Chaining Identity Mapping Modules for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to learn a fully-convolutional network model that consists of a\nChain of Identity Mapping Modules (CIMM) for image denoising. The CIMM\nstructure possesses two distinctive features that are important for the noise\nremoval task. Firstly, each residual unit employs identity mappings as the skip\nconnections and receives pre-activated input in order to preserve the gradient\nmagnitude propagated in both the forward and backward directions. Secondly, by\nutilizing dilated kernels for the convolution layers in the residual branch, in\nother words within an identity mapping module, each neuron in the last\nconvolution layer can observe the full receptive field of the first layer.\nAfter being trained on the BSD400 dataset, the proposed network produces\nremarkably higher numerical accuracy and better visual image quality than the\nstate-of-the-art when being evaluated on conventional benchmark images and the\nBSD68 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 03:51:54 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 01:35:32 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Anwar", "Saeed", ""], ["Huynh", "Cong Phouc", ""], ["Porikli", "Fatih", ""]]}, {"id": "1712.02941", "submitter": "Ken Sakurada", "authors": "Ken Sakurada, Weimin Wang, Nobuo Kawaguchi, Ryosuke Nakamura", "title": "Dense Optical Flow based Change Detection Network Robust to Difference\n  of Camera Viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for detecting scene changes from a pair of\nimages with a difference of camera viewpoints using a dense optical flow based\nchange detection network. In the case that camera poses of input images are\nfixed or known, such as with surveillance and satellite cameras, the pixel\ncorrespondence between the images captured at different times can be known.\nHence, it is possible to comparatively accurately detect scene changes between\nthe images by modeling the appearance of the scene. On the other hand, in case\nof cameras mounted on a moving object, such as ground and aerial vehicles, we\nmust consider the spatial correspondence between the images captured at\ndifferent times. However, it can be difficult to accurately estimate the camera\npose or 3D model of a scene, owing to the scene changes or lack of imagery. To\nsolve this problem, we propose a change detection convolutional neural network\nutilizing dense optical flow between input images to improve the robustness to\nthe difference between camera viewpoints. Our evaluation based on the panoramic\nchange detection dataset shows that the proposed method outperforms\nstate-of-the-art change detection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 05:05:51 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Sakurada", "Ken", ""], ["Wang", "Weimin", ""], ["Kawaguchi", "Nobuo", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1712.02950", "submitter": "Casey Chu", "authors": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN, a Master of Steganography", "comments": "NIPS 2017, workshop on Machine Deception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a\ntransformation between two image distributions. In a series of experiments, we\ndemonstrate an intriguing property of the model: CycleGAN learns to \"hide\"\ninformation about a source image into the images it generates in a nearly\nimperceptible, high-frequency signal. This trick ensures that the generator can\nrecover the original sample and thus satisfy the cyclic consistency\nrequirement, while the generated image remains realistic. We connect this\nphenomenon with adversarial attacks by viewing CycleGAN's training procedure as\ntraining a generator of adversarial examples and demonstrate that the cyclic\nconsistency loss causes CycleGAN to be especially vulnerable to adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:07:52 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 07:42:46 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Chu", "Casey", ""], ["Zhmoginov", "Andrey", ""], ["Sandler", "Mark", ""]]}, {"id": "1712.02956", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Anh-Dzung Doan, Ngai-Man\n  Cheung", "title": "Compact Hash Code Learning with Binary Deep Neural Network", "comments": "Accepted to IEEE Transactions on Multimedia, 2019. arXiv admin note:\n  text overlap with arXiv:1607.05140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact binary codes for image retrieval problem using deep neural\nnetworks has recently attracted increasing attention. However, training deep\nhashing networks is challenging due to the binary constraints on the hash\ncodes. In this paper, we propose deep network models and learning algorithms\nfor learning binary hash codes given image representations under both\nunsupervised and supervised manners. The novelty of our network design is that\nwe constrain one hidden layer to directly output the binary codes. This design\nhas overcome a challenging problem in some previous works: optimizing\nnon-smooth objective functions because of binarization. In addition, we propose\nto incorporate independence and balance properties in the direct and strict\nforms into the learning schemes. We also include a similarity preserving\nproperty in our objective functions. The resulting optimizations involving\nthese binary, independence, and balance constraints are difficult to solve. To\ntackle this difficulty, we propose to learn the networks with alternating\noptimization and careful relaxation. Furthermore, by leveraging the powerful\ncapacity of convolutional neural networks, we propose an end-to-end\narchitecture that jointly learns to extract visual features and produce binary\nhash codes. Experimental results for the benchmark datasets show that the\nproposed methods compare favorably or outperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:25:35 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 16:35:32 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 00:15:04 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Hoang", "Tuan", ""], ["Tan", "Dang-Khoa Le", ""], ["Doan", "Anh-Dzung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1712.02961", "submitter": "Dawei Yang", "authors": "Dawei Yang and Jia Deng", "title": "Shape from Shading through Shape Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the shape-from-shading problem by training deep\nnetworks with synthetic images. Unlike conventional approaches that combine\ndeep learning and synthetic imagery, we propose an approach that does not need\nany external shape dataset to render synthetic images. Our approach consists of\ntwo synergistic processes: the evolution of complex shapes from simple\nprimitives, and the training of a deep network for shape-from-shading. The\nevolution generates better shapes guided by the network training, while the\ntraining improves by using the evolved shapes. We show that our approach\nachieves state-of-the-art performance on a shape-from-shading benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:41:42 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yang", "Dawei", ""], ["Deng", "Jia", ""]]}, {"id": "1712.02974", "submitter": "Sayed Kamaledin Ghiasi-Shirazi", "authors": "Kamaledin Ghiasi-Shirazi", "title": "Learning 2D Gabor Filters by Infinite Kernel Learning Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gabor functions have wide-spread applications in image processing and\ncomputer vision. In this paper, we prove that 2D Gabor functions are\ntranslation-invariant positive-definite kernels and propose a novel formulation\nfor the problem of image representation with Gabor functions based on infinite\nkernel learning regression. Using this formulation, we obtain a support vector\nexpansion of an image based on a mixture of Gabor functions. The problem with\nthis representation is that all Gabor functions are present at all support\nvector pixels. Applying LASSO to this support vector expansion, we obtain a\nsparse representation in which each Gabor function is positioned at a very\nsmall set of pixels. As an application, we introduce a method for learning a\ndataset-specific set of Gabor filters that can be used subsequently for feature\nextraction. Our experiments show that use of the learned Gabor filters improves\nthe recognition accuracy of a recently introduced face recognition algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 07:50:58 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Ghiasi-Shirazi", "Kamaledin", ""]]}, {"id": "1712.02976", "submitter": "Fangzhou Liao", "authors": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, Jun\n  Zhu", "title": "Defense against Adversarial Attacks Using High-Level Representation\n  Guided Denoiser", "comments": null, "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose high-level\nrepresentation guided denoiser (HGD) as a defense for image classification.\nStandard denoiser suffers from the error amplification effect, in which small\nresidual adversarial noise is progressively amplified and leads to wrong\nclassifications. HGD overcomes this problem by using a loss function defined as\nthe difference between the target model's outputs activated by the clean image\nand denoised image. Compared with ensemble adversarial training which is the\nstate-of-the-art defending method on large images, HGD has three advantages.\nFirst, with HGD as a defense, the target model is more robust to either\nwhite-box or black-box adversarial attacks. Second, HGD can be trained on a\nsmall subset of the images and generalizes well to other images and unseen\nclasses. Third, HGD can be transferred to defend models other than the one\nguiding it. In NIPS competition on defense against adversarial attacks, our HGD\nsolution won the first place and outperformed other models by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 08:20:45 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 06:06:06 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liao", "Fangzhou", ""], ["Liang", "Ming", ""], ["Dong", "Yinpeng", ""], ["Pang", "Tianyu", ""], ["Hu", "Xiaolin", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.02982", "submitter": "Bob D. de Vos", "authors": "Bob D. de Vos, Nikolas Lessmann, Pim A. de Jong, Max A. Viergever,\n  Ivana Isgum", "title": "Direct and Real-Time Cardiovascular Risk Prediction", "comments": "Scientific paper at RSNA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery calcium (CAC) burden quantified in low-dose chest CT is a\npredictor of cardiovascular events. We propose an automatic method for CAC\nquantification, circumventing intermediate segmentation of CAC. The method\ndetermines a bounding box around the heart using a ConvNet for localization.\nSubsequently, a dedicated ConvNet analyzes axial slices within the bounding\nboxes to determine CAC quantity by regression. A dataset of 1,546 baseline CT\nscans was used from the National Lung Screening Trial with manually identified\nCAC. The method achieved an ICC of 0.98 between manual reference and\nautomatically obtained Agatston scores. Stratification of subjects into five\ncardiovascular risk categories resulted in an accuracy of 85\\% and Cohen's\nlinearly weighted $\\kappa$ of 0.90. The results demonstrate that real-time\nquantification of CAC burden in chest CT without the need for segmentation of\nCAC is possible.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 08:53:09 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["de Vos", "Bob D.", ""], ["Lessmann", "Nikolas", ""], ["de Jong", "Pim A.", ""], ["Viergever", "Max A.", ""], ["Isgum", "Ivana", ""]]}, {"id": "1712.03037", "submitter": "Junxuan Li", "authors": "Junxuan Li, Shaodi You and Antonio Robles-Kelly", "title": "A Frequency Domain Neural Network for Fast Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a frequency domain neural network for image\nsuper-resolution. The network employs the convolution theorem so as to cast\nconvolutions in the spatial domain as products in the frequency domain.\nMoreover, the non-linearity in deep nets, often achieved by a rectifier unit,\nis here cast as a convolution in the frequency domain. This not only yields a\nnetwork which is very computationally efficient at testing but also one whose\nparameters can all be learnt accordingly. The network can be trained using back\npropagation and is devoid of complex numbers due to the use of the Hartley\ntransform as an alternative to the Fourier transform. Moreover, the network is\npotentially applicable to other problems elsewhere in computer vision and image\nprocessing which are often cast in the frequency domain. We show results on\nsuper-resolution and compare against alternatives elsewhere in the literature.\nIn our experiments, our network is one to two orders of magnitude faster than\nthe alternatives with an imperceptible loss of performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 11:53:52 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Li", "Junxuan", ""], ["You", "Shaodi", ""], ["Robles-Kelly", "Antonio", ""]]}, {"id": "1712.03084", "submitter": "Anargyros Chatzitofis", "authors": "Dimitrios S. Alexiadis, Anargyros Chatzitofis, Nikolaos Zioulis, Olga\n  Zoidi, Georgios Louizis, Dimitrios Zarpalas, and Petros Daras, Senior Member,\n  IEEE", "title": "An Integrated Platform for Live 3D Human Reconstruction and Motion\n  Capturing", "comments": "16 pages, 12 figures, 3 tables", "journal-ref": "Alexiadis, D. S., Chatzitofis, A., Zioulis, N., Zoidi, O.,\n  Louizis, G., Zarpalas, D., & Daras, P. (2017). An Integrated Platform for\n  Live 3D Human Reconstruction and Motion Capturing. IEEE TCSVT Page(s): 798 -\n  813", "doi": "10.1109/TCSVT.2016.2576922", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The latest developments in 3D capturing, processing, and rendering provide\nmeans to unlock novel 3D application pathways. The main elements of an\nintegrated platform, which target tele-immersion and future 3D applications,\nare described in this paper, addressing the tasks of real-time capturing,\nrobust 3D human shape/appearance reconstruction, and skeleton-based motion\ntracking. More specifically, initially, the details of a multiple RGB-depth\n(RGB-D) capturing system are given, along with a novel sensors' calibration\nmethod. A robust, fast reconstruction method from multiple RGB-D streams is\nthen proposed, based on an enhanced variation of the volumetric Fourier\ntransform-based method, parallelized on the Graphics Processing Unit, and\naccompanied with an appropriate texture-mapping algorithm. On top of that,\ngiven the lack of relevant objective evaluation methods, a novel framework is\nproposed for the quantitative evaluation of real-time 3D reconstruction\nsystems. Finally, a generic, multiple depth stream-based method for accurate\nreal-time human skeleton tracking is proposed. Detailed experimental results\nwith multi-Kinect2 data sets verify the validity of our arguments and the\neffectiveness of the proposed system and methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 14:31:54 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Alexiadis", "Dimitrios S.", ""], ["Chatzitofis", "Anargyros", ""], ["Zioulis", "Nikolaos", ""], ["Zoidi", "Olga", ""], ["Louizis", "Georgios", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""], ["Member", "Senior", ""], ["IEEE", "", ""]]}, {"id": "1712.03111", "submitter": "Pascal Laube", "authors": "Pascal Laube, Michael Grunwald, Matthias O. Franz and Georg Umlauf", "title": "Image Inpainting for High-Resolution Textures using CNN Texture\n  Synthesis", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully applied to problems such as image\nsegmentation, image super-resolution, coloration and image inpainting. In this\nwork we propose the use of convolutional neural networks (CNN) for image\ninpainting of large regions in high-resolution textures. Due to limited\ncomputational resources processing high-resolution images with neural networks\nis still an open problem. Existing methods separate inpainting of global\nstructure and the transfer of details, which leads to blurry results and loss\nof global coherence in the detail transfer step. Based on advances in texture\nsynthesis using CNNs we propose patch-based image inpainting by a CNN that is\nable to optimize for global as well as detail texture statistics. Our method is\ncapable of filling large inpainting regions, oftentimes exceeding the quality\nof comparable methods for high-resolution images. For reference patch look-up\nwe propose to use the same summary statistics that are used in the inpainting\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:02:27 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 11:52:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Laube", "Pascal", ""], ["Grunwald", "Michael", ""], ["Franz", "Matthias O.", ""], ["Umlauf", "Georg", ""]]}, {"id": "1712.03121", "submitter": "Muhammad Jameel Malik", "authors": "Jameel Malik, Ahmed Elhayek, Didier Stricker", "title": "Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a\n  Single Depth Image", "comments": "This paper has been accepted and presented in 3DV-2017 conference\n  held at Qingdao, China. http://irc.cs.sdu.edu.cn/3dv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulated hand pose estimation is a challenging task for human-computer\ninteraction. The state-of-the-art hand pose estimation algorithms work only\nwith one or a few subjects for which they have been calibrated or trained.\nParticularly, the hybrid methods based on learning followed by model fitting or\nmodel based deep learning do not explicitly consider varying hand shapes and\nsizes. In this work, we introduce a novel hybrid algorithm for estimating the\n3D hand pose as well as bone-lengths of the hand skeleton at the same time,\nfrom a single depth image. The proposed CNN architecture learns hand pose\nparameters and scale parameters associated with the bone-lengths\nsimultaneously. Subsequently, a new hybrid forward kinematics layer employs\nboth parameters to estimate 3D joint positions of the hand. For end-to-end\ntraining, we combine three public datasets NYU, ICVL and MSRA-2015 in one\nunified format to achieve large variation in hand shapes and sizes. Among\nhybrid methods, our method shows improved accuracy over the state-of-the-art on\nthe combined dataset and the ICVL dataset that contain multiple subjects. Also,\nour algorithm is demonstrated to work well with unseen images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:25:00 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Malik", "Jameel", ""], ["Elhayek", "Ahmed", ""], ["Stricker", "Didier", ""]]}, {"id": "1712.03141", "submitter": "Battista Biggio", "authors": "Battista Biggio and Fabio Roli", "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning", "comments": "Accepted for publication on Pattern Recognition, 2018", "journal-ref": null, "doi": "10.1016/j.patcog.2018.07.023", "report-no": null, "categories": "cs.CV cs.CR cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:59:41 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:27:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Biggio", "Battista", ""], ["Roli", "Fabio", ""]]}, {"id": "1712.03149", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Jianshu Li, Bin Zhou, Jiashi Feng, Shuicheng Yan", "title": "Weaving Multi-scale Context for Single Shot Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating context information from multiple scales has been proved to be\neffective for improving accuracy of Single Shot Detectors (SSDs) on object\ndetection. However, existing multi-scale context fusion techniques are\ncomputationally expensive, which unfavorably diminishes the advantageous speed\nof SSD. In this work, we propose a novel network topology, called WeaveNet,\nthat can efficiently fuse multi-scale information and boost the detection\naccuracy with negligible extra cost. The proposed WeaveNet iteratively weaves\ncontext information from adjacent scales together to enable more sophisticated\ncontext reasoning while maintaining fast speed. Built by stacking light-weight\nblocks, WeaveNet is easy to train without requiring batch normalization and can\nbe further accelerated by our proposed architecture simplification.\nExperimental results on PASCAL VOC 2007, PASCAL VOC 2012 benchmarks show\nsignification performance boost brought by WeaveNet. For 320x320 input of batch\nsize = 8, WeaveNet reaches 79.5% mAP on PASCAL VOC 2007 test in 101 fps with\nonly 4 fps extra cost, and further improves to 79.7% mAP with more iterations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:16:08 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chen", "Yunpeng", ""], ["Li", "Jianshu", ""], ["Zhou", "Bin", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1712.03151", "submitter": "Jared Markowitz", "authors": "Jared Markowitz, Aurora C. Schmidt, Philippe M. Burlina, I-Jeng Wang", "title": "Combining Deep Universal Features, Semantic Attributes, and Hierarchical\n  Classification for Zero-Shot Learning", "comments": "17 pages, 4 figures, extension to work published in conference\n  proceedings of 2017 IAPR MVA Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address zero-shot (ZS) learning, building upon prior work in hierarchical\nclassification by combining it with approaches based on semantic attribute\nestimation. For both non-novel and novel image classes we compare multiple\nformulations of the problem, starting with deep universal features in each\ncase. We investigate the effect of using different posterior probabilities as\ninputs to the hierarchical classifier, comparing the performances of posteriors\nderived from distances to SVM classifier boundaries with those of posteriors\nbased on semantic attribute estimation. Using a dataset consisting of 150\nobject classes from the ImageNet ILSVRC2012 data set, we find that the\nhierarchical classification method that maximizes expected reward for non-novel\nclasses differs from the method that maximizes expected reward for novel\nclasses. We also show that using input posteriors based on semantic attributes\nimproves the expected reward for novel classes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:17:31 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Markowitz", "Jared", ""], ["Schmidt", "Aurora C.", ""], ["Burlina", "Philippe M.", ""], ["Wang", "I-Jeng", ""]]}, {"id": "1712.03159", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Christopher Zach", "title": "Minimal Solvers for Monocular Rolling Shutter Compensation under\n  Ackermann Motion", "comments": "Submitted to WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern automotive vehicles are often equipped with a budget commercial\nrolling shutter camera. These devices often produce distorted images due to the\ninter-row delay of the camera while capturing the image. Recent methods for\nmonocular rolling shutter motion compensation utilize blur kernel and the\nstraightness property of line segments. However, these methods are limited to\nhandling rotational motion and also are not fast enough to operate in real\ntime. In this paper, we propose a minimal solver for the rolling shutter motion\ncompensation which assumes known vertical direction of the camera. Thanks to\nthe Ackermann motion model of vehicles which consists of only two motion\nparameters, and two parameters for the simplified depth assumption that lead to\na 4-line algorithm. The proposed minimal solver estimates the rolling shutter\ncamera motion efficiently and accurately. The extensive experiments on real and\nsimulated datasets demonstrate the benefits of our approach in terms of\nqualitative and quantitative results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:26:43 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Purkait", "Pulak", ""], ["Zach", "Christopher", ""]]}, {"id": "1712.03162", "submitter": "Qi Dong", "authors": "Qi Dong and Shaogang Gong and Xiatian Zhu", "title": "Class Rectification Hard Mining for Imbalanced Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising detailed facial or clothing attributes in images of people is a\nchallenging task for computer vision, especially when the training data are\nboth in very large scale and extremely imbalanced among different attribute\nclasses. To address this problem, we formulate a novel scheme for batch\nincremental hard sample mining of minority attribute classes from imbalanced\nlarge scale training data. We develop an end-to-end deep learning framework\ncapable of avoiding the dominant effect of majority classes by discovering\nsparsely sampled boundaries of minority classes. This is made possible by\nintroducing a Class Rectification Loss (CRL) regularising algorithm. We\ndemonstrate the advantages and scalability of CRL over existing\nstate-of-the-art attribute recognition and imbalanced data learning models on\ntwo large scale imbalanced benchmark datasets, the CelebA facial attribute\ndataset and the X-Domain clothing attribute dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:32:56 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Dong", "Qi", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1712.03217", "submitter": "Mehmet Altan Toks\\\"oz", "authors": "Mehmet Altan Toks\\\"oz", "title": "Basic Thresholding Classification", "comments": "128 pages, 45 figures, PHd Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we propose a light-weight sparsity-based algorithm, basic\nthresholding classifier (BTC), for classification applications (such as face\nidentification, hyper-spectral image classification, etc.) which is capable of\nidentifying test samples extremely rapidly and performing high classification\naccuracy. Originally BTC is a linear classifier which works based on the\nassumption that the samples of the classes of a given dataset are linearly\nseparable. However, in practice those samples may not be linearly separable. In\nthis context, we also propose another algorithm namely kernel basic\nthresholding classifier (KBTC) which is a non-linear kernel version of the BTC\nalgorithm. KBTC can achieve promising results especially when the given samples\nare linearly non-separable. For both proposals, we introduce sufficient\nidentification conditions (SICs) under which BTC and KBTC can identify any test\nsample in the range space of a given dictionary. By using SICs, we develop\nparameter estimation procedures which do not require any cross validation. Both\nBTC and KBTC algorithms provide efficient classifier fusion schemes in which\nindividual classifier outputs are combined to produce better classification\nresults. For instance, for the application of face identification, this is done\nby combining the residuals having different random projectors. For spatial\napplications such as hyper-spectral image classification, the fusion is carried\nout by incorporating the spatial information, in which the output residual maps\nare filtered using a smoothing filter. Numerical results on publicly available\nface and hyper-spectral datasets show that our proposal outperforms well-known\nsupport vector machines (SVM)-based techniques, multinomial logistic regression\n(MLR)-based methods, and sparsity-based approaches like $l_1$-minimization and\nsimultaneous orthogonal matching pursuit (SOMP).\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 18:46:11 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Toks\u00f6z", "Mehmet Altan", ""]]}, {"id": "1712.03257", "submitter": "Dimitrios Christoforos Gklezakos", "authors": "Dimitrios C. Gklezakos and Rajesh P. N. Rao", "title": "Transformational Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem faced by object recognition systems is that objects and\ntheir features can appear in different locations, scales and orientations.\nCurrent deep learning methods attempt to achieve invariance to local\ntranslations via pooling, discarding the locations of features in the process.\nOther approaches explicitly learn transformed versions of the same feature,\nleading to representations that quickly explode in size. Instead of discarding\nthe rich and useful information about feature transformations to achieve\ninvariance, we argue that models should learn object features conjointly with\ntheir transformations to achieve equivariance. We propose a new model of\nunsupervised learning based on sparse coding that can learn object features\njointly with their affine transformations directly from images. Results based\non learning from natural images indicate that our approach matches the\nreconstruction quality of traditional sparse coding but with significantly\nfewer degrees of freedom while simultaneously learning transformations from\ndata. These results open the door to scaling up unsupervised learning to allow\ndeep feature+transformation learning in a manner consistent with the\nventral+dorsal stream architecture of the primate visual cortex.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 19:21:15 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Gklezakos", "Dimitrios C.", ""], ["Rao", "Rajesh P. N.", ""]]}, {"id": "1712.03316", "submitter": "Daniel Gordon", "authors": "Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon,\n  Dieter Fox, Ali Farhadi", "title": "IQA: Visual Question Answering in Interactive Environments", "comments": "Published in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Interactive Question Answering (IQA), the task of answering\nquestions that require an autonomous agent to interact with a dynamic visual\nenvironment. IQA presents the agent with a scene and a question, like: \"Are\nthere any apples in the fridge?\" The agent must navigate around the scene,\nacquire visual understanding of scene elements, interact with objects (e.g.\nopen refrigerators) and plan for a series of actions conditioned on the\nquestion. Popular reinforcement learning approaches with a single controller\nperform poorly on IQA owing to the large and diverse state space. We propose\nthe Hierarchical Interactive Memory Network (HIMN), consisting of a factorized\nset of controllers, allowing the system to operate at multiple levels of\ntemporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset\nbuilt upon AI2-THOR, a simulated photo-realistic environment of configurable\nindoor scenes with interactive objects (code and dataset available at\nhttps://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000\nquestions, each paired with a unique scene configuration. Our experiments show\nthat our proposed model outperforms popular single controller based methods on\nIQUAD V1. For sample questions and results, please view our video:\nhttps://youtu.be/pXd3C-1jr98\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 00:13:59 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 23:47:38 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 17:05:18 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Gordon", "Daniel", ""], ["Kembhavi", "Aniruddha", ""], ["Rastegari", "Mohammad", ""], ["Redmon", "Joseph", ""], ["Fox", "Dieter", ""], ["Farhadi", "Ali", ""]]}, {"id": "1712.03323", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul, Ramazan Gokberk Cinbis, Selim Aksoy", "title": "Fine-Grained Object Recognition and Zero-Shot Learning in Remote Sensing\n  Imagery", "comments": "G. Sumbul, R. G. Cinbis, S. Aksoy, \"Fine-Grained Object Recognition\n  and Zero-Shot Learning in Remote Sensing Imagery\", IEEE Transactions on\n  Geoscience and Remote Sensing (TGRS), in press, 2017", "journal-ref": null, "doi": "10.1109/TGRS.2017.2754648", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained object recognition that aims to identify the type of an object\namong a large number of subcategories is an emerging application with the\nincreasing resolution that exposes new details in image data. Traditional fully\nsupervised algorithms fail to handle this problem where there is low\nbetween-class variance and high within-class variance for the classes of\ninterest with small sample sizes. We study an even more extreme scenario named\nzero-shot learning (ZSL) in which no training example exists for some of the\nclasses. ZSL aims to build a recognition model for new unseen categories by\nrelating them to seen classes that were previously learned. We establish this\nrelation by learning a compatibility function between image features extracted\nvia a convolutional neural network and auxiliary information that describes the\nsemantics of the classes of interest by using training samples from the seen\nclasses. Then, we show how knowledge transfer can be performed for the unseen\nclasses by maximizing this function during inference. We introduce a new data\nset that contains 40 different types of street trees in 1-ft spatial resolution\naerial data, and evaluate the performance of this model with manually annotated\nattributes, a natural language model, and a scientific taxonomy as auxiliary\ninformation. The experiments show that the proposed model achieves 14.3%\nrecognition accuracy for the classes with no training examples, which is\nsignificantly better than a random guess accuracy of 6.3% for 16 test classes,\nand three other ZSL algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 00:44:39 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Sumbul", "Gencer", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Aksoy", "Selim", ""]]}, {"id": "1712.03337", "submitter": "Shihua Zhang", "authors": "Chihao Zhang, Shihua Zhang", "title": "Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise", "comments": "14 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 02:54:17 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zhang", "Chihao", ""], ["Zhang", "Shihua", ""]]}, {"id": "1712.03342", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt and Jinwei Gu and Kihwan Kim and James Hays and Jan\n  Kautz", "title": "Geometry-Aware Learning of Maps for Camera Localization", "comments": "CVPR 2018 camera ready paper + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps are a key component in image-based camera localization and visual SLAM\nsystems: they are used to establish geometric constraints between images,\ncorrect drift in relative pose estimation, and relocalize cameras after lost\ntracking. The exact definitions of maps, however, are often\napplication-specific and hand-crafted for different scenarios (e.g. 3D\nlandmarks, lines, planes, bags of visual words). We propose to represent maps\nas a deep neural net called MapNet, which enables learning a data-driven map\nrepresentation. Unlike prior work on learning maps, MapNet exploits cheap and\nubiquitous sensory inputs like visual odometry and GPS in addition to images\nand fuses them together for camera localization. Geometric constraints\nexpressed by these inputs, which have traditionally been used in bundle\nadjustment or pose-graph optimization, are formulated as loss terms in MapNet\ntraining and also used during inference. In addition to directly improving\nlocalization accuracy, this allows us to update the MapNet (i.e., maps) in a\nself-supervised manner using additional unlabeled video sequences from the\nscene. We also propose a novel parameterization for camera rotation which is\nbetter suited for deep-learning based camera pose regression. Experimental\nresults on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar\ndataset show significant performance improvement over prior work. The MapNet\nproject webpage is https://goo.gl/mRB3Au.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 05:26:57 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 17:24:18 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 02:31:33 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Gu", "Jinwei", ""], ["Kim", "Kihwan", ""], ["Hays", "James", ""], ["Kautz", "Jan", ""]]}, {"id": "1712.03351", "submitter": "Boyang Deng", "authors": "Boyang Deng, Junjie Yan, Dahua Lin", "title": "Peephole: Predicting Network Performance Before Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for performant networks has been a significant force that drives\nthe advancements of deep learning in recent years. While rewarding, improving\nnetwork design has never been an easy journey. The large design space combined\nwith the tremendous cost required for network training poses a major obstacle\nto this endeavor. In this work, we propose a new approach to this problem,\nnamely, predicting the performance of a network before training, based on its\narchitecture. Specifically, we develop a unified way to encode individual\nlayers into vectors and bring them together to form an integrated description\nvia LSTM. Taking advantage of the recurrent network's strong expressive power,\nthis method can reliably predict the performances of various network\narchitectures. Our empirical studies showed that it not only achieved accurate\npredictions but also produced consistent rankings across datasets -- a key\ndesideratum in performance prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 07:50:27 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Deng", "Boyang", ""], ["Yan", "Junjie", ""], ["Lin", "Dahua", ""]]}, {"id": "1712.03380", "submitter": "Siddhartha Chaudhuri", "authors": "Utkarsh Mall, G. Roshan Lal, Siddhartha Chaudhuri, Parag Chaudhuri", "title": "A Deep Recurrent Framework for Cleaning Motion Capture Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep, bidirectional, recurrent framework for cleaning noisy and\nincomplete motion capture data. It exploits temporal coherence and joint\ncorrelations to infer adaptive filters for each joint in each frame. A single\nmodel can be trained to denoise a heterogeneous mix of action types, under\nsubstantial amounts of noise. A signal that has both noise and gaps is\npreprocessed with a second bidirectional network that synthesizes missing\nframes from surrounding context. The approach handles a wide variety of noise\ntypes and long gaps, does not rely on knowledge of the noise distribution, and\noperates in a streaming setting. We validate our approach through extensive\nevaluations on noise both in joint angles and in joint positions, and show that\nit improves upon various alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 12:03:53 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Mall", "Utkarsh", ""], ["Lal", "G. Roshan", ""], ["Chaudhuri", "Siddhartha", ""], ["Chaudhuri", "Parag", ""]]}, {"id": "1712.03381", "submitter": "Rui Chen", "authors": "Rui Chen, Changshui Yang, Huizhu Jia, Xiaodong Xie", "title": "Noise Level Estimation for Overcomplete Dictionary Learning Based on\n  Tight Asymptotic Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we address the problem of estimating Gaussian noise level\nfrom the trained dictionaries in update stage. We first provide rigorous\nstatistical analysis on the eigenvalue distributions of a sample covariance\nmatrix. Then we propose an interval-bounded estimator for noise variance in\nhigh dimensional setting. To this end, an effective estimation method for noise\nlevel is devised based on the boundness and asymptotic behavior of noise\neigenvalue spectrum. The estimation performance of our method has been\nguaranteed both theoretically and empirically. The analysis and experiment\nresults have demonstrated that the proposed algorithm can reliably infer true\nnoise levels, and outperforms the relevant existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 12:42:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Rui", ""], ["Yang", "Changshui", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""]]}, {"id": "1712.03382", "submitter": "Muktabh Mayank Srivastava", "authors": "Muktabh Mayank Srivastava, Sonaal Kant", "title": "Visual aesthetic analysis using deep neural network: model and\n  techniques to increase accuracy without transfer learning", "comments": "Accepted at IEEE's 3rd International Conference for Convergence in\n  Technology (I2CT) Pune - 7-8 April 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We train a deep Convolutional Neural Network (CNN) from scratch for visual\naesthetic analysis in images and discuss techniques we adopt to improve the\naccuracy. We avoid the prevalent best transfer learning approaches of using\npretrained weights to perform the task and train a model from scratch to get\naccuracy of 78.7% on AVA2 Dataset close to the best models available (85.6%).\nWe further show that accuracy increases to 81.48% on increasing the training\nset by incremental 10 percentile of entire AVA dataset showing our algorithm\ngets better with more data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 12:56:24 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 11:19:21 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 11:11:49 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 05:15:23 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Srivastava", "Muktabh Mayank", ""], ["Kant", "Sonaal", ""]]}, {"id": "1712.03390", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg and R. Venkatesh Babu", "title": "NAG: Network for Adversary Generation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial perturbations can pose a serious threat for deploying machine\nlearning systems. Recent works have shown existence of image-agnostic\nperturbations that can fool classifiers over most natural images. Existing\nmethods present optimization approaches that solve for a fooling objective with\nan imperceptibility constraint to craft the perturbations. However, for a given\nclassifier, they generate one perturbation at a time, which is a single\ninstance from the manifold of adversarial perturbations. Also, in order to\nbuild robust models, it is essential to explore the manifold of adversarial\nperturbations. In this paper, we propose for the first time, a generative\napproach to model the distribution of adversarial perturbations. The\narchitecture of the proposed model is inspired from that of GANs and is trained\nusing fooling and diversity objectives. Our trained generator network attempts\nto capture the distribution of adversarial perturbations for a given classifier\nand readily generates a wide variety of such perturbations. Our experimental\nevaluation demonstrates that perturbations crafted by our model (i) achieve\nstate-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver\nexcellent cross model generalizability. Our work can be deemed as an important\nstep in the process of inferring about the complex manifolds of adversarial\nperturbations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 14:27:49 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:31:27 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Ojha", "Utkarsh", ""], ["Garg", "Utsav", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1712.03400", "submitter": "Lucas Rod\\'es-Guirao", "authors": "Federico Baldassarre, Diego Gonz\\'alez Mor\\'in, Lucas Rod\\'es-Guirao", "title": "Deep Koalarization: Image Colorization using CNNs and\n  Inception-ResNet-v2", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some of the most recent approaches to colorize gray-scale images\nusing deep learning methods. Inspired by these, we propose a model which\ncombines a deep Convolutional Neural Network trained from scratch with\nhigh-level features extracted from the Inception-ResNet-v2 pre-trained model.\nThanks to its fully convolutional architecture, our encoder-decoder model can\nprocess images of any size and aspect ratio. Other than presenting the training\nresults, we assess the \"public acceptance\" of the generated images by means of\na user study. Finally, we present a carousel of applications on different types\nof images, such as historical photographs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 15:29:35 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Baldassarre", "Federico", ""], ["Mor\u00edn", "Diego Gonz\u00e1lez", ""], ["Rod\u00e9s-Guirao", "Lucas", ""]]}, {"id": "1712.03451", "submitter": "Shangxuan Wu", "authors": "Xiaohan Jin, Ye Qi, Shangxuan Wu", "title": "CycleGAN Face-off", "comments": "Github repo: https://github.com/ShangxuanWu/CycleGAN-Face-off", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face-off is an interesting case of style transfer where the facial\nexpressions and attributes of one person could be fully transformed to another\nface. We are interested in the unsupervised training process which only\nrequires two sequences of unaligned video frames from each person and learns\nwhat shared attributes to extract automatically. In this project, we explored\nvarious improvements for adversarial training (i.e. CycleGAN[Zhu et al., 2017])\nto capture details in facial expressions and head poses and thus generate\ntransformation videos of higher consistency and stability.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 23:32:38 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 02:35:34 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 02:56:55 GMT"}, {"version": "v4", "created": "Mon, 25 Dec 2017 05:56:45 GMT"}, {"version": "v5", "created": "Wed, 4 Jul 2018 05:38:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Jin", "Xiaohan", ""], ["Qi", "Ye", ""], ["Wu", "Shangxuan", ""]]}, {"id": "1712.03452", "submitter": "Pulak Purkait", "authors": "Pulak Purkait, Cheng Zhao, Christopher Zach", "title": "SPP-Net: Deep Absolute Pose Regression with Synthetic Views", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image based localization is one of the important problems in computer vision\ndue to its wide applicability in robotics, augmented reality, and autonomous\nsystems. There is a rich set of methods described in the literature how to\ngeometrically register a 2D image w.r.t.\\ a 3D model. Recently, methods based\non deep (and convolutional) feedforward networks (CNNs) became popular for pose\nregression. However, these CNN-based methods are still less accurate than\ngeometry based methods despite being fast and memory efficient. In this work we\ndesign a deep neural network architecture based on sparse feature descriptors\nto estimate the absolute pose of an image. Our choice of using sparse feature\ndescriptors has two major advantages: first, our network is significantly\nsmaller than the CNNs proposed in the literature for this task---thereby making\nour approach more efficient and scalable. Second---and more importantly---,\nusage of sparse features allows to augment the training data with synthetic\nviewpoints, which leads to substantial improvements in the generalization\nperformance to unseen poses. Thus, our proposed method aims to combine the best\nof the two worlds---feature-based localization and CNN-based pose\nregression--to achieve state-of-the-art performance in the absolute pose\nestimation. A detailed analysis of the proposed architecture and a rigorous\nevaluation on the existing datasets are provided to support our method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 23:45:03 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Purkait", "Pulak", ""], ["Zhao", "Cheng", ""], ["Zach", "Christopher", ""]]}, {"id": "1712.03453", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu,\n  Srinath Sridhar, Gerard Pons-Moll, Christian Theobalt", "title": "Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB", "comments": "International Conference on 3D Vision (3DV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new single-shot method for multi-person 3D pose estimation in\ngeneral scenes from a monocular RGB camera. Our approach uses novel\nocclusion-robust pose-maps (ORPM) which enable full body pose inference even\nunder strong partial occlusions by other people and objects in the scene. ORPM\noutputs a fixed number of maps which encode the 3D joint locations of all\npeople in the scene. Body part associations allow us to infer 3D pose for an\narbitrary number of people without explicit bounding box prediction. To train\nour approach we introduce MuCo-3DHP, the first large scale training data set\nshowing real images of sophisticated multi-person interactions and occlusions.\nWe synthesize a large corpus of multi-person images by compositing images of\nindividual people (with ground truth from mutli-view performance capture). We\nevaluate our method on our new challenging 3D annotated multi-person test set\nMuPoTs-3D where we achieve state-of-the-art performance. To further stimulate\nresearch in multi-person 3D pose estimation, we will make our new datasets, and\nassociated code publicly available for research purposes.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 23:57:23 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 16:26:17 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 16:48:16 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Mehta", "Dushyant", ""], ["Sotnychenko", "Oleksandr", ""], ["Mueller", "Franziska", ""], ["Xu", "Weipeng", ""], ["Sridhar", "Srinath", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "1712.03474", "submitter": "Lingxiao Song", "authors": "Lingxiao Song, Zhihe Lu, Ran He, Zhenan Sun, Tieniu Tan", "title": "Geometry Guided Adversarial Facial Expression Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression synthesis has drawn much attention in the field of computer\ngraphics and pattern recognition. It has been widely used in face animation and\nrecognition. However, it is still challenging due to the high-level semantic\npresence of large and non-linear face geometry variations. This paper proposes\na Geometry-Guided Generative Adversarial Network (G2-GAN) for photo-realistic\nand identity-preserving facial expression synthesis. We employ facial geometry\n(fiducial points) as a controllable condition to guide facial texture synthesis\nwith specific expression. A pair of generative adversarial subnetworks are\njointly trained towards opposite tasks: expression removal and expression\nsynthesis. The paired networks form a mapping cycle between neutral expression\nand arbitrary expressions, which also facilitate other applications such as\nface transfer and expression invariant face recognition. Experimental results\nshow that our method can generate compelling perceptual results on various\nfacial expression synthesis databases. An expression invariant face recognition\nexperiment is also performed to further show the advantages of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 06:12:16 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Song", "Lingxiao", ""], ["Lu", "Zhihe", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1712.03491", "submitter": "Fanzi Wu", "authors": "Fanzi Wu, Songnan Li, Tianhao Zhao, King Ngi Ngan, Lv Sheng", "title": "3D Facial Expression Reconstruction using Cascaded Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel model fitting algorithm for 3D facial expression\nreconstruction from a single image. Face expression reconstruction from a\nsingle image is a challenging task in computer vision. Most state-of-the-art\nmethods fit the input image to a 3D Morphable Model (3DMM). These methods need\nto solve a stochastic problem and cannot deal with expression and pose\nvariations. To solve this problem, we adopt a 3D face expression model and use\na combined feature which is robust to scale, rotation and different lighting\nconditions. The proposed method applies a cascaded regression framework to\nestimate parameters for the 3DMM. 2D landmarks are detected and used to\ninitialize the 3D shape and mapping matrices. In each iteration, residues\nbetween the current 3DMM parameters and the ground truth are estimated and then\nused to update the 3D shapes. The mapping matrices are also calculated based on\nthe updated shapes and 2D landmarks. HOG features of the local patches and\ndisplacements between 3D landmark projections and 2D landmarks are exploited.\nCompared with existing methods, the proposed method is robust to expression and\npose changes and can reconstruct higher fidelity 3D face shape.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 09:53:21 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 10:44:48 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Wu", "Fanzi", ""], ["Li", "Songnan", ""], ["Zhao", "Tianhao", ""], ["Ngan", "King Ngi", ""], ["Sheng", "Lv", ""]]}, {"id": "1712.03534", "submitter": "Yong Man Ro", "authors": "Wissam J. Baddar, Geonmo Gu, Sangmin Lee and Yong Man Ro", "title": "Dynamics Transfer GAN: Generating Video by Transferring Arbitrary\n  Temporal Dynamics from a Source Video to a Single Target Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Dynamics Transfer GAN; a new method for generating\nvideo sequences based on generative adversarial learning. The spatial\nconstructs of a generated video sequence are acquired from the target image.\nThe dynamics of the generated video sequence are imported from a source video\nsequence, with arbitrary motion, and imposed onto the target image. To preserve\nthe spatial construct of the target image, the appearance of the source video\nsequence is suppressed and only the dynamics are obtained before being imposed\nonto the target image. That is achieved using the proposed appearance\nsuppressed dynamics feature. Moreover, the spatial and temporal consistencies\nof the generated video sequence are verified via two discriminator networks.\nOne discriminator validates the fidelity of the generated frames appearance,\nwhile the other validates the dynamic consistency of the generated video\nsequence. Experiments have been conducted to verify the quality of the video\nsequences generated by the proposed method. The results verified that Dynamics\nTransfer GAN successfully transferred arbitrary dynamics of the source video\nsequence onto a target image when generating the output video sequence. The\nexperimental results also showed that Dynamics Transfer GAN maintained the\nspatial constructs (appearance) of the target image while generating spatially\nand temporally consistent video sequences.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:18:26 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Baddar", "Wissam J.", ""], ["Gu", "Geonmo", ""], ["Lee", "Sangmin", ""], ["Ro", "Yong Man", ""]]}, {"id": "1712.03541", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support\n  Vector Machine (SVM) for Image Classification", "comments": "4 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) are similar to \"ordinary\" neural\nnetworks in the sense that they are made up of hidden layers consisting of\nneurons with \"learnable\" parameters. These neurons receive inputs, performs a\ndot product, and then follows it with a non-linearity. The whole network\nexpresses the mapping between raw image pixels and their class scores.\nConventionally, the Softmax function is the classifier used at the last layer\nof this network. However, there have been studies (Alalshekmubarak and Smith,\n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited\nstudies introduce the usage of linear support vector machine (SVM) in an\nartificial neural network architecture. This project is yet another take on the\nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the\nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST\ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax\nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both\nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao,\nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image\nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be\nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax\nreached a test accuracy of ~91.86%. The said results may be improved if data\npreprocessing techniques were employed on the datasets, and if the base CNN\nmodel was a relatively more sophisticated than the one used in this study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:50:28 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:25:08 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1712.03596", "submitter": "AmirAbbas Davari", "authors": "AmirAbbas Davari, Armin H\\\"aberle, Vincent Christlein, Andreas Maier,\n  Christian Riess", "title": "Sketch Layer Separation in Multi-Spectral Historical Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution imaging has delivered new prospects for detecting the\nmaterial composition and structure of cultural treasures. Despite the various\ntechniques for analysis, a significant diagnostic gap remained in the range of\navailable research capabilities for works on paper. Old master drawings were\nmostly composed in a multi-step manner with various materials. This resulted in\nthe overlapping of different layers which made the subjacent strata difficult\nto differentiate. The separation of stratified layers using imaging methods\ncould provide insights into the artistic work processes and help answer\nquestions about the object, its attribution, or in identifying forgeries. The\npattern recognition procedure was tested with mock replicas to achieve the\nseparation and the capability of displaying concealed red chalk under ink. In\ncontrast to RGB-sensor based imaging, the multi- or hyperspectral technology\nallows accurate layer separation by recording the characteristic signatures of\nthe material's reflectance. The risk of damage to the artworks as a result of\nthe examination can be reduced by using combinations of defined spectra for\nlightning and image capturing. By guaranteeing the maximum level of\nreadability, our results suggest that the technique can be applied to a broader\nrange of objects and assist in diagnostic research into cultural treasures in\nthe future.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 21:36:43 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Davari", "AmirAbbas", ""], ["H\u00e4berle", "Armin", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1712.03660", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Basem Assiri, Paul Rosen", "title": "Parallel Mapper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of Mapper has emerged in the last decade as a powerful and\neffective topological data analysis tool that approximates and generalizes\nother topological summaries, such as the Reeb graph, the contour tree, split,\nand joint trees. In this paper, we study the parallel analysis of the\nconstruction of Mapper. We give a provably correct parallel algorithm to\nexecute Mapper on multiple processors and discuss the performance results that\ncompare our approach to a reference sequential Mapper implementation. We report\nthe performance experiments that demonstrate the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 07:02:06 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 02:32:46 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 01:56:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Assiri", "Basem", ""], ["Rosen", "Paul", ""]]}, {"id": "1712.03686", "submitter": "Maria Perez-Ortiz", "authors": "Maria Perez-Ortiz and Rafal K. Mantiuk", "title": "A practical guide and software for analysing pairwise comparison\n  experiments", "comments": "Code available at https://github.com/mantiuk/pwcmp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular strategies to capture subjective judgments from humans involve\nthe construction of a unidimensional relative measurement scale, representing\norder preferences or judgments about a set of objects or conditions. This\ninformation is generally captured by means of direct scoring, either in the\nform of a Likert or cardinal scale, or by comparative judgments in pairs or\nsets. In this sense, the use of pairwise comparisons is becoming increasingly\npopular because of the simplicity of this experimental procedure. However, this\nstrategy requires non-trivial data analysis to aggregate the comparison ranks\ninto a quality scale and analyse the results, in order to take full advantage\nof the collected data. This paper explains the process of translating pairwise\ncomparison data into a measurement scale, discusses the benefits and\nlimitations of such scaling methods and introduces a publicly available\nsoftware in Matlab. We improve on existing scaling methods by introducing\noutlier analysis, providing methods for computing confidence intervals and\nstatistical testing and introducing a prior, which reduces estimation error\nwhen the number of observers is low. Most of our examples focus on image\nquality assessment.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 09:21:36 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 14:43:34 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Perez-Ortiz", "Maria", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1712.03687", "submitter": "Zexun Zhou", "authors": "Zexun Zhou, Zhongshi He, Ziyu Chen, Yuanyuan Jia, Haiyan Wang,\n  Jinglong Du, Dingding Chen", "title": "FHEDN: A based on context modeling Feature Hierarchy Encoder-Decoder\n  Network for face detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 09:27:14 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zhou", "Zexun", ""], ["He", "Zhongshi", ""], ["Chen", "Ziyu", ""], ["Jia", "Yuanyuan", ""], ["Wang", "Haiyan", ""], ["Du", "Jinglong", ""], ["Chen", "Dingding", ""]]}, {"id": "1712.03689", "submitter": "Andrea Asperti", "authors": "Andrea Asperti, Claudio Mastronardo", "title": "The Effectiveness of Data Augmentation for Detection of Gastrointestinal\n  Diseases from Endoscopical Images", "comments": null, "journal-ref": "Proceedings of the 5th International Conference on Bioimaging,\n  BIOIMAGING 2018, 19-21 January 2018, Funchal, Madeira - Portugal", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack, due to privacy concerns, of large public databases of medical\npathologies is a well-known and major problem, substantially hindering the\napplication of deep learning techniques in this field. In this article, we\ninvestigate the possibility to supply to the deficiency in the number of data\nby means of data augmentation techniques, working on the recent Kvasir dataset\nof endoscopical images of gastrointestinal diseases. The dataset comprises\n4,000 colored images labeled and verified by medical endoscopists, covering a\nfew common pathologies at different anatomical landmarks: Z-line, pylorus and\ncecum. We show how the application of data augmentation techniques allows to\nachieve sensible improvements of the classification with respect to previous\napproaches, both in terms of precision and recall.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 09:29:01 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Asperti", "Andrea", ""], ["Mastronardo", "Claudio", ""]]}, {"id": "1712.03727", "submitter": "Corneliu Florea", "authors": "Mihai Badea, Corneliu Florea, Laura Florea, Constantin Vertan", "title": "Can We Teach Computers to Understand Art? Domain Adaptation for\n  Enhancing Deep Networks Capacity to De-Abstract Art", "comments": "17 pages, 5 figures, 4 tables, preprint for journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans comprehend a natural scene at a single glance; painters and other\nvisual artists, through their abstract representations, stressed this capacity\nto the limit. The performance of computer vision solutions matched that of\nhumans in many problems of visual recognition. In this paper we address the\nproblem of recognizing the genre (subject) in digitized paintings using\nConvolutional Neural Networks (CNN) as part of the more general dealing with\nabstract and/or artistic representation of scenes. Initially we establish the\nstate of the art performance by training a CNN from scratch. In the next level\nof evaluation, we identify aspects that hinder the CNNs' recognition, such as\nartistic abstraction. Further, we test various domain adaptation methods that\ncould enhance the subject recognition capabilities of the CNNs. The evaluation\nis performed on a database of 80,000 annotated digitized paintings, which is\ntentatively extended with artistic photographs, either original or stylized, in\norder to emulate artistic representations. Surprisingly, the most efficient\ndomain adaptation is not the neural style transfer. Finally, the paper provides\nan experiment-based assessment of the abstraction level that CNNs are able to\nachieve.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 11:40:08 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Badea", "Mihai", ""], ["Florea", "Corneliu", ""], ["Florea", "Laura", ""], ["Vertan", "Constantin", ""]]}, {"id": "1712.03738", "submitter": "Ekta Vats", "authors": "Prashant Singh, Ekta Vats and Anders Hast", "title": "Learning Surrogate Models of Document Image Quality Metrics for\n  Automated Document Image Processing", "comments": null, "journal-ref": null, "doi": "10.1109/DAS.2018.14", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of document image quality metrics often depends upon the\navailability of a ground truth image corresponding to the document. This limits\nthe applicability of quality metrics in applications such as hyperparameter\noptimization of image processing algorithms that operate on-the-fly on unseen\ndocuments. This work proposes the use of surrogate models to learn the behavior\nof a given document quality metric on existing datasets where ground truth\nimages are available. The trained surrogate model can later be used to predict\nthe metric value on previously unseen document images without requiring access\nto ground truth images. The surrogate model is empirically evaluated on the\nDocument Image Binarization Competition (DIBCO) and the Handwritten Document\nImage Binarization Competition (H-DIBCO) datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 12:13:33 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Singh", "Prashant", ""], ["Vats", "Ekta", ""], ["Hast", "Anders", ""]]}, {"id": "1712.03742", "submitter": "Jaeyoon Yoo", "authors": "Jaeyoon Yoo, Yongjun Hong, YungKyun Noh and Sungroh Yoon", "title": "Domain Adaptation Using Adversarial Learning for Autonomous Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation has become an increasingly popular machine learning\napplication. Recent advances in deep learning have also resulted in great\nimprovements to autonomous navigation. However, prior outdoor autonomous\nnavigation depends on various expensive sensors or large amounts of real\nlabeled data which is difficult to acquire and sometimes erroneous. The\nobjective of this study is to train an autonomous navigation model that uses a\nsimulator (instead of real labeled data) and an inexpensive monocular camera.\nIn order to exploit the simulator satisfactorily, our proposed method is based\non domain adaptation with adversarial learning. Specifically, we propose our\nmodel with 1) a dilated residual block in the generator, 2) cycle loss, and 3)\nstyle loss to improve the adversarial learning performance for satisfactory\ndomain adaptation. In addition, we perform a theoretical analysis that supports\nthe justification of our proposed method. We present empirical results of\nnavigation in outdoor courses with various intersections using a commercial\nradio controlled car. We observe that our proposed method allows us to learn a\nfavorable navigation model by generating images with realistic textures. To the\nbest of our knowledge, this is the first work to apply domain adaptation with\nadversarial learning to autonomous navigation in real outdoor environments. Our\nproposed method can also be applied to precise image generation or other\nrobotic tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 12:23:06 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 11:40:59 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 17:11:50 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 06:12:01 GMT"}, {"version": "v5", "created": "Tue, 20 Feb 2018 06:02:51 GMT"}, {"version": "v6", "created": "Tue, 22 May 2018 02:33:38 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yoo", "Jaeyoon", ""], ["Hong", "Yongjun", ""], ["Noh", "YungKyun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1712.03747", "submitter": "Jose Bernal Moyano", "authors": "Jose Bernal, Kaisar Kushibar, Daniel S. Asfaw, Sergi Valverde, Arnau\n  Oliver, Robert Mart\\'i, Xavier Llad\\'o", "title": "Deep convolutional neural networks for brain image analysis on magnetic\n  resonance imaging: a review", "comments": null, "journal-ref": null, "doi": "10.1016/j.artmed.2018.08.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep convolutional neural networks (CNNs) have shown\nrecord-shattering performance in a variety of computer vision problems, such as\nvisual object recognition, detection and segmentation. These methods have also\nbeen utilised in medical image analysis domain for lesion segmentation,\nanatomical segmentation and classification. We present an extensive literature\nreview of CNN techniques applied in brain magnetic resonance imaging (MRI)\nanalysis, focusing on the architectures, pre-processing, data-preparation and\npost-processing strategies available in these works. The aim of this study is\nthree-fold. Our primary goal is to report how different CNN architectures have\nevolved, discuss state-of-the-art strategies, condense their results obtained\nusing public datasets and examine their pros and cons. Second, this paper is\nintended to be a detailed reference of the research activity in deep CNN for\nbrain MRI analysis. Finally, we present a perspective on the future of CNNs in\nwhich we hint some of the research directions in subsequent years.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 12:25:30 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 12:15:51 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 16:47:32 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bernal", "Jose", ""], ["Kushibar", "Kaisar", ""], ["Asfaw", "Daniel S.", ""], ["Valverde", "Sergi", ""], ["Oliver", "Arnau", ""], ["Mart\u00ed", "Robert", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1712.03781", "submitter": "Eunwoo Kim", "authors": "Eunwoo Kim, Chanho Ahn, Songhwai Oh", "title": "NestedNet: Learning Nested Sparse Structures in Deep Neural Networks", "comments": "To appear in CVPR 2018. Spotlight Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been increasing demands to construct compact deep\narchitectures to remove unnecessary redundancy and to improve the inference\nspeed. While many recent works focus on reducing the redundancy by eliminating\nunneeded weight parameters, it is not possible to apply a single deep\narchitecture for multiple devices with different resources. When a new device\nor circumstantial condition requires a new deep architecture, it is necessary\nto construct and train a new network from scratch. In this work, we propose a\nnovel deep learning framework, called a nested sparse network, which exploits\nan n-in-1-type nested structure in a neural network. A nested sparse network\nconsists of multiple levels of networks with a different sparsity ratio\nassociated with each level, and higher level networks share parameters with\nlower level networks to enable stable nested learning. The proposed framework\nrealizes a resource-aware versatile architecture as the same network can meet\ndiverse resource requirements. Moreover, the proposed nested network can learn\ndifferent forms of knowledge in its internal networks at different levels,\nenabling multiple tasks using a single network, such as coarse-to-fine\nhierarchical classification. In order to train the proposed nested sparse\nnetwork, we propose efficient weight connection learning and channel and layer\nscheduling strategies. We evaluate our network in multiple tasks, including\nadaptive deep compression, knowledge distillation, and learning class\nhierarchy, and demonstrate that nested sparse networks perform competitively,\nbut more efficiently, compared to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 14:09:06 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 04:44:56 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kim", "Eunwoo", ""], ["Ahn", "Chanho", ""], ["Oh", "Songhwai", ""]]}, {"id": "1712.03792", "submitter": "Wei Cui", "authors": "Yaoguang Li, Wei Cui, and Cong Wang", "title": "Identifying the Mislabeled Training Samples of ECG Signals using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classification accuracy of electrocardiogram signal is often affected by\ndiverse factors in which mislabeled training samples issue is one of the most\ninfluential problems. In order to mitigate this negative effect, the method of\ncross validation is introduced to identify the mislabeled samples. The method\nutilizes the cooperative advantages of different classifiers to act as a filter\nfor the training samples. The filter removes the mislabeled training samples\nand retains the correctly labeled ones with the help of 10-fold cross\nvalidation. Consequently, a new training set is provided to the final\nclassifiers to acquire higher classification accuracies. Finally, we\nnumerically show the effectiveness of the proposed method with the MIT-BIH\narrhythmia database.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 14:22:56 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Li", "Yaoguang", ""], ["Cui", "Wei", ""], ["Wang", "Cong", ""]]}, {"id": "1712.03812", "submitter": "Yu-Hui Huang", "authors": "Yu-Hui Huang, Xu Jia, Stamatios Georgoulis, Tinne Tuytelaars, Luc Van\n  Gool", "title": "Error Correction for Dense Semantic Image Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pixelwise semantic image labeling is an important, yet challenging, task with\nmany applications. Typical approaches to tackle this problem involve either the\ntraining of deep networks on vast amounts of images to directly infer the\nlabels or the use of probabilistic graphical models to jointly model the\ndependencies of the input (i.e. images) and output (i.e. labels). Yet, the\nformer approaches do not capture the structure of the output labels, which is\ncrucial for the performance of dense labeling, and the latter rely on carefully\nhand-designed priors that require costly parameter tuning via optimization\ntechniques, which in turn leads to long inference times. To alleviate these\nrestrictions, we explore how to arrive at dense semantic pixel labels given\nboth the input image and an initial estimate of the output labels. We propose a\nparallel architecture that: 1) exploits the context information through a\nLabelPropagation network to propagate correct labels from nearby pixels to\nimprove the object boundaries, 2) uses a LabelReplacement network to directly\nreplace possibly erroneous, initial labels with new ones, and 3) combines the\ndifferent intermediate results via a Fusion network to obtain the final\nper-pixel label. We experimentally validate our approach on two different\ndatasets for the semantic segmentation and face parsing tasks respectively,\nwhere we show improvements over the state-of-the-art. We also provide both a\nquantitative and qualitative analysis of the generated results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:11:01 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Huang", "Yu-Hui", ""], ["Jia", "Xu", ""], ["Georgoulis", "Stamatios", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.03825", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Yu Hin Lai and Lok Ming Lui", "title": "Variational models for joint subsampling and reconstruction of\n  turbulence-degraded images", "comments": "arXiv admin note: text overlap with arXiv:1704.03140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Turbulence-degraded image frames are distorted by both turbulent deformations\nand space-time-varying blurs. To suppress these effects, we propose a\nmulti-frame reconstruction scheme to recover a latent image from the observed\nimage sequence. Recent approaches are commonly based on registering each frame\nto a reference image, by which geometric turbulent deformations can be\nestimated and a sharp image can be restored. A major challenge is that a fine\nreference image is usually unavailable, as every turbulence-degraded frame is\ndistorted. A high-quality reference image is crucial for the accurate\nestimation of geometric deformations and fusion of frames. Besides, it is\nunlikely that all frames from the image sequence are useful, and thus frame\nselection is necessary and highly beneficial. In this work, we propose a\nvariational model for joint subsampling of frames and extraction of a clear\nimage. A fine image and a suitable choice of subsample are simultaneously\nobtained by iteratively reducing an energy functional. The energy consists of a\nfidelity term measuring the discrepancy between the extracted image and the\nsubsampled frames, as well as regularization terms on the extracted image and\nthe subsample. Different choices of fidelity and regularization terms are\nexplored. By carefully selecting suitable frames and extracting the image, the\nquality of the reconstructed image can be significantly improved. Extensive\nexperiments have been carried out, which demonstrate the efficacy of our\nproposed model. In addition, the extracted subsamples and images can be put in\nexisting algorithms to produce improved results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 09:41:24 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Lau", "Chun Pong", ""], ["Lai", "Yu Hin", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1712.03835", "submitter": "Matthias Meyer", "authors": "Matthias Meyer, Jan Beutel, Lothar Thiele", "title": "Unsupervised Feature Learning for Audio Analysis", "comments": "Presented at the 5th International Conference on Learning\n  Representations (ICLR) 2017, Workshop Track, Toulon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying acoustic events from a continuously streaming audio source is of\ninterest for many applications including environmental monitoring for basic\nresearch. In this scenario neither different event classes are known nor what\ndistinguishes one class from another. Therefore, an unsupervised feature\nlearning method for exploration of audio data is presented in this paper. It\nincorporates the two following novel contributions: First, an audio frame\npredictor based on a Convolutional LSTM autoencoder is demonstrated, which is\nused for unsupervised feature extraction. Second, a training method for\nautoencoders is presented, which leads to distinct features by amplifying event\nsimilarities. In comparison to standard approaches, the features extracted from\nthe audio frame predictor trained with the novel approach show 13 % better\nresults when used with a classifier and 36 % better results when used for\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:25:01 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Meyer", "Matthias", ""], ["Beutel", "Jan", ""], ["Thiele", "Lothar", ""]]}, {"id": "1712.03866", "submitter": "Iason Oikonomidis", "authors": "Paschalis Panteleris, Iason Oikonomidis, Antonis Argyros", "title": "Using a single RGB frame for real time 3D hand pose estimation in the\n  wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the real-time estimation of the full 3D pose of one\nor more human hands using a single commodity RGB camera. Recent work in the\narea has displayed impressive progress using RGBD input. However, since the\nintroduction of RGBD sensors, there has been little progress for the case of\nmonocular color input. We capitalize on the latest advancements of deep\nlearning, combining them with the power of generative hand pose estimation\ntechniques to achieve real-time monocular 3D hand pose estimation in\nunrestricted scenarios. More specifically, given an RGB image and the relevant\ncamera calibration information, we employ a state-of-the-art detector to\nlocalize hands. Given a crop of a hand in the image, we run the pretrained\nnetwork of OpenPose for hands to estimate the 2D location of hand joints.\nFinally, non-linear least-squares minimization fits a 3D model of the hand to\nthe estimated 2D joint positions, recovering the 3D hand pose. Extensive\nexperimental results provide comparison to the state of the art as well as\nqualitative assessment of the method in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 16:16:05 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Panteleris", "Paschalis", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "1712.03878", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Gundeep Arora, Ashish Mishra, Piyush Rai", "title": "Generalized Zero-Shot Learning via Synthesized Examples", "comments": "Accepted in CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative framework for generalized zero-shot learning where\nthe training and test classes are not necessarily disjoint. Built upon a\nvariational autoencoder based architecture, consisting of a probabilistic\nencoder and a probabilistic conditional decoder, our model can generate novel\nexemplars from seen/unseen classes, given their respective class attributes.\nThese exemplars can subsequently be used to train any off-the-shelf\nclassification model. One of the key aspects of our encoder-decoder\narchitecture is a feedback-driven mechanism in which a discriminator (a\nmultivariate regressor) learns to map the generated exemplars to the\ncorresponding class attribute vectors, leading to an improved generator. Our\nmodel's ability to generate and leverage examples from unseen classes to train\nthe classification model naturally helps to mitigate the bias towards\npredicting seen classes in generalized zero-shot learning settings. Through a\ncomprehensive set of experiments, we show that our model outperforms several\nstate-of-the-art methods, on several benchmark datasets, for both standard as\nwell as generalized zero-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 16:44:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 09:19:01 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 11:20:15 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 16:55:14 GMT"}, {"version": "v5", "created": "Tue, 12 Jun 2018 00:13:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Arora", "Gundeep", ""], ["Mishra", "Ashish", ""], ["Rai", "Piyush", ""]]}, {"id": "1712.03897", "submitter": "Kenneth Leidal", "authors": "Kenneth Leidal, David Harwath, and James Glass", "title": "Learning Modality-Invariant Representations for Speech and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the unsupervised learning of a semantic embedding\nspace for co-occurring sensory inputs. Specifically, we focus on the task of\nlearning a semantic vector space for both spoken and handwritten digits using\nthe TIDIGITs and MNIST datasets. Current techniques encode image and\naudio/textual inputs directly to semantic embeddings. In contrast, our\ntechnique maps an input to the mean and log variance vectors of a diagonal\nGaussian from which sample semantic embeddings are drawn. In addition to\nencouraging semantic similarity between co-occurring inputs,our loss function\nincludes a regularization term borrowed from variational autoencoders (VAEs)\nwhich drives the posterior distributions over embeddings to be unit Gaussian.\nWe can use this regularization term to filter out modality information while\npreserving semantic information. We speculate this technique may be more\nbroadly applicable to other areas of cross-modality/domain information\nretrieval and transfer learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 17:18:34 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Leidal", "Kenneth", ""], ["Harwath", "David", ""], ["Glass", "James", ""]]}, {"id": "1712.03904", "submitter": "Mahdi Rad", "authors": "Mahdi Rad, Markus Oberweger, Vincent Lepetit", "title": "Feature Mapping for Learning Fast and Accurate 3D Pose Inference from\n  Synthetic Images", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and efficient method for exploiting synthetic images when\ntraining a Deep Network to predict a 3D pose from an image. The ability of\nusing synthetic images for training a Deep Network is extremely valuable as it\nis easy to create a virtually infinite training set made of such images, while\ncapturing and annotating real images can be very cumbersome. However, synthetic\nimages do not resemble real images exactly, and using them for training can\nresult in suboptimal performance. It was recently shown that for exemplar-based\napproaches, it is possible to learn a mapping from the exemplar representations\nof real images to the exemplar representations of synthetic images. In this\npaper, we show that this approach is more general, and that a network can also\nbe applied after the mapping to infer a 3D pose: At run time, given a real\nimage of the target object, we first compute the features for the image, map\nthem to the feature space of synthetic images, and finally use the resulting\nfeatures as input to another network which predicts the 3D pose. Since this\nnetwork can be trained very effectively by using synthetic images, it performs\nvery well in practice, and inference is faster and more accurate than with an\nexemplar-based approach. We demonstrate our approach on the LINEMOD dataset for\n3D object pose estimation from color images, and the NYU dataset for 3D hand\npose estimation from depth maps. We show that it allows us to outperform the\nstate-of-the-art on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 17:27:49 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 15:56:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rad", "Mahdi", ""], ["Oberweger", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1712.03917", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Guillermo Garcia-Hernando, Bjorn Stenger, Gyeongsik\n  Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo Molchanov, Jan Kautz, Sina Honari,\n  Liuhao Ge, Junsong Yuan, Xinghao Chen, Guijin Wang, Fan Yang, Kai Akiyama,\n  Yang Wu, Qingfu Wan, Meysam Madadi, Sergio Escalera, Shile Li, Dongheui Lee,\n  Iason Oikonomidis, Antonis Argyros, Tae-Kyun Kim", "title": "Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future\n  Goals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we strive to answer two questions: What is the current state\nof 3D hand pose estimation from depth images? And, what are the next challenges\nthat need to be tackled? Following the successful Hands In the Million\nChallenge (HIM2017), we investigate the top 10 state-of-the-art methods on\nthree tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose\nestimation during object interaction. We analyze the performance of different\nCNN structures with regard to hand shape, joint visibility, view point and\narticulation distributions. Our findings include: (1) isolated 3D hand pose\nestimation achieves low mean errors (10 mm) in the view point range of [70,\n120] degrees, but it is far from being solved for extreme view points; (2) 3D\nvolumetric representations outperform 2D CNNs, better capturing the spatial\nstructure of the depth data; (3) Discriminative methods still generalize poorly\nto unseen hand shapes; (4) While joint occlusions pose a challenge for most\nmethods, explicit modeling of structure constraints can significantly narrow\nthe gap between errors on visible and occluded joints.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 17:55:19 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 14:31:37 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Yuan", "Shanxin", ""], ["Garcia-Hernando", "Guillermo", ""], ["Stenger", "Bjorn", ""], ["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""], ["Molchanov", "Pavlo", ""], ["Kautz", "Jan", ""], ["Honari", "Sina", ""], ["Ge", "Liuhao", ""], ["Yuan", "Junsong", ""], ["Chen", "Xinghao", ""], ["Wang", "Guijin", ""], ["Yang", "Fan", ""], ["Akiyama", "Kai", ""], ["Wu", "Yang", ""], ["Wan", "Qingfu", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""], ["Li", "Shile", ""], ["Lee", "Dongheui", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1712.03931", "submitter": "Manolis Savva", "authors": "Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser,\n  Vladlen Koltun", "title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex\n  Environments", "comments": "MINOS is a simulator designed to support research on end-to-end\n  navigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MINOS, a simulator designed to support the development of\nmultisensory models for goal-directed navigation in complex indoor\nenvironments. The simulator leverages large datasets of complex 3D environments\nand supports flexible configuration of multimodal sensor suites. We use MINOS\nto benchmark deep-learning-based navigation methods, to analyze the influence\nof environmental complexity on navigation performance, and to carry out a\ncontrolled study of multimodality in sensorimotor learning. The experiments\nshow that current deep reinforcement learning approaches fail in large\nrealistic environments. The experiments also indicate that multimodality is\nbeneficial in learning to navigate cluttered scenes. MINOS is released\nopen-source to the research community at http://minosworld.org . A video that\nshows MINOS can be found at https://youtu.be/c0mL9K64q84\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 18:24:58 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Dosovitskiy", "Alexey", ""], ["Funkhouser", "Thomas", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1712.03942", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Aran Khanna, Anima Anandkumar", "title": "StrassenNets: Deep Learning with a Multiplication Budget", "comments": "ICML 2018. Code available at https://github.com/mitscha/strassennets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of the arithmetic operations required to evaluate deep\nneural networks (DNNs) consists of matrix multiplications, in both convolution\nand fully connected layers. We perform end-to-end learning of low-cost\napproximations of matrix multiplications in DNN layers by casting matrix\nmultiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits)\nand learning their (ternary) edge weights from data. The SPNs disentangle\nmultiplication and addition operations and enable us to impose a budget on the\nnumber of multiplication operations. Combining our method with knowledge\ndistillation and applying it to image classification DNNs (trained on ImageNet)\nand language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction\nin number of multiplications (over 99.5%) while maintaining the predictive\nperformance of the full-precision models. Finally, we demonstrate that the\nproposed framework is able to rediscover Strassen's matrix multiplication\nalgorithm, learning to multiply $2 \\times 2$ matrices using only 7\nmultiplications instead of 8.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 18:49:07 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 12:59:10 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 10:59:23 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Tschannen", "Michael", ""], ["Khanna", "Aran", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1712.03991", "submitter": "Jianshu Zhang", "authors": "Jianshu Zhang, Jun Du, Lirong Dai", "title": "A GRU-based Encoder-Decoder Approach with Attention for Online\n  Handwritten Mathematical Expression Recognition", "comments": "Accepted by ICDAR 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a novel end-to-end approach based on the\nencoder-decoder framework with the attention mechanism for online handwritten\nmathematical expression recognition (OHMER). First, the input two-dimensional\nink trajectory information of handwritten expression is encoded via the gated\nrecurrent unit based recurrent neural network (GRU-RNN). Then the decoder is\nalso implemented by the GRU-RNN with a coverage-based attention model. The\nproposed approach can simultaneously accomplish the symbol recognition and\nstructural analysis to output a character sequence in LaTeX format. Validated\non the CROHME 2014 competition task, our approach significantly outperforms the\nstate-of-the-art with an expression recognition accuracy of 52.43% by only\nusing the official training dataset. Furthermore, the alignments between the\ninput trajectories of handwritten expressions and the output LaTeX sequences\nare visualized by the attention mechanism to show the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 02:20:25 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Zhang", "Jianshu", ""], ["Du", "Jun", ""], ["Dai", "Lirong", ""]]}, {"id": "1712.03999", "submitter": "Brian Dolhansky", "authors": "Brian Dolhansky, Cristian Canton Ferrer", "title": "Eye In-Painting with Exemplar Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to in-painting where the identity of\nthe object to remove or change is preserved and accounted for at inference\ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize\nexemplar information to produce high-quality, personalized in painting results.\nWe propose using exemplar information in the form of a reference image of the\nregion to in-paint, or a perceptual code describing that object. Unlike\nprevious conditional GAN formulations, this extra information can be inserted\nat multiple points within the adversarial network, thus increasing its\ndescriptive power. We show that ExGANs can produce photo-realistic personalized\nin-painting results that are both perceptually and semantically plausible by\napplying them to the task of closed to-open eye in-painting in natural\npictures. A new benchmark dataset is also introduced for the task of eye\nin-painting for future comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 19:40:55 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Dolhansky", "Brian", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "1712.04006", "submitter": "Alexander Bagnall", "authors": "Alexander Bagnall, Razvan Bunescu, Gordon Stewart", "title": "Training Ensembles to Detect Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new ensemble method for detecting and classifying adversarial\nexamples generated by state-of-the-art attacks, including DeepFool and C&W. Our\nmethod works by training the members of an ensemble to have low classification\nerror on random benign examples while simultaneously minimizing agreement on\nexamples outside the training distribution. We evaluate on both MNIST and\nCIFAR-10, against oblivious and both white- and black-box adversaries.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 20:30:11 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Bagnall", "Alexander", ""], ["Bunescu", "Razvan", ""], ["Stewart", "Gordon", ""]]}, {"id": "1712.04008", "submitter": "Michael Bernico", "authors": "Michael Bernico, Yuntao Li, Dingchao Zhang", "title": "Investigating the Impact of Data Volume and Domain Similarity on\n  Transfer Learning Applications", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning allows practitioners to recognize and apply knowledge\nlearned in previous tasks (source task) to new tasks or new domains (target\ntask), which share some commonality. The two important factors impacting the\nperformance of transfer learning models are: (a) the size of the target\ndataset, and (b) the similarity in distribution between source and target\ndomains. Thus far, there has been little investigation into just how important\nthese factors are. In this paper, we investigate the impact of target dataset\nsize and source/target domain similarity on model performance through a series\nof experiments. We find that more data is always beneficial, and model\nperformance improves linearly with the log of data size, until we are out of\ndata. As source/target domains differ, more data is required and fine tuning\nwill render better performance than feature extraction. When source/target\ndomains are similar and data size is small, fine tuning and feature extraction\nrenders equivalent performance. Our hope is that by beginning this quantitative\ninvestigation on the effect of data volume and domain similarity in transfer\nlearning we might inspire others to explore the significance of data in\ndeveloping more accurate statistical models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 20:30:44 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 18:14:44 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 19:09:19 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 19:35:08 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Bernico", "Michael", ""], ["Li", "Yuntao", ""], ["Zhang", "Dingchao", ""]]}, {"id": "1712.04046", "submitter": "Jason Poulos", "authors": "Jason Poulos and Rafael Valle", "title": "Character-Based Handwritten Text Transcription with Attention Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-021-05813-1", "report-no": null, "categories": "cs.CV cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper approaches the task of handwritten text recognition (HTR) with\nattentional encoder-decoder networks trained on sequences of characters, rather\nthan words. We experiment on lines of text from popular handwriting datasets\nand compare different activation functions for the attention mechanism used for\naligning image pixels and target characters. We find that softmax attention\nfocuses heavily on individual characters, while sigmoid attention focuses on\nmultiple characters at each step of the decoding. When the sequence alignment\nis one-to-one, softmax attention is able to learn a more precise alignment at\neach step of the decoding, whereas the alignment generated by sigmoid attention\nis much less precise. When a linear function is used to obtain attention\nweights, the model predicts a character by looking at the entire sequence of\ncharacters and performs poorly because it lacks a precise alignment between the\nsource and target. Future research may explore HTR in natural scene images,\nsince the model is capable of transcribing handwritten text without the need\nfor producing segmentations or bounding boxes of text in images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 21:57:03 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 19:33:31 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 17:00:03 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Poulos", "Jason", ""], ["Valle", "Rafael", ""]]}, {"id": "1712.04083", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Kristen Grauman", "title": "Learning Compressible 360{\\deg} Video Isomers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard video encoders developed for conventional narrow field-of-view video\nare widely applied to 360{\\deg} video as well, with reasonable results.\nHowever, while this approach commits arbitrarily to a projection of the\nspherical frames, we observe that some orientations of a 360{\\deg} video, once\nprojected, are more compressible than others. We introduce an approach to\npredict the sphere rotation that will yield the maximal compression rate. Given\nvideo clips in their original encoding, a convolutional neural network learns\nthe association between a clip's visual content and its compressibility at\ndifferent rotations of a cubemap projection. Given a novel video, our\nlearning-based approach efficiently infers the most compressible direction in\none shot, without repeated rendering and compression of the source video. We\nvalidate our idea on thousands of video clips and multiple popular video\ncodecs. The results show that this untapped dimension of 360{\\deg} compression\nhas substantial potential--\"good\" rotations are typically 8-10% more\ncompressible than bad ones, and our learning approach can predict them reliably\n82% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 00:35:23 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1712.04109", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Bo Xiong, Kristen Grauman", "title": "Im2Flow: Motion Hallucination from Static Images for Action Recognition", "comments": "Published in CVPR 2018, project page:\n  http://vision.cs.utexas.edu/projects/im2flow/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods to recognize actions in static images take the images at\ntheir face value, learning the appearances---objects, scenes, and body\nposes---that distinguish each action class. However, such models are deprived\nof the rich dynamic structure and motions that also define human activity. We\npropose an approach that hallucinates the unobserved future motion implied by a\nsingle snapshot to help static-image action recognition. The key idea is to\nlearn a prior over short-term dynamics from thousands of unlabeled videos,\ninfer the anticipated optical flow on novel static images, and then train\ndiscriminative models that exploit both streams of information. Our main\ncontributions are twofold. First, we devise an encoder-decoder convolutional\nneural network and a novel optical flow encoding that can translate a static\nimage into an accurate flow map. Second, we show the power of hallucinated flow\nfor recognition, successfully transferring the learned motion into a standard\ntwo-stream network for activity recognition. On seven datasets, we demonstrate\nthe power of the approach. It not only achieves state-of-the-art accuracy for\ndense optical flow prediction, but also consistently enhances recognition of\nactions and dynamic scenes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 03:11:34 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 22:08:17 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 16:18:10 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Gao", "Ruohan", ""], ["Xiong", "Bo", ""], ["Grauman", "Kristen", ""]]}, {"id": "1712.04119", "submitter": "Junshen Xu", "authors": "Junshen Xu, Enhao Gong, John Pauly, Greg Zaharchuk", "title": "200x Low-dose PET Reconstruction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron emission tomography (PET) is widely used in various clinical\napplications, including cancer diagnosis, heart disease and neuro disorders.\nThe use of radioactive tracer in PET imaging raises concerns due to the risk of\nradiation exposure. To minimize this potential risk in PET imaging, efforts\nhave been made to reduce the amount of radio-tracer usage. However, lowing dose\nresults in low Signal-to-Noise-Ratio (SNR) and loss of information, both of\nwhich will heavily affect clinical diagnosis. Besides, the ill-conditioning of\nlow-dose PET image reconstruction makes it a difficult problem for iterative\nreconstruction algorithms. Previous methods proposed are typically complicated\nand slow, yet still cannot yield satisfactory results at significantly low\ndose. Here, we propose a deep learning method to resolve this issue with an\nencoder-decoder residual deep network with concatenate skip connections.\nExperiments shows the proposed method can reconstruct low-dose PET image to a\nstandard-dose quality with only two-hundredth dose. Different cost functions\nfor training model are explored. Multi-slice input strategy is introduced to\nprovide the network with more structural information and make it more robust to\nnoise. Evaluation on ultra-low-dose clinical data shows that the proposed\nmethod can achieve better result than the state-of-the-art methods and\nreconstruct images with comparable quality using only 0.5% of the original\nregular dose.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:13:27 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Xu", "Junshen", ""], ["Gong", "Enhao", ""], ["Pauly", "John", ""], ["Zaharchuk", "Greg", ""]]}, {"id": "1712.04138", "submitter": "Shuang Liu", "authors": "Shuang Liu, Mete Ozay, Takayuki Okatani, Hongli Xu, Kai Sun and Yang\n  Lin", "title": "A vision based system for underwater docking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous underwater vehicles (AUVs) have been deployed for underwater\nexploration. However, its potential is confined by its limited on-board battery\nenergy and data storage capacity. This problem has been addressed using docking\nsystems by underwater recharging and data transfer for AUVs. In this work, we\npropose a vision based framework for underwater docking following these\nsystems. The proposed framework comprises two modules; (i) a detection module\nwhich provides location information on underwater docking stations in 2D images\ncaptured by an on-board camera, and (ii) a pose estimation module which\nrecovers the relative 3D position and orientation between docking stations and\nAUVs from the 2D images. For robust and credible detection of docking stations,\nwe propose a convolutional neural network called Docking Neural Network (DoNN).\nFor accurate pose estimation, a perspective-n-point algorithm is integrated\ninto our framework. In order to examine our framework in underwater docking\ntasks, we collected a dataset of 2D images, named Underwater Docking Images\nDataset (UDID), in an experimental water pool. To the best of our knowledge,\nUDID is the first publicly available underwater docking dataset. In the\nexperiments, we first evaluate performance of the proposed detection module on\nUDID and its deformed variations. Next, we assess the accuracy of the pose\nestimation module by ground experiments, since it is not feasible to obtain\ntrue relative position and orientation between docking stations and AUVs under\nwater. Then, we examine the pose estimation module by underwater experiments in\nour experimental water pool. Experimental results show that the proposed\nframework can be used to detect docking stations and estimate their relative\npose efficiently and successfully, compared to the state-of-the-art baseline\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 05:59:49 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Liu", "Shuang", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""], ["Xu", "Hongli", ""], ["Sun", "Kai", ""], ["Lin", "Yang", ""]]}, {"id": "1712.04139", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Hatice Ceylan Koydemir, Hongda Wang, Zhensong Wei,\n  Zhengshuang Ren, Harun Gunaydin, Yibo Zhang, Zoltan Gorocs, Kyle Liang, Derek\n  Tseng, Aydogan Ozcan", "title": "Deep learning enhanced mobile-phone microscopy", "comments": null, "journal-ref": "ACS Photonics (2018)", "doi": "10.1021/acsphotonics.8b00146", "report-no": null, "categories": "cs.LG cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile-phones have facilitated the creation of field-portable, cost-effective\nimaging and sensing technologies that approach laboratory-grade instrument\nperformance. However, the optical imaging interfaces of mobile-phones are not\ndesigned for microscopy and produce spatial and spectral distortions in imaging\nmicroscopic specimens. Here, we report on the use of deep learning to correct\nsuch distortions introduced by mobile-phone-based microscopes, facilitating the\nproduction of high-resolution, denoised and colour-corrected images, matching\nthe performance of benchtop microscopes with high-end objective lenses, also\nextending their limited depth-of-field. After training a convolutional neural\nnetwork, we successfully imaged various samples, including blood smears,\nhistopathology tissue sections, and parasites, where the recorded images were\nhighly compressed to ease storage and transmission for telemedicine\napplications. This method is applicable to other low-cost, aberrated imaging\nsystems, and could offer alternatives for costly and bulky microscopes, while\nalso providing a framework for standardization of optical images for clinical\nand biomedical applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:03:27 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Rivenson", "Yair", ""], ["Koydemir", "Hatice Ceylan", ""], ["Wang", "Hongda", ""], ["Wei", "Zhensong", ""], ["Ren", "Zhengshuang", ""], ["Gunaydin", "Harun", ""], ["Zhang", "Yibo", ""], ["Gorocs", "Zoltan", ""], ["Liang", "Kyle", ""], ["Tseng", "Derek", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1712.04142", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, Pheng-Ann Heng", "title": "Direction-aware Spatial Context Features for Shadow Detection", "comments": "Accepted for oral presentation in CVPR 2018. The journal version of\n  this paper is arXiv:1805.04635", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  pp. 7454-7462, 2018", "doi": "10.1109/CVPR.2018.00778", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow detection is a fundamental and challenging task, since it requires an\nunderstanding of global image semantics and there are various backgrounds\naround shadows. This paper presents a novel network for shadow detection by\nanalyzing image context in a direction-aware manner. To achieve this, we first\nformulate the direction-aware attention mechanism in a spatial recurrent neural\nnetwork (RNN) by introducing attention weights when aggregating spatial context\nfeatures in the RNN. By learning these weights through training, we can recover\ndirection-aware spatial context (DSC) for detecting shadows. This design is\ndeveloped into the DSC module and embedded in a CNN to learn DSC features at\ndifferent levels. Moreover, a weighted cross entropy loss is designed to make\nthe training more effective. We employ two common shadow detection benchmark\ndatasets and perform various experiments to evaluate our network. Experimental\nresults show that our network outperforms state-of-the-art methods and achieves\n97% accuracy and 38% reduction on balance error rate.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:29:51 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 11:28:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Xiaowei", ""], ["Zhu", "Lei", ""], ["Fu", "Chi-Wing", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1712.04143", "submitter": "Boyi Li", "authors": "Boyi Li and Wenqi Ren and Dengpan Fu and Dacheng Tao and Dan Feng and\n  Wenjun Zeng and Zhangyang Wang", "title": "Benchmarking Single Image Dehazing and Beyond", "comments": "IEEE Transactions on Image Processing(TIP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study and evaluation of existing single image\ndehazing algorithms, using a new large-scale benchmark consisting of both\nsynthetic and real-world hazy images, called REalistic Single Image DEhazing\n(RESIDE). RESIDE highlights diverse data sources and image contents, and is\ndivided into five subsets, each serving different training or evaluation\npurposes. We further provide a rich variety of criteria for dehazing algorithm\nevaluation, ranging from full-reference metrics, to no-reference metrics, to\nsubjective evaluation and the novel task-driven evaluation. Experiments on\nRESIDE shed light on the comparisons and limitations of state-of-the-art\ndehazing algorithms, and suggest promising future directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:33:20 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:27:35 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 14:33:39 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 00:13:07 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Boyi", ""], ["Ren", "Wenqi", ""], ["Fu", "Dengpan", ""], ["Tao", "Dacheng", ""], ["Feng", "Dan", ""], ["Zeng", "Wenjun", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1712.04248", "submitter": "Jonas Rauber", "authors": "Wieland Brendel, Jonas Rauber, Matthias Bethge", "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models", "comments": "Published as a conference paper at the Sixth International Conference\n  on Learning Representations (ICLR 2018)\n  https://openreview.net/forum?id=SyZI0GWCZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox .\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 11:36:26 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 14:40:42 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1712.04322", "submitter": "Francois Berry", "authors": "Kamel Abdelouahab (IP), Maxime Pelcat (IETR), Jocelyn S\\'erot (IP),\n  C\\'edric Bourrasset (IP), Fran\\c{c}ois Berry (IP), Jocelyn Serot (LASMEA)", "title": "Tactics to Directly Map CNN graphs on Embedded FPGAs", "comments": "IEEE Embedded Systems Letters, Institute of Electrical and\n  Electronics Engineers, A Para\\^itre, pp.1 - 1. arXiv admin note: text overlap\n  with arXiv:1705.04543", "journal-ref": null, "doi": "10.1109/LES.2017.2743247", "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are the state-of-the-art in image\nclassification. Since CNN feed forward propagation involves highly regular\nparallel computation, it benefits from a significant speed-up when running on\nfine grain parallel programmable logic devices. As a consequence, several\nstudies have proposed FPGA-based accelerators for CNNs. However, because of the\nlarge computationalpower required by CNNs, none of the previous studies has\nproposed a direct mapping of the CNN onto the physical resources of an FPGA,\nallocating each processing actor to its own hardware instance.In this paper, we\ndemonstrate the feasibility of the so called direct hardware mapping (DHM) and\ndiscuss several tactics we explore to make DHM usable in practice. As a proof\nof concept, we introduce the HADDOC2 open source tool, that automatically\ntransforms a CNN description into a synthesizable hardware description with\nplatform-independent direct hardware mapping.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:13:39 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Abdelouahab", "Kamel", "", "IP"], ["Pelcat", "Maxime", "", "IETR"], ["S\u00e9rot", "Jocelyn", "", "IP"], ["Bourrasset", "C\u00e9dric", "", "IP"], ["Berry", "Fran\u00e7ois", "", "IP"], ["Serot", "Jocelyn", "", "LASMEA"]]}, {"id": "1712.04391", "submitter": "Rinat Mukhometzianov", "authors": "Rinat Mukhometzianov, Ying Wang", "title": "Review. Machine learning techniques for traffic sign detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic road sign detection system localizes road signs from within\nimages captured by an on-board camera of a vehicle, and support the driver to\nproperly ride the vehicle. Most existing algorithms include a preprocessing\nstep, feature extraction and detection step. This paper arranges the methods\napplied to road sign detection into two groups: general machine learning,\nneural networks. In this review, the issues related to automatic road sign\ndetection are addressed, the popular existing methods developed to tackle the\nroad sign detection problem are reviewed, and a comparison of the features of\nthese methods is composed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 17:00:29 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 04:49:56 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Mukhometzianov", "Rinat", ""], ["Wang", "Ying", ""]]}, {"id": "1712.04407", "submitter": "Alexander Sage", "authors": "Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2018.00616", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a logo for a new brand is a lengthy and tedious back-and-forth\nprocess between a designer and a client. In this paper we explore to what\nextent machine learning can solve the creative task of the designer. For this,\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\nmulti-modal data is not straightforward and results in mode collapse for some\nstate-of-the-art methods. We propose the use of synthetic labels obtained\nthrough clustering to disentangle and stabilize GAN training. We are able to\ngenerate a high diversity of plausible logos and we demonstrate latent space\nexploration techniques to ease the logo design task in an interactive manner.\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\nachieving state-of-the-art Inception scores when using synthetic labels\nobtained via clustering the features of an ImageNet classifier. GANs can cope\nwith multi-modal data by means of synthetic labels achieved through clustering,\nand our results show the creative potential of such techniques for logo\nsynthesis and manipulation. Our dataset and models will be made publicly\navailable at https://data.vision.ee.ethz.ch/cvl/lld/.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 17:51:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sage", "Alexander", ""], ["Agustsson", "Eirikur", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.04415", "submitter": "Zhe Wu", "authors": "Zhe Wu, Bharat Singh, Larry S. Davis, V. S. Subrahmanian", "title": "Deception Detection in Videos", "comments": "AAAI 2018, project page: https://doubaibai.github.io/DARE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for covert automated deception detection in real-life\ncourtroom trial videos. We study the importance of different modalities like\nvision, audio and text for this task. On the vision side, our system uses\nclassifiers trained on low level video features which predict human\nmicro-expressions. We show that predictions of high-level micro-expressions can\nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense\nTrajectory) features which have been widely used for action recognition, are\nalso very good at predicting deception in videos. We fuse the score of\nclassifiers trained on IDT features and high-level micro-expressions to improve\nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio\ndomain also provide a significant boost in performance, while information from\ntranscripts is not very beneficial for our system. Using various classifiers,\nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when\nevaluated on subjects which were not part of the training set. Even though\nstate-of-the-art methods use human annotations of micro-expressions for\ndeception detection, our fully automated approach outperforms them by 5%. When\ncombined with human annotations of micro-expressions, our AUC improves to\n0.922. We also present results of a user-study to analyze how well do average\nhumans perform on this task, what modalities they use for deception detection\nand how they perform if only one modality is accessible. Our project page can\nbe found at \\url{https://doubaibai.github.io/DARE/}.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 18:16:43 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Wu", "Zhe", ""], ["Singh", "Bharat", ""], ["Davis", "Larry S.", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1712.04421", "submitter": "Dianna Radpour", "authors": "Dianna Radpour and Vivek Bheda", "title": "Conditional Generative Adversarial Networks for Emoji Synthesis with\n  Word Embedding Manipulation", "comments": "5 pages, 3 figures, 2 graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emojis have become a very popular part of daily digital communication. Their\nappeal comes largely in part due to their ability to capture and elicit\nemotions in a more subtle and nuanced way than just plain text is able to. In\nline with recent advances in the field of deep learning, there are far reaching\nimplications and applications that generative adversarial networks (GANs) can\nhave for image generation. In this paper, we present a novel application of\ndeep convolutional GANs (DC-GANs) with an optimized training procedure. We show\nthat via incorporation of word embeddings conditioned on Google's word2vec\nmodel into the network, the generator is able to synthesize highly realistic\nemojis that are virtually identical to the real ones.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 18:22:33 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 17:15:16 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 19:00:16 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Radpour", "Dianna", ""], ["Bheda", "Vivek", ""]]}, {"id": "1712.04426", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Qixing Huang, Karthik Ramani", "title": "3D Object Classification via Spherical Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new method for classifying 3D objects. Our main\nidea is to project a 3D object onto a spherical domain centered around its\nbarycenter and develop neural network to classify the spherical projection. We\nintroduce two complementary projections. The first captures depth variations of\na 3D object, and the second captures contour-information viewed from different\nangles. Spherical projections combine key advantages of two main-stream 3D\nclassification methods: image-based and 3D-based. Specifically, spherical\nprojections are locally planar, allowing us to use massive image datasets (e.g,\nImageNet) for pre-training. Also spherical projections are similar to\nvoxel-based methods, as they encode complete information of a 3D object in a\nsingle neural network capturing dependencies across different views. Our novel\nnetwork design can fully utilize these advantages. Experimental results on\nModelNet40 and ShapeNetCore show that our method is superior to prior methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 18:37:34 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Cao", "Zhangjie", ""], ["Huang", "Qixing", ""], ["Ramani", "Karthik", ""]]}, {"id": "1712.04440", "submitter": "Kaiming He", "authors": "Ilija Radosavovic, Piotr Doll\\'ar, Ross Girshick, Georgia Gkioxari,\n  Kaiming He", "title": "Data Distillation: Towards Omni-Supervised Learning", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate omni-supervised learning, a special regime of semi-supervised\nlearning in which the learner exploits all available labeled data plus\ninternet-scale sources of unlabeled data. Omni-supervised learning is\nlower-bounded by performance on existing labeled datasets, offering the\npotential to surpass state-of-the-art fully supervised methods. To exploit the\nomni-supervised setting, we propose data distillation, a method that ensembles\npredictions from multiple transformations of unlabeled data, using a single\nmodel, to automatically generate new training annotations. We argue that visual\nrecognition models have recently become accurate enough that it is now possible\nto apply classic ideas about self-training to challenging real-world data. Our\nexperimental results show that in the cases of human keypoint detection and\ngeneral object detection, state-of-the-art models trained with data\ndistillation surpass the performance of using labeled data from the COCO\ndataset alone.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 18:55:57 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Radosavovic", "Ilija", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""], ["Gkioxari", "Georgia", ""], ["He", "Kaiming", ""]]}, {"id": "1712.04480", "submitter": "Himalaya Jain", "authors": "Himalaya Jain, Joaquin Zepeda, Patrick P\\'erez, R\\'emi Gribonval", "title": "Learning a Complete Image Indexing Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 19:37:43 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Jain", "Himalaya", ""], ["Zepeda", "Joaquin", ""], ["P\u00e9rez", "Patrick", ""], ["Gribonval", "R\u00e9mi", ""]]}, {"id": "1712.04482", "submitter": "AmirAbbas Davari", "authors": "AmirAbbas Davari, Tobias Lindenberger, Armin H\\\"aberle, Vincent\n  Christlein, Andreas Maier, Christian Riess", "title": "Image Registration for the Alignment of Digitized Historical Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we conducted a survey on different registration algorithms and\ninvestigated their suitability for hyperspectral historical image registration\napplications. After the evaluation of different algorithms, we choose an\nintensity based registration algorithm with a curved transformation model. For\nthe transformation model, we select cubic B-splines since they should be\ncapable to cope with all non-rigid deformations in our hyperspectral images.\nFrom a number of similarity measures, we found that residual complexity and\nlocalized mutual information are well suited for the task at hand. In our\nevaluation, both measures show an acceptable performance in handling all\ndifficulties, e.g., capture range, non-stationary and spatially varying\nintensity distortions or multi-modality that occur in our application.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 19:41:42 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Davari", "AmirAbbas", ""], ["Lindenberger", "Tobias", ""], ["H\u00e4berle", "Armin", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1712.04489", "submitter": "Tarang Chugh", "authors": "Tarang Chugh, Kai Cao, Anil K. Jain", "title": "Fingerprint Spoof Buster", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary purpose of a fingerprint recognition system is to ensure a\nreliable and accurate user authentication, but the security of the recognition\nsystem itself can be jeopardized by spoof attacks. This study addresses the\nproblem of developing accurate, generalizable, and efficient algorithms for\ndetecting fingerprint spoof attacks. Specifically, we propose a deep\nconvolutional neural network based approach utilizing local patches centered\nand aligned using fingerprint minutiae. Experimental results on three\npublic-domain LivDet datasets (2011, 2013, and 2015) show that the proposed\napproach provides state-of-the-art accuracies in fingerprint spoof detection\nfor intra-sensor, cross-material, cross-sensor, as well as cross-dataset\ntesting scenarios. For example, in LivDet 2015, the proposed approach achieves\n99.03% average accuracy over all sensors compared to 95.51% achieved by the\nLivDet 2015 competition winners. Additionally, two new fingerprint presentation\nattack datasets containing more than 20,000 images, using two different\nfingerprint readers, and over 12 different spoof fabrication materials are\ncollected. We also present a graphical user interface, called Fingerprint Spoof\nBuster, that allows the operator to visually examine the local regions of the\nfingerprint highlighted as live or spoof, instead of relying on only a single\nscore as output by the traditional approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 20:05:08 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Chugh", "Tarang", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1712.04509", "submitter": "Hamid Reza Vaezi Joze", "authors": "Mark S. Drew, Hamid Reza Vaezi Joze, and Graham D. Finlayson", "title": "Camera Calibration for Daylight Specular-Point Locus", "comments": "April 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new camera calibration method aimed at finding a\nstraight-line locus, in a special colour feature space, that is traversed by\ndaylights and as well also approximately followed by specular points. The aim\nof the calibration is to enable recovering the colour of the illuminant in a\nscene, using the calibrated camera. First we prove theoretically that any\ncandidate specular points, for an image that is generated by a specific camera\nand taken under a daylight, must lie on a straight line in log-chromaticity\nspace, for a chromaticity that is generated using a geometric-mean denominator.\nUse is made of the assumptions that daylight illuminants can be approximated\nusing Planckians and that camera sensors are narrowband or can be made so by\nspectral sharpening. Then we show how a particular camera can be calibrated so\nas to discover this locus. As applications we use this curve for illuminant\ndetection, and also for re-lighting of images to show they would appear under\nlighting having a different colour temperature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 20:51:48 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Drew", "Mark S.", ""], ["Joze", "Hamid Reza Vaezi", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "1712.04569", "submitter": "Shuran Song", "authors": "Shuran Song, Andy Zeng, Angel X. Chang, Manolis Savva, Silvio\n  Savarese, Thomas Funkhouser", "title": "Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of\n  View", "comments": "Video summary: https://youtu.be/Au3GmktK-So", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Im2Pano3D, a convolutional neural network that generates a dense\nprediction of 3D structure and a probability distribution of semantic labels\nfor a full 360 panoramic view of an indoor scene when given only a partial\nobservation (<= 50%) in the form of an RGB-D image. To make this possible,\nIm2Pano3D leverages strong contextual priors learned from large-scale synthetic\nand real-world indoor scenes. To ease the prediction of 3D structure, we\npropose to parameterize 3D surfaces with their plane equations and train the\nmodel to predict these parameters directly. To provide meaningful training\nsupervision, we use multiple loss functions that consider both pixel level\naccuracy and global context consistency. Experiments demon- strate that\nIm2Pano3D is able to predict the semantics and 3D structure of the unobserved\nscene with more than 56% pixel accuracy and less than 0.52m average distance\nerror, which is significantly better than alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 23:47:53 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Song", "Shuran", ""], ["Zeng", "Andy", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""], ["Savarese", "Silvio", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1712.04575", "submitter": "Reza Arablouei", "authors": "Reza Arablouei", "title": "Fusing Multiple Multiband Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fusing an arbitrary number of multiband, i.e.,\npanchromatic, multispectral, or hyperspectral, images belonging to the same\nscene. We use the well-known forward observation and linear mixture models with\nGaussian perturbations to formulate the maximum-likelihood estimator of the\nendmember abundance matrix of the fused image. We calculate the Fisher\ninformation matrix for this estimator and examine the conditions for the\nuniqueness of the estimator. We use a vector total-variation penalty term\ntogether with nonnegativity and sum-to-one constraints on the endmember\nabundances to regularize the derived maximum-likelihood estimation problem. The\nregularization facilitates exploiting the prior knowledge that natural images\nare mostly composed of piecewise smooth regions with limited abrupt changes,\ni.e., edges, as well as coping with potential ill-posedness of the fusion\nproblem. We solve the resultant convex optimization problem using the\nalternating direction method of multipliers. We utilize the circular\nconvolution theorem in conjunction with the fast Fourier transform to alleviate\nthe computational complexity of the proposed algorithm. Experiments with\nmultiband images constructed from real hyperspectral datasets reveal the\nsuperior performance of the proposed algorithm in comparison with the\nstate-of-the-art algorithms, which need to be used in tandem to fuse more than\ntwo multiband images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 00:09:28 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 00:39:25 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Arablouei", "Reza", ""]]}, {"id": "1712.04604", "submitter": "Chase Gaudet", "authors": "Chase Gaudet, Anthony Maida", "title": "Deep Quaternion Networks", "comments": "IJCNN 2018, 8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of deep learning has seen significant advancement in recent years.\nHowever, much of the existing work has been focused on real-valued numbers.\nRecent work has shown that a deep learning system using the complex numbers can\nbe deeper for a fixed parameter budget compared to its real-valued counterpart.\nIn this work, we explore the benefits of generalizing one step further into the\nhyper-complex numbers, quaternions specifically, and provide the architecture\ncomponents needed to build deep quaternion networks. We develop the theoretical\nbasis by reviewing quaternion convolutions, developing a novel quaternion\nweight initialization scheme, and developing novel algorithms for quaternion\nbatch-normalization. These pieces are tested in a classification model by\nend-to-end training on the CIFAR-10 and CIFAR-100 data sets and a segmentation\nmodel by end-to-end training on the KITTI Road Segmentation data set. These\nquaternion networks show improved convergence compared to real-valued and\ncomplex-valued networks, especially on the segmentation task, while having\nfewer parameters\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 04:19:24 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 16:08:56 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 14:12:23 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Gaudet", "Chase", ""], ["Maida", "Anthony", ""]]}, {"id": "1712.04616", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Mingsheng Long, Chao Huang, Jianmin Wang", "title": "Transfer Adversarial Hashing for Hamming Space Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is widely applied to large-scale image retrieval due to the storage\nand retrieval efficiency. Existing work on deep hashing assumes that the\ndatabase in the target domain is identically distributed with the training set\nin the source domain. This paper relaxes this assumption to a transfer\nretrieval setting, which allows the database and the training set to come from\ndifferent but relevant domains. However, the transfer retrieval setting will\nintroduce two technical difficulties: first, the hash model trained on the\nsource domain cannot work well on the target domain due to the large\ndistribution gap; second, the domain gap makes it difficult to concentrate the\ndatabase points to be within a small Hamming ball. As a consequence, transfer\nretrieval performance within Hamming Radius 2 degrades significantly in\nexisting hashing methods. This paper presents Transfer Adversarial Hashing\n(TAH), a new hybrid deep architecture that incorporates a pairwise\n$t$-distribution cross-entropy loss to learn concentrated hash codes and an\nadversarial network to align the data distributions between the source and\ntarget domains. TAH can generate compact transfer hash codes for efficient\nimage retrieval on both source and target domains. Comprehensive experiments\nvalidate that TAH yields state of the art Hamming space retrieval performance\non standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 06:06:13 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Cao", "Zhangjie", ""], ["Long", "Mingsheng", ""], ["Huang", "Chao", ""], ["Wang", "Jianmin", ""]]}, {"id": "1712.04621", "submitter": "Luis Perez", "authors": "Luis Perez, Jason Wang", "title": "The Effectiveness of Data Augmentation in Image Classification using\n  Deep Learning", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore and compare multiple solutions to the problem of\ndata augmentation in image classification. Previous work has demonstrated the\neffectiveness of data augmentation through simple techniques, such as cropping,\nrotating, and flipping input images. We artificially constrain our access to\ndata to a small subset of the ImageNet dataset, and compare each data\naugmentation technique in turn. One of the more successful data augmentations\nstrategies is the traditional transformations mentioned above. We also\nexperiment with GANs to generate images of different styles. Finally, we\npropose a method to allow a neural net to learn augmentations that best improve\nthe classifier, which we call neural augmentation. We discuss the successes and\nshortcomings of this method on various datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 06:41:00 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Perez", "Luis", ""], ["Wang", "Jason", ""]]}, {"id": "1712.04646", "submitter": "Zhihang Li", "authors": "Zhihang Li, Yibo Hu, Ran He", "title": "Learning Disentangling and Fusing Networks for Face Completion Under\n  Structured Occlusions", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face completion aims to generate semantically new pixels for missing facial\ncomponents. It is a challenging generative task due to large variations of face\nappearance. This paper studies generative face completion under structured\nocclusions. We treat the face completion and corruption as disentangling and\nfusing processes of clean faces and occlusions, and propose a jointly\ndisentangling and fusing Generative Adversarial Network (DF-GAN). First, three\ndomains are constructed, corresponding to the distributions of occluded faces,\nclean faces and structured occlusions. The disentangling and fusing processes\nare formulated as the transformations between the three domains. Then the\ndisentangling and fusing networks are built to learn the transformations from\nunpaired data, where the encoder-decoder structure is adopted and allows DF-GAN\nto simulate structure occlusions by modifying the latent representations.\nFinally, the disentangling and fusing processes are unified into a dual\nlearning framework along with an adversarial strategy. The proposed method is\nevaluated on Meshface verification problem. Experimental results on four\nMeshface databases demonstrate the effectiveness of our proposed method for the\nface completion under structured occlusions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 08:05:43 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Li", "Zhihang", ""], ["Hu", "Yibo", ""], ["He", "Ran", ""]]}, {"id": "1712.04695", "submitter": "Jiankang Deng", "authors": "Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos\n  Zafeiriou", "title": "UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed robust 3D face alignment methods establish either dense or\nsparse correspondence between a 3D face model and a 2D facial image. The use of\nthese methods presents new challenges as well as opportunities for facial\ntexture analysis. In particular, by sampling the image using the fitted model,\na facial UV can be created. Unfortunately, due to self-occlusion, such a UV map\nis always incomplete. In this paper, we propose a framework for training Deep\nConvolutional Neural Network (DCNN) to complete the facial UV map extracted\nfrom in-the-wild images. To this end, we first gather complete UV maps by\nfitting a 3D Morphable Model (3DMM) to various multiview image and video\ndatasets, as well as leveraging on a new 3D dataset with over 3,000 identities.\nSecond, we devise a meticulously designed architecture that combines local and\nglobal adversarial DCNNs to learn an identity-preserving facial UV completion\nmodel. We demonstrate that by attaching the completed UV to the fitted mesh and\ngenerating instances of arbitrary poses, we can increase pose variations for\ntraining deep face recognition/verification models, and minimise pose\ndiscrepancy during testing, which lead to better performance. Experiments on\nboth controlled and in-the-wild UV datasets prove the effectiveness of our\nadversarial UV completion model. We achieve state-of-the-art verification\naccuracy, $94.05\\%$, under the CFP frontal-profile protocol only by combining\npose augmentation during training and pose discrepancy reduction during\ntesting. We will release the first in-the-wild UV dataset (we refer as WildUV)\nthat comprises of complete facial UV maps from 1,892 identities for research\npurposes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 10:52:42 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Deng", "Jiankang", ""], ["Cheng", "Shiyang", ""], ["Xue", "Niannan", ""], ["Zhou", "Yuxiang", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1712.04698", "submitter": "Hong-Yen Chen", "authors": "Hong-Yen Chen, Chung-Yen Su", "title": "An Enhanced Hybrid MobileNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complicated and deep neural network models can achieve high accuracy for\nimage recognition. However, they require a huge amount of computations and\nmodel parameters, which are not suitable for mobile and embedded devices.\nTherefore, MobileNet was proposed, which can reduce the number of parameters\nand computational cost dramatically. The main idea of MobileNet is to use a\ndepthwise separable convolution. Two hyper-parameters, a width multiplier and a\nresolution multiplier are used to the trade-off between the accuracy and the\nlatency. In this paper, we propose a new architecture to improve the MobileNet.\nInstead of using the resolution multiplier, we use a depth multiplier and\ncombine with either Fractional Max Pooling or the max pooling. Experimental\nresults on CIFAR database show that the proposed architecture can reduce the\namount of computational cost and increase the accuracy simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 10:54:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 04:21:26 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Chen", "Hong-Yen", ""], ["Su", "Chung-Yen", ""]]}, {"id": "1712.04711", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan, Shanmugasundaram Durairaj", "title": "Regularization and Optimization strategies in Deep Convolutional Neural\n  Network", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolution Neural Networks, known as ConvNets exceptionally perform well in\nmany complex machine learning tasks. The architecture of ConvNets demands the\nhuge and rich amount of data and involves with a vast number of parameters that\nleads the learning takes to be computationally expensive, slow convergence\ntowards the global minima, trap in local minima with poor predictions. In some\ncases, architecture overfits the data and make the architecture difficult to\ngeneralise for new samples that were not in the training set samples. To\naddress these limitations, many regularization and optimization strategies are\ndeveloped for the past few years. Also, studies suggested that these techniques\nsignificantly increase the performance of the networks as well as reducing the\ncomputational cost. In implementing these techniques, one must thoroughly\nunderstand the theoretical concept of how this technique works in increasing\nthe expressive power of the networks. This article is intended to provide the\ntheoretical concepts and mathematical formulation of the most commonly used\nstrategies in developing a ConvNet architecture.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 11:23:45 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Murugan", "Pushparaja", ""], ["Durairaj", "Shanmugasundaram", ""]]}, {"id": "1712.04741", "submitter": "Rene Vidal", "authors": "Rene Vidal, Joan Bruna, Raja Giryes, Stefano Soatto", "title": "Mathematics of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a dramatic increase in the performance of recognition\nsystems due to the introduction of deep architectures for representation\nlearning and classification. However, the mathematical reasons for this success\nremain elusive. This tutorial will review recent work that aims to provide a\nmathematical justification for several properties of deep networks, such as\nglobal optimality, geometric stability, and invariance of the learned\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 12:44:46 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Vidal", "Rene", ""], ["Bruna", "Joan", ""], ["Giryes", "Raja", ""], ["Soatto", "Stefano", ""]]}, {"id": "1712.04778", "submitter": "AmirAbbas Davari", "authors": "AmirAbbas Davari, Erchan Aptoula, Berrin Yanikoglu, Andreas Maier,\n  Christian Riess", "title": "GMM-Based Synthetic Samples for Classification of Hyperspectral Images\n  With Limited Training Data", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2018.2817361", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of training data that is required to train a classifier scales\nwith the dimensionality of the feature data. In hyperspectral remote sensing,\nfeature data can potentially become very high dimensional. However, the amount\nof training data is oftentimes limited. Thus, one of the core challenges in\nhyperspectral remote sensing is how to perform multi-class classification using\nonly relatively few training data points.\n  In this work, we address this issue by enriching the feature matrix with\nsynthetically generated sample points. This synthetic data is sampled from a\nGMM fitted to each class of the limited training data. Although, the true\ndistribution of features may not be perfectly modeled by the fitted GMM, we\ndemonstrate that a moderate augmentation by these synthetic samples can\neffectively replace a part of the missing training samples. We show the\nefficacy of the proposed approach on two hyperspectral datasets. The median\ngain in classification performance is $5\\%$. It is also encouraging that this\nperformance gain is remarkably stable for large variations in the number of\nadded samples, which makes it much easier to apply this method to real-world\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:12:06 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Davari", "AmirAbbas", ""], ["Aptoula", "Erchan", ""], ["Yanikoglu", "Berrin", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1712.04833", "submitter": "Frank Dennis Julca Aguilar", "authors": "Frank D. Julca-Aguilar and Nina S. T. Hirata", "title": "Symbol detection in online handwritten graphics using Faster R-CNN", "comments": "Submitted to DAS-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbol detection techniques in online handwritten graphics (e.g. diagrams and\nmathematical expressions) consist of methods specifically designed for a single\ngraphic type. In this work, we evaluate the Faster R-CNN object detection\nalgorithm as a general method for detection of symbols in handwritten graphics.\nWe evaluate different configurations of the Faster R-CNN method, and point out\nissues relative to the handwritten nature of the data. Considering the online\nrecognition context, we evaluate efficiency and accuracy trade-offs of using\nDeep Neural Networks of different complexities as feature extractors. We\nevaluate the method on publicly available flowchart and mathematical expression\n(CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used\non both datasets, enabling the possibility of developing general methods for\nsymbol detection, and furthermore, general graphic understanding methods that\ncould be built on top of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 16:05:13 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Julca-Aguilar", "Frank D.", ""], ["Hirata", "Nina S. T.", ""]]}, {"id": "1712.04837", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian\n  Schroff, Peng Wang, Hartwig Adam", "title": "MaskLab: Instance Segmentation by Refining Object Detection with\n  Semantic and Direction Features", "comments": "10 pages including reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of instance segmentation, the task of\nsimultaneously solving object detection and semantic segmentation. Towards this\ngoal, we present a model, called MaskLab, which produces three outputs: box\ndetection, semantic segmentation, and direction prediction. Building on top of\nthe Faster-RCNN object detector, the predicted boxes provide accurate\nlocalization of object instances. Within each region of interest, MaskLab\nperforms foreground/background segmentation by combining semantic and direction\nprediction. Semantic segmentation assists the model in distinguishing between\nobjects of different semantic classes including background, while the direction\nprediction, estimating each pixel's direction towards its corresponding center,\nallows separating instances of the same semantic class. Moreover, we explore\nthe effect of incorporating recent successful methods from both segmentation\nand detection (i.e. atrous convolution and hypercolumn). Our proposed model is\nevaluated on the COCO instance segmentation benchmark and shows comparable\nperformance with other state-of-art models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 16:09:55 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Hermans", "Alexander", ""], ["Papandreou", "George", ""], ["Schroff", "Florian", ""], ["Wang", "Peng", ""], ["Adam", "Hartwig", ""]]}, {"id": "1712.04850", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Erik Learned-Miller, Gustav Larsson, Michael Maire, Greg\n  Shakhnarovich", "title": "Self-Supervised Relative Depth Learning for Urban Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an agent moves through the world, the apparent motion of scene elements is\n(usually) inversely proportional to their depth. It is natural for a learning\nagent to associate image patterns with the magnitude of their displacement over\ntime: as the agent moves, faraway mountains don't move much; nearby trees move\na lot. This natural relationship between the appearance of objects and their\nmotion is a rich source of information about the world. In this work, we start\nby training a deep network, using fully automatic supervision, to predict\nrelative scene depth from single images. The relative depth training images are\nautomatically derived from simple videos of cars moving through a scene, using\nrecent motion segmentation techniques, and no human-provided labels. This proxy\ntask of predicting relative depth from a single image induces features in the\nnetwork that result in large improvements in a set of downstream tasks\nincluding semantic segmentation, joint road segmentation and car detection, and\nmonocular (absolute) depth estimation, over a network trained from scratch. The\nimprovement on the semantic segmentation task is greater than those produced by\nany other automatically supervised methods. Moreover, for monocular depth\nestimation, our unsupervised pre-training method even outperforms supervised\npre-training with ImageNet. In addition, we demonstrate benefits from learning\nto predict (unsupervised) relative depth in the specific videos associated with\nvarious downstream tasks. We adapt to the specific scenes in those tasks in an\nunsupervised manner to improve performance. In summary, for semantic\nsegmentation, we present state-of-the-art results among methods that do not use\nsupervised pre-training, and we even exceed the performance of supervised\nImageNet pre-trained models for monocular depth estimation, achieving results\nthat are comparable with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 16:39:14 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 15:39:03 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Jiang", "Huaizu", ""], ["Learned-Miller", "Erik", ""], ["Larsson", "Gustav", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "1712.04851", "submitter": "Chen Sun", "authors": "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy", "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in\n  Video Classification", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the steady progress in video analysis led by the adoption of\nconvolutional neural networks (CNNs), the relative improvement has been less\ndrastic as that in 2D static image classification. Three main challenges exist\nincluding spatial (image) feature representation, temporal information\nrepresentation, and model/computation complexity. It was recently shown by\nCarreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained\non ImageNet, could be a promising way for spatial and temporal representation\nlearning. However, as for model/computation complexity, 3D CNNs are much more\nexpensive than 2D CNNs and prone to overfit. We seek a balance between speed\nand accuracy by building an effective and efficient video classification system\nthrough systematic exploration of critical network design choices. In\nparticular, we show that it is possible to replace many of the 3D convolutions\nby low-cost 2D convolutions. Rather surprisingly, best result (in both speed\nand accuracy) is achieved when replacing the 3D convolutions at the bottom of\nthe network, suggesting that temporal representation learning on high-level\nsemantic features is more useful. Our conclusion generalizes to datasets with\nvery different properties. When combined with several other cost-effective\ndesigns including separable spatial/temporal convolution and feature gating,\nour system results in an effective video classification system that that\nproduces very competitive results on several action classification benchmarks\n(Kinetics, Something-something, UCF101 and HMDB), as well as two action\ndetection (localization) benchmarks (JHMDB and UCF101-24).\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 16:40:55 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 03:20:56 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Xie", "Saining", ""], ["Sun", "Chen", ""], ["Huang", "Jonathan", ""], ["Tu", "Zhuowen", ""], ["Murphy", "Kevin", ""]]}, {"id": "1712.04919", "submitter": "Xiao-Yang Liu", "authors": "Tao Deng, Xiao-Yang Liu, Feng Qian, Anwar Walid", "title": "Multidimensional Data Tensor Sensing for RF Tomographic Imaging", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio-frequency (RF) tomographic imaging is a promising technique for\ninferring multi-dimensional physical space by processing RF signals traversed\nacross a region of interest. However, conventional RF tomography schemes are\ngenerally based on vector compressed sensing, which ignores the geometric\nstructures of the target spaces and leads to low recovery precision. The\nrecently proposed transform-based tensor model is more appropriate for sensory\ndata processing, as it helps exploit the geometric structures of the\nthree-dimensional target and improve the recovery precision. In this paper, we\npropose a novel tensor sensing approach that achieves highly accurate\nestimation for real-world three-dimensional spaces. First, we use the\ntransform-based tensor model to formulate a tensor sensing problem, and propose\na fast alternating minimization algorithm called Alt-Min. Secondly, we drive an\nalgorithm which is optimized to reduce memory and computation requirements.\nFinally, we present evaluation of our Alt-Min approach using IKEA 3D data and\ndemonstrate significant improvement in recovery error and convergence speed\ncompared to prior tensor-based compressed sensing.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:40:22 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 15:38:35 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Deng", "Tao", ""], ["Liu", "Xiao-Yang", ""], ["Qian", "Feng", ""], ["Walid", "Anwar", ""]]}, {"id": "1712.04926", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall, Kamlesh Jaiswal", "title": "Object Classification using Ensemble of Local and Deep Features", "comments": "Accepted for publication at Ninth International Conference on\n  Advances in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an ensemble of local and deep features for object\nclassification. We also compare and contrast effectiveness of feature\nrepresentation capability of various layers of convolutional neural network. We\ndemonstrate with extensive experiments for object classification that the\nrepresentation capability of features from deep networks can be complemented\nwith information captured from local features. We also find out that features\nfrom various deep convolutional networks encode distinctive characteristic\ninformation. We establish that, as opposed to conventional practice,\nintermediate layers of deep networks can augment the classification\ncapabilities of features obtained from fully connected layers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 08:42:00 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""], ["Jaiswal", "Kamlesh", ""]]}, {"id": "1712.04927", "submitter": "Siddharth Srivastava", "authors": "Aarushi Agrawal, Prerana Mukherjee, Siddharth Srivastava, Brejesh Lall", "title": "Enhanced Characterness for Text Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text spotting is an interesting research problem as text may appear at any\nrandom place and may occur in various forms. Moreover, ability to detect text\nopens the horizons for improving many advanced computer vision problems. In\nthis paper, we propose a novel language agnostic text detection method\nutilizing edge enhanced Maximally Stable Extremal Regions in natural scenes by\ndefining strong characterness measures. We show that a simple combination of\ncharacterness cues help in rejecting the non text regions. These regions are\nfurther fine-tuned for rejecting the non-textual neighbor regions.\nComprehensive evaluation of the proposed scheme shows that it provides\ncomparative to better generalization performance to the traditional methods for\nthis task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 08:44:35 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Agrawal", "Aarushi", ""], ["Mukherjee", "Prerana", ""], ["Srivastava", "Siddharth", ""], ["Lall", "Brejesh", ""]]}, {"id": "1712.04961", "submitter": "Christine Kaeser-Chen", "authors": "Rohit Pandey, Marie White, Pavel Pidlypenskyi, Xue Wang, Christine\n  Kaeser-Chen", "title": "Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays", "comments": "Extended Abstract NIPS 2017 Machine Learning on the Phone and other\n  Consumer Devices Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile virtual reality (VR) head mounted displays (HMD) have become popular\namong consumers in recent years. In this work, we demonstrate real-time\negocentric hand gesture detection and localization on mobile HMDs. Our main\ncontributions are: 1) A novel mixed-reality data collection tool to automatic\nannotate bounding boxes and gesture labels; 2) The largest-to-date egocentric\nhand gesture and bounding box dataset with more than 400,000 annotated frames;\n3) A neural network that runs real time on modern mobile CPUs, and achieves\nhigher than 76% precision on gesture recognition across 8 classes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 19:06:37 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Pandey", "Rohit", ""], ["White", "Marie", ""], ["Pidlypenskyi", "Pavel", ""], ["Wang", "Xue", ""], ["Kaeser-Chen", "Christine", ""]]}, {"id": "1712.05015", "submitter": "Shen Yan", "authors": "Shen Yan", "title": "Learning Low-shot facial representations via 2D warping", "comments": "The new version should update the table as well as add some new\n  results. This paper is talking about one-shot learning, but the current\n  version is not mainly focus on that point. I should restructure the article.\n  After the page reduction I assume the new version would be 4 pages. The title\n  name should also be changed to one-shot learning and there are more previous\n  work should be cited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we mainly study the influence of the 2D warping module for\none-shot face recognition.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:39:52 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 15:08:07 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yan", "Shen", ""]]}, {"id": "1712.05021", "submitter": "Le Hou", "authors": "Le Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M. Kurc, Rajarsi R.\n  Gupta, Joel H. Saltz", "title": "Unsupervised Histopathology Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hematoxylin and Eosin stained histopathology image analysis is essential for\nthe diagnosis and study of complicated diseases such as cancer. Existing\nstate-of-the-art approaches demand extensive amount of supervised training data\nfrom trained pathologists. In this work we synthesize in an unsupervised\nmanner, large histopathology image datasets, suitable for supervised training\ntasks. We propose a unified pipeline that: a) generates a set of initial\nsynthetic histopathology images with paired information about the nuclei such\nas segmentation masks; b) refines the initial synthetic images through a\nGenerative Adversarial Network (GAN) to reference styles; c) trains a\ntask-specific CNN and boosts the performance of the task-specific CNN with\non-the-fly generated adversarial examples. Our main contribution is that the\nsynthetic images are not only realistic, but also representative (in reference\nstyles) and relatively challenging for training task-specific CNNs. We test our\nmethod for nucleus segmentation using images from four cancer types. When no\nsupervised data exists for a cancer type, our method without supervision cost\nsignificantly outperforms supervised methods which perform across-cancer\ngeneralization. Even when supervised data exists for all cancer types, our\napproach without supervision cost performs better than supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:52:12 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Hou", "Le", ""], ["Agarwal", "Ayush", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Gupta", "Rajarsi R.", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1712.05053", "submitter": "Alexey Shvets", "authors": "Vladimir Iglovikov, Alexander Rakhlin, Alexandr Kalinin, Alexey Shvets", "title": "Pediatric Bone Age Assessment Using Deep Convolutional Neural Networks", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": "10.1007/978-3-030-00889-5_34", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeletal bone age assessment is a common clinical practice to diagnose\nendocrine and metabolic disorders in child development. In this paper, we\ndescribe a fully automated deep learning approach to the problem of bone age\nassessment using data from Pediatric Bone Age Challenge organized by RSNA 2017.\nThe dataset for this competition is consisted of 12.6k radiological images of\nleft hand labeled by the bone age and sex of patients. Our approach utilizes\nseveral deep learning architectures: U-Net, ResNet-50, and custom VGG-style\nneural networks trained end-to-end. We use images of whole hands as well as\nspecific parts of a hand for both training and inference. This approach allows\nus to measure importance of specific hand bones for the automated bone age\nanalysis. We further evaluate performance of the method in the context of\nskeletal development stages. Our approach outperforms other common methods for\nbone age assessment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 23:56:15 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 19:46:33 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Iglovikov", "Vladimir", ""], ["Rakhlin", "Alexander", ""], ["Kalinin", "Alexandr", ""], ["Shvets", "Alexey", ""]]}, {"id": "1712.05055", "submitter": "Lu Jiang", "authors": "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei", "title": "MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks\n  on Corrupted Labels", "comments": null, "journal-ref": "published at ICML 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep networks are capable of memorizing the entire data even when the\nlabels are completely random. To overcome the overfitting on corrupted labels,\nwe propose a novel technique of learning another neural network, called\nMentorNet, to supervise the training of the base deep networks, namely,\nStudentNet. During training, MentorNet provides a curriculum (sample weighting\nscheme) for StudentNet to focus on the sample the label of which is probably\ncorrect. Unlike the existing curriculum that is usually predefined by human\nexperts, MentorNet learns a data-driven curriculum dynamically with StudentNet.\nExperimental results demonstrate that our approach can significantly improve\nthe generalization performance of deep networks trained on corrupted training\ndata. Notably, to the best of our knowledge, we achieve the best-published\nresult on WebVision, a large benchmark containing 2.2 million images of\nreal-world noisy labels. The code are at https://github.com/google/mentornet\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 00:02:37 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 21:27:39 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Jiang", "Lu", ""], ["Zhou", "Zhengyuan", ""], ["Leung", "Thomas", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1712.05080", "submitter": "Ting Liu", "authors": "Phuc Nguyen, Ting Liu, Gautam Prasad, Bohyung Han", "title": "Weakly Supervised Action Localization by Sparse Temporal Pooling Network", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly supervised temporal action localization algorithm on\nuntrimmed videos using convolutional neural networks. Our algorithm learns from\nvideo-level class labels and predicts temporal intervals of human actions with\nno requirement of temporal localization annotations. We design our network to\nidentify a sparse subset of key segments associated with target actions in a\nvideo using an attention module and fuse the key segments through adaptive\ntemporal pooling. Our loss function is comprised of two terms that minimize the\nvideo-level action classification error and enforce the sparsity of the segment\nselection. At inference time, we extract and score temporal proposals using\ntemporal class activations and class-agnostic attentions to estimate the time\nintervals that correspond to target actions. The proposed algorithm attains\nstate-of-the-art results on the THUMOS14 dataset and outstanding performance on\nActivityNet1.3 even with its weak supervision.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 03:34:03 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 17:23:46 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Nguyen", "Phuc", ""], ["Liu", "Ting", ""], ["Prasad", "Gautam", ""], ["Han", "Bohyung", ""]]}, {"id": "1712.05083", "submitter": "Tal Hassner", "authors": "Anh Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, and\n  Gerard Medioni", "title": "Extreme 3D Face Reconstruction: Seeing Through Occlusions", "comments": "Accepted to CVPR'18. Previously titled: \"Extreme 3D Face\n  Reconstruction: Looking Past Occlusions\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing single view, 3D face reconstruction methods can produce beautifully\ndetailed 3D results, but typically only for near frontal, unobstructed\nviewpoints. We describe a system designed to provide detailed 3D\nreconstructions of faces viewed under extreme conditions, out of plane\nrotations, and occlusions. Motivated by the concept of bump mapping, we propose\na layered approach which decouples estimation of a global shape from its\nmid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which\nacts as a foundation and then separately layer this foundation with details\nrepresented by a bump map. We show how a deep convolutional encoder-decoder can\nbe used to estimate such bump maps. We further show how this approach naturally\nextends to generate plausible details for occluded facial regions. We test our\napproach and its components extensively, quantitatively demonstrating the\ninvariance of our estimated facial details. We further provide numerous\nqualitative examples showing that our method produces detailed 3D face shapes\nin viewing conditions where existing state of the art often break down.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 03:53:52 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 23:22:21 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Tran", "Anh Tuan", ""], ["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Paz", "Eran", ""], ["Nirkin", "Yuval", ""], ["Medioni", "Gerard", ""]]}, {"id": "1712.05084", "submitter": "Thushan Ganegedara", "authors": "Thushan Ganegedara, Lionel Ott and Fabio Ramos", "title": "Learning to Navigate by Growing Deep Networks", "comments": "10 pages, Australasian Conference on Robotics and Automation, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptability is central to autonomy. Intuitively, for high-dimensional\nlearning problems such as navigating based on vision, internal models with\nhigher complexity allow to accurately encode the information available.\nHowever, most learning methods rely on models with a fixed structure and\ncomplexity. In this paper, we present a self-supervised framework for robots to\nlearn to navigate, without any prior knowledge of the environment, by\nincrementally building the structure of a deep network as new data becomes\navailable. Our framework captures images from a monocular camera and self\nlabels the images to continuously train and predict actions from a\ncomputationally efficient adaptive deep architecture based on Autoencoders\n(AE), in a self-supervised fashion. The deep architecture, named Reinforced\nAdaptive Denoising Autoencoders (RA-DAE), uses reinforcement learning to\ndynamically change the network structure by adding or removing neurons.\nExperiments were conducted in simulation and real-world indoor and outdoor\nenvironments to assess the potential of self-supervised navigation. RA-DAE\ndemonstrates better performance than equivalent non-adaptive deep learning\nalternatives and can continue to expand its knowledge, trading-off past and\npresent information.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 03:58:23 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ganegedara", "Thushan", ""], ["Ott", "Lionel", ""], ["Ramos", "Fabio", ""]]}, {"id": "1712.05087", "submitter": "Yi-Hsuan Tsai", "authors": "Yi-Hsuan Tsai, Ming-Yu Liu, Deqing Sun, Ming-Hsuan Yang, Jan Kautz", "title": "Learning Binary Residual Representations for Domain-specific Video\n  Streaming", "comments": "Accepted in AAAI'18. Project website at\n  https://research.nvidia.com/publication/2018-02_Learning-Binary-Residual", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study domain-specific video streaming. Specifically, we target a streaming\nsetting where the videos to be streamed from a server to a client are all in\nthe same domain and they have to be compressed to a small size for low-latency\ntransmission. Several popular video streaming services, such as the video game\nstreaming services of GeForce Now and Twitch, fall in this category. While\nconventional video compression standards such as H.264 are commonly used for\nthis task, we hypothesize that one can leverage the property that the videos\nare all in the same domain to achieve better video quality. Based on this\nhypothesis, we propose a novel video compression pipeline. Specifically, we\nfirst apply H.264 to compress domain-specific videos. We then train a novel\nbinary autoencoder to encode the leftover domain-specific residual information\nframe-by-frame into binary representations. These binary representations are\nthen compressed and sent to the client together with the H.264 stream. In our\nexperiments, we show that our pipeline yields consistent gains over standard\nH.264 compression across several benchmark datasets while using the same\nchannel bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 04:06:33 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Tsai", "Yi-Hsuan", ""], ["Liu", "Ming-Yu", ""], ["Sun", "Deqing", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1712.05114", "submitter": "Kungang Li", "authors": "Ning Li, Haopeng Liu, Bin Qiu, Wei Guo, Shijun Zhao, Kungang Li, Jie\n  He", "title": "Detection and Attention: Diagnosing Pulmonary Lung Cancer from CT by\n  Imitating Physicians", "comments": "8 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel and efficient method to build a Computer-Aided\nDiagnoses (CAD) system for lung nodule detection based on Computed Tomography\n(CT). This task was treated as an Object Detection on Video (VID) problem by\nimitating how a radiologist reads CT scans. A lung nodule detector was trained\nto automatically learn nodule features from still images to detect lung nodule\ncandidates with both high recall and accuracy. Unlike previous work which used\n3-dimensional information around the nodule to reduce false positives, we\npropose two simple but efficient methods, Multi-slice propagation (MSP) and\nMotionless-guide suppression (MLGS), which analyze sequence information of CT\nscans to reduce false negatives and suppress false positives. We evaluated our\nmethod in open-source LUNA16 dataset which contains 888 CT scans, and obtained\nstate-of-the-art result (Free-Response Receiver Operating Characteristic score\nof 0.892) with detection speed (end to end within 20 seconds per patient on a\nsingle NVidia GTX 1080) much higher than existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 07:34:32 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Li", "Ning", ""], ["Liu", "Haopeng", ""], ["Qiu", "Bin", ""], ["Guo", "Wei", ""], ["Zhao", "Shijun", ""], ["Li", "Kungang", ""], ["He", "Jie", ""]]}, {"id": "1712.05116", "submitter": "Longtao Chen", "authors": "Longtao Chen, Jing Lou, Wei Zhu, Qingyuan Xia, Mingwu Ren", "title": "Multi-appearance Segmentation and Extended 0-1 Program for Dense Small\n  Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0206168", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to address the fast multi-object tracking for dense small object in\nthe cluster background, we review track orientated multi-hypothesis\ntracking(TOMHT) with consideration of batch optimization. Employing\nautocorrelation based motion score test and staged hypotheses merging approach,\nwe build our homologous hypothesis generation and management method. A new\none-to-many constraint is proposed and applied to tackle the track exclusions\nduring complex occlusions. Besides, to achieve better results, we develop a\nmulti-appearance segmentation for detection, which exploits tree-like\ntopological information and realizes one threshold for one object. Experimental\nresults verify the strength of our methods, indicating speed and performance\nadvantages of our tracker.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 07:47:50 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Longtao", ""], ["Lou", "Jing", ""], ["Zhu", "Wei", ""], ["Xia", "Qingyuan", ""], ["Ren", "Mingwu", ""]]}, {"id": "1712.05200", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Andres Diaz-Pinto, Nishant Ravikumar, Stephan Ellmann,\n  Amirabbas Davari, Andreas Maier", "title": "Semi-Automatic Algorithm for Breast MRI Lesion Segmentation Using\n  Marker-Controlled Watershed Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is an effective imaging modality for\nidentifying and localizing breast lesions in women. Accurate and precise lesion\nsegmentation using a computer-aided-diagnosis (CAD) system, is a crucial step\nin evaluating tumor volume and in the quantification of tumor characteristics.\nHowever, this is a challenging task, since breast lesions have sophisticated\nshape, topological structure, and high variance in their intensity distribution\nacross patients. In this paper, we propose a novel marker-controlled watershed\ntransformation-based approach, which uses the brightest pixels in a region of\ninterest (determined by experts) as markers to overcome this challenge, and\naccurately segment lesions in breast MRI. The proposed approach was evaluated\non 106 lesions, which includes 64 malignant and 42 benign cases. Segmentation\nresults were quantified by comparison with ground truth labels, using the Dice\nsimilarity coefficient (DSC) and Jaccard index (JI) metrics. The proposed\nmethod achieved an average Dice coefficient of 0.7808$\\pm$0.1729 and Jaccard\nindex of 0.6704$\\pm$0.2167. These results illustrate that the proposed method\nshows promise for future work related to the segmentation and classification of\nbenign and malignant breast lesions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:45:08 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Diaz-Pinto", "Andres", ""], ["Ravikumar", "Nishant", ""], ["Ellmann", "Stephan", ""], ["Davari", "Amirabbas", ""], ["Maier", "Andreas", ""]]}, {"id": "1712.05231", "submitter": "Yang Li", "authors": "Yang Li, Jianke Zhu, Steven C.H. Hoi, Wenjie Song, Zhefeng Wang,\n  Hantang Liu", "title": "Robust Estimation of Similarity Transformation for Visual Object\n  Tracking", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing correlation filter-based tracking approaches only estimate\nsimple axis-aligned bounding boxes, and very few of them is capable of\nrecovering the underlying similarity transformation. To tackle this challenging\nproblem, in this paper, we propose a new correlation filter-based tracker with\na novel robust estimation of similarity transformation on the large\ndisplacements. In order to efficiently search in such a large 4-DoF space in\nreal-time, we formulate the problem into two 2-DoF sub-problems and apply an\nefficient Block Coordinates Descent solver to optimize the estimation result.\nSpecifically, we employ an efficient phase correlation scheme to deal with both\nscale and rotation changes simultaneously in log-polar coordinates. Moreover, a\nvariant of correlation filter is used to predict the translational motion\nindividually. Our experimental results demonstrate that the proposed tracker\nachieves very promising prediction performance compared with the\nstate-of-the-art visual object tracking methods while still retaining the\nadvantages of high efficiency and simplicity in conventional correlation\nfilter-based tracking methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:09:36 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 09:01:52 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Li", "Yang", ""], ["Zhu", "Jianke", ""], ["Hoi", "Steven C. H.", ""], ["Song", "Wenjie", ""], ["Wang", "Zhefeng", ""], ["Liu", "Hantang", ""]]}, {"id": "1712.05245", "submitter": "Binh-Son Hua", "authors": "Binh-Son Hua, Minh-Khoi Tran, Sai-Kit Yeung", "title": "Pointwise Convolutional Neural Networks", "comments": "10 pages, 6 figures, 10 tables. Paper accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with 3D data such as reconstructed point clouds and CAD models\nhas received great research interests recently. However, the capability of\nusing point clouds with convolutional neural network has been so far not fully\nexplored. In this paper, we present a convolutional neural network for semantic\nsegmentation and object recognition with 3D point clouds. At the core of our\nnetwork is pointwise convolution, a new convolution operator that can be\napplied at each point of a point cloud. Our fully convolutional network design,\nwhile being surprisingly simple to implement, can yield competitive accuracy in\nboth semantic segmentation and object recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:25:52 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 04:55:01 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Hua", "Binh-Son", ""], ["Tran", "Minh-Khoi", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1712.05248", "submitter": "HaiLiang Li", "authors": "Hailiang Li, Kin-Man Lam, Miaohui Wang", "title": "Image Super-resolution via Feature-augmented Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent random-forest (RF)-based image super-resolution approaches inherit\nsome properties from dictionary-learning-based algorithms, but the\neffectiveness of the properties in RF is overlooked in the literature. In this\npaper, we present a novel feature-augmented random forest (FARF) for image\nsuper-resolution, where the conventional gradient-based features are augmented\nwith gradient magnitudes and different feature recipes are formulated on\ndifferent stages in an RF. The advantages of our method are that, firstly, the\ndictionary-learning-based features are enhanced by adding gradient magnitudes,\nbased on the observation that the non-linear gradient magnitude are with highly\ndiscriminative property. Secondly, generalized locality-sensitive hashing (LSH)\nis used to replace principal component analysis (PCA) for feature\ndimensionality reduction and original high-dimensional features are employed,\ninstead of the compressed ones, for the leaf-nodes' regressors, since\nregressors can benefit from higher dimensional features. This\noriginal-compressed coupled feature sets scheme unifies the unsupervised LSH\nevaluation on both image super-resolution and content-based image retrieval\n(CBIR). Finally, we present a generalized weighted ridge regression (GWRR)\nmodel for the leaf-nodes' regressors. Experiment results on several public\nbenchmark datasets show that our FARF method can achieve an average gain of\nabout 0.3 dB, compared to traditional RF-based methods. Furthermore, a\nfine-tuned FARF model can compare to or (in many cases) outperform some recent\nstateof-the-art deep-learning-based algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:27:39 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Li", "Hailiang", ""], ["Lam", "Kin-Man", ""], ["Wang", "Miaohui", ""]]}, {"id": "1712.05271", "submitter": "Bin Fan", "authors": "Bin Fan and Qingqun Kong and Xinchao Wang and Zhiheng Wang and Shiming\n  Xiang and Chunhong Pan and Pascal Fua", "title": "A Performance Evaluation of Local Features for Image Based 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper performs a comprehensive and comparative evaluation of the state\nof the art local features for the task of image based 3D reconstruction. The\nevaluated local features cover the recently developed ones by using powerful\nmachine learning techniques and the elaborately designed handcrafted features.\nTo obtain a comprehensive evaluation, we choose to include both float type\nfeatures and binary ones. Meanwhile, two kinds of datasets have been used in\nthis evaluation. One is a dataset of many different scene types with\ngroundtruth 3D points, containing images of different scenes captured at fixed\npositions, for quantitative performance evaluation of different local features\nin the controlled image capturing situations. The other dataset contains\nInternet scale image sets of several landmarks with a lot of unrelated images,\nwhich is used for qualitative performance evaluation of different local\nfeatures in the free image collection situations. Our experimental results show\nthat binary features are competent to reconstruct scenes from controlled image\nsequences with only a fraction of processing time compared to use float type\nfeatures. However, for the case of large scale image set with many distracting\nimages, float type features show a clear advantage over binary ones.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 15:04:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Fan", "Bin", ""], ["Kong", "Qingqun", ""], ["Wang", "Xinchao", ""], ["Wang", "Zhiheng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""], ["Fua", "Pascal", ""]]}, {"id": "1712.05277", "submitter": "Guido Borghi", "authors": "Guido Borghi, Matteo Fabbri, Roberto Vezzani, Simone Calderara, Rita\n  Cucchiara", "title": "Face-from-Depth for Head Pose Estimation on Depth Images", "comments": "Submitted to IEEE Transactions on PAMI, updated version (second\n  round). arXiv admin note: substantial text overlap with arXiv:1611.10195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 20:57:15 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 13:51:43 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Borghi", "Guido", ""], ["Fabbri", "Matteo", ""], ["Vezzani", "Roberto", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1712.05319", "submitter": "Jose Dolz", "authors": "Jose Dolz and Christian Desrosiers and Li Wang and Jing Yuan and\n  Dinggang Shen and Ismail Ben Ayed", "title": "Deep CNN ensembles and suggestive annotations for infant brain MRI\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise 3D segmentation of infant brain tissues is an essential step towards\ncomprehensive volumetric studies and quantitative analysis of early brain\ndevelopement. However, computing such segmentations is very challenging,\nespecially for 6-month infant brain, due to the poor image quality, among other\ndifficulties inherent to infant brain MRI, e.g., the isointense contrast\nbetween white and gray matter and the severe partial volume effect due to small\nbrain sizes. This study investigates the problem with an ensemble of semi-dense\nfully convolutional neural networks (CNNs), which employs T1-weighted and\nT2-weighted MR images as input. We demonstrate that the ensemble agreement is\nhighly correlated with the segmentation errors. Therefore, our method provides\nmeasures that can guide local user corrections. To the best of our knowledge,\nthis work is the first ensemble of 3D CNNs for suggesting annotations within\nimages. Furthermore, inspired by the very recent success of dense networks, we\npropose a novel architecture, SemiDenseNet, which connects all convolutional\nlayers directly to the end of the network. Our architecture allows the\nefficient propagation of gradients during training, while limiting the number\nof parameters, requiring one order of magnitude less parameters than popular\nmedical image segmentation networks such as 3D U-Net. Another contribution of\nour work is the study of the impact that early or late fusions of multiple\nimage modalities might have on the performances of deep architectures. We\nreport evaluations of our method on the public data of the MICCAI iSEG-2017\nChallenge on 6-month infant brain MRI segmentation, and show very competitive\nresults among 21 teams, ranking first or second in most metrics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 16:27:42 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 16:54:19 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""], ["Wang", "Li", ""], ["Yuan", "Jing", ""], ["Shen", "Dinggang", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1712.05404", "submitter": "Christian Bartz", "authors": "Christian Bartz, Haojin Yang, Christoph Meinel", "title": "SEE: Towards Semi-Supervised End-to-End Scene Text Recognition", "comments": "AAAI-18. arXiv admin note: substantial text overlap with\n  arXiv:1707.08831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and recognizing text in natural scene images is a challenging, yet\nnot completely solved task. In recent years several new systems that try to\nsolve at least one of the two sub-tasks (text detection and text recognition)\nhave been proposed. In this paper we present SEE, a step towards\nsemi-supervised neural networks for scene text detection and recognition, that\ncan be optimized end-to-end. Most existing works consist of multiple deep\nneural networks and several pre-processing steps. In contrast to this, we\npropose to use a single deep neural network, that learns to detect and\nrecognize text from natural images, in a semi-supervised way. SEE is a network\nthat integrates and jointly learns a spatial transformer network, which can\nlearn to detect text regions in an image, and a text recognition network that\ntakes the identified text regions and recognizes their textual content. We\nintroduce the idea behind our novel approach and show its feasibility, by\nperforming a range of experiments on standard benchmark datasets, where we\nachieve competitive results.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:59:26 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Bartz", "Christian", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1712.05444", "submitter": "Hongyu Ren", "authors": "Hongyu Ren, Diqi Chen, Yizhou Wang", "title": "RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality\n  Assessment", "comments": "AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the free-energy brain theory, which implies that human visual\nsystem (HVS) tends to reduce uncertainty and restore perceptual details upon\nseeing a distorted image, we propose restorative adversarial net (RAN), a\nGAN-based model for no-reference image quality assessment (NR-IQA). RAN, which\nmimics the process of HVS, consists of three components: a restorator, a\ndiscriminator and an evaluator. The restorator restores and reconstructs input\ndistorted image patches, while the discriminator distinguishes the\nreconstructed patches from the pristine distortion-free patches. After\nrestoration, we observe that the perceptual distance between the restored and\nthe distorted patches is monotonic with respect to the distortion level. We\nfurther define Gain of Restoration (GoR) based on this phenomenon. The\nevaluator predicts perceptual score by extracting feature representations from\nthe distorted and restored patches to measure GoR. Eventually, the quality\nscore of an input image is estimated by weighted sum of the patch scores.\nExperimental results on Waterloo Exploration, LIVE and TID2013 show the\neffectiveness and generalization ability of RAN compared to the\nstate-of-the-art NR-IQA models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 20:37:49 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Ren", "Hongyu", ""], ["Chen", "Diqi", ""], ["Wang", "Yizhou", ""]]}, {"id": "1712.05474", "submitter": "Roozbeh Mottaghi", "authors": "Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs,\n  Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce The House Of inteRactions (THOR), a framework for visual AI\nresearch, available at http://ai2thor.allenai.org. AI2-THOR consists of near\nphoto-realistic 3D indoor scenes, where AI agents can navigate in the scenes\nand interact with objects to perform tasks. AI2-THOR enables research in many\ndifferent domains including but not limited to deep reinforcement learning,\nimitation learning, learning by interaction, planning, visual question\nanswering, unsupervised representation learning, object detection and\nsegmentation, and learning models of cognition. The goal of AI2-THOR is to\nfacilitate building visually intelligent models and push the research forward\nin this domain.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 23:17:24 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 23:45:48 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 18:29:15 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kolve", "Eric", ""], ["Mottaghi", "Roozbeh", ""], ["Han", "Winson", ""], ["VanderBilt", "Eli", ""], ["Weihs", "Luca", ""], ["Herrasti", "Alvaro", ""], ["Gordon", "Daniel", ""], ["Zhu", "Yuke", ""], ["Gupta", "Abhinav", ""], ["Farhadi", "Ali", ""]]}, {"id": "1712.05482", "submitter": "Shailja Shailja", "authors": "Shailja, Soumabh Bhowmick, Jayanta Mukhopadhyay", "title": "Visual Based Navigation of Mobile Robots", "comments": "Bachelor Thesis, Electrical Engineering Department, IIT Kharagpur,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an algorithm to generate a complete map of the traversable\nregion for a personal assistant robot using monocular vision only. Using\nmultiple taken by a simple webcam, obstacle detection and avoidance algorithms\nhave been developed. Simple Linear Iterative Clustering (SLIC) has been used\nfor segmentation to reduce the memory and computation cost. A simple mapping\ntechnique using inverse perspective mapping and occupancy grids, which is\nrobust, and supports very fast updates has been used to create the map for\nindoor navigation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 00:08:54 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Shailja", "", ""], ["Bhowmick", "Soumabh", ""], ["Mukhopadhyay", "Jayanta", ""]]}, {"id": "1712.05558", "submitter": "Nikita Kitaev", "authors": "Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak\n  Zhang, Yuandong Tian, Dhruv Batra, Devi Parikh", "title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven\n  Communication", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a goal-driven collaborative task that combines\nlanguage, perception, and action. Specifically, we develop a Collaborative\nimage-Drawing game between two agents, called CoDraw. Our game is grounded in a\nvirtual world that contains movable clip art objects. The game involves two\nplayers: a Teller and a Drawer. The Teller sees an abstract scene containing\nmultiple clip art pieces in a semantically meaningful configuration, while the\nDrawer tries to reconstruct the scene on an empty canvas using available clip\nart pieces. The two players communicate with each other using natural language.\nWe collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages\nexchanged between human players. We define protocols and metrics to evaluate\nlearned agents in this testbed, highlighting the need for a novel \"crosstalk\"\nevaluation condition which pairs agents trained independently on disjoint\nsubsets of the training data. We present models for our task and benchmark them\nusing both fully automated evaluation and by having them play the game live\nwith humans.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 06:38:15 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 08:00:14 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 13:01:42 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Kitaev", "Nikita", ""], ["Chen", "Xinlei", ""], ["Rohrbach", "Marcus", ""], ["Zhang", "Byoung-Tak", ""], ["Tian", "Yuandong", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1712.05577", "submitter": "George Philipp", "authors": "George Philipp, Dawn Song, Jaime G. Carbonell", "title": "The exploding gradient problem demystified - definition, prevalence,\n  impact, origin, tradeoffs, and solutions", "comments": "An earlier version of this paper was named \"Gradients explode - Deep\n  Networks are shallow - ResNet explained\" and presented at the ICLR 2018\n  workshop (https://openreview.net/forum?id=rJjcdFkPM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas it is believed that techniques such as Adam, batch normalization and,\nmore recently, SeLU nonlinearities \"solve\" the exploding gradient problem, we\nshow that this is not the case in general and that in a range of popular MLP\narchitectures, exploding gradients exist and that they limit the depth to which\nnetworks can be effectively trained, both in theory and in practice. We explain\nwhy exploding gradients occur and highlight the *collapsing domain problem*,\nwhich can arise in architectures that avoid exploding gradients.\n  ResNets have significantly lower gradients and thus can circumvent the\nexploding gradient problem, enabling the effective training of much deeper\nnetworks. We show this is a direct consequence of the Pythagorean equation. By\nnoticing that *any neural network is a residual network*, we devise the\n*residual trick*, which reveals that introducing skip connections simplifies\nthe network mathematically, and that this simplicity may be the major cause for\ntheir success.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 08:25:51 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 15:36:32 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 17:12:48 GMT"}, {"version": "v4", "created": "Fri, 6 Apr 2018 21:32:29 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Philipp", "George", ""], ["Song", "Dawn", ""], ["Carbonell", "Jaime G.", ""]]}, {"id": "1712.05586", "submitter": "Christian Reul", "authors": "Christian Reul, Christoph Wick, Uwe Springmann, and Frank Puppe", "title": "Transfer Learning for OCRopus Model Training on Early Printed Books", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is presented that significantly reduces the character error rates\nfor OCR text obtained from OCRopus models trained on early printed books when\nonly small amounts of diplomatic transcriptions are available. This is achieved\nby building from already existing models during training instead of starting\nfrom scratch. To overcome the discrepancies between the set of characters of\nthe pretrained model and the additional ground truth the OCRopus code is\nadapted to allow for alphabet expansion or reduction. The character set is now\ncapable of flexibly adding and deleting characters from the pretrained alphabet\nwhen an existing model is loaded. For our experiments we use a self-trained\nmixed model on early Latin prints and the two standard OCRopus models on modern\nEnglish and German Fraktur texts. The evaluation on seven early printed books\nshowed that training from the Latin mixed model reduces the average amount of\nerrors by 43% and 26%, respectively compared to training from scratch with 60\nand 150 lines of ground truth, respectively. Furthermore, it is shown that even\nbuilding from mixed models trained on data unrelated to the newly added\ntraining and test data can lead to significantly improved recognition results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 09:14:38 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 11:32:12 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Reul", "Christian", ""], ["Wick", "Christoph", ""], ["Springmann", "Uwe", ""], ["Puppe", "Frank", ""]]}, {"id": "1712.05615", "submitter": "Egor Ershov I", "authors": "E.I. Ershov and S.M. Karpenko", "title": "Fast Hough Transform and approximation properties of dyadic patterns", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hough transform is a popular low-level computer vision algorithm. Its\ncomputationally effective modification, Fast Hough transform (FHT), makes use\nof special subsets of image matrix to approximate geometric lines on it.\nBecause of their special structure, these subset are called dyadic patterns.\n  In this paper various properties of dyadic patterns are investigated. Exact\nupper bounds on approximation error are derived. In a simplest case, this error\nproves to be equal to $\\frac{1}{6} log(n)$ for $n \\times n$ sized images, as\nwas conjectured previously by Goetz et al.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 11:02:01 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Ershov", "E. I.", ""], ["Karpenko", "S. M.", ""]]}, {"id": "1712.05647", "submitter": "Ribana Roscher", "authors": "Ribana Roscher, Katja Herzog, Annemarie Kunkel, Anna Kicherer,\n  Reinhard T\\\"opfer, Wolfgang F\\\"orstner", "title": "Automated Image Analysis Framework for the High-Throughput Determination\n  of Grapevine Berry Sizes Using Conditional Random Fields", "comments": null, "journal-ref": "Computers and Electronics in Agriculture 100 (2014), 148--158", "doi": "10.1016/j.compag.2013.11.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The berry size is one of the most important fruit traits in grapevine\nbreeding. Non-invasive, image-based phenotyping promises a fast and precise\nmethod for the monitoring of the grapevine berry size. In the present study an\nautomated image analyzing framework was developed in order to estimate the size\nof grapevine berries from images in a high-throughput manner. The framework\nincludes (i) the detection of circular structures which are potentially berries\nand (ii) the classification of these into the class 'berry' or 'non-berry' by\nutilizing a conditional random field. The approach used the concept of a\none-class classification, since only the target class 'berry' is of interest\nand needs to be modeled. Moreover, the classification was carried out by using\nan automated active learning approach, i.e no user interaction is required\nduring the classification process and in addition, the process adapts\nautomatically to changing image conditions, e.g. illumination or berry color.\nThe framework was tested on three datasets consisting in total of 139 images.\nThe images were taken in an experimental vineyard at different stages of\ngrapevine growth according to the BBCH scale. The mean berry size of a plant\nestimated by the framework correlates with the manually measured berry size by\n$0.88$.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 12:51:37 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Roscher", "Ribana", ""], ["Herzog", "Katja", ""], ["Kunkel", "Annemarie", ""], ["Kicherer", "Anna", ""], ["T\u00f6pfer", "Reinhard", ""], ["F\u00f6rstner", "Wolfgang", ""]]}, {"id": "1712.05652", "submitter": "Jack Lindsey", "authors": "Jack Lindsey", "title": "Pre-training Attention Mechanisms", "comments": "Presented at NIPS 2017 Workshop on Cognitively Informed Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks with differentiable attention mechanisms have had\nsuccess in generative and classification tasks. We show that the classification\nperformance of such models can be enhanced by guiding a randomly initialized\nmodel to attend to salient regions of the input in early training iterations.\nWe further show that, if explicit heuristics for guidance are unavailable, a\nmodel that is pretrained on an unsupervised reconstruction task can discover\ngood attention policies without supervision. We demonstrate that increased\nefficiency of the attention mechanism itself contributes to these performance\nimprovements. Based on these insights, we introduce bootstrapped glimpse\nmimicking, a simple, theoretically task-general method of more effectively\ntraining attention models. Our work draws inspiration from and parallels\nresults on human learning of attention.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 12:59:22 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Lindsey", "Jack", ""]]}, {"id": "1712.05695", "submitter": "Altaf Khan", "authors": "Altaf H. Khan", "title": "Lightweight Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the weights in a Lightweight Neural Network have a value of zero,\nwhile the remaining ones are either +1 or -1. These universal approximators\nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass\nand achieve classification accuracies similar to conventional continuous-weight\nnetworks. Their training regimen focuses on error reduction initially, but\nlater emphasizes discretization of weights. They ignore insignificant inputs,\nremove unnecessary weights, and drop unneeded hidden neurons. We have\nsuccessfully tested them on the MNIST, credit card fraud, and credit card\ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4\nmillion weights.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 14:56:05 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Khan", "Altaf H.", ""]]}, {"id": "1712.05765", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, Qixing Huang", "title": "Unsupervised Domain Adaptation for 3D Keypoint Estimation via View\n  Consistency", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel unsupervised domain adaptation technique\nfor the task of 3D keypoint prediction from a single depth scan or image. Our\nkey idea is to utilize the fact that predictions from different views of the\nsame or similar objects should be consistent with each other. Such view\nconsistency can provide effective regularization for keypoint prediction on\nunlabeled instances. In addition, we introduce a geometric alignment term to\nregularize predictions in the target domain. The resulting loss function can be\neffectively optimized via alternating minimization. We demonstrate the\neffectiveness of our approach on real datasets and present experimental results\nshowing that our approach is superior to state-of-the-art general-purpose\ndomain adaptation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 17:46:13 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 04:38:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhou", "Xingyi", ""], ["Karpur", "Arjun", ""], ["Gan", "Chuang", ""], ["Luo", "Linjie", ""], ["Huang", "Qixing", ""]]}, {"id": "1712.05773", "submitter": "Johannes Sch\\\"onberger", "authors": "Johannes L. Sch\\\"onberger, Marc Pollefeys, Andreas Geiger, Torsten\n  Sattler", "title": "Semantic Visual Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual localization under a wide range of viewing conditions is a\nfundamental problem in computer vision. Handling the difficult cases of this\nproblem is not only very challenging but also of high practical relevance,\ne.g., in the context of life-long localization for augmented reality or\nautonomous robots. In this paper, we propose a novel approach based on a joint\n3D geometric and semantic understanding of the world, enabling it to succeed\nunder conditions where previous approaches failed. Our method leverages a novel\ngenerative model for descriptor learning, trained on semantic scene completion\nas an auxiliary task. The resulting 3D descriptors are robust to missing\nobservations by encoding high-level 3D geometric and semantic information.\nExperiments on several challenging large-scale localization datasets\ndemonstrate reliable localization under extreme viewpoint, illumination, and\ngeometry changes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:02:47 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 10:47:51 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Sch\u00f6nberger", "Johannes L.", ""], ["Pollefeys", "Marc", ""], ["Geiger", "Andreas", ""], ["Sattler", "Torsten", ""]]}, {"id": "1712.05790", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Kevin Matzen, Matt Uyttendaele", "title": "Deep Burst Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an inherent issue of low-light image capture, one which is\nexacerbated on mobile devices due to their narrow apertures and small sensors.\nOne strategy for mitigating noise in a low-light situation is to increase the\nshutter time of the camera, thus allowing each photosite to integrate more\nlight and decrease noise variance. However, there are two downsides of long\nexposures: (a) bright regions can exceed the sensor range, and (b) camera and\nscene motion will result in blurred images. Another way of gathering more light\nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently\nintegrate the content, thus avoiding the above downsides. In this paper, we use\nthe burst-capture strategy and implement the intelligent integration via a\nrecurrent fully convolutional deep neural net (CNN). We build our novel,\nmultiframe architecture to be a simple addition to any single frame denoising\nmodel, and design to handle an arbitrary number of noisy input frames. We show\nthat it achieves state of the art denoising results on our burst dataset,\nimproving on the best published multi-frame techniques, such as VBM4D and\nFlexISP. Finally, we explore other applications of image enhancement by\nintegrating content from multiple frames and demonstrate that our DNN\narchitecture generalizes well to image super-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:55:16 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Matzen", "Kevin", ""], ["Uyttendaele", "Matt", ""]]}, {"id": "1712.05799", "submitter": "Stylianos Moschoglou", "authors": "Stylianos Moschoglou, Evangelos Ververas, Yannis Panagakis, Mihalis\n  Nicolaou, Stefanos Zafeiriou", "title": "Multi-Attribute Robust Component Analysis for Facial UV Maps", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2877108", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, due to the collection of large scale 3D face models, as well as the\nadvent of deep learning, a significant progress has been made in the field of\n3D face alignment \"in-the-wild\". That is, many methods have been proposed that\nestablish sparse or dense 3D correspondences between a 2D facial image and a 3D\nface model. The utilization of 3D face alignment introduces new challenges and\nresearch directions, especially on the analysis of facial texture images. In\nparticular, texture does not suffer any more from warping effects (that\noccurred when 2D face alignment methods were used). Nevertheless, since facial\nimages are commonly captured in arbitrary recording conditions, a considerable\namount of missing information and gross outliers is observed (e.g., due to\nself-occlusion, or subjects wearing eye-glasses). Given that many annotated\ndatabases have been developed for face analysis tasks, it is evident that\ncomponent analysis techniques need to be developed in order to alleviate issues\narising from the aforementioned challenges. In this paper, we propose a novel\ncomponent analysis technique that is suitable for facial UV maps containing a\nconsiderable amount of missing information and outliers, while additionally,\nincorporates knowledge from various attributes (such as age and identity). We\nevaluate the proposed Multi-Attribute Robust Component Analysis (MA-RCA) on\nproblems such as UV completion and age progression, where the proposed method\noutperforms compared techniques. Finally, we demonstrate that MA-RCA method is\npowerful enough to provide weak annotations for training deep learning systems\nfor various applications, such as illumination transfer.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:14:59 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Moschoglou", "Stylianos", ""], ["Ververas", "Evangelos", ""], ["Panagakis", "Yannis", ""], ["Nicolaou", "Mihalis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1712.05839", "submitter": "Tobias Tiecke", "authors": "Tobias G. Tiecke, Xianming Liu, Amy Zhang, Andreas Gros, Nan Li,\n  Gregory Yetman, Talip Kilic, Siobhan Murray, Brian Blankespoor, Espen B.\n  Prydz, Hai-Anh H. Dang", "title": "Mapping the world population one building at a time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High resolution datasets of population density which accurately map\nsparsely-distributed human populations do not exist at a global scale.\nTypically, population data is obtained using censuses and statistical modeling.\nMore recently, methods using remotely-sensed data have emerged, capable of\neffectively identifying urbanized areas. Obtaining high accuracy in estimation\nof population distribution in rural areas remains a very challenging task due\nto the simultaneous requirements of sufficient sensitivity and resolution to\ndetect very sparse populations through remote sensing as well as reliable\nperformance at a global scale. Here, we present a computer vision method based\non machine learning to create population maps from satellite imagery at a\nglobal scale, with a spatial sensitivity corresponding to individual buildings\nand suitable for global deployment. By combining this settlement data with\ncensus data, we create population maps with ~30 meter resolution for 18\ncountries. We validate our method, and find that the building identification\nhas an average precision and recall of 0.95 and 0.91, respectively and that the\npopulation estimates have a standard error of a factor ~2 or less. Based on our\ndata, we analyze 29 percent of the world population, and show that 99 percent\nlives within 36 km of the nearest urban cluster. The resulting high-resolution\npopulation datasets have applications in infrastructure planning, vaccination\ncampaign planning, disaster response efforts and risk analysis such as high\naccuracy flood risk analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 21:18:05 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Tiecke", "Tobias G.", ""], ["Liu", "Xianming", ""], ["Zhang", "Amy", ""], ["Gros", "Andreas", ""], ["Li", "Nan", ""], ["Yetman", "Gregory", ""], ["Kilic", "Talip", ""], ["Murray", "Siobhan", ""], ["Blankespoor", "Brian", ""], ["Prydz", "Espen B.", ""], ["Dang", "Hai-Anh H.", ""]]}, {"id": "1712.05870", "submitter": "Tai-Xiang Jiang", "authors": "Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, and Liang-Jian Deng", "title": "Multi-dimensional imaging data recovery via minimizing the partial sum\n  of tubal nuclear norm", "comments": null, "journal-ref": null, "doi": "10.1016/j.cam.2019.112680", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate tensor recovery problems within the tensor\nsingular value decomposition (t-SVD) framework. We propose the partial sum of\nthe tubal nuclear norm (PSTNN) of a tensor. The PSTNN is a surrogate of the\ntensor tubal multi-rank. We build two PSTNN-based minimization models for two\ntypical tensor recovery problems, i.e., the tensor completion and the tensor\nprincipal component analysis. We give two algorithms based on the alternating\ndirection method of multipliers (ADMM) to solve proposed PSTNN-based tensor\nrecovery models. Experimental results on the synthetic data and real-world data\nreveal the superior of the proposed PSTNN.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 22:51:13 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 17:40:15 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 08:12:53 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Jiang", "Tai-Xiang", ""], ["Huang", "Ting-Zhu", ""], ["Zhao", "Xi-Le", ""], ["Deng", "Liang-Jian", ""]]}, {"id": "1712.05896", "submitter": "Hongwei Qin", "authors": "Congrui Hetang, Hongwei Qin, Shaohui Liu, Junjie Yan", "title": "Impression Network for Video Object Detection", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object detection is more challenging compared to image object\ndetection. Previous works proved that applying object detector frame by frame\nis not only slow but also inaccurate. Visual clues get weakened by defocus and\nmotion blur, causing failure on corresponding frames. Multi-frame feature\nfusion methods proved effective in improving the accuracy, but they\ndramatically sacrifice the speed. Feature propagation based methods proved\neffective in improving the speed, but they sacrifice the accuracy. So is it\npossible to improve speed and performance simultaneously?\n  Inspired by how human utilize impression to recognize objects from blurry\nframes, we propose Impression Network that embodies a natural and efficient\nfeature aggregation mechanism. In our framework, an impression feature is\nestablished by iteratively absorbing sparsely extracted frame features. The\nimpression feature is propagated all the way down the video, helping enhance\nfeatures of low-quality frames. This impression mechanism makes it possible to\nperform long-range multi-frame feature fusion among sparse keyframes with\nminimal overhead. It significantly improves per-frame detection baseline on\nImageNet VID while being 3 times faster (20 fps). We hope Impression Network\ncan provide a new perspective on video feature enhancement. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 02:53:30 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Hetang", "Congrui", ""], ["Qin", "Hongwei", ""], ["Liu", "Shaohui", ""], ["Yan", "Junjie", ""]]}, {"id": "1712.05927", "submitter": "Zhichao Liu", "authors": "Bingzhe Wu, Haodong Duan, Zhichao Liu, Guangyu Sun", "title": "SRPGAN: Perceptual Generative Adversarial Network for Single Image Super\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution (SISR) is to reconstruct a high resolution\nimage from a single low resolution image. The SISR task has been a very\nattractive research topic over the last two decades. In recent years,\nconvolutional neural network (CNN) based models have achieved great performance\non SISR task. Despite the breakthroughs achieved by using CNN models, there are\nstill some problems remaining unsolved, such as how to recover high frequency\ndetails of high resolution images. Previous CNN based models always use a pixel\nwise loss, such as l2 loss. Although the high resolution images constructed by\nthese models have high peak signal-to-noise ratio (PSNR), they often tend to be\nblurry and lack high-frequency details, especially at a large scaling factor.\nIn this paper, we build a super resolution perceptual generative adversarial\nnetwork (SRPGAN) framework for SISR tasks. In the framework, we propose a\nrobust perceptual loss based on the discriminator of the built SRPGAN model. We\nuse the Charbonnier loss function to build the content loss and combine it with\nthe proposed perceptual loss and the adversarial loss. Compared with other\nstate-of-the-art methods, our method has demonstrated great ability to\nconstruct images with sharp edges and rich details. We also evaluate our method\non different benchmarks and compare it with previous CNN based methods. The\nresults show that our method can achieve much higher structural similarity\nindex (SSIM) scores on most of the benchmarks than the previous state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 09:52:43 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 17:46:16 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wu", "Bingzhe", ""], ["Duan", "Haodong", ""], ["Liu", "Zhichao", ""], ["Sun", "Guangyu", ""]]}, {"id": "1712.05954", "submitter": "Vasily Morzhakov", "authors": "Vasily Morzhakov, Alexey Redozubov", "title": "An Artificial Neural Network Architecture Based on Context\n  Transformations in Cortical Minicolumns", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical minicolumns are considered a model of cortical organization. Their\nfunction is still a source of research and not reflected properly in modern\narchitecture of nets in algorithms of Artificial Intelligence. We assume its\nfunction and describe it in this article. Furthermore, we show how this\nproposal allows to construct a new architecture, that is not based on\nconvolutional neural networks, test it on MNIST data and receive close to\nConvolutional Neural Network accuracy. We also show that the proposed\narchitecture possesses an ability to train on a small quantity of samples. To\nachieve these results, we enable the minicolumns to remember context\ntransformations.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 13:14:03 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Morzhakov", "Vasily", ""], ["Redozubov", "Alexey", ""]]}, {"id": "1712.05969", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Learning a Virtual Codec Based on Deep Convolutional Neural Network to\n  Compress Image", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural network has been proved to efficiently\neliminate coding artifacts caused by the coarse quantization of traditional\ncodec, it's difficult to train any neural network in front of the encoder for\ngradient's back-propagation. In this paper, we propose an end-to-end image\ncompression framework based on convolutional neural network to resolve the\nproblem of non-differentiability of the quantization function in the standard\ncodec. First, the feature description neural network is used to get a valid\ndescription in the low-dimension space with respect to the ground-truth image\nso that the amount of image data is greatly reduced for storage or\ntransmission. After image's valid description, standard image codec such as\nJPEG is leveraged to further compress image, which leads to image's great\ndistortion and compression artifacts, especially blocking artifacts, detail\nmissing, blurring, and ringing artifacts. Then, we use a post-processing neural\nnetwork to remove these artifacts. Due to the challenge of directly learning a\nnon-linear function for a standard codec based on convolutional neural network,\nwe propose to learn a virtual codec neural network to approximate the\nprojection from the valid description image to the post-processed compressed\nimage, so that the gradient could be efficiently back-propagated from the\npost-processing neural network to the feature description neural network during\ntraining. Meanwhile, an advanced learning algorithm is proposed to train our\ndeep neural networks for compression. Obviously, the priority of the proposed\nmethod is compatible with standard existing codecs and our learning strategy\ncan be easily extended into these codecs based on convolutional neural network.\nExperimental results have demonstrated the advances of the proposed method as\ncompared to several state-of-the-art approaches, especially at very low\nbit-rate.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 14:55:13 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 16:50:36 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 08:33:09 GMT"}, {"version": "v4", "created": "Tue, 2 Jan 2018 12:26:16 GMT"}, {"version": "v5", "created": "Mon, 8 Jan 2018 03:52:47 GMT"}, {"version": "v6", "created": "Sun, 14 Jan 2018 03:44:22 GMT"}, {"version": "v7", "created": "Tue, 16 Jan 2018 10:02:55 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1712.06020", "submitter": "Ruobing Shen", "authors": "Ruobing Shen, Eric Kendinibilir, Ismail Ben Ayed, Andrea Lodi, Andrea\n  Tramontani, Gerhard Reinelt", "title": "An ILP Solver for Multi-label MRFs with Connectivity Constraints", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer Linear Programming (ILP) formulations of Markov random fields (MRFs)\nmodels with global connectivity priors were investigated previously in computer\nvision, e.g., \\cite{globalinter,globalconn}. In these works, only Linear\nPrograming (LP) relaxations \\cite{globalinter,globalconn} or simplified\nversions \\cite{graphcutbase} of the problem were solved. This paper\ninvestigates the ILP of multi-label MRF with exact connectivity priors via a\nbranch-and-cut method, which provably finds globally optimal solutions. The\nmethod enforces connectivity priors iteratively by a cutting plane method, and\nprovides feasible solutions with a guarantee on sub-optimality even if we\nterminate it earlier. The proposed ILP can be applied as a post-processing\nmethod on top of any existing multi-label segmentation approach. As it provides\nglobally optimal solution, it can be used off-line to generate ground-truth\nlabeling, which serves as quality check for any fast on-line algorithm.\nFurthermore, it can be used to generate ground-truth proposals for weakly\nsupervised segmentation. We demonstrate the power and usefulness of our model\nby several experiments on the BSDS500 and PASCAL image dataset, as well as on\nmedical images with trained probability maps.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 21:19:44 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 09:43:47 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Shen", "Ruobing", ""], ["Kendinibilir", "Eric", ""], ["Ayed", "Ismail Ben", ""], ["Lodi", "Andrea", ""], ["Tramontani", "Andrea", ""], ["Reinelt", "Gerhard", ""]]}, {"id": "1712.06061", "submitter": "Praneeth Narayanamurthy", "authors": "Praneeth Narayanamurthy and Namrata Vaswani", "title": "Nearly Optimal Robust Subspace Tracking", "comments": "A [short\n  version](http://proceedings.mlr.press/v80/narayanamurthy18a.html) will be\n  presented at ICML 2018 (Long Talk). arXiv admin note: text overlap with\n  arXiv:1803.00651", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the robust subspace tracking (RST) problem and obtain\none of the first two provable guarantees for it. The goal of RST is to track\nsequentially arriving data vectors that lie in a slowly changing\nlow-dimensional subspace, while being robust to corruption by additive sparse\noutliers. It can also be interpreted as a dynamic (time-varying) extension of\nrobust PCA (RPCA), with the minor difference that RST also requires a short\ntracking delay. We develop a recursive projected compressive sensing algorithm\nthat we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its\ntracking delay is nearly optimal. We prove that NORST solves both the RST and\nthe dynamic RPCA problems under weakened standard RPCA assumptions, two simple\nextra assumptions (slow subspace change and most outlier magnitudes lower\nbounded), and a few minor assumptions.\n  Our guarantee shows that NORST enjoys a near optimal tracking delay of $O(r\n\\log n \\log(1/\\epsilon))$. Its required delay between subspace change times is\nthe same, and its memory complexity is $n$ times this value. Thus both these\nare also nearly optimal. Here $n$ is the ambient space dimension, $r$ is the\nsubspaces' dimension, and $\\epsilon$ is the tracking accuracy. NORST also has\nthe best outlier tolerance compared with all previous RPCA or RST methods, both\ntheoretically and empirically (including for real videos), without requiring\nany model on how the outlier support is generated. This is possible because of\nthe extra assumptions it uses.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 06:14:58 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 00:29:16 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 23:03:55 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 17:41:33 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Narayanamurthy", "Praneeth", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1712.06080", "submitter": "Xingang Pan", "authors": "Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, Xiaoou Tang", "title": "Spatial As Deep: Spatial CNN for Traffic Scene Understanding", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are usually built by stacking\nconvolutional operations layer-by-layer. Although CNN has shown strong\ncapability to extract semantics from raw pixels, its capacity to capture\nspatial relationships of pixels across rows and columns of an image is not\nfully explored. These relationships are important to learn semantic objects\nwith strong shape priors but weak appearance coherences, such as traffic lanes,\nwhich are often occluded or not even painted on the road surface as shown in\nFig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes\ntraditional deep layer-by-layer convolutions to slice-byslice convolutions\nwithin feature maps, thus enabling message passings between pixels across rows\nand columns in a layer. Such SCNN is particular suitable for long continuous\nshape structure or large objects, with strong spatial relationship but less\nappearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a\nnewly released very challenging traffic lane detection dataset and Cityscapse\ndataset. The results show that SCNN could learn the spatial relationship for\nstructure output and significantly improves the performance. We show that SCNN\noutperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet)\nin the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN\nwon the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an\naccuracy of 96.53%.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 09:37:52 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Pan", "Xingang", ""], ["Shi", "Jianping", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1712.06087", "submitter": "Assaf Shocher", "authors": "Assaf Shocher, Nadav Cohen, Michal Irani", "title": "\"Zero-Shot\" Super-Resolution using Deep Internal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance\nin the past few years. However, being supervised, these SR methods are\nrestricted to specific training data, where the acquisition of the\nlow-resolution (LR) images from their high-resolution (HR) counterparts is\npredetermined (e.g., bicubic downscaling), without any distracting artifacts\n(e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images,\nhowever, rarely obey these restrictions, resulting in poor SR results by SotA\n(State of the Art) methods. In this paper we introduce \"Zero-Shot\" SR, which\nexploits the power of Deep Learning, but does not rely on prior training. We\nexploit the internal recurrence of information inside a single image, and train\na small image-specific CNN at test time, on examples extracted solely from the\ninput image itself. As such, it can adapt itself to different settings per\nimage. This allows to perform SR of real old photos, noisy images, biological\ndata, and other images where the acquisition process is unknown or non-ideal.\nOn such images, our method outperforms SotA CNN-based SR methods, as well as\nprevious unsupervised SR methods. To the best of our knowledge, this is the\nfirst unsupervised CNN-based SR method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 11:00:30 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Shocher", "Assaf", ""], ["Cohen", "Nadav", ""], ["Irani", "Michal", ""]]}, {"id": "1712.06096", "submitter": "Jong Chul Ye", "authors": "Yeo Hun Yoon, Shujaat Khan, Jaeyoung Huh, and Jong Chul Ye", "title": "Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF\n  Data using Deep Learning", "comments": "The title has been changed. This version will appear in IEEE Trans.\n  on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In portable, three dimensional, and ultra-fast ultrasound imaging systems,\nthere is an increasing demand for the reconstruction of high quality images\nfrom a limited number of radio-frequency (RF) measurements due to receiver (Rx)\nor transmit (Xmit) event sub-sampling. However, due to the presence of side\nlobe artifacts from RF sub-sampling, the standard beamformer often produces\nblurry images with less contrast, which are unsuitable for diagnostic purposes.\nExisting compressed sensing approaches often require either hardware changes or\ncomputationally expensive algorithms, but their quality improvements are\nlimited. To address this problem, here we propose a novel deep learning\napproach that directly interpolates the missing RF data by utilizing redundancy\nin the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF\ndata from a multi-line acquisition B-mode system confirm that the proposed\nmethod can effectively reduce the data rate without sacrificing image quality.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 12:15:08 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 03:58:18 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 09:19:13 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Yoon", "Yeo Hun", ""], ["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1712.06107", "submitter": "Ritika S", "authors": "S Ritika, Shruti Mittal, Dattaraj Rao", "title": "Railway Track Specific Traffic Signal Selection Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the railway transportation Industry moving actively towards automation,\naccurate location and inventory of wayside track assets like traffic signals,\ncrossings, switches, mileposts, etc. is of extreme importance. With the new\nPositive Train Control (PTC) regulation coming into effect, many railway safety\nrules will be tied directly to location of assets like mileposts and signals.\nNewer speed regulations will be enforced based on location of the Train with\nrespect to a wayside asset. Hence it is essential for the railroads to have an\naccurate database of the types and locations of these assets. This paper talks\nabout a real-world use-case of detecting railway signals from a camera mounted\non a moving locomotive and tracking their locations. The camera is engineered\nto withstand the environment factors on a moving train and provide a consistent\nsteady image at around 30 frames per second. Using advanced image analysis and\ndeep learning techniques, signals are detected in these camera images and a\ndatabase of their locations is created. Railway signals differ a lot from road\nsignals in terms of shapes and rules for placement with respect to track. Due\nto space constraint and traffic densities in urban areas signals are not placed\non the same side of the track and multiple lines can run in parallel. Hence\nthere is need to associate signal detected with the track on which the train\nruns. We present a method to associate the signals to the specific track they\nbelong to using a video feed from the front facing camera mounted on the lead\nlocomotive. A pipeline of track detection, region of interest selection, signal\ndetection has been implemented which gives an overall accuracy of 94.7% on a\nroute covering 150km with 247 signals.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 13:00:25 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Ritika", "S", ""], ["Mittal", "Shruti", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1712.06116", "submitter": "Kai Zhang", "authors": "Kai Zhang, Wangmeng Zuo, Lei Zhang", "title": "Learning a Single Convolutional Super-Resolution Network for Multiple\n  Degradations", "comments": "CVPR 2018, code: https://github.com/cszn/SRMD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the unprecedented success of deep convolutional\nneural networks (CNNs) in single image super-resolution (SISR). However,\nexisting CNN-based SISR methods mostly assume that a low-resolution (LR) image\nis bicubicly downsampled from a high-resolution (HR) image, thus inevitably\ngiving rise to poor performance when the true degradation does not follow this\nassumption. Moreover, they lack scalability in learning a single model to\nnon-blindly deal with multiple degradations. To address these issues, we\npropose a general framework with dimensionality stretching strategy that\nenables a single convolutional super-resolution network to take two key factors\nof the SISR degradation process, i.e., blur kernel and noise level, as input.\nConsequently, the super-resolver can handle multiple and even spatially variant\ndegradations, which significantly improves the practicability. Extensive\nexperimental results on synthetic and real LR images show that the proposed\nconvolutional super-resolution network not only can produce favorable results\non multiple degradations but also is computationally efficient, providing a\nhighly effective and scalable solution to practical SISR applications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 14:04:47 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 13:41:28 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1712.06131", "submitter": "Ambra Demontis", "authors": "Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera and Fabio\n  Roli", "title": "Super-sparse Learning in Similarity Spaces", "comments": null, "journal-ref": "IEEE Computational Intell. Mag., 11(4):36-45, Nov 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applications, input samples are more naturally represented in\nterms of similarities between each other, rather than in terms of feature\nvectors. In these settings, machine-learning algorithms can become very\ncomputationally demanding, as they may require matching the test samples\nagainst a very large set of reference prototypes. To mitigate this issue,\ndifferent approaches have been developed to reduce the number of required\nreference prototypes. Current reduction approaches select a small subset of\nrepresentative prototypes in the space induced by the similarity measure, and\nthen separately train the classification function on the reduced subset.\nHowever, decoupling these two steps may not allow reducing the number of\nprototypes effectively without compromising accuracy. We overcome this\nlimitation by jointly learning the classification function along with an\noptimal set of virtual prototypes, whose number can be either fixed a priori or\noptimized according to application-specific criteria. Creating a super-sparse\nset of virtual prototypes provides much sparser solutions, drastically reducing\ncomplexity at test time, at the expense of a slightly increased complexity\nduring training. A much smaller set of prototypes also results in\neasier-to-interpret decisions. We empirically show that our approach can reduce\nup to ten times the complexity of Support Vector Machines, LASSO and ridge\nregression at test time, without almost affecting their classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 15:59:38 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Demontis", "Ambra", ""], ["Melis", "Marco", ""], ["Biggio", "Battista", ""], ["Fumera", "Giorgio", ""], ["Roli", "Fabio", ""]]}, {"id": "1712.06145", "submitter": "Dong-Qing Zhang", "authors": "Dong-Qing Zhang", "title": "clcNet: Improving the Efficiency of Convolutional Neural Network using\n  Channel Local Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise convolution and grouped convolution has been successfully applied\nto improve the efficiency of convolutional neural network (CNN). We suggest\nthat these models can be considered as special cases of a generalized\nconvolution operation, named channel local convolution(CLC), where an output\nchannel is computed using a subset of the input channels. This definition\nentails computation dependency relations between input and output channels,\nwhich can be represented by a channel dependency graph(CDG). By modifying the\nCDG of grouped convolution, a new CLC kernel named interlaced grouped\nconvolution (IGC) is created. Stacking IGC and GC kernels results in a\nconvolution block (named CLC Block) for approximating regular convolution. By\nresorting to the CDG as an analysis tool, we derive the rule for setting the\nmeta-parameters of IGC and GC and the framework for minimizing the\ncomputational cost. A new CNN model named clcNet is then constructed using CLC\nblocks, which shows significantly higher computational efficiency and fewer\nparameters compared to state-of-the-art networks, when being tested using the\nImageNet-1K dataset. Source code is available at\nhttps://github.com/dqzhang17/clcnet.torch .\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 17:07:54 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 17:13:31 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 09:33:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Dong-Qing", ""]]}, {"id": "1712.06203", "submitter": "Kuang Gong", "authors": "Kuang Gong, Jaewon Yang, Kyungsang Kim, Georges El Fakhri, Youngho\n  Seo, Quanzheng Li", "title": "Attenuation correction for brain PET imaging using deep neural network\n  based on dixon and ZTE MR images", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": "10.1088/1361-6560/aac763", "report-no": null, "categories": "physics.med-ph cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography (PET) is a functional imaging modality widely\nused in neuroscience studies. To obtain meaningful quantitative results from\nPET images, attenuation correction is necessary during image reconstruction.\nFor PET/MR hybrid systems, PET attenuation is challenging as Magnetic Resonance\n(MR) images do not reflect attenuation coefficients directly. To address this\nissue, we present deep neural network methods to derive the continuous\nattenuation coefficients for brain PET imaging from MR images. With only Dixon\nMR images as the network input, the existing U-net structure was adopted and\nanalysis using forty patient data sets shows it is superior than other Dixon\nbased methods. When both Dixon and zero echo time (ZTE) images are available,\nwe have proposed a modified U-net structure, named GroupU-net, to efficiently\nmake use of both Dixon and ZTE information through group convolution modules\nwhen the network goes deeper. Quantitative analysis based on fourteen real\npatient data sets demonstrates that both network approaches can perform better\nthan the standard methods, and the proposed network structure can further\nreduce the PET quantification error compared to the U-net structure.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 23:07:35 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 20:19:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Gong", "Kuang", ""], ["Yang", "Jaewon", ""], ["Kim", "Kyungsang", ""], ["Fakhri", "Georges El", ""], ["Seo", "Youngho", ""], ["Li", "Quanzheng", ""]]}, {"id": "1712.06228", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Byoung-Tak Zhang", "title": "Visual Explanations from Hadamard Product in Multimodal Deep Networks", "comments": "8 pages, 5 figures, including appendix, NIPS 2017 Workshop on\n  Visually-Grounded Interaction and Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual explanation of learned representation of models helps to\nunderstand the fundamentals of learning. The attentional models of previous\nworks used to visualize the attended regions over an image or text using their\nlearned weights to confirm their intended mechanism. Kim et al. (2016) show\nthat the Hadamard product in multimodal deep networks, which is well-known for\nthe joint function of visual question answering tasks, implicitly performs an\nattentional mechanism for visual inputs. In this work, we extend their work to\nshow that the Hadamard product in multimodal deep networks performs not only\nfor visual inputs but also for textual inputs simultaneously using the proposed\ngradient-based visualization technique. The attentional effect of Hadamard\nproduct is visualized for both visual and textual inputs by analyzing the two\ninputs and an output of the Hadamard product with the proposed method and\ncompared with learned attentional weights of a visual question answering model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 02:37:20 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1712.06229", "submitter": "Chen Gao", "authors": "Brian E. Moore, Chen Gao, Raj Rao Nadakuditi", "title": "Panoramic Robust PCA for Foreground-Background Separation on Noisy,\n  Free-Motion Camera Video", "comments": "IEEE TCI. Project webpage: https://gaochen315.github.io/pRPCA/ Code:\n  https://github.com/gaochen315/panoramicRPCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new robust PCA method for foreground-background\nseparation on freely moving camera video with possible dense and sparse\ncorruptions. Our proposed method registers the frames of the corrupted video\nand then encodes the varying perspective arising from camera motion as missing\ndata in a global model. This formulation allows our algorithm to produce a\npanoramic background component that automatically stitches together corrupted\ndata from partially overlapping frames to reconstruct the full field of view.\nWe model the registered video as the sum of a low-rank component that captures\nthe background, a smooth component that captures the dynamic foreground of the\nscene, and a sparse component that isolates possible outliers and other sparse\ncorruptions in the video. The low-rank portion of our model is based on a\nrecent low-rank matrix estimator (OptShrink) that has been shown to yield\nsuperior low-rank subspace estimates in practice. To estimate the smooth\nforeground component of our model, we use a weighted total variation framework\nthat enables our method to reliably decouple the true foreground of the video\nfrom sparse corruptions. We perform extensive numerical experiments on both\nstatic and moving camera video subject to a variety of dense and sparse\ncorruptions. Our experiments demonstrate the state-of-the-art performance of\nour proposed method compared to existing methods both in terms of foreground\nand background estimation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 02:45:54 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 18:59:20 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 03:42:51 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Moore", "Brian E.", ""], ["Gao", "Chen", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1712.06302", "submitter": "Jose Oramas", "authors": "Jose Oramas, Kaili Wang, Tinne Tuytelaars", "title": "Visual Explanation by Interpretation: Improving Visual Feedback\n  Capabilities of Deep Neural Networks", "comments": "Accepted at International Conference on Learning Representations\n  (ICLR) 2019. Project website:\n  http://homes.esat.kuleuven.be/~joramas/projects/visualExplanationByInterpretation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation and explanation of deep models is critical towards wide\nadoption of systems that rely on them. In this paper, we propose a novel scheme\nfor both interpretation as well as explanation in which, given a pretrained\nmodel, we automatically identify internal features relevant for the set of\nclasses considered by the model, without relying on additional annotations. We\ninterpret the model through average visualizations of this reduced set of\nfeatures. Then, at test time, we explain the network prediction by accompanying\nthe predicted class label with supporting visualizations derived from the\nidentified features. In addition, we propose a method to address the artifacts\nintroduced by stridded operations in deconvNet-based visualizations. Moreover,\nwe introduce an8Flower, a dataset specifically designed for objective\nquantitative evaluation of methods for visual explanation.Experiments on the\nMNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces\ndetailed explanations with good coverage of relevant features of the classes of\ninterest\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 09:17:44 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 15:04:25 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 12:11:15 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Oramas", "Jose", ""], ["Wang", "Kaili", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1712.06316", "submitter": "Yue Luo", "authors": "Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu,\n  Jiahao Pang, Liang Lin", "title": "LSTM Pose Machines", "comments": "Poster in IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observed that recent state-of-the-art results on single image human pose\nestimation were achieved by multi-stage Convolution Neural Networks (CNN).\nNotwithstanding the superior performance on static images, the application of\nthese models on videos is not only computationally intensive, it also suffers\nfrom performance degeneration and flicking. Such suboptimal results are mainly\nattributed to the inability of imposing sequential geometric consistency,\nhandling severe image quality degradation (e.g. motion blur and occlusion) as\nwell as the inability of capturing the temporal correlation among video frames.\nIn this paper, we proposed a novel recurrent network to tackle these problems.\nWe showed that if we were to impose the weight sharing scheme to the\nmulti-stage CNN, it could be re-written as a Recurrent Neural Network (RNN).\nThis property decouples the relationship among multiple network stages and\nresults in significantly faster speed in invoking the network for videos. It\nalso enables the adoption of Long Short-Term Memory (LSTM) units between video\nframes. We found such memory augmented RNN is very effective in imposing\ngeometric consistency among frames. It also well handles input quality\ndegradation in videos while successfully stabilizes the sequential outputs. The\nexperiments showed that our approach significantly outperformed current\nstate-of-the-art methods on two large-scale video pose estimation benchmarks.\nWe also explored the memory cells inside the LSTM and provided insights on why\nsuch mechanism would benefit the prediction for video-based pose estimations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 09:56:45 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 02:54:09 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 03:39:55 GMT"}, {"version": "v4", "created": "Fri, 9 Mar 2018 07:44:38 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Luo", "Yue", ""], ["Ren", "Jimmy", ""], ["Wang", "Zhouxia", ""], ["Sun", "Wenxiu", ""], ["Pan", "Jinshan", ""], ["Liu", "Jianbo", ""], ["Pang", "Jiahao", ""], ["Lin", "Liang", ""]]}, {"id": "1712.06317", "submitter": "Fanyi Xiao", "authors": "Fanyi Xiao, Yong Jae Lee", "title": "Video Object Detection with an Aligned Spatial-Temporal Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Spatial-Temporal Memory Networks for video object detection. At\nits core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent\ncomputation unit to model long-term temporal appearance and motion dynamics.\nThe STMM's design enables full integration of pretrained backbone CNN weights,\nwhich we find to be critical for accurate detection. Furthermore, in order to\ntackle object motion in videos, we propose a novel MatchTrans module to align\nthe spatial-temporal memory from frame to frame. Our method produces\nstate-of-the-art results on the benchmark ImageNet VID dataset, and our\nablative studies clearly demonstrate the contribution of our different design\nchoices. We release our code and models at\nhttp://fanyix.cs.ucdavis.edu/project/stmn/project.html.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 10:02:23 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 00:54:29 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 00:47:46 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1712.06326", "submitter": "Tim Dahmen", "authors": "Tim Dahmen, Patrick Trampert, Pascal Peter, Pinak Bheed, Joachim\n  Weickert, Philipp Slusallek", "title": "Space-Filling Curve Indices as Acceleration Structure for Exemplar-Based\n  Inpainting", "comments": "submitted to Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based inpainting is the process of reconstructing missing parts of\nan image by searching the remaining data for patches that fit seamlessly. The\nimage is completed to a plausible-looking solution by repeatedly inserting the\npatch that is the best match according to some cost function. We present an\nacceleration structure that uses a multi-index scheme to accelerate this search\nprocedure drastically, particularly in the case of very large datasets. The\nindex scheme uses ideas such as dimensionality reduction and k-nearest neighbor\nsearch on space-filling curves that are well known in the field of multimedia\ndatabases. Our method has a theoretic runtime of O(log2 n) per iteration and\nreaches a speedup factor of up to 660 over the original method. The approach\nhas the advantage of being agnostic to most modelbased parts of exemplar-based\ninpainting such as the order in which patches are processed and the cost\nfunction used to determine patch similarity. Thus, the acceleration structure\ncan be used in conjunction with most exemplar-based inpainting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 10:28:36 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 08:57:58 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Dahmen", "Tim", ""], ["Trampert", "Patrick", ""], ["Peter", "Pascal", ""], ["Bheed", "Pinak", ""], ["Weickert", "Joachim", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1712.06391", "submitter": "Xudong Mao", "authors": "Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen\n  Paul Smolley", "title": "On the Effectiveness of Least Squares Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning with generative adversarial networks (GANs) has proven\nto be hugely successful. Regular GANs hypothesize the discriminator as a\nclassifier with the sigmoid cross entropy loss function. However, we found that\nthis loss function may lead to the vanishing gradients problem during the\nlearning process. To overcome such a problem, we propose in this paper the\nLeast Squares Generative Adversarial Networks (LSGANs) which adopt the least\nsquares loss for both the discriminator and the generator. We show that\nminimizing the objective function of LSGAN yields minimizing the Pearson\n$\\chi^2$ divergence. We also show that the derived objective function that\nyields minimizing the Pearson $\\chi^2$ divergence performs better than the\nclassical one of using least squares for classification. There are two benefits\nof LSGANs over regular GANs. First, LSGANs are able to generate higher quality\nimages than regular GANs. Second, LSGANs perform more stably during the\nlearning process. For evaluating the image quality, we conduct both qualitative\nand quantitative experiments, and the experimental results show that LSGANs can\ngenerate higher quality images than regular GANs. Furthermore, we evaluate the\nstability of LSGANs in two groups. One is to compare between LSGANs and regular\nGANs without gradient penalty. We conduct three experiments, including Gaussian\nmixture distribution, difficult architectures, and a newly proposed method ---\ndatasets with small variability, to illustrate the stability of LSGANs. The\nother one is to compare between LSGANs with gradient penalty (LSGANs-GP) and\nWGANs with gradient penalty (WGANs-GP). The experimental results show that\nLSGANs-GP succeed in training for all the difficult architectures used in\nWGANs-GP, including 101-layer ResNet.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 13:36:09 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 07:48:53 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Mao", "Xudong", ""], ["Li", "Qing", ""], ["Xie", "Haoran", ""], ["Lau", "Raymond Y. K.", ""], ["Wang", "Zhen", ""], ["Smolley", "Stephen Paul", ""]]}, {"id": "1712.06405", "submitter": "Djordje Slijepcevic", "authors": "Djordje Slijepcevic, Matthias Zeppelzauer, Anna-Maria Gorgas, Caterine\n  Schwab, Michael Sch\\\"uller, Arnold Baca, Christian Breiteneder, Brian Horsak", "title": "Automatic Classification of Functional Gait Disorders", "comments": "9 pages, 3 figures, IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2017.2785682", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a comprehensive investigation of the automatic\nclassification of functional gait disorders based solely on ground reaction\nforce (GRF) measurements. The aim of the study is twofold: (1) to investigate\nthe suitability of stateof-the-art GRF parameterization techniques\n(representations) for the discrimination of functional gait disorders; and (2)\nto provide a first performance baseline for the automated classification of\nfunctional gait disorders for a large-scale dataset. The utilized database\ncomprises GRF measurements from 279 patients with gait disorders (GDs) and data\nfrom 161 healthy controls (N). Patients were manually classified into four\nclasses with different functional impairments associated with the \"hip\",\n\"knee\", \"ankle\", and \"calcaneus\". Different parameterizations are investigated:\nGRF parameters, global principal component analysis (PCA)-based representations\nand a combined representation applying PCA on GRF parameters. The\ndiscriminative power of each parameterization for different classes is\ninvestigated by linear discriminant analysis (LDA). Based on this analysis, two\nclassification experiments are pursued: (1) distinction between healthy and\nimpaired gait (N vs. GD) and (2) multi-class classification between healthy\ngait and all four GD classes. Experiments show promising results and reveal\namong others that several factors, such as imbalanced class cardinalities and\nvarying numbers of measurement sessions per patient have a strong impact on the\nclassification accuracy and therefore need to be taken into account. The\nresults represent a promising first step towards the automated classification\nof gait disorders and a first performance baseline for future developments in\nthis direction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 14:09:01 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 10:00:52 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Slijepcevic", "Djordje", ""], ["Zeppelzauer", "Matthias", ""], ["Gorgas", "Anna-Maria", ""], ["Schwab", "Caterine", ""], ["Sch\u00fcller", "Michael", ""], ["Baca", "Arnold", ""], ["Breiteneder", "Christian", ""], ["Horsak", "Brian", ""]]}, {"id": "1712.06424", "submitter": "Danyang Sun", "authors": "Danyang Sun, Tongzheng Ren, Chongxun Li, Hang Su, Jun Zhu", "title": "Learning to Write Stylized Chinese Characters by Reading a Handful of\n  Examples", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically writing stylized Chinese characters is an attractive yet\nchallenging task due to its wide applicabilities. In this paper, we propose a\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\ngenerate Chinese characters. Specifically, we propose to capture the different\ncharacteristics of a Chinese character by disentangling the latent features\ninto content-related and style-related components. Considering of the complex\nshapes and structures, we incorporate the structure information as prior\nknowledge into our framework to guide the generation. Our framework shows a\npowerful one-shot/low-shot generalization ability by inferring the style\ncomponent given a character with unseen style. To the best of our knowledge,\nthis is the first attempt to learn to write new-style Chinese characters by\nobserving only one or a few examples. Extensive experiments demonstrate its\neffectiveness in generating different stylized Chinese characters by fusing the\nfeature vectors corresponding to different contents and styles, which is of\nsignificant importance in real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 13:33:51 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 09:18:22 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 11:17:09 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sun", "Danyang", ""], ["Ren", "Tongzheng", ""], ["Li", "Chongxun", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.06452", "submitter": "Ester Bonmati", "authors": "Ester Bonmati, Yipeng Hu, Nikhil Sindhwani, Hans Peter Dietz, Jan\n  D'hooge, Dean Barratt, Jan Deprest, Tom Vercauteren", "title": "Automatic segmentation method of pelvic floor levator hiatus in\n  ultrasound using a self-normalising neural network", "comments": null, "journal-ref": null, "doi": "10.1117/1.JMI.5.2.021206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the levator hiatus in ultrasound allows to extract biometrics\nwhich are of importance for pelvic floor disorder assessment. In this work, we\npresent a fully automatic method using a convolutional neural network (CNN) to\noutline the levator hiatus in a 2D image extracted from a 3D ultrasound volume.\nIn particular, our method uses a recently developed scaled exponential linear\nunit (SELU) as a nonlinear self-normalising activation function, which for the\nfirst time has been applied in medical imaging with CNN. SELU has important\nadvantages such as being parameter-free and mini-batch independent, which may\nhelp to overcome memory constraints during training. A dataset with 91 images\nfrom 35 patients during Valsalva, contraction and rest, all labelled by three\noperators, is used for training and evaluation in a leave-one-patient-out\ncross-validation. Results show a median Dice similarity coefficient of 0.90\nwith an interquartile range of 0.08, with equivalent performance to the three\noperators (with a Williams' index of 1.03), and outperforming a U-Net\narchitecture without the need for batch normalisation. We conclude that the\nproposed fully automatic method achieved equivalent accuracy in segmenting the\npelvic floor levator hiatus compared to a previous semi-automatic approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:04:21 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Bonmati", "Ester", ""], ["Hu", "Yipeng", ""], ["Sindhwani", "Nikhil", ""], ["Dietz", "Hans Peter", ""], ["D'hooge", "Jan", ""], ["Barratt", "Dean", ""], ["Deprest", "Jan", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1712.06463", "submitter": "Xu Jia", "authors": "Xu Jia, Hong Chang, Tinne Tuytelaars", "title": "Super-Resolution with Deep Adaptive Image Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have recently pushed the state-of-the-art on the\nproblem of Single Image Super-Resolution (SISR). In this work, we revisit the\nmore traditional interpolation-based methods, that were popular before, now\nwith the help of deep learning. In particular, we propose to use a\nConvolutional Neural Network (CNN) to estimate spatially variant interpolation\nkernels and apply the estimated kernels adaptively to each position in the\nimage. The whole model is trained in an end-to-end manner. We explore two ways\nto improve the results for the case of large upscaling factors, and propose a\nrecursive extension of our basic model. This achieves results that are on par\nwith state-of-the-art methods. We visualize the estimated adaptive\ninterpolation kernels to gain more insight on the effectiveness of the proposed\nmethod. We also extend the method to the task of joint image filtering and\nagain achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:19:14 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Jia", "Xu", ""], ["Chang", "Hong", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1712.06467", "submitter": "Chaoqun Hong", "authors": "Chaoqun Hong, Jun Yu", "title": "Multi-modal Face Pose Estimation with Multi-task Manifold Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face pose estimation aims at estimating the gazing direction or head\npostures with 2D images. It gives some very important information such as\ncommunicative gestures, saliency detection and so on, which attracts plenty of\nattention recently. However, it is challenging because of complex background,\nvarious orientations and face appearance visibility. Therefore, a descriptive\nrepresentation of face images and mapping it to poses are critical. In this\npaper, we make use of multi-modal data and propose a novel face pose estimation\nmethod that uses a novel deep learning framework named Multi-task Manifold Deep\nLearning $M^2DL$. It is based on feature extraction with improved deep neural\nnetworks and multi-modal mapping relationship with multi-task learning. In the\nproposed deep learning based framework, Manifold Regularized Convolutional\nLayers (MRCL) improve traditional convolutional layers by learning the\nrelationship among outputs of neurons. Besides, in the proposed mapping\nrelationship learning method, different modals of face representations are\nnaturally combined to learn the mapping function from face images to poses. In\nthis way, the computed mapping model with multiple tasks is improved.\nExperimental results on three challenging benchmark datasets DPOSE, HPID and\nBKHPD demonstrate the outstanding performance of $M^2DL$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:22:26 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Hong", "Chaoqun", ""], ["Yu", "Jun", ""]]}, {"id": "1712.06492", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Matthias K\\\"ummerer, Thomas S. A. Wallis and Matthias\n  Bethge", "title": "Guiding human gaze with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eye fixation patterns of human observers are a fundamental indicator of\nthe aspects of an image to which humans attend. Thus, manipulating fixation\npatterns to guide human attention is an exciting challenge in digital image\nprocessing. Here, we present a new model for manipulating images to change the\ndistribution of human fixations in a controlled fashion. We use the\nstate-of-the-art model for fixation prediction to train a convolutional neural\nnetwork to transform images so that they satisfy a given fixation distribution.\nFor network training, we carefully design a loss function to achieve a\nperceptual effect while preserving naturalness of the transformed images.\nFinally, we evaluate the success of our model by measuring human fixations for\na set of manipulated images. On our test images we can in-/decrease the\nprobability to fixate on selected objects on average by 43/22% but show that\nthe effectiveness of the model depends on the semantic content of the\nmanipulated images.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 16:09:37 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Gatys", "Leon A.", ""], ["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1712.06530", "submitter": "Brian Kenji Iwana", "authors": "Brian Kenji Iwana and Seiichi Uchida", "title": "Dynamic Weight Alignment for Temporal Convolutional Neural Networks", "comments": "Accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method of improving temporal Convolutional Neural\nNetworks (CNN) by determining the optimal alignment of weights and inputs using\ndynamic programming. Conventional CNN convolutions linearly match the shared\nweights to a window of the input. However, it is possible that there exists a\nmore optimal alignment of weights. Thus, we propose the use of Dynamic Time\nWarping (DTW) to dynamically align the weights to the input of the\nconvolutional layer. Specifically, the dynamic alignment overcomes issues such\nas temporal distortion by finding the minimal distance matching of the weights\nand the inputs under constraints. We demonstrate the effectiveness of the\nproposed architecture on the Unipen online handwritten digit and character\ndatasets, the UCI Spoken Arabic Digit dataset, and the UCI Activities of Daily\nLife dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:16:07 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 14:35:19 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 17:23:54 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 12:37:06 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 06:50:57 GMT"}, {"version": "v6", "created": "Thu, 7 Feb 2019 08:10:19 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1712.06566", "submitter": "Zhigang Shen", "authors": "Zhexiong Shang and Zhigang Shen", "title": "Multi-point Vibration Measurement for Mode Identification of Bridge\n  Structures using Video-based Motion Magnification", "comments": "15 pages including 9 figures and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based vibration mode identification gained increased attentions in\ncivil and construction communities. A recent video-based motion magnification\nmethod was developed to measure and visualize small structure motions. This new\napproach presents a potential for low-cost vibration measurement and mode shape\nidentification. Pilot studies using this approach on simple rigid body\nstructures was reported. Its validity on complex outdoor structures have not\nbeen investigated. The objective is to investigate the capacity of video-based\nmotion magnification approach in measuring the modal frequency and visualizing\nthe mode shapes of complex steel bridges. A novel method that increases the\nperformance of the current motion magnification for efficient structure modal\nanalysis is introduced. This method was tested in both indoor and outdoor\nenvironments for validation. The results of the investigation show that motion\nmagnification can be an efficient tool for modal analysis on complex bridge\nstructures. With the developed method, mode frequencies of multiple structures\nare simultaneously measured and mode shapes of each structure are automatically\nvisualized.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 18:21:44 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Shang", "Zhexiong", ""], ["Shen", "Zhigang", ""]]}, {"id": "1712.06584", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik", "title": "End-to-end Recovery of Human Shape and Pose", "comments": "CVPR 2018, Project page with code: https://akanazawa.github.io/hmr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Human Mesh Recovery (HMR), an end-to-end framework for\nreconstructing a full 3D mesh of a human body from a single RGB image. In\ncontrast to most current methods that compute 2D or 3D joint locations, we\nproduce a richer and more useful mesh representation that is parameterized by\nshape and 3D joint angles. The main objective is to minimize the reprojection\nloss of keypoints, which allow our model to be trained using images in-the-wild\nthat only have ground truth 2D annotations. However, the reprojection loss\nalone leaves the model highly under constrained. In this work we address this\nproblem by introducing an adversary trained to tell whether a human body\nparameter is real or not using a large database of 3D human meshes. We show\nthat HMR can be trained with and without using any paired 2D-to-3D supervision.\nWe do not rely on intermediate 2D keypoint detections and infer 3D pose and\nshape parameters directly from image pixels. Our model runs in real-time given\na bounding box containing the person. We demonstrate our approach on various\nimages in-the-wild and out-perform previous optimization based methods that\noutput 3D meshes and show competitive results on tasks such as 3D joint\nlocation estimation and part segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 18:57:40 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 23:12:25 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Black", "Michael J.", ""], ["Jacobs", "David W.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1712.06651", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Objects that Sound", "comments": "Appears in: European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper our objectives are, first, networks that can embed audio and\nvisual inputs into a common space that is suitable for cross-modal retrieval;\nand second, a network that can localize the object that sounds in an image,\ngiven the audio signal. We achieve both these objectives by training from\nunlabelled video using only audio-visual correspondence (AVC) as the objective\nfunction. This is a form of cross-modal self-supervision from video.\n  To this end, we design new network architectures that can be trained for\ncross-modal retrieval and localizing the sound source in an image, by using the\nAVC task. We make the following contributions: (i) show that audio and visual\nembeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and\nbetween-mode retrieval; (ii) explore various architectures for the AVC task,\nincluding those for the visual stream that ingest a single image, or multiple\nimages, or a single image and multi-frame optical flow; (iii) show that the\nsemantic object that sounds within an image can be localized (using only the\nsound, no motion or flow information); and (iv) give a cautionary tale on how\nto avoid undesirable shortcuts in the data preparation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 19:52:53 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 16:26:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1712.06679", "submitter": "Jiang Liu", "authors": "Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann", "title": "DecideNet: Counting Varying Density Crowds Through Attention Guided\n  Detection and Density Estimation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In real-world crowd counting applications, the crowd densities vary greatly\nin spatial and temporal domains. A detection based counting method will\nestimate crowds accurately in low density scenes, while its reliability in\ncongested areas is downgraded. A regression based approach, on the other hand,\ncaptures the general density information in crowded regions. Without knowing\nthe location of each person, it tends to overestimate the count in low density\nareas. Thus, exclusively using either one of them is not sufficient to handle\nall kinds of scenes with varying densities. To address this issue, a novel\nend-to-end crowd counting framework, named DecideNet (DEteCtIon and Density\nEstimation Network) is proposed. It can adaptively decide the appropriate\ncounting mode for different locations on the image based on its real density\nconditions. DecideNet starts with estimating the crowd density by generating\ndetection and regression based density maps separately. To capture inevitable\nvariation in densities, it incorporates an attention module, meant to\nadaptively assess the reliability of the two types of estimations. The final\ncrowd counts are obtained with the guidance of the attention module to adopt\nsuitable estimations from the two kinds of density maps. Experimental results\nshow that our method achieves state-of-the-art performance on three challenging\ncrowd counting datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 21:17:35 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 04:51:35 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Liu", "Jiang", ""], ["Gao", "Chenqiang", ""], ["Meng", "Deyu", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1712.06682", "submitter": "Jason Xie", "authors": "Jason Xie, Tingwen Bao", "title": "Synthesizing Novel Pairs of Image and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating novel pairs of image and text is a problem that combines computer\nvision and natural language processing. In this paper, we present strategies\nfor generating novel image and caption pairs based on existing captioning\ndatasets. The model takes advantage of recent advances in generative\nadversarial networks and sequence-to-sequence modeling. We make generalizations\nto generate paired samples from multiple domains. Furthermore, we study cycles\n-- generating from image to text then back to image and vise versa, as well as\nits connection with autoencoders.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 21:25:37 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Xie", "Jason", ""], ["Bao", "Tingwen", ""]]}, {"id": "1712.06715", "submitter": "Jiajun Shen", "authors": "Jiajun Shen, Yali Amit", "title": "Deformable Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric variations of objects, which do not modify the object class, pose a\nmajor challenge for object recognition. These variations could be rigid as well\nas non-rigid transformations. In this paper, we design a framework for training\ndeformable classifiers, where latent transformation variables are introduced,\nand a transformation of the object image to a reference instantiation is\ncomputed in terms of the classifier output, separately for each class. The\nclassifier outputs for each class, after transformation, are compared to yield\nthe final decision. As a by-product of the classification this yields a\ntransformation of the input object to a reference pose, which can be used for\ndownstream tasks such as the computation of object support. We apply a two-step\ntraining mechanism for our framework, which alternates between optimizing over\nthe latent transformation variables and the classifier parameters to minimize\nthe loss function. We show that multilayer perceptrons, also known as deep\nnetworks, are well suited for this approach and achieve state of the art\nresults on the rotated MNIST and the Google Earth dataset, and produce\ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 23:12:06 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Shen", "Jiajun", ""], ["Amit", "Yali", ""]]}, {"id": "1712.06742", "submitter": "Henry Leopold", "authors": "Henry A Leopold, Jeff Orchard, John S Zelek and Vasudevan\n  Lakshminarayanan", "title": "PixelBNN: Augmenting the PixelCNN with batch normalization and the\n  presentation of a fast architecture for retinal vessel segmentation", "comments": "Manuscript accepted into SPIE Journal of Medical Imaging special\n  section on Radiomics and Deep Learning", "journal-ref": null, "doi": "10.3390/jimaging5020026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of retinal fundus images is essential for eye-care physicians in the\ndiagnosis, care and treatment of patients. Accurate fundus and/or retinal\nvessel maps give rise to longitudinal studies able to utilize multimedia image\nregistration and disease/condition status measurements, as well as applications\nin surgery preparation and biometrics. The segmentation of retinal morphology\nhas numerous applications in assessing ophthalmologic and cardiovascular\ndisease pathologies. The early detection of many such conditions is often the\nmost effective method for reducing patient risk. Computer aided segmentation of\nthe vasculature has proven to be a challenge, mainly due to inconsistencies\nsuch as noise and variations in hue and brightness that can greatly reduce the\nquality of fundus images. This paper presents PixelBNN, a highly efficient deep\nmethod for automating the segmentation of fundus morphologies. The model was\ntrained, tested and cross tested on the DRIVE, STARE and CHASE\\_DB1 retinal\nvessel segmentation datasets. Performance was evaluated using G-mean, Mathews\nCorrelation Coefficient and F1-score. The network was 8.5 times faster than the\ncurrent state-of-the-art at test time and performed comparatively well,\nconsidering a 5 to 19 times reduction in information from resizing images\nduring preprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 01:30:11 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Leopold", "Henry A", ""], ["Orchard", "Jeff", ""], ["Zelek", "John S", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "1712.06760", "submitter": "Chen Feng", "authors": "Yiru Shen, Chen Feng, Yaoqing Yang, Dong Tian", "title": "Mining Point Cloud Local Structures by Kernel Correlation and Graph\n  Pooling", "comments": "Accepted in CVPR'18. *indicates equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike on images, semantic learning on 3D point clouds using a deep network\nis challenging due to the naturally unordered data structure. Among existing\nworks, PointNet has achieved promising results by directly learning on point\nsets. However, it does not take full advantage of a point's local neighborhood\nthat contains fine-grained structural information which turns out to be helpful\ntowards better semantic learning. In this regard, we present two new operations\nto improve PointNet with a more efficient exploitation of local structures. The\nfirst one focuses on local 3D geometric structures. In analogy to a convolution\nkernel for images, we define a point-set kernel as a set of learnable 3D points\nthat jointly respond to a set of neighboring data points according to their\ngeometric affinities measured by kernel correlation, adapted from a similar\ntechnique for point cloud registration. The second one exploits local\nhigh-dimensional feature structures by recursive feature aggregation on a\nnearest-neighbor-graph computed from 3D positions. Experiments show that our\nnetwork can efficiently capture local information and robustly achieve better\nperformances on major datasets. Our code is available at\nhttp://www.merl.com/research/license#KCNet\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 03:06:42 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 21:23:26 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Shen", "Yiru", ""], ["Feng", "Chen", ""], ["Yang", "Yaoqing", ""], ["Tian", "Dong", ""]]}, {"id": "1712.06761", "submitter": "Paul Vicol", "authors": "Paul Vicol, Makarand Tapaswi, Lluis Castrejon, Sanja Fidler", "title": "MovieGraphs: Towards Understanding Human-Centric Situations from Videos", "comments": "Spotlight at CVPR 2018. Webpage: http://moviegraphs.cs.toronto.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in artificial intelligence to build socially\nintelligent robots. This requires machines to have the ability to \"read\"\npeople's emotions, motivations, and other factors that affect behavior. Towards\nthis goal, we introduce a novel dataset called MovieGraphs which provides\ndetailed, graph-based annotations of social situations depicted in movie clips.\nEach graph consists of several types of nodes, to capture who is present in the\nclip, their emotional and physical attributes, their relationships (i.e.,\nparent/child), and the interactions between them. Most interactions are\nassociated with topics that provide additional details, and reasons that give\nmotivations for actions. In addition, most interactions and many attributes are\ngrounded in the video with time stamps. We provide a thorough analysis of our\ndataset, showing interesting common-sense correlations between different social\naspects of scenes, as well as across scenes over time. We propose a method for\nquerying videos and text with graphs, and show that: 1) our graphs contain rich\nand sufficient information to summarize and localize each scene; and 2)\nsubgraphs allow us to describe situations at an abstract level and retrieve\nmultiple semantically relevant situations. We also propose methods for\ninteraction understanding via ordering, and reason understanding. MovieGraphs\nis the first benchmark to focus on inferred properties of human-centric\nsituations, and opens up an exciting avenue towards socially-intelligent AI\nagents.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 03:08:25 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 18:59:49 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Vicol", "Paul", ""], ["Tapaswi", "Makarand", ""], ["Castrejon", "Lluis", ""], ["Fidler", "Sanja", ""]]}, {"id": "1712.06780", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Tian Tsong Ng, Minh N. Do", "title": "Tracking objects using 3D object proposals", "comments": "4 pages, 4 figures, published in APSIPA 2017", "journal-ref": "2017 Asia-Pacific Signal and Information Processing Association\n  Annual Summit and Conference (APSIPA ASC)", "doi": "10.1109/APSIPA.2017.8282298", "report-no": null, "categories": "cs.RO cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object proposals, quickly detected regions in a 3D scene that likely\ncontain an object of interest, are an effective approach to improve the\ncomputational efficiency and accuracy of the object detection framework. In\nthis work, we propose a novel online method that uses our previously developed\n3D object proposals, in a RGB-D video sequence, to match and track static\nobjects in the scene using shape matching. Our main observation is that depth\nimages provide important information about the geometry of the scene that is\noften ignored in object matching techniques. Our method takes less than a\nsecond in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus,\nhas potential to be used in low-power chips in Unmanned Aerial Vehicles (UAVs),\nquadcopters, and drones.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 04:52:34 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Ng", "Tian Tsong", ""], ["Do", "Minh N.", ""]]}, {"id": "1712.06820", "submitter": "Huan-Cheng Hsu", "authors": "Huan-Cheng Hsu, Ching-Hang Chen, Hsiao-Rong Tyan, Hong-Yuan Mark Liao", "title": "Hierarchical Cross Network for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (person re-ID) aims at matching target person(s)\ngrabbed from different and non-overlapping camera views. It plays an important\nrole for public safety and has application in various tasks such as, human\nretrieval, human tracking, and activity analysis. In this paper, we propose a\nnew network architecture called Hierarchical Cross Network (HCN) to perform\nperson re-ID. In addition to the backbone model of a conventional CNN, HCN is\nequipped with two additional maps called hierarchical cross feature maps. The\nmaps of an HCN are formed by merging layers with different resolutions and\nsemantic levels. With the hierarchical cross feature maps, an HCN can\neffectively uncover additional semantic features which could not be discovered\nby a conventional CNN. Although the proposed HCN can discover features with\nhigher semantics, its representation power is still limited. To derive more\ngeneral representations, we augment the data during the training process by\ncombining multiple datasets. Experiment results show that the proposed method\noutperformed several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 08:36:47 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Hsu", "Huan-Cheng", ""], ["Chen", "Ching-Hang", ""], ["Tyan", "Hsiao-Rong", ""], ["Liao", "Hong-Yuan Mark", ""]]}, {"id": "1712.06830", "submitter": "Ruoteng Li", "authors": "Ruoteng Li, Loong-Fah Cheong, and Robby T. Tan", "title": "Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network", "comments": "9 pages, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a single input rainy image, our goal is to visually remove rain streaks\nand the veiling effect caused by scattering and transmission of rain streaks\nand rain droplets. We are particularly concerned with heavy rain, where rain\nstreaks of various sizes and directions can overlap each other and the veiling\neffect reduces contrast severely. To achieve our goal, we introduce a\nscale-aware multi-stage convolutional neural network. Our main idea here is\nthat different sizes of rain-streaks visually degrade the scene in different\nways. Large nearby streaks obstruct larger regions and are likely to reflect\nspecular highlights more prominently than smaller distant streaks. These\ndifferent effects of different streaks have their own characteristics in their\nimage features, and thus need to be treated differently. To realize this, we\ncreate parallel sub-networks that are trained and made aware of these different\nscales of rain streaks. To our knowledge, this idea of parallel sub-networks\nthat treats the same class of objects according to their unique sub-classes is\nnovel, particularly in the context of rain removal. To verify our idea, we\nconducted experiments on both synthetic and real images, and found that our\nmethod is effective and outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 09:12:39 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Li", "Ruoteng", ""], ["Cheong", "Loong-Fah", ""], ["Tan", "Robby T.", ""]]}, {"id": "1712.06837", "submitter": "Timo Hinzmann", "authors": "Timo Hinzmann, Tim Taubner, and Roland Siegwart", "title": "Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for\n  Fixed-wing Aerial Platforms", "comments": "Accepted for publication in IEEE International Conference on Robotics\n  and Automation (ICRA), 2018, Brisbane", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computationally efficient method to estimate the\ntime-varying relative pose between two visual-inertial sensor rigs mounted on\nthe flexible wings of a fixed-wing unmanned aerial vehicle (UAV). The estimated\nrelative poses are used to generate highly accurate depth maps in real-time and\ncan be employed for obstacle avoidance in low-altitude flights or landing\nmaneuvers. The approach is structured as follows: Initially, a wing model is\nidentified by fitting a probability density function to measured deviations\nfrom the nominal relative baseline transformation. At run-time, the prior\nknowledge about the wing model is fused in an Extended Kalman filter~(EKF)\ntogether with relative pose measurements obtained from solving a relative\nperspective N-point problem (PNP), and the linear accelerations and angular\nvelocities measured by the two inertial measurement units (IMU) which are\nrigidly attached to the cameras. Results obtained from extensive synthetic\nexperiments demonstrate that our proposed framework is able to estimate highly\naccurate baseline transformations and depth maps.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 09:34:46 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 09:40:37 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Hinzmann", "Timo", ""], ["Taubner", "Tim", ""], ["Siegwart", "Roland", ""]]}, {"id": "1712.06861", "submitter": "Ignacio Rocco", "authors": "Ignacio Rocco, Relja Arandjelovi\\'c, Josef Sivic", "title": "End-to-end weakly-supervised semantic alignment", "comments": "In 2018 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of semantic alignment where the goal is to compute dense\nsemantic correspondence aligning two images depicting objects of the same\ncategory. This is a challenging task due to large intra-class variation,\nchanges in viewpoint and background clutter. We present the following three\nprincipal contributions. First, we develop a convolutional neural network\narchitecture for semantic alignment that is trainable in an end-to-end manner\nfrom weak image-level supervision in the form of matching image pairs. The\noutcome is that parameters are learnt from rich appearance variation present in\ndifferent but semantically related images without the need for tedious manual\nannotation of correspondences at training time. Second, the main component of\nthis architecture is a differentiable soft inlier scoring module, inspired by\nthe RANSAC inlier scoring procedure, that computes the quality of the alignment\nbased on only geometrically consistent correspondences thereby reducing the\neffect of background clutter. Third, we demonstrate that the proposed approach\nachieves state-of-the-art performance on multiple standard benchmarks for\nsemantic alignment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 10:52:22 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 15:09:04 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Rocco", "Ignacio", ""], ["Arandjelovi\u0107", "Relja", ""], ["Sivic", "Josef", ""]]}, {"id": "1712.06882", "submitter": "Regis Perrier", "authors": "Mathilde Bourjot, Regis Perrier, Jean Fran\\c{c}ois Mainguet", "title": "Comparison of fingerprint authentication algorithms for small imaging\n  sensors", "comments": "On going work which will be improved with more experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for biometric systems has been increasing with the growth of the\nsmartphone market. Biometric devices allow the user to authenticate easily\nwhile securing its private data without the need to remember any access code.\nAmongst them, fingerprint sensors are the most widespread because they seem to\nprovide a good balance between reliability, cost and ease of use. According to\nsmartphone manufacturers, the security level is guaranteed to be high. However,\nthe size of those sensors, which is only a few millimeters squared, prevents\nthe use of minutiae algorithms. To the best of our knowledge, very few studies\nshed light onto this problem, though many pattern recognition algorithms\nalready exist as well as commercial solutions which are supposedly robust. In\nthis article we try to provide insights on how to tackle this problem by\nanalyzing the performance of three algorithms dedicated to pattern recognition.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 11:51:10 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Bourjot", "Mathilde", ""], ["Perrier", "Regis", ""], ["Mainguet", "Jean Fran\u00e7ois", ""]]}, {"id": "1712.06897", "submitter": "Jie Lyu", "authors": "Jie Lyu, Zejian Yuan, Dapeng Chen", "title": "Learning Fixation Point Strategy for Object Detection and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel recurrent attentional structure to localize and recognize\nobjects jointly. The network can learn to extract a sequence of local\nobservations with detailed appearance and rough context, instead of sliding\nwindows or convolutions on the entire image. Meanwhile, those observations are\nfused to complete detection and classification tasks. On training, we present a\nhybrid loss function to learn the parameters of the multi-task network\nend-to-end. Particularly, the combination of stochastic and object-awareness\nstrategy, named SA, can select more abundant context and ensure the last\nfixation close to the object. In addition, we build a real-world dataset to\nverify the capacity of our method in detecting the object of interest including\nthose small ones. Our method can predict a precise bounding box on an image,\nand achieve high speed on large images without pooling operations. Experimental\nresults indicate that the proposed method can mine effective context by several\nlocal observations. Moreover, the precision and speed are easily improved by\nchanging the number of recurrent steps. Finally, we will open the source code\nof our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 12:28:01 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Lyu", "Jie", ""], ["Yuan", "Zejian", ""], ["Chen", "Dapeng", ""]]}, {"id": "1712.06899", "submitter": "Ludmila Kuncheva", "authors": "Ludmila I. Kuncheva, Paria Yousefi, and Iain A. D. Gunn", "title": "On the Evaluation of Video Keyframe Summaries using User Ground Truth", "comments": "12 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the great interest in creating keyframe summaries from video, it is\nsurprising how little has been done to formalise their evaluation and\ncomparison. User studies are often carried out to demonstrate that a proposed\nmethod generates a more appealing summary than one or two rival methods. But\nlarger comparison studies cannot feasibly use such user surveys. Here we\npropose a discrimination capacity measure as a formal way to quantify the\nimprovement over the uniform baseline, assuming that one or more ground truth\nsummaries are available. Using the VSUMM video collection, we examine 10 video\nfeature types, including CNN and SURF, and 6 methods for matching frames from\ntwo summaries. Our results indicate that a simple frame representation through\nhue histograms suffices for the purposes of comparing keyframe summaries. We\nsubsequently propose a formal protocol for comparing summaries when ground\ntruth is available.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 12:37:02 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Kuncheva", "Ludmila I.", ""], ["Yousefi", "Paria", ""], ["Gunn", "Iain A. D.", ""]]}, {"id": "1712.06908", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Partha Pratim Roy, Akash Mohta, Umapada Pal", "title": "Cross-language Framework for Word Recognition and Spotting of Indic\n  Scripts", "comments": "Accepted in Pattern Recognition, Elsevier(2018)", "journal-ref": null, "doi": "10.1016/j.patcog.2018.01.034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten word recognition and spotting of low-resource scripts are\ndifficult as sufficient training data is not available and it is often\nexpensive for collecting data of such scripts. This paper presents a novel\ncross language platform for handwritten word recognition and spotting for such\nlow-resource scripts where training is performed with a sufficiently large\ndataset of an available script (considered as source script) and testing is\ndone on other scripts (considered as target script). Training with one source\nscript and testing with another script to have a reasonable result is not easy\nin handwriting domain due to the complex nature of handwriting variability\namong scripts. Also it is difficult in mapping between source and target\ncharacters when they appear in cursive word images. The proposed Indic cross\nlanguage framework exploits a large resource of dataset for training and uses\nit for recognizing and spotting text of other target scripts where sufficient\namount of training data is not available. Since, Indic scripts are mostly\nwritten in 3 zones, namely, upper, middle and lower, we employ zone-wise\ncharacter (or component) mapping for efficient learning purpose. The\nperformance of our cross-language framework depends on the extent of similarity\nbetween the source and target scripts. Hence, we devise an entropy based script\nsimilarity score using source to target character mapping that will provide a\nfeasibility of cross language transcription. We have tested our approach in\nthree Indic scripts, namely, Bangla, Devanagari and Gurumukhi, and the\ncorresponding results are reported.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:12:29 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 15:10:48 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Roy", "Partha Pratim", ""], ["Mohta", "Akash", ""], ["Pal", "Umapada", ""]]}, {"id": "1712.06909", "submitter": "Asha Anoosheh", "authors": "Asha Anoosheh, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "title": "ComboGAN: Unrestrained Scalability for Image Domain Translation", "comments": "Source code provided here: https://github.com/AAnoosheh/ComboGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This year alone has seen unprecedented leaps in the area of learning-based\nimage translation, namely CycleGAN, by Zhu et al. But experiments so far have\nbeen tailored to merely two domains at a time, and scaling them to more would\nrequire an quadratic number of models to be trained. And with two-domain models\ntaking days to train on current hardware, the number of domains quickly becomes\nlimited by the time and resources required to process them. In this paper, we\npropose a multi-component image translation model and training scheme which\nscales linearly - both in resource consumption and time required - with the\nnumber of domains. We demonstrate its capabilities on a dataset of paintings by\n14 different artists and on images of the four different seasons in the Alps.\nNote that 14 data groups would need (14 choose 2) = 91 different CycleGAN\nmodels: a total of 182 generator/discriminator pairs; whereas our model\nrequires only 14 generator/discriminator pairs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:18:42 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Anoosheh", "Asha", ""], ["Agustsson", "Eirikur", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.06914", "submitter": "Ludmila Kuncheva", "authors": "Iain A. D. Gunn, Ludmila I. Kuncheva, and Paria Yousefi", "title": "Bipartite Graph Matching for Keyframe Summary Evaluation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A keyframe summary, or \"static storyboard\", is a collection of frames from a\nvideo designed to summarise its semantic content. Many algorithms have been\nproposed to extract such summaries automatically. How best to evaluate these\noutputs is an important but little-discussed question. We review the current\nmethods for matching frames between two summaries in the formalism of graph\ntheory. Our analysis revealed different behaviours of these methods, which we\nillustrate with a number of case studies. Based on the results, we recommend a\ngreedy matching algorithm due to Kannappan et al.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:27:26 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Gunn", "Iain A. D.", ""], ["Kuncheva", "Ludmila I.", ""], ["Yousefi", "Paria", ""]]}, {"id": "1712.06930", "submitter": "Yixing Huang", "authors": "Yixing Huang, Oliver Taubmann, Xiaolin Huang, Viktor Haase, Guenter\n  Lauritsch, and Andreas Maier", "title": "Scale-Space Anisotropic Total Variation for Limited Angle Tomography", "comments": "8 pages, 12 figures (48 subfigrues in total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses streak reduction in limited angle tomography. Although\nthe iterative reweighted total variation (wTV) algorithm reduces small streaks\nwell, it is rather inept at eliminating large ones since total variation (TV)\nregularization is scale-dependent and may regard these streaks as homogeneous\nareas. Hence, the main purpose of this paper is to reduce streak artifacts at\nvarious scales. We propose the scale-space anisotropic total variation (ssaTV)\nalgorithm in two different implementations. The first implementation (ssaTV-1)\nutilizes an anisotropic gradient-like operator which uses 2s neighboring pixels\nalong the streaks' normal direction at each scale s. The second implementation\n(ssaTV-2) makes use of anisotropic down-sampling and up-sampling operations,\nsimilarly oriented along the streaks' normal direction, to apply TV\nregularization at various scales. Experiments on numerical and clinical data\ndemonstrate that both ssaTV algorithms reduce streak artifacts more effectively\nand efficiently than wTV, particularly when using multiple scales.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:59:10 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 14:25:50 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Huang", "Yixing", ""], ["Taubmann", "Oliver", ""], ["Huang", "Xiaolin", ""], ["Haase", "Viktor", ""], ["Lauritsch", "Guenter", ""], ["Maier", "Andreas", ""]]}, {"id": "1712.07022", "submitter": "Marzieh Haghighi", "authors": "Marzieh Haghighi, Simon K. Warfield, Sila Kurugol", "title": "Automatic Renal Segmentation in DCE-MRI using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI)\nimages could help in diagnosis and treatment of kidney diseases of children.\nAutomatic segmentation of renal parenchyma is an important step in this\nprocess. In this paper, we propose a time and memory efficient fully automated\nsegmentation method which achieves high segmentation accuracy with running time\nin the order of seconds in both normal kidneys and kidneys with hydronephrosis.\nThe proposed method is based on a cascaded application of two 3D convolutional\nneural networks that employs spatial and temporal information at the same time\nin order to learn the tasks of localization and segmentation of kidneys,\nrespectively. Segmentation performance is evaluated on both normal and abnormal\nkidneys with varying levels of hydronephrosis. We achieved a mean dice\ncoefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric\npatients, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:13:01 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Haghighi", "Marzieh", ""], ["Warfield", "Simon K.", ""], ["Kurugol", "Sila", ""]]}, {"id": "1712.07107", "submitter": "Xiaoyong Yuan", "authors": "Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li", "title": "Adversarial Examples: Attacks and Defenses for Deep Learning", "comments": "Github: https://github.com/chbrian/awesome-adversarial-examples-dl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid progress and significant successes in a wide spectrum of\napplications, deep learning is being applied in many safety-critical\nenvironments. However, deep neural networks have been recently found vulnerable\nto well-designed input samples, called adversarial examples. Adversarial\nexamples are imperceptible to human but can easily fool deep neural networks in\nthe testing/deploying stage. The vulnerability to adversarial examples becomes\none of the major risks for applying deep neural networks in safety-critical\nenvironments. Therefore, attacks and defenses on adversarial examples draw\ngreat attention. In this paper, we review recent findings on adversarial\nexamples for deep neural networks, summarize the methods for generating\nadversarial examples, and propose a taxonomy of these methods. Under the\ntaxonomy, applications for adversarial examples are investigated. We further\nelaborate on countermeasures for adversarial examples and explore the\nchallenges and the potential solutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:44:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 15:51:54 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 02:32:57 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yuan", "Xiaoyong", ""], ["He", "Pan", ""], ["Zhu", "Qile", ""], ["Li", "Xiaolin", ""]]}, {"id": "1712.07113", "submitter": "Andrew Ilyas", "authors": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin", "title": "Query-Efficient Black-box Adversarial Examples (superceded)", "comments": "Superceded by \"Black-Box Adversarial Attacks with Limited Queries and\n  Information.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Note that this paper is superceded by \"Black-Box Adversarial Attacks with\nLimited Queries and Information.\"\n  Current neural network-based image classifiers are susceptible to adversarial\nexamples, even in the black-box setting, where the attacker is limited to query\naccess without access to gradients. Previous methods --- substitute networks\nand coordinate-based finite-difference methods --- are either unreliable or\nquery-inefficient, making these methods impractical for certain problems.\n  We introduce a new method for reliably generating adversarial examples under\nmore restricted, practical black-box threat models. First, we apply natural\nevolution strategies to perform black-box attacks using two to three orders of\nmagnitude fewer queries than previous methods. Second, we introduce a new\nalgorithm to perform targeted adversarial attacks in the partial-information\nsetting, where the attacker only has access to a limited number of target\nclasses. Using these techniques, we successfully perform the first targeted\nadversarial attack against a commercially deployed machine learning system, the\nGoogle Cloud Vision API, in the partial information setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:58:10 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 17:20:27 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Athalye", "Anish", ""], ["Lin", "Jessy", ""]]}, {"id": "1712.07116", "submitter": "Wellington Pinheiro dos Santos", "authors": "Sidney Marlon Lopes de Lima, Abel Guilhermino da Silva Filho,\n  Wellington Pinheiro dos Santos", "title": "Detection and classification of masses in mammographic images in a\n  multi-kernel approach", "comments": null, "journal-ref": "Computer Methods and Programs in Biomedicine, 134 (2016), 11-29", "doi": "10.1016/j.cmpb.2016.04.029", "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, breast cancer is the main cause\nof cancer death among adult women in the world. Although breast cancer occurs\nindiscriminately in countries with several degrees of social and economic\ndevelopment, among developing and underdevelopment countries mortality rates\nare still high, due to low availability of early detection technologies. From\nthe clinical point of view, mammography is still the most effective diagnostic\ntechnology, given the wide diffusion of the use and interpretation of these\nimages. Herein this work we propose a method to detect and classify\nmammographic lesions using the regions of interest of images. Our proposal\nconsists in decomposing each image using multi-resolution wavelets. Zernike\nmoments are extracted from each wavelet component. Using this approach we can\ncombine both texture and shape features, which can be applied both to the\ndetection and classification of mammary lesions. We used 355 images of fatty\nbreast tissue of IRMA database, with 233 normal instances (no lesion), 72\nbenign, and 83 malignant cases. Classification was performed by using SVM and\nELM networks with modified kernels, in order to optimize accuracy rates,\nreaching 94.11%. Considering both accuracy rates and training times, we defined\nthe ration between average percentage accuracy and average training time in a\nreverse order. Our proposal was 50 times higher than the ratio obtained using\nthe best method of the state-of-the-art. As our proposed model can combine high\naccuracy rate with low learning time, whenever a new data is received, our work\nwill be able to save a lot of time, hours, in learning process in relation to\nthe best method of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 03:57:39 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["de Lima", "Sidney Marlon Lopes", ""], ["Filho", "Abel Guilhermino da Silva", ""], ["Santos", "Wellington Pinheiro dos", ""]]}, {"id": "1712.07122", "submitter": "Zhexiong Shang", "authors": "Zhexiong Shang and Zhigang Shen", "title": "Real-time 3D Reconstruction on Construction Site using Visual SLAM and\n  UAV", "comments": "10 pages, 7 figures, conference paper submitted to CRC 2018", "journal-ref": null, "doi": "10.1061/9780784481264.030", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction can be used as a platform to monitor the performance of\nactivities on construction site, such as construction progress monitoring,\nstructure inspection and post-disaster rescue. Comparing to other sensors, RGB\nimage has the advantages of low-cost, texture rich and easy to implement that\nhas been used as the primary method for 3D reconstruction in construction\nindustry. However, the image-based 3D reconstruction always requires extended\ntime to acquire and/or to process the image data, which limits its application\non time critical projects. Recent progress in Visual Simultaneous Localization\nand Mapping (SLAM) make it possible to reconstruct a 3D map of construction\nsite in real-time. Integrated with Unmanned Aerial Vehicle (UAV), the obstacles\nareas that are inaccessible for the ground equipment can also be sensed.\nDespite these advantages of visual SLAM and UAV, until now, such technique has\nnot been fully investigated on construction site. Therefore, the objective of\nthis research is to present a pilot study of using visual SLAM and UAV for\nreal-time construction site reconstruction. The system architecture and the\nexperimental setup are introduced, and the preliminary results and the\npotential applications using Visual SLAM and UAV on construction site are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:46:46 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Shang", "Zhexiong", ""], ["Shen", "Zhigang", ""]]}, {"id": "1712.07136", "submitter": "Hang Qi", "authors": "Hang Qi, Matthew Brown, David G. Lowe", "title": "Low-Shot Learning with Imprinted Weights", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision is able to immediately recognize novel visual categories after\nseeing just one or a few training examples. We describe how to add a similar\ncapability to ConvNet classifiers by directly setting the final layer weights\nfrom novel training examples during low-shot learning. We call this process\nweight imprinting as it directly sets weights for a new category based on an\nappropriately scaled copy of the embedding layer activations for that training\nexample. The imprinting process provides a valuable complement to training with\nstochastic gradient descent, as it provides immediate good classification\nperformance and an initialization for any further fine-tuning in the future. We\nshow how this imprinting process is related to proxy-based embeddings. However,\nit differs in that only a single imprinted weight vector is learned for each\nnovel category, rather than relying on a nearest-neighbor distance to training\ninstances as typically used with embedding methods. Our experiments show that\nusing averaging of imprinted weights provides better generalization than using\nnearest-neighbor instance embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 19:00:08 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 18:00:01 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Qi", "Hang", ""], ["Brown", "Matthew", ""], ["Lowe", "David G.", ""]]}, {"id": "1712.07168", "submitter": "Alex Levinshtein", "authors": "Alex Levinshtein (1), Cheng Chang (1), Edmund Phung (1), Irina Kezele\n  (1), Wenzhangzhi Guo (1), Parham Aarabi (1 and 2) ((1) ModiFace Inc, (2)\n  University of Toronto)", "title": "Real-time deep hair matting on mobile devices", "comments": "7 pages, 7 figures, submitted to CRV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality is an emerging technology in many application domains.\nAmong them is the beauty industry, where live virtual try-on of beauty products\nis of great importance. In this paper, we address the problem of live hair\ncolor augmentation. To achieve this goal, hair needs to be segmented quickly\nand accurately. We show how a modified MobileNet CNN architecture can be used\nto segment the hair in real-time. Instead of training this network using large\namounts of accurate segmentation data, which is difficult to obtain, we use\ncrowd sourced hair segmentation data. While such data is much simpler to\nobtain, the segmentations there are noisy and coarse. Despite this, we show how\nour system can produce accurate and fine-detailed hair mattes, while running at\nover 30 fps on an iPad Pro tablet.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 19:36:08 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 20:48:35 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Levinshtein", "Alex", "", "1 and 2"], ["Chang", "Cheng", "", "1 and 2"], ["Phung", "Edmund", "", "1 and 2"], ["Kezele", "Irina", "", "1 and 2"], ["Guo", "Wenzhangzhi", "", "1 and 2"], ["Aarabi", "Parham", "", "1 and 2"]]}, {"id": "1712.07194", "submitter": "Li Chen", "authors": "Li Chen, Yanjun Xie, Jie Sun, Niranjan Balu, Mahmud Mossa-Basha,\n  Kristi Pimentel, Thomas S. Hatsukami, Jenq-Neng Hwang, Chun Yuan", "title": "Y-net: 3D intracranial artery segmentation using a convolutional\n  autoencoder", "comments": "5 pages, 4 figures, an improved version after accepted by IEEE\n  International Conference on Bioinformatics and Biomedicine, Kansas City, MO,\n  USA, November 13 - 16, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of intracranial arteries on magnetic resonance\nangiography (MRA) allows for quantification of cerebrovascular features, which\nprovides tools for understanding aging and pathophysiological adaptations of\nthe cerebrovascular system. Using a convolutional autoencoder (CAE) for\nsegmentation is promising as it takes advantage of the autoencoder structure in\neffective noise reduction and feature extraction by representing high\ndimensional information with low dimensional latent variables. In this report,\nan optimized CAE model (Y-net) was trained to learn a 3D segmentation model of\nintracranial arteries from 49 cases of MRA data. The trained model was shown to\nperform better than the three traditional segmentation methods in both binary\nclassification and visual evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:38:51 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Chen", "Li", ""], ["Xie", "Yanjun", ""], ["Sun", "Jie", ""], ["Balu", "Niranjan", ""], ["Mossa-Basha", "Mahmud", ""], ["Pimentel", "Kristi", ""], ["Hatsukami", "Thomas S.", ""], ["Hwang", "Jenq-Neng", ""], ["Yuan", "Chun", ""]]}, {"id": "1712.07195", "submitter": "Wei Shen", "authors": "Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, Alan Yuille", "title": "Deep Regression Forests for Age Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation from facial images is typically cast as a nonlinear regression\nproblem. The main challenge of this problem is the facial feature space w.r.t.\nages is heterogeneous, due to the large variation in facial appearance across\ndifferent persons of the same age and the non-stationary property of aging\npatterns. In this paper, we propose Deep Regression Forests (DRFs), an\nend-to-end model, for age estimation. DRFs connect the split nodes to a fully\nconnected layer of a convolutional neural network (CNN) and deal with\nheterogeneous data by jointly learning input-dependant data partitions at the\nsplit nodes and data abstractions at the leaf nodes. This joint learning\nfollows an alternating strategy: First, by fixing the leaf nodes, the split\nnodes as well as the CNN parameters are optimized by Back-propagation; Then, by\nfixing the split nodes, the leaf nodes are optimized by iterating a step-size\nfree and fast-converging update rule derived from Variational Bounding. We\nverify the proposed DRFs on three standard age estimation benchmarks and\nachieve state-of-the-art results on all of them.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:42:15 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Shen", "Wei", ""], ["Guo", "Yilu", ""], ["Wang", "Yan", ""], ["Zhao", "Kai", ""], ["Wang", "Bo", ""], ["Yuille", "Alan", ""]]}, {"id": "1712.07233", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Hyperparameters Optimization in Deep Convolutional Neural Network /\n  Bayesian Approach with Gaussian Process Prior", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Network is known as ConvNet have been extensively used\nin many complex machine learning tasks. However, hyperparameters optimization\nis one of a crucial step in developing ConvNet architectures, since the\naccuracy and performance are reliant on the hyperparameters. This multilayered\narchitecture parameterized by a set of hyperparameters such as the number of\nconvolutional layers, number of fully connected dense layers & neurons, the\nprobability of dropout implementation, learning rate. Hence the searching the\nhyperparameter over the hyperparameter space are highly difficult to build such\ncomplex hierarchical architecture. Many methods have been proposed over the\ndecade to explore the hyperparameter space and find the optimum set of\nhyperparameter values. Reportedly, Gird search and Random search are said to be\ninefficient and extremely expensive, due to a large number of hyperparameters\nof the architecture. Hence, Sequential model-based Bayesian Optimization is a\npromising alternative technique to address the extreme of the unknown cost\nfunction. The recent study on Bayesian Optimization by Snoek in nine\nconvolutional network parameters is achieved the lowerest error report in the\nCIFAR-10 benchmark. This article is intended to provide the overview of the\nmathematical concept behind the Bayesian Optimization over a Gaussian prior.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 21:48:56 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1712.07257", "submitter": "Naiyan Wang", "authors": "Jianfu Zhang, Naiyan Wang, Liqing Zhang", "title": "Multi-shot Pedestrian Re-identification via Sequential Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-shot pedestrian re-identification problem is at the core of\nsurveillance video analysis. It matches two tracks of pedestrians from\ndifferent cameras. In contrary to existing works that aggregate single frames\nfeatures by time series model such as recurrent neural network, in this paper,\nwe propose an interpretable reinforcement learning based approach to this\nproblem. Particularly, we train an agent to verify a pair of images at each\ntime. The agent could choose to output the result (same or different) or\nrequest another pair of images to verify (unsure). By this way, our model\nimplicitly learns the difficulty of image pairs, and postpone the decision when\nthe model does not accumulate enough evidence. Moreover, by adjusting the\nreward for unsure action, we can easily trade off between speed and accuracy.\nIn three open benchmarks, our method are competitive with the state-of-the-art\nmethods while only using 3% to 6% images. These promising results demonstrate\nthat our method is favorable in both efficiency and performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 23:24:04 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 02:53:06 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Zhang", "Jianfu", ""], ["Wang", "Naiyan", ""], ["Zhang", "Liqing", ""]]}, {"id": "1712.07262", "submitter": "Chen Feng", "authors": "Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian", "title": "FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation", "comments": "Accepted as a spotlight paper in CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep networks that directly handle points in a point set, e.g.,\nPointNet, have been state-of-the-art for supervised learning tasks on point\nclouds such as classification and segmentation. In this work, a novel\nend-to-end deep auto-encoder is proposed to address unsupervised learning\nchallenges on point clouds. On the encoder side, a graph-based enhancement is\nenforced to promote local structures on top of PointNet. Then, a novel\nfolding-based decoder deforms a canonical 2D grid onto the underlying 3D object\nsurface of a point cloud, achieving low reconstruction errors even for objects\nwith delicate structures. The proposed decoder only uses about 7% parameters of\na decoder with fully-connected neural networks, yet leads to a more\ndiscriminative representation that achieves higher linear SVM classification\naccuracy than the benchmark. In addition, the proposed decoder structure is\nshown, in theory, to be a generic architecture that is able to reconstruct an\narbitrary point cloud from a 2D grid. Our code is available at\nhttp://www.merl.com/research/license#FoldingNet\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 23:49:25 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 00:46:03 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Yang", "Yaoqing", ""], ["Feng", "Chen", ""], ["Shen", "Yiru", ""], ["Tian", "Dong", ""]]}, {"id": "1712.07271", "submitter": "Andrew Owens", "authors": "Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman,\n  Antonio Torralba", "title": "Learning Sight from Sound: Ambient Sound Provides Supervision for Visual\n  Learning", "comments": "Journal preprint of arXiv:1608.07017 (unpublished submission to IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds. This paper\nextends an earlier conference paper, Owens et al. 2016, with additional\nexperiments and discussion.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 00:10:40 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Owens", "Andrew", ""], ["Wu", "Jiajun", ""], ["McDermott", "Josh H.", ""], ["Freeman", "William T.", ""], ["Torralba", "Antonio", ""]]}, {"id": "1712.07286", "submitter": "Jianing Li", "authors": "Jianing Li, Shiliang Zhang, Jingdong Wang, Wen Gao, Qi Tian", "title": "LVreID: Person Re-Identification with Long Sequence Videos", "comments": "There is experimental error in secction 5.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly establishes a large-scale Long sequence Video database for\nperson re-IDentification (LVreID).\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 01:38:53 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 12:58:53 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 05:44:52 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2018 02:16:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Jianing", ""], ["Zhang", "Shiliang", ""], ["Wang", "Jingdong", ""], ["Gao", "Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1712.07312", "submitter": "Wellington Pinheiro dos Santos", "authors": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel\n  Guilhermino da Silva Filho", "title": "Analysis of supervised and semi-supervised GrowCut applied to\n  segmentation of masses in mammography images", "comments": null, "journal-ref": "Computer Methods in Biomechanics and Biomedical Engineering:\n  Imaging & Visualization, v. 5, p. 1-19, 2017", "doi": "10.1080/21681163.2015.1127775", "report-no": null, "categories": "cs.CV cs.AI cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is already one of the most common form of cancer worldwide.\nMammography image analysis is still the most effective diagnostic method to\npromote the early detection of breast cancer. Accurately segmenting tumors in\ndigital mammography images is important to improve diagnosis capabilities of\nhealth specialists and avoid misdiagnosis. In this work, we evaluate the\nfeasibility of applying GrowCut to segment regions of tumor and we propose two\nGrowCut semi-supervised versions. All the analysis was performed by evaluating\nthe application of segmentation techniques to a set of images obtained from the\nMini-MIAS mammography image database. GrowCut segmentation was compared to\nRegion Growing, Active Contours, Random Walks and Graph Cut techniques.\nExperiments showed that GrowCut, when compared to the other techniques, was\nable to acquire better results for the metrics analyzed. Moreover, the proposed\nsemi-supervised versions of GrowCut was proved to have a clinically\nsatisfactory quality of segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 03:50:15 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Cordeiro", "Filipe Rolim", ""], ["Santos", "Wellington Pinheiro dos", ""], ["Filho", "Abel Guilhermino da Silva", ""]]}, {"id": "1712.07322", "submitter": "Huai Qian Khor Mr.", "authors": "Huai-Qian Khor, John See", "title": "Lost in Time: Temporal Analytics for Long-Term Video Surveillance", "comments": "To Appear in Springer LNEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video surveillance is a well researched area of study with substantial work\ndone in the aspects of object detection, tracking and behavior analysis. With\nthe abundance of video data captured over a long period of time, we can\nunderstand patterns in human behavior and scene dynamics through data-driven\ntemporal analytics. In this work, we propose two schemes to perform descriptive\nand predictive analytics on long-term video surveillance data. We generate\nheatmap and footmap visualizations to describe spatially pooled trajectory\npatterns with respect to time and location. We also present two approaches for\nanomaly prediction at the day-level granularity: a trajectory-based statistical\napproach, and a time-series based approach. Experimentation with one year data\nfrom a single camera demonstrates the ability to uncover interesting insights\nabout the scene and to predict anomalies reasonably well.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:07:48 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Khor", "Huai-Qian", ""], ["See", "John", ""]]}, {"id": "1712.07329", "submitter": "Zichen Yang", "authors": "Zichen Yang, Haifeng Liu and Deng Cai", "title": "On the Diversity of Realistic Image Synthesis", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image processing tasks can be formulated as translating images between\ntwo image domains, such as colorization, super resolution and conditional image\nsynthesis. In most of these tasks, an input image may correspond to multiple\noutputs. However, current existing approaches only show very minor diversity of\nthe outputs. In this paper, we present a novel approach to synthesize diverse\nrealistic images corresponding to a semantic layout. We introduce a diversity\nloss objective, which maximizes the distance between synthesized image pairs\nand links the input noise to the semantic segments in the synthesized images.\nThus, our approach can not only produce diverse images, but also allow users to\nmanipulate the output images by adjusting the noise manually. Experimental\nresults show that images synthesized by our approach are significantly more\ndiverse than that of the current existing works and equipping our diversity\nloss does not degrade the reality of the base networks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:52:52 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Yang", "Zichen", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "1712.07384", "submitter": "Ram Prabhakar Kathirvel", "authors": "K. Ram Prabhakar, V. Sai Srikar, R. Venkatesh Babu", "title": "DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme\n  Exposure Image Pairs", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning architecture for fusing static\nmulti-exposure images. Current multi-exposure fusion (MEF) approaches use\nhand-crafted features to fuse input sequence. However, the weak hand-crafted\nrepresentations are not robust to varying input conditions. Moreover, they\nperform poorly for extreme exposure image pairs. Thus, it is highly desirable\nto have a method that is robust to varying input conditions and capable of\nhandling extreme exposure without artifacts. Deep representations have known to\nbe robust to input conditions and have shown phenomenal performance in a\nsupervised setting. However, the stumbling block in using deep learning for MEF\nwas the lack of sufficient training data and an oracle to provide the\nground-truth for supervision. To address the above issues, we have gathered a\nlarge dataset of multi-exposure image stacks for training and to circumvent the\nneed for ground truth images, we propose an unsupervised deep learning\nframework for MEF utilizing a no-reference quality metric as loss function. The\nproposed approach uses a novel CNN architecture trained to learn the fusion\noperation without reference ground truth image. The model fuses a set of common\nlow level features extracted from each image to generate artifact-free\nperceptually pleasing results. We perform extensive quantitative and\nqualitative evaluation and show that the proposed technique outperforms\nexisting state-of-the-art approaches for a variety of natural images.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 09:47:51 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Prabhakar", "K. Ram", ""], ["Srikar", "V. Sai", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1712.07394", "submitter": "Xianqiang Lv", "authors": "Xianqiang Lv and Hao Zhu and Qing Wang", "title": "Light Field Segmentation From Super-pixel Graph Representation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate segmentation of light field is an important task in\ncomputer vision and graphics. The large volume of input data and the redundancy\nof light field make it an open challenge. In the paper, we propose a novel\ngraph representation for interactive light field segmentation based on light\nfield super-pixel (LFSP). The LFSP not only maintains light field redundancy,\nbut also greatly reduces the graph size. These advantages make LFSP useful to\nimprove segmentation efficiency. Based on LFSP graph structure, we present an\nefficient light field segmentation algorithm using graph-cuts. Experimental\nresults on both synthetic and real dataset demonstrate that our method is\nsuperior to previous light field segmentation algorithms with respect to\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 10:20:12 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lv", "Xianqiang", ""], ["Zhu", "Hao", ""], ["Wang", "Qing", ""]]}, {"id": "1712.07420", "submitter": "Martin Wistuba", "authors": "Martin Wistuba", "title": "Finding Competitive Network Architectures Within a Day Using UCT", "comments": null, "journal-ref": "Proceedings of the 5th IEEE International Conference on Data\n  Science and Advanced Analytics, pages 263-272, 2018", "doi": "10.1109/DSAA.2018.00037", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural network architectures for a new data set is a laborious\ntask which requires human deep learning expertise. In order to make deep\nlearning available for a broader audience, automated methods for finding a\nneural network architecture are vital. Recently proposed methods can already\nachieve human expert level performances. However, these methods have run times\nof months or even years of GPU computing time, ignoring hardware constraints as\nfaced by many researchers and companies. We propose the use of Monte Carlo\nplanning in combination with two different UCT (upper confidence bound applied\nto trees) derivations to search for network architectures. We adapt the UCT\nalgorithm to the needs of network architecture search by proposing two ways of\nsharing information between different branches of the search tree. In an\nempirical study we are able to demonstrate that this method is able to find\ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.\nExtending the search time to five GPU days, we are able to outperform human\narchitectures and our competitors which consider the same types of layers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 11:24:50 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 13:57:50 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wistuba", "Martin", ""]]}, {"id": "1712.07436", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Alex Bewley and Ingmar Posner", "title": "Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments", "comments": "International Conference on Robotics and Automation 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous appearance shifts such as changes in weather and lighting\nconditions can impact the performance of deployed machine learning models.\nWhile unsupervised domain adaptation aims to address this challenge, current\napproaches do not utilise the continuity of the occurring shifts. In\nparticular, many robotics applications exhibit these conditions and thus\nfacilitate the potential to incrementally adapt a learnt model over minor\nshifts which integrate to massive differences over time. Our work presents an\nadversarial approach for lifelong, incremental domain adaptation which benefits\nfrom unsupervised alignment to a series of intermediate domains which\nsuccessively diverge from the labelled source domain. We empirically\ndemonstrate that our incremental approach improves handling of large appearance\nchanges, e.g. day to night, on a traversable-path segmentation task compared\nwith a direct, single alignment step approach. Furthermore, by approximating\nthe feature distribution for the source domain with a generative adversarial\nnetwork, the deployment module can be rendered fully independent of retaining\npotentially large amounts of the related source training data for only a minor\nreduction in performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:08:04 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 16:05:02 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1712.07465", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Zhouxia Wang, Guanbin Li, Liang Lin", "title": "Recurrent Attentional Reinforcement Learning for Multi-label Image\n  Recognition", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing multiple labels of images is a fundamental but challenging task\nin computer vision, and remarkable progress has been attained by localizing\nsemantic-aware image regions and predicting their labels with deep\nconvolutional neural networks. The step of hypothesis regions (region\nproposals) localization in these existing multi-label image recognition\npipelines, however, usually takes redundant computation cost, e.g., generating\nhundreds of meaningless proposals with non-discriminative information and\nextracting their features, and the spatial contextual dependency modeling among\nthe localized regions are often ignored or over-simplified. To resolve these\nissues, this paper proposes a recurrent attention reinforcement learning\nframework to iteratively discover a sequence of attentional and informative\nregions that are related to different semantic objects and further predict\nlabel scores conditioned on these regions. Besides, our method explicitly\nmodels long-term dependencies among these attentional regions that help to\ncapture semantic label co-occurrence and thus facilitate multi-label\nrecognition. Extensive experiments and comparisons on two large-scale\nbenchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior\nperformance over existing state-of-the-art methods in both performance and\nefficiency as well as explicitly identifying image-level semantic labels to\nspecific object regions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 13:14:46 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Chen", "Tianshui", ""], ["Wang", "Zhouxia", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1712.07472", "submitter": "Vladislav Golyanik", "authors": "Vladislav Golyanik, Torben Fetzer, Didier Stricker", "title": "Accurate 3D Reconstruction of Dynamic Scenes from Monocular Image\n  Sequences with Severe Occlusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces an accurate solution to dense orthographic Non-Rigid\nStructure from Motion (NRSfM) in scenarios with severe occlusions or, likewise,\ninaccurate correspondences. We integrate a shape prior term into variational\noptimisation framework. It allows to penalize irregularities of the\ntime-varying structure on the per-pixel level if correspondence quality\nindicator such as an occlusion tensor is available. We make a realistic\nassumption that several non-occluded views of the scene are sufficient to\nestimate an initial shape prior, though the entire observed scene may exhibit\nnon-rigid deformations. Experiments on synthetic and real image data show that\nthe proposed framework significantly outperforms state of the art methods for\ncorrespondence establishment in combination with the state of the art NRSfM\nmethods. Together with the profound insights into optimisation methods,\nimplementation details for heterogeneous platforms are provided.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 13:27:39 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Fetzer", "Torben", ""], ["Stricker", "Didier", ""]]}, {"id": "1712.07487", "submitter": "Sebastian Sudholt", "authors": "Sebastian Sudholt and Gernot Fink", "title": "Attribute CNNs for Word Spotting in Handwritten Documents", "comments": "under review at IJDAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word spotting has become a field of strong research interest in document\nimage analysis over the last years. Recently, AttributeSVMs were proposed which\npredict a binary attribute representation. At their time, this influential\nmethod defined the state-of-the-art in segmentation-based word spotting. In\nthis work, we present an approach for learning attribute representations with\nConvolutional Neural Networks (CNNs). By taking a probabilistic perspective on\ntraining CNNs, we derive two different loss functions for binary and\nreal-valued word string embeddings. In addition, we propose two different CNN\narchitectures, specifically designed for word spotting. These architectures are\nable to be trained in an end-to-end fashion. In a number of experiments, we\ninvestigate the influence of different word string embeddings and optimization\nstrategies. We show our Attribute CNNs to achieve state-of-the-art results for\nsegmentation-based word spotting on a large variety of data sets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:11:27 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Sudholt", "Sebastian", ""], ["Fink", "Gernot", ""]]}, {"id": "1712.07488", "submitter": "Qiao-Kang Liang", "authors": "Yang Nan, Gianmarc Coppola, Qiaokang Liang, Kunglin Zou, Wei Sun, Dan\n  Zhang, Yaonan Wang, Guanzhen Yu", "title": "Partial Labeled Gastric Tumor Segmentation via patch-based Reiterative\n  Learning", "comments": "16 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastric cancer is the second leading cause of cancer-related deaths\nworldwide, and the major hurdle in biomedical image analysis is the\ndetermination of the cancer extent. This assignment has high clinical relevance\nand would generally require vast microscopic assessment by pathologists. Recent\nadvances in deep learning have produced inspiring results on biomedical image\nsegmentation, while its outcome is reliant on comprehensive annotation. This\nrequires plenty of labor costs, for the ground truth must be annotated\nmeticulously by pathologists. In this paper, a reiterative learning framework\nwas presented to train our network on partial annotated biomedical images, and\nsuperior performance was achieved without any pre-trained or further manual\nannotation. We eliminate the boundary error of patch-based model through our\noverlapped region forecast algorithm. Through these advisable methods, a mean\nintersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09%\non the partial labeled dataset was achieved, which made us win the 2017 China\nBig Data & Artificial Intelligence Innovation and Entrepreneurship\nCompetitions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:12:13 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Nan", "Yang", ""], ["Coppola", "Gianmarc", ""], ["Liang", "Qiaokang", ""], ["Zou", "Kunglin", ""], ["Sun", "Wei", ""], ["Zhang", "Dan", ""], ["Wang", "Yaonan", ""], ["Yu", "Guanzhen", ""]]}, {"id": "1712.07493", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Liang Lin, Wangmeng Zuo, Xiaonan Luo, Lei Zhang", "title": "Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating deep neural networks (DNNs) has been attracting increasing\nattention as it can benefit a wide range of applications, e.g., enabling mobile\nsystems with limited computing resources to own powerful visual recognition\nability. A practical strategy to this goal usually relies on a two-stage\nprocess: operating on the trained DNNs (e.g., approximating the convolutional\nfilters with tensor decomposition) and fine-tuning the amended network, leading\nto difficulty in balancing the trade-off between acceleration and maintaining\nrecognition performance. In this work, aiming at a general and comprehensive\nway for neural network acceleration, we develop a Wavelet-like Auto-Encoder\n(WAE) that decomposes the original input image into two low-resolution channels\n(sub-images) and incorporate the WAE into the classification neural networks\nfor joint training. The two decomposed channels, in particular, are encoded to\ncarry the low-frequency information (e.g., image profiles) and high-frequency\n(e.g., image details or noises), respectively, and enable reconstructing the\noriginal input image through the decoding process. Then, we feed the\nlow-frequency channel into a standard classification network such as VGG or\nResNet and employ a very lightweight network to fuse with the high-frequency\nchannel to obtain the classification result. Compared to existing DNN\nacceleration solutions, our framework has the following advantages: i) it is\ntolerant to any existing convolutional neural networks for classification\nwithout amending their structures; ii) the WAE provides an interpretable way to\npreserve the main components of the input image for classification.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:24:00 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Chen", "Tianshui", ""], ["Lin", "Liang", ""], ["Zuo", "Wangmeng", ""], ["Luo", "Xiaonan", ""], ["Zhang", "Lei", ""]]}, {"id": "1712.07540", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Image Registration Techniques: A Survey", "comments": null, "journal-ref": null, "doi": "10.17605/OSF.IO/RV65C", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Registration is the process of aligning two or more images of the same\nscene with reference to a particular image. The images are captured from\nvarious sensors at different times and at multiple view-points. Thus to get a\nbetter picture of any change of a scene or object over a considerable period of\ntime image registration is important. Image registration finds application in\nmedical sciences, remote sensing and in computer vision. This paper presents a\ndetailed review of several approaches which are classified accordingly along\nwith their contributions and drawbacks. The main steps of an image registration\nprocedure are also discussed. Different performance measures are presented that\ndetermine the registration quality and accuracy. The scope for the future\nresearch are presented as well.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:44:28 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1712.07576", "submitter": "Ching-Yao Chuang", "authors": "Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler", "title": "Learning to Act Properly: Predicting and Explaining Affordances from\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of affordance reasoning in diverse scenes that appear\nin the real world. Affordances relate the agent's actions to their effects when\ntaken on the surrounding objects. In our work, we take the egocentric view of\nthe scene, and aim to reason about action-object affordances that respect both\nthe physical world as well as the social norms imposed by the society. We also\naim to teach artificial agents why some actions should not be taken in certain\nsituations, and what would likely happen if these actions would be taken. We\ncollect a new dataset that builds upon ADE20k, referred to as ADE-Affordance,\nwhich contains annotations enabling such rich visual reasoning. We propose a\nmodel that exploits Graph Neural Networks to propagate contextual information\nfrom the scene in order to perform detailed affordance reasoning about each\nobject. Our model is showcased through various ablation studies, pointing to\nsuccesses and challenges in this complex task.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 16:54:09 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 05:26:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Chuang", "Ching-Yao", ""], ["Li", "Jiaman", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1712.07629", "submitter": "Daniel DeTone", "authors": "Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich", "title": "SuperPoint: Self-Supervised Interest Point Detection and Description", "comments": "Camera-ready version for CVPR 2018 Deep Learning for Visual SLAM\n  Workshop (DL4VSLAM2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised framework for training interest point\ndetectors and descriptors suitable for a large number of multiple-view geometry\nproblems in computer vision. As opposed to patch-based neural networks, our\nfully-convolutional model operates on full-sized images and jointly computes\npixel-level interest point locations and associated descriptors in one forward\npass. We introduce Homographic Adaptation, a multi-scale, multi-homography\napproach for boosting interest point detection repeatability and performing\ncross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on\nthe MS-COCO generic image dataset using Homographic Adaptation, is able to\nrepeatedly detect a much richer set of interest points than the initial\npre-adapted deep model and any other traditional corner detector. The final\nsystem gives rise to state-of-the-art homography estimation results on HPatches\nwhen compared to LIFT, SIFT and ORB.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:38:35 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 18:52:54 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 22:07:30 GMT"}, {"version": "v4", "created": "Thu, 19 Apr 2018 15:59:15 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["DeTone", "Daniel", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1712.07632", "submitter": "Yuri G. Gordienko", "authors": "Yu.Gordienko, Peng Gang, Jiang Hui, Wei Zeng, Yu.Kochura, O.Alienin,\n  O. Rokovyi, S. Stirenko", "title": "Deep Learning with Lung Segmentation and Bone Shadow Exclusion\n  Techniques for Chest X-Ray Analysis of Lung Cancer", "comments": "10 pages, 7 figures; The First International Conference on Computer\n  Science, Engineering and Education Applications (ICCSEEA2018)\n  (www.uacnconf.org/iccseea2018) (accepted)", "journal-ref": "In: Hu Z., Petoukhov S., Dychka I., He M. (eds) Advances in\n  Computer Science for Engineering and Education. ICCSEEA 2018. Advances in\n  Intelligent Systems and Computing, vol 754, p. 638-647. Springer, Cham", "doi": "10.1007/978-3-319-91008-6_63", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress of computing, machine learning, and especially deep\nlearning, for image recognition brings a meaningful effect for automatic\ndetection of various diseases from chest X-ray images (CXRs). Here efficiency\nof lung segmentation and bone shadow exclusion techniques is demonstrated for\nanalysis of 2D CXRs by deep learning approach to help radiologists identify\nsuspicious lesions and nodules in lung cancer patients. Training and validation\nwas performed on the original JSRT dataset (dataset #01), BSE-JSRT dataset,\ni.e. the same JSRT dataset, but without clavicle and rib shadows (dataset #02),\noriginal JSRT dataset after segmentation (dataset #03), and BSE-JSRT dataset\nafter segmentation (dataset #04). The results demonstrate the high efficiency\nand usefulness of the considered pre-processing techniques in the simplified\nconfiguration even. The pre-processed dataset without bones (dataset #02)\ndemonstrates the much better accuracy and loss results in comparison to the\nother pre-processed datasets after lung segmentation (datasets #02 and #03).\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:40:49 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gordienko", "Yu.", ""], ["Gang", "Peng", ""], ["Hui", "Jiang", ""], ["Zeng", "Wei", ""], ["Kochura", "Yu.", ""], ["Alienin", "O.", ""], ["Rokovyi", "O.", ""], ["Stirenko", "S.", ""]]}, {"id": "1712.07639", "submitter": "R. Lily Hu", "authors": "R. Lily Hu, Jeremy Karnowski, Ross Fadely, Jean-Patrick Pommier", "title": "Image Segmentation to Distinguish Between Overlapping Human Chromosomes", "comments": "Presented at NIPS 2017 Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medicine, visualizing chromosomes is important for medical diagnostics,\ndrug development, and biomedical research. Unfortunately, chromosomes often\noverlap and it is necessary to identify and distinguish between the overlapping\nchromosomes. A segmentation solution that is fast and automated will enable\nscaling of cost effective medicine and biomedical research. We apply neural\nnetwork-based image segmentation to the problem of distinguishing between\npartially overlapping DNA chromosomes. A convolutional neural network is\ncustomized for this problem. The results achieved intersection over union (IOU)\nscores of 94.7% for the overlapping region and 88-94% on the non-overlapping\nchromosome regions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:48:41 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Hu", "R. Lily", ""], ["Karnowski", "Jeremy", ""], ["Fadely", "Ross", ""], ["Pommier", "Jean-Patrick", ""]]}, {"id": "1712.07642", "submitter": "Fereshteh Sadeghi", "authors": "Fereshteh Sadeghi, Alexander Toshev, Eric Jang, Sergey Levine", "title": "Sim2Real View Invariant Visual Servoing by Recurrent Control", "comments": "Supplementary video:\n  https://fsadeghi.github.io/Sim2RealViewInvariantServo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are remarkably proficient at controlling their limbs and tools from a\nwide range of viewpoints and angles, even in the presence of optical\ndistortions. In robotics, this ability is referred to as visual servoing:\nmoving a tool or end-point to a desired location using primarily visual\nfeedback. In this paper, we study how viewpoint-invariant visual servoing\nskills can be learned automatically in a robotic manipulation scenario. To this\nend, we train a deep recurrent controller that can automatically determine\nwhich actions move the end-point of a robotic arm to a desired object. The\nproblem that must be solved by this controller is fundamentally ambiguous:\nunder severe variation in viewpoint, it may be impossible to determine the\nactions in a single feedforward operation. Instead, our visual servoing system\nmust use its memory of past movements to understand how the actions affect the\nrobot motion from the current viewpoint, correcting mistakes and gradually\nmoving closer to the target. This ability is in stark contrast to most visual\nservoing methods, which either assume known dynamics or require a calibration\nphase. We show how we can learn this recurrent controller using simulated data\nand a reinforcement learning objective. We then describe how the resulting\nmodel can be transferred to a real-world robot by disentangling perception from\ncontrol and only adapting the visual layers. The adapted model can servo to\npreviously unseen objects from novel viewpoints on a real-world Kuka IIWA\nrobotic arm. For supplementary videos, see:\nhttps://fsadeghi.github.io/Sim2RealViewInvariantServo\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:54:29 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Sadeghi", "Fereshteh", ""], ["Toshev", "Alexander", ""], ["Jang", "Eric", ""], ["Levine", "Sergey", ""]]}, {"id": "1712.07682", "submitter": "Giovanni Montana", "authors": "Mauro Annarumma, Giovanni Montana", "title": "Deep metric learning for multi-labelled radiographs", "comments": "SAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many radiological studies can reveal the presence of several co-existing\nabnormalities, each one represented by a distinct visual pattern. In this\narticle we address the problem of learning a distance metric for plain\nradiographs that captures a notion of \"radiological similarity\": two chest\nradiographs are considered to be similar if they share similar abnormalities.\nDeep convolutional neural networks (DCNs) are used to learn a low-dimensional\nembedding for the radiographs that is equipped with the desired metric. Two\nloss functions are proposed to deal with multi-labelled images and potentially\nnoisy labels. We report on a large-scale study involving over 745,000 chest\nradiographs whose labels were automatically extracted from free-text\nradiological reports through a natural language processing system. Using 4,500\nvalidated exams, we demonstrate that the methodology performs satisfactorily on\nclustering and image retrieval tasks. Remarkably, the learned metric separates\nnormal exams from those having radiological abnormalities.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 20:55:08 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Annarumma", "Mauro", ""], ["Montana", "Giovanni", ""]]}, {"id": "1712.07695", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Shunxing Bao, Albert Assad, Richard G.\n  Abramson and Bennett A. Landman", "title": "Adversarial Synthesis Learning Enables Segmentation Without Target\n  Modality Ground Truth", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lack of generalizability is one key limitation of deep learning based\nsegmentation. Typically, one manually labels new training images when\nsegmenting organs in different imaging modalities or segmenting abnormal organs\nfrom distinct disease cohorts. The manual efforts can be alleviated if one is\nable to reuse manual labels from one modality (e.g., MRI) to train a\nsegmentation network for a new modality (e.g., CT). Previously, two stage\nmethods have been proposed to use cycle generative adversarial networks\n(CycleGAN) to synthesize training images for a target modality. Then, these\nefforts trained a segmentation network independently using synthetic images.\nHowever, these two independent stages did not use the complementary information\nbetween synthesis and segmentation. Herein, we proposed a novel end-to-end\nsynthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT\nimage synthesis and CT splenomegaly segmentation simultaneously without using\nmanual labels on CT. The end-to-end EssNet achieved significantly higher median\nDice similarity coefficient (0.9188) than the two stages strategy (0.8801), and\neven higher than canonical multi-atlas segmentation (0.9125) and ResNet method\n(0.9107), which used the CT manual labels.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:22:01 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Bao", "Shunxing", ""], ["Assad", "Albert", ""], ["Abramson", "Richard G.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1712.07721", "submitter": "Oytun Ulutan", "authors": "Oytun Ulutan, Benjamin S. Riggan, Nasser M. Nasrabadi, B.S. Manjunath", "title": "An Order Preserving Bilinear Model for Person Detection in Multi-Modal\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new order preserving bilinear framework that exploits\nlow-resolution video for person detection in a multi-modal setting using deep\nneural networks. In this setting cameras are strategically placed such that\nless robust sensors, e.g. geophones that monitor seismic activity, are located\nwithin the field of views (FOVs) of cameras. The primary challenge is being\nable to leverage sufficient information from videos where there are less than\n40 pixels on targets, while also taking advantage of less discriminative\ninformation from other modalities, e.g. seismic. Unlike state-of-the-art\nmethods, our bilinear framework retains spatio-temporal order when computing\nthe vector outer products between pairs of features. Despite the high\ndimensionality of these outer products, we demonstrate that our order\npreserving bilinear framework yields better performance than recent orderless\nbilinear models and alternative fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 21:44:35 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 21:24:27 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Ulutan", "Oytun", ""], ["Riggan", "Benjamin S.", ""], ["Nasrabadi", "Nasser M.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1712.07732", "submitter": "Ding Liu", "authors": "Ding Liu, Bowen Cheng, Zhangyang Wang, Haichao Zhang, Thomas S. Huang", "title": "Enhance Visual Recognition under Adverse Conditions via Deep Networks", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2908802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition under adverse conditions is a very important and\nchallenging problem of high practical value, due to the ubiquitous existence of\nquality distortions during image acquisition, transmission, or storage. While\ndeep neural networks have been extensively exploited in the techniques of\nlow-quality image restoration and high-quality image recognition tasks\nrespectively, few studies have been done on the important problem of\nrecognition from very low-quality images. This paper proposes a deep learning\nbased framework for improving the performance of image and video recognition\nmodels under adverse conditions, using robust adverse pre-training or its\naggressive variant. The robust adverse pre-training algorithms leverage the\npower of pre-training and generalizes conventional unsupervised pre-training\nand data augmentation methods. We further develop a transfer learning approach\nto cope with real-world datasets of unknown adverse conditions. The proposed\nframework is comprehensively evaluated on a number of image and video\nrecognition benchmarks, and obtains significant performance improvements under\nvarious single or mixed adverse conditions. Our visualization and analysis\nfurther add to the explainability of results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 22:19:12 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 23:46:02 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liu", "Ding", ""], ["Cheng", "Bowen", ""], ["Wang", "Zhangyang", ""], ["Zhang", "Haichao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1712.07758", "submitter": "Mingze Xu", "authors": "Mingze Xu, David J Crandall, Geoffrey C Fox, and John D Paden", "title": "Automatic Estimation of Ice Bottom Surfaces from Radar Imagery", "comments": "5 pages, 3 figures, published in ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-penetrating radar on planes and satellites now makes it practical to\ncollect 3D observations of the subsurface structure of the polar ice sheets,\nproviding crucial data for understanding and tracking global climate change.\nBut converting these noisy readings into useful observations is generally done\nby hand, which is impractical at a continental scale. In this paper, we propose\na computer vision-based technique for extracting 3D ice-bottom surfaces by\nviewing the task as an inference problem on a probabilistic graphical model. We\nfirst generate a seed surface subject to a set of constraints, and then\nincorporate additional sources of evidence to refine it via discrete energy\nminimization. We evaluate the performance of the tracking algorithm on 7\ntopographic sequences (each with over 3000 radar images) collected from the\nCanadian Arctic Archipelago with respect to human-labeled ground truth.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 00:56:47 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Xu", "Mingze", ""], ["Crandall", "David J", ""], ["Fox", "Geoffrey C", ""], ["Paden", "John D", ""]]}, {"id": "1712.07778", "submitter": "Guanbin Li", "authors": "Haofeng Li, Guanbin Li, Liang Lin, Yizhou Yu", "title": "Context-Aware Semantic Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently image inpainting has witnessed rapid progress due to generative\nadversarial networks (GAN) that are able to synthesize realistic contents.\nHowever, most existing GAN-based methods for semantic inpainting apply an\nauto-encoder architecture with a fully connected layer, which cannot accurately\nmaintain spatial information. In addition, the discriminator in existing GANs\nstruggle to understand high-level semantics within the image context and yield\nsemantically consistent content. Existing evaluation criteria are biased\ntowards blurry results and cannot well characterize edge preservation and\nvisual authenticity in the inpainting results. In this paper, we propose an\nimproved generative adversarial network to overcome the aforementioned\nlimitations. Our proposed GAN-based framework consists of a fully convolutional\ndesign for the generator which helps to better preserve spatial structures and\na joint loss function with a revised perceptual loss to capture high-level\nsemantics in the context. Furthermore, we also introduce two novel measures to\nbetter assess the quality of image inpainting results. Experimental results\ndemonstrate that our method outperforms the state of the art under a wide range\nof criteria.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 03:19:00 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Li", "Haofeng", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1712.07798", "submitter": "Lily Peng", "authors": "Avinash V. Varadarajan, Ryan Poplin, Katy Blumer, Christof\n  Angermueller, Joe Ledsam, Reena Chopra, Pearse A. Keane, Greg S. Corrado,\n  Lily Peng, Dale R. Webster", "title": "Deep learning for predicting refractive error from retinal fundus images", "comments": null, "journal-ref": "Investigative Ophthalmology & Visual Science (2018)", "doi": "10.1167/iovs.18-23887", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refractive error, one of the leading cause of visual impairment, can be\ncorrected by simple interventions like prescribing eyeglasses. We trained a\ndeep learning algorithm to predict refractive error from the fundus photographs\nfrom participants in the UK Biobank cohort, which were 45 degree field of view\nimages and the AREDS clinical trial, which contained 30 degree field of view\nimages. Our model use the \"attention\" method to identify features that are\ncorrelated with refractive error. Mean absolute error (MAE) of the algorithm's\nprediction compared to the refractive error obtained in the AREDS and UK\nBiobank. The resulting algorithm had a MAE of 0.56 diopters (95% CI: 0.55-0.56)\nfor estimating spherical equivalent on the UK Biobank dataset and 0.91 diopters\n(95% CI: 0.89-0.92) for the AREDS dataset. The baseline expected MAE (obtained\nby simply predicting the mean of this population) was 1.81 diopters (95% CI:\n1.79-1.84) for UK Biobank and 1.63 (95% CI: 1.60-1.67) for AREDS. Attention\nmaps suggested that the foveal region was one of the most important areas used\nby the algorithm to make this prediction, though other regions also contribute\nto the prediction. The ability to estimate refractive error with high accuracy\nfrom retinal fundus photos has not been previously known and demonstrates that\ndeep learning can be applied to make novel predictions from medical images.\nGiven that several groups have recently shown that it is feasible to obtain\nretinal fundus photos using mobile phones and inexpensive attachments, this\nwork may be particularly relevant in regions of the world where autorefractors\nmay not be readily available.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:27:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Varadarajan", "Avinash V.", ""], ["Poplin", "Ryan", ""], ["Blumer", "Katy", ""], ["Angermueller", "Christof", ""], ["Ledsam", "Joe", ""], ["Chopra", "Reena", ""], ["Keane", "Pearse A.", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""]]}, {"id": "1712.07805", "submitter": "Kang Li", "authors": "Qixue Xiao, Kang Li, Deyue Zhang, Yier Jin", "title": "Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers security risks buried in the data processing pipeline in\ncommon deep learning applications. Deep learning models usually assume a fixed\nscale for their training and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as Caffe,\nTensorFlow, and Torch, all provide data scaling functions to resize input to\nthe dimensions used by deep learning models. Image scaling algorithms are\nintended to preserve the visual features of an image after scaling. However,\ncommon image scaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically different from\nthe corresponding input images.\n  This paper presents a downscaling attack that targets the data scaling\nprocess in deep learning applications. By carefully crafting input data that\nmismatches with the dimension used by deep learning models, attackers can\ncreate deceiving effects. A deep learning application effectively consumes data\nthat are not the same as those presented to users. The visual inconsistency\nenables practical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples to popular\ndeep-learning-based image classification applications. To address the\ndownscaling attacks, the paper also suggests multiple potential mitigation\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 06:17:43 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Xiao", "Qixue", ""], ["Li", "Kang", ""], ["Zhang", "Deyue", ""], ["Jin", "Yier", ""]]}, {"id": "1712.07835", "submitter": "Binqiang Wang", "authors": "Xiaoqiang Lu and Binqiang Wang and Xiangtao Zheng and Xuelong Li", "title": "Exploring Models and Data for Remote Sensing Image Caption Generation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TGRS.2017.2776321", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent development of artificial satellite, remote sensing images\nhave attracted extensive attention. Recently, noticeable progress has been made\nin scene classification and target detection.However, it is still not clear how\nto describe the remote sensing image content with accurate and concise\nsentences. In this paper, we investigate to describe the remote sensing images\nwith accurate and flexible sentences. First, some annotated instructions are\npresented to better describe the remote sensing images considering the special\ncharacteristics of remote sensing images. Second, in order to exhaustively\nexploit the contents of remote sensing images, a large-scale aerial image data\nset is constructed for remote sensing image caption. Finally, a comprehensive\nreview is presented on the proposed data set to fully advance the task of\nremote sensing caption. Extensive experiments on the proposed data set\ndemonstrate that the content of the remote sensing image can be completely\ndescribed by generating language descriptions. The data set is available at\nhttps://github.com/201528014227051/RSICD_optimal\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 08:45:37 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lu", "Xiaoqiang", ""], ["Wang", "Binqiang", ""], ["Zheng", "Xiangtao", ""], ["Li", "Xuelong", ""]]}, {"id": "1712.07881", "submitter": "Francis Tom", "authors": "Francis Tom, Debdoot Sheet", "title": "Simulating Patho-realistic Ultrasound Images using Deep Generative\n  Networks with Adversarial Learning", "comments": "To appear in the Proceedings of the 2018 IEEE International Symposium\n  on Biomedical Imaging (ISBI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging makes use of backscattering of waves during their\ninteraction with scatterers present in biological tissues. Simulation of\nsynthetic ultrasound images is a challenging problem on account of inability to\naccurately model various factors of which some include intra-/inter scanline\ninterference, transducer to surface coupling, artifacts on transducer elements,\ninhomogeneous shadowing and nonlinear attenuation. Current approaches typically\nsolve wave space equations making them computationally expensive and slow to\noperate. We propose a generative adversarial network (GAN) inspired approach\nfor fast simulation of patho-realistic ultrasound images. We apply the\nframework to intravascular ultrasound (IVUS) simulation. A stage 0 simulation\nperformed using pseudo B-mode ultrasound image simulator yields speckle mapping\nof a digitally defined phantom. The stage I GAN subsequently refines them to\npreserve tissue specific speckle intensities. The stage II GAN further refines\nthem to generate high resolution images with patho-realistic speckle profiles.\nWe evaluate patho-realism of simulated images with a visual Turing test\nindicating an equivocal confusion in discriminating simulated from real. We\nalso quantify the shift in tissue specific intensity distributions of the real\nand simulated images to prove their similarity.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 11:31:56 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 19:23:03 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Tom", "Francis", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1712.07920", "submitter": "Aljosa Osep", "authors": "Aljo\\v{s}a O\\v{s}ep, Wolfgang Mehner, Paul Voigtlaender, Bastian Leibe", "title": "Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking", "comments": "ICRA'18 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common paradigm for vision-based multi-object tracking is\ntracking-by-detection, due to the availability of reliable detectors for\nseveral important object categories such as cars and pedestrians. However,\nfuture mobile systems will need a capability to cope with rich human-made\nenvironments, in which obtaining detectors for every possible object category\nwould be infeasible. In this paper, we propose a model-free multi-object\ntracking approach that uses a category-agnostic image segmentation method to\ntrack objects. We present an efficient segmentation mask-based tracker which\nassociates pixel-precise masks reported by the segmentation. Our approach can\nutilize semantic information whenever it is available for classifying objects\nat the track level, while retaining the capability to track generic unknown\nobjects in the absence of such information. We demonstrate experimentally that\nour approach achieves performance comparable to state-of-the-art\ntracking-by-detection methods for popular object categories such as cars and\npedestrians. Additionally, we show that the proposed method can discover and\nrobustly track a large variety of other objects.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 13:05:06 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["O\u0161ep", "Aljo\u0161a", ""], ["Mehner", "Wolfgang", ""], ["Voigtlaender", "Paul", ""], ["Leibe", "Bastian", ""]]}, {"id": "1712.07923", "submitter": "Vincent Christlein", "authors": "Vincent Christlein and Andreas Maier", "title": "Encoding CNN Activations for Writer Recognition", "comments": "(revised) DAS2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoding of local features is an essential part for writer identification\nand writer retrieval. While CNN activations have already been used as local\nfeatures in related works, the encoding of these features has attracted little\nattention so far. In this work, we compare the established VLAD encoding with\ntriangulation embedding. We further investigate generalized max pooling as an\nalternative to sum pooling and the impact of decorrelation and Exemplar SVMs.\nWith these techniques, we set new standards on two publicly available datasets\n(ICDAR13, KHATT).\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 13:07:40 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 00:27:34 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Christlein", "Vincent", ""], ["Maier", "Andreas", ""]]}, {"id": "1712.08002", "submitter": "Fabien Baradel", "authors": "Fabien Baradel, Christian Wolf, Julien Mille", "title": "Human Action Recognition: Pose-based Attention draws focus to Hands", "comments": "ICCV 2017 Workshop \"Hands in action\". arXiv admin note: text overlap\n  with arXiv:1703.10106", "journal-ref": "ICCV 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new spatio-temporal attention based mechanism for human action\nrecognition able to automatically attend to the hands most involved into the\nstudied action and detect the most discriminative moments in an action.\nAttention is handled in a recurrent manner employing Recurrent Neural Network\n(RNN) and is fully-differentiable. In contrast to standard soft-attention based\nmechanisms, our approach does not use the hidden RNN state as input to the\nattention model. Instead, attention distributions are extracted using external\ninformation: human articulated pose. We performed an extensive ablation study\nto show the strengths of this approach and we particularly studied the\nconditioning aspect of the attention mechanism. We evaluate the method on the\nlargest currently available human action recognition dataset, NTU-RGB+D, and\nreport state-of-the-art results. Other advantages of our model are certain\naspects of explanability, as the spatial and temporal attention distributions\nat test time allow to study and verify on which parts of the input data the\nmethod focuses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:58:46 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Baradel", "Fabien", ""], ["Wolf", "Christian", ""], ["Mille", "Julien", ""]]}, {"id": "1712.08036", "submitter": "Dattaraj Rao", "authors": "Dattaraj J Rao, Shruti Mittal, S. Ritika", "title": "Siamese Neural Networks for One-shot detection of Railway Track Switches", "comments": "6 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning methods have been extensively used to analyze video data to\nextract valuable information by classifying image frames and detecting objects.\nWe describe a unique approach for using video feed from a moving Locomotive to\ncontinuously monitor the Railway Track and detect significant assets like\nSwitches on the Track. The technique used here is called Siamese Networks,\nwhich uses 2 identical networks to learn the similarity between of 2 images.\nHere we will use a Siamese network to continuously compare Track images and\ndetect any significant difference in the Track. Switch will be one of those\nimages that will be different and we will find a mapping that clearly\ndistinguishes the Switch from other possible Track anomalies. The same method\nwill then be extended to detect any abnormalities on the Railway Track. Railway\nTransportation is unique in the sense that is has wheeled vehicles, Trains\npulled by Locomotives, running on guided Rails at very high speeds nearing 200\nmph. Multiple Tracks on the Rail network are connected to each other using an\nequipment called Switch or a Turnout. Switch is either operated manually or\nautomatically through command from a Control center and it governs the movement\nof Trains on different Tracks of the network. Accurate location of these\nSwitches is very important for the railroad and getting a true picture of their\nstate in field is important. Modern trains use high definition video cameras\nfacing the Track that continuously record video from track. Using a Siamese\nnetwork and comparing to benchmark images, we describe a method to monitor the\nTrack and highlight anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:02:21 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Rao", "Dattaraj J", ""], ["Mittal", "Shruti", ""], ["Ritika", "S.", ""]]}, {"id": "1712.08062", "submitter": "Earlence T Fernandes", "authors": "Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Dawn Song,\n  Tadayoshi Kohno, Amir Rahmati, Atul Prakash, Florian Tramer", "title": "Note on Attacking Object Detectors with Adversarial Stickers", "comments": "Short Note: The full version of this paper was accepted to USENIX\n  WOOT 2018, and is available at arXiv:1807.07769", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven to be a powerful tool for computer vision and has\nseen widespread adoption for numerous tasks. However, deep learning algorithms\nare known to be vulnerable to adversarial examples. These adversarial inputs\nare created such that, when provided to a deep learning algorithm, they are\nvery likely to be mislabeled. This can be problematic when deep learning is\nused to assist in safety critical decisions. Recent research has shown that\nclassifiers can be attacked by physical adversarial examples under various\nphysical conditions. Given the fact that state-of-the-art objection detection\nalgorithms are harder to be fooled by the same set of adversarial examples,\nhere we show that these detectors can also be attacked by physical adversarial\nexamples. In this note, we briefly show both static and dynamic test results.\nWe design an algorithm that produces physical adversarial inputs, which can\nfool the YOLO object detector and can also attack Faster-RCNN with relatively\nhigh success rate based on transferability. Furthermore, our algorithm can\ncompress the size of the adversarial inputs to stickers that, when attached to\nthe targeted object, result in the detector either mislabeling or not detecting\nthe object a high percentage of the time. This note provides a small set of\nresults. Our upcoming paper will contain a thorough evaluation on other object\ndetectors, and will present the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:33:01 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 19:37:07 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Eykholt", "Kevin", ""], ["Evtimov", "Ivan", ""], ["Fernandes", "Earlence", ""], ["Li", "Bo", ""], ["Song", "Dawn", ""], ["Kohno", "Tadayoshi", ""], ["Rahmati", "Amir", ""], ["Prakash", "Atul", ""], ["Tramer", "Florian", ""]]}, {"id": "1712.08084", "submitter": "Viral Parekh", "authors": "Viral Parekh, Pin Sym Foong, Shendong Zhao and Ramanathan Subramanian", "title": "AVEID: Automatic Video System for Measuring Engagement In Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engagement in dementia is typically measured using behavior observational\nscales (BOS) that are tedious and involve intensive manual labor to annotate,\nand are therefore not easily scalable. We propose AVEID, a low cost and\neasy-to-use video-based engagement measurement tool to determine the engagement\nlevel of a person with dementia (PwD) during digital interaction. We show that\nthe objective behavioral measures computed via AVEID correlate well with\nsubjective expert impressions for the popular MPES and OME BOS, confirming its\nviability and effectiveness. Moreover, AVEID measures can be obtained for a\nvariety of engagement designs, thereby facilitating large-scale studies with\nPwD populations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:57:35 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Parekh", "Viral", ""], ["Foong", "Pin Sym", ""], ["Zhao", "Shendong", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1712.08087", "submitter": "Ksenia Konyushkova", "authors": "Ksenia Konyushkova, Jasper Uijlings, Christoph Lampert and Vittorio\n  Ferrari", "title": "Learning Intelligent Dialogs for Bounding Box Annotation", "comments": "This paper appeared at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Intelligent Annotation Dialogs for bounding box annotation. We\ntrain an agent to automatically choose a sequence of actions for a human\nannotator to produce a bounding box in a minimal amount of time. Specifically,\nwe consider two actions: box verification, where the annotator verifies a box\ngenerated by an object detector, and manual box drawing. We explore two kinds\nof agents, one based on predicting the probability that a box will be\npositively verified, and the other based on reinforcement learning. We\ndemonstrate that (1) our agents are able to learn efficient annotation\nstrategies in several scenarios, automatically adapting to the image\ndifficulty, the desired quality of the boxes, and the detector strength; (2) in\nall scenarios the resulting annotation dialogs speed up annotation compared to\nmanual box drawing alone and box verification alone, while also outperforming\nany fixed combination of verification and drawing in most scenarios; (3) in a\nrealistic scenario where the detector is iteratively re-trained, our agents\nevolve a series of strategies that reflect the shifting trade-off between\nverification and drawing as the detector grows stronger.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:07:01 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 15:18:50 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 14:50:26 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Konyushkova", "Ksenia", ""], ["Uijlings", "Jasper", ""], ["Lampert", "Christoph", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1712.08107", "submitter": "Jordi De La Torre", "authors": "Jordi de la Torre and Aida Valls and Domenec Puig", "title": "A Deep Learning Interpretable Classifier for Diabetic Retinopathy\n  Disease Grading", "comments": "Submitted to Elsevier", "journal-ref": null, "doi": "10.1016/j.neucom.2018.07.102", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have been proven to be very successful in image\nclassification tasks, also for medical diagnosis, but their main concern is its\nlack of interpretability. They use to work as intuition machines with high\nstatistical confidence but unable to give interpretable explanations about the\nreported results. The vast amount of parameters of these models make difficult\nto infer a rationale interpretation from them. In this paper we present a\ndiabetic retinopathy interpretable classifier able to classify retine images\ninto the different levels of disease severity and of explaining its results by\nassigning a score for every point in the hidden and input space, evaluating its\ncontribution to the final classification in a linear way. The generated visual\nmaps can be interpreted by an expert in order to compare its own knowledge with\nthe interpretation given by the model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:40:32 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["de la Torre", "Jordi", ""], ["Valls", "Aida", ""], ["Puig", "Domenec", ""]]}, {"id": "1712.08125", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik", "title": "Unifying Map and Landmark Based Representations for Visual Navigation", "comments": "Project page with videos: https://s-gupta.github.io/cmpl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This works presents a formulation for visual navigation that unifies map\nbased spatial reasoning and path planning, with landmark based robust plan\nexecution in noisy environments. Our proposed formulation is learned from data\nand is thus able to leverage statistical regularities of the world. This allows\nit to efficiently navigate in novel environments given only a sparse set of\nregistered images as input for building representations for space. Our\nformulation is based on three key ideas: a learned path planner that outputs\npath plans to reach the goal, a feature synthesis engine that predicts features\nfor locations along the planned path, and a learned goal-driven closed loop\ncontroller that can follow plans given these synthesized features. We test our\napproach for goal-driven navigation in simulated real world environments and\nreport performance gains over competitive baseline approaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 18:02:14 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Gupta", "Saurabh", ""], ["Fouhey", "David", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1712.08232", "submitter": "Tali Dekel", "authors": "Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, William T. Freeman", "title": "Smart, Sparse Contours to Represent and Edit Images", "comments": "Accepted to CVPR'18; Project page: contour2im.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reconstructing an image from information stored at\ncontour locations. We show that high-quality reconstructions with high fidelity\nto the source image can be obtained from sparse input, e.g., comprising less\nthan $6\\%$ of image pixels. This is a significant improvement over existing\ncontour-based reconstruction methods that require much denser input to capture\nsubtle texture information and to ensure image quality. Our model, based on\ngenerative adversarial networks, synthesizes texture and details in regions\nwhere no input information is provided. The semantic knowledge encoded into our\nmodel and the sparsity of the input allows to use contours as an intuitive\ninterface for semantically-aware image manipulation: local edits in contour\ndomain translate to long-range and coherent changes in pixel space. We can\nperform complex structural changes such as changing facial expression by simple\nedits of contours. Our experiments demonstrate that humans as well as a face\nrecognition system mostly cannot distinguish between our reconstructions and\nthe source images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:11:49 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 18:35:47 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Dekel", "Tali", ""], ["Gan", "Chuang", ""], ["Krishnan", "Dilip", ""], ["Liu", "Ce", ""], ["Freeman", "William T.", ""]]}, {"id": "1712.08263", "submitter": "Arnold Wiliem", "authors": "Siqi Yang, Arnold Wiliem, Shaokang Chen, Brian C. Lovell", "title": "Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks", "comments": "to appear ECCV 2018 (accepted version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows that it is possible to fool/attack recent state-of-the-art\nface detectors which are based on the single-stage networks. Successfully\nattacking face detectors could be a serious malware vulnerability when\ndeploying a smart surveillance system utilizing face detectors. We show that\nexisting adversarial perturbation methods are not effective to perform such an\nattack, especially when there are multiple faces in the input image. This is\nbecause the adversarial perturbation specifically generated for one face may\ndisrupt the adversarial perturbation for another face. In this paper, we call\nthis problem the Instance Perturbation Interference (IPI) problem. This IPI\nproblem is addressed by studying the relationship between the deep neural\nnetwork receptive field and the adversarial perturbation. As such, we propose\nthe Localized Instance Perturbation (LIP) that uses adversarial perturbation\nconstrained to the Effective Receptive Field (ERF) of a target to perform the\nattack. Experiment results show the LIP method massively outperforms existing\nadversarial perturbation generation methods -- often by a factor of 2 to 10.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 00:42:42 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 01:23:11 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Yang", "Siqi", ""], ["Wiliem", "Arnold", ""], ["Chen", "Shaokang", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1712.08268", "submitter": "Xin Chen", "authors": "Heyi Li, Yunke Tian, Klaus Mueller, Xin Chen", "title": "Beyond saliency: understanding convolutional neural networks from\n  saliency prediction on layer-wise relevance propagation", "comments": "35 pages, 15 figures", "journal-ref": "Image and Vision Computing, 2019", "doi": "10.1016/j.imavis.2019.02.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the tremendous achievements of deep convolutional neural networks\n(CNNs) in many computer vision tasks, understanding how they actually work\nremains a significant challenge. In this paper, we propose a novel two-step\nunderstanding method, namely Salient Relevance (SR) map, which aims to shed\nlight on how deep CNNs recognize images and learn features from areas, referred\nto as attention areas, therein. Our proposed method starts out with a\nlayer-wise relevance propagation (LRP) step which estimates a pixel-wise\nrelevance map over the input image. Following, we construct a context-aware\nsaliency map, SR map, from the LRP-generated map which predicts areas close to\nthe foci of attention instead of isolated pixels that LRP reveals. In human\nvisual system, information of regions is more important than of pixels in\nrecognition. Consequently, our proposed approach closely simulates human\nrecognition. Experimental results using the ILSVRC2012 validation dataset in\nconjunction with two well-established deep CNN models, AlexNet and VGG-16,\nclearly demonstrate that our proposed approach concisely identifies not only\nkey pixels but also attention areas that contribute to the underlying neural\nnetwork's comprehension of the given images. As such, our proposed SR map\nconstitutes a convenient visual interface which unveils the visual attention of\nthe network and reveals which type of objects the model has learned to\nrecognize after training. The source code is available at\nhttps://github.com/Hey1Li/Salient-Relevance-Propagation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 01:23:22 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 22:52:26 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 17:18:45 GMT"}, {"version": "v4", "created": "Sat, 15 Sep 2018 01:12:58 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 22:16:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Heyi", ""], ["Tian", "Yunke", ""], ["Mueller", "Klaus", ""], ["Chen", "Xin", ""]]}, {"id": "1712.08273", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Recurrent Pixel Embedding for Instance Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a differentiable, end-to-end trainable framework for solving\npixel-level grouping problems such as instance segmentation consisting of two\nnovel components. First, we regress pixels into a hyper-spherical embedding\nspace so that pixels from the same group have high cosine similarity while\nthose from different groups have similarity below a specified margin. We\nanalyze the choice of embedding dimension and margin, relating them to\ntheoretical results on the problem of distributing points uniformly on the\nsphere. Second, to group instances, we utilize a variant of mean-shift\nclustering, implemented as a recurrent neural network parameterized by kernel\nbandwidth. This recurrent grouping module is differentiable, enjoys convergent\ndynamics and probabilistic interpretability. Backpropagating the group-weighted\nloss through this module allows learning to focus on only correcting embedding\nerrors that won't be resolved during subsequent clustering. Our framework,\nwhile conceptually simple and theoretically abundant, is also practically\neffective and computationally efficient. We demonstrate substantial\nimprovements over state-of-the-art instance segmentation for object proposal\ngeneration, as well as demonstrating the benefits of grouping loss for\nclassification tasks such as boundary detection and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 01:48:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1712.08283", "submitter": "Menyuan Liu", "authors": "Fanyang Meng, Hong Liu, Yongsheng Liang, Wei Liu, Jihong Pei", "title": "A Bidirectional Adaptive Bandwidth Mean Shift Strategy for Clustering", "comments": "Accepted by ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bandwidth of a kernel function is a crucial parameter in the mean shift\nalgorithm. This paper proposes a novel adaptive bandwidth strategy which\ncontains three main contributions. (1) The differences among different adaptive\nbandwidth are analyzed. (2) A new mean shift vector based on bidirectional\nadaptive bandwidth is defined, which combines the advantages of different\nadaptive bandwidth strategies. (3) A bidirectional adaptive bandwidth mean\nshift (BAMS) strategy is proposed to improve the ability to escape from the\nlocal maximum density. Compared with contemporary adaptive bandwidth mean shift\nstrategies, experiments demonstrate the effectiveness of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 02:28:46 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Meng", "Fanyang", ""], ["Liu", "Hong", ""], ["Liang", "Yongsheng", ""], ["Liu", "Wei", ""], ["Pei", "Jihong", ""]]}, {"id": "1712.08290", "submitter": "Gopal Sharma", "authors": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis,\n  Subhransu Maji", "title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry", "comments": "Accepted at CVPR-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural architecture that takes as input a 2D or 3D shape and\noutputs a program that generates the shape. The instructions in our program are\nbased on constructive solid geometry principles, i.e., a set of boolean\noperations on shape primitives defined recursively. Bottom-up techniques for\nthis shape parsing task rely on primitive detection and are inherently slow\nsince the search space over possible primitive combinations is large. In\ncontrast, our model uses a recurrent neural network that parses the input shape\nin a top-down manner, which is significantly faster and yields a compact and\neasy-to-interpret sequence of modeling instructions. Our model is also more\neffective as a shape detector compared to existing state-of-the-art detection\ntechniques. We finally demonstrate that our network can be trained on novel\ndatasets without ground-truth program annotations through policy gradient\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 03:18:57 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 18:03:22 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sharma", "Gopal", ""], ["Goyal", "Rishabh", ""], ["Liu", "Difan", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""]]}, {"id": "1712.08297", "submitter": "Yanning Zhou", "authors": "Yanning Zhou, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng", "title": "SFCN-OPI: Detection and Fine-grained Classification of Nuclei Using\n  Sibling FCN with Objectness Prior Interaction", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell nuclei detection and fine-grained classification have been fundamental\nyet challenging problems in histopathology image analysis. Due to the nuclei\ntiny size, significant inter-/intra-class variances, as well as the inferior\nimage quality, previous automated methods would easily suffer from limited\naccuracy and robustness. In the meanwhile, existing approaches usually deal\nwith these two tasks independently, which would neglect the close relatedness\nof them. In this paper, we present a novel method of sibling fully\nconvolutional network with prior objectness interaction (called SFCN-OPI) to\ntackle the two tasks simultaneously and interactively using a unified\nend-to-end framework. Specifically, the sibling FCN branches share features in\nearlier layers while holding respective higher layers for specific tasks. More\nimportantly, the detection branch outputs the objectness prior which\ndynamically interacts with the fine-grained classification sibling branch\nduring the training and testing processes. With this mechanism, the\nfine-grained classification successfully focuses on regions with high\nconfidence of nuclei existence and outputs the conditional probability, which\nin turn benefits the detection through back propagation. Extensive experiments\non colon cancer histology images have validated the effectiveness of our\nproposed SFCN-OPI and our method has outperformed the state-of-the-art methods\nby a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 03:56:56 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Zhou", "Yanning", ""], ["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1712.08314", "submitter": "Ekaba Bisong", "authors": "Ekaba Bisong", "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients", "comments": "Serious issues with the content and not appropriate for high-level\n  academic distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Artifical Neural Networks are a particular class of learning systems modeled\nafter biological neural functions with an interesting penchant for Hebbian\nlearning, that is \"neurons that wire together, fire together\". However, unlike\ntheir natural counterparts, artificial neural networks have a close and\nstringent coupling between the modules of neurons in the network. This coupling\nor locking imposes upon the network a strict and inflexible structure that\nprevent layers in the network from updating their weights until a full\nfeed-forward and backward pass has occurred. Such a constraint though may have\nsufficed for a while, is now no longer feasible in the era of very-large-scale\nmachine learning, coupled with the increased desire for parallelization of the\nlearning process across multiple computing infrastructures. To solve this\nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are\nintroduced as a viable alternative to the backpropagation algorithm. This paper\nperforms a speed benchmark to compare the speed and accuracy capabilities of\nSG-DNI as opposed to a standard neural interface using multilayer perceptron\nMLP. SG-DNI shows good promise, in that it not only captures the learning\nproblem, it is also over 3-fold faster due to it asynchronous learning\ncapabilities.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 06:28:28 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 23:16:33 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 14:06:52 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Bisong", "Ekaba", ""]]}, {"id": "1712.08315", "submitter": "Lili Zhao", "authors": "Xu Liu, Lili Zhao, Dajun Ding, Yajiao Dong", "title": "Deep Hashing with Category Mask for Fast Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end deep hashing framework with category mask\nfor fast video retrieval. We train our network in a supervised way by fully\nexploiting inter-class diversity and intra-class identity. Classification loss\nis optimized to maximize inter-class diversity, while intra-pair is introduced\nto learn representative intra-class identity. We investigate the binary bits\ndistribution related to categories and find out that the effectiveness of\nbinary bits is highly correlated with data categories, and some bits may\ndegrade classification performance of some categories. We then design hash code\ngeneration scheme with category mask to filter out bits with negative\ncontribution. Experimental results demonstrate the proposed method outperforms\nseveral state-of-the-arts under various evaluation metrics on public datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 06:35:48 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 07:20:26 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Liu", "Xu", ""], ["Zhao", "Lili", ""], ["Ding", "Dajun", ""], ["Dong", "Yajiao", ""]]}, {"id": "1712.08324", "submitter": "Greg Stephens", "authors": "Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev, Greg J\n  Stephens", "title": "Towards dense object tracking in a 2D honeybee hive", "comments": "15 pages, including supplementary figures. 1 supplemental movie\n  available as an ancillary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From human crowds to cells in tissue, the detection and efficient tracking of\nmultiple objects in dense configurations is an important and unsolved problem.\nIn the past, limitations of image analysis have restricted studies of dense\ngroups to tracking a single or subset of marked individuals, or to\ncoarse-grained group-level dynamics, all of which yield incomplete information.\nHere, we combine convolutional neural networks (CNNs) with the model\nenvironment of a honeybee hive to automatically recognize all individuals in a\ndense group from raw image data. We create new, adapted individual labeling and\nuse the segmentation architecture U-Net with a loss function dependent on both\nobject identity and orientation. We additionally exploit temporal regularities\nof the video recording in a recurrent manner and achieve near human-level\nperformance while reducing the network size by 94% compared to the original\nU-Net architecture. Given our novel application of CNNs, we generate extensive\nproblem-specific image data in which labeled examples are produced through a\ncustom interface with Amazon Mechanical Turk. This dataset contains over\n375,000 labeled bee instances across 720 video frames at 2 FPS, representing an\nextensive resource for the development and testing of tracking methods. We\ncorrectly detect 96% of individuals with a location error of ~7% of a typical\nbody dimension, and orientation error of 12 degrees, approximating the\nvariability of human raters. Our results provide an important step towards\nefficient image-based dense object tracking by allowing for the accurate\ndetermination of object location and orientation across time-series image data\nefficiently within one network architecture.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 07:20:57 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Bozek", "Katarzyna", ""], ["Hebert", "Laetitia", ""], ["Mikheyev", "Alexander S", ""], ["Stephens", "Greg J", ""]]}, {"id": "1712.08394", "submitter": "Kunfeng Wang", "authors": "Xuan Li, Kunfeng Wang, Yonglin Tian, Lan Yan, and Fei-Yue Wang", "title": "The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for\n  Traffic Vision Research", "comments": "To be published in IEEE ITSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video image datasets are playing an essential role in design and evaluation\nof traffic vision algorithms. Nevertheless, a longstanding inconvenience\nconcerning image datasets is that manually collecting and annotating\nlarge-scale diversified datasets from real scenes is time-consuming and prone\nto error. For that virtual datasets have begun to function as a proxy of real\ndatasets. In this paper, we propose to construct large-scale artificial scenes\nfor traffic vision research and generate a new virtual dataset called\n\"ParallelEye\". First of all, the street map data is used to build 3D scene\nmodel of Zhongguancun Area, Beijing. Then, the computer graphics, virtual\nreality, and rule modeling technologies are utilized to synthesize large-scale,\nrealistic virtual urban traffic scenes, in which the fidelity and geography\nmatch the real world well. Furthermore, the Unity3D platform is used to render\nthe artificial scenes and generate accurate ground-truth labels, e.g.,\nsemantic/instance segmentation, object bounding box, object tracking, optical\nflow, and depth. The environmental conditions in artificial scenes can be\ncontrolled completely. As a result, we present a viable implementation pipeline\nfor constructing large-scale artificial scenes for traffic vision research. The\nexperimental results demonstrate that this pipeline is able to generate\nphotorealistic virtual datasets with low modeling time and high accuracy\nlabeling.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 11:16:19 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Li", "Xuan", ""], ["Wang", "Kunfeng", ""], ["Tian", "Yonglin", ""], ["Yan", "Lan", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1712.08409", "submitter": "Nils Bore", "authors": "Nils Bore, Johan Ekekrantz, Patric Jensfelt, John Folkesson", "title": "Detection and Tracking of General Movable Objects in Large 3D Maps", "comments": "Submitted for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of detection and tracking of general objects\nwith long-term dynamics, observed by a mobile robot moving in a large\nenvironment. A key problem is that due to the environment scale, it can only\nobserve a subset of the objects at any given time. Since some time passes\nbetween observations of objects in different places, the objects might be moved\nwhen the robot is not there. We propose a model for this movement in which the\nobjects typically only move locally, but with some small probability they jump\nlonger distances, through what we call global motion. For filtering, we\ndecompose the posterior over local and global movements into two linked\nprocesses. The posterior over the global movements and measurement associations\nis sampled, while we track the local movement analytically using Kalman\nfilters. This novel filter is evaluated on point cloud data gathered\nautonomously by a mobile robot over an extended period of time. We show that\ntracking jumping objects is feasible, and that the proposed probabilistic\ntreatment outperforms previous methods when applied to real world data. The key\nto efficient probabilistic tracking in this scenario is focused sampling of the\nobject posteriors.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 11:53:52 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 09:31:47 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Bore", "Nils", ""], ["Ekekrantz", "Johan", ""], ["Jensfelt", "Patric", ""], ["Folkesson", "John", ""]]}, {"id": "1712.08416", "submitter": "Laura Sevilla-Lara", "authors": "Laura Sevilla-Lara, Yiyi Liao, Fatma Guney, Varun Jampani, Andreas\n  Geiger, Michael J. Black", "title": "On the Integration of Optical Flow and Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the top performing action recognition methods use optical flow as a\n\"black box\" input. Here we take a deeper look at the combination of flow and\naction recognition, and investigate why optical flow is helpful, what makes a\nflow method good for action recognition, and how we can make it better. In\nparticular, we investigate the impact of different flow algorithms and input\ntransformations to better understand how these affect a state-of-the-art action\nrecognition method. Furthermore, we fine tune two neural-network flow methods\nend-to-end on the most widely used action recognition dataset (UCF101). Based\non these experiments, we make the following five observations: 1) optical flow\nis useful for action recognition because it is invariant to appearance, 2)\noptical flow methods are optimized to minimize end-point-error (EPE), but the\nEPE of current methods is not well correlated with action recognition\nperformance, 3) for the flow methods tested, accuracy at boundaries and at\nsmall displacements is most correlated with action recognition performance, 4)\ntraining optical flow to minimize classification error instead of minimizing\nEPE improves recognition performance, and 5) optical flow learned for the task\nof action recognition differs from traditional optical flow especially inside\nthe human body and at the boundary of the body. These observations may\nencourage optical flow researchers to look beyond EPE as a goal and guide\naction recognition researchers to seek better motion cues, leading to a tighter\nintegration of the optical flow and action recognition communities.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 12:16:29 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Sevilla-Lara", "Laura", ""], ["Liao", "Yiyi", ""], ["Guney", "Fatma", ""], ["Jampani", "Varun", ""], ["Geiger", "Andreas", ""], ["Black", "Michael J.", ""]]}, {"id": "1712.08425", "submitter": "Erik Dam", "authors": "Erik B Dam", "title": "Simple Methods for Scanner Drift Normalization Validated for Automatic\n  Segmentation of Knee Magnetic Resonance Imaging - with data from the\n  Osteoarthritis Initiative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scanner drift is a well-known magnetic resonance imaging (MRI) artifact\ncharacterized by gradual signal degradation and scan intensity changes over\ntime. In addition, hardware and software updates may imply abrupt changes in\nsignal. The combined effects are particularly challenging for automatic image\nanalysis methods used in longitudinal studies. The implication is increased\nmeasurement variation and a risk of bias in the estimations (e.g. in the volume\nchange for a structure). We proposed two quite different approaches for scanner\ndrift normalization and demonstrated the performance for segmentation of knee\nMRI using the fully automatic KneeIQ framework. The validation included a total\nof 1975 scans from both high-field and low-field MRI. The results demonstrated\nthat the pre-processing method denoted Atlas Affine Normalization significantly\nremoved scanner drift effects and ensured that the cartilage volume change\nquantifications became consistent with manual expert scores.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 12:58:22 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Dam", "Erik B", ""]]}, {"id": "1712.08470", "submitter": "Kunfeng Wang", "authors": "Yonglin Tian, Xuan Li, Kunfeng Wang, and Fei-Yue Wang", "title": "Training and Testing Object Detectors with Virtual Images", "comments": "To be published in IEEE/CAA Journal of Automatica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of computer vision, deep learning has produced a variety of\nstate-of-the-art models that rely on massive labeled data. However, collecting\nand annotating images from the real world has a great demand for labor and\nmoney investments and is usually too passive to build datasets with specific\ncharacteristics, such as small area of objects and high occlusion level. Under\nthe framework of Parallel Vision, this paper presents a purposeful way to\ndesign artificial scenes and automatically generate virtual images with precise\nannotations. A virtual dataset named ParallelEye is built, which can be used\nfor several computer vision tasks. Then, by training the DPM (Deformable Parts\nModel) and Faster R-CNN detectors, we prove that the performance of models can\nbe significantly improved by combining ParallelEye with publicly available\nreal-world datasets during the training phase. In addition, we investigate the\npotential of testing the trained models from a specific aspect using\nintentionally designed virtual datasets, in order to discover the flaws of\ntrained models. From the experimental results, we conclude that our virtual\ndataset is viable to train and test the object detectors.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 14:45:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Tian", "Yonglin", ""], ["Li", "Xuan", ""], ["Wang", "Kunfeng", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1712.08521", "submitter": "Luiza Mici", "authors": "Luiza Mici and German I. Parisi and Stefan Wermter", "title": "An Incremental Self-Organizing Architecture for Sensorimotor Learning\n  and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  During visuomotor tasks, robots must compensate for temporal delays inherent\nin their sensorimotor processing systems. Delay compensation becomes crucial in\na dynamic environment where the visual input is constantly changing, e.g.,\nduring the interacting with a human demonstrator. For this purpose, the robot\nmust be equipped with a prediction mechanism for using the acquired perceptual\nexperience to estimate possible future motor commands. In this paper, we\npresent a novel neural network architecture that learns prototypical visuomotor\nrepresentations and provides reliable predictions on the basis of the visual\ninput. These predictions are used to compensate for the delayed motor behavior\nin an online manner. We investigate the performance of our method with a set of\nexperiments comprising a humanoid robot that has to learn and generate visually\nperceived arm motion trajectories. We evaluate the accuracy in terms of mean\nprediction error and analyze the response of the network to novel movement\ndemonstrations. Additionally, we report experiments with incomplete data\nsequences, showing the robustness of the proposed architecture in the case of a\nnoisy and faulty visual sensor.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 15:34:19 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 15:21:24 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Mici", "Luiza", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1712.08583", "submitter": "Umang Natubhai Yadav", "authors": "Umang Yadav, Sherif N Abbas, Dimitrios Hatzinakos", "title": "Evaluation of PPG Biometrics for Authentication in different states", "comments": "Accepted at 11th IAPR/IEEE International Conference on Biometrics,\n  2018. 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amongst all medical biometric traits, Photoplethysmograph (PPG) is the\neasiest to acquire. PPG records the blood volume change with just combination\nof Light Emitting Diode and Photodiode from any part of the body. With IoT and\nsmart homes' penetration, PPG recording can easily be integrated with other\nvital wearable devices. PPG represents peculiarity of hemodynamics and\ncardiovascular system for each individual. This paper presents non-fiducial\nmethod for PPG based biometric authentication. Being a physiological signal,\nPPG signal alters with physical/mental stress and time. For robustness, these\nvariations cannot be ignored. While, most of the previous works focused only on\nsingle session, this paper demonstrates extensive performance evaluation of PPG\nbiometrics against single session data, different emotions, physical exercise\nand time-lapse using Continuous Wavelet Transform (CWT) and Direct Linear\nDiscriminant Analysis (DLDA). When evaluated on different states and datasets,\nequal error rate (EER) of $0.5\\%$-$6\\%$ was achieved for $45$-$60$s average\ntraining time. Our CWT/DLDA based technique outperformed all other\ndimensionality reduction techniques and previous work.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 17:29:31 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Yadav", "Umang", ""], ["Abbas", "Sherif N", ""], ["Hatzinakos", "Dimitrios", ""]]}, {"id": "1712.08585", "submitter": "Birgit Komander", "authors": "Birgit Komander and Dirk A. Lorenz and Lena Vestweber", "title": "Denoising of image gradients and total generalized variation denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit total variation denoising and study an augmented model where we\nassume that an estimate of the image gradient is available. We show that this\nincreases the image reconstruction quality and derive that the resulting model\nresembles the total generalized variation denoising method, thus providing a\nnew motivation for this model. Further, we propose to use a constraint\ndenoising model and develop a variational denoising model that is basically\nparameter free, i.e. all model parameters are estimated directly from the noisy\nimage.\n  Moreover, we use Chambolle-Pock's primal dual method as well as the\nDouglas-Rachford method for the new models. For the latter one has to solve\nlarge discretizations of partial differential equations. We propose to do this\nin an inexact manner using the preconditioned conjugate gradients method and\nderive preconditioners for this. Numerical experiments show that the resulting\nmethod has good denoising properties and also that preconditioning does\nincrease convergence speed significantly. Finally we analyze the duality gap of\ndifferent formulations of the TGV denoising problem and derive a simple\nstopping criterion.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 17:33:16 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 08:57:36 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 09:30:12 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Komander", "Birgit", ""], ["Lorenz", "Dirk A.", ""], ["Vestweber", "Lena", ""]]}, {"id": "1712.08604", "submitter": "Aneeq Zia", "authors": "Aneeq Zia, Irfan Essa", "title": "Automated Surgical Skill Assessment in RMIS Training", "comments": "Accepted at IPCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Manual feedback in basic RMIS training can consume a significant\namount of time from expert surgeons' schedule and is prone to subjectivity.\nWhile VR-based training tasks can generate automated score reports, there is no\nmechanism of generating automated feedback for surgeons performing basic\nsurgical tasks in RMIS training. In this paper, we explore the usage of\ndifferent holistic features for automated skill assessment using only robot\nkinematic data and propose a weighted feature fusion technique for improving\nscore prediction performance.\n  Methods: We perform our experiments on the publicly available JIGSAWS dataset\nand evaluate four different types of holistic features from robot kinematic\ndata - Sequential Motion Texture (SMT), Discrete Fourier Transform (DFT),\nDiscrete Cosine Transform (DCT) and Approximate Entropy (ApEn). The features\nare then used for skill classification and exact skill score prediction. Along\nwith using these features individually, we also evaluate the performance using\nour proposed weighted combination technique.\n  Results: Our results demonstrate that these holistic features outperform all\nprevious HMM based state-of-the-art methods for skill classification on the\nJIGSAWS dataset. Also, our proposed feature fusion strategy significantly\nimproves performance for skill score predictions achieving up to 0.61 average\nspearman correlation coefficient.\n  Conclusions: Holistic features capturing global information from robot\nkinematic data can successfully be used for evaluating surgeon skill in basic\nsurgical tasks on the da Vinci robot. Using the framework presented can\npotentially allow for real time score feedback in RMIS training.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 18:25:00 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Zia", "Aneeq", ""], ["Essa", "Irfan", ""]]}, {"id": "1712.08675", "submitter": "Xianzhi Du", "authors": "Xianzhi Du, Xiaolong Wang, Dawei Li, Jingwen Zhu, Serafettin Tasci,\n  Cameron Upright, Stephen Walsh, Larry Davis", "title": "Boundary-sensitive Network for Portrait Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the general semantic segmentation problem, portrait segmentation\nhas higher precision requirement on boundary area. However, this problem has\nnot been well studied in previous works. In this paper, we propose a\nboundary-sensitive deep neural network (BSN) for portrait segmentation. BSN\nintroduces three novel techniques. First, an individual boundary-sensitive\nkernel is proposed by dilating the contour line and assigning the boundary\npixels with multi-class labels. Second, a global boundary-sensitive kernel is\nemployed as a position sensitive prior to further constrain the overall shape\nof the segmentation map. Third, we train a boundary-sensitive attribute\nclassifier jointly with the segmentation network to reinforce the network with\nsemantic boundary shape information. We have evaluated BSN on the current\nlargest public portrait segmentation dataset, i.e, the PFCN dataset, as well as\nthe portrait images collected from other three popular image segmentation\ndatasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superior\nquantitative and qualitative performance over state-of-the-arts on all the\ndatasets, especially on the boundary area.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 22:32:38 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 18:26:29 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Du", "Xianzhi", ""], ["Wang", "Xiaolong", ""], ["Li", "Dawei", ""], ["Zhu", "Jingwen", ""], ["Tasci", "Serafettin", ""], ["Upright", "Cameron", ""], ["Walsh", "Stephen", ""], ["Davis", "Larry", ""]]}, {"id": "1712.08690", "submitter": "Aneesh Rangnekar", "authors": "Aneesh Rangnekar, Nilay Mokashi, Emmett Ientilucci, Christopher Kanan\n  and Matthew Hoffman", "title": "Aerial Spectral Super-Resolution using Conditional Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring spectral signatures from ground based natural images has acquired a\nlot of interest in applied deep learning. In contrast to the spectra of ground\nbased images, aerial spectral images have low spatial resolution and suffer\nfrom higher noise interference. In this paper, we train a conditional\nadversarial network to learn an inverse mapping from a trichromatic space to 31\nspectral bands within 400 to 700 nm. The network is trained on AeroCampus, a\nfirst of its kind aerial hyperspectral dataset. AeroCampus consists of high\nspatial resolution color images and low spatial resolution hyperspectral images\n(HSI). Color images synthesized from 31 spectral bands are used to train our\nnetwork. With a baseline root mean square error of 2.48 on the synthesized RGB\ntest data, we show that it is possible to generate spectral signatures in\naerial imagery.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 00:21:20 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rangnekar", "Aneesh", ""], ["Mokashi", "Nilay", ""], ["Ientilucci", "Emmett", ""], ["Kanan", "Christopher", ""], ["Hoffman", "Matthew", ""]]}, {"id": "1712.08697", "submitter": "Alexander Trott", "authors": "Alexander Trott, Caiming Xiong, Richard Socher", "title": "Interpretable Counting for Visual Question Answering", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions that require counting a variety of objects in images remain a major\nchallenge in visual question answering (VQA). The most common approaches to VQA\ninvolve either classifying answers based on fixed length representations of\nboth the image and question or summing fractional counts estimated from each\nsection of the image. In contrast, we treat counting as a sequential decision\nprocess and force our model to make discrete choices of what to count.\nSpecifically, the model sequentially selects from detected objects and learns\ninteractions between objects that influence subsequent selections. A\ndistinction of our approach is its intuitive and interpretable output, as\ndiscrete counts are automatically grounded in the image. Furthermore, our\nmethod outperforms the state of the art architecture for VQA on multiple\nmetrics that evaluate counting.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 01:44:45 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 03:00:43 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Trott", "Alexander", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1712.08714", "submitter": "Anurag Ghosh", "authors": "Anurag Ghosh, Suriya Singh, C.V. Jawahar", "title": "Towards Structured Analysis of Broadcast Badminton Videos", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports video data is recorded for nearly every major tournament but remains\narchived and inaccessible to large scale data mining and analytics. It can only\nbe viewed sequentially or manually tagged with higher-level labels which is\ntime consuming and prone to errors. In this work, we propose an end-to-end\nframework for automatic attributes tagging and analysis of sport videos. We use\ncommonly available broadcast videos of matches and, unlike previous approaches,\ndoes not rely on special camera setups or additional sensors.\n  Our focus is on Badminton as the sport of interest. We propose a method to\nanalyze a large corpus of badminton broadcast videos by segmenting the points\nplayed, tracking and recognizing the players in each point and annotating their\nrespective badminton strokes. We evaluate the performance on 10 Olympic matches\nwith 20 players and achieved 95.44% point segmentation accuracy, 97.38% player\ndetection score (mAP@0.5), 97.98% player identification accuracy, and stroke\nsegmentation edit scores of 80.48%. We further show that the automatically\nannotated videos alone could enable the gameplay analysis and inference by\ncomputing understandable metrics such as player's reaction time, speed, and\nfootwork around the court, etc.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 04:51:31 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ghosh", "Anurag", ""], ["Singh", "Suriya", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1712.08726", "submitter": "Tao Tan", "authors": "Dongsheng Jiang, Weiqiang Dou, Luc Vosters, Xiayu Xu, Yue Sun, Tao Tan", "title": "Denoising of 3D magnetic resonance images with multi-channel residual\n  learning of convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The denoising of magnetic resonance (MR) images is a task of great importance\nfor improving the acquired image quality. Many methods have been proposed in\nthe literature to retrieve noise free images with good performances. Howerever,\nthe state-of-the-art denoising methods, all needs a time-consuming optimization\nprocesses and their performance strongly depend on the estimated noise level\nparameter. Within this manuscript we propose the idea of denoising MRI Rician\nnoise using a convolutional neural network. The advantage of the proposed\nmethodology is that the learning based model can be directly used in the\ndenosing process without optimization and even without the noise level\nparameter. Specifically, a ten convolutional layers neural network combined\nwith residual learning and multi-channel strategy was proposed. Two training\nways: training on a specific noise level and training on a general level were\nconducted to demonstrate the capability of our methods. Experimental results\nover synthetic and real 3D MR data demonstrate our proposed network can achieve\nsuperior performance compared with other methods in term of both of the peak\nsignal to noise ratio and the global of structure similarity index. Without\nnoise level parameter, our general noise-applicable model is also better than\nthe other compared methods in two datasets. Furthermore, our training model\nshows good general applicability.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 07:35:51 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 14:04:52 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Jiang", "Dongsheng", ""], ["Dou", "Weiqiang", ""], ["Vosters", "Luc", ""], ["Xu", "Xiayu", ""], ["Sun", "Yue", ""], ["Tan", "Tao", ""]]}, {"id": "1712.08730", "submitter": "Parneet Kaur", "authors": "Parneet Kaur, Karan Sikka, Ajay Divakaran", "title": "Combining Weakly and Webly Supervised Learning for Classifying Food\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food classification from images is a fine-grained classification problem.\nManual curation of food images is cost, time and scalability prohibitive. On\nthe other hand, web data is available freely but contains noise. In this paper,\nwe address the problem of classifying food images with minimal data curation.\nWe also tackle a key problems with food images from the web where they often\nhave multiple cooccuring food types but are weakly labeled with a single label.\nWe first demonstrate that by sequentially adding a few manually curated samples\nto a larger uncurated dataset from two web sources, the top-1 classification\naccuracy increases from 50.3% to 72.8%. To tackle the issue of weak labels, we\naugment the deep model with Weakly Supervised learning (WSL) that results in an\nincrease in performance to 76.2%. Finally, we show some qualitative results to\nprovide insights into the performance improvements using the proposed ideas.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 08:00:06 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kaur", "Parneet", ""], ["Sikka", "Karan", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1712.08745", "submitter": "Kunfeng Wang", "authors": "Wenwen Zhang, Kunfeng Wang, Hua Qu, Jihong Zhao, and Fei-Yue Wang", "title": "Scene-Specific Pedestrian Detection Based on Parallel Vision", "comments": "To be published in IEEE ITSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a special type of object detection, pedestrian detection in generic scenes\nhas made a significant progress trained with large amounts of labeled training\ndata manually. While the models trained with generic dataset work bad when they\nare directly used in specific scenes. With special viewpoints, flow light and\nbackgrounds, datasets from specific scenes are much different from the datasets\nfrom generic scenes. In order to make the generic scene pedestrian detectors\nwork well in specific scenes, the labeled data from specific scenes are needed\nto adapt the models to the specific scenes. While labeling the data manually\nspends much time and money, especially for specific scenes, each time with a\nnew specific scene, large amounts of images must be labeled. What's more, the\nlabeling information is not so accurate in the pixels manually and different\npeople make different labeling information. In this paper, we propose an\nACP-based method, with augmented reality's help, we build the virtual world of\nspecific scenes, and make people walking in the virtual scenes where it is\npossible for them to appear to solve this problem of lacking labeled data and\nthe results show that data from virtual world is helpful to adapt generic\npedestrian detectors to specific scenes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 09:33:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Wenwen", ""], ["Wang", "Kunfeng", ""], ["Qu", "Hua", ""], ["Zhao", "Jihong", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1712.08776", "submitter": "Jianwei Zhang", "authors": "Jianwei Zhang, Xu Chen, Xuezhong Xiao", "title": "Texture Object Segmentation Based on Affine Invariant Texture Detection", "comments": "6pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the issue of segmenting rich texture images, a novel detection\nmethods based on the affine invariable principle is proposed. Considering the\nsimilarity between the texture areas, we first take the affine transform to get\nnumerous shapes, and utilize the KLT algorithm to verify the similarity. The\ntransforms include rotation, proportional transformation and perspective\ndeformation to cope with a variety of situations. Then we propose an improved\nLBP method combining canny edge detection to handle the boundary in the\nsegmentation process. Moreover, human-computer interaction of this method which\nhelps splitting the matched texture area from the original images is\nuser-friendly.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 13:56:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Jianwei", ""], ["Chen", "Xu", ""], ["Xiao", "Xuezhong", ""]]}, {"id": "1712.08832", "submitter": "Aljosa Osep", "authors": "Aljo\\v{s}a O\\v{s}ep and Paul Voigtlaender and Jonathon Luiten and\n  Stefan Breuers and Bastian Leibe", "title": "Large-Scale Object Discovery and Detector Adaptation from Unlabeled\n  Video", "comments": "CVPR'18 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore object discovery and detector adaptation based on unlabeled video\nsequences captured from a mobile platform. We propose a fully automatic\napproach for object mining from video which builds upon a generic object\ntracking approach. By applying this method to three large video datasets from\nautonomous driving and mobile robotics scenarios, we demonstrate its robustness\nand generality. Based on the object mining results, we propose a novel approach\nfor unsupervised object discovery by appearance-based clustering. We show that\nthis approach successfully discovers interesting objects relevant to driving\nscenarios. In addition, we perform self-supervised detector adaptation in order\nto improve detection performance on the KITTI dataset for existing categories.\nOur approach has direct relevance for enabling large-scale object learning for\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:22:11 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["O\u0161ep", "Aljo\u0161a", ""], ["Voigtlaender", "Paul", ""], ["Luiten", "Jonathon", ""], ["Breuers", "Stefan", ""], ["Leibe", "Bastian", ""]]}, {"id": "1712.08838", "submitter": "Ahmed Taha", "authors": "Rohan Chandra, Sachin Grover, Kyungjun Lee, Moustafa Meshry, and Ahmed\n  Taha", "title": "Texture Synthesis with Recurrent Variational Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent variational auto-encoder for texture synthesis. A\nnovel loss function, FLTBNK, is used for training the texture synthesizer. It\nis rotational and partially color invariant loss function. Unlike L2 loss,\nFLTBNK explicitly models the correlation of color intensity between pixels. Our\ntexture synthesizer generates neighboring tiles to expand a sample texture and\nis evaluated using various texture patterns from Describable Textures Dataset\n(DTD). We perform both quantitative and qualitative experiments with various\nloss functions to evaluate the performance of our proposed loss function\n(FLTBNK) --- a mini-human subject study is used for the qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:38:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chandra", "Rohan", ""], ["Grover", "Sachin", ""], ["Lee", "Kyungjun", ""], ["Meshry", "Moustafa", ""], ["Taha", "Ahmed", ""]]}, {"id": "1712.08868", "submitter": "Kanji Tanaka", "authors": "Yamaguchi Kousuke, Tanaka Kanji, Sugimoto Takuma", "title": "Use of Generative Adversarial Network for Cross-Domain Change Detection", "comments": "5 pages, 4 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of cross-domain change detection from a\nnovel perspective of image-to-image translation. In general, change detection\naims to identify interesting changes between a given query image and a\nreference image of the same scene taken at a different time. This problem\nbecomes a challenging one when query and reference images involve different\ndomains (e.g., time of the day, weather, and season) due to variations in\nobject appearance and a limited amount of training examples. In this study, we\naddress the above issue by leveraging a generative adversarial network (GAN).\nOur key concept is to use a limited amount of training data to train a\nGAN-based image translator that maps a reference image to a virtual image that\ncannot be discriminated from query domain images. This enables us to treat the\ncross-domain change detection task as an in-domain image comparison. This\nallows us to leverage the large body of literature on in-domain generic change\ndetectors. In addition, we also consider the use of visual place recognition as\na method for mining more appropriate reference images over the space of virtual\nimages. Experiments validate efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 02:23:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kousuke", "Yamaguchi", ""], ["Kanji", "Tanaka", ""], ["Takuma", "Sugimoto", ""]]}, {"id": "1712.08877", "submitter": "Yuanchao Bai", "authors": "Yuanchao Bai, Gene Cheung, Xianming Liu, Wen Gao", "title": "Blind Image Deblurring via Reweighted Graph Total Variation", "comments": "5 pages, submitted to IEEE International Conference on Acoustics,\n  Speech and Signal Processing, Calgary, Alberta, Canada, April, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring, i.e., deblurring without knowledge of the blur\nkernel, is a highly ill-posed problem. The problem can be solved in two parts:\ni) estimate a blur kernel from the blurry image, and ii) given estimated blur\nkernel, de-convolve blurry input to restore the target image. In this paper, by\ninterpreting an image patch as a signal on a weighted graph, we first argue\nthat a skeleton image---a proxy that retains the strong gradients of the target\nbut smooths out the details---can be used to accurately estimate the blur\nkernel and has a unique bi-modal edge weight distribution. We then design a\nreweighted graph total variation (RGTV) prior that can efficiently promote\nbi-modal edge weight distribution given a blurry patch. However, minimizing a\nblind image deblurring objective with RGTV results in a non-convex\nnon-differentiable optimization problem. We propose a fast algorithm that\nsolves for the skeleton image and the blur kernel alternately. Finally with the\ncomputed blur kernel, recent non-blind image deblurring algorithms can be\napplied to restore the target image. Experimental results show that our\nalgorithm can robustly estimate the blur kernel with large kernel size, and the\nreconstructed sharp image is competitive against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 05:01:47 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Bai", "Yuanchao", ""], ["Cheung", "Gene", ""], ["Liu", "Xianming", ""], ["Gao", "Wen", ""]]}, {"id": "1712.08900", "submitter": "Thiago Santini", "authors": "Thiago Santini, Wolfgang Fuhl, Enkelejda Kasneci", "title": "PuRe: Robust pupil detection for real-time pervasive eye tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2018.02.002", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, accurate, and robust pupil detection is an essential prerequisite\nto enable pervasive eye-tracking and its applications -- e.g., gaze-based human\ncomputer interaction, health monitoring, foveated rendering, and advanced\ndriver assistance. However, automated pupil detection has proved to be an\nintricate task in real-world scenarios due to a large mixture of challenges\nsuch as quickly changing illumination and occlusions. In this paper, we\nintroduce the Pupil Reconstructor PuRe, a method for pupil detection in\npervasive scenarios based on a novel edge segment selection and conditional\nsegment combination schemes; the method also includes a confidence measure for\nthe detected pupil. The proposed method was evaluated on over 316,000 images\nacquired with four distinct head-mounted eye tracking devices. Results show a\npupil detection rate improvement of over 10 percentage points w.r.t.\nstate-of-the-art algorithms in the two most challenging data sets (6.46 for all\ndata sets), further pushing the envelope for pupil detection. Moreover, we\nadvance the evaluation protocol of pupil detection algorithms by also\nconsidering eye images in which pupils are not present. In this aspect, PuRe\nimproved precision and specificity w.r.t. state-of-the-art algorithms by 25.05\nand 10.94 percentage points, respectively, demonstrating the meaningfulness of\nPuRe's confidence measure. PuRe operates in real-time for modern eye trackers\n(at 120 fps).\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 10:09:10 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Santini", "Thiago", ""], ["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1712.09004", "submitter": "Hang Yan", "authors": "Hang Yan, Qi Shan, Yasutaka Furukawa", "title": "RIDI: Robust IMU Double Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel data-driven approach for inertial navigation,\nwhich learns to estimate trajectories of natural human motions just from an\ninertial measurement unit (IMU) in every smartphone. The key observation is\nthat human motions are repetitive and consist of a few major modes (e.g.,\nstanding, walking, or turning). Our algorithm regresses a velocity vector from\nthe history of linear accelerations and angular velocities, then corrects\nlow-frequency bias in the linear accelerations, which are integrated twice to\nestimate positions. We have acquired training data with ground-truth motions\nacross multiple human subjects and multiple phone placements (e.g., in a bag or\na hand). The qualitatively and quantitatively evaluations have demonstrated\nthat our algorithm has surprisingly shown comparable results to full Visual\nInertial navigation. To our knowledge, this paper is the first to integrate\nsophisticated machine learning techniques with inertial navigation, potentially\nopening up a new line of research in the domain of data-driven inertial\nnavigation. We will publicly share our code and data to facilitate further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 04:43:14 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 04:11:43 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Yan", "Hang", ""], ["Shan", "Qi", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1712.09025", "submitter": "Hoang Tran Vu", "authors": "Hoang Tran Vu and Ching-Chun Huang", "title": "Domain Adaptation Meets Disentangled Representation Learning and Style\n  Transfer", "comments": "22 pages, 7 figures, ACCV2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been proposed to solve the domain adaptation problem\nrecently. However, the success of them implicitly funds on the assumption that\nthe information of domains are fully transferrable. If the assumption is not\nsatisfied, the effect of negative transfer may degrade domain adaptation. In\nthis paper, a better learning network has been proposed by considering three\ntasks - domain adaptation, disentangled representation, and style transfer\nsimultaneously. Firstly, the learned features are disentangled into common\nparts and specific parts. The common parts represent the transferrable\nfeatures, which are suitable for domain adaptation with less negative transfer.\nConversely, the specific parts characterize the unique style of each individual\ndomain. Based on this, the new concept of feature exchange across domains,\nwhich can not only enhance the transferability of common features but also be\nuseful for image style transfer, is introduced. These designs allow us to\nintroduce five types of training objectives to realize the three challenging\ntasks at the same time. The experimental results show that our architecture can\nbe adaptive well to full transfer learning and partial transfer learning upon a\nwell-learned disentangled representation. Besides, the trained network also\ndemonstrates high potential to generate style-transferred images.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 07:27:08 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 13:32:55 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 08:43:01 GMT"}, {"version": "v4", "created": "Sat, 7 Jul 2018 12:21:53 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Vu", "Hoang Tran", ""], ["Huang", "Ching-Chun", ""]]}, {"id": "1712.09048", "submitter": "Guanjun Guo", "authors": "Guanjun Guo, Hanzi Wang, Chunhua Shen, Yan Yan, Hong-Yuan Mark Liao", "title": "Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep\n  Neural Networks and Cascaded Regression", "comments": "13 pages, 13 figures, To appear in IEEE Transactions on Multimedia,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress, computational visual aesthetic is still challenging.\nImage cropping, which refers to the removal of unwanted scene areas, is an\nimportant step to improve the aesthetic quality of an image. However, it is\nchallenging to evaluate whether cropping leads to aesthetically pleasing\nresults because the assessment is typically subjective. In this paper, we\npropose a novel cascaded cropping regression (CCR) method to perform image\ncropping by learning the knowledge from professional photographers. The\nproposed CCR method improves the convergence speed of the cascaded method,\nwhich directly uses random-ferns regressors. In addition, a two-step learning\nstrategy is proposed and used in the CCR method to address the problem of\nlacking labelled cropping data. Specifically, a deep convolutional neural\nnetwork (CNN) classifier is first trained on large-scale visual aesthetic\ndatasets. The deep CNN model is then designed to extract features from several\nimage cropping datasets, upon which the cropping bounding boxes are predicted\nby the proposed CCR method. Experimental results on public image cropping\ndatasets demonstrate that the proposed method significantly outperforms several\nstate-of-the-art image cropping methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 09:49:39 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 07:02:02 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Guo", "Guanjun", ""], ["Wang", "Hanzi", ""], ["Shen", "Chunhua", ""], ["Yan", "Yan", ""], ["Liao", "Hong-Yuan Mark", ""]]}, {"id": "1712.09078", "submitter": "Jinshan Pan", "authors": "Yang Liu, Jinshan Pan, Zhixun Su", "title": "Deep Blind Image Inpainting", "comments": "conference, 9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting is a challenging problem as it needs to fill the information\nof the corrupted regions. Most of the existing inpainting algorithms assume\nthat the positions of the corrupted regions are known. Different from the\nexisting methods that usually make some assumptions on the corrupted regions,\nwe present an efficient blind image inpainting algorithm to directly restore a\nclear image from a corrupted input. Our algorithm is motivated by the residual\nlearning algorithm which aims to learn the missing infor- mation in corrupted\nregions. However, directly using exist- ing residual learning algorithms in\nimage restoration does not well solve this problem as little information is\navailable in the corrupted regions. To solve this problem, we introduce an\nencoder and decoder architecture to capture more useful information and develop\na robust loss function to deal with outliers. Our algorithm can predict the\nmissing information in the corrupted regions, thus facilitating the clear image\nrestoration. Both qualitative and quantitative experimental demonstrate that\nour algorithm can deal with the corrupted regions of arbitrary shapes and\nperforms favorably against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 14:45:37 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Liu", "Yang", ""], ["Pan", "Jinshan", ""], ["Su", "Zhixun", ""]]}, {"id": "1712.09093", "submitter": "JIachi Zhang", "authors": "Jiachi Zhang, Xiaolei Shen, Tianqi Zhuo, Hong Zhou", "title": "Brain Tumor Segmentation Based on Refined Fully Convolutional Neural\n  Networks with A Hierarchical Dice Loss", "comments": "14 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a basic task in computer vision, semantic segmentation can provide\nfundamental information for object detection and instance segmentation to help\nthe artificial intelligence better understand real world. Since the proposal of\nfully convolutional neural network (FCNN), it has been widely used in semantic\nsegmentation because of its high accuracy of pixel-wise classification as well\nas high precision of localization. In this paper, we apply several famous FCNN\nto brain tumor segmentation, making comparisons and adjusting network\narchitectures to achieve better performance measured by metrics such as\nprecision, recall, mean of intersection of union (mIoU) and dice score\ncoefficient (DSC). The adjustments to the classic FCNN include adding more\nconnections between convolutional layers, enlarging decoders after up sample\nlayers and changing the way shallower layers' information is reused. Besides\nthe structure modification, we also propose a new classifier with a\nhierarchical dice loss. Inspired by the containing relationship between\nclasses, the loss function converts multiple classification to multiple binary\nclassification in order to counteract the negative effect caused by imbalance\ndata set. Massive experiments have been done on the training set and testing\nset in order to assess our refined fully convolutional neural networks and new\ntypes of loss function. Competitive figures prove they are more effective than\ntheir predecessors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 16:06:34 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 07:57:50 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 04:00:30 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhang", "Jiachi", ""], ["Shen", "Xiaolei", ""], ["Zhuo", "Tianqi", ""], ["Zhou", "Hong", ""]]}, {"id": "1712.09153", "submitter": "Janghoon Choi", "authors": "Janghoon Choi, Junseok Kwon, Kyoung Mu Lee", "title": "Deep Meta Learning for Real-Time Target-Aware Visual Tracking", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel on-line visual tracking framework based on\nthe Siamese matching network and meta-learner network, which run at real-time\nspeeds. Conventional deep convolutional feature-based discriminative visual\ntracking algorithms require continuous re-training of classifiers or\ncorrelation filters, which involve solving complex optimization tasks to adapt\nto the new appearance of a target object. To alleviate this complex process,\nour proposed algorithm incorporates and utilizes a meta-learner network to\nprovide the matching network with new appearance information of the target\nobjects by adding target-aware feature space. The parameters for the\ntarget-specific feature space are provided instantly from a single forward-pass\nof the meta-learner network. By eliminating the necessity of continuously\nsolving complex optimization tasks in the course of tracking, experimental\nresults demonstrate that our algorithm performs at a real-time speed while\nmaintaining competitive performance among other state-of-the-art tracking\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 01:14:16 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 00:52:10 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 06:49:29 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Choi", "Janghoon", ""], ["Kwon", "Junseok", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1712.09161", "submitter": "Aisha Urooj", "authors": "Cecilia La Place, Aisha Urooj Khan and Ali Borji", "title": "Segmenting Sky Pixels in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor scene parsing models are often trained on ideal datasets and produce\nquality results. However, this leads to a discrepancy when applied to the real\nworld. The quality of scene parsing, particularly sky classification, decreases\nin night time images, images involving varying weather conditions, and scene\nchanges due to seasonal weather. This project focuses on approaching these\nchallenges by using a state-of-the-art model in conjunction with a non-ideal\ndataset: SkyFinder and a subset from SUN database with Sky object. We focus\nspecifically on sky segmentation, the task of determining sky and not-sky\npixels, and improving upon an existing state-of-the-art model: RefineNet. As a\nresult of our efforts, we have seen an improvement of 10-15% in the average MCR\ncompared to the prior methods on SkyFinder dataset. We have also improved from\nan off-the shelf-model in terms of average mIOU by nearly 35%. Further, we\nanalyze our trained models on images w.r.t two aspects: times of day and\nweather, and find that, in spite of facing same challenges as prior methods,\nour trained models significantly outperform them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 02:07:46 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 19:19:55 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["La Place", "Cecilia", ""], ["Khan", "Aisha Urooj", ""], ["Borji", "Ali", ""]]}, {"id": "1712.09184", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri and\n  Du Tran", "title": "Detect-and-Track: Efficient Pose Estimation in Videos", "comments": "In CVPR 2018. Ranked first in ICCV 2017 PoseTrack challenge (keypoint\n  tracking in videos). Code: https://github.com/facebookresearch/DetectAndTrack\n  and webpage: https://rohitgirdhar.github.io/DetectAndTrack/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating and tracking human body\nkeypoints in complex, multi-person video. We propose an extremely lightweight\nyet highly effective approach that builds upon the latest advancements in human\ndetection and video understanding. Our method operates in two-stages: keypoint\nestimation in frames or short clips, followed by lightweight tracking to\ngenerate keypoint predictions linked over the entire video. For frame-level\npose estimation we experiment with Mask R-CNN, as well as our own proposed 3D\nextension of this model, which leverages temporal information over small clips\nto generate more robust frame predictions. We conduct extensive ablative\nexperiments on the newly released multi-person video pose estimation benchmark,\nPoseTrack, to validate various design choices of our model. Our approach\nachieves an accuracy of 55.2% on the validation and 51.8% on the test set using\nthe Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art\nperformance on the ICCV 2017 PoseTrack keypoint tracking challenge.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 05:56:39 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 18:49:57 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Girdhar", "Rohit", ""], ["Gkioxari", "Georgia", ""], ["Torresani", "Lorenzo", ""], ["Paluri", "Manohar", ""], ["Tran", "Du", ""]]}, {"id": "1712.09196", "submitter": "Ajil Jalal", "authors": "Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, Alexandros G.\n  Dimakis", "title": "The Robust Manifold Defense: Adversarial Training using Generative\n  Models", "comments": "Added pseudo code for defense-gan break", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new type of attack for finding adversarial examples for image\nclassifiers. Our method exploits spanners, i.e. deep neural networks whose\ninput space is low-dimensional and whose output range approximates the set of\nimages of interest. Spanners may be generators of GANs or decoders of VAEs. The\nkey idea in our attack is to search over latent code pairs to find ones that\ngenerate nearby images with different classifier outputs. We argue that our\nattack is stronger than searching over perturbations of real images. Moreover,\nwe show that our stronger attack can be used to reduce the accuracy of\nDefense-GAN to 3\\%, resolving an open problem from the well-known paper by\nAthalye et al. We combine our attack with normal adversarial training to obtain\nthe most robust known MNIST classifier, significantly improving the state of\nthe art against PGD attacks. Our formulation involves solving a min-max\nproblem, where the min player sets the parameters of the classifier and the max\nplayer is running our attack, and is thus searching for adversarial examples in\nthe {\\em low-dimensional} input space of the spanner.\n  All code and models are available at\n\\url{https://github.com/ajiljalal/manifold-defense.git}\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 07:28:14 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 14:42:03 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 13:23:51 GMT"}, {"version": "v4", "created": "Thu, 4 Jul 2019 15:26:38 GMT"}, {"version": "v5", "created": "Wed, 10 Jul 2019 03:51:45 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Jalal", "Ajil", ""], ["Ilyas", "Andrew", ""], ["Daskalakis", "Constantinos", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1712.09213", "submitter": "Milad Abdollahzadeh", "authors": "Touba Malekzadeh and Milad Abdollahzadeh and Hossein Nejati and\n  Ngai-Man Cheung", "title": "Aircraft Fuselage Defect Detection using Deep Neural Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure flight safety of aircraft structures, it is necessary to have\nregular maintenance using visual and nondestructive inspection (NDI) methods.\nIn this paper, we propose an automatic image-based aircraft defect detection\nusing Deep Neural Networks (DNNs). To the best of our knowledge, this is the\nfirst work for aircraft defect detection using DNNs. We perform a comprehensive\nevaluation of state-of-the-art feature descriptors and show that the best\nperformance is achieved by vgg-f DNN as feature extractor with a linear SVM\nclassifier. To reduce the processing time, we propose to apply SURF key point\ndetector to identify defect patch candidates. Our experiment results suggest\nthat we can achieve over 96% accuracy at around 15s processing time for a\nhigh-resolution (20-megapixel) image on a laptop.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 09:07:34 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 08:30:30 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Malekzadeh", "Touba", ""], ["Abdollahzadeh", "Milad", ""], ["Nejati", "Hossein", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1712.09216", "submitter": "Dror Aiger", "authors": "Dror Aiger, Brett Allen, Aleksey Golovinskiy", "title": "Large-Scale 3D Scene Classification With Multi-View Volumetric CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to classify imagery using a convo- lutional neural\nnetwork (CNN) on multi-view image pro- jections. The power of our method comes\nfrom using pro- jections of multiple images at multiple depth planes near the\nreconstructed surface. This enables classification of categories whose salient\naspect is appearance change un- der different viewpoints, such as water, trees,\nand other materials with complex reflection/light response proper- ties. Our\nmethod does not require boundary labelling in images and works on pixel-level\nclassification with a small (few pixels) context, which simplifies the cre-\nation of a training set. We demonstrate this application on large-scale aerial\nimagery collections, and extend the per-pixel classification to robustly create\na consistent 2D classification which can be used to fill the gaps in non-\nreconstructible water regions. We also apply our method to classify tree\nregions. In both cases, the training data can quickly be generated using a\nsmall number of manually- created polygons on a map. We show that even with a\nvery simple and standard network our CNN outperforms the state-of-the-art image\nclassification, the Inception-V3 model retrained from a large collection of\naerial images.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 09:13:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Aiger", "Dror", ""], ["Allen", "Brett", ""], ["Golovinskiy", "Aleksey", ""]]}, {"id": "1712.09299", "submitter": "Guy Ben-Yosef", "authors": "Guy Ben-Yosef, Alon Yachin, Shimon Ullman", "title": "A model for interpreting social interactions in local image regions", "comments": "In AAAI spring symposium on Science of Intelligence: Computational\n  Principles of Natural and Artificial Intelligence, Palo Alto, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding social interactions (such as 'hug' or 'fight') is a basic and\nimportant capacity of the human visual system, but a challenging and still open\nproblem for modeling. In this work we study visual recognition of social\ninteractions, based on small but recognizable local regions. The approach is\nbased on two novel key components: (i) A given social interaction can be\nrecognized reliably from reduced images (called 'minimal images'). (ii) The\nrecognition of a social interaction depends on identifying components and\nrelations within the minimal image (termed 'interpretation'). We show\npsychophysics data for minimal images and modeling results for their\ninterpretation. We discuss the integration of minimal configurations in\nrecognizing social interactions in a detailed, high-resolution image.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 16:24:08 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ben-Yosef", "Guy", ""], ["Yachin", "Alon", ""], ["Ullman", "Shimon", ""]]}, {"id": "1712.09300", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Jichang Guo, and Zhongfei (Mark) Zhang", "title": "Zero-Shot Learning via Latent Space Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) is typically achieved by resorting to a class\nsemantic embedding space to transfer the knowledge from the seen classes to\nunseen ones. Capturing the common semantic characteristics between the visual\nmodality and the class semantic modality (e.g., attributes or word vector) is a\nkey to the success of ZSL. In this paper, we propose a novel encoder-decoder\napproach, namely Latent Space Encoding (LSE), to connect the semantic relations\nof different modalities. Instead of requiring a projection function to transfer\ninformation across different modalities like most previous work, LSE per- forms\nthe interactions of different modalities via a feature aware latent space,\nwhich is learned in an implicit way. Specifically, different modalities are\nmodeled separately but optimized jointly. For each modality, an encoder-decoder\nframework is performed to learn a feature aware latent space via jointly\nmaximizing the recoverability of the original space from the latent space and\nthe predictability of the latent space from the original space. To relate\ndifferent modalities together, their features referring to the same concept are\nenforced to share the same latent codings. In this way, the common semantic\ncharacteristics of different modalities are generalized with the latent\nrepresentations. Another property of the proposed approach is that it is easily\nextended to more modalities. Extensive experimental results on four benchmark\ndatasets (AwA, CUB, aPY, and ImageNet) clearly demonstrate the superiority of\nthe proposed approach on several ZSL tasks, including traditional ZSL,\ngeneralized ZSL, and zero-shot retrieval (ZSR).\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 16:26:36 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 15:31:25 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Yu", "Yunlong", "", "Mark"], ["Ji", "Zhong", "", "Mark"], ["Guo", "Jichang", "", "Mark"], ["Zhongfei", "", "", "Mark"], ["Zhang", "", ""]]}, {"id": "1712.09374", "submitter": "Hang Zhao", "authors": "Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan", "title": "HACS: Human Action Clips and Segments Dataset for Recognition and\n  Temporal Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new large-scale dataset for recognition and temporal\nlocalization of human actions collected from Web videos. We refer to it as HACS\n(Human Action Clips and Segments). We leverage both consensus and disagreement\namong visual classifiers to automatically mine candidate short clips from\nunlabeled videos, which are subsequently validated by human annotators. The\nresulting dataset is dubbed HACS Clips. Through a separate process we also\ncollect annotations defining action segment boundaries. This resulting dataset\nis called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips\nsampled from 504K untrimmed videos, and HACS Seg-ments contains 139K action\nsegments densely annotatedin 50K untrimmed videos spanning 200 action\ncategories. HACS Clips contains more labeled examples than any existing video\nbenchmark. This renders our dataset both a large scale action recognition\nbenchmark and an excellent source for spatiotemporal feature learning. In our\ntransferlearning experiments on three target datasets, HACS Clips outperforms\nKinetics-600, Moments-In-Time and Sports1Mas a pretraining source. On HACS\nSegments, we evaluate state-of-the-art methods of action proposal generation\nand action localization, and highlight the new challenges posed by our dense\ntemporal annotations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 19:09:11 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 21:49:09 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 07:35:48 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhao", "Hang", ""], ["Torralba", "Antonio", ""], ["Torresani", "Lorenzo", ""], ["Yan", "Zhicheng", ""]]}, {"id": "1712.09382", "submitter": "Ira Kemelmacher-Shlizerman", "authors": "Eli Shlizerman, Lucio M. Dery, Hayden Schoen, Ira\n  Kemelmacher-Shlizerman", "title": "Audio to Body Dynamics", "comments": "Link with videos https://arviolin.github.io/AudioBodyDynamics/", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2018", "doi": "10.1109/CVPR.2018.00790", "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that gets as input an audio of violin or piano playing,\nand outputs a video of skeleton predictions which are further used to animate\nan avatar. The key idea is to create an animation of an avatar that moves their\nhands similarly to how a pianist or violinist would do, just from audio. Aiming\nfor a fully detailed correct arms and fingers motion is a goal, however, it's\nnot clear if body movement can be predicted from music at all. In this paper,\nwe present the first result that shows that natural body dynamics can be\npredicted at all. We built an LSTM network that is trained on violin and piano\nrecital videos uploaded to the Internet. The predicted points are applied onto\na rigged avatar to create the animation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 23:45:00 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Shlizerman", "Eli", ""], ["Dery", "Lucio M.", ""], ["Schoen", "Hayden", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1712.09392", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Kai Cao, and Anil K. Jain", "title": "RaspiReader: Open Source Fingerprint Reader", "comments": "substantial text overlap with arXiv:1708.07887", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We open source an easy to assemble, spoof resistant, high resolution, optical\nfingerprint reader, called RaspiReader, using ubiquitous components. By using\nour open source STL files and software, RaspiReader can be built in under one\nhour for only US $175. As such, RaspiReader provides the fingerprint research\ncommunity a seamless and simple method for quickly prototyping new ideas\ninvolving fingerprint reader hardware. In particular, we posit that this open\nsource fingerprint reader will facilitate the exploration of novel fingerprint\nspoof detection techniques involving both hardware and software. We demonstrate\none such spoof detection technique by specially customizing RaspiReader with\ntwo cameras for fingerprint image acquisition. One camera provides high\ncontrast, frustrated total internal reflection (FTIR) fingerprint images, and\nthe other outputs direct images of the finger in contact with the platen. Using\nboth of these image streams, we extract complementary information which, when\nfused together and used for spoof detection, results in marked performance\nimprovement over previous methods relying only on grayscale FTIR images\nprovided by COTS optical readers. Finally, fingerprint matching experiments\nbetween images acquired from the FTIR output of RaspiReader and images acquired\nfrom a COTS reader verify the interoperability of the RaspiReader with existing\nCOTS optical readers.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 20:21:17 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1712.09401", "submitter": "Dinh-Luan Nguyen", "authors": "Dinh-Luan Nguyen, Kai Cao, Anil K. Jain", "title": "Robust Minutiae Extractor: Integrating Deep Networks and Fingerprint\n  Domain Knowledge", "comments": "Accepted to International Conference on Biometrics (ICB 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully automatic minutiae extractor, called MinutiaeNet, based on\ndeep neural networks with compact feature representation for fast comparison of\nminutiae sets. Specifically, first a network, called CoarseNet, estimates the\nminutiae score map and minutiae orientation based on convolutional neural\nnetwork and fingerprint domain knowledge (enhanced image, orientation field,\nand segmentation map). Subsequently, another network, called FineNet, refines\nthe candidate minutiae locations based on score map. We demonstrate the\neffectiveness of using the fingerprint domain knowledge together with the deep\nnetworks. Experimental results on both latent (NIST SD27) and plain (FVC 2004)\npublic domain fingerprint datasets provide comprehensive empirical support for\nthe merits of our method. Further, our method finds minutiae sets that are\nbetter in terms of precision and recall in comparison with state-of-the-art on\nthese two datasets. Given the lack of annotated fingerprint datasets with\nminutiae ground truth, the proposed approach to robust minutiae detection will\nbe useful to train network-based fingerprint matching algorithms as well as for\nevaluating fingerprint individuality at scale. MinutiaeNet is implemented in\nTensorflow: https://github.com/luannd/MinutiaeNet\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 20:55:43 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Nguyen", "Dinh-Luan", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1712.09448", "submitter": "S\\'ebastien Ehrhardt", "authors": "Sebastien Ehrhardt and Aron Monszpart and Niloy Mitra and Andrea\n  Vedaldi", "title": "Taking Visual Motion Prediction To New Heightfields", "comments": "arXiv admin note: text overlap with arXiv:1706.02179", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and estimating the associated parameters. In order to be able to\nleverage the approximation capabilities of artificial intelligence techniques\nin such physics related contexts, researchers have handcrafted the relevant\nstates, and then used neural networks to learn the state transitions using\nsimulation runs as training data. Unfortunately, such approaches are unsuited\nfor modeling complex real-world scenarios, where manually authoring relevant\nstate spaces tend to be tedious and challenging. In this work, we investigate\nif neural networks can implicitly learn physical states of real-world\nmechanical processes only based on visual data while internally modeling\nnon-homogeneous environment and in the process enable long-term physical\nextrapolation. We develop a recurrent neural network architecture for this task\nand also characterize resultant uncertainties in the form of evolving variance\nestimates. We evaluate our setup to extrapolate motion of rolling ball(s) on\nbowls of varying shape and orientation, and on arbitrary heightfields using\nonly images as input. We report significant improvements over existing\nimage-based methods both in terms of accuracy of predictions and complexity of\nscenarios; and report competitive performance with approaches that, unlike us,\nassume access to internal physical states.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 13:22:14 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Ehrhardt", "Sebastien", ""], ["Monszpart", "Aron", ""], ["Mitra", "Niloy", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1712.09458", "submitter": "Michael Henry", "authors": "Jesse M. Johns, Jeremiah Rounds, and Michael J. Henry", "title": "Multi-modal Geolocation Estimation Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the location where an image was taken based solely on the contents\nof the image is a challenging task, even for humans, as properly labeling an\nimage in such a fashion relies heavily on contextual information, and is not as\nsimple as identifying a single object in the image. Thus any methods which\nattempt to do so must somehow account for these complexities, and no single\nmodel to date is completely capable of addressing all challenges. This work\ncontributes to the state of research in image geolocation inferencing by\nintroducing a novel global meshing strategy, outlining a variety of training\nprocedures to overcome the considerable data limitations when training these\nmodels, and demonstrating how incorporating additional information can be used\nto improve the overall performance of a geolocation inference model. In this\nwork, it is shown that Delaunay triangles are an effective type of mesh for\ngeolocation in relatively low volume scenarios when compared to results from\nstate of the art models which use quad trees and an order of magnitude more\ntraining data. In addition, the time of posting, learned user albuming, and\nother meta data are easily incorporated to improve geolocation by up to 11% for\ncountry-level (750 km) locality accuracy to 3% for city-level (25 km)\nlocalities.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 23:52:39 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Johns", "Jesse M.", ""], ["Rounds", "Jeremiah", ""], ["Henry", "Michael J.", ""]]}, {"id": "1712.09491", "submitter": "Arjun Nitin Bhagoji", "authors": "Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn Song", "title": "Exploring the Space of Black-box Attacks on Deep Neural Networks", "comments": "25 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing black-box attacks on deep neural networks (DNNs) so far have largely\nfocused on transferability, where an adversarial instance generated for a\nlocally trained model can \"transfer\" to attack other learning models. In this\npaper, we propose novel Gradient Estimation black-box attacks for adversaries\nwith query access to the target model's class probabilities, which do not rely\non transferability. We also propose strategies to decouple the number of\nqueries required to generate each adversarial sample from the dimensionality of\nthe input. An iterative variant of our attack achieves close to 100%\nadversarial success rates for both targeted and untargeted attacks on DNNs. We\ncarry out extensive experiments for a thorough comparative evaluation of\nblack-box attacks and show that the proposed Gradient Estimation attacks\noutperform all transferability based black-box attacks we tested on both MNIST\nand CIFAR-10 datasets, achieving adversarial success rates similar to well\nknown, state-of-the-art white-box attacks. We also apply the Gradient\nEstimation attacks successfully against a real-world Content Moderation\nclassifier hosted by Clarifai. Furthermore, we evaluate black-box attacks\nagainst state-of-the-art defenses. We show that the Gradient Estimation attacks\nare very effective even against these defenses.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 04:39:02 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Bhagoji", "Arjun Nitin", ""], ["He", "Warren", ""], ["Li", "Bo", ""], ["Song", "Dawn", ""]]}, {"id": "1712.09531", "submitter": "Jia-Nan Wu", "authors": "Zhimeng Zhang, Jianan Wu, Xuan Zhang, Chi Zhang", "title": "Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent\n  Progress on DukeMTMC Project", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many methods perform well in single camera tracking, multi-camera\ntracking remains a challenging problem with less attention. DukeMTMC is a\nlarge-scale, well-annotated multi-camera tracking benchmark which makes great\nprogress in this field. This report is dedicated to briefly introduce our\nmethod on DukeMTMC and show that simple hierarchical clustering with\nwell-trained person re-identification features can get good results on this\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 09:32:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zhang", "Zhimeng", ""], ["Wu", "Jianan", ""], ["Zhang", "Xuan", ""], ["Zhang", "Chi", ""]]}, {"id": "1712.09532", "submitter": "Sang Phan", "authors": "Sang Phan and Gustav Eje Henter and Yusuke Miyao and Shin'ichi Satoh", "title": "Consensus-based Sequence Training for Video Captioning", "comments": "11 pages, 4 figures, 5 tables. Github repo at\n  https://github.com/mynlp/cst_captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Captioning models are typically trained using the cross-entropy loss.\nHowever, their performance is evaluated on other metrics designed to better\ncorrelate with human assessments. Recently, it has been shown that\nreinforcement learning (RL) can directly optimize these metrics in tasks such\nas captioning. However, this is computationally costly and requires specifying\na baseline reward at each step to make training converge. We propose a fast\napproach to optimize one's objective of interest through the REINFORCE\nalgorithm. First we show that, by replacing model samples with ground-truth\nsentences, RL training can be seen as a form of weighted cross-entropy loss,\ngiving a fast, RL-based pre-training algorithm. Second, we propose to use the\nconsensus among ground-truth captions of the same video as the baseline reward.\nThis can be computed very efficiently. We call the complete proposal\nConsensus-based Sequence Training (CST). Applied to the MSRVTT video captioning\nbenchmark, our proposals train significantly faster than comparable methods and\nestablish a new state-of-the-art on the task, improving the CIDEr score from\n47.3 to 54.2.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 09:38:52 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Phan", "Sang", ""], ["Henter", "Gustav Eje", ""], ["Miyao", "Yusuke", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1712.09558", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Xingyang Ni, Francesco Cricri, Lixin Fan, Emre Aksu", "title": "Memory-Efficient Deep Salient Object Segmentation Networks on Gridized\n  Superpixels", "comments": "6 pages, submitted to MMSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision algorithms with pixel-wise labeling tasks, such as semantic\nsegmentation and salient object detection, have gone through a significant\naccuracy increase with the incorporation of deep learning. Deep segmentation\nmethods slightly modify and fine-tune pre-trained networks that have hundreds\nof millions of parameters. In this work, we question the need to have such\nmemory demanding networks for the specific task of salient object segmentation.\nTo this end, we propose a way to learn a memory-efficient network from scratch\nby training it only on salient object detection datasets. Our method encodes\nimages to gridized superpixels that preserve both the object boundaries and the\nconnectivity rules of regular pixels. This representation allows us to use\nconvolutional neural networks that operate on regular grids. By using these\nencoded images, we train a memory-efficient network using only 0.048\\% of the\nnumber of parameters that other deep salient object detection networks have.\nOur method shows comparable accuracy with the state-of-the-art deep salient\nobject detection methods and provides a faster and a much more memory-efficient\nalternative to them. Due to its easy deployment, such a network is preferable\nfor applications in memory limited devices such as mobile phones and IoT\ndevices.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 12:20:13 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 10:11:36 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Aytekin", "Caglar", ""], ["Ni", "Xingyang", ""], ["Cricri", "Francesco", ""], ["Fan", "Lixin", ""], ["Aksu", "Emre", ""]]}, {"id": "1712.09665", "submitter": "Tom B Brown", "authors": "Tom B. Brown, Dandelion Man\\'e, Aurko Roy, Mart\\'in Abadi, Justin\n  Gilmer", "title": "Adversarial Patch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to create universal, robust, targeted adversarial image\npatches in the real world. The patches are universal because they can be used\nto attack any scene, robust because they work under a wide variety of\ntransformations, and targeted because they can cause a classifier to output any\ntarget class. These adversarial patches can be printed, added to any scene,\nphotographed, and presented to image classifiers; even when the patches are\nsmall, they cause the classifiers to ignore the other items in the scene and\nreport a chosen target class.\n  To reproduce the results from the paper, our code is available at\nhttps://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 20:03:51 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 01:44:59 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Brown", "Tom B.", ""], ["Man\u00e9", "Dandelion", ""], ["Roy", "Aurko", ""], ["Abadi", "Mart\u00edn", ""], ["Gilmer", "Justin", ""]]}, {"id": "1712.09684", "submitter": "Nitin Agarwal", "authors": "Nitin Agarwal, Xiangmin Xu, Gopi Meenakshisundaram", "title": "Geometry Processing of Conventionally Produced Mouse Brain Slice Images", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain mapping research in most neuroanatomical laboratories relies on\nconventional processing techniques, which often introduce histological\nartifacts such as tissue tears and tissue loss. In this paper we present\ntechniques and algorithms for automatic registration and 3D reconstruction of\nconventionally produced mouse brain slices in a standardized atlas space. This\nis achieved first by constructing a virtual 3D mouse brain model from annotated\nslices of Allen Reference Atlas (ARA). Virtual re-slicing of the reconstructed\nmodel generates ARA-based slice images corresponding to the microscopic images\nof histological brain sections. These image pairs are aligned using a geometric\napproach through contour images. Histological artifacts in the microscopic\nimages are detected and removed using Constrained Delaunay Triangulation before\nperforming global alignment. Finally, non-linear registration is performed by\nsolving Laplace's equation with Dirichlet boundary conditions. Our methods\nprovide significant improvements over previously reported registration\ntechniques for the tested slices in 3D space, especially on slices with\nsignificant histological artifacts. Further, as an application we count the\nnumber of neurons in various anatomical regions using a dataset of 51\nmicroscopic slices from a single mouse brain. This work represents a\nsignificant contribution to this subfield of neuroscience as it provides tools\nto neuroanatomist for analyzing and processing histological data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 21:05:06 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Agarwal", "Nitin", ""], ["Xu", "Xiangmin", ""], ["Meenakshisundaram", "Gopi", ""]]}, {"id": "1712.09708", "submitter": "Youssef Tamaazousti", "authors": "Youssef Tamaazousti, Herv\\'e Le Borgne, C\\'eline Hudelot, Mohamed El\n  Amine Seddik, Mohamed Tamaazousti", "title": "Learning More Universal Representations for Transfer-Learning", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A representation is supposed universal if it encodes any element of the\nvisual world (e.g., objects, scenes) in any configuration (e.g., scale,\ncontext). While not expecting pure universal representations, the goal in the\nliterature is to improve the universality level, starting from a representation\nwith a certain level. To do so, the state-of-the-art consists in learning\nCNN-based representations on a diversified training problem (e.g., ImageNet\nmodified by adding annotated data). While it effectively increases\nuniversality, such approach still requires a large amount of efforts to satisfy\nthe needs in annotated data. In this work, we propose two methods to improve\nuniversality, but pay special attention to limit the need of annotated data. We\nalso propose a unified framework of the methods based on the diversifying of\nthe training problem. Finally, to better match Atkinson's cognitive study about\nuniversal human representations, we proposed to rely on the transfer-learning\nscheme as well as a new metric to evaluate universality. This latter, aims us\nto demonstrates the interest of our methods on 10 target-problems, relating to\nthe classification task and a variety of visual domains.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 23:14:46 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 08:58:38 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 00:14:43 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 17:00:16 GMT"}, {"version": "v5", "created": "Mon, 3 Sep 2018 01:03:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Tamaazousti", "Youssef", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Hudelot", "C\u00e9line", ""], ["Seddik", "Mohamed El Amine", ""], ["Tamaazousti", "Mohamed", ""]]}, {"id": "1712.09709", "submitter": "Qiangeng Xu", "authors": "Qiangeng Xu, John Kender", "title": "Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro\n  Gesture", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the research of the impact of gestures using by a lecturer, one\nchallenging task is to infer the attention of a group of audiences. Two\nimportant measurements that can help infer the level of attention are eye\nmovement data and Electroencephalography (EEG) data. Under the fundamental\nassumption that a group of people would look at the same place if they all pay\nattention at the same time, we apply a method, \"Time Warp Edit Distance\", to\ncalculate the similarity of their eye movement trajectories. Moreover, we also\ncluster eye movement pattern of audiences based on these pair-wised similarity\nmetrics. Besides, since we don't have a direct metric for the \"attention\"\nground truth, a visual assessment would be beneficial to evaluate the\ngesture-attention relationship. Thus we also implement a visualization tool.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 23:26:30 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 18:36:29 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Xu", "Qiangeng", ""], ["Kender", "John", ""]]}, {"id": "1712.09713", "submitter": "Charles Zheng", "authors": "Charles Zheng, Rakesh Achanta, Yuval Benjamini", "title": "Extrapolating Expected Accuracies for Large Multi-Class Problems", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of multi-class classification generally increases with the\nnumber of classes. Using data from a subset of the classes, can we predict how\nwell a classifier will scale with an increased number of classes? Under the\nassumptions that the classes are sampled identically and independently from a\npopulation, and that the classifier is based on independently learned scoring\nfunctions, we show that the expected accuracy when the classifier is trained on\nk classes is the (k-1)st moment of a certain distribution that can be estimated\nfrom data. We present an unbiased estimation method based on the theory, and\ndemonstrate its application on a facial recognition example.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 23:49:39 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zheng", "Charles", ""], ["Achanta", "Rakesh", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1712.09775", "submitter": "Uche Nnolim", "authors": "Uche A. Nnolim", "title": "Sky detection and log illumination refinement for PDE-based hazy image\n  contrast enhancement", "comments": "22 pages, 13 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the results of a sky detection technique used to improve\nthe performance of a previously developed partial differential equation\n(PDE)-based hazy image enhancement algorithm. Additionally, a proposed\nalternative method utilizes a function for log illumination refinement to\nimprove de-hazing results while avoiding over-enhancement of sky or homogeneous\nregions. The algorithms were tested with several benchmark and calibration\nimages and compared with several standard algorithms from the literature.\nResults indicate that the algorithms yield mostly consistent results and\nsurpasses several of the other algorithms in terms of colour and contrast\nenhancement in addition to improved edge visibility.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 06:30:26 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 08:56:18 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Nnolim", "Uche A.", ""]]}, {"id": "1712.09789", "submitter": "Jun Chen", "authors": "Jun Chen, Keisuke Nonaka, Ryosuke Watanabe, Hiroshi Sankoh, Houari\n  Sabirin, and Sei Naito", "title": "Efficient Parallel Connected Components Labeling with a Coarse-to-fine\n  Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes a new parallel approach to solve connected components on\na 2D binary image implemented with CUDA. We employ the following strategies to\naccelerate neighborhood exploration after dividing an input image into\nindependent blocks. In the local labeling stage, a coarse-labeling algorithm,\nincluding row-column connection and label-equivalence list unification, is\napplied first to sort out the mess of an initialized local label map; a\nrefinement algorithm is then introduced to merge separated sub-regions from a\nsingle component. In the block merge stage, we scan the pixels located on the\nboundary of each block instead of solving the connectivity of all the pixels.\nWith the proposed method, the length of label-equivalence lists is compressed,\nand the number of memory accesses is reduced. Thus, the efficiency of connected\ncomponents labeling is improved. Experimental results show that our method\noutperforms the other approaches between $29\\%$ and $80\\%$ on average.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 08:50:22 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 08:18:10 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Chen", "Jun", ""], ["Nonaka", "Keisuke", ""], ["Watanabe", "Ryosuke", ""], ["Sankoh", "Hiroshi", ""], ["Sabirin", "Houari", ""], ["Naito", "Sei", ""]]}, {"id": "1712.09792", "submitter": "Shreyas Malakarjun Patil", "authors": "Shreyas Malakarjun Patil, Aditya Nigam, Arnav Bhavsar, Chiranjoy\n  Chattopadhyay", "title": "Siamese LSTM based Fiber Structural Similarity Network (FS2Net) for\n  Rotation Invariant Brain Tractography Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep learning architecture combining\nstacked Bi-directional LSTM and LSTMs with the Siamese network architecture for\nsegmentation of brain fibers, obtained from tractography data, into\nanatomically meaningful clusters. The proposed network learns the structural\ndifference between fibers of different classes, which enables it to classify\nfibers with high accuracy. Importantly, capturing such deep inter and intra\nclass structural relationship also ensures that the segmentation is robust to\nrelative rotation among test and training data, hence can be used with\nunregistered data. Our extensive experimentation over order of\nhundred-thousands of fibers show that the proposed model achieves\nstate-of-the-art results, even in cases of large relative rotations between\ntest and training data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 09:09:28 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Patil", "Shreyas Malakarjun", ""], ["Nigam", "Aditya", ""], ["Bhavsar", "Arnav", ""], ["Chattopadhyay", "Chiranjoy", ""]]}, {"id": "1712.09809", "submitter": "Yancong Wei", "authors": "Qiangqiang Yuan, Yancong Wei, Xiangchao Meng, Huanfeng Shen, Liangpei\n  Zhang", "title": "A Multi-Scale and Multi-Depth Convolutional Neural Network for Remote\n  Sensing Imagery Pan-Sharpening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pan-sharpening is a fundamental and significant task in the field of remote\nsensing imagery processing, in which high-resolution spatial details from\npanchromatic images are employed to enhance the spatial resolution of\nmulti-spectral (MS) images. As the transformation from low spatial resolution\nMS image to high-resolution MS image is complex and highly non-linear, inspired\nby the powerful representation for non-linear relationships of deep neural\nnetworks, we introduce multi-scale feature extraction and residual learning\ninto the basic convolutional neural network (CNN) architecture and propose the\nmulti-scale and multi-depth convolutional neural network (MSDCNN) for the\npan-sharpening of remote sensing imagery. Both the quantitative assessment\nresults and the visual assessment confirm that the proposed network yields\nhigh-resolution MS images that are superior to the images produced by the\ncompared state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:28:38 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Yuan", "Qiangqiang", ""], ["Wei", "Yancong", ""], ["Meng", "Xiangchao", ""], ["Shen", "Huanfeng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1712.09867", "submitter": "Wen Liu", "authors": "Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao", "title": "Future Frame Prediction for Anomaly Detection -- A New Baseline", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in videos refers to the identification of events that do\nnot conform to expected behavior. However, almost all existing methods tackle\nthe problem by minimizing the reconstruction errors of training data, which\ncannot guarantee a larger reconstruction error for an abnormal event. In this\npaper, we propose to tackle the anomaly detection problem within a video\nprediction framework. To the best of our knowledge, this is the first work that\nleverages the difference between a predicted future frame and its ground truth\nto detect an abnormal event. To predict a future frame with higher quality for\nnormal events, other than the commonly used appearance (spatial) constraints on\nintensity and gradient, we also introduce a motion (temporal) constraint in\nvideo prediction by enforcing the optical flow between predicted frames and\nground truth frames to be consistent, and this is the first work that\nintroduces a temporal constraint into the video prediction task. Such spatial\nand motion constraints facilitate the future frame prediction for normal\nevents, and consequently facilitate to identify those abnormal events that do\nnot conform the expectation. Extensive experiments on both a toy dataset and\nsome publicly available datasets validate the effectiveness of our method in\nterms of robustness to the uncertainty in normal events and the sensitivity to\nabnormal events.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 14:04:52 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 04:38:38 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 13:25:36 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Liu", "Wen", ""], ["Luo", "Weixin", ""], ["Lian", "Dongze", ""], ["Gao", "Shenghua", ""]]}, {"id": "1712.09872", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha and\n  Vijayan K. Asari", "title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks", "comments": "12 pages,22 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:1705.02680", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 14:31:56 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 17:22:42 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 18:40:54 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Sidike", "Peheding", ""], ["Hasan", "Mahmudul", ""], ["Taha", "Tark M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1712.09888", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, and\n  Vijayan K. Asari", "title": "Improved Inception-Residual Convolutional Neural Network for Object\n  Recognition", "comments": "17 pages, 15 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning and computer vision have driven many of the greatest\nadvances in the modeling of Deep Convolutional Neural Networks (DCNNs).\nNowadays, most of the research has been focused on improving recognition\naccuracy with better DCNN models and learning approaches. The recurrent\nconvolutional approach is not applied very much, other than in a few DCNN\narchitectures. On the other hand, Inception-v4 and Residual networks have\npromptly become popular among computer the vision community. In this paper, we\nintroduce a new DCNN model called the Inception Recurrent Residual\nConvolutional Neural Network (IRRCNN), which utilizes the power of the\nRecurrent Convolutional Neural Network (RCNN), the Inception network, and the\nResidual network. This approach improves the recognition accuracy of the\nInception-residual network with same number of network parameters. In addition,\nthis proposed architecture generalizes the Inception network, the RCNN, and the\nResidual network with significantly improved training accuracy. We have\nempirically evaluated the performance of the IRRCNN model on different\nbenchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The\nexperimental results show higher recognition accuracy against most of the\npopular DCNN models including the RCNN. We have also investigated the\nperformance of the IRRCNN approach against the Equivalent Inception Network\n(EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the\nCIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in\nclassification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100\ndataset respectively. Furthermore, the experiment has been conducted on the\nTinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing\naccuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the\nEIRN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 15:08:14 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Hasan", "Mahmudul", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1712.09913", "submitter": "Hao Li", "authors": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein", "title": "Visualizing the Loss Landscape of Neural Nets", "comments": "NIPS 2018 (extended version, 10.5 pages), code is available at\n  https://github.com/tomgoldstein/loss-landscape", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network training relies on our ability to find \"good\" minimizers of\nhighly non-convex loss functions. It is well-known that certain network\narchitecture designs (e.g., skip connections) produce loss functions that train\neasier, and well-chosen training parameters (batch size, learning rate,\noptimizer) produce minimizers that generalize better. However, the reasons for\nthese differences, and their effects on the underlying loss landscape, are not\nwell understood. In this paper, we explore the structure of neural loss\nfunctions, and the effect of loss landscapes on generalization, using a range\nof visualization methods. First, we introduce a simple \"filter normalization\"\nmethod that helps us visualize loss function curvature and make meaningful\nside-by-side comparisons between loss functions. Then, using a variety of\nvisualizations, we explore how network architecture affects the loss landscape,\nand how training parameters affect the shape of minimizers.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:15:42 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 18:23:03 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 06:25:20 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Li", "Hao", ""], ["Xu", "Zheng", ""], ["Taylor", "Gavin", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1712.10042", "submitter": "Lingkun Luo", "authors": "Lingkun Luo, Liming Chen, Shiqiang Hu, Ying Lu, Xiaofang Wang", "title": "Discriminative and Geometry Aware Unsupervised Domain Adaptation", "comments": "18pages, 12figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims to generalize a learning model across training\nand testing data despite the mismatch of their data distributions. In light of\na theoretical estimation of upper error bound, we argue in this paper that an\neffective DA method should 1) search a shared feature subspace where source and\ntarget data are not only aligned in terms of distributions as most state of the\nart DA methods do, but also discriminative in that instances of different\nclasses are well separated; 2) account for the geometric structure of the\nunderlying data manifold when inferring data labels on the target domain. In\ncomparison with a baseline DA method which only cares about data distribution\nalignment between source and target, we derive three different DA models,\nnamely CDDA, GA-DA, and DGA-DA, to highlight the contribution of Close yet\nDiscriminative DA(CDDA) based on 1), Geometry Aware DA (GA-DA) based on 2), and\nfinally Discriminative and Geometry Aware DA (DGA-DA) implementing jointly 1)\nand 2). Using both synthetic and real data, we show the effectiveness of the\nproposed approach which consistently outperforms state of the art DA methods\nover 36 image classification DA tasks through 6 popular benchmarks. We further\ncarry out in-depth analysis of the proposed DA method in quantifying the\ncontribution of each term of our DA model and provide insights into the\nproposed DA methods in visualizing both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 20:02:49 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Luo", "Lingkun", ""], ["Chen", "Liming", ""], ["Hu", "Shiqiang", ""], ["Lu", "Ying", ""], ["Wang", "Xiaofang", ""]]}, {"id": "1712.10136", "submitter": "Koustav Mullick Mr.", "authors": "Koustav Mullick and Anoop M. Namboodiri", "title": "Learning Deep and Compact Models for Gesture Recognition", "comments": "Accepted at 2017 IEEE International Conference on Image Processing\n  (ICIP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look at the problem of developing a compact and accurate model for gesture\nrecognition from videos in a deep-learning framework. Towards this we propose a\njoint 3DCNN-LSTM model that is end-to-end trainable and is shown to be better\nsuited to capture the dynamic information in actions. The solution achieves\nclose to state-of-the-art accuracy on the ChaLearn dataset, with only half the\nmodel size. We also explore ways to derive a much more compact representation\nin a knowledge distillation framework followed by model compression. The final\nmodel is less than $1~MB$ in size, which is less than one hundredth of our\ninitial model, with a drop of $7\\%$ in accuracy, and is suitable for real-time\ngesture recognition on mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 07:38:43 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Mullick", "Koustav", ""], ["Namboodiri", "Anoop M.", ""]]}, {"id": "1712.10151", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Shota Horiguchi, Daiki Ikami, Kiyoharu Aizawa", "title": "Significance of Softmax-based Features in Comparison to Distance Metric\n  Learning-based Features", "comments": "6 pages", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2019.2911075", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of useful deep features is important for many computer vision\ntasks. Deep features extracted from classification networks have proved to\nperform well in those tasks. To obtain features of greater usefulness,\nend-to-end distance metric learning (DML) has been applied to train the feature\nextractor directly. However, in these DML studies, there were no equitable\ncomparisons between features extracted from a DML-based network and those from\na softmax-based network. In this paper, by presenting objective comparisons\nbetween these two approaches under the same network architecture, we show that\nthe softmax-based features perform competitive, or even better, to the\nstate-of-the-art DML features when the size of the dataset, that is, the number\nof training samples per class, is large. The results suggest that softmax-based\nfeatures should be properly taken into account when evaluating the performance\nof deep features.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:01:09 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 04:05:27 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Horiguchi", "Shota", ""], ["Ikami", "Daiki", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1712.10152", "submitter": "Sowmya V", "authors": "V. Sowmya, D. Govind, K. P. Soman", "title": "Exploring the significance of using perceptually relevant image\n  decolorization method for scene classification", "comments": "This article is accepted in SPIE J.of Electronic Imaging with title:\n  Significance of perceptually relevant image decolorization for scene\n  classification", "journal-ref": "Sowmya Viswanathan, Govind Divakaran, Kutti Padanyl Soman,\n  Significance of perceptually relevant image decolorization for scene\n  classification, J. Electron. Imaging 26(6), 063019 (2017)", "doi": "10.1117/1.JEI.26.6.063019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A color image contains luminance and chrominance components representing the\nintensity and color information respectively. The objective of the work\npresented in this paper is to show the significance of incorporating the\nchrominance information for the task of scene classification. An improved\ncolor-to-grayscale image conversion algorithm by effectively incorporating the\nchrominance information is proposed using color-to-gay structure similarity\nindex (C2G-SSIM) and singular value decomposition (SVD) to improve the\nperceptual quality of the converted grayscale images. The experimental result\nanalysis based on the image quality assessment for image decolorization called\nC2G-SSIM and success rate (Cadik and COLOR250 datasets) shows that the proposed\nimage decolorization technique performs better than 8 existing benchmark\nalgorithms for image decolorization. In the second part of the paper, the\neffectiveness of incorporating the chrominance component in scene\nclassification task is demonstrated using the deep belief network (DBN) based\nimage classification system developed using dense scale invariant feature\ntransform (SIFT) as features. The levels of chrominance information\nincorporated by the proposed image decolorization technique is confirmed by the\nimprovement in the overall scene classification accuracy . Also, the overall\nscene classification performance is improved by the combination of models\nobtained using the proposed and the conventional decolorization methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:04:02 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Sowmya", "V.", ""], ["Govind", "D.", ""], ["Soman", "K. P.", ""]]}, {"id": "1712.10164", "submitter": "Mohammad Amin Khorsandi", "authors": "Mohammad Amin Khorsandi, Nader Karimi, Shadrokh Samavi", "title": "Polyp detection inside the capsule endoscopy: an approach for power\n  consumption reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule endoscopy is a novel and non-invasive method for diagnosis, which\nassists gastroenterologists to monitor the digestive track. Although this new\ntechnology has many advantages over the conventional endoscopy, there are\nweaknesses that limits the usage of this technology. Some weaknesses are due to\nusing small-size batteries. Radio transmitter consumes the largest portion of\nenergy; consequently, a simple way to reduce the power consumption is to reduce\nthe data to be transmitted. Many works are proposed to reduce the amount of\ndata to be transmitted consist of specific compression methods and reduction in\nvideo resolution and frame rate. We proposed a system inside the capsule for\ndetecting informative frames and sending these frames instead of several\nnon-informative frames. In this work, we specifically focused on hardware\nfriendly algorithm (with capability of parallelism and pipeline) for\nimplementation of polyp detection. Two features of positive contrast and\ncustomized edges of polyps are exploited to define whether the frame consists\nof polyp or not. The proposed method is devoid of complex and iterative\nstructure to save power and reduce the response time. Experimental results\nindicate acceptable rate of detection of our work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:54:08 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Khorsandi", "Mohammad Amin", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1712.10207", "submitter": "Shima Rafiei", "authors": "Ebrahim Nasr-Esfahani, Shima Rafiei, Mohammad H. Jafari, Nader Karimi,\n  James S. Wrobel, S.M. Reza Soroushmehr, Shadrokh Samavi, Kayvan Najarian", "title": "Dense Pooling layers in Fully Convolutional Network for Skin Lesion\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the essential tasks in medical image analysis is segmentation and\naccurate detection of borders. Lesion segmentation in skin images is an\nessential step in the computerized detection of skin cancer. However, many of\nthe state-of-the-art segmentation methods have deficiencies in their border\ndetection phase. In this paper, a new class of fully convolutional network is\nproposed, with new dense pooling layers for segmentation of lesion regions in\nskin images. This network leads to highly accurate segmentation of lesions on\nskin lesion datasets which outperforms state-of-the-art algorithms in the skin\nlesion segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 12:46:52 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 06:38:44 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 10:32:04 GMT"}, {"version": "v4", "created": "Sat, 31 Aug 2019 09:34:49 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Nasr-Esfahani", "Ebrahim", ""], ["Rafiei", "Shima", ""], ["Jafari", "Mohammad H.", ""], ["Karimi", "Nader", ""], ["Wrobel", "James S.", ""], ["Soroushmehr", "S. M. Reza", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1712.10211", "submitter": "Hefeng Wu", "authors": "Daiguo Deng, Ruomei Wang, Hefeng Wu, Huayong He, Qi Li, Xiaonan Luo", "title": "Learning Deep Similarity Models with Focus Ranking for Fabric Image\n  Retrieval", "comments": "11 pages, 9 figures, accepted by Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2017.12.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabric image retrieval is beneficial to many applications including clothing\nsearching, online shopping and cloth modeling. Learning pairwise image\nsimilarity is of great importance to an image retrieval task. With the\nresurgence of Convolutional Neural Networks (CNNs), recent works have achieved\nsignificant progresses via deep representation learning with metric embedding,\nwhich drives similar examples close to each other in a feature space, and\ndissimilar ones apart from each other. In this paper, we propose a novel\nembedding method termed focus ranking that can be easily unified into a CNN for\njointly learning image representations and metrics in the context of\nfine-grained fabric image retrieval. Focus ranking aims to rank similar\nexamples higher than all dissimilar ones by penalizing ranking disorders via\nthe minimization of the overall cost attributed to similar samples being ranked\nbelow dissimilar ones. At the training stage, training samples are organized\ninto focus ranking units for efficient optimization. We build a large-scale\nfabric image retrieval dataset (FIRD) with about 25,000 images of 4,300\nfabrics, and test the proposed model on the FIRD dataset. Experimental results\nshow the superiority of the proposed model over existing metric embedding\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 13:02:12 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Deng", "Daiguo", ""], ["Wang", "Ruomei", ""], ["Wu", "Hefeng", ""], ["He", "Huayong", ""], ["Li", "Qi", ""], ["Luo", "Xiaonan", ""]]}, {"id": "1712.10215", "submitter": "Angela Dai", "authors": "Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\\\"urgen\n  Sturm, Matthias Nie{\\ss}ner", "title": "ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for\n  3D Scans", "comments": "Video: https://youtu.be/5s5s8iH0NF8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ScanComplete, a novel data-driven approach for taking an\nincomplete 3D scan of a scene as input and predicting a complete 3D model along\nwith per-voxel semantic labels. The key contribution of our method is its\nability to handle large scenes with varying spatial extent, managing the cubic\ngrowth in data size as scene size increases. To this end, we devise a\nfully-convolutional generative 3D CNN model whose filter kernels are invariant\nto the overall scene size. The model can be trained on scene subvolumes but\ndeployed on arbitrarily large scenes at test time. In addition, we propose a\ncoarse-to-fine inference strategy in order to produce high-resolution output\nwhile also leveraging large input context sizes. In an extensive series of\nexperiments, we carefully evaluate different model design choices, considering\nboth deterministic and probabilistic models for completion and semantic\ninference. Our results show that we outperform other methods not only in the\nsize of the environments handled and processing efficiency, but also with\nregard to completion quality and semantic segmentation performance by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 13:17:06 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 01:51:41 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Dai", "Angela", ""], ["Ritchie", "Daniel", ""], ["Bokeloh", "Martin", ""], ["Reed", "Scott", ""], ["Sturm", "J\u00fcrgen", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1712.10248", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Jawook Gu, Jong Chul Ye", "title": "Deep Learning Interior Tomography for Region-of-Interest Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interior tomography for the region-of-interest (ROI) imaging has advantages\nof using a small detector and reducing X-ray radiation dose. However, standard\nanalytic reconstruction suffers from severe cupping artifacts due to existence\nof null space in the truncated Radon transform. Existing penalized\nreconstruction methods may address this problem but they require extensive\ncomputations due to the iterative reconstruction. Inspired by the recent deep\nlearning approaches to low-dose and sparse view CT, here we propose a deep\nlearning architecture that removes null space signals from the FBP\nreconstruction. Experimental results have shown that the proposed method\nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR\nover existing methods in spite of significantly reduced run-time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 14:59:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 15:54:24 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Han", "Yoseob", ""], ["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}]