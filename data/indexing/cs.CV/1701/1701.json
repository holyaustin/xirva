[{"id": "1701.00031", "submitter": "Ricardo Borsoi", "authors": "Ricardo A. Borsoi, Julio C. C. Aya, Guilherme H. Costa, Jos\\'e C. M.\n  Bermudez", "title": "Super-Resolution Reconstruction of Electrical Impedance Tomography\n  Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.compeleceng.2018.05.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical Impedance Tomography (EIT) systems are becoming popular because\nthey present several advantages over competing systems. However, EIT leads to\nimages with very low resolution. Moreover, the nonuniform sampling\ncharacteristic of EIT precludes the straightforward application of traditional\nimage ruper-resolution techniques. In this work, we propose a resampling based\nSuper-Resolution method for EIT image quality improvement. Preliminary results\nshow that the proposed technique can lead to substantial improvements in EIT\nimage resolution, making it more competitive with other technologies.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 23:23:38 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 17:33:41 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 23:43:16 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Borsoi", "Ricardo A.", ""], ["Aya", "Julio C. C.", ""], ["Costa", "Guilherme H.", ""], ["Bermudez", "Jos\u00e9 C. M.", ""]]}, {"id": "1701.00040", "submitter": "Emmanuel Osegi", "authors": "E.N. Osegi", "title": "p-DLA: A Predictive System Model for Onshore Oil and Gas Pipeline\n  Dataset Classification and Monitoring - Part 1", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the rise in militant activity and rogue behaviour in oil and gas regions\naround the world, oil pipeline disturbances is on the increase leading to huge\nlosses to multinational operators and the countries where such facilities\nexist. However, this situation can be averted if adequate predictive monitoring\nschemes are put in place. We propose in the first part of this paper, an\nartificial intelligence predictive monitoring system capable of predictive\nclassification and pattern recognition of pipeline datasets. The predictive\nsystem is based on a highly sparse predictive Deviant Learning Algorithm\n(p-DLA) designed to synthesize a sequence of memory predictive clusters for\neventual monitoring, control and decision making. The DLA (p-DLA) is compared\nwith a popular machine learning algorithm, the Long Short-Term Memory (LSTM)\nwhich is based on a temporal version of the standard feed-forward\nback-propagation trained artificial neural networks (ANNs). The results of\nsimulations study show impressive results and validates the sparse memory\npredictive approach which favours the sub-synthesis of a highly compressed and\nlow dimensional knowledge discovery and information prediction scheme. It also\nshows that the proposed new approach is competitive with a well-known and\nproven AI approach such as the LSTM.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 00:40:17 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Osegi", "E. N.", ""]]}, {"id": "1701.00142", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov,\n  Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt", "title": "EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras\n  (Extended Abstract)", "comments": "Short version of a SIGGRAPH Asia 2016 paper arXiv:1609.07306,\n  presented at EPIC@ECCV16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marker-based and marker-less optical skeletal motion-capture methods use an\noutside-in arrangement of cameras placed around a scene, with viewpoints\nconverging on the center. They often create discomfort by possibly needed\nmarker suits, and their recording volume is severely restricted and often\nconstrained to indoor scenes with controlled backgrounds. We therefore propose\na new method for real-time, marker-less and egocentric motion capture which\nestimates the full-body skeleton pose from a lightweight stereo pair of fisheye\ncameras that are attached to a helmet or virtual-reality headset. It combines\nthe strength of a new generative pose estimation framework for fisheye views\nwith a ConvNet-based body-part detector trained on a new automatically\nannotated and augmented dataset. Our inside-in method captures full-body motion\nin general indoor and outdoor scenes, and also crowded scenes.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 16:49:39 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Rhodin", "Helge", ""], ["Richardt", "Christian", ""], ["Casas", "Dan", ""], ["Insafutdinov", "Eldar", ""], ["Shafiei", "Mohammad", ""], ["Seidel", "Hans-Peter", ""], ["Schiele", "Bernt", ""], ["Theobalt", "Christian", ""]]}, {"id": "1701.00165", "submitter": "Amit Shaked", "authors": "Amit Shaked and Lior Wolf", "title": "Improved Stereo Matching with Constant Highway Networks and Reflective\n  Confidence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved three-step pipeline for the stereo matching problem\nand introduce multiple novelties at each stage. We propose a new highway\nnetwork architecture for computing the matching cost at each possible\ndisparity, based on multilevel weighted residual shortcuts, trained with a\nhybrid loss that supports multilevel comparison of image patches. A novel\npost-processing step is then introduced, which employs a second deep\nconvolutional neural network for pooling global information from multiple\ndisparities. This network outputs both the image disparity map, which replaces\nthe conventional \"winner takes all\" strategy, and a confidence in the\nprediction. The confidence score is achieved by training the network with a new\ntechnique that we call the reflective loss. Lastly, the learned confidence is\nemployed in order to better detect outliers in the refinement step. The\nproposed pipeline achieves state of the art accuracy on the largest and most\ncompetitive stereo benchmarks, and the learned confidence is shown to\noutperform all existing alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 20:24:16 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Shaked", "Amit", ""], ["Wolf", "Lior", ""]]}, {"id": "1701.00169", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz, Marco A. Contreras, and Jun Zhang", "title": "Vertical stratification of forest canopy for segmentation of under-story\n  trees within small-footprint airborne LiDAR point clouds", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing 130C (2017) pp.\n  385-392", "doi": "10.1016/j.isprsjprs.2017.07.001", "report-no": null, "categories": "cs.CV cs.CE cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne LiDAR point cloud representing a forest contains 3D data, from which\nvertical stand structure even of understory layers can be derived. This paper\npresents a tree segmentation approach for multi-story stands that stratifies\nthe point cloud to canopy layers and segments individual tree crowns within\neach layer using a digital surface model based tree segmentation method. The\nnovelty of the approach is the stratification procedure that separates the\npoint cloud to an overstory and multiple understory tree canopy layers by\nanalyzing vertical distributions of LiDAR points within overlapping locales.\nThe procedure does not make a priori assumptions about the shape and size of\nthe tree crowns and can, independent of the tree segmentation method, be\nutilized to vertically stratify tree crowns of forest canopies. We applied the\nproposed approach to the University of Kentucky Robinson Forest - a natural\ndeciduous forest with complex and highly variable terrain and vegetation\nstructure. The segmentation results showed that using the stratification\nprocedure strongly improved detecting understory trees (from 46% to 68%) at the\ncost of introducing a fair number of over-segmented understory trees (increased\nfrom 1% to 16%), while barely affecting the overall segmentation quality of\noverstory trees. Results of vertical stratification of the canopy showed that\nthe point density of understory canopy layers were suboptimal for performing a\nreasonable tree segmentation, suggesting that acquiring denser LiDAR point\nclouds would allow more improvements in segmenting understory trees. As shown\nby inspecting correlations of the results with forest structure, the\nsegmentation approach is applicable to a variety of forest types.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 21:53:09 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 19:28:07 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 15:01:30 GMT"}, {"version": "v4", "created": "Sat, 15 Jul 2017 19:55:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""], ["Zhang", "Jun", ""]]}, {"id": "1701.00193", "submitter": "Hao Liu", "authors": "Hao Liu, Zequn Jie, Karlekar Jayashree, Meibin Qi, Jianguo Jiang,\n  Shuicheng Yan, Jiashi Feng", "title": "Video-based Person Re-identification with Accumulative Motion Context", "comments": "accepted by TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video based person re-identification plays a central role in realistic\nsecurity and video surveillance. In this paper we propose a novel Accumulative\nMotion Context (AMOC) network for addressing this important problem, which\neffectively exploits the long-range motion context for robustly identifying the\nsame person under challenging conditions. Given a video sequence of the same or\ndifferent persons, the proposed AMOC network jointly learns appearance\nrepresentation and motion context from a collection of adjacent frames using a\ntwo-stream convolutional architecture. Then AMOC accumulates clues from motion\ncontext by recurrent aggregation, allowing effective information flow among\nadjacent frames and capturing dynamic gist of the persons. The architecture of\nAMOC is end-to-end trainable and thus motion context can be adapted to\ncomplement appearance clues under unfavorable conditions (e.g. occlusions).\nExtensive experiments are conduced on three public benchmark datasets, i.e.,\nthe iLIDS-VID, PRID-2011 and MARS datasets, to investigate the performance of\nAMOC. The experimental results demonstrate that the proposed AMOC network\noutperforms state-of-the-arts for video-based re-identification significantly\nand confirm the advantage of exploiting long-range motion context for video\nbased person re-identification, validating our motivation evidently.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 04:20:20 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 03:27:01 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Liu", "Hao", ""], ["Jie", "Zequn", ""], ["Jayashree", "Karlekar", ""], ["Qi", "Meibin", ""], ["Jiang", "Jianguo", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1701.00198", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz, Marco A. Contreras, and Jun Zhang", "title": "A robust approach for tree segmentation in deciduous forests using\n  small-footprint airborne LiDAR data", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation 52 (pp. 532-541): Elsevier (2016)", "doi": "10.1016/j.jag.2016.07.006", "report-no": null, "categories": "cs.CV cs.CE cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a non-parametric approach for segmenting trees from\nairborne LiDAR data in deciduous forests. Based on the LiDAR point cloud, the\napproach collects crown information such as steepness and height on-the-fly to\ndelineate crown boundaries, and most importantly, does not require a priori\nassumptions of crown shape and size. The approach segments trees iteratively\nstarting from the tallest within a given area to the smallest until all trees\nhave been segmented. To evaluate its performance, the approach was applied to\nthe University of Kentucky Robinson Forest, a deciduous closed-canopy forest\nwith complex terrain and vegetation conditions. The approach identified 94% of\ndominant and co-dominant trees with a false detection rate of 13%. About 62% of\nintermediate, overtopped, and dead trees were also detected with a false\ndetection rate of 15%. The overall segmentation accuracy was 77%. Correlations\nof the segmentation scores of the proposed approach with local terrain and\nstand metrics was not significant, which is likely an indication of the\nrobustness of the approach as results are not sensitive to the differences in\nterrain and stand structures.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 04:49:47 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""], ["Zhang", "Jun", ""]]}, {"id": "1701.00294", "submitter": "Alejandro Frery", "authors": "Jos\\'e Naranjo-Torres, Juliana Gambini, and Alejandro C. Frery", "title": "The Geodesic Distance between $\\mathcal{G}_I^0$ Models and its\n  Application to Region Discrimination", "comments": "Accepted for publication in the IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing (J-STARS), 1 January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathcal{G}_I^0$ distribution is able to characterize different regions\nin monopolarized SAR imagery. It is indexed by three parameters: the number of\nlooks (which can be estimated in the whole image), a scale parameter and a\ntexture parameter. This paper presents a new proposal for feature extraction\nand region discrimination in SAR imagery, using the geodesic distance as a\nmeasure of dissimilarity between $\\mathcal{G}_I^0$ models. We derive geodesic\ndistances between models that describe several practical situations, assuming\nthe number of looks known, for same and different texture and for same and\ndifferent scale. We then apply this new tool to the problems of (i)~identifying\nedges between regions with different texture, and (ii)~quantify the\ndissimilarity between pairs of samples in actual SAR data. We analyze the\nadvantages of using the geodesic distance when compared to stochastic\ndistances.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 22:37:13 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Naranjo-Torres", "Jos\u00e9", ""], ["Gambini", "Juliana", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1701.00295", "submitter": "Denis Tome", "authors": "Denis Tome, Chris Russell and Lourdes Agapito", "title": "Lifting from the Deep: Convolutional 3D Pose Estimation from a Single\n  Image", "comments": "Paper presented at CVPR 17", "journal-ref": null, "doi": "10.1109/CVPR.2017.603", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified formulation for the problem of 3D human pose estimation\nfrom a single raw RGB image that reasons jointly about 2D joint estimation and\n3D pose reconstruction to improve both tasks. We take an integrated approach\nthat fuses probabilistic knowledge of 3D human pose with a multi-stage CNN\narchitecture and uses the knowledge of plausible 3D landmark locations to\nrefine the search for better 2D locations. The entire process is trained\nend-to-end, is extremely efficient and obtains state- of-the-art results on\nHuman3.6M outperforming previous approaches both on 2D and 3D errors.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 22:50:51 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 13:27:06 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 15:36:04 GMT"}, {"version": "v4", "created": "Wed, 11 Oct 2017 12:51:11 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Tome", "Denis", ""], ["Russell", "Chris", ""], ["Agapito", "Lourdes", ""]]}, {"id": "1701.00326", "submitter": "Carlos Oscar Sorzano S.", "authors": "Carlos Oscar S. Sorzano and Jose Maria Carazo", "title": "Challenges ahead Electron Microscopy for Structural Biology from the\n  Image Processing point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of Direct Electron Detectors (DEDs), the resolution\nand range of macromolecules amenable to this technique has significantly\nwidened, generating a broad interest that explains the well over a dozen\nreviews in top journal in the last two years. Similarly, the number of job\noffers to lead EM groups and/or coordinate EM facilities has exploded, and FEI\n(the main microscope manufacturer for Life Sciences) has received more than 100\norders of high-end electron microscopes by summer 2016. Strategic corporate\nmovements are also happening, with very big players entering the market through\nkey acquisitions (Thermo Fisher has recently bought FEI for \\$4.2B), partly\nattracted by new Pharma interest in the field, now perceived to be in a\nposition to impact structure-based drug design. The scientific perspectives are\nindeed extremely positive but, in these moments of well-founded generalized\noptimists, we want to make a reflection on some of the hurdles ahead us, since\nthey certainly exist and they indeed limit the informational content of cryoEM\nprojects. Here we focus on image processing aspects, particularly in the\nso-called area of Single Particle Analysis, discussing some of the current\nresolution and high-throughput limiting factors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 07:34:38 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Sorzano", "Carlos Oscar S.", ""], ["Carazo", "Jose Maria", ""]]}, {"id": "1701.00338", "submitter": "Jing Liu", "authors": "Stefan Engblom, Carl Nettelblad, Jing Liu", "title": "Assessing Uncertainties in X-ray Single-particle Three-dimensional\n  reconstructions", "comments": "21 pages", "journal-ref": "Phys. Rev. E 98, 013303 (2018)", "doi": "10.1103/PhysRevE.98.013303", "report-no": null, "categories": "stat.ME cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern technology for producing extremely bright and coherent X-ray laser\npulses provides the possibility to acquire a large number of diffraction\npatterns from individual biological nanoparticles, including proteins, viruses,\nand DNA. These two-dimensional diffraction patterns can be practically\nreconstructed and retrieved down to a resolution of a few \\angstrom. In\nprinciple, a sufficiently large collection of diffraction patterns will contain\nthe required information for a full three-dimensional reconstruction of the\nbiomolecule. The computational methodology for this reconstruction task is\nstill under development and highly resolved reconstructions have not yet been\nproduced.\n  We analyze the Expansion-Maximization-Compression scheme, the current state\nof the art approach for this very challenging application, by isolating\ndifferent sources of uncertainty. Through numerical experiments on synthetic\ndata we evaluate their respective impact. We reach conclusions of relevance for\nhandling actual experimental data, as well as pointing out certain improvements\nto the underlying estimation algorithm.\n  We also introduce a practically applicable computational methodology in the\nform of bootstrap procedures for assessing reconstruction uncertainty in the\nreal data case. We evaluate the sharpness of this approach and argue that this\ntype of procedure will be critical in the near future when handling the\nincreasing amount of data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 08:58:19 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Engblom", "Stefan", ""], ["Nettelblad", "Carl", ""], ["Liu", "Jing", ""]]}, {"id": "1701.00352", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han", "title": "Weakly Supervised Semantic Segmentation using Web-Crawled Videos", "comments": "CVPR 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for weakly supervised semantic segmentation\nbased on image-level class labels only. In weakly supervised setting, it is\ncommonly observed that trained model overly focuses on discriminative parts\nrather than the entire object area. Our goal is to overcome this limitation\nwith no additional human intervention by retrieving videos relevant to target\nclass labels from web repository, and generating segmentation labels from the\nretrieved videos to simulate strong supervision for semantic segmentation.\nDuring this process, we take advantage of image classification with\ndiscriminative localization technique to reject false alarms in retrieved\nvideos and identify relevant spatio-temporal volumes within retrieved videos.\nAlthough the entire procedure does not require any additional supervision, the\nsegmentation annotations obtained from videos are sufficiently strong to learn\na model for semantic segmentation. The proposed algorithm substantially\noutperforms existing methods based on the same level of supervision and is even\nas competitive as the approaches relying on extra annotations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 10:12:03 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 03:00:28 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 11:03:34 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hong", "Seunghoon", ""], ["Yeo", "Donghun", ""], ["Kwak", "Suha", ""], ["Lee", "Honglak", ""], ["Han", "Bohyung", ""]]}, {"id": "1701.00405", "submitter": "V S R Veeravasarapu", "authors": "V S R Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan", "title": "Adversarially Tuned Scene Generation", "comments": "9 pages, accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization performance of trained computer vision systems that use\ncomputer graphics (CG) generated data is not yet effective due to the concept\nof 'domain-shift' between virtual and real data. Although simulated data\naugmented with a few real world samples has been shown to mitigate domain shift\nand improve transferability of trained models, guiding or bootstrapping the\nvirtual data generation with the distributions learnt from target real world\ndomain is desired, especially in the fields where annotating even few real\nimages is laborious (such as semantic labeling, and intrinsic images etc.). In\norder to address this problem in an unsupervised manner, our work combines\nrecent advances in CG (which aims to generate stochastic scene layouts coupled\nwith large collections of 3D object models) and generative adversarial training\n(which aims train generative models by measuring discrepancy between generated\nand real data in terms of their separability in the space of a deep\ndiscriminatively-trained classifier). Our method uses iterative estimation of\nthe posterior density of prior distributions for a generative graphical model.\nThis is done within a rejection sampling framework. Initially, we assume\nuniform distributions as priors on the parameters of a scene described by a\ngenerative graphical model. As iterations proceed the prior distributions get\nupdated to distributions that are closer to the (unknown) distributions of\ntarget data. We demonstrate the utility of adversarially tuned scene generation\non two real-world benchmark datasets (CityScapes and CamVid) for traffic scene\nsemantic labeling with a deep convolutional net (DeepLab). We realized\nperformance improvements by 2.28 and 3.14 points (using the IoU metric) between\nthe DeepLab models trained on simulated sets prepared from the scene generation\nmodels before and after tuning to CityScapes and CamVid respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:36:20 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 14:28:40 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Veeravasarapu", "V S R", ""], ["Rothkopf", "Constantin", ""], ["Visvanathan", "Ramesh", ""]]}, {"id": "1701.00449", "submitter": "Hamid Tizhoosh", "authors": "Morteza Babaie, H.R. Tizhoosh, Shujin Zhu, M.E. Shiri", "title": "Retrieving Similar X-Ray Images from Big Image Data Using Radon Barcodes\n  with Single Projections", "comments": "Accepted for publication in ICPRAM 2017: The International Conference\n  on Pattern Recognition Applications and Methods, Porto, Portugal, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of Radon barcodes (RBC) has been introduced recently. In this paper,\nwe propose a content-based image retrieval approach for big datasets based on\nRadon barcodes. Our method (Single Projection Radon Barcode, or SP-RBC) uses\nonly a few Radon single projections for each image as global features that can\nserve as a basis for weak learners. This is our most important contribution in\nthis work, which improves the results of the RBC considerably. As a matter of\nfact, only one projection of an image, as short as a single SURF feature\nvector, can already achieve acceptable results. Nevertheless, using multiple\nprojections in a long vector will not deliver anticipated improvements. To\nexploit the information inherent in each projection, our method uses the\noutcome of each projection separately and then applies more precise local\nsearch on the small subset of retrieved images. We have tested our method using\nIRMA 2009 dataset a with 14,400 x-ray images as part of imageCLEF initiative.\nOur approach leads to a substantial decrease in the error rate in comparison\nwith other non-learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 17:00:53 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""], ["Zhu", "Shujin", ""], ["Shiri", "M. E.", ""]]}, {"id": "1701.00458", "submitter": "Guillermo Cabrera", "authors": "Guillermo Cabrera-Vives, Ignacio Reyes, Francisco F\\\"orster, Pablo A.\n  Est\\'evez and Juan-Carlos Maureira", "title": "Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient\n  Detection", "comments": null, "journal-ref": "The Astrophysical Journal, 2017", "doi": "10.3847/1538-4357/836/1/97", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep-HiTS, a rotation invariant convolutional neural network\n(CNN) model for classifying images of transients candidates into artifacts or\nreal sources for the High cadence Transient Survey (HiTS). CNNs have the\nadvantage of learning the features automatically from the data while achieving\nhigh performance. We compare our CNN model against a feature engineering\napproach using random forests (RF). We show that our CNN significantly\noutperforms the RF model reducing the error by almost half. Furthermore, for a\nfixed number of approximately 2,000 allowed false transient candidates per\nnight we are able to reduce the miss-classified real transients by\napproximately 1/5. To the best of our knowledge, this is the first time CNNs\nhave been used to detect astronomical transient events. Our approach will be\nvery useful when processing images from next generation instruments such as the\nLarge Synoptic Survey Telescope (LSST). We have made all our code and data\navailable to the community for the sake of allowing further developments and\ncomparisons at https://github.com/guille-c/Deep-HiTS.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 17:18:09 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Cabrera-Vives", "Guillermo", ""], ["Reyes", "Ignacio", ""], ["F\u00f6rster", "Francisco", ""], ["Est\u00e9vez", "Pablo A.", ""], ["Maureira", "Juan-Carlos", ""]]}, {"id": "1701.00485", "submitter": "Wenjia Meng", "authors": "Wenjia Meng, Zonghua Gu, Ming Zhang, Zhaohui Wu", "title": "Two-Bit Networks for Deep Learning on Resource-Constrained Embedded\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 04:28:16 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 13:54:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Meng", "Wenjia", ""], ["Gu", "Zonghua", ""], ["Zhang", "Ming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1701.00495", "submitter": "Ariel Ephrat", "authors": "Ariel Ephrat and Shmuel Peleg", "title": "Vid2speech: Speech Reconstruction from Silent Video", "comments": "Accepted for publication at ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speechreading is a notoriously difficult task for humans to perform. In this\npaper we present an end-to-end model based on a convolutional neural network\n(CNN) for generating an intelligible acoustic speech signal from silent video\nframes of a speaking person. The proposed CNN generates sound features for each\nframe based on its neighboring frames. Waveforms are then synthesized from the\nlearned speech features to produce intelligible speech. We show that by\nleveraging the automatic feature learning capabilities of a CNN, we can obtain\nstate-of-the-art word intelligibility on the GRID dataset, and show promising\nresults for learning out-of-vocabulary (OOV) words.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 19:00:22 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 17:35:17 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Ephrat", "Ariel", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1701.00561", "submitter": "Hanxi Li", "authors": "Xinyu Wang, Hanxi Li, Yi Li, Fumin Shen, Fatih Porikli", "title": "Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is a fundamental problem in computer vision. Recently, some\ndeep-learning-based tracking algorithms have been achieving record-breaking\nperformances. However, due to the high complexity of deep learning, most deep\ntrackers suffer from low tracking speed, and thus are impractical in many\nreal-world applications. Some new deep trackers with smaller network structure\nachieve high efficiency while at the cost of significant decrease on precision.\nIn this paper, we propose to transfer the feature for image classification to\nthe visual tracking domain via convolutional channel reductions. The channel\nreduction could be simply viewed as an additional convolutional layer with the\nspecific task. It not only extracts useful information for object tracking but\nalso significantly increases the tracking speed. To better accommodate the\nuseful feature of the target in different scales, the adaptation filters are\ndesigned with different sizes. The yielded visual tracker is real-time and also\nillustrates the state-of-the-art accuracies in the experiment involving two\nwell-adopted benchmarks with more than 100 test videos.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 01:10:51 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Wang", "Xinyu", ""], ["Li", "Hanxi", ""], ["Li", "Yi", ""], ["Shen", "Fumin", ""], ["Porikli", "Fatih", ""]]}, {"id": "1701.00599", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Michael Gygli, Luc Van Gool", "title": "AENet: Learning Deep Audio Features for Video Analysis", "comments": "12 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1604.07160", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep network for audio event recognition, called AENet. In\ncontrast to speech, sounds coming from audio events may be produced by a wide\nvariety of sources. Furthermore, distinguishing them often requires analyzing\nan extended time period due to the lack of clear sub-word units that are\npresent in speech. In order to incorporate this long-time frequency structure\nof audio events, we introduce a convolutional neural network (CNN) operating on\na large temporal input. In contrast to previous works this allows us to train\nan audio event detection system end-to-end. The combination of our network\narchitecture and a novel data augmentation outperforms previous methods for\naudio event detection by 16%. Furthermore, we perform transfer learning and\nshow that our model learnt generic audio features, similar to the way CNNs\nlearn generic features on vision tasks. In video analysis, combining visual\nfeatures and traditional audio features such as MFCC typically only leads to\nmarginal improvements. Instead, combining visual features with our AENet\nfeatures, which can be computed efficiently on a GPU, leads to significant\nperformance improvements on action recognition and video highlight detection.\nIn video highlight detection, our audio features improve the performance by\nmore than 8% over visual features alone.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 07:35:54 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 04:07:11 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Takahashi", "Naoya", ""], ["Gygli", "Michael", ""], ["Van Gool", "Luc", ""]]}, {"id": "1701.00669", "submitter": "Matthias Vestner", "authors": "Matthias Vestner, Roee Litman, Emanuele Rodol\\`a, Alex Bronstein,\n  Daniel Cremers", "title": "Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel\n  Density Estimation in the Product Space", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for the computation of correspondences between deformable\nshapes rely on some variant of nearest neighbor matching in a descriptor space.\nSuch are, for example, various point-wise correspondence recovery algorithms\nused as a post-processing stage in the functional correspondence framework.\nSuch frequently used techniques implicitly make restrictive assumptions (e.g.,\nnear-isometry) on the considered shapes and in practice suffer from lack of\naccuracy and result in poor surjectivity. We propose an alternative recovery\ntechnique capable of guaranteeing a bijective correspondence and producing\nsignificantly higher accuracy and smoothness. Unlike other methods our approach\ndoes not depend on the assumption that the analyzed shapes are isometric. We\nderive the proposed method from the statistical framework of kernel density\nestimation and demonstrate its performance on several challenging deformable 3D\nshape matching datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:43:44 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 11:40:41 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Vestner", "Matthias", ""], ["Litman", "Roee", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex", ""], ["Cremers", "Daniel", ""]]}, {"id": "1701.00694", "submitter": "Ming Yan", "authors": "Xiaolin Huang and Yan Xia and Lei Shi and Yixing Huang and Ming Yan\n  and Joachim Hornegger and Andreas Maier", "title": "Mixed one-bit compressive sensing with applications to overexposure\n  correction for CT reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a measurement falls outside the quantization or measurable range, it\nbecomes saturated and cannot be used in classical reconstruction methods. For\nexample, in C-arm angiography systems, which provide projection radiography,\nfluoroscopy, digital subtraction angiography, and are widely used for medical\ndiagnoses and interventions, the limited dynamic range of C-arm flat detectors\nleads to overexposure in some projections during an acquisition, such as\nimaging relatively thin body parts (e.g., the knee). Aiming at overexposure\ncorrection for computed tomography (CT) reconstruction, we in this paper\npropose a mixed one-bit compressive sensing (M1bit-CS) to acquire information\nfrom both regular and saturated measurements. This method is inspired by the\nrecent progress on one-bit compressive sensing, which deals with only sign\nobservations. Its successful applications imply that information carried by\nsaturated measurements is useful to improve recovery quality. For the proposed\nM1bit-CS model, alternating direction methods of multipliers is developed and\nan iterative saturation detection scheme is established. Then we evaluate\nM1bit-CS on one-dimensional signal recovery tasks. In some experiments, the\nperformance of the proposed algorithms on mixed measurements is almost the same\nas recovery on unsaturated ones with the same amount of measurements. Finally,\nwe apply the proposed method to overexposure correction for CT reconstruction\non a phantom and a simulated clinical image. The results are promising, as the\ntypical streaking artifacts and capping artifacts introduced by saturated\nprojection data are effectively reduced, yielding significant error reduction\ncompared with existing algorithms based on extrapolation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 14:35:33 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Huang", "Xiaolin", ""], ["Xia", "Yan", ""], ["Shi", "Lei", ""], ["Huang", "Yixing", ""], ["Yan", "Ming", ""], ["Hornegger", "Joachim", ""], ["Maier", "Andreas", ""]]}, {"id": "1701.00723", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang", "title": "Image denoising using group sparsity residual and external nonlocal\n  self-similarity prior", "comments": "arXiv admin note: text overlap with arXiv:1609.03302", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonlocal image representation has been successfully used in many\nimage-related inverse problems including denoising, deblurring and deblocking.\nHowever, a majority of reconstruction methods only exploit the nonlocal\nself-similarity (NSS) prior of the degraded observation image, it is very\nchallenging to reconstruct the latent clean image. In this paper we propose a\nnovel model for image denoising via group sparsity residual and external NSS\nprior. To boost the performance of image denoising, the concept of group\nsparsity residual is proposed, and thus the problem of image denoising is\ntransformed into one that reduces the group sparsity residual. Due to the fact\nthat the groups contain a large amount of NSS information of natural images, we\nobtain a good estimation of the group sparse coefficients of the original image\nby the external NSS prior based on Gaussian Mixture model (GMM) learning and\nthe group sparse coefficients of noisy image is used to approximate the\nestimation. Experimental results have demonstrated that the proposed method not\nonly outperforms many state-of-the-art methods, but also delivers the best\nqualitative denoising results with finer details and less ringing artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 15:32:16 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Zhang", "Xinggan", ""], ["Wang", "Qiong", ""], ["Bai", "Yechao", ""], ["Tang", "Lan", ""]]}, {"id": "1701.00794", "submitter": "Zhipeng Jia", "authors": "Zhipeng Jia, Xingyi Huang, Eric I-Chao Chang, Yan Xu", "title": "Constrained Deep Weak Supervision for Histopathology Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2017.2724070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new weakly-supervised learning algorithm to learn\nto segment cancerous regions in histopathology images. Our work is under a\nmultiple instance learning framework (MIL) with a new formulation, deep weak\nsupervision (DWS); we also propose an effective way to introduce constraints to\nour neural networks to assist the learning process. The contributions of our\nalgorithm are threefold: (1) We build an end-to-end learning system that\nsegments cancerous regions with fully convolutional networks (FCN) in which\nimage-to-image weakly-supervised learning is performed. (2) We develop a deep\nweek supervision formulation to exploit multi-scale learning under weak\nsupervision within fully convolutional networks. (3) Constraints about positive\ninstances are introduced in our approach to effectively explore additional\nweakly-supervised information that is easy to obtain and enjoys a significant\nboost to the learning process. The proposed algorithm, abbreviated as DWS-MIL,\nis easy to implement and can be trained efficiently. Our system demonstrates\nstate-of-the-art results on large-scale histopathology image datasets and can\nbe applied to various applications in medical imaging beyond histopathology\nimages such as MRI, CT, and ultrasound images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 19:29:41 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Jia", "Zhipeng", ""], ["Huang", "Xingyi", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""]]}, {"id": "1701.00804", "submitter": "Yuki Itoh", "authors": "Yuki Itoh, Siwei Feng, Marco F. Duarte and Mario Parente", "title": "Semi-Supervised Endmember Identification In Nonlinear Spectral Mixtures\n  Via Semantic Representation", "comments": "15 pages, 11 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TGRS.2017.2667226", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new hyperspectral unmixing method for nonlinearly mixed\nhyperspectral data using a semantic representation in a semi-supervised\nfashion, assuming the availability of a spectral reference library. Existing\nsemi-supervised unmixing algorithms select members from an endmember library\nthat are present at each of the pixels; most such methods assume a linear\nmixing model. However, those methods will fail in the presence of nonlinear\nmixing among the observed spectra. To address this issue, we develop an\nendmember selection method using a recently proposed semantic spectral\nrepresentation obtained via non-homogeneous hidden Markov chain (NHMC) model\nfor a wavelet transform of the spectra. The semantic representation can encode\nspectrally discriminative features for any observed spectrum and, therefore,\nour proposed method can perform endmember selection without any assumption on\nthe mixing model. Experimental results show that in the presence of\nsufficiently nonlinear mixing our proposed method outperforms dictionary-based\nsparse unmixing approaches based on linear models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 19:59:15 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Itoh", "Yuki", ""], ["Feng", "Siwei", ""], ["Duarte", "Marco F.", ""], ["Parente", "Mario", ""]]}, {"id": "1701.00823", "submitter": "Ding Liu", "authors": "Ding Liu, Zhaowen Wang, Nasser Nasrabadi, Thomas Huang", "title": "Learning a Mixture of Deep Networks for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SR) is an ill-posed problem which aims to\nrecover high-resolution (HR) images from their low-resolution (LR)\nobservations. The crux of this problem lies in learning the complex mapping\nbetween low-resolution patches and the corresponding high-resolution patches.\nPrior arts have used either a mixture of simple regression models or a single\nnon-linear neural network for this propose. This paper proposes the method of\nlearning a mixture of SR inference modules in a unified framework to tackle\nthis problem. Specifically, a number of SR inference modules specialized in\ndifferent image local patterns are first independently applied on the LR image\nto obtain various HR estimates, and the resultant HR estimates are adaptively\naggregated to form the final HR image. By selecting neural networks as the SR\ninference module, the whole procedure can be incorporated into a unified\nnetwork and be optimized jointly. Extensive experiments are conducted to\ninvestigate the relation between restoration performance and different network\narchitectures. Compared with other current image SR approaches, our proposed\nmethod achieves state-of-the-arts restoration results on a wide range of images\nconsistently while allowing more flexible design choices. The source codes are\navailable in http://www.ifp.illinois.edu/~dingliu2/accv2016.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 20:41:46 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Liu", "Ding", ""], ["Wang", "Zhaowen", ""], ["Nasrabadi", "Nasser", ""], ["Huang", "Thomas", ""]]}, {"id": "1701.00892", "submitter": "Wenji Li", "authors": "Zhun Fan, Jiewei Lu, Wenji Li, Caimin Wei, Han Huang, Xinye Cai,\n  Xinjian Chen", "title": "A Hierarchical Image Matting Model for Blood Vessel Segmentation in\n  Fundus images", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a hierarchical image matting model is proposed to extract\nblood vessels from fundus images. More specifically, a hierarchical strategy\nutilizing the continuity and extendibility of retinal blood vessels is\nintegrated into the image matting model for blood vessel segmentation. Normally\nthe matting models require the user specified trimap, which separates the input\nimage into three regions manually: the foreground, background and unknown\nregions. However, since creating a user specified trimap is a tedious and\ntime-consuming task, region features of blood vessels are used to generate the\ntrimap automatically in this paper. The proposed model has low computational\ncomplexity and outperforms many other state-ofart supervised and unsupervised\nmethods in terms of accuracy, which achieves a vessel segmentation accuracy of\n96:0%, 95:7% and 95:1% in an average time of 10:72s, 15:74s and 50:71s on\nimages from three publicly available fundus image datasets DRIVE, STARE, and\nCHASE DB1, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 03:24:29 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 13:39:58 GMT"}, {"version": "v3", "created": "Mon, 9 Oct 2017 14:52:55 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Fan", "Zhun", ""], ["Lu", "Jiewei", ""], ["Li", "Wenji", ""], ["Wei", "Caimin", ""], ["Huang", "Han", ""], ["Cai", "Xinye", ""], ["Chen", "Xinjian", ""]]}, {"id": "1701.00939", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory is Robust to Adversarial Inputs", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 12, December 2018 p.3151-3167", "doi": "10.1162/neco_a_01143", "report-no": null, "categories": "cs.LG cs.CR cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 09:40:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1701.00951", "submitter": "Wei Lian", "authors": "Wei Lian, Lei Zhang", "title": "A Concave Optimization Algorithm for Matching Partially Overlapping\n  Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point matching refers to the process of finding spatial transformation and\ncorrespondences between two sets of points. In this paper, we focus on the case\nthat there is only partial overlap between two point sets. Following the\napproach of the robust point matching method, we model point matching as a\nmixed linear assignment-least square problem and show that after eliminating\nthe transformation variable, the resulting problem of minimization with respect\nto point correspondence is a concave optimization problem. Furthermore, this\nproblem has the property that the objective function can be converted into a\nform with few nonlinear terms via a linear transformation. Based on these\nproperties, we employ the branch-and-bound (BnB) algorithm to optimize the\nresulting problem where the dimension of the search space is small. To further\nimprove efficiency of the BnB algorithm where computation of the lower bound is\nthe bottleneck, we propose a new lower bounding scheme which has a\nk-cardinality linear assignment formulation and can be efficiently solved.\nExperimental results show that the proposed algorithm outperforms\nstate-of-the-art methods in terms of robustness to disturbances and point\nmatching accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 10:28:10 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Lian", "Wei", ""], ["Zhang", "Lei", ""]]}, {"id": "1701.00995", "submitter": "Michal Balazia", "authors": "Michal Balazia and Petr Sojka", "title": "An Evaluation Framework and Database for MoCap-Based Gait Recognition\n  Methods", "comments": "Preprint. Full paper published at the 1st IAPR Workshop on\n  Proceedings of Reproducible Research in Pattern Recognition, RRPR 2016,\n  Cancun, Mexico, Dec 2016. Springer, LNCS, Volume 10214, pp. 33-47, 2017. DOI:\n  10.1007/978-3-319-56414-2_3. arXiv admin note: text overlap with\n  arXiv:1609.06936, arXiv:1609.04392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a contribution to reproducible research, this paper presents a framework\nand a database to improve the development, evaluation and comparison of methods\nfor gait recognition from motion capture (MoCap) data. The evaluation framework\nprovides implementation details and source codes of state-of-the-art\nhuman-interpretable geometric features as well as our own approaches where gait\nfeatures are learned by a modification of Fisher's Linear Discriminant Analysis\nwith the Maximum Margin Criterion, and by a combination of Principal Component\nAnalysis and Linear Discriminant Analysis. It includes a description and source\ncodes of a mechanism for evaluating four class separability coefficients of\nfeature space and four rank-based classifier performance metrics. This\nframework also contains a tool for learning a custom classifier and for\nclassifying a custom query on a custom gallery. We provide an experimental\ndatabase along with source codes for its extraction from the general CMU MoCap\ndatabase.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:17:49 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 23:54:03 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 11:38:41 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Balazia", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "1701.01035", "submitter": "Wei Lian", "authors": "Wei Lian", "title": "Path-following based Point Matching using Similarity Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of 3D point matching where the poses of two point sets\nare unknown, we adapt a recently proposed path following based method to use\nsimilarity transformation instead of the original affine transformation. The\nreduced number of transformation parameters leads to more constrained and\ndesirable matching results. Experimental results demonstrate better robustness\nof the proposed method over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:53:07 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Lian", "Wei", ""]]}, {"id": "1701.01036", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jiaying Liu and Xiaodi Hou", "title": "Demystifying Neural Style Transfer", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer has recently demonstrated very exciting results which\ncatches eyes in both academia and industry. Despite the amazing results, the\nprinciple of neural style transfer, especially why the Gram matrices could\nrepresent style remains unclear. In this paper, we propose a novel\ninterpretation of neural style transfer by treating it as a domain adaptation\nproblem. Specifically, we theoretically show that matching the Gram matrices of\nfeature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with\nthe second order polynomial kernel. Thus, we argue that the essence of neural\nstyle transfer is to match the feature distributions between the style images\nand the generated images. To further support our standpoint, we experiment with\nseveral other distribution alignment methods, and achieve appealing results. We\nbelieve this novel interpretation connects these two important research fields,\nand could enlighten future researches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:54:20 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 13:21:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1701.01077", "submitter": "Monit Shah Singh", "authors": "Monit Shah Singh, Vinaychandran Pondenkandath, Bo Zhou, Paul Lukowicz\n  and Marcus Liwicki", "title": "Transforming Sensor Data to the Image Domain for Deep Learning - an\n  Application to Footstep Detection", "comments": "8 pages, 8 figures, Published in IJCNN, 2017 Copyright: IEEE 2017\n  DOI: 10.1109/IJCNN.2017.7966182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the state-of-the-art in\nvarious computer vision tasks, but they are still premature for most sensor\ndata, especially in pervasive and wearable computing. A major reason for this\nis the limited amount of annotated training data. In this paper, we propose the\nidea of leveraging the discriminative power of pre-trained deep CNNs on\n2-dimensional sensor data by transforming the sensor modality to the visual\ndomain. By three proposed strategies, 2D sensor output is converted into\npressure distribution imageries. Then we utilize a pre-trained CNN for transfer\nlearning on the converted imagery data. We evaluate our method on a gait\ndataset of floor surface pressure mapping. We obtain a classification accuracy\nof 87.66%, which outperforms the conventional machine learning methods by over\n10%.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 17:12:43 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 01:10:23 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 18:07:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Singh", "Monit Shah", ""], ["Pondenkandath", "Vinaychandran", ""], ["Zhou", "Bo", ""], ["Lukowicz", "Paul", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1701.01081", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E.\n  O'Connor, Jordi Torres, Elisa Sayrol and Xavier Giro-i-Nieto", "title": "SalGAN: Visual Saliency Prediction with Generative Adversarial Networks", "comments": "Submitted for review to Computer Vision and Image Understanding\n  (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SalGAN, a deep convolutional neural network for visual saliency\nprediction trained with adversarial examples. The first stage of the network\nconsists of a generator model whose weights are learned by back-propagation\ncomputed from a binary cross entropy (BCE) loss over downsampled versions of\nthe saliency maps. The resulting prediction is processed by a discriminator\nnetwork trained to solve a binary classification task between the saliency maps\ngenerated by the generative stage and the ground truth ones. Our experiments\nshow how adversarial training allows reaching state-of-the-art performance\nacross different metrics when combined with a widely-used loss function like\nBCE. Our results can be reproduced with the source code and trained models\navailable at https://imatge-upc.github.io/saliency-salgan-2017/.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 17:16:54 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 16:14:24 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 18:52:31 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Pan", "Junting", ""], ["Ferrer", "Cristian Canton", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel E.", ""], ["Torres", "Jordi", ""], ["Sayrol", "Elisa", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1701.01142", "submitter": "Anastasios Karakostas", "authors": "Anastasios Karakostas, Alexia Briassouli, Konstantinos Avgerinakis,\n  Ioannis Kompatsiaris, Magda Tsolaki", "title": "The Dem@Care Experiments and Datasets: a Technical Report", "comments": "4pages 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Dem@Care is the development of a complete system providing\npersonal health services to people with dementia, as well as medical\nprofessionals and caregivers, by using a multitude of sensors, for\ncontext-aware, multi-parametric monitoring of lifestyle, ambient environment,\nand health parameters. Multi-sensor data analysis, combined with intelligent\ndecision making mechanisms, will allow an accurate representation of the\nperson's current status and will provide the appropriate feedback, both to the\nperson and the associated caregivers, enhancing the standard clinical workflow.\nWithin the project framework, several data collection activities have taken\nplace to assist technical development and evaluation tasks. In all these\nactivities, particular attention has been paid to adhere to ethical guidelines\nand preserve the participants' privacy. This technical report describes shorty\nthe (a) the main objectives of the project, (b) the main ethical principles and\n(c) the datasets that have been already created.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 19:43:18 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Karakostas", "Anastasios", ""], ["Briassouli", "Alexia", ""], ["Avgerinakis", "Konstantinos", ""], ["Kompatsiaris", "Ioannis", ""], ["Tsolaki", "Magda", ""]]}, {"id": "1701.01218", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny and Ahmed Elgammal", "title": "Overlapping Cover Local Regression Machines", "comments": "Long Article with more experiments and analysis of conference paper\n  \"Overlapping Domain Cover for Scalable and Accurate Regression Kernel\n  Machines\", presented orally 2015 at the British Machine Vision Conference\n  2015 (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Overlapping Domain Cover (ODC) notion for kernel machines, as\na set of overlapping subsets of the data that covers the entire training set\nand optimized to be spatially cohesive as possible. We show how this notion\nbenefit the speed of local kernel machines for regression in terms of both\nspeed while achieving while minimizing the prediction error. We propose an\nefficient ODC framework, which is applicable to various regression models and\nin particular reduces the complexity of Twin Gaussian Processes (TGP)\nregression from cubic to quadratic. Our notion is also applicable to several\nkernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as\nshown in our experiments). We also theoretically justified the idea behind our\nmethod to improve local prediction by the overlapping cover. We validated and\nanalyzed our method on three benchmark human pose estimation datasets and\ninteresting findings are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 06:04:53 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1701.01272", "submitter": "Weishan Dong", "authors": "Weishan Dong, Ting Yuan, Kai Yang, Changsheng Li, Shilei Zhang", "title": "Autoencoder Regularized Network For Driving Style Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study learning generalized driving style representations\nfrom automobile GPS trip data. We propose a novel Autoencoder Regularized deep\nneural Network (ARNet) and a trip encoding framework trip2vec to learn drivers'\ndriving styles directly from GPS records, by combining supervised and\nunsupervised feature learning in a unified architecture. Experiments on a\nchallenging driver number estimation problem and the driver identification\nproblem show that ARNet can learn a good generalized driving style\nrepresentation: It significantly outperforms existing methods and alternative\narchitectures by reaching the least estimation error on average (0.68, less\nthan one driver) and the highest identification accuracy (by at least 3%\nimprovement) compared with traditional supervised learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 10:38:07 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Dong", "Weishan", ""], ["Yuan", "Ting", ""], ["Yang", "Kai", ""], ["Li", "Changsheng", ""], ["Zhang", "Shilei", ""]]}, {"id": "1701.01370", "submitter": "G\\\"ul Varol", "authors": "G\\\"ul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J.\n  Black, Ivan Laptev, Cordelia Schmid", "title": "Learning from Synthetic Humans", "comments": "Appears in: 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017). 9 pages", "journal-ref": null, "doi": "10.1109/CVPR.2017.492", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating human pose, shape, and motion from images and videos are\nfundamental challenges with many applications. Recent advances in 2D human pose\nestimation use large amounts of manually-labeled training data for learning\nconvolutional neural networks (CNNs). Such data is time consuming to acquire\nand difficult to extend. Moreover, manual labeling of 3D pose, depth and motion\nis impractical. In this work we present SURREAL (Synthetic hUmans foR REAL\ntasks): a new large-scale dataset with synthetically-generated but realistic\nimages of people rendered from 3D sequences of human motion capture data. We\ngenerate more than 6 million frames together with ground truth pose, depth\nmaps, and segmentation masks. We show that CNNs trained on our synthetic\ndataset allow for accurate human depth estimation and human part segmentation\nin real RGB images. Our results and the new dataset open up new possibilities\nfor advancing person analysis using cheap and large-scale synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 16:27:46 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 14:24:17 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 12:34:53 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Varol", "G\u00fcl", ""], ["Romero", "Javier", ""], ["Martin", "Xavier", ""], ["Mahmood", "Naureen", ""], ["Black", "Michael J.", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1701.01480", "submitter": "Yi-Ling Chen", "authors": "Yi-Ling Chen, Tzu-Wei Huang, Kai-Han Chang, Yu-Chen Tsai, Hwann-Tzong\n  Chen, Bing-Yu Chen", "title": "Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset\n  and Comparative Study", "comments": "The dataset presented in this article can be found on <a\n  href=\"https://github.com/yiling-chen/flickr-cropping-dataset\">Github</a>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic photo cropping is an important tool for improving visual quality of\ndigital photos without resorting to tedious manual selection. Traditionally,\nphoto cropping is accomplished by determining the best proposal window through\nvisual quality assessment or saliency detection. In essence, the performance of\nan image cropper highly depends on the ability to correctly rank a number of\nvisually similar proposal windows. Despite the ranking nature of automatic\nphoto cropping, little attention has been paid to learning-to-rank algorithms\nin tackling such a problem. In this work, we conduct an extensive study on\ntraditional approaches as well as ranking-based croppers trained on various\nimage features. In addition, a new dataset consisting of high quality cropping\nand pairwise ranking annotations is presented to evaluate the performance of\nvarious baselines. The experimental results on the new dataset provide useful\ninsights into the design of better photo cropping algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 21:22:22 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Chen", "Yi-Ling", ""], ["Huang", "Tzu-Wei", ""], ["Chang", "Kai-Han", ""], ["Tsai", "Yu-Chen", ""], ["Chen", "Hwann-Tzong", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1701.01486", "submitter": "Mehdi Noroozi", "authors": "Mehdi Noroozi, Paramanand Chandramouli, Paolo Favaro", "title": "Motion Deblurring in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of image deblurring is a very ill-posed problem as both the image\nand the blur are unknown. Moreover, when pictures are taken in the wild, this\ntask becomes even more challenging due to the blur varying spatially and the\nocclusions between the object. Due to the complexity of the general image model\nwe propose a novel convolutional network architecture which directly generates\nthe sharp image.This network is built in three stages, and exploits the\nbenefits of pyramid schemes often used in blind deconvolution. One of the main\ndifficulties in training such a network is to design a suitable dataset. While\nuseful data can be obtained by synthetically blurring a collection of images,\nmore realistic data must be collected in the wild. To obtain such data we use a\nhigh frame rate video camera and keep one frame as the sharp image and frame\naverage as the corresponding blurred image. We show that this realistic dataset\nis key in achieving state-of-the-art performance and dealing with occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 21:39:03 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 13:51:31 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Noroozi", "Mehdi", ""], ["Chandramouli", "Paramanand", ""], ["Favaro", "Paolo", ""]]}, {"id": "1701.01546", "submitter": "Yong Shean Chong", "authors": "Yong Shean Chong and Yong Haur Tay", "title": "Abnormal Event Detection in Videos using Spatiotemporal Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient method for detecting anomalies in videos. Recent\napplications of convolutional neural networks have shown promises of\nconvolutional layers for object detection and recognition, especially in\nimages. However, convolutional neural networks are supervised and require\nlabels as learning signals. We propose a spatiotemporal architecture for\nanomaly detection in videos including crowded scenes. Our architecture includes\ntwo main components, one for spatial feature representation, and one for\nlearning the temporal evolution of the spatial features. Experimental results\non Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of\nour method is comparable to state-of-the-art methods at a considerable speed of\nup to 140 fps.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 05:25:50 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Chong", "Yong Shean", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1701.01573", "submitter": "Bappaditya Mandal", "authors": "Bappaditya Mandal, David Lee and Nizar Ouarti", "title": "Distinguishing Posed and Spontaneous Smiles by Facial Dynamics", "comments": "16 pages, 8 figures, ACCV 2016, Second Workshop on Spontaneous Facial\n  Behavior Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smile is one of the key elements in identifying emotions and present state of\nmind of an individual. In this work, we propose a cluster of approaches to\nclassify posed and spontaneous smiles using deep convolutional neural network\n(CNN) face features, local phase quantization (LPQ), dense optical flow and\nhistogram of gradient (HOG). Eulerian Video Magnification (EVM) is used for\nmicro-expression smile amplification along with three normalization procedures\nfor distinguishing posed and spontaneous smiles. Although the deep CNN face\nmodel is trained with large number of face images, HOG features outperforms\nthis model for overall face smile classification task. Using EVM to amplify\nmicro-expressions did not have a significant impact on classification accuracy,\nwhile the normalizing facial features improved classification accuracy. Unlike\nmany manual or semi-automatic methodologies, our approach aims to automatically\nclassify all smiles into either `spontaneous' or `posed' categories, by using\nsupport vector machines (SVM). Experimental results on large UvA-NEMO smile\ndatabase show promising results as compared to other relevant methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 08:41:01 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 02:34:05 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2017 06:00:13 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Mandal", "Bappaditya", ""], ["Lee", "David", ""], ["Ouarti", "Nizar", ""]]}, {"id": "1701.01619", "submitter": "Andreas Veit", "authors": "Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta,\n  Serge Belongie", "title": "Learning From Noisy Large-Scale Datasets With Minimal Supervision", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 12:38:57 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 01:25:42 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Veit", "Andreas", ""], ["Alldrin", "Neil", ""], ["Chechik", "Gal", ""], ["Krasin", "Ivan", ""], ["Gupta", "Abhinav", ""], ["Belongie", "Serge", ""]]}, {"id": "1701.01687", "submitter": "Tal Remez", "authors": "Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein", "title": "Deep Convolutional Denoising of Low-Light Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson distribution is used for modeling noise in photon-limited imaging.\nWhile canonical examples include relatively exotic types of sensing like\nspectral imaging or astronomy, the problem is relevant to regular photography\nnow more than ever due to the booming market for mobile cameras. Restricted\nform factor limits the amount of absorbed light, thus computational\npost-processing is called for. In this paper, we make use of the powerful\nframework of deep convolutional neural networks for Poisson denoising. We\ndemonstrate how by training the same network with images having a specific peak\nvalue, our denoiser outperforms previous state-of-the-art by a large margin\nboth visually and quantitatively. Being flexible and data-driven, our solution\nresolves the heavy ad hoc engineering used in previous methods and is an order\nof magnitude faster. We further show that by adding a reasonable prior on the\nclass of the image being processed, another significant boost in performance is\nachieved.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 16:35:54 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Remez", "Tal", ""], ["Litany", "Or", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1701.01692", "submitter": "Eshed Ohn-Bar", "authors": "Eshed Ohn-Bar and Mohan M. Trivedi", "title": "To Boost or Not to Boost? On the Limits of Boosted Trees for Object\n  Detection", "comments": "ICPR, December 2016. Added WIDER FACE test results (Fig. 5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to study the modeling limitations of the commonly employed boosted\ndecision trees classifier. Inspired by the success of large, data-hungry visual\nrecognition models (e.g. deep convolutional neural networks), this paper\nfocuses on the relationship between modeling capacity of the weak learners,\ndataset size, and dataset properties. A set of novel experiments on the Caltech\nPedestrian Detection benchmark results in the best known performance among\nnon-CNN techniques while operating at fast run-time speed. Furthermore, the\nperformance is on par with deep architectures (9.71% log-average miss rate),\nwhile using only HOG+LUV channels as features. The conclusions from this study\nare shown to generalize over different object detection domains as demonstrated\non the FDDB face detection benchmark (93.37% accuracy). Despite the impressive\nperformance, this study reveals the limited modeling capacity of the common\nboosted trees model, motivating a need for architectural changes in order to\ncompete with multi-level and very deep architectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 16:51:32 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Ohn-Bar", "Eshed", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1701.01698", "submitter": "Tal Remez", "authors": "Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein", "title": "Deep Class Aware Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand for high image quality in mobile devices brings forth\nthe need for better computational enhancement techniques, and image denoising\nin particular. At the same time, the images captured by these devices can be\ncategorized into a small set of semantic classes. However simple, this\nobservation has not been exploited in image denoising until now. In this paper,\nwe demonstrate how the reconstruction quality improves when a denoiser is aware\nof the type of content in the image. To this end, we first propose a new fully\nconvolutional deep neural network architecture which is simple yet powerful as\nit achieves state-of-the-art performance even without being class-aware. We\nfurther show that a significant boost in performance of up to $0.4$ dB PSNR can\nbe achieved by making our network class-aware, namely, by fine-tuning it for\nimages belonging to a specific semantic class. Relying on the hugely successful\nexisting image classifiers, this research advocates for using a class-aware\napproach in all image enhancement tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 17:13:42 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 20:25:59 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Remez", "Tal", ""], ["Litany", "Or", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1701.01745", "submitter": "Hao Sun", "authors": "Hao Sun, Alina Zare", "title": "Map-guided Hyperspectral Image Superpixel Segmentation Using Proportion\n  Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A map-guided superpixel segmentation method for hyperspectral imagery is\ndeveloped and introduced. The proposed approach develops a\nhyperspectral-appropriate version of the SLIC superpixel segmentation\nalgorithm, leverages map information to guide segmentation, and incorporates\nthe semi-supervised Partial Membership Latent Dirichlet Allocation (sPM-LDA) to\nobtain a final superpixel segmentation. The proposed method is applied to two\nreal hyperspectral data sets and quantitative cluster validity metrics indicate\nthat the proposed approach outperforms existing hyperspectral superpixel\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 19:52:10 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sun", "Hao", ""], ["Zare", "Alina", ""]]}, {"id": "1701.01779", "submitter": "George Papandreou", "authors": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev,\n  Jonathan Tompson, Chris Bregler, Kevin Murphy", "title": "Towards Accurate Multi-person Pose Estimation in the Wild", "comments": "Paper describing an improved version of the G-RMI entry to the 2016\n  COCO keypoints challenge (http://image-net.org/challenges/ilsvrc+coco2016).\n  Camera ready version to appear in the Proceedings of CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for multi-person detection and 2-D pose estimation that\nachieves state-of-art results on the challenging COCO keypoints task. It is a\nsimple, yet powerful, top-down approach consisting of two stages.\n  In the first stage, we predict the location and scale of boxes which are\nlikely to contain people; for this we use the Faster RCNN detector. In the\nsecond stage, we estimate the keypoints of the person potentially contained in\neach proposed bounding box. For each keypoint type we predict dense heatmaps\nand offsets using a fully convolutional ResNet. To combine these outputs we\nintroduce a novel aggregation procedure to obtain highly localized keypoint\npredictions. We also use a novel form of keypoint-based Non-Maximum-Suppression\n(NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based\nconfidence score estimation, instead of box-level scoring.\n  Trained on COCO data alone, our final system achieves average precision of\n0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming\nthe winner of the 2016 COCO keypoints challenge and other recent state-of-art.\nFurther, by using additional in-house labeled data we obtain an even higher\naverage precision of 0.685 on the test-dev set and 0.673 on the test-standard\nset, more than 5% absolute improvement compared to the previous best performing\nmethod on the same dataset.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 23:56:02 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:30:58 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Papandreou", "George", ""], ["Zhu", "Tyler", ""], ["Kanazawa", "Nori", ""], ["Toshev", "Alexander", ""], ["Tompson", "Jonathan", ""], ["Bregler", "Chris", ""], ["Murphy", "Kevin", ""]]}, {"id": "1701.01814", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Song Liu and Zhimin Gao and Chang Tang\n  and Philip Ogunbona", "title": "Large-scale Isolated Gesture Recognition Using Convolutional Neural\n  Networks", "comments": "arXiv admin note: text overlap with arXiv:1608.06338", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes three simple, compact yet effective representations of\ndepth sequences, referred to respectively as Dynamic Depth Images (DDI),\nDynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images\n(DDMNI). These dynamic images are constructed from a sequence of depth maps\nusing bidirectional rank pooling to effectively capture the spatial-temporal\ninformation. Such image-based representations enable us to fine-tune the\nexisting ConvNets models trained on image data for classification of depth\nsequences, without introducing large parameters to learn. Upon the proposed\nrepresentations, a convolutional Neural networks (ConvNets) based method is\ndeveloped for gesture recognition and evaluated on the Large-scale Isolated\nGesture Recognition at the ChaLearn Looking at People (LAP) challenge 2016. The\nmethod achieved 55.57\\% classification accuracy and ranked $2^{nd}$ place in\nthis challenge but was very close to the best performance even though we only\nused depth data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 10:31:58 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Liu", "Song", ""], ["Gao", "Zhimin", ""], ["Tang", "Chang", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1701.01821", "submitter": "Zelun Luo", "authors": "Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei", "title": "Unsupervised Learning of Long-Term Motion Dynamics for Videos", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised representation learning approach that compactly\nencodes the motion dependencies in videos. Given a pair of images from a video\nclip, our framework learns to predict the long-term 3D motions. To reduce the\ncomplexity of the learning framework, we propose to describe the motion as a\nsequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent\nNeural Network based Encoder-Decoder framework to predict these sequences of\nflows. We argue that in order for the decoder to reconstruct these sequences,\nthe encoder must learn a robust video representation that captures long-term\nmotion dependencies and spatial-temporal relations. We demonstrate the\neffectiveness of our learned temporal representations on activity\nclassification across multiple modalities and datasets such as NTU RGB+D and\nMSR Daily Activity 3D. Our framework is generic to any input modality, i.e.,\nRGB, Depth, and RGB-D videos.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 12:03:11 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 04:13:24 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 22:09:03 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Luo", "Zelun", ""], ["Peng", "Boya", ""], ["Huang", "De-An", ""], ["Alahi", "Alexandre", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1701.01833", "submitter": "Yanzhao Zhou", "authors": "Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao", "title": "Oriented Response Networks", "comments": "Accepted in CVPR 2017. Source code available at http://yzhou.work/ORN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolution Neural Networks (DCNNs) are capable of learning\nunprecedentedly effective image representations. However, their ability in\nhandling significant local and global image rotations remains limited. In this\npaper, we propose Active Rotating Filters (ARFs) that actively rotate during\nconvolution and produce feature maps with location and orientation explicitly\nencoded. An ARF acts as a virtual filter bank containing the filter itself and\nits multiple unmaterialised rotated versions. During back-propagation, an ARF\nis collectively updated using errors from all its rotated versions. DCNNs using\nARFs, referred to as Oriented Response Networks (ORNs), can produce\nwithin-class rotation-invariant deep features while maintaining inter-class\ndiscrimination for classification tasks. The oriented response produced by ORNs\ncan also be used for image and object orientation estimation tasks. Over\nmultiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we\nconsistently observe that replacing regular filters with the proposed ARFs\nleads to significant reduction in the number of network parameters and\nimprovement in classification performance. We report the best results on\nseveral commonly used benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 14:18:01 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 02:32:23 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhou", "Yanzhao", ""], ["Ye", "Qixiang", ""], ["Qiu", "Qiang", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1701.01875", "submitter": "Zeshan Hussain", "authors": "Hardie Cate, Fahim Dalvi, and Zeshan Hussain", "title": "Sign Language Recognition Using Temporal Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devices like the Myo armband available in the market today enable us to\ncollect data about the position of a user's hands and fingers over time. We can\nuse these technologies for sign language translation since each sign is roughly\na combination of gestures across time. In this work, we utilize a dataset\ncollected by a group at the University of South Wales, which contains\nparameters, such as hand position, hand rotation, and finger bend, for 95\nunique signs. For each input stream representing a sign, we predict which sign\nclass this stream falls into. We begin by implementing baseline SVM and\nlogistic regression models, which perform reasonably well on high quality data.\nLower quality data requires a more sophisticated approach, so we explore\ndifferent methods in temporal classification, including long short term memory\narchitectures and sequential pattern mining methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 20:09:52 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Cate", "Hardie", ""], ["Dalvi", "Fahim", ""], ["Hussain", "Zeshan", ""]]}, {"id": "1701.01876", "submitter": "Zeshan Hussain", "authors": "Hardie Cate, Fahim Dalvi, and Zeshan Hussain", "title": "DeepFace: Face Generation using Deep Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use CNNs to build a system that both classifies images of faces based on a\nvariety of different facial attributes and generates new faces given a set of\ndesired facial characteristics. After introducing the problem and providing\ncontext in the first section, we discuss recent work related to image\ngeneration in Section 2. In Section 3, we describe the methods used to\nfine-tune our CNN and generate new images using a novel approach inspired by a\nGaussian mixture model. In Section 4, we discuss our working dataset and\ndescribe our preprocessing steps and handling of facial attributes. Finally, in\nSections 5, 6 and 7, we explain our experiments and results and conclude in the\nfollowing section. Our classification system has 82\\% test accuracy.\nFurthermore, our generation pipeline successfully creates well-formed faces.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 20:22:05 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Cate", "Hardie", ""], ["Dalvi", "Fahim", ""], ["Hussain", "Zeshan", ""]]}, {"id": "1701.01879", "submitter": "Burak Benligiray", "authors": "Caner Gacav, Burak Benligiray, Cihan Topal", "title": "Greedy Search for Descriptive Spatial Face Features", "comments": "International Conference on Acoustics, Speech and Signal Processing\n  (ICASSP), 2017", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952406", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition methods use a combination of geometric and\nappearance-based features. Spatial features are derived from displacements of\nfacial landmarks, and carry geometric information. These features are either\nselected based on prior knowledge, or dimension-reduced from a large pool. In\nthis study, we produce a large number of potential spatial features using two\ncombinations of facial landmarks. Among these, we search for a descriptive\nsubset of features using sequential forward selection. The chosen feature\nsubset is used to classify facial expressions in the extended Cohn-Kanade\ndataset (CK+), and delivered 88.7% recognition accuracy without using any\nappearance-based features.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 20:36:18 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 20:03:32 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Gacav", "Caner", ""], ["Benligiray", "Burak", ""], ["Topal", "Cihan", ""]]}, {"id": "1701.01885", "submitter": "Zeshan Hussain", "authors": "Zeshan Hussain, Tariq Patanam, and Hardie Cate", "title": "Group Visual Sentiment Analysis", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a framework for classifying images according to\nhigh-level sentiment. We subdivide the task into three primary problems:\nemotion classification on faces, human pose estimation, and 3D estimation and\nclustering of groups of people. We introduce novel algorithms for matching body\nparts to a common individual and clustering people in images based on physical\nlocation and orientation. Our results outperform several baseline approaches.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 21:12:24 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Hussain", "Zeshan", ""], ["Patanam", "Tariq", ""], ["Cate", "Hardie", ""]]}, {"id": "1701.01892", "submitter": "Charika De Alvis Ms", "authors": "Charika De Alvis, Lionel Ott, Fabio Ramos", "title": "Urban Scene Segmentation with Laser-Constrained CRFs", "comments": "In International Conference On Intelligent Robots and Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots typically possess sensors of different modalities, such as colour\ncameras, inertial measurement units, and 3D laser scanners. Often, solving a\nparticular problem becomes easier when more than one modality is used. However,\nwhile there are undeniable benefits to combine sensors of different modalities\nthe process tends to be complicated. Segmenting scenes observed by the robot\ninto a discrete set of classes is a central requirement for autonomy as\nunderstanding the scene is the first step to reason about future situations.\nScene segmentation is commonly performed using either image data or 3D point\ncloud data. In computer vision many successful methods for scene segmentation\nare based on conditional random fields (CRF) where the maximum a posteriori\n(MAP) solution to the segmentation can be obtained by inference. In this paper\nwe devise a new CRF inference method for scene segmentation that incorporates\nglobal constraints, enforcing the sets of nodes are assigned the same class\nlabel. To do this efficiently, the CRF is formulated as a relaxed quadratic\nprogram whose MAP solution is found using a gradient-based optimisation\napproach. The proposed method is evaluated on images and 3D point cloud data\ngathered in urban environments where image data provides the appearance\nfeatures needed by the CRF, while the 3D point cloud data provides global\nspatial constraints over sets of nodes. Comparisons with belief propagation,\nconventional quadratic programming relaxation, and higher order potential CRF\nshow the benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 22:58:26 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["De Alvis", "Charika", ""], ["Ott", "Lionel", ""], ["Ramos", "Fabio", ""]]}, {"id": "1701.01909", "submitter": "Amir Sadeghian", "authors": "Amir Sadeghian, Alexandre Alahi, and Silvio Savarese", "title": "Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing solutions to the Multi-Target Tracking (MTT) problem\ndo not combine cues in a coherent end-to-end fashion over a long period of\ntime. However, we present an online method that encodes long-term temporal\ndependencies across multiple cues. One key challenge of tracking methods is to\naccurately track occluded targets or those which share similar appearance\nproperties with surrounding objects. To address this challenge, we present a\nstructure of Recurrent Neural Networks (RNN) that jointly reasons on multiple\ncues over a temporal window. We are able to correct many data association\nerrors and recover observations from an occluded state. We demonstrate the\nrobustness of our data-driven approach by tracking multiple targets using their\nappearance, motion, and even interactions. Our method outperforms previous\nworks on multiple publicly available datasets including the challenging MOT\nbenchmark.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 03:29:26 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 21:42:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Sadeghian", "Amir", ""], ["Alahi", "Alexandre", ""], ["Savarese", "Silvio", ""]]}, {"id": "1701.01911", "submitter": "Nannan Wang", "authors": "Nannan Wang and Xinbo Gao and Jie Li", "title": "Random Sampling for Fast Face Sketch Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based face sketch synthesis plays an important role in both digital\nentertainment and law enforcement. It generally consists of two parts: neighbor\nselection and reconstruction weight representation. The most time-consuming or\nmain computation complexity for exemplar-based face sketch synthesis methods\nlies in the neighbor selection process. State-of-the-art face sketch synthesis\nmethods perform neighbor selection online in a data-driven manner by $K$\nnearest neighbor ($K$-NN) searching. Actually, the online search increases the\ntime consuming for synthesis. Moreover, since these methods need to traverse\nthe whole training dataset for neighbor selection, the computational complexity\nincreases with the scale of the training database and hence these methods have\nlimited scalability. In this paper, we proposed a simple but effective offline\nrandom sampling in place of online $K$-NN search to improve the synthesis\nefficiency. Extensive experiments on public face sketch databases demonstrate\nthe superiority of the proposed method in comparison to state-of-the-art\nmethods, in terms of both synthesis quality and time consumption. The proposed\nmethod could be extended to other heterogeneous face image transformation\nproblems such as face hallucination. We release the source codes of our\nproposed methods and the evaluation metrics for future study online:\nhttp://www.ihitworld.com/RSLCR.html.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 03:47:59 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 02:43:48 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Wang", "Nannan", ""], ["Gao", "Xinbo", ""], ["Li", "Jie", ""]]}, {"id": "1701.01924", "submitter": "Yiren Zhou", "authors": "Yiren Zhou, Sibo Song, Ngai-Man Cheung", "title": "On Classification of Distorted Images with Deep Convolutional Neural\n  Networks", "comments": "5 pages, 8 figures, ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image blur and image noise are common distortions during image acquisition.\nIn this paper, we systematically study the effect of image distortions on the\ndeep neural network (DNN) image classifiers. First, we examine the DNN\nclassifier performance under four types of distortions. Second, we propose two\napproaches to alleviate the effect of image distortion: re-training and\nfine-tuning with noisy images. Our results suggest that, under certain\nconditions, fine-tuning with noisy images can alleviate much effect due to\ndistorted inputs, and is more practical than re-training.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 08:03:00 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhou", "Yiren", ""], ["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1701.01930", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi, Michael Laurence Humber, Dirk Tiede and Stefan Lang", "title": "Stage 4 validation of the Satellite Image Automatic Mapper lightweight\n  computer program for Earth observation Level 2 product generation, Part 1\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Space Agency (ESA) defines an Earth Observation (EO) Level 2\nproduct as a multispectral (MS) image corrected for geometric, atmospheric,\nadjacency and topographic effects, stacked with its scene classification map\n(SCM), whose legend includes quality layers such as cloud and cloud-shadow. No\nESA EO Level 2 product has ever been systematically generated at the ground\nsegment. To contribute toward filling an information gap from EO big data to\nthe ESA EO Level 2 product, an original Stage 4 validation (Val) of the\nSatellite Image Automatic Mapper (SIAM) lightweight computer program was\nconducted by independent means on an annual Web-Enabled Landsat Data (WELD)\nimage composite time-series of the conterminous U.S. The core of SIAM is a one\npass prior knowledge based decision tree for MS reflectance space\nhyperpolyhedralization into static color names presented in literature in\nrecent years. For the sake of readability this paper is split into two. The\npresent Part 1 Theory provides the multidisciplinary background of a priori\ncolor naming in cognitive science, from linguistics to computer vision. To cope\nwith dictionaries of MS color names and land cover class names that do not\ncoincide and must be harmonized, an original hybrid guideline is proposed to\nidentify a categorical variable pair relationship. An original quantitative\nmeasure of categorical variable pair association is also proposed. The\nsubsequent Part 2 Validation discusses Stage 4 Val results collected by an\noriginal protocol for wall-to-wall thematic map quality assessment without\nsampling where the test and reference map legends can differ. Conclusions are\nthat the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the 4\nclass taxonomy of the FAO Land Cover Classification System at the Dichotomous\nPhase Level 1 vegetation/nonvegetation and Level 2 terrestrial/aquatic.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 09:26:49 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Baraldi", "Andrea", ""], ["Humber", "Michael Laurence", ""], ["Tiede", "Dirk", ""], ["Lang", "Stefan", ""]]}, {"id": "1701.01932", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi, Michael Laurence Humber, Dirk Tiede and Stefan Lang", "title": "Stage 4 validation of the Satellite Image Automatic Mapper lightweight\n  computer program for Earth observation Level 2 product generation, Part 2\n  Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Space Agency (ESA) defines an Earth Observation (EO) Level 2\nproduct as a multispectral (MS) image corrected for geometric, atmospheric,\nadjacency and topographic effects, stacked with its scene classification map\n(SCM) whose legend includes quality layers such as cloud and cloud-shadow. No\nESA EO Level 2 product has ever been systematically generated at the ground\nsegment. To contribute toward filling an information gap from EO big sensory\ndata to the ESA EO Level 2 product, a Stage 4 validation (Val) of an off the\nshelf Satellite Image Automatic Mapper (SIAM) lightweight computer program for\nprior knowledge based MS color naming was conducted by independent means. A\ntime-series of annual Web Enabled Landsat Data (WELD) image composites of the\nconterminous U.S. (CONUS) was selected as input dataset. The annual SIAM WELD\nmaps of the CONUS were validated in comparison with the U.S. National Land\nCover Data (NLCD) 2006 map. These test and reference maps share the same\nspatial resolution and spatial extent, but their map legends are not the same\nand must be harmonized. For the sake of readability this paper is split into\ntwo. The previous Part 1 Theory provided the multidisciplinary background of a\npriori color naming. The present Part 2 Validation presents and discusses Stage\n4 Val results collected from the test SIAM WELD map time series and the\nreference NLCD map by an original protocol for wall to wall thematic map\nquality assessment without sampling, where the test and reference map legends\ncan differ in agreement with the Part 1. Conclusions are that the SIAM-WELD\nmaps instantiate a Level 2 SCM product whose legend is the FAO Land Cover\nClassification System (LCCS) taxonomy at the Dichotomous Phase (DP) Level 1\nvegetation/nonvegetation, Level 2 terrestrial/aquatic or superior LCCS level.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 09:35:30 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Baraldi", "Andrea", ""], ["Humber", "Michael Laurence", ""], ["Tiede", "Dirk", ""], ["Lang", "Stefan", ""]]}, {"id": "1701.01940", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi, Dirk Tiede, and Stefan Lang", "title": "Automated Linear-Time Detection and Quality Assessment of Superpixels in\n  Uncalibrated True- or False-Color RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capable of automated near real time superpixel detection and quality\nassessment in an uncalibrated monitor typical red green blue (RGB) image,\ndepicted in either true or false colors, an original low level computer vision\n(CV) lightweight computer program, called RGB Image Automatic Mapper (RGBIAM),\nis designed and implemented. Constrained by the Calibration Validation (CalVal)\nrequirements of the Quality Assurance Framework for Earth Observation (QA4EO)\nguidelines, RGBIAM requires as mandatory an uncalibrated RGB image pre\nprocessing first stage, consisting of an automated statistical model based\ncolor constancy algorithm. The RGBIAM hybrid inference pipeline comprises: (I)\na direct quantitative to nominal (QN) RGB variable transform, where RGB pixel\nvalues are mapped onto a prior dictionary of color names, equivalent to a\nstatic polyhedralization of the RGB cube. Prior color naming is the deductive\ncounterpart of inductive vector quantization (VQ), whose typical VQ error\nfunction to minimize is a root mean square error (RMSE). In the output multi\nlevel color map domain, superpixels are automatically detected in linear time\nas connected sets of pixels featuring the same color label. (II) An inverse\nnominal to quantitative (NQ) RGB variable transform, where a superpixelwise\nconstant RGB image approximation is generated in linear time to assess a VQ\nerror image. The hybrid direct and inverse RGBIAM QNQ transform is: (i) general\npurpose, data and application independent. (ii) Automated, i.e., it requires no\nuser machine interaction. (iii) Near real time, with a computational complexity\nincreasing linearly with the image size. (iv) Implemented in tile streaming\nmode, to cope with massive images. Collected outcome and process quality\nindicators, including degree of automation, computational efficiency, VQ rate\nand VQ error, are consistent with theoretical expectations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 11:09:59 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Baraldi", "Andrea", ""], ["Tiede", "Dirk", ""], ["Lang", "Stefan", ""]]}, {"id": "1701.01941", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi and Jo\\~ao V. B. Soares", "title": "Multi-Objective Software Suite of Two-Dimensional Shape Descriptors for\n  Object-Based Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years two sets of planar (2D) shape attributes, provided with an\nintuitive physical meaning, were proposed to the remote sensing community by,\nrespectively, Nagao & Matsuyama and Shackelford & Davis in their seminal works\non the increasingly popular geographic object based image analysis (GEOBIA)\nparadigm. These two published sets of intuitive geometric features were\nselected as initial conditions by the present R&D software project, whose\nmulti-objective goal was to accomplish: (i) a minimally dependent and maximally\ninformative design (knowledge/information representation) of a general purpose,\nuser and application independent dictionary of 2D shape terms provided with a\nphysical meaning intuitive to understand by human end users and (ii) an\neffective (accurate, scale invariant, easy to use) and efficient implementation\nof 2D shape descriptors. To comply with the Quality Assurance Framework for\nEarth Observation guidelines, the proposed suite of geometric functions is\nvalidated by means of a novel quantitative quality assurance policy, centered\non inter feature dependence (causality) assessment. This innovative\nmultivariate feature validation strategy is alternative to traditional feature\nselection procedures based on either inductive data learning classification\naccuracy estimation, which is inherently case specific, or cross correlation\nestimation, because statistical cross correlation does not imply causation. The\nproject deliverable is an original general purpose software suite of seven\nvalidated off the shelf 2D shape descriptors intuitive to use. Alternative to\nexisting commercial or open source software libraries of tens of planar shape\nfunctions whose informativeness remains unknown, it is eligible for use in\n(GE)OBIA systems in operating mode, expected to mimic human reasoning based on\na convergence of evidence approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 11:16:43 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 08:12:20 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Baraldi", "Andrea", ""], ["Soares", "Jo\u00e3o V. B.", ""]]}, {"id": "1701.01942", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi, Francesca Despini, and Sergio Teggi", "title": "Multi-spectral Image Panchromatic Sharpening, Outcome and Process\n  Quality Assessment Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral (MS) image panchromatic (PAN) sharpening algorithms proposed to\nthe remote sensing community are ever increasing in number and variety. Their\naim is to sharpen a coarse spatial resolution MS image with a fine spatial\nresolution PAN image acquired simultaneously by a spaceborne or airborne Earth\nobservation (EO) optical imaging sensor pair. Unfortunately, to date, no\nstandard evaluation procedure for MS image PAN sharpening outcome and process\nis community agreed upon, in contrast with the Quality Assurance Framework for\nEarth Observation (QA4EO) guidelines proposed by the intergovernmental Group on\nEarth Observations (GEO). In general, process is easier to measure, outcome is\nmore important. The original contribution of the present study is fourfold.\nFirst, existing procedures for quantitative quality assessment (Q2A) of the\n(sole) PAN sharpened MS product are critically reviewed. Their conceptual and\nimplementation drawbacks are highlighted to be overcome for quality\nimprovement. Second, a novel (to the best of these authors' knowledge, the\nfirst) protocol for Q2A of MS image PAN sharpening product and process is\ndesigned, implemented and validated by independent means. Third, within this\nprotocol, an innovative categorization of spectral and spatial image quality\nindicators and metrics is presented. Fourth, according to this new taxonomy, an\noriginal third order isotropic multi scale gray level co occurrence matrix\n(TIMS GLCM) calculator and a TIMS GLCM texture feature extractor are proposed\nto replace popular second order GLCMs.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 11:22:59 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Baraldi", "Andrea", ""], ["Despini", "Francesca", ""], ["Teggi", "Sergio", ""]]}, {"id": "1701.01945", "submitter": "Bernhard Schmitzer", "authors": "Bernhard Schmitzer and Benedikt Wirth", "title": "A Framework for Wasserstein-1-Type Metrics", "comments": "to appear in Journal of Convex Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unifying framework for generalising the Wasserstein-1 metric to\na discrepancy measure between nonnegative measures of different mass. This\ngeneralization inherits the convexity and computational efficiency from the\nWasserstein-1 metric, and it includes several previous approaches from the\nliterature as special cases. For various specific instances of the generalized\nWasserstein-1 metric we furthermore demonstrate their usefulness in\napplications by numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 11:31:30 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 09:24:56 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Schmitzer", "Bernhard", ""], ["Wirth", "Benedikt", ""]]}, {"id": "1701.01996", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "MS and PAN image fusion by combining Brovey and wavelet methods", "comments": "9 pages; 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the existing fusion algorithms, the wavelet fusion method is the most\nfrequently discussed one in recent publications because the wavelet approach\npreserves the spectral characteristics of the multispectral image better than\nother methods. The Brovey is also a popular fusion method used for its ability\nin preserving the spatial information of the PAN image. This study presents a\nnew fusion approach that integrates the advantages of both the Brovey (which\npreserves a high degree of spatial information) and the wavelet (which\npreserves a high degree of spectral information) techniques to reduce the\ncolour distortion of fusion results. Visual and statistical analyzes show that\nthe proposed algorithm clearly improves the merging quality in terms of:\ncorrelation coefficient and UIQI; compared to fusion methods including, IHS,\nBrovey, PCA , HPF, discrete wavelet transform (DWT), and a-trous wavelet.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 18:39:04 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1701.02096", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "Improved Texture Networks: Maximizing Quality and Diversity in\n  Feed-forward Stylization and Texture Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent work of Gatys et al., who characterized the style of an image by\nthe statistics of convolutional neural network filters, ignited a renewed\ninterest in the texture generation and image stylization problems. While their\nimage generation technique uses a slow optimization process, recently several\nauthors have proposed to learn generator neural networks that can produce\nsimilar outputs in one quick forward pass. While generator networks are\npromising, they are still inferior in visual quality and diversity compared to\ngeneration-by-optimization. In this work, we advance them in two significant\nways. First, we introduce an instance normalization module to replace batch\nnormalization with significant improvements to the quality of image\nstylization. Second, we improve diversity by introducing a new learning\nformulation that encourages generators to sample unbiasedly from the Julesz\ntexture ensemble, which is the equivalence class of all images characterized by\ncertain filter responses. Together, these two improvements take feed forward\ntexture synthesis and image stylization much closer to the quality of\ngeneration-via-optimization, while retaining the speed advantage.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 08:54:14 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 14:47:29 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1701.02123", "submitter": "Changsoo Je", "authors": "Changsoo Je, Kyuhyoung Choi, Sang Wook Lee", "title": "Green-Blue Stripe Pattern for Range Sensing from a Single Image", "comments": "7 pages, 5 figures. Updated version of a conference paper", "journal-ref": "Proc. 30th Fall Semiannual Conference of Korea Information Science\n  Society, vol. 2, pp. 661-663, Seoul, Korea, October, 2003", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for rapid high-resolution range\nsensing using green-blue stripe pattern. We use green and blue for designing\nhigh-frequency stripe projection pattern. For accurate and reliable range\nrecovery, we identify the stripe patterns by our color-stripe segmentation and\nunwrapping algorithms. The experimental result for a naked human face shows the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 10:16:11 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:31:41 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Je", "Changsoo", ""], ["Choi", "Kyuhyoung", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1701.02127", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Discrete approximations of the affine Gaussian derivative model for\n  visual receptive fields", "comments": "41 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The affine Gaussian derivative model can in several respects be regarded as a\ncanonical model for receptive fields over a spatial image domain: (i) it can be\nderived by necessity from scale-space axioms that reflect structural properties\nof the world, (ii) it constitutes an excellent model for the receptive fields\nof simple cells in the primary visual cortex and (iii) it is covariant under\naffine image deformations, which enables more accurate modelling of image\nmeasurements under the local image deformations caused by the perspective\nmapping, compared to the more commonly used Gaussian derivative model based on\nderivatives of the rotationally symmetric Gaussian kernel.\n  This paper presents a theory for discretizing the affine Gaussian scale-space\nconcept underlying the affine Gaussian derivative model, so that scale-space\nproperties hold also for the discrete implementation.\n  Two ways of discretizing spatial smoothing with affine Gaussian kernels are\npresented: (i) by solving semi-discretized affine diffusion equation, which has\nderived by necessity from the requirements of a semi-group structure over scale\nas parameterized by a family of spatial covariance matrices and obeying\nnon-creation of new structures from any finer to any coarser scale in terms of\nnon-enhancement of local extrema and (ii) approximating these semi-discrete\naffine receptive fields by parameterized families of 3x3-kernels as obtained\nfrom an additional discretization along the scale direction. The latter\ndiscrete approach can be optionally complemented by spatial subsampling at\ncoarser scales, leading to the notion of affine hybrid pyramids.\n  Using these theoretical results, we outline hybrid architectures for discrete\napproximations of affine covariant receptive field families, to be used as a\nfirst processing layer for affine covariant and affine invariant visual\noperations at higher levels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 10:42:03 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 09:46:18 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 15:20:56 GMT"}, {"version": "v4", "created": "Mon, 30 Oct 2017 14:59:02 GMT"}, {"version": "v5", "created": "Wed, 20 Dec 2017 15:39:39 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1701.02141", "submitter": "Mattia Rossi", "authors": "Mattia Rossi and Pascal Frossard", "title": "Light Field Super-Resolution Via Graph-Based Regularization", "comments": "This new version includes more material. In particular, we added: a\n  new section on the computational complexity of the proposed algorithm,\n  experimental comparisons with a CNN-based super-resolution algorithm, and new\n  experiments on a third dataset", "journal-ref": null, "doi": "10.1109/TIP.2018.2828983", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras capture the 3D information in a scene with a single\nexposure. This special feature makes light field cameras very appealing for a\nvariety of applications: from post-capture refocus, to depth estimation and\nimage-based rendering. However, light field cameras suffer by design from\nstrong limitations in their spatial resolution, which should therefore be\naugmented by computational methods. On the one hand, off-the-shelf single-frame\nand multi-frame super-resolution algorithms are not ideal for light field data,\nas they do not consider its particular structure. On the other hand, the few\nsuper-resolution algorithms explicitly tailored for light field data exhibit\nsignificant limitations, such as the need to estimate an explicit disparity map\nat each view. In this work we propose a new light field super-resolution\nalgorithm meant to address these limitations. We adopt a multi-frame alike\nsuper-resolution approach, where the complementary information in the different\nlight field views is used to augment the spatial resolution of the whole light\nfield. We show that coupling the multi-frame approach with a graph regularizer,\nthat enforces the light field structure via nonlocal self similarities, permits\nto avoid the costly and challenging disparity estimation step for all the\nviews. Extensive experiments show that the new algorithm compares favorably to\nthe other state-of-the-art methods for light field super-resolution, both in\nterms of PSNR and visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:32:30 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 17:17:15 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Rossi", "Mattia", ""], ["Frossard", "Pascal", ""]]}, {"id": "1701.02166", "submitter": "Rigas Kouskouridas", "authors": "Caner Sahin, Rigas Kouskouridas and Tae-Kyun Kim", "title": "A Learning-based Variable Size Part Extraction Architecture for 6D\n  Object Pose Recovery in Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques for 6D object pose recovery depend on\nocclusion-free point clouds to accurately register objects in 3D space. To deal\nwith this shortcoming, we introduce a novel architecture called Iterative Hough\nForest with Histogram of Control Points that is capable of estimating the 6D\npose of occluded and cluttered objects given a candidate 2D bounding box. Our\nIterative Hough Forest (IHF) is learnt using parts extracted only from the\npositive samples. These parts are represented with Histogram of Control Points\n(HoCP), a \"scale-variant\" implicit volumetric description, which we derive from\nrecently introduced Implicit B-Splines (IBS). The rich discriminative\ninformation provided by the scale-variant HoCP features is leveraged during\ninference. An automatic variable size part extraction framework iteratively\nrefines the object's initial pose that is roughly aligned due to the extraction\nof coarsest parts, the ones occupying the largest area in image pixels. The\niterative refinement is accomplished based on finer (smaller) parts that are\nrepresented with more discriminative control point descriptors by using our\nIterative Hough Forest. Experiments conducted on a publicly available dataset\nreport that our approach show better registration performance than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 13:20:32 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sahin", "Caner", ""], ["Kouskouridas", "Rigas", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1701.02258", "submitter": "Changzhe Jiao", "authors": "Changzhe Jiao, Alina Zare", "title": "Multiple Instance Hybrid Estimator for Learning Target Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signature-based detectors for hyperspectral target detection rely on knowing\nthe specific target signature in advance. However, target signature are often\ndifficult or impossible to obtain. Furthermore, common methods for obtaining\ntarget signatures, such as from laboratory measurements or manual selection\nfrom an image scene, usually do not capture the discriminative features of\ntarget class. In this paper, an approach for estimating a discriminative target\nsignature from imprecise labels is presented. The proposed approach maximizes\nthe response of the hybrid sub-pixel detector within a multiple instance\nlearning framework and estimates a set of discriminative target signatures.\nAfter learning target signatures, any signature based detector can then be\napplied on test data. Both simulated and real hyperspectral target detection\nexperiments are shown to illustrate the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 16:59:01 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Jiao", "Changzhe", ""], ["Zare", "Alina", ""]]}, {"id": "1701.02273", "submitter": "Du Yong Kim", "authors": "Du Yong Kim", "title": "Visual Multiple-Object Tracking for Unknown Clutter Rate", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-object tracking applications, model parameter tuning is a\nprerequisite for reliable performance. In particular, it is difficult to know\nstatistics of false measurements due to various sensing conditions and changes\nin the field of views. In this paper we are interested in designing a\nmulti-object tracking algorithm that handles unknown false measurement rate.\nRecently proposed robust multi-Bernoulli filter is employed for clutter\nestimation while generalized labeled multi-Bernoulli filter is considered for\ntarget tracking. Performance evaluation with real videos demonstrates the\neffectiveness of the tracking algorithm for real-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 17:40:40 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 08:29:13 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 15:38:10 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Kim", "Du Yong", ""]]}, {"id": "1701.02343", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Erdem Yoruk, Rene Vidal, Laurent Younes, Donald Geman", "title": "Information Pursuit: A Bayesian Framework for Sequential Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite enormous progress in object detection and classification, the problem\nof incorporating expected contextual relationships among object instances into\nmodern recognition systems remains a key challenge. In this work we propose\nInformation Pursuit, a Bayesian framework for scene parsing that combines prior\nmodels for the geometry of the scene and the spatial arrangement of objects\ninstances with a data model for the output of high-level image classifiers\ntrained to answer specific questions about the scene. In the proposed\nframework, the scene interpretation is progressively refined as evidence\naccumulates from the answers to a sequence of questions. At each step, we\nchoose the question to maximize the mutual information between the new answer\nand the full interpretation given the current evidence obtained from previous\ninquiries. We also propose a method for learning the parameters of the model\nfrom synthesized, annotated scenes obtained by top-down sampling from an\neasy-to-learn generative scene model. Finally, we introduce a database of\nannotated indoor scenes of dining room tables, which we use to evaluate the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 20:39:12 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yoruk", "Erdem", ""], ["Vidal", "Rene", ""], ["Younes", "Laurent", ""], ["Geman", "Donald", ""]]}, {"id": "1701.02354", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon Leonardos,\n  Kostantinos G. Derpanis, Kostas Daniilidis", "title": "MonoCap: Monocular Human Motion Capture using a CNN Coupled with a\n  Geometric Prior", "comments": "Accepted by PAMI. Extended version of the following paper: Sparseness\n  Meets Deepness: 3D Human Pose Estimation from Monocular Video. X Zhou, M Zhu,\n  S Leonardos, K Derpanis, K Daniilidis. CVPR 2016. arXiv admin note:\n  substantial text overlap with arXiv:1511.09439", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering 3D full-body human pose is a challenging problem with many\napplications. It has been successfully addressed by motion capture systems with\nbody worn markers and multiple cameras. In this paper, we address the more\nchallenging case of not only using a single camera but also not leveraging\nmarkers: going directly from 2D appearance to 3D geometry. Deep learning\napproaches have shown remarkable abilities to discriminatively learn 2D\nappearance features. The missing piece is how to integrate 2D, 3D and temporal\ninformation to recover 3D geometry and account for the uncertainties arising\nfrom the discriminative model. We introduce a novel approach that treats 2D\njoint locations as latent variables whose uncertainty distributions are given\nby a deep fully convolutional neural network. The unknown 3D poses are modeled\nby a sparse representation and the 3D parameter estimates are realized via an\nExpectation-Maximization algorithm, where it is shown that the 2D joint\nlocation uncertainties can be conveniently marginalized out during inference.\nExtensive evaluation on benchmark datasets shows that the proposed approach\nachieves greater accuracy over state-of-the-art baselines. Notably, the\nproposed approach does not require synchronized 2D-3D data for training and is\napplicable to \"in-the-wild\" images, which is demonstrated with the MPII\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:25:06 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 09:28:11 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Zhu", "Menglong", ""], ["Pavlakos", "Georgios", ""], ["Leonardos", "Spyridon", ""], ["Derpanis", "Kostantinos G.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1701.02357", "submitter": "Bharat Singh", "authors": "Carlos Castillo, Soham De, Xintong Han, Bharat Singh, Abhay Kumar\n  Yadav, and Tom Goldstein", "title": "Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware\n  Semantic Segmentation", "comments": null, "journal-ref": "ICASSP 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is an important task in which the style of a source image is\nmapped onto that of a target image. The method is useful for synthesizing\nderivative works of a particular artist or specific painting. This work\nconsiders targeted style transfer, in which the style of a template image is\nused to alter only part of a target image. For example, an artist may wish to\nalter the style of only one particular object in a target image without\naltering the object's general morphology or surroundings. This is useful, for\nexample, in augmented reality applications (such as the recently released\nPokemon GO), where one wants to alter the appearance of a single real-world\nobject in an image frame to make it appear as a cartoon. Most notably, the\nrendering of real-world objects into cartoon characters has been used in a\nnumber of films and television show, such as the upcoming series Son of Zorn.\nWe present a method for targeted style transfer that simultaneously segments\nand stylizes single objects selected by the user. The method uses a Markov\nrandom field model to smooth and anti-alias outlier pixels near object\nboundaries, so that stylized objects naturally blend into their surroundings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:30:03 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Castillo", "Carlos", ""], ["De", "Soham", ""], ["Han", "Xintong", ""], ["Singh", "Bharat", ""], ["Yadav", "Abhay Kumar", ""], ["Goldstein", "Tom", ""]]}, {"id": "1701.02362", "submitter": "Brian Chu", "authors": "Brian Chu, Daylen Yang, Ravi Tadinada", "title": "Visualizing Residual Networks", "comments": "UC Berkeley CS 280 final project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks are the current state of the art on ImageNet. Similar work\nin the direction of utilizing shortcut connections has been done extremely\nrecently with derivatives of residual networks and with highway networks. This\nwork potentially challenges our understanding that CNNs learn layers of local\nfeatures that are followed by increasingly global features. Through qualitative\nvisualization and empirical analysis, we explore the purpose that residual skip\nconnections serve. Our assessments show that the residual shortcut connections\nforce layers to refine features, as expected. We also provide alternate\nvisualizations that confirm that residual networks learn what is already\nintuitively known about CNNs in general.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:42:46 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Chu", "Brian", ""], ["Yang", "Daylen", ""], ["Tadinada", "Ravi", ""]]}, {"id": "1701.02426", "submitter": "Danfei Xu", "authors": "Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei", "title": "Scene Graph Generation by Iterative Message Passing", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a visual scene goes beyond recognizing individual objects in\nisolation. Relationships between objects also constitute rich semantic\ninformation about the scene. In this work, we explicitly model the objects and\ntheir relationships using scene graphs, a visually-grounded graphical structure\nof an image. We propose a novel end-to-end model that generates such structured\nscene representation from an input image. The model solves the scene graph\ninference problem using standard RNNs and learns to iteratively improves its\npredictions via message passing. Our joint inference model can take advantage\nof contextual cues to make better predictions on objects and their\nrelationships. The experiments show that our model significantly outperforms\nprevious methods for generating scene graphs using Visual Genome dataset and\ninferring support relations with NYU Depth v2 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 03:06:58 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:11:32 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Xu", "Danfei", ""], ["Zhu", "Yuke", ""], ["Choy", "Christopher B.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1701.02468", "submitter": "Christoph Lassner", "authors": "Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo,\n  Michael J. Black, Peter V. Gehler", "title": "Unite the People: Closing the Loop Between 3D and 2D Human\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D models provide a common ground for different representations of human\nbodies. In turn, robust 2D estimation has proven to be a powerful tool to\nobtain 3D fits \"in-the- wild\". However, depending on the level of detail, it\ncan be hard to impossible to acquire labeled data for training 2D estimators on\nlarge scale. We propose a hybrid approach to this problem: with an extended\nversion of the recently introduced SMPLify method, we obtain high quality 3D\nbody model fits for multiple human pose datasets. Human annotators solely sort\ngood and bad fits. This procedure leads to an initial dataset, UP-3D, with rich\nannotations. With a comprehensive set of experiments, we show how this data can\nbe used to train discriminative models that produce results with an\nunprecedented level of detail: our models predict 31 segments and 91 landmark\nlocations on the body. Using the 91 landmark pose estimator, we present\nstate-of-the art results for 3D human pose and shape estimation using an order\nof magnitude less training data and without assumptions about gender or pose in\nthe fitting procedure. We show that UP-3D can be enhanced with these improved\nfits to grow in quantity and quality, which makes the system deployable on\nlarge scale. The data, code and models are available for research purposes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:14:58 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 16:27:16 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 01:58:38 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Lassner", "Christoph", ""], ["Romero", "Javier", ""], ["Kiefel", "Martin", ""], ["Bogo", "Federica", ""], ["Black", "Michael J.", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1701.02470", "submitter": "Manuela Hirschmugl", "authors": "Manuela Hirschmugl, Heinz Gallaun, Matthias Dees, Pawan Datta, Janik\n  Deutscher, Nikos Koutsias, Mathias Schardt", "title": "Methods for Mapping Forest Disturbance and Degradation from Optical\n  Earth Observation Data: a Review", "comments": "This is the Authors' accepted version only! The final version of this\n  paper can be located at Springer.com as part of the Current Forestry Reports\n  (2017) 3: 32. doi:10.1007/s40725-017-0047-2", "journal-ref": "Current Forestry Reports 2017", "doi": "10.1007/s40725-017-0047-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose of review: This paper presents a review of the current state of the\nart in remote sensing based monitoring of forest disturbances and forest\ndegradation from optical Earth Observation data. Part one comprises an overview\nof currently available optical remote sensing sensors, which can be used for\nforest disturbance and degradation mapping. Part two reviews the two main\ncategories of existing approaches: classical image-to-image change detection\nand time series analysis. Recent findings: With the launch of the Sentinel-2a\nsatellite and available Landsat imagery, time series analysis has become the\nmost promising but also most demanding category of degradation mapping\napproaches. Four time series classification methods are distinguished. The\nmethods are explained and their benefits and drawbacks are discussed. A\nseparate chapter presents a number of recent forest degradation mapping studies\nfor two different ecosystems: temperate forests with a geographical focus on\nEurope and tropical forests with a geographical focus on Africa. Summary: The\nreview revealed that a wide variety of methods for the detection of forest\ndegradation is already available. Today, the main challenge is to transfer\nthese approaches to high resolution time series data from multiple sensors.\nFuture research should also focus on the classification of disturbance types\nand the development of robust up-scalable methods to enable near real time\ndisturbance mapping in support of operational reactive measures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:32:04 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 12:40:08 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 10:13:46 GMT"}, {"version": "v4", "created": "Wed, 22 Mar 2017 10:17:06 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Hirschmugl", "Manuela", ""], ["Gallaun", "Heinz", ""], ["Dees", "Matthias", ""], ["Datta", "Pawan", ""], ["Deutscher", "Janik", ""], ["Koutsias", "Nikos", ""], ["Schardt", "Mathias", ""]]}, {"id": "1701.02477", "submitter": "Abhinav Thanda", "authors": "Abhinav Thanda, Shankar M Venkatesan", "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:47:56 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Thanda", "Abhinav", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "1701.02485", "submitter": "Uzair Nadeem", "authors": "Syed Afaq Ali Shah, Uzair Nadeem, Mohammed Bennamoun, Ferdous Sohel,\n  Roberto Togneri", "title": "Efficient Image Set Classification using Linear Regression based Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel image set classification technique using linear regression\nmodels. Downsampled gallery image sets are interpreted as subspaces of a high\ndimensional space to avoid the computationally expensive training step. We\nestimate regression models for each test image using the class specific gallery\nsubspaces. Images of the test set are then reconstructed using the regression\nmodels. Based on the minimum reconstruction error between the reconstructed and\nthe original images, a weighted voting strategy is used to classify the test\nset. We performed extensive evaluation on the benchmark UCSD/Honda, CMU Mobo\nand YouTube Celebrity datasets for face classification, and ETH-80 dataset for\nobject classification. The results demonstrate that by using only a small\namount of training data, our technique achieved competitive classification\naccuracy and superior computational speed compared with the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 09:17:29 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Shah", "Syed Afaq Ali", ""], ["Nadeem", "Uzair", ""], ["Bennamoun", "Mohammed", ""], ["Sohel", "Ferdous", ""], ["Togneri", "Roberto", ""]]}, {"id": "1701.02610", "submitter": "Ender Konukoglu", "authors": "Ender Konukoglu and Ben Glocker", "title": "Reconstructing Subject-Specific Effect Maps", "comments": "29 pages, 16 figures, will appear in Neuroimage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models allow subject-specific inference when analyzing disease\nrelated alterations in neuroimaging data. Given a subject's data, inference can\nbe made at two levels: global, i.e. identifiying condition presence for the\nsubject, and local, i.e. detecting condition effect on each individual\nmeasurement extracted from the subject's data. While global inference is widely\nused, local inference, which can be used to form subject-specific effect maps,\nis rarely used because existing models often yield noisy detections composed of\ndispersed isolated islands. In this article, we propose a reconstruction\nmethod, named RSM, to improve subject-specific detections of predictive\nmodeling approaches and in particular, binary classifiers. RSM specifically\naims to reduce noise due to sampling error associated with using a finite\nsample of examples to train classifiers. The proposed method is a wrapper-type\nalgorithm that can be used with different binary classifiers in a diagnostic\nmanner, i.e. without information on condition presence. Reconstruction is posed\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\nestimated from training data in a classifier-specific fashion. Experimental\nevaluation is performed on synthetically generated data and data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\nsynthetic data demonstrate that using RSM yields higher detection accuracy\ncompared to using models directly or with bootstrap averaging. Analyses on the\nADNI dataset show that RSM can also improve correlation between\nsubject-specific detections in cortical thickness data and non-imaging markers\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\nthe longitudinal ADNI dataset show improvement on detection reliability when\nRSM is used.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:25:20 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 21:44:05 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 10:18:58 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Konukoglu", "Ender", ""], ["Glocker", "Ben", ""]]}, {"id": "1701.02620", "submitter": "Marco Buzzelli", "authors": "Simone Bianco, Marco Buzzelli, Davide Mazzini, Raimondo Schettini", "title": "Deep Learning for Logo Recognition", "comments": "Preprint accepted in Neurocomputing", "journal-ref": "Neurocomputing 245, 23-30 (2017)", "doi": "10.1016/j.neucom.2017.03.051", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method for logo recognition using deep learning.\nOur recognition pipeline is composed of a logo region proposal followed by a\nConvolutional Neural Network (CNN) specifically trained for logo\nclassification, even if they are not precisely localized. Experiments are\ncarried out on the FlickrLogos-32 database, and we evaluate the effect on\nrecognition performance of synthetic versus real data augmentation, and image\npre-processing. Moreover, we systematically investigate the benefits of\ndifferent training choices such as class-balancing, sample-weighting and\nexplicit modeling the background class (i.e. no-logo regions). Experimental\nresults confirm the feasibility of the proposed method, that outperforms the\nmethods in the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:51:39 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 07:30:48 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Bianco", "Simone", ""], ["Buzzelli", "Marco", ""], ["Mazzini", "Davide", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1701.02632", "submitter": "Cristian Gonz\\'alez Garc\\'ia Cgg", "authors": "Cristian Gonz\\'alez Garc\\'ia, Daniel Meana-Llori\\'an, B. Cristina\n  Pelayo G-Bustelo, Juan Manuel Cueva Lovelle, N\\'estor Garcia-Fernandez", "title": "Midgar: Detection of people through computer vision in the Internet of\n  Things scenarios to improve the security in Smart Cities, Smart Towns, and\n  Smart Homes", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2016.12.033", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Could we use Computer Vision in the Internet of Things for using pictures as\nsensors? This is the principal hypothesis that we want to resolve. Currently,\nin order to create safety areas, cities, or homes, people use IP cameras.\nNevertheless, this system needs people who watch the camera images, watch the\nrecording after something occurred, or watch when the camera notifies them of\nany movement. These are the disadvantages. Furthermore, there are many Smart\nCities and Smart Homes around the world. This is why we thought of using the\nidea of the Internet of Things to add a way of automating the use of IP\ncameras. In our case, we propose the analysis of pictures through Computer\nVision to detect people in the analysed pictures. With this analysis, we are\nable to obtain if these pictures contain people and handle the pictures as if\nthey were sensors with two possible states. Notwithstanding, Computer Vision is\na very complicated field. This is why we needed a second hypothesis: Could we\nwork with Computer Vision in the Internet of Things with a good accuracy to\nautomate or semi-automate this kind of events? The demonstration of these\nhypotheses required a testing over our Computer Vision module to check the\npossibilities that we have to use this module in a possible real environment\nwith a good accuracy. Our proposal, as a possible solution, is the analysis of\nentire sequence instead of isolated pictures for using pictures as sensors in\nthe Internet of Things.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:19:14 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 15:22:23 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 06:58:50 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Garc\u00eda", "Cristian Gonz\u00e1lez", ""], ["Meana-Llori\u00e1n", "Daniel", ""], ["G-Bustelo", "B. Cristina Pelayo", ""], ["Lovelle", "Juan Manuel Cueva", ""], ["Garcia-Fernandez", "N\u00e9stor", ""]]}, {"id": "1701.02664", "submitter": "Hugo Jair  Escalante", "authors": "Sergio Escalera, Xavier Bar\\'o, Hugo Jair Escalante, Isabelle Guyon", "title": "ChaLearn Looking at People: A Review of Events and Resources", "comments": "Paper to appear in proceedings of IJCNN 2017 - IEEE - Associated\n  website: http://chalearnlap.cvc.uab.es", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the historic of ChaLearn Looking at People (LAP) events.\nWe started in 2011 (with the release of the first Kinect device) to run\nchallenges related to human action/activity and gesture recognition. Since then\nwe have regularly organized events in a series of competitions covering all\naspects of visual analysis of humans. So far we have organized more than 10\ninternational challenges and events in this field. This paper reviews\nassociated events, and introduces the ChaLearn LAP platform where public\nresources (including code, data and preprints of papers) related to the\norganized events are available. We also provide a discussion on perspectives of\nChaLearn LAP activities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:21:42 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 17:08:31 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Escalera", "Sergio", ""], ["Bar\u00f3", "Xavier", ""], ["Escalante", "Hugo Jair", ""], ["Guyon", "Isabelle", ""]]}, {"id": "1701.02676", "submitter": "Hao Dong", "authors": "Hao Dong, Paarth Neekhara, Chao Wu, Yike Guo", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's useful to automatically transform an image from its original form to\nsome synthetic form (style, partial contents, etc.), while keeping the original\nstructure or semantics. We define this requirement as the \"image-to-image\ntranslation\" problem, and propose a general approach to achieve it, based on\ndeep convolutional and conditional generative adversarial networks (GANs),\nwhich has gained a phenomenal success to learn mapping images from noise input\nsince 2014. In this work, we develop a two step (unsupervised) learning method\nto translate images between different domains by using unlabeled images without\nspecifying any correspondence between them, so that to avoid the cost of\nacquiring labeled data. Compared with prior works, we demonstrated the capacity\nof generality in our model, by which variance of translations can be conduct by\na single type of model. Such capability is desirable in applications like\nbidirectional translation\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:43:03 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Dong", "Hao", ""], ["Neekhara", "Paarth", ""], ["Wu", "Chao", ""], ["Guo", "Yike", ""]]}, {"id": "1701.02704", "submitter": "Drew Linsley", "authors": "Drew Linsley, Sven Eberhardt, Tarun Sharma, Pankaj Gupta, and Thomas\n  Serre", "title": "What are the visual features underlying human versus machine vision?", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep Convolutional Networks (DCNs) are approaching the accuracy of\nhuman observers at object recognition, it is unknown whether they leverage\nsimilar visual representations to achieve this performance. To address this, we\nintroduce Clicktionary, a web-based game for identifying visual features used\nby human observers during object recognition. Importance maps derived from the\ngame are consistent across participants and uncorrelated with image saliency\nmeasures. These results suggest that Clicktionary identifies image regions that\nare meaningful and diagnostic for object recognition but different than those\ndriving eye movements. Surprisingly, Clicktionary importance maps are only\nweakly correlated with relevance maps derived from DCNs trained for object\nrecognition. Our study demonstrates that the narrowing gap between the object\nrecognition accuracy of human observers and DCNs obscures distinct visual\nstrategies used by each to achieve this performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 17:43:32 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 19:19:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Linsley", "Drew", ""], ["Eberhardt", "Sven", ""], ["Sharma", "Tarun", ""], ["Gupta", "Pankaj", ""], ["Serre", "Thomas", ""]]}, {"id": "1701.02718", "submitter": "Roozbeh Mottaghi", "authors": "Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi", "title": "See the Glass Half Full: Reasoning about Liquid Containers, their Volume\n  and Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have rich understanding of liquid containers and their contents; for\nexample, we can effortlessly pour water from a pitcher to a cup. Doing so\nrequires estimating the volume of the cup, approximating the amount of water in\nthe pitcher, and predicting the behavior of water when we tilt the pitcher.\nVery little attention in computer vision has been made to liquids and their\ncontainers. In this paper, we study liquid containers and their contents, and\npropose methods to estimate the volume of containers, approximate the amount of\nliquid in them, and perform comparative volume estimations all from a single\nRGB image. Furthermore, we show the results of the proposed model for\npredicting the behavior of liquids inside containers when one tilts the\ncontainers. We also introduce a new dataset of Containers Of liQuid contEnt\n(COQE) that contains more than 5,000 images of 10,000 liquid containers in\ncontext labelled with volume, amount of content, bounding box annotation, and\ncorresponding similar 3D CAD models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 18:25:15 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 21:42:02 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Mottaghi", "Roozbeh", ""], ["Schenck", "Connor", ""], ["Fox", "Dieter", ""], ["Farhadi", "Ali", ""]]}, {"id": "1701.02797", "submitter": "Kele Xu", "authors": "Kele Xu, Xi Liu, Hengxing Cai and Zhifeng Gao", "title": "Full-reference image quality assessment-based B-mode ultrasound image\n  similarity measure", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, the number of new full-reference image quality\nassessment algorithms has been increasing drastically. Yet, despite of the\nremarkable progress that has been made, the medical ultrasound image similarity\nmeasurement remains largely unsolved due to a high level of speckle noise\ncontamination. Potential applications of the ultrasound image similarity\nmeasurement seem evident in several aspects. To name a few, ultrasound imaging\nquality assessment, abnormal function region detection, etc. In this paper, a\ncomparative study was made on full-reference image quality assessment methods\nfor ultrasound image visual structural similarity measure. Moreover, based on\nthe image similarity index, a generic ultrasound motion tracking\nre-initialization framework is given in this work. The experiments are\nconducted on synthetic data and real-ultrasound liver data and the results\ndemonstrate that, with proposed similarity-based tracking re-initialization,\nthe mean error of landmarks tracking can be decreased from 2 mm to about 1.5 mm\nin the ultrasound liver sequence.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 21:54:02 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 20:45:49 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Xu", "Kele", ""], ["Liu", "Xi", ""], ["Cai", "Hengxing", ""], ["Gao", "Zhifeng", ""]]}, {"id": "1701.02815", "submitter": "Bo Dai", "authors": "Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song", "title": "Stochastic Generative Hashing", "comments": "21 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 00:23:34 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 21:36:09 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dai", "Bo", ""], ["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["He", "Niao", ""], ["Song", "Le", ""]]}, {"id": "1701.02829", "submitter": "Chenglong Li", "authors": "Chenglong Li, Guizhao Wang, Yunpeng Ma, Aihua Zheng, Bin Luo, and Jin\n  Tang", "title": "A Unified RGB-T Saliency Detection Benchmark: Dataset, Baselines,\n  Analysis and A Novel Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress, image saliency detection still remains a\nchallenging task in complex scenes and environments. Integrating multiple\ndifferent but complementary cues, like RGB and Thermal (RGB-T), may be an\neffective way for boosting saliency detection performance. The current research\nin this direction, however, is limited by the lack of a comprehensive\nbenchmark. This work contributes such a RGB-T image dataset, which includes 821\nspatially aligned RGB-T image pairs and their ground truth annotations for\nsaliency detection purpose. The image pairs are with high diversity recorded\nunder different scenes and environmental conditions, and we annotate 11\nchallenges on these image pairs for performing the challenge-sensitive analysis\nfor different saliency detection algorithms. We also implement 3 kinds of\nbaseline methods with different modality inputs to provide a comprehensive\ncomparison platform.\n  With this benchmark, we propose a novel approach, multi-task manifold ranking\nwith cross-modality consistency, for RGB-T saliency detection. In particular,\nwe introduce a weight for each modality to describe the reliability, and\nintegrate them into the graph-based manifold ranking algorithm to achieve\nadaptive fusion of different source data. Moreover, we incorporate the\ncross-modality consistent constraints to integrate different modalities\ncollaboratively. For the optimization, we design an efficient algorithm to\niteratively solve several subproblems with closed-form solutions. Extensive\nexperiments against other baseline methods on the newly created benchmark\ndemonstrate the effectiveness of the proposed approach, and we also provide\nbasic insights and potential future research directions for RGB-T saliency\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 02:38:23 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Li", "Chenglong", ""], ["Wang", "Guizhao", ""], ["Ma", "Yunpeng", ""], ["Zheng", "Aihua", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1701.02870", "submitter": "Ramakrishna Vedantam", "authors": "Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal\n  Chechik", "title": "Context-aware Captions from Context-agnostic Supervision", "comments": "Accepted to CVPR 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an inference technique to produce discriminative context-aware\nimage captions (captions that describe differences between images or visual\nconcepts) using only generic context-agnostic training data (captions that\ndescribe a concept or an image in isolation). For example, given images and\ncaptions of \"siamese cat\" and \"tiger cat\", we generate language that describes\nthe \"siamese cat\" in a way that distinguishes it from \"tiger cat\". Our key\nnovelty is that we show how to do joint inference over a language model that is\ncontext-agnostic and a listener which distinguishes closely-related concepts.\nWe first apply our technique to a justification task, namely to describe why an\nimage contains a particular fine-grained category as opposed to another\nclosely-related category of the CUB-200-2011 dataset. We then study\ndiscriminative image captioning to generate language that uniquely refers to\none of two semantically-similar images in the COCO dataset. Evaluations with\ndiscriminative ground truth for justification and human studies for\ndiscriminative image captioning reveal that our approach outperforms baseline\ngenerative and speaker-listener approaches for discrimination.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 07:42:58 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 08:59:56 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 23:29:36 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Bengio", "Samy", ""], ["Murphy", "Kevin", ""], ["Parikh", "Devi", ""], ["Chechik", "Gal", ""]]}, {"id": "1701.02892", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Chi Xu and Yu Zhang and Tingshao Zhu and Li Cheng", "title": "Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:52:53 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Xu", "Chi", ""], ["Zhang", "Yu", ""], ["Zhu", "Tingshao", ""], ["Cheng", "Li", ""]]}, {"id": "1701.02898", "submitter": "Riccardo Volpi", "authors": "Matteo Zanotto, Riccardo Volpi, Alessandro Maccione, Luca Berdondini,\n  Diego Sona, Vittorio Murino", "title": "Modeling Retinal Ganglion Cell Population Activity with Restricted\n  Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The retina is a complex nervous system which encodes visual stimuli before\nhigher order processing occurs in the visual cortex. In this study we evaluated\nwhether information about the stimuli received by the retina can be retrieved\nfrom the firing rate distribution of Retinal Ganglion Cells (RGCs), exploiting\nHigh-Density 64x64 MEA technology. To this end, we modeled the RGC population\nactivity using mean-covariance Restricted Boltzmann Machines, latent variable\nmodels capable of learning the joint distribution of a set of continuous\nobserved random variables and a set of binary unobserved random units. The idea\nwas to figure out if binary latent states encode the regularities associated to\ndifferent visual stimuli, as modes in the joint distribution. We measured the\ngoodness of mcRBM encoding by calculating the Mutual Information between the\nlatent states and the stimuli shown to the retina. Results show that binary\nstates can encode the regularities associated to different stimuli, using both\ngratings and natural scenes as stimuli. We also discovered that hidden\nvariables encode interesting properties of retinal activity, interpreted as\npopulation receptive fields. We further investigated the ability of the model\nto learn different modes in population activity by comparing results associated\nto a retina in normal conditions and after pharmacologically blocking GABA\nreceptors (GABAC at first, and then also GABAA and GABAB). As expected, Mutual\nInformation tends to decrease if we pharmacologically block receptors. We\nfinally stress that the computational method described in this work could\npotentially be applied to any kind of neural data obtained through MEA\ntechnology, though different techniques should be applied to interpret the\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 09:27:29 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 10:01:46 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Zanotto", "Matteo", ""], ["Volpi", "Riccardo", ""], ["Maccione", "Alessandro", ""], ["Berdondini", "Luca", ""], ["Sona", "Diego", ""], ["Murino", "Vittorio", ""]]}, {"id": "1701.02965", "submitter": "Qingnan Fan", "authors": "Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf", "title": "Revisiting Deep Intrinsic Image Decompositions", "comments": "Accepted by CVPR'18 as Oral presentation (Conference on Computer\n  Vision and Pattern Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While invaluable for many computer vision applications, decomposing a natural\nimage into intrinsic reflectance and shading layers represents a challenging,\nunderdetermined inverse problem. As opposed to strict reliance on conventional\noptimization or filtering solutions with strong prior assumptions, deep\nlearning based approaches have also been proposed to compute intrinsic image\ndecompositions when granted access to sufficient labeled training data. The\ndownside is that current data sources are quite limited, and broadly speaking\nfall into one of two categories: either dense fully-labeled images in\nsynthetic/narrow settings, or weakly-labeled data from relatively diverse\nnatural scenes. In contrast to many previous learning-based approaches, which\nare often tailored to the structure of a particular dataset (and may not work\nwell on others), we adopt core network structures that universally reflect\nloose prior knowledge regarding the intrinsic image formation process and can\nbe largely shared across datasets. We then apply flexibly supervised loss\nlayers that are customized for each source of ground truth labels. The\nresulting deep architecture achieves state-of-the-art results on all of the\nmajor intrinsic image benchmarks, and runs considerably faster than most at\ntest time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:20:28 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 12:17:53 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 04:15:21 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 11:26:37 GMT"}, {"version": "v5", "created": "Wed, 30 May 2018 04:35:16 GMT"}, {"version": "v6", "created": "Sat, 16 Jun 2018 03:27:19 GMT"}, {"version": "v7", "created": "Mon, 25 Jun 2018 00:49:32 GMT"}, {"version": "v8", "created": "Fri, 31 Aug 2018 16:05:29 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Fan", "Qingnan", ""], ["Yang", "Jiaolong", ""], ["Hua", "Gang", ""], ["Chen", "Baoquan", ""], ["Wipf", "David", ""]]}, {"id": "1701.03056", "submitter": "Baris Kayalibay", "authors": "Baris Kayalibay, Grady Jensen, Patrick van der Smagt", "title": "CNN-based Segmentation of Medical Imaging Data", "comments": "24 pages, Code available on\n  https://github.com/BRML/CNNbasedMedicalSegmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been applied to a wide variety of computer\nvision tasks. Recent advances in semantic segmentation have enabled their\napplication to medical image segmentation. While most CNNs use two-dimensional\nkernels, recent CNN-based publications on medical image segmentation featured\nthree-dimensional kernels, allowing full access to the three-dimensional\nstructure of medical images. Though closely related to semantic segmentation,\nmedical image segmentation includes specific challenges that need to be\naddressed, such as the scarcity of labelled data, the high class imbalance\nfound in the ground truth and the high memory demand of three-dimensional\nimages. In this work, a CNN-based method with three-dimensional filters is\ndemonstrated and applied to hand and brain MRI. Two modifications to an\nexisting CNN architecture are discussed, along with methods on addressing the\naforementioned challenges. While most of the existing literature on medical\nimage segmentation focuses on soft tissue and the major organs, this work is\nvalidated on data both from the central nervous system as well as the bones of\nthe hand.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:50:30 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 10:22:17 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Kayalibay", "Baris", ""], ["Jensen", "Grady", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1701.03077", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A General and Adaptive Robust Loss Function", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the Cauchy/Lorentzian, Geman-McClure,\nWelsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2\nloss functions. By introducing robustness as a continuous parameter, our loss\nfunction allows algorithms built around robust loss minimization to be\ngeneralized, which improves performance on basic vision tasks such as\nregistration and clustering. Interpreting our loss as the negative log of a\nunivariate density yields a general probability distribution that includes\nnormal and Cauchy distributions as special cases. This probabilistic\ninterpretation enables the training of neural networks in which the robustness\nof the loss automatically adapts itself during training, which improves\nperformance on learning-based tasks such as generative image synthesis and\nunsupervised monocular depth estimation, without requiring any manual parameter\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:39:14 GMT"}, {"version": "v10", "created": "Thu, 4 Apr 2019 20:05:33 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 21:16:44 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 18:11:17 GMT"}, {"version": "v4", "created": "Fri, 26 Jan 2018 23:48:44 GMT"}, {"version": "v5", "created": "Sun, 11 Feb 2018 17:33:16 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2018 17:36:46 GMT"}, {"version": "v7", "created": "Tue, 11 Dec 2018 17:53:58 GMT"}, {"version": "v8", "created": "Sat, 9 Feb 2019 20:56:30 GMT"}, {"version": "v9", "created": "Sat, 30 Mar 2019 00:48:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "1701.03102", "submitter": "Xiang Xiang", "authors": "Xiang Xiang, Trac D. Tran", "title": "Linear Disentangled Representation Learning for Facial Actions", "comments": "Codes available at https://github.com/eglxiang/icassp15_emotion and\n  https://github.com/eglxiang/FacialAU. arXiv admin note: text overlap with\n  arXiv:1410.1606", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited annotated data available for the recognition of facial expression and\naction units embarrasses the training of deep networks, which can learn\ndisentangled invariant features. However, a linear model with just several\nparameters normally is not demanding in terms of training data. In this paper,\nwe propose an elegant linear model to untangle confounding factors in\nchallenging realistic multichannel signals such as 2D face videos. The simple\nyet powerful model does not rely on huge training data and is natural for\nrecognizing facial actions without explicitly disentangling the identity. Base\non well-understood intuitive linear models such as Sparse Representation based\nClassification (SRC), previous attempts require a prepossessing of explicit\ndecoupling which is practically inexact. Instead, we exploit the low-rank\nproperty across frames to subtract the underlying neutral faces which are\nmodeled jointly with sparse representation on the action components with group\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\nautomatic method on raw face videos performs as competitive as SRC applied on\nmanually prepared action components and performs even better than SRC in terms\nof true positive rate. We apply the model to the even more challenging task of\nfacial action unit recognition, verified on the MPI Face Video Database\n(MPI-VDB) achieving a decent performance. All the programs and data have been\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:34:29 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1701.03126", "submitter": "Chiori Hori Dr.", "authors": "Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R.\n  Hershey, Tim K. Marks", "title": "Attention-Based Multimodal Fusion for Video Description", "comments": "Resubmitted to the rebuttal for CVPR 2017 for review, 8 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently successful methods for video description are based on\nencoder-decoder sentence generation using recur-rent neural networks (RNNs).\nRecent work has shown the advantage of integrating temporal and/or spatial\nattention mechanisms into these models, in which the decoder net-work predicts\neach word in the description by selectively giving more weight to encoded\nfeatures from specific time frames (temporal attention) or to features from\nspecific spatial regions (spatial attention). In this paper, we propose to\nexpand the attention model to selectively attend not just to specific times or\nspatial regions, but to specific modalities of input such as image features,\nmotion features, and audio features. Our new modality-dependent attention\nmechanism, which we call multimodal attention, provides a natural way to fuse\nmultimodal information for video description. We evaluate our method on the\nYoutube2Text dataset, achieving results that are competitive with current state\nof the art. More importantly, we demonstrate that our model incorporating\nmultimodal attention as well as temporal attention significantly outperforms\nthe model that uses temporal attention alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:16:42 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 22:57:10 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Hori", "Chiori", ""], ["Hori", "Takaaki", ""], ["Lee", "Teng-Yok", ""], ["Sumi", "Kazuhiro", ""], ["Hershey", "John R.", ""], ["Marks", "Tim K.", ""]]}, {"id": "1701.03151", "submitter": "Mengtian Li", "authors": "Mengtian Li and Daniel Huber", "title": "Guaranteed Parameter Estimation for Discrete Energy Minimization", "comments": "WACV 2017: IEEE Winter Conference on Applications of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural learning, a method to estimate the parameters for discrete energy\nminimization, has been proven to be effective in solving computer vision\nproblems, especially in 3D scene parsing. As the complexity of the models\nincreases, structural learning algorithms turn to approximate inference to\nretain tractability. Unfortunately, such methods often fail because the\napproximation can be arbitrarily poor. In this work, we propose a method to\novercome this limitation through exploiting the properties of the joint problem\nof training time inference and learning. With the help of the learning\nframework, we transform the inapproximable inference problem into a polynomial\ntime solvable one, thereby enabling tractable exact inference while still\nallowing an arbitrary graph structure and full potential interactions. Our\nlearning algorithm is guaranteed to return a solution with a bounded error to\nthe global optimal within the feasible parameter space. We demonstrate the\neffectiveness of this method on two point cloud scene parsing datasets. Our\napproach runs much faster and solves a problem that is intractable for\nprevious, well-known approaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 20:41:14 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Li", "Mengtian", ""], ["Huber", "Daniel", ""]]}, {"id": "1701.03153", "submitter": "Igor Barros Barbosa", "authors": "Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander\n  Rognhaugen, Theoharis Theoharis", "title": "Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in\n  Re-identification", "comments": "14 pages", "journal-ref": "Computer Vision and Image Understanding 167 (2018)", "doi": "10.1016/j.cviu.2017.12.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identification is generally carried out by encoding the appearance of a\nsubject in terms of outfit, suggesting scenarios where people do not change\ntheir attire. In this paper we overcome this restriction, by proposing a\nframework based on a deep convolutional neural network, SOMAnet, that\nadditionally models other discriminative aspects, namely, structural attributes\nof the human figure (e.g. height, obesity, gender). Our method is unique in\nmany respects. First, SOMAnet is based on the Inception architecture, departing\nfrom the usual siamese framework. This spares expensive data preparation\n(pairing images across cameras) and allows the understanding of what the\nnetwork learned. Second, and most notably, the training data consists of a\nsynthetic 100K instance dataset, SOMAset, created by photorealistic human body\ngeneration software. Synthetic data represents a good compromise between\nrealistic imagery, usually not required in re-identification since surveillance\ncameras capture low-resolution silhouettes, and complete control of the\nsamples, which is useful in order to customize the data w.r.t. the surveillance\nscenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and fine-tuned on\nrecent re-identification benchmarks, outperforms all competitors, matching\nsubjects even with different apparel. The combination of synthetic data with\nInception architectures opens up new research avenues in re-identification.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 20:43:41 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:14:53 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Barbosa", "Igor Barros", ""], ["Cristani", "Marco", ""], ["Caputo", "Barbara", ""], ["Rognhaugen", "Aleksander", ""], ["Theoharis", "Theoharis", ""]]}, {"id": "1701.03244", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Xiaorong Hou", "title": "Light Source Point Cluster Selection Based Atmosphere Light Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmosphere light value is a highly critical parameter in defogging algorithms\nthat are based on an atmosphere scattering model. Any error in atmosphere light\nvalue will produce a direct impact on the accuracy of scattering computation\nand thus bring chromatic distortion to restored images. To address this\nproblem, this paper propose a method that relies on clustering statistics to\nestimate atmosphere light value. It starts by selecting in the original image\nsome potential atmosphere light source points, which are grouped into point\nclusters by means of clustering technique. From these clusters, a number of\nclusters containing candidate atmosphere light source points are selected, the\npoints are then analyzed statistically, and the cluster containing the most\ncandidate points is used for estimating atmosphere light value. The mean\nbrightness vector of the candidate atmosphere light points in the chosen point\ncluster is taken as the estimate of atmosphere light value, while their\ngeometric center in the image is accepted as the location of atmosphere light.\nExperimental results suggest that this statistics clustering method produces\nmore accurate atmosphere brightness vectors and light source locations. This\naccuracy translates to, from a subjective perspective, more natural defogging\neffect on the one hand and to the improvement in various objective image\nquality indicators on the other hand.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 05:56:28 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Zhang", "Wenbo", ""], ["Hou", "Xiaorong", ""]]}, {"id": "1701.03246", "submitter": "Jue Wang", "authors": "Jue Wang, Anoop Cherian, Fatih Porikli", "title": "Ordered Pooling of Optical Flow Sequences for Action Recognition", "comments": "Accepted in WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training of Convolutional Neural Networks (CNNs) on long video sequences is\ncomputationally expensive due to the substantial memory requirements and the\nmassive number of parameters that deep architectures demand. Early fusion of\nvideo frames is thus a standard technique, in which several consecutive frames\nare first agglomerated into a compact representation, and then fed into the CNN\nas an input sample. For this purpose, a summarization approach that represents\na set of consecutive RGB frames by a single dynamic image to capture pixel\ndynamics is proposed recently. In this paper, we introduce a novel ordered\nrepresentation of consecutive optical flow frames as an alternative and argue\nthat this representation captures the action dynamics more effectively than RGB\nframes. We provide intuitions on why such a representation is better for action\nrecognition. We validate our claims on standard benchmark datasets and\ndemonstrate that using summaries of flow images lead to significant\nimprovements over RGB frames while achieving accuracy comparable to the\nstate-of-the-art on UCF101 and HMDB datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 06:08:18 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 05:27:03 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""], ["Porikli", "Fatih", ""]]}, {"id": "1701.03266", "submitter": "Demian Wassermann", "authors": "Demian Wassermann (ATHENA), Matt Toews, Marc Niethammer, William Wells\n  Iii", "title": "Probabilistic Diffeomorphic Registration: Representing Uncertainty", "comments": null, "journal-ref": "Biomedical Image Registration, 2014, London, United Kingdom. pp.72\n  - 82, 2014", "doi": "10.1007/978-3-319-08554-8_8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel mathematical framework for representing\nuncertainty in large deformation diffeomorphic image registration. The Bayesian\nposterior distribution over the deformations aligning a moving and a fixed\nimage is approximated via a variational formulation. A stochastic differential\nequation (SDE) modeling the deformations as the evolution of a time-varying\nvelocity field leads to a prior density over deformations in the form of a\nGaussian process. This permits estimating the full posterior distribution in\norder to represent uncertainty, in contrast to methods in which the posterior\nis approximated via Monte Carlo sampling or maximized in maximum a-posteriori\n(MAP) estimation. The frame-work is demonstrated in the case of landmark-based\nimage registration, including simulated data and annotated pre and\nintra-operative 3D images.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 08:29:31 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Wassermann", "Demian", "", "ATHENA"], ["Toews", "Matt", ""], ["Niethammer", "Marc", ""], ["Wells", "William", "Iii"]]}, {"id": "1701.03330", "submitter": "Joachim Dehais", "authors": "Joachim Dehais, Marios Anthimopoulos, Sergey Shevchik, Stavroula\n  Mougiakakou", "title": "Two-view 3D Reconstruction for Food Volume Estimation", "comments": "10 pages, 7 Tables, 8 Figures in IEEE Transactions on Multimedia,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prevalence of diet-related chronic diseases coupled with the\nineffectiveness of traditional diet management methods have resulted in a need\nfor novel tools to accurately and automatically assess meals. Recently,\ncomputer vision based systems that use meal images to assess their content have\nbeen proposed. Food portion estimation is the most difficult task for\nindividuals assessing their meals and it is also the least studied area. The\npresent paper proposes a three-stage system to calculate portion sizes using\ntwo images of a dish acquired by mobile devices. The first stage consists in\nunderstanding the configuration of the different views, after which a dense 3D\nmodel is built from the two images; finally, this 3D model serves to extract\nthe volume of the different items. The system was extensively tested on 77 real\ndishes of known volume, and achieved an average error of less than 10% in 5.5\nseconds per dish. The proposed pipeline is computationally tractable and\nrequires no user input, making it a viable option for fully automated dietary\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 13:10:22 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Dehais", "Joachim", ""], ["Anthimopoulos", "Marios", ""], ["Shevchik", "Sergey", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1701.03364", "submitter": "Yuan-Hang Zhang", "authors": "Yuan-Hang Zhang, Xie Li, Jing-Yun Xiao", "title": "A Digital Fuzzy Edge Detector for Color Images", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is a classic problem in the field of image processing, which\nlays foundations for other tasks such as image segmentation. Conventionally,\nthis operation is performed using gradient operators such as the Roberts or\nSobel operator, which can discover local changes in intensity levels. These\noperators, however, perform poorly on low contrast images. In this paper, we\npropose an edge detector architecture for color images based on fuzzy theory\nand the Sobel operator. First, the R, G and B channels are extracted from an\nimage and enhanced using fuzzy methods, in order to suppress noise and improve\nthe contrast between the background and the objects. The Sobel operator is then\napplied to each of the channels, which are finally combined into an edge map of\nthe origin image. Experimental results obtained through an FPGA-based\nimplementation have proved the proposed method effective.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 14:52:32 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 05:17:13 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Zhang", "Yuan-Hang", ""], ["Li", "Xie", ""], ["Xiao", "Jing-Yun", ""]]}, {"id": "1701.03400", "submitter": "Nicholas Fraser", "authors": "Nicholas J. Fraser, Yaman Umuroglu, Giulio Gambardella, Michaela\n  Blott, Philip Leong, Magnus Jahre and Kees Vissers", "title": "Scaling Binarized Neural Networks on Reconfigurable Logic", "comments": "To appear in the PARMA-DITAM workshop at HiPEAC 2017, January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized neural networks (BNNs) are gaining interest in the deep learning\ncommunity due to their significantly lower computational and memory cost. They\nare particularly well suited to reconfigurable logic devices, which contain an\nabundance of fine-grained compute resources and can result in smaller, lower\npower implementations, or conversely in higher classification rates. Towards\nthis end, the Finn framework was recently proposed for building fast and\nflexible field programmable gate array (FPGA) accelerators for BNNs. Finn\nutilized a novel set of optimizations that enable efficient mapping of BNNs to\nhardware and implemented fully connected, non-padded convolutional and pooling\nlayers, with per-layer compute resources being tailored to user-provided\nthroughput requirements. However, FINN was not evaluated on larger topologies\ndue to the size of the chosen FPGA, and exhibited decreased accuracy due to\nlack of padding. In this paper, we improve upon Finn to show how padding can be\nemployed on BNNs while still maintaining a 1-bit datapath and high accuracy.\nBased on this technique, we demonstrate numerous experiments to illustrate\nflexibility and scalability of the approach. In particular, we show that a\nlarge BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5\nplatform can classify images at 12 kFPS with 671 us latency while drawing less\nthan 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our\nimplementation of this network achieves 14.8 trillion operations per second. We\nbelieve this is the fastest classification rate reported to date on this\nbenchmark at this level of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 16:42:47 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 09:12:48 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Fraser", "Nicholas J.", ""], ["Umuroglu", "Yaman", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Leong", "Philip", ""], ["Jahre", "Magnus", ""], ["Vissers", "Kees", ""]]}, {"id": "1701.03420", "submitter": "Mohsen Joneidi", "authors": "Mojtaba Sahraee-Ardakan and Mohsen Joneidi", "title": "Joint Dictionary Learning for Example-based Image Super-resolution", "comments": "5 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new joint dictionary learning method for\nexample-based image super-resolution (SR), using sparse representation. The\nlow-resolution (LR) dictionary is trained from a set of LR sample image\npatches. Using the sparse representation coefficients of these LR patches over\nthe LR dictionary, the high-resolution (HR) dictionary is trained by minimizing\nthe reconstruction error of HR sample patches. The error criterion used here is\nthe mean square error. In this way we guarantee that the HR patches have the\nsame sparse representation over HR dictionary as the LR patches over the LR\ndictionary, and at the same time, these sparse representations can well\nreconstruct the HR patches. Simulation results show the effectiveness of our\nmethod compared to the state-of-art SR algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 17:21:46 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Sahraee-Ardakan", "Mojtaba", ""], ["Joneidi", "Mohsen", ""]]}, {"id": "1701.03439", "submitter": "Ruotian Luo", "authors": "Ruotian Luo, Gregory Shakhnarovich", "title": "Comprehension-guided referring expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider generation and comprehension of natural language referring\nexpression for objects in an image. Unlike generic \"image captioning\" which\nlacks natural standard evaluation criteria, quality of a referring expression\nmay be measured by the receiver's ability to correctly infer which object is\nbeing described. Following this intuition, we propose two approaches to utilize\nmodels trained for comprehension task to generate better expressions. First, we\nuse a comprehension module trained on human-generated expressions, as a\n\"critic\" of referring expression generator. The comprehension module serves as\na differentiable proxy of human evaluation, providing training signal to the\ngeneration module. Second, we use the comprehension module in a\ngenerate-and-rerank pipeline, which chooses from candidate expressions\ngenerated by a model according to their performance on the comprehension task.\nWe show that both approaches lead to improved referring expression generation\non multiple benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:03:52 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Luo", "Ruotian", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1701.03534", "submitter": "Andrew Ling", "authors": "Utku Aydonat, Shane O'Connell, Davor Capalija, Andrew C. Ling, Gordon\n  R. Chiu", "title": "An OpenCL(TM) Deep Learning Accelerator on Arria 10", "comments": "To be published at FPGA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (CNNs) have become a practical means to perform\nvision tasks, particularly in the area of image classification. FPGAs are well\nknown to be able to perform convolutions efficiently, however, most recent\nefforts to run CNNs on FPGAs have shown limited advantages over other devices\nsuch as GPUs. Previous approaches on FPGAs have often been memory bound due to\nthe limited external memory bandwidth on the FPGA device. We show a novel\narchitecture written in OpenCL(TM), which we refer to as a Deep Learning\nAccelerator (DLA), that maximizes data reuse and minimizes external memory\nbandwidth. Furthermore, we show how we can use the Winograd transform to\nsignificantly boost the performance of the FPGA. As a result, when running our\nDLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or\n23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs\nand is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the\nstate-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the\nbest publicly known implementation of AlexNet on nVidia's TitanX GPU.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 00:31:15 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Aydonat", "Utku", ""], ["O'Connell", "Shane", ""], ["Capalija", "Davor", ""], ["Ling", "Andrew C.", ""], ["Chiu", "Gordon R.", ""]]}, {"id": "1701.03551", "submitter": "Liang Lin", "authors": "Keze Wang and Dongyu Zhang and Ya Li and Ruimao Zhang and Liang Lin", "title": "Cost-Effective Active Learning for Deep Image Classification", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT) 2016", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2589879", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in learning-based image classification, however, heavily\nrely on the large number of annotated training samples, which may require\nconsiderable human efforts. In this paper, we propose a novel active learning\nframework, which is capable of building a competitive classifier with optimal\nfeature representation via a limited amount of labeled training instances in an\nincremental learning manner. Our approach advances the existing active learning\nmethods in two aspects. First, we incorporate deep convolutional neural\nnetworks into active learning. Through the properly designed framework, the\nfeature representation and the classifier can be simultaneously updated with\nprogressively annotated informative samples. Second, we present a\ncost-effective sample selection strategy to improve the classification\nperformance with less manual annotations. Unlike traditional methods focusing\non only the uncertain samples of low prediction confidence, we especially\ndiscover the large amount of high confidence samples from the unlabeled set for\nfeature learning. Specifically, these high confidence samples are automatically\nselected and iteratively assigned pseudo-labels. We thus call our framework\n\"Cost-Effective Active Learning\" (CEAL) standing for the two\nadvantages.Extensive experiments demonstrate that the proposed CEAL framework\ncan achieve promising results on two challenging image classification datasets,\ni.e., face recognition on CACD database [1] and object categorization on\nCaltech-256 [2].\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 03:07:45 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Wang", "Keze", ""], ["Zhang", "Dongyu", ""], ["Li", "Ya", ""], ["Zhang", "Ruimao", ""], ["Lin", "Liang", ""]]}, {"id": "1701.03555", "submitter": "Liang Lin", "authors": "Liang Lin and Keze Wang and Deyu Meng and Wangmeng Zuo and Lei Zhang", "title": "Active Self-Paced Learning for Cost-Effective and Progressive Face\n  Identification", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2017", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2652459", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to develop a novel cost-effective framework for face\nidentification, which progressively maintains a batch of classifiers with the\nincreasing face images of different individuals. By naturally combining two\nrecently rising techniques: active learning (AL) and self-paced learning (SPL),\nour framework is capable of automatically annotating new instances and\nincorporating them into training under weak expert re-certification. We first\ninitialize the classifier using a few annotated samples for each individual,\nand extract image features using the convolutional neural nets. Then, a number\nof candidates are selected from the unannotated samples for classifier\nupdating, in which we apply the current classifiers ranking the samples by the\nprediction confidence. In particular, our approach utilizes the high-confidence\nand low-confidence samples in the self-paced and the active user-query way,\nrespectively. The neural nets are later fine-tuned based on the updated\nclassifiers. Such heuristic implementation is formulated as solving a concise\nactive SPL optimization problem, which also advances the SPL development by\nsupplementing a rational dynamic curriculum constraint. The new model finely\naccords with the \"instructor-student-collaborative\" learning mode in human\neducation. The advantages of this proposed framework are two-folds: i) The\nrequired number of annotated samples is significantly decreased while the\ncomparable performance is guaranteed. A dramatic reduction of user effort is\nalso achieved over other state-of-the-art active learning techniques. ii) The\nmixture of SPL and AL effectively improves not only the classifier accuracy\ncompared to existing AL/SPL methods but also the robustness against noisy data.\nWe evaluate our framework on two challenging datasets, and demonstrate very\npromising results. (http://hcp.sysu.edu.cn/projects/aspl/)\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 03:30:42 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:18:47 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Keze", ""], ["Meng", "Deyu", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1701.03572", "submitter": "Bharath Ramesh", "authors": "Anli Lim, Bharath Ramesh, Yue Yang, Cheng Xiang, Zhi Gao, Feng Lin", "title": "Real-Time Optical flow-based Video Stabilization for Unmanned Aerial\n  Vehicles", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of a novel algorithm to tackle the\nproblem of real-time video stabilization for unmanned aerial vehicles (UAVs).\nThere are two main components in the algorithm: (1) By designing a suitable\nmodel for the global motion of UAV, the proposed algorithm avoids the necessity\nof estimating the most general motion model, projective transformation, and\nconsiders simpler motion models, such as rigid transformation and similarity\ntransformation. (2) To achieve a high processing speed, optical-flow based\ntracking is employed in lieu of conventional tracking and matching methods used\nby state-of-the-art algorithms. These two new ideas resulted in a real-time\nstabilization algorithm, developed over two phases. Stage I considers\nprocessing the whole sequence of frames in the video while achieving an average\nprocessing speed of 50fps on several publicly available benchmark videos. Next,\nStage II undertakes the task of real-time video stabilization using a\nmulti-threading implementation of the algorithm designed in Stage I.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 06:51:57 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Lim", "Anli", ""], ["Ramesh", "Bharath", ""], ["Yang", "Yue", ""], ["Xiang", "Cheng", ""], ["Gao", "Zhi", ""], ["Lin", "Feng", ""]]}, {"id": "1701.03779", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Hamid R. Tizhoosh, Kan Wu, Dun Huang, Hadi\n  Tadayyon, Gregory J. Czarnota", "title": "Tumour Ellipsification in Ultrasound Images for Treatment Prediction in\n  Breast Cancer", "comments": "Accepted at BHI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in using quantitative ultrasound (QUS) methods have provided\na promising framework to non-invasively and inexpensively monitor or predict\nthe effectiveness of therapeutic cancer responses. One of the earliest steps in\nusing QUS methods is contouring a region of interest (ROI) inside the tumour in\nultrasound B-mode images. While manual segmentation is a very time-consuming\nand tedious task for human experts, auto-contouring is also an extremely\ndifficult task for computers due to the poor quality of ultrasound B-mode\nimages. However, for the purpose of cancer response prediction, a rough\nboundary of the tumour as an ROI is only needed. In this research, a\nsemi-automated tumour localization approach is proposed for ROI estimation in\nultrasound B-mode images acquired from patients with locally advanced breast\ncancer (LABC). The proposed approach comprised several modules, including 1)\nfeature extraction using keypoint descriptors, 2) augmenting the feature\ndescriptors with the distance of the keypoints to the user-input pixel as the\ncentre of the tumour, 3) supervised learning using a support vector machine\n(SVM) to classify keypoints as \"tumour\" or \"non-tumour\", and 4) computation of\nan ellipse as an outline of the ROI representing the tumour. Experiments with\n33 B-mode images from 10 LABC patients yielded promising results with an\naccuracy of 76.7% based on the Dice coefficient performance measure. The\nresults demonstrated that the proposed method can potentially be used as the\nfirst stage in a computer-assisted cancer response prediction system for\nsemi-automated contouring of breast tumours.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 18:53:59 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Tizhoosh", "Hamid R.", ""], ["Wu", "Kan", ""], ["Huang", "Dun", ""], ["Tadayyon", "Hadi", ""], ["Czarnota", "Gregory J.", ""]]}, {"id": "1701.03869", "submitter": "Wenwen Ding", "authors": "Wenwen Ding, Kai Liu", "title": "Learning Linear Dynamical Systems with High-Order Tensor Data for\n  Skeleton based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been renewed interest in developing methods for\nskeleton-based human action recognition. A skeleton sequence can be naturally\nrepresented as a high-order tensor time series. In this paper, we model and\nanalyze tensor time series with Linear Dynamical System (LDS) which is the most\ncommon for encoding spatio-temporal time-series data in various disciplines dut\nto its relative simplicity and efficiency. However, the traditional LDS treats\nthe latent and observation state at each frame of video as a column vector.\nSuch a vector representation fails to take into account the curse of\ndimensionality as well as valuable structural information with human action.\nConsidering this fact, we propose generalized Linear Dynamical System (gLDS)\nfor modeling tensor observation in the time series and employ Tucker\ndecomposition to estimate the LDS parameters as action descriptors. Therefore,\nan action can be represented as a subspace corresponding to a point on a\nGrassmann manifold. Then we perform classification using dictionary learning\nand sparse coding over Grassmann manifold. Experiments on MSR Action3D Dataset,\nUCF Kinect Dataset and Northwestern-UCLA Multiview Action3D Dataset demonstrate\nthat our proposed method achieves superior performance to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 02:07:23 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ding", "Wenwen", ""], ["Liu", "Kai", ""]]}, {"id": "1701.03916", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun and St\\'ephane Marchand-Maillet", "title": "On H\\\"older projective divergences", "comments": "25 pages", "journal-ref": null, "doi": "10.3390/e19030122", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework to build distances by measuring the tightness of\ninequalities, and introduce the notion of proper statistical divergences and\nimproper pseudo-divergences. We then consider the H\\\"older ordinary and reverse\ninequalities, and present two novel classes of H\\\"older divergences and\npseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz\ndivergence. We report closed-form formulas for those statistical\ndissimilarities when considering distributions belonging to the same\nexponential family provided that the natural parameter space is a cone (e.g.,\nmultivariate Gaussians), or affine (e.g., categorical distributions). Those new\nclasses of H\\\"older distances are invariant to rescaling, and thus do not\nrequire distributions to be normalized. Finally, we show how to compute\nstatistical H\\\"older centroids with respect to those divergences, and carry out\ncenter-based clustering toy experiments on a set of Gaussian distributions that\ndemonstrate empirically that symmetrized H\\\"older divergences outperform the\nsymmetric Cauchy-Schwarz divergence.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 12:57:44 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1701.04010", "submitter": "Kapil Ahuja", "authors": "Aditya A. Shastri, Deepti Tamrakar, and Kapil Ahuja", "title": "Density-Wise Two Stage Mammogram Classification using Texture Exploiting\n  Descriptors", "comments": "28 Pages, 8 Figures, and 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is becoming pervasive with each passing day. Hence, its early\ndetection is a big step in saving the life of any patient. Mammography is a\ncommon tool in breast cancer diagnosis. The most important step here is\nclassification of mammogram patches as normal-abnormal and benign-malignant.\n  Texture of a breast in a mammogram patch plays a significant role in these\nclassifications. We propose a variation of Histogram of Gradients (HOG) and\nGabor filter combination called Histogram of Oriented Texture (HOT) that\nexploits this fact. We also revisit the Pass Band - Discrete Cosine Transform\n(PB-DCT) descriptor that captures texture information well. All features of a\nmammogram patch may not be useful. Hence, we apply a feature selection\ntechnique called Discrimination Potentiality (DP). Our resulting descriptors,\nDP-HOT and DP-PB-DCT, are compared with the standard descriptors.\n  Density of a mammogram patch is important for classification, and has not\nbeen studied exhaustively. The Image Retrieval in Medical Application (IRMA)\ndatabase from RWTH Aachen, Germany is a standard database that provides\nmammogram patches, and most researchers have tested their frameworks only on a\nsubset of patches from this database. We apply our two new descriptors on all\nimages of the IRMA database for density wise classification, and compare with\nthe standard descriptors. We achieve higher accuracy than all of the existing\nstandard descriptors (more than 92%).\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 08:59:06 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 08:45:25 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 21:28:52 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 04:34:02 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Shastri", "Aditya A.", ""], ["Tamrakar", "Deepti", ""], ["Ahuja", "Kapil", ""]]}, {"id": "1701.04018", "submitter": "Yi\\u{g}it Oktar", "authors": "Yigit Oktar, Mehmet Turkan", "title": "Boosting Dictionary Learning with Error Codes", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional sparse representations based dictionary learning algorithms,\ninitial dictionaries are generally assumed to be proper representatives of the\nsystem at hand. However, this may not be the case, especially in some systems\nrestricted to random initializations. Therefore, a supposedly optimal\nstate-update based on such an improper model might lead to undesired effects\nthat will be conveyed to successive iterations. In this paper, we propose a\ndictionary learning method which includes a general feedback process that codes\nthe intermediate error left over from a less intensive initial learning\nattempt, and then adjusts sparse codes accordingly. Experimental observations\nshow that such an additional step vastly improves rates of convergence in\nhigh-dimensional cases, also results in better converged states in the case of\nrandom initializations. Improvements also scale up with more lenient sparsity\nconstraints.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 10:12:36 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Oktar", "Yigit", ""], ["Turkan", "Mehmet", ""]]}, {"id": "1701.04043", "submitter": "Chen Longxi", "authors": "Longxi Chen, Yipeng Liu, Ce Zhu", "title": "Iterative Block Tensor Singular Value Thresholding for Extraction of Low\n  Rank Component of Image Data", "comments": "accepted by ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor principal component analysis (TPCA) is a multi-linear extension of\nprincipal component analysis which converts a set of correlated measurements\ninto several principal components. In this paper, we propose a new robust TPCA\nmethod to extract the princi- pal components of the multi-way data based on\ntensor singular value decomposition. The tensor is split into a number of\nblocks of the same size. The low rank component of each block tensor is\nextracted using iterative tensor singular value thresholding method. The prin-\ncipal components of the multi-way data are the concatenation of all the low\nrank components of all the block tensors. We give the block tensor incoherence\nconditions to guarantee the successful decom- position. This factorization has\nsimilar optimality properties to that of low rank matrix derived from singular\nvalue decomposition. Ex- perimentally, we demonstrate its effectiveness in two\napplications, including motion separation for surveillance videos and\nillumination normalization for face images.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 13:48:27 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Chen", "Longxi", ""], ["Liu", "Yipeng", ""], ["Zhu", "Ce", ""]]}, {"id": "1701.04082", "submitter": "Yusuke Uchida", "authors": "Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, Shin'ichi Satoh", "title": "Embedding Watermarks into Deep Neural Networks", "comments": null, "journal-ref": "ICMR '17 Proceedings of the 2017 ACM on International Conference\n  on Multimedia Retrieval Pages 269-277", "doi": "10.1145/3078971.3078974", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved significant progress. Sharing\ntrained models of these deep neural networks is very important in the rapid\nprogress of researching or developing deep neural network systems. At the same\ntime, it is necessary to protect the rights of shared trained models. To this\nend, we propose to use a digital watermarking technology to protect\nintellectual property or detect intellectual property infringement of trained\nmodels. Firstly, we formulate a new problem: embedding watermarks into deep\nneural networks. We also define requirements, embedding situations, and attack\ntypes for watermarking to deep neural networks. Secondly, we propose a general\nframework to embed a watermark into model parameters using a parameter\nregularizer. Our approach does not hurt the performance of networks into which\na watermark is embedded. Finally, we perform comprehensive experiments to\nreveal the potential of watermarking to deep neural networks as a basis of this\nnew problem. We show that our framework can embed a watermark in the situations\nof training a network from scratch, fine-tuning, and distilling without hurting\nthe performance of a deep neural network. The embedded watermark does not\ndisappear even after fine-tuning or parameter pruning; the watermark completely\nremains even after removing 65% of parameters were pruned. The implementation\nof this research is: https://github.com/yu4u/dnn-watermark\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 17:32:02 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 17:54:13 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Uchida", "Yusuke", ""], ["Nagai", "Yuki", ""], ["Sakazawa", "Shigeyuki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1701.04101", "submitter": "Mike Kasper", "authors": "Mike Kasper, Nima Keivan, Gabe Sibley, Christoffer Heckman", "title": "Light Source Estimation with Analytical Path-tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for light source estimation in scenes\nreconstructed with a RGB-D camera based on an analytically-derived formulation\nof path-tracing. Our algorithm traces the reconstructed scene with a custom\npath-tracer and computes the analytical derivatives of the light transport\nequation from principles in optics. These derivatives are then used to perform\ngradient descent, minimizing the photometric error between one or more captured\nreference images and renders of our current lighting estimation using an\nenvironment map parameterization for light sources. We show that our approach\nof modeling all light sources as points at infinity approximates lights located\nnear the scene with surprising accuracy. Due to the analytical formulation of\nderivatives, optimization to the solution is considerably accelerated. We\nverify our algorithm using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 19:15:39 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Kasper", "Mike", ""], ["Keivan", "Nima", ""], ["Sibley", "Gabe", ""], ["Heckman", "Christoffer", ""]]}, {"id": "1701.04128", "submitter": "Wenjie Luo", "authors": "Wenjie Luo and Yujia Li and Raquel Urtasun and Richard Zemel", "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study characteristics of receptive fields of units in deep convolutional\nnetworks. The receptive field size is a crucial issue in many visual tasks, as\nthe output must respond to large enough areas in the image to capture\ninformation about large objects. We introduce the notion of an effective\nreceptive field, and show that it both has a Gaussian distribution and only\noccupies a fraction of the full theoretical receptive field. We analyze the\neffective receptive field in several architecture designs, and the effect of\nnonlinear activations, dropout, sub-sampling and skip connections on it. This\nleads to suggestions for ways to address its tendency to be too small.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 23:52:49 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 06:32:29 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Luo", "Wenjie", ""], ["Li", "Yujia", ""], ["Urtasun", "Raquel", ""], ["Zemel", "Richard", ""]]}, {"id": "1701.04175", "submitter": "Chuong Nguyen", "authors": "Chuong V. Nguyen, Michael Milford, Robert Mahony", "title": "3D tracking of water hazards with polarized stereo cameras", "comments": "7 pages, ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current self-driving car systems operate well in sunny weather but struggle\nin adverse conditions. One of the most commonly encountered adverse conditions\ninvolves water on the road caused by rain, sleet, melting snow or flooding.\nWhile some advances have been made in using conventional RGB camera and LIDAR\ntechnology for detecting water hazards, other sources of information such as\npolarization offer a promising and potentially superior approach to this\nproblem in terms of performance and cost. In this paper, we present a novel\nstereo-polarization system for detecting and tracking water hazards based on\npolarization and color variation of reflected light, with consideration of the\neffect of polarized light from sky as function of reflection and azimuth\nangles. To evaluate this system, we present a new large `water on road'\ndatasets spanning approximately 2 km of driving in various on-road and off-road\nconditions and demonstrate for the first time reliable water detection and\ntracking over a wide range of realistic car driving water conditions using\npolarized vision as the primary sensing modality. Our system successfully\ndetects water hazards up to more than 100m. Finally, we discuss several\ninteresting challenges and propose future research directions for further\nimproving robust autonomous car perception in hazardous wet conditions using\npolarization sensors.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 05:47:30 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 07:36:42 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Nguyen", "Chuong V.", ""], ["Milford", "Michael", ""], ["Mahony", "Robert", ""]]}, {"id": "1701.04185", "submitter": "Rohit M Thanki", "authors": "Rohit M. Thanki, Ved Vyas Dwivedi and Komal R. Borisagar", "title": "A Watermarking Technique Using Discrete Curvelet Transform for Security\n  of Multiple Biometric Features", "comments": null, "journal-ref": "International Journal of Information Processing,volume 10, issue\n  1, pp. 103 - 114 (2016)", "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The robustness and security of the biometric watermarking approach can be\nimproved by using a multiple watermarking. This multiple watermarking proposed\nfor improving security of biometric features and data. When the imposter tries\nto create the spoofed biometric feature, the invisible biometric watermark\nfeatures can provide appropriate protection to multimedia data. In this paper,\na biometric watermarking technique with multiple biometric watermarks are\nproposed in which biometric features of fingerprint, face, iris and signature\nis embedded in the image. Before embedding, fingerprint, iris, face and\nsignature features are extracted using Shen-Castan edge detection and Principal\nComponent Analysis. These all biometric watermark features are embedded into\nvarious mid band frequency curvelet coefficients of host image. All four\nfingerprint features, iris features, facial features and signature features are\nthe biometric characteristics of the individual and they are used for cross\nverification and copyright protection if any manipulation occurs. The proposed\ntechnique is fragile enough; features cannot be extracted from the watermarked\nimage when an imposter tries to remove watermark features illegally. It can use\nfor multiple copyright authentication and verification.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 06:41:21 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Thanki", "Rohit M.", ""], ["Dwivedi", "Ved Vyas", ""], ["Borisagar", "Komal R.", ""]]}, {"id": "1701.04210", "submitter": "Laura Lopez-Fuentes", "authors": "Laura Lopez-Fuentes, Andrew D.Bagdanov, Joost van de Weijer, Harald\n  Skinnemoen", "title": "Bandwidth limited object recognition in high resolution imagery", "comments": "9 pages, 9 figures, accepted in WACV", "journal-ref": "Applications of Computer Vision (WACV), 2017 IEEE Winter\n  Conference on. IEEE, 2017. p. 1197-1205", "doi": "10.1109/WACV.2017.138", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method to optimize bandwidth usage for object\ndetection in critical communication scenarios. We develop two operating models\nof active information seeking. The first model identifies promising regions in\nlow resolution imagery and progressively requests higher resolution regions on\nwhich to perform recognition of higher semantic quality. The second model\nidentifies promising regions in low resolution imagery while simultaneously\npredicting the approximate location of the object of higher semantic quality.\nFrom this general framework, we develop a car recognition system via\nidentification of its license plate and evaluate the performance of both models\non a car dataset that we introduce. Results are compared with traditional JPEG\ncompression and demonstrate that our system saves up to one order of magnitude\nof bandwidth while sacrificing little in terms of recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 09:16:35 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lopez-Fuentes", "Laura", ""], ["Bagdanov", "Andrew D.", ""], ["van de Weijer", "Joost", ""], ["Skinnemoen", "Harald", ""]]}, {"id": "1701.04224", "submitter": "Chunlin Tian", "authors": "Chunlin Tian, Weijun Ji", "title": "Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and\n  Lipreading", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aduio-visual Speech Recognition (AVSR) which employs both the video and\naudio information to do Automatic Speech Recognition (ASR) is one of the\napplication of multimodal leaning making ASR system more robust and accuracy.\nThe traditional models usually treated AVSR as inference or projection but\nstrict prior limits its ability. As the revival of deep learning, Deep Neural\nNetworks (DNN) becomes an important toolkit in many traditional classification\ntasks including ASR, image classification, natural language processing. Some\nDNN models were used in AVSR like Multimodal Deep Autoencoders (MDAEs),\nMultimodal Deep Belief Network (MDBN) and Multimodal Deep Boltzmann Machine\n(MDBM) that actually work better than traditional methods. However, such DNN\nmodels have several shortcomings: (1) They don't balance the modal fusion and\ntemporal fusion, or even haven't temporal fusion; (2)The architecture of these\nmodels isn't end-to-end, the training and testing getting cumbersome. We\npropose a DNN model, Auxiliary Multimodal LSTM (am-LSTM), to overcome such\nweakness. The am-LSTM could be trained and tested once, moreover easy to train\nand preventing overfitting automatically. The extensibility and flexibility are\nalso take into consideration. The experiments show that am-LSTM is much better\nthan traditional methods and other DNN models in three datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 10:08:22 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 14:57:06 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Tian", "Chunlin", ""], ["Ji", "Weijun", ""]]}, {"id": "1701.04249", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Geometric features for voxel-based surface recognition", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a library of geometric voxel features for CAD surface\nrecognition/retrieval tasks. Our features include local versions of the\nintrinsic volumes (the usual 3D volume, surface area, integrated mean and\nGaussian curvature) and a few closely related quantities. We also compute Haar\nwavelet and statistical distribution features by aggregating raw voxel\nfeatures. We apply our features to object classification on the ESB data set\nand demonstrate accurate results with a small number of shallow decision trees.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:30:31 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1701.04256", "submitter": "Andrea Baraldi", "authors": "Andrea Baraldi", "title": "Automatic Spatial Context-Sensitive Cloud/Cloud-Shadow Detection in\n  Multi-Source Multi-Spectral Earth Observation Images: AutoCloud+", "comments": "Invitation to tender ESA/AO/1 8373/15/I NB, VAE: Next Generation EO\n  based Information Services", "journal-ref": null, "doi": "10.13140/RG.2.2.34162.71363", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed Earth observation (EO) based value adding system (EO VAS),\nhereafter identified as AutoCloud+, consists of an innovative EO image\nunderstanding system (EO IUS) design and implementation capable of automatic\nspatial context sensitive cloud/cloud shadow detection in multi source multi\nspectral (MS) EO imagery, whether or not radiometrically calibrated, acquired\nby multiple platforms, either spaceborne or airborne, including unmanned aerial\nvehicles (UAVs). It is worth mentioning that the same EO IUS architecture is\nsuitable for a large variety of EO based value adding products and services,\nincluding: (i) low level image enhancement applications, such as automatic MS\nimage topographic correction, co registration, mosaicking and compositing, (ii)\nhigh level MS image land cover (LC) and LC change (LCC) classification and\n(iii) content based image storage/retrieval in massive multi source EO image\ndatabases (big data mining).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:58:40 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Baraldi", "Andrea", ""]]}, {"id": "1701.04284", "submitter": "Dominik Alexander Klein", "authors": "Dominik Alexander Klein, Boris Illing, Bastian Gaspers, Dirk Schulz,\n  Armin Bernd Cremers", "title": "Hierarchical Salient Object Detection for Assisted Grasping", "comments": "Accepted for ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual scene decomposition into semantic entities is one of the major\nchallenges when creating a reliable object grasping system. Recently, we\nintroduced a bottom-up hierarchical clustering approach which is able to\nsegment objects and parts in a scene. In this paper, we introduce a transform\nfrom such a segmentation into a corresponding, hierarchical saliency function.\nIn comprehensive experiments we demonstrate its ability to detect salient\nobjects in a scene. Furthermore, this hierarchical saliency defines a most\nsalient corresponding region (scale) for every point in an image. Based on\nthis, an easy-to-use pick and place manipulation system was developed and\ntested exemplarily.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 13:21:35 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 12:29:05 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Klein", "Dominik Alexander", ""], ["Illing", "Boris", ""], ["Gaspers", "Bastian", ""], ["Schulz", "Dirk", ""], ["Cremers", "Armin Bernd", ""]]}, {"id": "1701.04520", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Yee Hui Lee, Stefan Winkler", "title": "Systematic study of color spaces and components for the segmentation of\n  sky/cloud images", "comments": "Published in Proc. IEEE International Conference on Image Processing\n  (ICIP), Oct. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sky/cloud imaging using ground-based Whole Sky Imagers (WSI) is a\ncost-effective means to understanding cloud cover and weather patterns. The\naccurate segmentation of clouds in these images is a challenging task, as\nclouds do not possess any clear structure. Several algorithms using different\ncolor models have been proposed in the literature. This paper presents a\nsystematic approach for the selection of color spaces and components for\noptimal segmentation of sky/cloud images. Using mainly principal component\nanalysis (PCA) and fuzzy clustering for evaluation, we identify the most\nsuitable color components for this task.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 03:27:56 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1701.04540", "submitter": "Joy Egede", "authors": "Joy Egede, Michel Valstar and Brais Martinez", "title": "Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and\n  Dynamics for Automatic Pain Estimation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic continuous time, continuous value assessment of a patient's pain\nfrom face video is highly sought after by the medical profession. Despite the\nrecent advances in deep learning that attain impressive results in many\ndomains, pain estimation risks not being able to benefit from this due to the\ndifficulty in obtaining data sets of considerable size. In this work we propose\na combination of hand-crafted and deep-learned features that makes the most of\ndeep learning techniques in small sample settings. Encoding shape, appearance,\nand dynamics, our method significantly outperforms the current state of the\nart, attaining a RMSE error of less than 1 point on a 16-level pain scale,\nwhilst simultaneously scoring a 67.3% Pearson correlation coefficient between\nour predicted pain level time series and the ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 06:05:48 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Egede", "Joy", ""], ["Valstar", "Michel", ""], ["Martinez", "Brais", ""]]}, {"id": "1701.04568", "submitter": "Mahesh Gorijala", "authors": "Mahesh Gorijala, Ambedkar Dukkipati", "title": "Image Generation and Editing with Variational Info Generative\n  AdversarialNetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an enormous interest in generative models for images\nin deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and\nVariational Auto-Encoder (VAE) have surfaced as two most prominent and popular\nmodels. While VAEs tend to produce excellent reconstructions but blurry\nsamples, GANs generate sharp but slightly distorted images. In this paper we\npropose a new model called Variational InfoGAN (ViGAN). Our aim is two fold:\n(i) To generated new images conditioned on visual descriptions, and (ii) modify\nthe image, by fixing the latent representation of image and varying the visual\ndescription. We evaluate our model on Labeled Faces in the Wild (LFW), celebA\nand a modified version of MNIST datasets and demonstrate the ability of our\nmodel to generate new images as well as to modify a given image by changing\nattributes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 08:48:28 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Gorijala", "Mahesh", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1701.04658", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis and Jordi Pont-Tuset and Pablo Arbel\\'aez and\n  Luc Van Gool", "title": "Convolutional Oriented Boundaries: From Image Segmentation to High-Level\n  Tasks", "comments": "Accepted by T-PAMI. Extended version of \"Convolutional Oriented\n  Boundaries\", ECCV 2016 (arXiv:1608.02755). Project page:\n  http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Convolutional Oriented Boundaries (COB), which produces multiscale\noriented contours and region hierarchies starting from generic image\nclassification Convolutional Neural Networks (CNNs). COB is computationally\nefficient, because it requires a single CNN forward pass for multi-scale\ncontour detection and it uses a novel sparse boundary representation for\nhierarchical segmentation; it gives a significant leap in performance over the\nstate-of-the-art, and it generalizes very well to unseen categories and\ndatasets. Particularly, we show that learning to estimate not only contour\nstrength but also orientation provides more accurate results. We perform\nextensive experiments for low-level applications on BSDS, PASCAL Context,\nPASCAL Segmentation, and NYUD to evaluate boundary detection performance,\nshowing that COB provides state-of-the-art contours and region hierarchies in\nall datasets. We also evaluate COB on high-level tasks when coupled with\nmultiple pipelines for object proposals, semantic contours, semantic\nsegmentation, and object detection on MS-COCO, SBD, and PASCAL; showing that\nCOB also improves the results for all tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 13:04:33 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 17:08:42 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Pont-Tuset", "Jordi", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1701.04674", "submitter": "Ron Dekel", "authors": "Ron Dekel", "title": "Human perception in computer vision", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has made remarkable progress in recent years. Deep neural\nnetwork (DNN) models optimized to identify objects in images exhibit\nunprecedented task-trained accuracy and, remarkably, some generalization\nability: new visual problems can now be solved more easily based on previous\nlearning. Biological vision (learned in life and through evolution) is also\naccurate and general-purpose. Is it possible that these different learning\nregimes converge to similar problem-dependent optimal computations? We\ntherefore asked whether the human system-level computation of visual perception\nhas DNN correlates and considered several anecdotal test cases. We found that\nperceptual sensitivity to image changes has DNN mid-computation correlates,\nwhile sensitivity to segmentation, crowding and shape has DNN end-computation\ncorrelates. Our results quantify the applicability of using DNN computation to\nestimate perceptual loss, and are consistent with the fascinating theoretical\nview that properties of human perception are a consequence of\narchitecture-independent visual learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:00:30 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Dekel", "Ron", ""]]}, {"id": "1701.04743", "submitter": "Suvam Patra", "authors": "Suvam Patra and Himanshu Aggarwal and Himani Arora and Chetan Arora\n  and Subhashis Banerjee", "title": "Computing Egomotion with Local Loop Closures for Egocentric Videos", "comments": "Accepted in WACV 2017", "journal-ref": null, "doi": "10.1109/WACV.2017.57", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the camera pose is an important step in many egocentric video\napplications. It has been widely reported that, state of the art SLAM\nalgorithms fail on egocentric videos. In this paper, we propose a robust method\nfor camera pose estimation, designed specifically for egocentric videos. In an\negocentric video, the camera views the same scene point multiple times as the\nwearer's head sweeps back and forth. We use this specific motion profile to\nperform short loop closures aligned with wearer's footsteps. For egocentric\nvideos, depth estimation is usually noisy. In an important departure, we use 2D\ncomputations for rotation averaging which do not rely upon depth estimates. The\ntwo modification results in much more stable algorithm as is evident from our\nexperiments on various egocentric video datasets for different egocentric\napplications. The proposed algorithm resolves a long standing problem in\negocentric vision and unlocks new usage scenarios for future applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 16:08:02 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Patra", "Suvam", ""], ["Aggarwal", "Himanshu", ""], ["Arora", "Himani", ""], ["Arora", "Chetan", ""], ["Banerjee", "Subhashis", ""]]}, {"id": "1701.04752", "submitter": "Xinhan Di", "authors": "Xinhan Di and Pengqian Yu", "title": "3D Reconstruction of Simple Objects from A Single View Silhouette Image", "comments": "Submitted Nov 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent deep neural networks have achieved promising results for 3D\nreconstruction from a single-view image, these rely on the availability of RGB\ntextures in images and extra information as supervision. In this work, we\npropose novel stacked hierarchical networks and an end to end training strategy\nto tackle a more challenging task for the first time, 3D reconstruction from a\nsingle-view 2D silhouette image. We demonstrate that our model is able to\nconduct 3D reconstruction from a single-view silhouette image both\nqualitatively and quantitatively. Evaluation is performed using Shapenet for\nthe single-view reconstruction and results are presented in comparison with a\nsingle network, to highlight the improvements obtained with the proposed\nstacked networks and the end to end training strategy. Furthermore, 3D re-\nconstruction in forms of IoU is compared with the state of art 3D\nreconstruction from a single-view RGB image, and the proposed model achieves\nhigher IoU than the state of art of reconstruction from a single view RGB\nimage.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 16:39:32 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""]]}, {"id": "1701.04769", "submitter": "Unaiza Ahsan", "authors": "Unaiza Ahsan, Chen Sun, James Hays and Irfan Essa", "title": "Complex Event Recognition from Images with Few Training Examples", "comments": "Accepted to Winter Applications of Computer Vision (WACV'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to leverage concept-level representations for complex event\nrecognition in photographs given limited training examples. We introduce a\nnovel framework to discover event concept attributes from the web and use that\nto extract semantic features from images and classify them into social event\ncategories with few training examples. Discovered concepts include a variety of\nobjects, scenes, actions and event sub-types, leading to a discriminative and\ncompact representation for event images. Web images are obtained for each\ndiscovered event concept and we use (pretrained) CNN features to train concept\nclassifiers. Extensive experiments on challenging event datasets demonstrate\nthat our proposed method outperforms several baselines using deep CNN features\ndirectly in classifying images into events with limited training examples. We\nalso demonstrate that our method achieves the best overall accuracy on a\ndataset with unseen event categories using a single training example.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:16:55 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Ahsan", "Unaiza", ""], ["Sun", "Chen", ""], ["Hays", "James", ""], ["Essa", "Irfan", ""]]}, {"id": "1701.04851", "submitter": "Forrester Cole", "authors": "Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar\n  Mosseri, William T. Freeman", "title": "Synthesizing Normalized Faces from Facial Identity Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for synthesizing a frontal, neutral-expression image of a\nperson's face given an input face photograph. This is achieved by learning to\ngenerate facial landmarks and textures from features extracted from a\nfacial-recognition network. Unlike previous approaches, our encoding feature\nvector is largely invariant to lighting, pose, and facial expression.\nExploiting this invariance, we train our decoder network using only frontal,\nneutral-expression photographs. Since these photographs are well aligned, we\ncan decompose them into a sparse set of landmark points and aligned texture\nmaps. The decoder then predicts landmarks and textures independently and\ncombines them using a differentiable image warping operation. The resulting\nimages can be used for a number of applications, such as analyzing facial\nattributes, exposure and white balance adjustment, or creating a 3-D avatar.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 20:03:46 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 12:52:59 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 14:27:34 GMT"}, {"version": "v4", "created": "Tue, 17 Oct 2017 15:27:21 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Cole", "Forrester", ""], ["Belanger", "David", ""], ["Krishnan", "Dilip", ""], ["Sarna", "Aaron", ""], ["Mosseri", "Inbar", ""], ["Freeman", "William T.", ""]]}, {"id": "1701.04923", "submitter": "Jie Lin", "authors": "Vijay Chandrasekhar, Jie Lin, Qianli Liao, Olivier Mor\\`ere, Antoine\n  Veillard, Lingyu Duan, Tomaso Poggio", "title": "Compression of Deep Neural Networks for Image Instance Retrieval", "comments": "10 pages, accepted by DCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image instance retrieval is the problem of retrieving images from a database\nwhich contain the same object. Convolutional Neural Network (CNN) based\ndescriptors are becoming the dominant approach for generating {\\it global image\ndescriptors} for the instance retrieval problem. One major drawback of\nCNN-based {\\it global descriptors} is that uncompressed deep neural network\nmodels require hundreds of megabytes of storage making them inconvenient to\ndeploy in mobile applications or in custom hardware. In this work, we study the\nproblem of neural network model compression focusing on the image instance\nretrieval task. We study quantization, coding, pruning and weight sharing\ntechniques for reducing model size for the instance retrieval problem. We\nprovide extensive experimental results on the trade-off between retrieval\nperformance and model size for different types of networks on several data sets\nproviding the most comprehensive study on this topic. We compress models to the\norder of a few MBs: two orders of magnitude smaller than the uncompressed\nmodels while achieving negligible loss in retrieval performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:57:55 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Chandrasekhar", "Vijay", ""], ["Lin", "Jie", ""], ["Liao", "Qianli", ""], ["Mor\u00e8re", "Olivier", ""], ["Veillard", "Antoine", ""], ["Duan", "Lingyu", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1701.04925", "submitter": "Fahimeh Rezazadegan", "authors": "Fahimeh Rezazadegan, Sareh Shirazi, Ben Upcroft and Michael Milford", "title": "Action Recognition: From Static Datasets to Moving Robots", "comments": null, "journal-ref": "Robotics and Automation (ICRA), 2017 IEEE International Conference\n  on", "doi": "10.1109/ICRA.2017.7989361", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have achieved state-of-the- art performance in\nrecognizing human activities, but often rely on utilizing background cues\npresent in typical computer vision datasets that predominantly have a\nstationary camera. If these models are to be employed by autonomous robots in\nreal world environments, they must be adapted to perform independently of\nbackground cues and camera motion effects. To address these challenges, we\npropose a new method that firstly generates generic action region proposals\nwith good potential to locate one human action in unconstrained videos\nregardless of camera motion and then uses action proposals to extract and\nclassify effective shape and motion features by a ConvNet framework. In a range\nof experiments, we demonstrate that by actively proposing action regions during\nboth training and testing, state-of-the-art or better performance is achieved\non benchmarks. We show the outperformance of our approach compared to the\nstate-of-the-art in two new datasets; one emphasizes on irrelevant background,\nthe other highlights the camera motion. We also validate our action recognition\nmethod in an abnormal behavior detection scenario to improve workplace safety.\nThe results verify a higher success rate for our method due to the ability of\nour system to recognize human actions regardless of environment and camera\nmotion.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 02:10:56 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Rezazadegan", "Fahimeh", ""], ["Shirazi", "Sareh", ""], ["Upcroft", "Ben", ""], ["Milford", "Michael", ""]]}, {"id": "1701.04928", "submitter": "Bhautik Joshi", "authors": "Bhautik Joshi, Kristen Stewart, David Shapiro", "title": "Bringing Impressionism to Life with Neural Style Transfer in Come Swim", "comments": "3 pages, 6 figures, paper is a case study of how Neural Style\n  Transfer can be used in a movie production context", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer is a striking, recently-developed technique that uses\nneural networks to artistically redraw an image in the style of a source style\nimage. This paper explores the use of this technique in a production setting,\napplying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style\nof the impressionistic painting that inspired the film. We document how the\ntechnique can be driven within the framework of an iterative creative process\nto achieve a desired look, and propose a mapping of the broad parameter space\nto a key set of creative controls. We hope that this mapping can provide\ninsights into priorities for future research.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 02:37:48 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Joshi", "Bhautik", ""], ["Stewart", "Kristen", ""], ["Shapiro", "David", ""]]}, {"id": "1701.04949", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Eric Chalmers, Artur Luczak", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in\n  Caffe", "comments": "21 pages, 11 figures, 5 tables, 62 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of several models of a deep convolutional\nauto-encoder in the Caffe deep learning framework and their experimental\nevaluation on the example of MNIST dataset. We have created five models of a\nconvolutional auto-encoder which differ architecturally by the presence or\nabsence of pooling and unpooling layers in the auto-encoder's encoder and\ndecoder parts. Our results show that the developed models provide very good\nresults in dimensionality reduction and unsupervised clustering tasks, and\nsmall classification errors when we used the learned internal code as an input\nof a supervised linear classifier and multi-layer perceptron. The best results\nwere provided by a model where the encoder part contains convolutional and\npooling layers, followed by an analogous decoder part with deconvolution and\nunpooling layers without the use of switch variables in the decoder part. The\npaper also discusses practical details of the creation of a deep convolutional\nauto-encoder in the very popular Caffe deep learning framework. We believe that\nour approach and results presented in this paper could help other researchers\nto build efficient deep neural network architectures in the future.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 05:24:24 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Chalmers", "Eric", ""], ["Luczak", "Artur", ""]]}, {"id": "1701.05003", "submitter": "Yang Wang", "authors": "Yang Wang, Xuemin Lin, Lin Wu, Wenjie Zhang", "title": "Effective Multi-Query Expansions: Collaborative Deep Networks for Robust\n  Landmark Retrieval", "comments": "Accepted to Appear in IEEE Trans on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2655449", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query photo issued by a user (q-user), the landmark retrieval is to\nreturn a set of photos with their landmarks similar to those of the query,\nwhile the existing studies on the landmark retrieval focus on exploiting\ngeometries of landmarks for similarity matches between candidate photos and a\nquery photo. We observe that the same landmarks provided by different users\nover social media community may convey different geometry information depending\non the viewpoints and/or angles, and may subsequently yield very different\nresults. In fact, dealing with the landmarks with \\illshapes caused by the\nphotography of q-users is often nontrivial and has seldom been studied. In this\npaper we propose a novel framework, namely multi-query expansions, to retrieve\nsemantically robust landmarks by two steps. Firstly, we identify the top-$k$\nphotos regarding the latent topics of a query landmark to construct multi-query\nset so as to remedy its possible \\illshape. For this purpose, we significantly\nextend the techniques of Latent Dirichlet Allocation. Then, motivated by the\ntypical \\emph{collaborative filtering} methods, we propose to learn a\n\\emph{collaborative} deep networks based semantically, nonlinear and high-level\nfeatures over the latent factor for landmark photo as the training set, which\nis formed by matrix factorization over \\emph{collaborative} user-photo matrix\nregarding the multi-query set. The learned deep network is further applied to\ngenerate the features for all the other photos, meanwhile resulting into a\ncompact multi-query set within such space. Extensive experiments are conducted\non real-world social media data with both landmark photos together with their\nuser information to show the superior performance over the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 10:48:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Wang", "Yang", ""], ["Lin", "Xuemin", ""], ["Wu", "Lin", ""], ["Zhang", "Wenjie", ""]]}, {"id": "1701.05013", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, Isabel Pino Pe\\~na, Jesper Holst Pedersen, David\n  A. Lynch, Lauge S{\\o}rensen, Marleen de Bruijne", "title": "Transfer learning for multi-center classification of chronic obstructive\n  pulmonary disease", "comments": "Accepted at Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2017.2769800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic obstructive pulmonary disease (COPD) is a lung disease which can be\nquantified using chest computed tomography (CT) scans. Recent studies have\nshown that COPD can be automatically diagnosed using weakly supervised learning\nof intensity and texture distributions. However, up till now such classifiers\nhave only been evaluated on scans from a single domain, and it is unclear\nwhether they would generalize across domains, such as different scanners or\nscanning protocols. To address this problem, we investigate classification of\nCOPD in a multi-center dataset with a total of 803 scans from three different\ncenters, four different scanners, with heterogenous subject distributions. Our\nmethod is based on Gaussian texture features, and a weighted logistic\nclassifier, which increases the weights of samples similar to the test data. We\nshow that Gaussian texture features outperform intensity features previously\nused in multi-center classification tasks. We also show that a weighting\nstrategy based on a classifier that is trained to discriminate between scans\nfrom different domains, can further improve the results. To encourage further\nresearch into transfer learning methods for classification of COPD, upon\nacceptance of the paper we will release two feature datasets used in this study\non http://bigr.nl/research/projects/copd\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 11:13:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 14:10:34 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Pe\u00f1a", "Isabel Pino", ""], ["Pedersen", "Jesper Holst", ""], ["Lynch", "David A.", ""], ["S\u00f8rensen", "Lauge", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1701.05083", "submitter": "Mohammad Amin Khorsandi", "authors": "M. A. Khorsandi, N. Karimi, S. Samavi", "title": "A Novel Architecture for Computing Approximate Radon Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radon transform is a type of transform which is used in image processing to\ntransfer the image into intercept-slope coordinate. Its diagonal properties\nmade it appropriate for some applications which need processes in different\ndegrees. Radon transform computation needs a lot of arithmetic operations which\nmakes it a compute-intensive algorithm. In literature an approximate algorithm\nfor computing Radon transform is introduces which reduces the complexity of\ncomputations. But this algorithm is complex and need arbitrary accesses to\nmemory. In this paper we proposed an algorithm which accesses to memory\nsequentially. In the following an architecture is introduced which uses\npipeline to reduce the time complexity of algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 11:48:08 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Khorsandi", "M. A.", ""], ["Karimi", "N.", ""], ["Samavi", "S.", ""]]}, {"id": "1701.05084", "submitter": "Ni Chen", "authors": "Ni Chen, Zhenbo Ren, Dayan Li, Edmund Y. Lam, and Guohai Situ", "title": "Analysis of the noise in back-projection light field acquisition and its\n  optimization", "comments": null, "journal-ref": null, "doi": "10.1364/AO.56.000F20", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field reconstruction from images captured by focal plane sweeping can\nachieve high lateral resolution comparable to the modern camera sensor. This is\nimpossible for the conventional micro-lenslet based light field capture\nsystems. However, the severe defocus noise and the low depth resolution limit\nits applications. In this paper, we analyze the defocus noise and the depth\nresolution in the focal plane sweeping based light field reconstruction\ntechnique, and propose a method to reduce the defocus noise and improve the\ndepth resolution. Both numerical and experimental results verify the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 01:09:36 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Chen", "Ni", ""], ["Ren", "Zhenbo", ""], ["Li", "Dayan", ""], ["Lam", "Edmund Y.", ""], ["Situ", "Guohai", ""]]}, {"id": "1701.05088", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Temporal scale selection in time-causal scale space", "comments": "44 pages, 15 figures, 10 tables in Journal of Mathematical Imaging\n  and Vision, 2017", "journal-ref": null, "doi": "10.1007/s10851-016-0691-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing and developing scale selection mechanisms for generating\nhypotheses about characteristic scales in signals, it is essential that the\nselected scale levels reflect the extent of the underlying structures in the\nsignal.\n  This paper presents a theory and in-depth theoretical analysis about the\nscale selection properties of methods for automatically selecting local\ntemporal scales in time-dependent signals based on local extrema over temporal\nscales of scale-normalized temporal derivative responses. Specifically, this\npaper develops a novel theoretical framework for performing such temporal scale\nselection over a time-causal and time-recursive temporal domain as is necessary\nwhen processing continuous video or audio streams in real time or when\nmodelling biological perception.\n  For a recently developed time-causal and time-recursive scale-space concept\ndefined by convolution with a scale-invariant limit kernel, we show that it is\npossible to transfer a large number of the desirable scale selection properties\nthat hold for the Gaussian scale-space concept over a non-causal temporal\ndomain to this temporal scale-space concept over a truly time-causal domain.\nSpecifically, we show that for this temporal scale-space concept, it is\npossible to achieve true temporal scale invariance although the temporal scale\nlevels have to be discrete, which is a novel theoretical construction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 09:48:49 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1701.05105", "submitter": "Zetao Chen", "authors": "Zetao Chen, Adam Jacobson, Niko Sunderhauf, Ben Upcroft, Lingqiao Liu,\n  Chunhua Shen, Ian Reid and Michael Milford", "title": "Deep Learning Features at Scale for Visual Place Recognition", "comments": "8 pages, 10 figures. Accepted by International Conference on Robotics\n  and Automation (ICRA) 2017. This is the submitted version. The final\n  published version may be slightly different", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning techniques in the computer vision domain has\ntriggered a range of initial investigations into their utility for visual place\nrecognition, all using generic features from networks that were trained for\nother types of recognition tasks. In this paper, we train, at large scale, two\nCNN architectures for the specific place recognition task and employ a\nmulti-scale feature encoding method to generate condition- and\nviewpoint-invariant features. To enable this training to occur, we have\ndeveloped a massive Specific PlacEs Dataset (SPED) with hundreds of examples of\nplace appearance change at thousands of different places, as opposed to the\nsemantic place type datasets currently available. This new dataset enables us\nto set up a training regime that interprets place recognition as a\nclassification problem. We comprehensively evaluate our trained networks on\nseveral challenging benchmark place recognition datasets and demonstrate that\nthey achieve an average 10% increase in performance over other place\nrecognition algorithms and pre-trained CNNs. By analyzing the network responses\nand their differences from pre-trained networks, we provide insights into what\na network learns when training for place recognition, and what these results\nsignify for future research in this area.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:28:03 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Chen", "Zetao", ""], ["Jacobson", "Adam", ""], ["Sunderhauf", "Niko", ""], ["Upcroft", "Ben", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Milford", "Michael", ""]]}, {"id": "1701.05221", "submitter": "Nikolaos Fragoulis Dr", "authors": "I. Theodorakopoulos, V. Pothos, D. Kastaniotis and N. Fragoulis", "title": "Parsimonious Inference on Convolutional Neural Networks: Learning and\n  applying on-line kernel activation rules", "comments": "17 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new, radical CNN design approach is presented in this paper, considering\nthe reduction of the total computational load during inference. This is\nachieved by a new holistic intervention on both the CNN architecture and the\ntraining procedure, which targets to the parsimonious inference by learning to\nexploit or remove the redundant capacity of a CNN architecture. This is\naccomplished, by the introduction of a new structural element that can be\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\nor even improving its recognition accuracy. Our approach formulates a\nsystematic and data-driven method for developing CNNs that are trained to\neventually change size and form in real-time during inference, targeting to the\nsmaller possible computational footprint. Results are provided for the optimal\nimplementation on a few modern, high-end mobile computing platforms indicating\na significant speed-up of up to x3 times.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:03:12 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 06:43:02 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 08:57:29 GMT"}, {"version": "v4", "created": "Thu, 26 Jan 2017 08:58:52 GMT"}, {"version": "v5", "created": "Tue, 31 Jan 2017 12:15:43 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Theodorakopoulos", "I.", ""], ["Pothos", "V.", ""], ["Kastaniotis", "D.", ""], ["Fragoulis", "N.", ""]]}, {"id": "1701.05268", "submitter": "Martin Rais PhD", "authors": "Martin Rais, Gabriele Facciolo, Enric Meinhardt-Llopis, Jean-Michel\n  Morel, Antoni Buades, and Bartomeu Coll", "title": "Accurate Motion Estimation through Random Sample Aggregated Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider the classic problem of estimating accurately a 2D\ntransformation from point matches between images containing outliers. RANSAC\ndiscriminates outliers by randomly generating minimalistic sampled hypotheses\nand verifying their consensus over the input data. Its response is based on the\nsingle hypothesis that obtained the largest inlier support. In this article we\nshow that the resulting accuracy can be improved by aggregating all generated\nhypotheses. This yields RANSAAC, a framework that improves systematically over\nRANSAC and its state-of-the-art variants by statistically aggregating\nhypotheses. To this end, we introduce a simple strategy that allows to rapidly\naverage 2D transformations, leading to an almost negligible extra computational\ncost. We give practical applications on projective transforms and\nhomography+distortion models and demonstrate a significant performance gain in\nboth cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 01:13:41 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Rais", "Martin", ""], ["Facciolo", "Gabriele", ""], ["Meinhardt-Llopis", "Enric", ""], ["Morel", "Jean-Michel", ""], ["Buades", "Antoni", ""], ["Coll", "Bartomeu", ""]]}, {"id": "1701.05349", "submitter": "Suyog Jain", "authors": "Suyog Dutt Jain, Bo Xiong, Kristen Grauman", "title": "Pixel Objectness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end learning framework for generating foreground object\nsegmentations. Given a single novel image, our approach produces pixel-level\nmasks for all \"object-like\" regions---even for object categories never seen\nduring training. We formulate the task as a structured prediction problem of\nassigning foreground/background labels to all pixels, implemented using a deep\nfully convolutional network. Key to our idea is training with a mix of\nimage-level object category examples together with relatively few images with\nboundary-level annotations. Our method substantially improves the\nstate-of-the-art on foreground segmentation for ImageNet and MIT Object\nDiscovery datasets. Furthermore, on over 1 million images, we show that it\ngeneralizes well to segment object categories unseen in the foreground maps\nused for training. Finally, we demonstrate how our approach benefits image\nretrieval and image retargeting, both of which flourish when given our\nhigh-quality foreground maps.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 09:48:45 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 07:36:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Jain", "Suyog Dutt", ""], ["Xiong", "Bo", ""], ["Grauman", "Kristen", ""]]}, {"id": "1701.05360", "submitter": "James Booth", "authors": "James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George\n  Trigeorgis, Yannis Panagakis, and Stefanos Zafeiriou", "title": "3D Face Morphable Models \"In-the-Wild\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Morphable Models (3DMMs) are powerful statistical models of 3D facial\nshape and texture, and among the state-of-the-art methods for reconstructing\nfacial shape from single images. With the advent of new 3D sensors, many 3D\nfacial datasets have been collected containing both neutral as well as\nexpressive faces. However, all datasets are captured under controlled\nconditions. Thus, even though powerful 3D facial shape models can be learnt\nfrom such data, it is difficult to build statistical texture models that are\nsufficient to reconstruct faces captured in unconstrained conditions\n(\"in-the-wild\"). In this paper, we propose the first, to the best of our\nknowledge, \"in-the-wild\" 3DMM by combining a powerful statistical model of\nfacial shape, which describes both identity and expression, with an\n\"in-the-wild\" texture model. We show that the employment of such an\n\"in-the-wild\" texture model greatly simplifies the fitting procedure, because\nthere is no need to optimize with regards to the illumination parameters.\nFurthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary\nimages. Finally, we have captured the first 3D facial database with relatively\nunconstrained conditions and report quantitative evaluations with\nstate-of-the-art performance. Complementary qualitative reconstruction results\nare demonstrated on standard \"in-the-wild\" facial databases. An open source\nimplementation of our technique is released as part of the Menpo Project.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:27:38 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Booth", "James", ""], ["Antonakos", "Epameinondas", ""], ["Ploumpis", "Stylianos", ""], ["Trigeorgis", "George", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1701.05377", "submitter": "Uwe Springmann", "authors": "Florian Fink, Klaus-U. Schulz and Uwe Springmann", "title": "Profiling of OCR'ed Historical Texts Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of ground truth it is not possible to automatically determine\nthe exact spectrum and occurrences of OCR errors in an OCR'ed text. Yet, for\ninteractive postcorrection of OCR'ed historical printings it is extremely\nuseful to have a statistical profile available that provides an estimate of\nerror classes with associated frequencies, and that points to conjectured\nerrors and suspicious tokens. The method introduced in Reffle (2013) computes\nsuch a profile, combining lexica, pattern sets and advanced matching techniques\nin a specialized Expectation Maximization (EM) procedure. Here we improve this\nmethod in three respects: First, the method in Reffle (2013) is not adaptive:\nuser feedback obtained by actual postcorrection steps cannot be used to compute\nrefined profiles. We introduce a variant of the method that is open for\nadaptivity, taking correction steps of the user into account. This leads to\nhigher precision with respect to recognition of erroneous OCR tokens. Second,\nduring postcorrection often new historical patterns are found. We show that\nadding new historical patterns to the linguistic background resources leads to\na second kind of improvement, enabling even higher precision by telling\nhistorical spellings apart from OCR errors. Third, the method in Reffle (2013)\ndoes not make any active use of tokens that cannot be interpreted in the\nunderlying channel model. We show that adding these uninterpretable tokens to\nthe set of conjectured errors leads to a significant improvement of the recall\nfor error detection, at the same time improving precision.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 11:31:14 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Fink", "Florian", ""], ["Schulz", "Klaus-U.", ""], ["Springmann", "Uwe", ""]]}, {"id": "1701.05384", "submitter": "Suyog Jain", "authors": "Suyog Dutt Jain, Bo Xiong, Kristen Grauman", "title": "FusionSeg: Learning to combine motion and appearance for fully automatic\n  segmention of generic objects in videos", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end learning framework for segmenting generic objects in\nvideos. Our method learns to combine appearance and motion information to\nproduce pixel level segmentation masks for all prominent objects in videos. We\nformulate this task as a structured prediction problem and design a two-stream\nfully convolutional neural network which fuses together motion and appearance\nin a unified framework. Since large-scale video datasets with pixel level\nsegmentations are problematic, we show how to bootstrap weakly annotated videos\ntogether with existing image recognition datasets for training. Through\nexperiments on three challenging video segmentation benchmarks, our method\nsubstantially improves the state-of-the-art for segmenting generic (unseen)\nobjects. Code and pre-trained models are available on the project website.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 12:16:30 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:09:46 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Jain", "Suyog Dutt", ""], ["Xiong", "Bo", ""], ["Grauman", "Kristen", ""]]}, {"id": "1701.05412", "submitter": "Xin Yuan", "authors": "Xin Yuan, Gang Huang, Hong Jiang, Paul Wilford", "title": "Block-wise Lensless Compressive Camera", "comments": "5 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing lensless compressive camera\n($\\text{L}^2\\text{C}^2$)~\\cite{Huang13ICIP} suffers from low capture rates,\nresulting in low resolution images when acquired over a short time. In this\nwork, we propose a new regime to mitigate these drawbacks. We replace the\nglobal-based compressive sensing used in the existing $\\text{L}^2\\text{C}^2$ by\nthe local block (patch) based compressive sensing. We use a single sensor for\neach block, rather than for the entire image, thus forming a multiple but\nspatially parallel sensor $\\text{L}^2\\text{C}^2$. This new camera retains the\nadvantages of existing $\\text{L}^2\\text{C}^2$ while leading to the following\nadditional benefits: 1) Since each block can be very small, {\\em e.g.}$~8\\times\n8$ pixels, we only need to capture $\\sim 10$ measurements to achieve reasonable\nreconstruction. Therefore the capture time can be reduced significantly. 2) The\ncoding patterns used in each block can be the same, therefore the sensing\nmatrix is only of the block size compared to the entire image size in existing\n$\\text{L}^2\\text{C}^2$. This saves the memory requirement of the sensing matrix\nas well as speeds up the reconstruction. 3) Patch based image reconstruction is\nfast and since real time stitching algorithms exist, we can perform real time\nreconstruction. 4) These small blocks can be integrated to any desirable\nnumber, leading to ultra high resolution images while retaining fast capture\nrate and fast reconstruction. We develop multiple geometries of this block-wise\n$\\text{L}^2\\text{C}^2$ in this paper. We have built prototypes of the proposed\nblock-wise $\\text{L}^2\\text{C}^2$ and demonstrated excellent results of real\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 13:45:37 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Yuan", "Xin", ""], ["Huang", "Gang", ""], ["Jiang", "Hong", ""], ["Wilford", "Paul", ""]]}, {"id": "1701.05419", "submitter": "Mario Corsolini", "authors": "Mario Corsolini and Andrea Carta", "title": "Moving to VideoKifu: the last steps toward a fully automatic\n  record-keeping of a Go game", "comments": "20 pages, 14 figures. Accepted for publication in the \"Journal of\n  Baduk Studies\", datasets available from http://www.oipaz.net/PhotoKifu.html", "journal-ref": "Journal of Baduk Studies, 13(2):45-63, December 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper [ arXiv:1508.03269 ] we described the techniques we\nsuccessfully employed for automatically reconstructing the whole move sequence\nof a Go game by means of a set of pictures. Now we describe how it is possible\nto reconstruct the move sequence by means of a video stream (which may be\nprovided by an unattended webcam), possibly in real-time. Although the basic\nalgorithms remain the same, we will discuss the new problems that arise when\ndealing with videos, with special care for the ones that could block a\nreal-time analysis and require an improvement of our previous techniques or\neven a completely brand new approach. Eventually we present a number of\npreliminary but positive experimental results supporting the effectiveness of\nthe software we are developing, built on the ideas here outlined.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:06:08 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Corsolini", "Mario", ""], ["Carta", "Andrea", ""]]}, {"id": "1701.05432", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Piotr Koniusz, Stephen Gould", "title": "Higher-order Pooling of CNN Features via Kernel Linearization for Action\n  Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful deep learning algorithms for action recognition extend models\ndesigned for image-based tasks such as object recognition to video. Such\nextensions are typically trained for actions on single video frames or very\nshort clips, and then their predictions from sliding-windows over the video\nsequence are pooled for recognizing the action at the sequence level. Usually\nthis pooling step uses the first-order statistics of frame-level action\npredictions. In this paper, we explore the advantages of using higher-order\ncorrelations; specifically, we introduce Higher-order Kernel (HOK) descriptors\ngenerated from the late fusion of CNN classifier scores from all the frames in\na sequence. To generate these descriptors, we use the idea of kernel\nlinearization. Specifically, a similarity kernel matrix, which captures the\ntemporal evolution of deep classifier scores, is first linearized into kernel\nfeature maps. The HOK descriptors are then generated from the higher-order\nco-occurrences of these feature maps, and are then used as input to a\nvideo-level classifier. We provide experiments on two fine-grained action\nrecognition datasets and show that our scheme leads to state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:30:49 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Cherian", "Anoop", ""], ["Koniusz", "Piotr", ""], ["Gould", "Stephen", ""]]}, {"id": "1701.05498", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis\n  Lourakis, Xenophon Zabulis", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "comments": "WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e.\ntranslation and rotation, of texture-less rigid objects. The dataset features\nthirty industry-relevant objects with no significant texture and no\ndiscriminative color or reflectance properties. The objects exhibit symmetries\nand mutual similarities in shape and/or size. Compared to other datasets, a\nunique property is that some of the objects are parts of others. The dataset\nincludes training and test images that were captured with three synchronized\nsensors, specifically a structured-light and a time-of-flight RGB-D sensor and\na high-resolution RGB camera. There are approximately 39K training and 10K test\nimages from each sensor. Additionally, two types of 3D models are provided for\neach object, i.e. a manually created CAD model and a semi-automatically\nreconstructed one. Training images depict individual objects against a black\nbackground. Test images originate from twenty test scenes having varying\ncomplexity, which increases from simple scenes with several isolated objects to\nvery challenging ones with multiple instances of several objects and with a\nhigh amount of clutter and occlusion. The images were captured from a\nsystematically sampled view sphere around the object/scene, and are annotated\nwith accurate ground truth 6D poses of all modeled objects. Initial evaluation\nresults indicate that the state of the art in 6D object pose estimation has\nample room for improvement, especially in difficult cases with significant\nocclusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 16:16:36 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Hodan", "Tomas", ""], ["Haluza", "Pavel", ""], ["Obdrzalek", "Stepan", ""], ["Matas", "Jiri", ""], ["Lourakis", "Manolis", ""], ["Zabulis", "Xenophon", ""]]}, {"id": "1701.05524", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Kate Saenko", "title": "Synthetic to Real Adaptation with Generative Correlation Alignment\n  Networks", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic images rendered from 3D CAD models are useful for augmenting\ntraining data for object recognition algorithms. However, the generated images\nare non-photorealistic and do not match real image statistics. This leads to a\nlarge domain discrepancy, causing models trained on synthetic data to perform\npoorly on real domains. Recent work has shown the great potential of deep\nconvolutional neural networks to generate realistic images, but has not\nutilized generative models to address synthetic-to-real domain adaptation. In\nthis work, we propose a Deep Generative Correlation Alignment Network (DGCAN)\nto synthesize images using a novel domain adaption algorithm. DGCAN leverages a\nshape preserving loss and a low level statistic matching loss to minimize the\ndomain discrepancy between synthetic and real images in deep feature space.\nExperimentally, we show training off-the-shelf classifiers on the newly\ngenerated data can significantly boost performance when testing on the real\nimage domains (PASCAL VOC 2007 benchmark and Office dataset), improving upon\nseveral existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:42:00 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 20:41:32 GMT"}, {"version": "v3", "created": "Sat, 18 Mar 2017 12:56:45 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Peng", "Xingchao", ""], ["Saenko", "Kate", ""]]}, {"id": "1701.05549", "submitter": "Krzysztof Cios", "authors": "Krzysztof J. Cios", "title": "Deep Neural Networks - A Brief History", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction to deep neural networks and their history.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:43:56 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Cios", "Krzysztof J.", ""]]}, {"id": "1701.05588", "submitter": "Mohammad Mahmoodi", "authors": "Mohammad Reza Mahmoodi", "title": "High Performance Novel Skin Segmentation Algorithm for Images With\n  Complex Background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin Segmentation is widely used in biometric applications such as face\ndetection, face recognition, face tracking, and hand gesture recognition.\nHowever, several challenges such as nonlinear illumination, equipment effects,\npersonal interferences, ethnicity variations, etc., are involved in detection\nprocess that result in the inefficiency of color based methods. Even though\nmany ideas have already been proposed, the problem has not been satisfactorily\nsolved yet. This paper introduces a technique that addresses some limitations\nof the previous works. The proposed algorithm consists of three main steps\nincluding initial seed generation of skin map, Otsu segmentation in color\nimages, and finally a two-stage diffusion. The initial seed of skin pixels is\nprovided based on the idea of ternary image as there are certain pixels in\nimages which are associated to human complexion with very high probability. The\nOtsu segmentation is performed on several color channels in order to identify\nhomogeneous regions. The result accompanying with the edge map of the image is\nutilized in two consecutive diffusion steps in order to annex initially\nunidentified skin pixels to the seed. Both quantitative and qualitative results\ndemonstrate the effectiveness of the proposed system in compare with the\nstate-of-the-art works.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:22:58 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Mahmoodi", "Mohammad Reza", ""]]}, {"id": "1701.05595", "submitter": "Mohammad Mahmoodi", "authors": "Mohammad Reza Mahmoodi", "title": "Fast and Efficient Skin Detection for Facial Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient skin detection system is proposed. The algorithm\nis based on a very fast efficient pre-processing step utilizing the concept of\nternary conversion in order to identify candidate windows and subsequently, a\nnovel local two-stage diffusion method which has F-score accuracy of 0.5978 on\nSDD dataset. The pre-processing step has been proven to be useful to boost the\nspeed of the system by eliminating 82% of an image in average. This is obtained\nby keeping the true positive rate above 98%. In addition, a novel segmentation\nalgorithm is also designed to process candidate windows which is quantitatively\nand qualitatively proven to be very efficient in term of accuracy. The\nalgorithm has been implemented in FPGA to obtain real-time processing speed.\nThe system is designed fully pipeline and the inherent parallel structure of\nthe algorithm is fully exploited to maximize the performance. The system is\nimplemented on a Spartan-6 LXT45 Xilinx FPGA and it is capable of processing 98\nframes of 640*480 24-bit color images per second.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:43:27 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Mahmoodi", "Mohammad Reza", ""]]}, {"id": "1701.05616", "submitter": "Mingchen Gao Mingchen Gao", "authors": "Mingchen Gao, Ziyue Xu, Le Lu, Adam P. Harrison, Ronald M. Summers,\n  Daniel J. Mollura", "title": "Holistic Interstitial Lung Disease Detection using Deep Convolutional\n  Neural Networks: Multi-label Learning and Unordered Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting and detecting interstitial lung disease (ILD) patterns\ngiven any computed tomography (CT) slice without any pre-processing\nprerequisites, such as manually delineated regions of interest (ROIs), is a\nclinically desirable, yet challenging goal. The majority of existing work\nrelies on manually-provided ILD ROIs to extract sampled 2D image patches from\nCT slices and, from there, performs patch-based ILD categorization. Acquiring\nmanual ROIs is labor intensive and serves as a bottleneck towards\nfully-automated CT imaging ILD screening over large-scale populations.\nFurthermore, despite the considerable high frequency of more than one ILD\npattern on a single CT slice, previous works are only designed to detect one\nILD pattern per slice or patch.\n  To tackle these two critical challenges, we present multi-label deep\nconvolutional neural networks (CNNs) for detecting ILDs from holistic CT slices\n(instead of ROIs or sub-images). Conventional single-labeled CNN models can be\naugmented to cope with the possible presence of multiple ILD pattern labels,\nvia 1) continuous-valued deep regression based robust norm loss functions or 2)\na categorical objective as the sum of element-wise binary logistic losses. Our\nmethods are evaluated and validated using a publicly available database of 658\npatient CT scans under five-fold cross-validation, achieving promising\nperformance on detecting four major ILD patterns: Ground Glass, Reticular,\nHoneycomb, and Emphysema. We also investigate the effectiveness of a CNN\nactivation-based deep-feature encoding scheme using Fisher vector encoding,\nwhich treats ILD detection as spatially-unordered deep texture classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 21:52:21 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Gao", "Mingchen", ""], ["Xu", "Ziyue", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Summers", "Ronald M.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1701.05652", "submitter": "Sifeng Xia", "authors": "Sifeng Xia, Wenhan Yang, Jiaying Liu and Zongming Guo", "title": "Dual Recovery Network with Online Compensation for Image\n  Super-Resolution", "comments": "ISCAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) methods essentially lead to a loss of some\nhigh-frequency (HF) information when predicting high-resolution (HR) images\nfrom low-resolution (LR) images without using external references. To address\nthis issue, we additionally utilize online retrieved data to facilitate image\nSR in a unified deep framework. A novel dual high-frequency recovery network\n(DHN) is proposed to predict an HR image with three parts: an LR image, an\ninternal inferred HF (IHF) map (HF missing part inferred solely from the LR\nimage) and an external extracted HF (EHF) map. In particular, we infer the HF\ninformation based on both the LR image and similar HR references which are\nretrieved online. For the EHF map, we align the references with affine\ntransformation and then in the aligned references, part of HF signals are\nextracted by the proposed DHN to compensate for the HF loss. Extensive\nexperimental results demonstrate that our DHN achieves notably better\nperformance than state-of-the-art SR methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 01:44:21 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 05:27:42 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 07:31:53 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Xia", "Sifeng", ""], ["Yang", "Wenhan", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""]]}, {"id": "1701.05676", "submitter": "Sehyung Lee", "authors": "Sehyung Lee, Jongwoo Lim, Il Hong Suh", "title": "Efficient Feature Matching by Progressive Candidate Search", "comments": "9 pages including 1 references, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel feature matching algorithm that systematically utilizes\nthe geometric properties of features such as position, scale, and orientation,\nin addition to the conventional descriptor vectors. In challenging scenes with\nthe presence of repetitive patterns or with a large viewpoint change, it is\nhard to find the correct correspondences using feature descriptors only, since\nthe descriptor distances of the correct matches may not be the least among the\ncandidates due to appearance changes. Assuming that the layout of the nearby\nfeatures does not changed much, we propose the bidirectional transfer measure\nto gauge the geometric consistency of a pair of feature correspondences. The\nfeature matching problem is formulated as a Markov random field (MRF) which\nuses descriptor distances and relative geometric similarities together. The\nunmatched features are explicitly modeled in the MRF to minimize its negative\nimpact. For speed and stability, instead of solving the MRF on the entire\nfeatures at once, we start with a small set of confident feature matches, and\nthen progressively search the candidates in nearby features and expand the MRF\nwith them. Experimental comparisons show that the proposed algorithm finds\nbetter feature correspondences, i.e. more matches with higher inlier ratio, in\nmany challenging scenes with much lower computational cost than the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 03:52:48 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Lee", "Sehyung", ""], ["Lim", "Jongwoo", ""], ["Suh", "Il Hong", ""]]}, {"id": "1701.05703", "submitter": "Tomo Miyazaki", "authors": "Tomo Miyazaki, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro\n  Omachi, Masakazu Iwamura, Seiichi Uchida, Koichi Kise", "title": "Automatic Generation of Typographic Font from a Small Font Subset", "comments": "12 pages, 17 figures", "journal-ref": "IEEE Computer Graphics and Applications, 2019", "doi": "10.1109/MCG.2019.2931431", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses the automatic generation of a typographic font from a\nsubset of characters. Specifically, we use a subset of a typographic font to\nextrapolate additional characters. Consequently, we obtain a complete font\ncontaining a number of characters sufficient for daily use. The automated\ngeneration of Japanese fonts is in high demand because a Japanese font requires\nover 1,000 characters. Unfortunately, professional typographers create most\nfonts, resulting in significant financial and time investments for font\ngeneration. The proposed method can be a great aid for font creation because\ndesigners do not need to create the majority of the characters for a new font.\nThe proposed method uses strokes from given samples for font generation. The\nstrokes, from which we construct characters, are extracted by exploiting a\ncharacter skeleton dataset. This study makes three main contributions: a novel\nmethod of extracting strokes from characters, which is applicable to both\nstandard fonts and their variations; a fully automated approach for\nconstructing characters; and a selection method for sample characters. We\ndemonstrate our proposed method by generating 2,965 characters in 47 fonts.\nObjective and subjective evaluations verify that the generated characters are\nsimilar to handmade characters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 06:14:22 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Miyazaki", "Tomo", ""], ["Tsuchiya", "Tatsunori", ""], ["Sugaya", "Yoshihiro", ""], ["Omachi", "Shinichiro", ""], ["Iwamura", "Masakazu", ""], ["Uchida", "Seiichi", ""], ["Kise", "Koichi", ""]]}, {"id": "1701.05748", "submitter": "Alberto Pretto", "authors": "Filippo Basso, Emanuele Menegatti and Alberto Pretto", "title": "Robust Intrinsic and Extrinsic Calibration of RGB-D Cameras", "comments": null, "journal-ref": "Published in IEEE Transactions on Robotics, vol. 34, no. 5, 2018", "doi": "10.1109/TRO.2018.2853742", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color-depth cameras (RGB-D cameras) have become the primary sensors in most\nrobotics systems, from service robotics to industrial robotics applications.\nTypical consumer-grade RGB-D cameras are provided with a coarse intrinsic and\nextrinsic calibration that generally does not meet the accuracy requirements\nneeded by many robotics applications (e.g., highly accurate 3D environment\nreconstruction and mapping, high precision object recognition and localization,\n...). In this paper, we propose a human-friendly, reliable and accurate\ncalibration framework that enables to easily estimate both the intrinsic and\nextrinsic parameters of a general color-depth sensor couple. Our approach is\nbased on a novel two components error model. This model unifies the error\nsources of RGB-D pairs based on different technologies, such as\nstructured-light 3D cameras and time-of-flight cameras. Our method provides\nsome important advantages compared to other state-of-the-art systems: it is\ngeneral (i.e., well suited for different types of sensors), based on an easy\nand stable calibration protocol, provides a greater calibration accuracy, and\nhas been implemented within the ROS robotics framework. We report detailed\nexperimental validations and performance comparisons to support our statements.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 10:32:03 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 11:01:51 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Basso", "Filippo", ""], ["Menegatti", "Emanuele", ""], ["Pretto", "Alberto", ""]]}, {"id": "1701.05766", "submitter": "Osman Tursun", "authors": "Osman Tursun, Cemal Aker, Sinan Kalkan", "title": "A Large-scale Dataset and Benchmark for Similar Trademark Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trademark retrieval (TR) has become an important yet challenging problem due\nto an ever increasing trend in trademark applications and infringement\nincidents. There have been many promising attempts for the TR problem, which,\nhowever, fell impracticable since they were evaluated with limited and mostly\ntrivial datasets. In this paper, we provide a large-scale dataset with\nbenchmark queries with which different TR approaches can be evaluated\nsystematically. Moreover, we provide a baseline on this benchmark using the\nwidely-used methods applied to TR in the literature. Furthermore, we identify\nand correct two important issues in TR approaches that were not addressed\nbefore: reversal of contrast, and presence of irrelevant text in trademarks\nseverely affect the TR methods. Lastly, we applied deep learning, namely,\nseveral popular Convolutional Neural Network models, to the TR problem. To the\nbest of the authors, this is the first attempt to do so.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 11:36:30 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 11:44:56 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Tursun", "Osman", ""], ["Aker", "Cemal", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1701.05818", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (Palaiseau, OBELIX), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Fusion of Heterogeneous Data in Convolutional Networks for Urban\n  Semantic Labeling (Invited Paper)", "comments": "Joint Urban Remote Sensing Event (JURSE), Mar 2017, Dubai, United\n  Arab Emirates. Joint Urban Remote Sensing Event 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel module to perform fusion of heterogeneous\ndata using fully convolutional networks for semantic labeling. We introduce\nresidual correction as a way to learn how to fuse predictions coming out of a\ndual stream architecture. Especially, we perform fusion of DSM and IRRG optical\ndata on the ISPRS Vaihingen dataset over a urban area and obtain new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 15:10:09 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Audebert", "Nicolas", "", "Palaiseau, OBELIX"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1701.05847", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Zuwei Li, Maja Pantic", "title": "End-To-End Visual Speech Recognition With LSTMs", "comments": "Accepted for publication, ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional visual speech recognition systems consist of two stages, feature\nextraction and classification. Recently, several deep learning approaches have\nbeen presented which automatically extract features from the mouth images and\naim to replace the feature extraction stage. However, research on joint\nlearning of features and classification is very limited. In this work, we\npresent an end-to-end visual speech recognition system based on Long-Short\nMemory (LSTM) networks. To the best of our knowledge, this is the first model\nwhich simultaneously learns to extract features directly from the pixels and\nperform classification and also achieves state-of-the-art performance in visual\nspeech classification. The model consists of two streams which extract features\ndirectly from the mouth and difference images, respectively. The temporal\ndynamics in each stream are modelled by an LSTM and the fusion of the two\nstreams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement\nof 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the\nCUAVE database when compared with other methods which use a similar visual\nfront-end.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 16:36:09 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Petridis", "Stavros", ""], ["Li", "Zuwei", ""], ["Pantic", "Maja", ""]]}, {"id": "1701.05957", "submitter": "He Zhang", "authors": "He Zhang, Vishwanath Sindagi, Vishal M. Patel", "title": "Image De-raining Using a Conditional Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe weather conditions such as rain and snow adversely affect the visual\nquality of images captured under such conditions thus rendering them useless\nfor further usage and sharing. In addition, such degraded images drastically\naffect performance of vision systems. Hence, it is important to solve the\nproblem of single image de-raining/de-snowing. However, this is a difficult\nproblem to solve due to its inherent ill-posed nature. Existing approaches\nattempt to introduce prior information to convert it into a well-posed problem.\nIn this paper, we investigate a new point of view in addressing the single\nimage de-raining problem. Instead of focusing only on deciding what is a good\nprior or a good framework to achieve good quantitative and qualitative\nperformance, we also ensure that the de-rained image itself does not degrade\nthe performance of a given computer vision algorithm such as detection and\nclassification. In other words, the de-rained result should be\nindistinguishable from its corresponding clear image to a given discriminator.\nThis criterion can be directly incorporated into the optimization framework by\nusing the recently introduced conditional generative adversarial networks\n(GANs). To minimize artifacts introduced by GANs and ensure better visual\nquality, a new refined loss function is introduced. Based on this, we propose a\nnovel single image de-raining method called Image De-raining Conditional\nGeneral Adversarial Network (ID-CGAN), which considers quantitative, visual and\nalso discriminative performance into the objective function. Experiments\nevaluated on synthetic images and real images show that the proposed method\noutperforms many recent state-of-the-art single image de-raining methods in\nterms of quantitative and visual performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 01:06:44 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 19:26:19 GMT"}, {"version": "v3", "created": "Sat, 4 Feb 2017 16:12:45 GMT"}, {"version": "v4", "created": "Sun, 2 Jun 2019 19:18:19 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Zhang", "He", ""], ["Sindagi", "Vishwanath", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1701.06106", "submitter": "Sahil Garg", "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "title": "Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 00:35:24 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 08:15:55 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Garg", "Sahil", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1701.06109", "submitter": "Hunter Elliott", "authors": "David Richmond, Anna Payne-Tobin Jost, Talley Lambert, Jennifer\n  Waters, Hunter Elliott", "title": "DeadNet: Identifying Phototoxicity from Label-free Microscopy Images of\n  Cells using Deep ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure to intense illumination light is an unavoidable consequence of\nfluorescence microscopy, and poses a risk to the health of the sample in every\nlive-cell fluorescence microscopy experiment. Furthermore, the possible\nside-effects of phototoxicity on the scientific conclusions that are drawn from\nan imaging experiment are often unaccounted for. Previously, controlling for\nphototoxicity in imaging experiments required additional labels and\nexperiments, limiting its widespread application. Here we provide a\nproof-of-principle demonstration that the phototoxic effects of an imaging\nexperiment can be identified directly from a single phase-contrast image using\ndeep convolutional neural networks (ConvNets). This lays the groundwork for an\nautomated tool for assessing cell health in a wide range of imaging\nexperiments. Interpretability of such a method is crucial for its adoption. We\ntake steps towards interpreting the classification mechanism of the trained\nConvNet by visualizing salient features of images that contribute to accurate\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 01:43:05 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Richmond", "David", ""], ["Jost", "Anna Payne-Tobin", ""], ["Lambert", "Talley", ""], ["Waters", "Jennifer", ""], ["Elliott", "Hunter", ""]]}, {"id": "1701.06121", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Xiao-Ping Zhang", "title": "Multimodal Fusion via a Series of Transfers for Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-infrared imaging has been considered as a solution to provide high\nquality photographs in dim lighting conditions. This imaging system captures\ntwo types of multimodal images: one is near-infrared gray image (NGI) and the\nother is the visible color image (VCI). NGI is noise-free but it is grayscale,\nwhereas the VCI has colors but it contains noise. Moreover, there exist serious\nedge and brightness discrepancies between NGI and VCI. To deal with this\nproblem, a new transfer-based fusion method is proposed for noise removal.\nDifferent from conventional fusion approaches, the proposed method conducts a\nseries of transfers: contrast, detail, and color transfers. First, the proposed\ncontrast and detail transfers aim at solving the serious discrepancy problem,\nthereby creating a new noise-free and detail-preserving NGI. Second, the\nproposed color transfer models the unknown colors from the denoised VCI via a\nlinear transform, and then transfers natural-looking colors into the newly\ngenerated NGI. Experimental results show that the proposed transfer-based\nfusion method is highly successful in solving the discrepancy problem, thereby\ndescribing edges and textures clearly as well as removing noise completely on\nthe fused images. Most of all, the proposed method is superior to conventional\nfusion methods and guided filtering, and even the state-of-the-art fusion\nmethods based on scale map and layer decomposition.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 04:46:34 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "1701.06123", "submitter": "Mete Ozay", "authors": "Mete Ozay, Takayuki Okatani", "title": "Optimization on Product Submanifolds of Convolution Kernels", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 05:35:39 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:08:19 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1701.06141", "submitter": "Tianli Liao", "authors": "Nan Li, Tianli Liao, Chao Wang", "title": "Perception-based energy functions in seam-cutting", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stitching is challenging in consumer-level photography, due to\nalignment difficulties in unconstrained shooting environment. Recent studies\nshow that seam-cutting approaches can effectively relieve artifacts generated\nby local misalignment. Normally, seam-cutting is described in terms of energy\nminimization, however, few of existing methods consider human perception in\ntheir energy functions, which sometimes causes that a seam with minimum energy\nis not most invisible in the overlapping region. In this paper, we propose a\nnovel perception-based energy function in the seam-cutting framework, which\nconsiders the nonlinearity and the nonuniformity of human perception in energy\nminimization. Our perception-based approach adopts a sigmoid metric to\ncharacterize the perception of color discrimination, and a saliency weight to\nsimulate that human eyes incline to pay more attention to salient objects. In\naddition, our seam-cutting composition can be easily implemented into other\nstitching pipelines. Experiments show that our method outperforms the\nseam-cutting method of the normal energy function, and a user study\ndemonstrates that our composed results are more consistent with human\nperception.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 09:25:34 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Li", "Nan", ""], ["Liao", "Tianli", ""], ["Wang", "Chao", ""]]}, {"id": "1701.06171", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Aleksander Wieczorek, Mario Wieser, Clemens Blumer,\n  Sonali Parbhoo, Andreas Morel-Forster, Volker Roth, Thomas Vetter", "title": "Greedy Structure Learning of Hierarchical Compositional Models", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of learning a hierarchical generative\nmodel of an object from a set of images which show examples of the object in\nthe presence of variable background clutter. Existing approaches to this\nproblem are limited by making strong a-priori assumptions about the object's\ngeometric structure and require segmented training data for learning. In this\npaper, we propose a novel framework for learning hierarchical compositional\nmodels (HCMs) which do not suffer from the mentioned limitations. We present a\ngeneralized formulation of HCMs and describe a greedy structure learning\nframework that consists of two phases: Bottom-up part learning and top-down\nmodel composition. Our framework integrates the foreground-background\nsegmentation problem into the structure learning task via a background model.\nAs a result, we can jointly optimize for the number of layers in the hierarchy,\nthe number of parts per layer and a foreground-background segmentation based on\nclass labels only. We show that the learned HCMs are semantically meaningful\nand achieve competitive results when compared to other generative object models\nat object classification on a standard transfer learning dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 14:56:31 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 09:33:51 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 20:15:00 GMT"}, {"version": "v4", "created": "Sun, 14 Apr 2019 13:05:08 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kortylewski", "Adam", ""], ["Wieczorek", "Aleksander", ""], ["Wieser", "Mario", ""], ["Blumer", "Clemens", ""], ["Parbhoo", "Sonali", ""], ["Morel-Forster", "Andreas", ""], ["Roth", "Volker", ""], ["Vetter", "Thomas", ""]]}, {"id": "1701.06183", "submitter": "Razafindradina Henri Bruno rhb", "authors": "Henri Bruno Razafindradina and Paul Auguste Randriamitantsoa and\n  Nicolas Raft Razafindrakoto", "title": "Image Compression with SVD : A New Quality Metric Based On Energy Ratio", "comments": "6 pages, International Journal of Computer Science and Network,\n  Volume 5, Issue 6, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital image compression is a technique that allows to reduce the size of an\nimage in order to increase the capacity storage devices and to optimize the use\nof network bandwidth. The quality of compressed images with the techniques\nbased on the discrete cosine transform or the wavelet transform is generally\nmeasured with PSNR or SSIM. Theses metrics are not suitable to images\ncompressed with the singular values decomposition. This paper presents a new\nmetric based on the energy ratio to measure the quality of the images coded\nwith the SVD. A series of tests on 512 * 512 pixels images show that, for a\nrank k = 40 corresponding to a SSIM = 0,94 or PSNR = 35 dB, 99,9% of the energy\nare restored. Three areas of image quality assessments were identified. This\nnew metric is also very accurate and could overcome the weaknesses of PSNR and\nSSIM.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 16:23:42 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Razafindradina", "Henri Bruno", ""], ["Randriamitantsoa", "Paul Auguste", ""], ["Razafindrakoto", "Nicolas Raft", ""]]}, {"id": "1701.06190", "submitter": "Yoonsik Kim", "authors": "Yoonsik Kim, Insung Hwang, and Nam Ik Cho", "title": "A New Convolutional Network-in-Network Structure and Its Applications in\n  Skin Detection, Semantic Segmentation, and Artifact Reduction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inception network has been shown to provide good performance on image\nclassification problems, but there are not much evidences that it is also\neffective for the image restoration or pixel-wise labeling problems. For image\nrestoration problems, the pooling is generally not used because the decimated\nfeatures are not helpful for the reconstruction of an image as the output.\nMoreover, most deep learning architectures for the restoration problems do not\nuse dense prediction that need lots of training parameters. From these\nobservations, for enjoying the performance of inception-like structure on the\nimage based problems we propose a new convolutional network-in-network\nstructure. The proposed network can be considered a modification of inception\nstructure where pool projection and pooling layer are removed for maintaining\nthe entire feature map size, and a larger kernel filter is added instead.\nProposed network greatly reduces the number of parameters on account of removed\ndense prediction and pooling, which is an advantage, but may also reduce the\nreceptive field in each layer. Hence, we add a larger kernel than the original\ninception structure for not increasing the depth of layers. The proposed\nstructure is applied to typical image-to-image learning problems, i.e., the\nproblems where the size of input and output are same such as skin detection,\nsemantic segmentation, and compression artifacts reduction. Extensive\nexperiments show that the proposed network brings comparable or better results\nthan the state-of-the-art convolutional neural networks for these problems.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 17:16:53 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Kim", "Yoonsik", ""], ["Hwang", "Insung", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1701.06264", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi", "title": "Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities", "comments": "The source codes for both LS-GAN and GLS-GAN are available at\n  \\url{https://github.com/maple-research-lab}. LS-GAN is also supported by\n  Microsoft CNTK at\n  \\url{https://www.cntk.ai/pythondocs/CNTK_206C_WGAN_LSGAN.html}. The original\n  codes of LS-GAN and GLS-GAN are also available at\n  https://github.com/guojunq/lsgan/ and https://github.com/guojunq/glsgan/", "journal-ref": "in International Journal of Computer Vision (IJCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Lipschitz regularization theory and algorithms\nfor a novel Loss-Sensitive Generative Adversarial Network (LS-GAN).\nSpecifically, it trains a loss function to distinguish between real and fake\nsamples by designated margins, while learning a generator alternately to\nproduce realistic samples by minimizing their losses. The LS-GAN further\nregularizes its loss function with a Lipschitz regularity condition on the\ndensity of real data, yielding a regularized model that can better generalize\nto produce new data from a reasonable number of training examples than the\nclassic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it\ncontains a large family of regularized GAN models, including both LS-GAN and\nWasserstein GAN, as its special cases. Compared with the other GAN models, we\nwill conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive\nability in generating new images in terms of the Minimum Reconstruction Error\n(MRE) assessed on a separate test set. We further extend the LS-GAN to a\nconditional form for supervised and semi-supervised learning problems, and\ndemonstrate its outstanding performance on image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 04:46:22 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 16:34:53 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 16:34:28 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 00:25:06 GMT"}, {"version": "v5", "created": "Wed, 15 Mar 2017 06:44:32 GMT"}, {"version": "v6", "created": "Mon, 19 Mar 2018 03:54:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Qi", "Guo-Jun", ""]]}, {"id": "1701.06333", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Normative theory of visual receptive fields", "comments": "15 pages, 17 figures. arXiv admin note: text overlap with\n  arXiv:1210.0754", "journal-ref": "Substantially revised version in Heliyon 7(1): e05897: 1-20, 2021", "doi": "10.1016/j.heliyon.2021.e05897", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives an overview of a normative computational theory of visual\nreceptive fields, by which idealized functional models of early spatial,\nspatio-chromatic and spatio-temporal receptive fields can be derived in an\naxiomatic way based on structural properties of the environment in combination\nwith assumptions about the internal structure of a vision system to guarantee\nconsistent handling of image representations over multiple spatial and temporal\nscales. Interestingly, this theory leads to predictions about visual receptive\nfield shapes with qualitatively very good similarity to biological receptive\nfields measured in the retina, the LGN and the primary visual cortex (V1) of\nmammals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 11:13:07 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 13:55:58 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 07:46:27 GMT"}, {"version": "v4", "created": "Thu, 16 Feb 2017 13:52:12 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1701.06351", "submitter": "Yichao Yan", "authors": "Yichao Yan, Bingbing Ni, Zhichao Song, Chao Ma, Yan Yan and Xiaokang\n  Yang", "title": "Person Re-Identification via Recurrent Feature Aggregation", "comments": "14 pages, 4 figures, in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the person re-identification problem by effectively exploiting a\nglobally discriminative feature representation from a sequence of tracked human\nregions/patches. This is in contrast to previous person re-id works, which rely\non either single frame based person to person patch matching, or graph based\nsequence to sequence matching. We show that a progressive/sequential fusion\nframework based on long short term memory (LSTM) network aggregates the\nframe-wise human region representation at each time stamp and yields a sequence\nlevel human feature representation. Since LSTM nodes can remember and propagate\npreviously accumulated good features and forget newly input inferior ones, even\nwith simple hand-crafted features, the proposed recurrent feature aggregation\nnetwork (RFA-Net) is effective in generating highly discriminative sequence\nlevel human representations. Extensive experimental results on two person\nre-identification benchmarks demonstrate that the proposed method performs\nfavorably against state-of-the-art person re-identification methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 12:05:16 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Yan", "Yichao", ""], ["Ni", "Bingbing", ""], ["Song", "Zhichao", ""], ["Ma", "Chao", ""], ["Yan", "Yan", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1701.06393", "submitter": "Brijnesh Jain", "authors": "David Schultz and Brijnesh Jain", "title": "Nonsmooth Analysis and Subgradient Methods for Averaging in Dynamic Time\n  Warping Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series averaging in dynamic time warping (DTW) spaces has been\nsuccessfully applied to improve pattern recognition systems. This article\nproposes and analyzes subgradient methods for the problem of finding a sample\nmean in DTW spaces. The class of subgradient methods generalizes existing\nsample mean algorithms such as DTW Barycenter Averaging (DBA). We show that DBA\nis a majorize-minimize algorithm that converges to necessary conditions of\noptimality after finitely many iterations. Empirical results show that for\nincreasing sample sizes the proposed stochastic subgradient (SSG) algorithm is\nmore stable and finds better solutions in shorter time than the DBA algorithm\non average. Therefore, SSG is useful in online settings and for non-small\nsample sizes. The theoretical and empirical results open new paths for devising\nsample mean algorithms: nonsmooth optimization methods and modified variants of\npairwise averaging methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 13:58:44 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Schultz", "David", ""], ["Jain", "Brijnesh", ""]]}, {"id": "1701.06439", "submitter": "Teik Koon Cheang", "authors": "Teik Koon Cheang, Yong Shean Chong, Yong Haur Tay", "title": "Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN", "comments": "5 pages, 3 figures, International Workshop on Advanced Image\n  Technology, January, 8-10, 2017. Penang, Malaysia. Proceeding IWAIT2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While vehicle license plate recognition (VLPR) is usually done with a sliding\nwindow approach, it can have limited performance on datasets with characters\nthat are of variable width. This can be solved by hand-crafting algorithms to\nprescale the characters. While this approach can work fairly well, the\nrecognizer is only aware of the pixels within each detector window, and fails\nto account for other contextual information that might be present in other\nparts of the image. A sliding window approach also requires training data in\nthe form of presegmented characters, which can be more difficult to obtain. In\nthis paper, we propose a unified ConvNet-RNN model to recognize real-world\ncaptured license plate photographs. By using a Convolutional Neural Network\n(ConvNet) to perform feature extraction and using a Recurrent Neural Network\n(RNN) for sequencing, we address the problem of sliding window approaches being\nunable to access the context of the entire image by feeding the entire image as\ninput to the ConvNet. This has the added benefit of being able to perform\nend-to-end training of the entire model on labelled, full license plate images.\nExperimental results comparing the ConvNet-RNN architecture to a sliding\nwindow-based approach shows that the ConvNet-RNN architecture performs\nsignificantly better.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:11:12 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Cheang", "Teik Koon", ""], ["Chong", "Yong Shean", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1701.06452", "submitter": "Giovanni Montana", "authors": "Petros-Pavlos Ypsilantis and Giovanni Montana", "title": "Learning what to look in chest X-rays with a recurrent visual attention\n  model", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:29:47 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ypsilantis", "Petros-Pavlos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1701.06462", "submitter": "Eu Koon Cheang", "authors": "Eu Koon Cheang, Teik Koon Cheang, Yong Haur Tay", "title": "Using Convolutional Neural Networks to Count Palm Trees in Satellite\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a supervised learning system for counting and\nlocalizing palm trees in high-resolution, panchromatic satellite imagery\n(40cm/pixel to 1.5m/pixel). A convolutional neural network classifier trained\non a set of palm and no-palm images is applied across a satellite image scene\nin a sliding window fashion. The resultant confidence map is smoothed with a\nuniform filter. A non-maximal suppression is applied onto the smoothed\nconfidence map to obtain peaks. Trained with a small dataset of 500 images of\nsize 40x40 cropped from satellite images, the system manages to achieve a tree\ncount accuracy of over 99%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:38:52 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Cheang", "Eu Koon", ""], ["Cheang", "Teik Koon", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1701.06487", "submitter": "Steven Diamond", "authors": "Steven Diamond, Vincent Sitzmann, Frank Julca-Aguilar, Stephen Boyd,\n  Gordon Wetzstein, Felix Heide", "title": "Dirty Pixels: Towards End-to-End Image Processing and Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world imaging systems acquire measurements that are degraded by noise,\noptical aberrations, and other imperfections that make image processing for\nhuman viewing and higher-level perception tasks challenging. Conventional\ncameras address this problem by compartmentalizing imaging from high-level task\nprocessing. As such, conventional imaging involves processing the RAW sensor\nmeasurements in a sequential pipeline of steps, such as demosaicking,\ndenoising, deblurring, tone-mapping and compression. This pipeline is optimized\nto obtain a visually pleasing image. High-level processing, on the other hand,\ninvolves steps such as feature extraction, classification, tracking, and\nfusion. While this siloed design approach allows for efficient development, it\nalso dictates compartmentalized performance metrics, without knowledge of the\nhigher-level task of the camera system. For example, today's demosaicking and\ndenoising algorithms are designed using perceptual image quality metrics but\nnot with domain-specific tasks such as object detection in mind. We propose an\nend-to-end differentiable architecture that jointly performs demosaicking,\ndenoising, deblurring, tone-mapping, and classification. The architecture\nlearns processing pipelines whose outputs differ from those of existing ISPs\noptimized for perceptual quality, preserving fine detail at the cost of\nincreased noise and artifacts. We demonstrate on captured and simulated data\nthat our model substantially improves perception in low light and other\nchallenging conditions, which is imperative for real-world applications.\nFinally, we found that the proposed model also achieves state-of-the-art\naccuracy when optimized for image reconstruction in low-light conditions,\nvalidating the architecture itself as a potentially useful drop-in network for\nreconstruction and analysis tasks beyond the applications demonstrated in this\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 16:46:12 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 02:14:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Diamond", "Steven", ""], ["Sitzmann", "Vincent", ""], ["Julca-Aguilar", "Frank", ""], ["Boyd", "Stephen", ""], ["Wetzstein", "Gordon", ""], ["Heide", "Felix", ""]]}, {"id": "1701.06599", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Mohammadhadi\n  Bagheri, Isabella Nogues, Jianhua Yao, Ronald M. Summers", "title": "Unsupervised Joint Mining of Deep Features and Image Labels for\n  Large-scale Radiology Image Categorization and Scene Recognition", "comments": "WACV 2017. arXiv admin note: text overlap with arXiv:1603.07965 V2:\n  supplementary material appended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent rapid and tremendous success of deep convolutional neural networks\n(CNN) on many challenging computer vision tasks largely derives from the\naccessibility of the well-annotated ImageNet and PASCAL VOC datasets.\nNevertheless, unsupervised image categorization (i.e., without the ground-truth\nlabeling) is much less investigated, yet critically important and difficult\nwhen annotations are extremely hard to obtain in the conventional way of\n\"Google Search\" and crowd sourcing. We address this problem by presenting a\nlooped deep pseudo-task optimization (LDPO) framework for joint mining of deep\nCNN features and image labels. Our method is conceptually simple and rests upon\nthe hypothesized \"convergence\" of better labels leading to better trained CNN\nmodels which in turn feed more discriminative image representations to\nfacilitate more meaningful clusters/labels. Our proposed method is validated in\ntackling two important applications: 1) Large-scale medical image annotation\nhas always been a prohibitively expensive and easily-biased task even for\nwell-trained radiologists. Significantly better image categorization results\nare achieved via our proposed approach compared to the previous\nstate-of-the-art method. 2) Unsupervised scene recognition on representative\nand publicly available datasets with our proposed technique is examined. The\nLDPO achieves excellent quantitative scene classification results. On the MIT\nindoor scene dataset, it attains a clustering accuracy of 75.3%, compared to\nthe state-of-the-art supervised classification accuracy of 81.0% (when both are\nbased on the VGG-VD model).\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 19:34:22 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 19:09:02 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Wang", "Xiaosong", ""], ["Lu", "Le", ""], ["Shin", "Hoo-chang", ""], ["Kim", "Lauren", ""], ["Bagheri", "Mohammadhadi", ""], ["Nogues", "Isabella", ""], ["Yao", "Jianhua", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1701.06641", "submitter": "Valero Laparra", "authors": "Valero Laparra, Alex Berardino, Johannes Ball\\'e, and Eero P.\n  Simoncelli", "title": "Perceptually Optimized Image Rendering", "comments": null, "journal-ref": "J. Optical Society of America, A. 34(9):1511-1525. Sep 2017", "doi": "10.1364/JOSAA.34.001511", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for rendering photographic images, taking into account\ndisplay limitations, so as to optimize perceptual similarity between the\nrendered image and the original scene. We formulate this as a constrained\noptimization problem, in which we minimize a measure of perceptual\ndissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics\nthe early stage transformations of the human visual system. When rendering\nimages acquired with higher dynamic range than that of the display, we find\nthat the optimized solution boosts the contrast of low-contrast features\nwithout introducing significant artifacts, yielding results of comparable\nvisual quality to current state-of-the art methods with no manual intervention\nor parameter settings. We also examine a variety of other display constraints,\nincluding limitations on minimum luminance (black point), mean luminance (as a\nproxy for energy consumption), and quantized luminance levels (halftoning).\nFinally, we show that the method may be used to enhance details and contrast of\nimages degraded by optical scattering (e.g. fog).\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 21:38:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Laparra", "Valero", ""], ["Berardino", "Alex", ""], ["Ball\u00e9", "Johannes", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1701.06643", "submitter": "Sergey Korolev", "authors": "Sergey Korolev, Amir Safiullin, Mikhail Belyaev, Yulia Dodonova", "title": "Residual and Plain Convolutional Neural Networks for 3D Brain MRI\n  Classification", "comments": "IEEE International Symposium on Biomedical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years there have been a number of studies that applied deep\nlearning algorithms to neuroimaging data. Pipelines used in those studies\nmostly require multiple processing steps for feature extraction, although\nmodern advancements in deep learning for image classification can provide a\npowerful framework for automatic feature generation and more straightforward\nanalysis. In this paper, we show how similar performance can be achieved\nskipping these feature extraction steps with the residual and plain 3D\nconvolutional neural network architectures. We demonstrate the performance of\nthe proposed approach for classification of Alzheimer's disease versus mild\ncognitive impairment and normal controls on the Alzheimer's Disease National\nInitiative (ADNI) dataset of 3D structural MRI brain scans.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 21:54:50 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Korolev", "Sergey", ""], ["Safiullin", "Amir", ""], ["Belyaev", "Mikhail", ""], ["Dodonova", "Yulia", ""]]}, {"id": "1701.06659", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi, Alexander C. Berg", "title": "DSSD : Deconvolutional Single Shot Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper is an approach for introducing additional\ncontext into state-of-the-art general object detection. To achieve this we\nfirst combine a state-of-the-art classifier (Residual-101[14]) with a fast\ndetection framework (SSD[18]). We then augment SSD+Residual-101 with\ndeconvolution layers to introduce additional large-scale context in object\ndetection and improve accuracy, especially for small objects, calling our\nresulting system DSSD for deconvolutional single shot detector. While these two\ncontributions are easily described at a high-level, a naive implementation does\nnot succeed. Instead we show that carefully adding additional stages of learned\ntransformations, specifically a module for feed-forward connections in\ndeconvolution and a new output module, enables this new approach and forms a\npotential way forward for further detection research. Results are shown on both\nPASCAL VOC and COCO detection. Our DSSD with $513 \\times 513$ input achieves\n81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO,\noutperforming a state-of-the-art method R-FCN[3] on each dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 22:33:35 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Liu", "Wei", ""], ["Ranga", "Ananth", ""], ["Tyagi", "Ambrish", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1701.06708", "submitter": "Jonghye Woo", "authors": "Jonghye Woo, Fangxu Xing, Maureen Stone, Jordan Green, Timothy G.\n  Reese, Thomas J. Brady, Van J. Wedeen, Jerry L. Prince, and Georges El Fakhri", "title": "Speech Map: A Statistical Multimodal Atlas of 4D Tongue Motion During\n  Speech from Tagged and Cine MR Images", "comments": "Accepted at Journal of Computer Methods in Biomechanics and\n  Biomedical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative measurement of functional and anatomical traits of 4D tongue\nmotion in the course of speech or other lingual behaviors remains a major\nchallenge in scientific research and clinical applications. Here, we introduce\na statistical multimodal atlas of 4D tongue motion using healthy subjects,\nwhich enables a combined quantitative characterization of tongue motion in a\nreference anatomical configuration. This atlas framework, termed Speech Map,\ncombines cine- and tagged-MRI in order to provide both the anatomic reference\nand motion information during speech. Our approach involves a series of steps\nincluding (1) construction of a common reference anatomical configuration from\ncine-MRI, (2) motion estimation from tagged-MRI, (3) transformation of the\nmotion estimations to the reference anatomical configuration, and (4)\ncomputation of motion quantities such as Lagrangian strain. Using this\nframework, the anatomic configuration of the tongue appears motionless, while\nthe motion fields and associated strain measurements change over the time\ncourse of speech. In addition, to form a succinct representation of the\nhigh-dimensional and complex motion fields, principal component analysis is\ncarried out to characterize the central tendencies and variations of motion\nfields of our speech tasks. Our proposed method provides a platform to\nquantitatively and objectively explain the differences and variability of\ntongue motion by illuminating internal motion and strain that have so far been\nintractable. The findings are used to understand how tongue function for speech\nis limited by abnormal internal motion and strain in glossectomy patients.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 02:07:27 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 02:48:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Woo", "Jonghye", ""], ["Xing", "Fangxu", ""], ["Stone", "Maureen", ""], ["Green", "Jordan", ""], ["Reese", "Timothy G.", ""], ["Brady", "Thomas J.", ""], ["Wedeen", "Van J.", ""], ["Prince", "Jerry L.", ""], ["Fakhri", "Georges El", ""]]}, {"id": "1701.06715", "submitter": "Xiaohao Cai", "authors": "Juheon Lee, David Coomes, Carola-Bibiane Schonlieb, Xiaohao Cai, Jan\n  Lellmann, Michele Dalponte, Yadvinder Malhi, Nathalie Butt, Mike Morecroft", "title": "A graph cut approach to 3D tree delineation, using integrated airborne\n  LiDAR and hyperspectral imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising individual trees within remotely sensed imagery has important\napplications in forest ecology and management. Several algorithms for tree\ndelineation have been suggested, mostly based on locating local maxima or\ninverted basins in raster canopy height models (CHMs) derived from Light\nDetection And Ranging (LiDAR) data or photographs. However, these algorithms\noften lead to inaccurate estimates of forest stand characteristics due to the\nlimited information content of raster CHMs. Here we develop a 3D tree\ndelineation method which uses graph cut to delineate trees from the full 3D\nLiDAR point cloud, and also makes use of any optical imagery available\n(hyperspectral imagery in our case). First, conventional methods are used to\nlocate local maxima in the CHM and generate an initial map of trees. Second, a\ngraph is built from the LiDAR point cloud, fused with the hyperspectral data.\nFor computational efficiency, the feature space of hyperspectral imagery is\nreduced using robust PCA. Third, a multi-class normalised cut is applied to the\ngraph, using the initial map of trees to constrain the number of clusters and\ntheir locations. Finally, recursive normalised cut is used to subdivide, if\nnecessary, each of the clusters identified by the initial analysis. We call\nthis approach Multiclass Cut followed by Recursive Cut (MCRC). The\neffectiveness of MCRC was tested using three datasets: i) NewFor, ii) a\nconiferous forest in the Italian Alps, and iii) a deciduous woodland in the UK.\nThe performance of MCRC was usually superior to that of other delineation\nmethods, and was further improved by including high-resolution optical imagery.\nSince MCRC delineates the entire LiDAR point cloud in 3D, it allows individual\ncrown characteristics to be measured. By making full use of the data available,\ngraph cut has the potential to considerably improve the accuracy of tree\ndelineation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 02:41:30 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Lee", "Juheon", ""], ["Coomes", "David", ""], ["Schonlieb", "Carola-Bibiane", ""], ["Cai", "Xiaohao", ""], ["Lellmann", "Jan", ""], ["Dalponte", "Michele", ""], ["Malhi", "Yadvinder", ""], ["Butt", "Nathalie", ""], ["Morecroft", "Mike", ""]]}, {"id": "1701.06772", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Xiaojie Jin, Jiashi Feng, Shuicheng Yan", "title": "Training Group Orthogonal Neural Networks with Privileged Information", "comments": "Proceedings of the IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning rich and diverse representations is critical for the performance of\ndeep convolutional neural networks (CNNs). In this paper, we consider how to\nuse privileged information to promote inherent diversity of a single CNN model\nsuch that the model can learn better representations and offer stronger\ngeneralization ability. To this end, we propose a novel group orthogonal\nconvolutional neural network (GoCNN) that learns untangled representations\nwithin each layer by exploiting provided privileged information and enhances\nrepresentation diversity effectively. We take image classification as an\nexample where image segmentation annotations are used as privileged information\nduring the training process. Experiments on two benchmark datasets -- ImageNet\nand PASCAL VOC -- clearly demonstrate the strong generalization ability of our\nproposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance\nof state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses\nprivileged information of 10% of the training images, confirming effectiveness\nof GoCNN on utilizing available privileged knowledge to train better CNNs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 09:00:19 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 05:57:25 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Chen", "Yunpeng", ""], ["Jin", "Xiaojie", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1701.06805", "submitter": "M{\\aa}ns Larsson", "authors": "M{\\aa}ns Larsson, Anurag Arnab, Fredrik Kahl, Shuai Zheng, Philip Torr", "title": "A Projected Gradient Descent Method for CRF Inference allowing\n  End-To-End Training of Arbitrary Pairwise Potentials", "comments": "Presented at EMMCVPR 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are we using the right potential functions in the Conditional Random Field\nmodels that are popular in the Vision community? Semantic segmentation and\nother pixel-level labelling tasks have made significant progress recently due\nto the deep learning paradigm. However, most state-of-the-art structured\nprediction methods also include a random field model with a hand-crafted\nGaussian potential to model spatial priors, label consistencies and\nfeature-based image conditioning.\n  In this paper, we challenge this view by developing a new inference and\nlearning framework which can learn pairwise CRF potentials restricted only by\ntheir dependence on the image pixel values and the size of the support. Both\nstandard spatial and high-dimensional bilateral kernels are considered. Our\nframework is based on the observation that CRF inference can be achieved via\nprojected gradient descent and consequently, can easily be integrated in deep\nneural networks to allow for end-to-end training. It is empirically\ndemonstrated that such learned potentials can improve segmentation accuracy and\nthat certain label class interactions are indeed better modelled by a\nnon-Gaussian potential. In addition, we compare our inference method to the\ncommonly used mean-field algorithm. Our framework is evaluated on several\npublic benchmarks for semantic segmentation with improved performance compared\nto previous state-of-the-art CNN+CRF models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 10:45:32 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 13:51:02 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 10:31:27 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Larsson", "M\u00e5ns", ""], ["Arnab", "Anurag", ""], ["Kahl", "Fredrik", ""], ["Zheng", "Shuai", ""], ["Torr", "Philip", ""]]}, {"id": "1701.06854", "submitter": "Rahul Mitra", "authors": "Rahul Mitra, Jiakai Zhang, Sanath Narayan, Shuaib Ahmed, Sharat\n  Chandran, Arjun Jain", "title": "Improved Descriptors for Patch Matching and Reconstruction", "comments": "9 pages, ICCV Workshop on Compact and Efficient Feature\n  Representation and Learning (CEFRL), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional neural network (ConvNet) based approach for\nlearning local image descriptors which can be used for significantly improved\npatch matching and 3D reconstructions. A multi-resolution ConvNet is used for\nlearning keypoint descriptors. We also propose a new dataset consisting of an\norder of magnitude more number of scenes, images, and positive and negative\ncorrespondences compared to the currently available Multi-View Stereo (MVS)\n[18] dataset. The new dataset also has better coverage of the overall\nviewpoint, scale, and lighting changes in comparison to the MVS dataset. We\nevaluate our approach on publicly available datasets, such as Oxford Affine\nCovariant Regions Dataset (ACRD) [12], MVS [18], Synthetic [6] and Strecha [15]\ndatasets to quantify the image descriptor performance. Scenes from the Oxford\nACRD, MVS and Synthetic datasets are used for evaluating the patch matching\nperformance of the learnt descriptors while the Strecha dataset is used to\nevaluate the 3D reconstruction task. Experiments show that the proposed\ndescriptor outperforms the current state-of-the-art descriptors in both the\nevaluation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 13:05:12 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:03:27 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 07:50:11 GMT"}, {"version": "v4", "created": "Sun, 27 Aug 2017 17:45:22 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Mitra", "Rahul", ""], ["Zhang", "Jiakai", ""], ["Narayan", "Sanath", ""], ["Ahmed", "Shuaib", ""], ["Chandran", "Sharat", ""], ["Jain", "Arjun", ""]]}, {"id": "1701.06859", "submitter": "Laurent Perrinet", "authors": "Laurent Perrinet (INT)", "title": "Sparse models for Computer Vision", "comments": null, "journal-ref": "Biologically inspired computer vision, 2015, 9783527680863", "doi": "10.1002/9783527680863.ch14", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of images in the brain is known to be sparse. That is, as\nneural activity is recorded in a visual area ---for instance the primary visual\ncortex of primates--- only a few neurons are active at a given time with\nrespect to the whole population. It is believed that such a property reflects\nthe efficient match of the representation with the statistics of natural\nscenes. Applying such a paradigm to computer vision therefore seems a promising\napproach towards more biomimetic algorithms. Herein, we will describe a\nbiologically-inspired approach to this problem. First, we will describe an\nunsupervised learning paradigm which is particularly adapted to the efficient\ncoding of image patches. Then, we will outline a complete multi-scale framework\n---SparseLets--- implementing a biologically inspired sparse representation of\nnatural images. Finally, we will propose novel methods for integrating prior\ninformation into these algorithms and provide some preliminary experimental\nresults. We will conclude by giving some perspective on applying such\nalgorithms to computer vision. More specifically, we will propose that\nbio-inspired approaches may be applied to computer vision using predictive\ncoding schemes, sparse models being one simple and efficient instance of such\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 13:20:11 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Perrinet", "Laurent", "", "INT"]]}, {"id": "1701.06944", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Hanno Ackermann, Weiyao Lin, Sitong Feng, Bodo\n  Rosenhahn", "title": "Motion Segmentation via Global and Local Sparse Subspace Optimization", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new framework for segmenting feature-based moving\nobjects under affine subspace model. Since the feature trajectories in practice\nare high-dimensional and contain a lot of noise, we firstly apply the sparse\nPCA to represent the original trajectories with a low-dimensional global\nsubspace, which consists of the orthogonal sparse principal vectors.\nSubsequently, the local subspace separation will be achieved via automatically\nsearching the sparse representation of the nearest neighbors for each projected\ndata. In order to refine the local subspace estimation result and deal with the\nmissing data problem, we propose an error estimation to encourage the projected\ndata that span a same local subspace to be clustered together. In the end, the\nsegmentation of different motions is achieved through the spectral clustering\non an affinity matrix, which is constructed with both the error estimation and\nsparse neighbors optimization. We test our method extensively and compare it\nwith state-of-the-art methods on the Hopkins 155 dataset and Freiburg-Berkeley\nMotion Segmentation dataset. The results show that our method is comparable\nwith the other motion segmentation methods, and in many cases exceed them in\nterms of precision and computation time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 15:49:53 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Yang", "Michael Ying", ""], ["Ackermann", "Hanno", ""], ["Lin", "Weiyao", ""], ["Feng", "Sitong", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1701.07046", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Gaurav Sharma, Brejesh Lall", "title": "Large Scale Novel Object Discovery in 3D", "comments": "Accepted for publication at IEEE Winter Conference on Applications of\n  Computer Vision (WACV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for discovering never-seen-before objects in 3D point\nclouds obtained from sensors like Microsoft Kinect. We generate supervoxels\ndirectly from the point cloud data and use them with a Siamese network, built\non a recently proposed 3D convolutional neural network architecture. We use\nknown objects to train a non-linear embedding of supervoxels, by optimizing the\ncriteria that supervoxels which fall on the same object should be closer than\nthose which fall on different objects, in the embedding space. We test on\nunknown objects, which were not seen during training, and perform clustering in\nthe learned embedding space of supervoxels to effectively perform novel object\ndiscovery. We validate the method with extensive experiments, quantitatively\nshowing that it can discover numerous unseen objects while being trained on\nonly a few dense 3D models. We also show very good qualitative results of\nobject discovery in point cloud data when the test objects, either specific\ninstances or even categories, were never seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 12:58:52 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 11:39:15 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Sharma", "Gaurav", ""], ["Lall", "Brejesh", ""]]}, {"id": "1701.07122", "submitter": "Chunhua Shen", "authors": "Tong Shen, Guosheng Lin, Chunhua Shen, Ian Reid", "title": "Learning Multi-level Region Consistency with Dense Multi-label Networks\n  for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is a fundamental task in image understanding.\nPer-pixel semantic labelling of an image benefits greatly from the ability to\nconsider region consistency both locally and globally. However, many Fully\nConvolutional Network based methods do not impose such consistency, which may\ngive rise to noisy and implausible predictions. We address this issue by\nproposing a dense multi-label network module that is able to encourage the\nregion consistency at different levels. This simple but effective module can be\neasily integrated into any semantic segmentation systems. With comprehensive\nexperiments, we show that the dense multi-label can successfully remove the\nimplausible labels and clear the confusion so as to boost the performance of\nsemantic segmentation systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:11:43 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Shen", "Tong", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1701.07158", "submitter": "Jae Kyu Choi", "authors": "Jae Kyu Choi, Bin Dong, and Xiaoqun Zhang", "title": "An Edge Driven Wavelet Frame Model for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet frame systems are known to be effective in capturing singularities\nfrom noisy and degraded images. In this paper, we introduce a new edge driven\nwavelet frame model for image restoration by approximating images as piecewise\nsmooth functions. With an implicit representation of image singularities sets,\nthe proposed model inflicts different strength of regularization on smooth and\nsingular image regions and edges. The proposed edge driven model is robust to\nboth image approximation and singularity estimation. The implicit formulation\nalso enables an asymptotic analysis of the proposed models and a rigorous\nconnection between the discrete model and a general continuous variational\nmodel. Finally, numerical results on image inpainting and deblurring show that\nthe proposed model is compared favorably against several popular image\nrestoration models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 04:56:10 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Choi", "Jae Kyu", ""], ["Dong", "Bin", ""], ["Zhang", "Xiaoqun", ""]]}, {"id": "1701.07164", "submitter": "Byunghwee Lee", "authors": "Byunghwee Lee, Daniel Kim, Seunghye Sun, Hawoong Jeong, Juyong Park", "title": "Historic Emergence of Diversity in Painting: Heterogeneity in Chromatic\n  Distance in Images and Characterization of Massive Painting Data Set", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0204430", "report-no": null, "categories": "cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Painting is an art form that has long functioned as a major channel for the\ncreative expression and communication of humans, its evolution taking place\nunder an interplay with the science, technology, and social environments of the\ntimes. Therefore, understanding the process based on comprehensive data could\nshed light on how humans acted and manifested creatively under changing\nconditions. Yet, there exist few systematic frameworks that characterize the\nprocess for painting, which would require robust statistical methods for\ndefining painting characteristics and identifying human's creative\ndevelopments, and data of high quality and sufficient quantity. Here we propose\nthat the color contrast of a painting image signifying the heterogeneity in\ninter-pixel chromatic distance can be a useful representation of its style,\nintegrating both the color and geometry. From the color contrasts of paintings\nfrom a large-scale, comprehensive archive of 179,853 high-quality images\nspanning several centuries we characterize the temporal evolutionary patterns\nof paintings, and present a deep study of an extraordinary expansion in\ncreative diversity and individuality that came to define the modern era.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 05:15:09 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 02:41:53 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Lee", "Byunghwee", ""], ["Kim", "Daniel", ""], ["Sun", "Seunghye", ""], ["Jeong", "Hawoong", ""], ["Park", "Juyong", ""]]}, {"id": "1701.07174", "submitter": "Yuanyi Zhong", "authors": "Yuanyi Zhong, Jiansheng Chen, Bo Huang", "title": "Towards End-to-End Face Recognition through Alignment Learning", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": "10.1109/LSP.2017.2715076", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of effective methods have been proposed for face recognition during\nthe past decade. Although these methods differ essentially in many aspects, a\ncommon practice of them is to specifically align the facial area based on the\nprior knowledge of human face structure before feature extraction. In most\nsystems, the face alignment module is implemented independently. This has\nactually caused difficulties in the designing and training of end-to-end face\nrecognition models. In this paper we study the possibility of alignment\nlearning in end-to-end face recognition, in which neither prior knowledge on\nfacial landmarks nor artificially defined geometric transformations are\nrequired. Specifically, spatial transformer layers are inserted in front of the\nfeature extraction layers in a Convolutional Neural Network (CNN) for face\nrecognition. Only human identity clues are used for driving the neural network\nto automatically learn the most suitable geometric transformation and the most\nappropriate facial area for the recognition task. To ensure reproducibility,\nour model is trained purely on the publicly available CASIA-WebFace dataset,\nand is tested on the Labeled Face in the Wild (LFW) dataset. We have achieved a\nverification accuracy of 99.08\\% which is comparable to state-of-the-art single\nmodel based methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 06:10:41 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhong", "Yuanyi", ""], ["Chen", "Jiansheng", ""], ["Huang", "Bo", ""]]}, {"id": "1701.07248", "submitter": "Johan Thunberg", "authors": "Johan Thunberg, Florian Bernard, Jorge Goncalves", "title": "Distributed methods for synchronization of orthogonal matrices over\n  graphs", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": "10.1016/j.automatica.2017.02.025", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of synchronizing orthogonal matrices over\ndirected graphs. For synchronized transformations (or matrices), composite\ntransformations over loops equal the identity. We formulate the synchronization\nproblem as a least-squares optimization problem with nonlinear constraints. The\nsynchronization problem appears as one of the key components in applications\nranging from 3D-localization to image registration. The main contributions of\nthis work can be summarized as the introduction of two novel algorithms; one\nfor symmetric graphs and one for graphs that are possibly asymmetric. Under\ngeneral conditions, the former has guaranteed convergence to the solution of a\nspectral relaxation to the synchronization problem. The latter is stable for\nsmall step sizes when the graph is quasi-strongly connected. The proposed\nmethods are verified in numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:34:45 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 11:26:23 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 12:11:11 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Thunberg", "Johan", ""], ["Bernard", "Florian", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1701.07275", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Universal representations:The missing link between faces, text,\n  planktons, and cat breeds", "comments": "10 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of large labelled datasets and high-capacity models, the\nperformance of machine vision systems has been improving rapidly. However, the\ntechnology has still major limitations, starting from the fact that different\nvision problems are still solved by different models, trained from scratch or\nfine-tuned on the target data. The human visual system, in stark contrast,\nlearns a universal representation for vision in the early life of an\nindividual. This representation works well for an enormous variety of vision\nproblems, with little or no change, with the major advantage of requiring\nlittle training data to solve any of them.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 12:07:15 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1701.07354", "submitter": "David Villacis", "authors": "David Villacis, Santeri Kaupinm\\\"aki, Samuli Siltanen, Teemu Helenius", "title": "Photographic dataset: playing cards", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a photographic dataset collected for testing image processing\nalgorithms. The idea is to have images that can exploit the properties of total\nvariation, therefore a set of playing cards was distributed on the scene. The\ndataset is made available at www.fips.fi/photographic_dataset2.php\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 15:35:09 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Villacis", "David", ""], ["Kaupinm\u00e4ki", "Santeri", ""], ["Siltanen", "Samuli", ""], ["Helenius", "Teemu", ""]]}, {"id": "1701.07368", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Yi Zhu, Alexander G. Hauptmann", "title": "Deep Local Video Feature for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of representing an entire video using CNN features\nfor human action recognition. Currently, limited by GPU memory, we have not\nbeen able to feed a whole video into CNN/RNNs for end-to-end learning. A common\npractice is to use sampled frames as inputs and video labels as supervision.\nOne major problem of this popular approach is that the local samples may not\ncontain the information indicated by global labels. To deal with this problem,\nwe propose to treat the deep networks trained on local inputs as local feature\nextractors. After extracting local features, we aggregate them into global\nfeatures and train another mapping function on the same training data to map\nthe global features into global labels. We study a set of problems regarding\nthis new type of local features such as how to aggregate them into global\nfeatures. Experimental results on HMDB51 and UCF101 datasets show that, for\nthese new local features, a simple maximum pooling on the sparsely sampled\nfeatures lead to significant performance improvement.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 16:23:17 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 13:50:09 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Zhu", "Yi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1701.07372", "submitter": "Abdolrahim Kadkhodamohammadi", "authors": "Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin,\n  Nicolas Padoy", "title": "A Multi-view RGB-D Approach for Human Pose Estimation in Operating Rooms", "comments": "WACV 2017. Supplementary material video: https://youtu.be/L3A0BzT0FKQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches have been proposed for human pose estimation in single and\nmulti-view RGB images. However, some environments, such as the operating room,\nare still very challenging for state-of-the-art RGB methods. In this paper, we\npropose an approach for multi-view 3D human pose estimation from RGB-D images\nand demonstrate the benefits of using the additional depth channel for pose\nrefinement beyond its use for the generation of improved features. The proposed\nmethod permits the joint detection and estimation of the poses without knowing\na priori the number of persons present in the scene. We evaluate this approach\non a novel multi-view RGB-D dataset acquired during live surgeries and\nannotated with ground truth 3D poses.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 16:43:41 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kadkhodamohammadi", "Abdolrahim", ""], ["Gangi", "Afshin", ""], ["de Mathelin", "Michel", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1701.07393", "submitter": "Youyi Zheng", "authors": "Shuai Du and Youyi Zheng", "title": "Recovering 3D Planar Arrangements from Videos", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring 3D geometry of real world objects has various applications in 3D\ndigitization, such as navigation and content generation in virtual\nenvironments. Image remains one of the most popular media for such visual tasks\ndue to its simplicity of acquisition. Traditional image-based 3D reconstruction\napproaches heavily exploit point-to-point correspondence among multiple images\nto estimate camera motion and 3D geometry. Establishing point-to-point\ncorrespondence lies at the center of the 3D reconstruction pipeline, which\nhowever is easily prone to errors. In this paper, we propose an optimization\nframework which traces image points using a novel structure-guided dynamic\ntracking algorithm and estimates both the camera motion and a 3D structure\nmodel by enforcing a set of planar constraints. The key to our method is a\nstructure model represented as a set of planes and their arrangements.\nConstraints derived from the structure model is used both in the correspondence\nestablishment stage and the bundle adjustment stage in our reconstruction\npipeline. Experiments show that our algorithm can effectively localize\nstructure correspondence across dense image frames while faithfully\nreconstructing the camera motion and the underlying structured 3D model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:33:52 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Du", "Shuai", ""], ["Zheng", "Youyi", ""]]}, {"id": "1701.07395", "submitter": "Christian Reul", "authors": "Christian Reul, Marco Dittrich, and Martin Gruner", "title": "Case Study of a highly automated Layout Analysis and OCR of an\n  incunabulum: 'Der Heiligen Leben' (1488)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides the first thorough documentation of a high quality\ndigitization process applied to an early printed book from the incunabulum\nperiod (1450-1500). The entire OCR related workflow including preprocessing,\nlayout analysis and text recognition is illustrated in detail using the example\nof 'Der Heiligen Leben', printed in Nuremberg in 1488. For each step the\nrequired time expenditure was recorded. The character recognition yielded\nexcellent results both on character (97.57%) and word (92.19%) level.\nFurthermore, a comparison of a highly automated (LAREX) and a manual (Aletheia)\nmethod for layout analysis was performed. By considerably automating the\nsegmentation the required human effort was reduced significantly from over 100\nhours to less than six hours, resulting in only a slight drop in OCR accuracy.\nRealistic estimates for the human effort necessary for full text extraction\nfrom incunabula can be derived from this study. The printed pages of the\ncomplete work together with the OCR result is available online ready to be\ninspected and downloaded.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 15:37:01 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Reul", "Christian", ""], ["Dittrich", "Marco", ""], ["Gruner", "Martin", ""]]}, {"id": "1701.07396", "submitter": "Christian Reul", "authors": "Christian Reul, Uwe Springmann, and Frank Puppe", "title": "LAREX - A semi-automatic open-source Tool for Layout Analysis and Region\n  Extraction on Early Printed Books", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-automatic open-source tool for layout analysis on early printed books\nis presented. LAREX uses a rule based connected components approach which is\nvery fast, easily comprehensible for the user and allows an intuitive manual\ncorrection if necessary. The PageXML format is used to support integration into\nexisting OCR workflows. Evaluations showed that LAREX provides an efficient and\nflexible way to segment pages of early printed books.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 09:48:59 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Reul", "Christian", ""], ["Springmann", "Uwe", ""], ["Puppe", "Frank", ""]]}, {"id": "1701.07398", "submitter": "Alon Hazan", "authors": "Alon Hazan, Yuval Harel and Ron Meir", "title": "Learning an attention model in an artificial visual system", "comments": null, "journal-ref": "IEEE International Conference on the Science of Electrical\n  Engineering (ICSEE) (2016)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Human visual perception of the world is of a large fixed image that is\nhighly detailed and sharp. However, receptor density in the retina is not\nuniform: a small central region called the fovea is very dense and exhibits\nhigh resolution, whereas a peripheral region around it has much lower spatial\nresolution. Thus, contrary to our perception, we are only able to observe a\nvery small region around the line of sight with high resolution. The perception\nof a complete and stable view is aided by an attention mechanism that directs\nthe eyes to the numerous points of interest within the scene. The eyes move\nbetween these targets in quick, unconscious movements, known as \"saccades\".\nOnce a target is centered at the fovea, the eyes fixate for a fraction of a\nsecond while the visual system extracts the necessary information. An\nartificial visual system was built based on a fully recurrent neural network\nset within a reinforcement learning protocol, and learned to attend to regions\nof interest while solving a classification task. The model is consistent with\nseveral experimentally observed phenomena, and suggests novel predictions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 09:07:59 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Hazan", "Alon", ""], ["Harel", "Yuval", ""], ["Meir", "Ron", ""]]}, {"id": "1701.07481", "submitter": "David Harwath", "authors": "David Harwath and James R. Glass", "title": "Learning Word-Like Units from Joint Audio-Visual Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of images and spoken audio captions, we present a method\nfor discovering word-like acoustic units in the continuous speech signal and\ngrounding them to semantically relevant image regions. For example, our model\nis able to detect spoken instances of the word 'lighthouse' within an utterance\nand associate them with image regions containing lighthouses. We do not use any\nform of conventional automatic speech recognition, nor do we use any text\ntranscriptions or conventional linguistic annotations. Our model effectively\nimplements a form of spoken language acquisition, in which the computer learns\nnot only to recognize word categories by sound, but also to enrich the words it\nlearns with semantics by grounding them in images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:40:56 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 15:15:41 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 22:10:25 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Harwath", "David", ""], ["Glass", "James R.", ""]]}, {"id": "1701.07604", "submitter": "Libin Sun", "authors": "Libin Sun and James Hays", "title": "Super-resolution Using Constrained Deep Texture Synthesis", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hallucinating high frequency image details in single image super-resolution\nis a challenging task. Traditional super-resolution methods tend to produce\noversmoothed output images due to the ambiguity in mapping between low and high\nresolution patches. We build on recent success in deep learning based texture\nsynthesis and show that this rich feature space can facilitate successful\ntransfer and synthesis of high frequency image details to improve the visual\nquality of super-resolution results on a wide variety of natural textures and\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 07:52:57 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Sun", "Libin", ""], ["Hays", "James", ""]]}, {"id": "1701.07675", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov, Taras\n  Holotyak", "title": "Sparse Ternary Codes for similarity search have higher coding gain than\n  dense binary codes", "comments": "Accepted at 2017 IEEE International Symposium on Information Theory\n  (ISIT'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Approximate Nearest Neighbor (ANN) search\nin pattern recognition where feature vectors in a database are encoded as\ncompact codes in order to speed-up the similarity search in large-scale\ndatabases. Considering the ANN problem from an information-theoretic\nperspective, we interpret it as an encoding, which maps the original feature\nvectors to a less entropic sparse representation while requiring them to be as\ninformative as possible. We then define the coding gain for ANN search using\ninformation-theoretic measures. We next show that the classical approach to\nthis problem, which consists of binarization of the projected vectors is\nsub-optimal. Instead, a properly designed ternary encoding achieves higher\ncoding gains and lower complexity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 12:41:58 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 09:58:45 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""], ["Holotyak", "Taras", ""]]}, {"id": "1701.07717", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng and Yi Yang", "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification\n  Baseline in vitro", "comments": "9 pages, 6 figures, accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper is a simple semi-supervised pipeline that\nonly uses the original training set without collecting extra data. It is\nchallenging in 1) how to obtain more training data only from the training set\nand 2) how to use the newly generated data. In this work, the generative\nadversarial network (GAN) is used to generate unlabeled samples. We propose the\nlabel smoothing regularization for outliers (LSRO). This method assigns a\nuniform label distribution to the unlabeled images, which regularizes the\nsupervised model and improves the baseline. We verify the proposed method on a\npractical problem: person re-identification (re-ID). This task aims to retrieve\na query person from other cameras. We adopt the deep convolutional generative\nadversarial network (DCGAN) for sample generation, and a baseline convolutional\nneural network (CNN) for representation learning. Experiments show that adding\nthe GAN-generated data effectively improves the discriminative ability of\nlearned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and\nDukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1\nprecision over the baseline CNN, respectively. We additionally apply the\nproposed method to fine-grained bird recognition and achieve a +0.6%\nimprovement over a strong baseline. The code is available at\nhttps://github.com/layumi/Person-reID_GAN.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 14:30:40 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 09:46:29 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 13:41:32 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 01:21:22 GMT"}, {"version": "v5", "created": "Tue, 22 Aug 2017 01:21:10 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1701.07732", "submitter": "Liang Zheng", "authors": "Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang", "title": "Pose Invariant Embedding for Deep Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian misalignment, which mainly arises from detector errors and pose\nvariations, is a critical problem for a robust person re-identification (re-ID)\nsystem. With bad alignment, the background noise will significantly compromise\nthe feature learning and matching process. To address this problem, this paper\nintroduces the pose invariant embedding (PIE) as a pedestrian descriptor.\nFirst, in order to align pedestrians to a standard pose, the PoseBox structure\nis introduced, which is generated through pose estimation followed by affine\ntransformations. Second, to reduce the impact of pose estimation errors and\ninformation loss during PoseBox construction, we design a PoseBox fusion (PBF)\nCNN architecture that takes the original image, the PoseBox, and the pose\nestimation confidence as input. The proposed PIE descriptor is thus defined as\nthe fully connected layer of the PBF network for the retrieval task.\nExperiments are conducted on the Market-1501, CUHK03, and VIPeR datasets. We\nshow that PoseBox alone yields decent re-ID accuracy and that when integrated\nin the PBF network, the learned PIE descriptor produces competitive performance\ncompared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 14:59:19 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Zheng", "Liang", ""], ["Huang", "Yujia", ""], ["Lu", "Huchuan", ""], ["Yang", "Yi", ""]]}, {"id": "1701.07847", "submitter": "Dmitry Petrov", "authors": "Dmitry Petrov, Boris Gutman, Alexander Ivanov, Joshua Faskowitz, Neda\n  Jahanshad, Mikhail Belyaev, Paul Thompson", "title": "Structural Connectome Validation Using Pairwise Classification", "comments": "Accepted for IEEE International Symposium on Biomedical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the extent to which structural connectomes and\ntopological derivative measures are unique to individual changes within human\nbrains. To do so, we classify structural connectome pairs from two large\nlongitudinal datasets as either belonging to the same individual or not. Our\ndata is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative\n(PPMI). We achieve 0.99 area under the ROC curve score for features which\nrepresent either weights or network structure of the connectomes (node degrees,\nPageRank and local efficiency). Our approach may be useful for eliminating\nnoisy features as a preprocessing step in brain aging studies and early\ndiagnosis classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:13:36 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 19:55:15 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Petrov", "Dmitry", ""], ["Gutman", "Boris", ""], ["Ivanov", "Alexander", ""], ["Faskowitz", "Joshua", ""], ["Jahanshad", "Neda", ""], ["Belyaev", "Mikhail", ""], ["Thompson", "Paul", ""]]}, {"id": "1701.07879", "submitter": "Gerard Rinkus", "authors": "Gerard Rinkus", "title": "A Radically New Theory of how the Brain Represents and Computes with\n  Probabilities", "comments": "33 pages, 10 figures - Sec. explaining single cell tuning fns as\n  artifacts of embedding SDRs in superposition removed (for future paper) -\n  Clarified that a given SDR code represents the whole likelihood distribution\n  over stored hypotheses at a coarsely-ranked level of fidelity (Submitted for\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is believed to implement probabilistic reasoning and to represent\ninformation via population, or distributed, coding. Most previous\npopulation-based probabilistic (PPC) theories share several basic properties:\n1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e.,\nall(most) units participate in every code; 3) graded synapses; 4) rate coding;\n5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy\nunits; and 7) noise/correlation is considered harmful. We present a radically\ndifferent theory that assumes: 1) binary units; 2) only a small subset of\nunits, i.e., a sparse distributed representation (SDR) (cell assembly),\ncomprises any individual code; 3) binary synapses; 4) signaling formally\nrequires only single (i.e., first) spikes; 5) units initially have completely\nflat TFs (all weights zero); 6) units are far less intrinsically noisy than\ntraditionally thought; rather 7) noise is a resource generated/used to cause\nsimilar inputs to map to similar codes, controlling a tradeoff between storage\ncapacity and embedding the input space statistics in the pattern of\nintersections over stored codes, epiphenomenally determining correlation\npatterns across neurons. The theory, Sparsey, was introduced 20+ years ago as a\ncanonical cortical circuit/algorithm model achieving efficient sequence\nlearning/recognition, but not elaborated as an alternative to PPC theories.\nHere, we show that: a) the active SDR simultaneously represents both the most\nsimilar/likely input and the entire (coarsely-ranked) similarity\nlikelihood/distribution over all stored inputs (hypotheses); and b) given an\ninput, the SDR code selection algorithm, which underlies both learning and\ninference, updates both the most likely hypothesis and the entire likelihood\ndistribution (cf. belief update) with a number of steps that remains constant\nas the number of stored items increases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:16:32 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:37:42 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:16:39 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 23:00:01 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Rinkus", "Gerard", ""]]}, {"id": "1701.07901", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Tao He, Lianli Gao, Xing Xu, Heng Tao Shen", "title": "Deep Region Hashing for Efficient Large-scale Instance Search from\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance Search (INS) is a fundamental problem for many applications, while\nit is more challenging comparing to traditional image search since the\nrelevancy is defined at the instance level.\n  Existing works have demonstrated the success of many complex ensemble systems\nthat are typically conducted by firstly generating object proposals, and then\nextracting handcrafted and/or CNN features of each proposal for matching.\nHowever, object bounding box proposals and feature extraction are often\nconducted in two separated steps, thus the effectiveness of these methods\ncollapses. Also, due to the large amount of generated proposals, matching speed\nbecomes the bottleneck that limits its application to large-scale datasets. To\ntackle these issues, in this paper we propose an effective and efficient Deep\nRegion Hashing (DRH) approach for large-scale INS using an image patch as the\nquery. Specifically, DRH is an end-to-end deep neural network which consists of\nobject proposal, feature extraction, and hash code generation. DRH shares\nfull-image convolutional feature map with the region proposal network, thus\nenabling nearly cost-free region proposals. Also, each high-dimensional,\nreal-valued region features are mapped onto a low-dimensional, compact binary\ncodes for the efficient object region level matching on large-scale dataset.\nExperimental results on four datasets show that our DRH can achieve even better\nperformance than the state-of-the-arts in terms of MAP, while the efficiency is\nimproved by nearly 100 times.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 23:18:58 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Song", "Jingkuan", ""], ["He", "Tao", ""], ["Gao", "Lianli", ""], ["Xu", "Xing", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1701.08006", "submitter": "Yifang Xu", "authors": "Nan Li, Yifang Xu, and Chao Wang", "title": "Quasi-homography warps in image stitching", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The naturalness of warps is gaining extensive attentions in image stitching.\nRecent warps such as SPHP and AANAP, use global similarity warps to mitigate\nprojective distortion (which enlarges regions), however, they necessarily bring\nin perspective distortion (which generates inconsistencies). In this paper, we\npropose a novel quasi-homography warp, which effectively balances the\nperspective distortion against the projective distortion in the non-overlapping\nregion to create a more natural-looking panorama. Our approach formulates the\nwarp as the solution of a bivariate system, where perspective distortion and\nprojective distortion are characterized as slope preservation and scale\nlinearization respectively. Because our proposed warp only relies on a global\nhomography, thus it is totally parameter-free. A comprehensive experiment shows\nthat a quasi-homography warp outperforms some state-of-the-art warps in urban\nscenes, including homography, AutoStitch and SPHP. A user study demonstrates\nthat it wins most users' favor, comparing to homography and SPHP.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 10:45:45 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:38:06 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Li", "Nan", ""], ["Xu", "Yifang", ""], ["Wang", "Chao", ""]]}, {"id": "1701.08025", "submitter": "Magnus Andersson", "authors": "Hanqing Zhang, Tim Stangner, Krister Wiklund, Alvaro Rodriguez, Magnus\n  Andersson", "title": "UmUTracker: A versatile MATLAB program for automated particle tracking\n  of 2D light microscopy or 3D digital holography data", "comments": "Manuscript including supplementary materials", "journal-ref": null, "doi": "10.1016/j.cpc.2017.05.029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a versatile and fast MATLAB program (UmUTracker) that\nautomatically detects and tracks particles by analyzing video sequences\nacquired by either light microscopy or digital in-line holographic microscopy.\nOur program detects the 2D lateral positions of particles with an algorithm\nbased on the isosceles triangle transform, and reconstructs their 3D axial\npositions by a fast implementation of the Rayleigh-Sommerfeld model using a\nradial intensity profile. To validate the accuracy and performance of our\nprogram, we first track the 2D position of polystyrene particles using bright\nfield and digital holographic microscopy. Second, we determine the 3D particle\nposition by analyzing synthetic and experimentally acquired holograms. Finally,\nto highlight the full program features, we profile the microfluidic flow in a\n100 micrometer high flow chamber. This result agrees with computational fluid\ndynamic simulations. On a regular desktop computer UmUTracker can detect,\nanalyze, and track multiple particles at 5 frames per second for a template\nsize of 201 x 201 in a 1024 x 1024 image. To enhance usability and to make it\neasy to implement new functions we used object-oriented programming. UmUTracker\nis suitable for studies related to: particle dynamics, cell localization,\ncolloids and microfluidic flow measurement.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:20:45 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 08:53:23 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Zhang", "Hanqing", ""], ["Stangner", "Tim", ""], ["Wiklund", "Krister", ""], ["Rodriguez", "Alvaro", ""], ["Andersson", "Magnus", ""]]}, {"id": "1701.08092", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI), Michel Jourlin (LHC, IPRI)", "title": "Double-sided probing by map of Asplund's distances using Logarithmic\n  Image Processing in the framework of Mathematical Morphology", "comments": "The final publication is available at link.springer.com", "journal-ref": "13th International Symposium on Mathematical Morphology, ISMM\n  2017, May 2017, Fontainebleau, France. Springer International Publishing,\n  pp.408-420, 2017, Mathematical Morphology and Its Applications to Signal and\n  Image Processing: 13th International Symposium, ISMM 2017, Fontainebleau,\n  France, May 15--17, 2017, Proceedings. http://cmm.ensmp.fr/ismm2017/", "doi": "10.1007/978-3-319-57240-6_33", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the link between Mathematical Morphology and the map of\nAsplund's distances between a probe and a grey scale function, using the\nLogarithmic Image Processing scalar multiplication. We demonstrate that the map\nis the logarithm of the ratio between a dilation and an erosion of the function\nby a structuring function: the probe. The dilations and erosions are mappings\nfrom the lattice of the images into the lattice of the positive functions.\nUsing a flat structuring element, the expression of the map of Asplund's\ndistances can be simplified with a dilation and an erosion of the image; these\nmappings stays in the lattice of the images. We illustrate our approach by an\nexample of pattern matching with a non-flat structuring function.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 15:55:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:47:09 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 09:44:30 GMT"}, {"version": "v4", "created": "Thu, 18 May 2017 14:22:37 GMT"}, {"version": "v5", "created": "Thu, 25 Jan 2018 07:57:28 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"], ["Jourlin", "Michel", "", "LHC, IPRI"]]}, {"id": "1701.08107", "submitter": "Ahmed Karam Eldaly MSc", "authors": "Ahmed Karam Eldaly, Yoann Altmann, Antonios Perperidis, Nikola\n  Krstajic, Tushar Choudhary, Kevin Dhaliwal, and Stephen McLaughlin", "title": "Deconvolution and Restoration of Optical Endomicroscopy Images", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2018.2811939", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical endomicroscopy (OEM) is an emerging technology platform with\npreclinical and clinical imaging applications. Pulmonary OEM via fibre bundles\nhas the potential to provide in vivo, in situ molecular signatures of disease\nsuch as infection and inflammation. However, enhancing the quality of data\nacquired by this technique for better visualization and subsequent analysis\nremains a challenging problem. Cross coupling between fiber cores and sparse\nsampling by imaging fiber bundles are the main reasons for image degradation,\nand poor detection performance (i.e., inflammation, bacteria, etc.). In this\nwork, we address the problem of deconvolution and restoration of OEM data. We\npropose a hierarchical Bayesian model to solve this problem and compare three\nestimation algorithms to exploit the resulting joint posterior distribution.\nThe first method is based on Markov chain Monte Carlo (MCMC) methods, however,\nit exhibits a relatively long computational time. The second and third\nalgorithms deal with this issue and are based on a variational Bayes (VB)\napproach and an alternating direction method of multipliers (ADMM) algorithm\nrespectively. Results on both synthetic and real datasets illustrate the\neffectiveness of the proposed methods for restoration of OEM images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 16:37:03 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 10:12:20 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 13:44:54 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Eldaly", "Ahmed Karam", ""], ["Altmann", "Yoann", ""], ["Perperidis", "Antonios", ""], ["Krstajic", "Nikola", ""], ["Choudhary", "Tushar", ""], ["Dhaliwal", "Kevin", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "1701.08180", "submitter": "Jhony Heriberto Giraldo Zuluaga", "authors": "Jhony-Heriberto Giraldo-Zuluaga, Alexander Gomez, Augusto Salazar, and\n  Ang\\'elica Diaz-Pulido", "title": "Camera-trap images segmentation using multi-layer robust principal\n  component analysis", "comments": "This is a pre-print of an article published in The Visual Computer.\n  The final authenticated version is available online at:\n  https://doi.org/10.1007/s00371-017-1463-9", "journal-ref": "The Visual Computer, 1-13 (2017)", "doi": "10.1007/s00371-017-1463-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of animals from camera-trap images is a difficult task. To\nillustrate, there are various challenges due to environmental conditions and\nhardware limitation in these images. We proposed a multi-layer robust principal\ncomponent analysis (multi-layer RPCA) approach for background subtraction. Our\nmethod computes sparse and low-rank images from a weighted sum of descriptors,\nusing color and texture features as case of study for camera-trap images\nsegmentation. The segmentation algorithm is composed of histogram equalization\nor Gaussian filtering as pre-processing, and morphological filters with active\ncontour as post-processing. The parameters of our multi-layer RPCA were\noptimized with an exhaustive search. The database consists of camera-trap\nimages from the Colombian forest taken by the Instituto de Investigaci\\'on de\nRecursos Biol\\'ogicos Alexander von Humboldt. We analyzed the performance of\nour method in inherent and therefore challenging situations of camera-trap\nimages. Furthermore, we compared our method with some state-of-the-art\nalgorithms of background subtraction, where our multi-layer RPCA outperformed\nthese other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average\nfine-grained F-measure for color and infrared sequences, respectively. To our\nbest knowledge, this paper is the first work proposing multi-layer RPCA and\nusing it for camera-trap images segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 19:41:29 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 22:15:32 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Giraldo-Zuluaga", "Jhony-Heriberto", ""], ["Gomez", "Alexander", ""], ["Salazar", "Augusto", ""], ["Diaz-Pulido", "Ang\u00e9lica", ""]]}, {"id": "1701.08222", "submitter": "Ayush Bhandari", "authors": "Ayush Bhandari, Aurelien Bourquard and Ramesh Raskar", "title": "Sampling Without Time: Recovering Echoes of Light via Temporal Phase\n  Retrieval", "comments": "12 pages, 4 figures, to appear at the 42nd IEEE International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of sampling and reconstruction of a\ncontinuous-time sparse signal without assuming the knowledge of the sampling\ninstants or the sampling rate. This topic has its roots in the problem of\nrecovering multiple echoes of light from its low-pass filtered and\nauto-correlated, time-domain measurements. Our work is closely related to the\ntopic of sparse phase retrieval and in this context, we discuss the advantage\nof phase-free measurements. While this problem is ill-posed, cues based on\nphysical constraints allow for its appropriate regularization. We validate our\ntheory with experiments based on customized, optical time-of-flight imaging\nsensors. What singles out our approach is that our sensing method allows for\ntemporal phase retrieval as opposed to the usual case of spatial phase\nretrieval. Preliminary experiments and results demonstrate a compelling\ncapability of our phase-retrieval based imaging device.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 23:52:08 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Bhandari", "Ayush", ""], ["Bourquard", "Aurelien", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1701.08237", "submitter": "Tong Ke", "authors": "Tong Ke, Stergios Roumeliotis", "title": "An Efficient Algebraic Solution to the Perspective-Three-Point Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an algebraic solution to the classical\nperspective-3-point (P3P) problem for determining the position and attitude of\na camera from observations of three known reference points. In contrast to\nprevious approaches, we first directly determine the camera's attitude by\nemploying the corresponding geometric constraints to formulate a system of\ntrigonometric equations. This is then efficiently solved, following an\nalgebraic approach, to determine the unknown rotation matrix and subsequently\nthe camera's position. As compared to recent alternatives, our method avoids\ncomputing unnecessary (and potentially numerically unstable) intermediate\nresults, and thus achieves higher numerical accuracy and robustness at a lower\ncomputational cost. These benefits are validated through extensive Monte-Carlo\nsimulations for both nominal and close-to-singular geometric configurations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 02:16:48 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ke", "Tong", ""], ["Roumeliotis", "Stergios", ""]]}, {"id": "1701.08251", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley,\n  Jianfeng Gao, Georgios P. Spithourakis, Lucy Vanderwende", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question\n  and Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of image sharing on social media and the engagement it creates\nbetween users reflects the important role that visual context plays in everyday\nconversations. We present a novel task, Image-Grounded Conversations (IGC), in\nwhich natural-sounding conversations are generated about a shared image. To\nbenchmark progress, we introduce a new multiple-reference dataset of\ncrowd-sourced, event-centric conversations on images. IGC falls on the\ncontinuum between chit-chat and goal-directed conversation models, where visual\ngrounding constrains the topic of conversation to event-driven utterances.\nExperiments with models trained on social media data show that the combination\nof visual and textual context enhances the quality of generated conversational\nturns. In human evaluation, the gap between human performance and that of both\nneural and retrieval architectures suggests that multi-modal IGC presents an\ninteresting challenge for dialogue research.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 05:06:11 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 00:36:35 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Brockett", "Chris", ""], ["Dolan", "Bill", ""], ["Galley", "Michel", ""], ["Gao", "Jianfeng", ""], ["Spithourakis", "Georgios P.", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1701.08257", "submitter": "Nitin Malik Dr", "authors": "Smriti Tikoo, Nitin Malik", "title": "Detection of Face using Viola Jones and Recognition using Back\n  Propagation Neural Network", "comments": "ISSN 2320-088X, 8 pages, 5 figures, 1 table", "journal-ref": "Int J. Computer Science and Mobile Computing, vol. 5, issue 5, pp.\n  288-295 (May 2016)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and recognition of the facial images of people is an intricate\nproblem which has garnered much attention during recent years due to its ever\nincreasing applications in numerous fields. It continues to pose a challenge in\nfinding a robust solution to it. Its scope extends to catering the security,\ncommercial and law enforcement applications. Research for moreover a decade on\nthis subject has brought about remarkable development with the modus operandi\nlike human computer interaction, biometric analysis and content based coding of\nimages, videos and surveillance. A trivial task for brain but cumbersome to be\nimitated artificially. The commonalities in faces does pose a problem on\nvarious grounds but features such as skin color, gender differentiate a person\nfrom the other. In this paper the facial detection has been carried out using\nViola-Jones algorithm and recognition of face has been done using Back\nPropagation Neural Network (BPNN).\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 05:51:44 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tikoo", "Smriti", ""], ["Malik", "Nitin", ""]]}, {"id": "1701.08259", "submitter": "Nitin Malik Dr", "authors": "Smriti Tikoo, Nitin Malik", "title": "Detection, Segmentation and Recognition of Face and its Features Using\n  Neural Network", "comments": "Google Scholar Indexed Journal, 5 pages, 10 figures, Journal of\n  Biosensors and Bioelectronics, vol. 7, no. 2, June-Sept 2016", "journal-ref": null, "doi": "10.4172/2155-6210.1000210", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and recognition has been prevalent with research scholars and\ndiverse approaches have been incorporated till date to serve purpose. The\nrampant advent of biometric analysis systems, which may be full body scanners,\nor iris detection and recognition systems and the finger print recognition\nsystems, and surveillance systems deployed for safety and security purposes\nhave contributed to inclination towards same. Advances has been made with\nfrontal view, lateral view of the face or using facial expressions such as\nanger, happiness and gloominess, still images and video image to be used for\ndetection and recognition. This led to newer methods for face detection and\nrecognition to be introduced in achieving accurate results and economically\nfeasible and extremely secure. Techniques such as Principal Component analysis\n(PCA), Independent component analysis (ICA), Linear Discriminant Analysis\n(LDA), have been the predominant ones to be used. But with improvements needed\nin the previous approaches Neural Networks based recognition was like boon to\nthe industry. It not only enhanced the recognition but also the efficiency of\nthe process. Choosing Backpropagation as the learning method was clearly out of\nits efficiency to recognize nonlinear faces with an acceptance ratio of more\nthan 90% and execution time of only few seconds.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 06:44:31 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tikoo", "Smriti", ""], ["Malik", "Nitin", ""]]}, {"id": "1701.08261", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Rodrigo Benenson, Anna Khoreva, Zeynep Akata, Mario\n  Fritz, Bernt Schiele", "title": "Exploiting saliency for object segmentation from image level labels", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been remarkable improvements in the semantic labelling task in the\nrecent years. However, the state of the art methods rely on large-scale\npixel-level annotations. This paper studies the problem of training a\npixel-wise semantic labeller network from image-level annotations of the\npresent object classes. Recently, it has been shown that high quality seeds\nindicating discriminative object regions can be obtained from image-level\nlabels. Without additional information, obtaining the full extent of the object\nis an inherently ill-posed problem due to co-occurrences. We propose using a\nsaliency model as additional information and hereby exploit prior knowledge on\nthe object extent and image statistics. We show how to combine both information\nsources in order to recover 80% of the fully supervised performance - which is\nthe new state of the art in weakly supervised training for pixel-wise semantic\nlabelling. The code is available at https://goo.gl/KygSeb.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 06:58:40 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 09:05:11 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Oh", "Seong Joon", ""], ["Benenson", "Rodrigo", ""], ["Khoreva", "Anna", ""], ["Akata", "Zeynep", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1701.08280", "submitter": "Kunal Narayan Chaudhury", "authors": "Sanjay Ghosh, Amit K. Mandal, and Kunal N. Chaudhury", "title": "Pruned non-local means", "comments": "Published in IET Image Processing", "journal-ref": null, "doi": "10.1049/iet-ipr.2016.0331", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Non-Local Means (NLM), each pixel is denoised by performing a weighted\naveraging of its neighboring pixels, where the weights are computed using image\npatches. We demonstrate that the denoising performance of NLM can be improved\nby pruning the neighboring pixels, namely, by rejecting neighboring pixels\nwhose weights are below a certain threshold $\\lambda$. While pruning can\npotentially reduce pixel averaging in uniform-intensity regions, we demonstrate\nthat there is generally an overall improvement in the denoising performance. In\nparticular, the improvement comes from pixels situated close to edges and\ncorners. The success of the proposed method strongly depends on the choice of\nthe global threshold $\\lambda$, which in turn depends on the noise level and\nthe image characteristics. We show how Stein's unbiased estimator of the\nmean-squared error can be used to optimally tune $\\lambda$, at a marginal\ncomputational overhead. We present some representative denoising results to\ndemonstrate the superior performance of the proposed method over NLM and its\nvariants.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 10:57:01 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 12:11:05 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Ghosh", "Sanjay", ""], ["Mandal", "Amit K.", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1701.08289", "submitter": "Xudong Sun", "authors": "Xudong Sun, Pengcheng Wu, Steven C.H. Hoi", "title": "Face Detection using Deep Learning: An Improved Faster RCNN Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present a new face detection scheme using deep learning\nand achieve the state-of-the-art detection performance on the well-known FDDB\nface detetion benchmark evaluation. In particular, we improve the\nstate-of-the-art faster RCNN framework by combining a number of strategies,\nincluding feature concatenation, hard negative mining, multi-scale training,\nmodel pretraining, and proper calibration of key parameters. As a consequence,\nthe proposed scheme obtained the state-of-the-art face detection performance,\nmaking it the best model in terms of ROC curves among all the published methods\non the FDDB benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 13:33:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Sun", "Xudong", ""], ["Wu", "Pengcheng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1701.08291", "submitter": "Ilke Cugu", "authors": "\\.Ilke \\c{C}u\\u{g}u, Eren \\c{S}ener, \\c{C}a\\u{g}r{\\i} Erciyes, Burak\n  Balc{\\i}, Emre Ak{\\i}n, It{\\i}r \\\"Onal, Ahmet O\\u{g}uz Aky\\\"uz", "title": "Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel tree classification system called Treelogy, that fuses\ndeep representations with hand-crafted features obtained from leaf images to\nperform leaf-based plant classification. Key to this system are segmentation of\nthe leaf from an untextured background, using convolutional neural networks\n(CNNs) for learning deep representations, extracting hand-crafted features with\na number of image processing techniques, training a linear SVM with feature\nvectors, merging SVM and CNN results, and identifying the species from a\ndataset of 57 trees. Our classification results show that fusion of deep\nrepresentations with hand-crafted features leads to the highest accuracy. The\nproposed algorithm is embedded in a smart-phone application, which is publicly\navailable. Furthermore, our novel dataset comprised of 5408 leaf images is also\nmade public for use of other researchers.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 13:41:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["\u00c7u\u011fu", "\u0130lke", ""], ["\u015eener", "Eren", ""], ["Erciyes", "\u00c7a\u011fr\u0131", ""], ["Balc\u0131", "Burak", ""], ["Ak\u0131n", "Emre", ""], ["\u00d6nal", "It\u0131r", ""], ["Aky\u00fcz", "Ahmet O\u011fuz", ""]]}, {"id": "1701.08341", "submitter": "Upal Mahbub", "authors": "Upal Mahbub, Sayantan Sarkar, Rama Chellappa", "title": "Pooling Facial Segments to Face: The Shallow and Deep Ends", "comments": "8 pages, 7 figures, 3 tables, accepted for publication in FG2017", "journal-ref": "2017 12th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2017)", "doi": "10.1109/FG.2017.80", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic face detection algorithms do not perform very well in the mobile\ndomain due to significant presence of occluded and partially visible faces. One\npromising technique to handle the challenge of partial faces is to design face\ndetectors based on facial segments. In this paper two such face detectors\nnamely, SegFace and DeepSegFace, are proposed that detect the presence of a\nface given arbitrary combinations of certain face segments. Both methods use\nproposals from facial segments as input that are found using weak boosted\nclassifiers. SegFace is a shallow and fast algorithm using traditional\nfeatures, tailored for situations where real time constraints must be\nsatisfied. On the other hand, DeepSegFace is a more powerful algorithm based on\na deep convolutional neutral network (DCNN) architecture. DeepSegFace offers\ncertain advantages over other DCNN-based face detectors as it requires\nrelatively little amount of data to train by utilizing a novel data\naugmentation scheme and is very robust to occlusion by design. Extensive\nexperiments show the superiority of the proposed methods, specially\nDeepSegFace, over other state-of-the-art face detectors in terms of\nprecision-recall and ROC curve on two mobile face datasets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 00:30:09 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mahbub", "Upal", ""], ["Sarkar", "Sayantan", ""], ["Chellappa", "Rama", ""]]}, {"id": "1701.08349", "submitter": "Xiaoxia Sun", "authors": "Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran", "title": "Supervised Deep Sparse Coding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the deep sparse coding network (SCN), a novel deep\nnetwork that encodes intermediate representations with nonnegative sparse\ncoding. The SCN is built upon a number of cascading bottleneck modules, where\neach module consists of two sparse coding layers with relatively wide and slim\ndictionaries that are specialized to produce high dimensional discriminative\nfeatures and low dimensional representations for clustering, respectively.\nDuring training, both the dictionaries and regularization parameters are\noptimized with an end-to-end supervised learning algorithm based on multilevel\noptimization. Effectiveness of an SCN with seven bottleneck modules is verified\non several popular benchmark datasets. Remarkably, with few parameters to\nlearn, our SCN achieves 5.81% and 19.93% classification error rate on CIFAR-10\nand CIFAR-100, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 04:03:39 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 01:33:04 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 01:31:04 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Sun", "Xiaoxia", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1701.08374", "submitter": "Habib Ghaffari Hadigheh", "authors": "Habib Ghaffari Hadigheh and Ghazali bin sulong", "title": "Feature base fusion for splicing forgery detection based on neuro fuzzy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of researches on image forensics have been mainly focused on detection\nof artifacts introduced by a single processing tool. They lead in the\ndevelopment of many specialized algorithms looking for one or more particular\nfootprints under specific settings. Naturally, the performance of such\nalgorithms are not perfect, and accordingly the provided output might be noisy,\ninaccurate and only partially correct. Furthermore, a forged image in practical\nscenarios is often the result of utilizing several tools available by\nimage-processing software systems. Therefore, reliable tamper detection\nrequires developing more poweful tools to deal with various tempering\nscenarios. Fusion of forgery detection tools based on Fuzzy Inference System\nhas been used before for addressing this problem. Adjusting the membership\nfunctions and defining proper fuzzy rules for attaining to better results are\ntime-consuming processes. This can be accounted as main disadvantage of fuzzy\ninference systems. In this paper, a Neuro-Fuzzy inference system for fusion of\nforgery detection tools is developed. The neural network characteristic of\nthese systems provides appropriate tool for automatically adjusting the\nmembership functions. Moreover, initial fuzzy inference system is generated\nbased on fuzzy clustering techniques. The proposed framework is implemented and\nvalidated on a benchmark image splicing data set in which three forgery\ndetection tools are fused based on adaptive Neuro-Fuzzy inference system. The\noutcome of the proposed method reveals that applying Neuro Fuzzy inference\nsystems could be a better approach for fusion of forgery detection tools.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:19:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Hadigheh", "Habib Ghaffari", ""], ["sulong", "Ghazali bin", ""]]}, {"id": "1701.08376", "submitter": "Ronald Clark", "authors": "Ronald Clark, Sen Wang, Hongkai Wen, Andrew Markham, Niki Trigoni", "title": "VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning\n  Problem", "comments": "AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an on-manifold sequence-to-sequence learning\napproach to motion estimation using visual and inertial sensors. It is to the\nbest of our knowledge the first end-to-end trainable method for visual-inertial\nodometry which performs fusion of the data at an intermediate\nfeature-representation level. Our method has numerous advantages over\ntraditional approaches. Specifically, it eliminates the need for tedious manual\nsynchronization of the camera and IMU as well as eliminating the need for\nmanual calibration between the IMU and camera. A further advantage is that our\nmodel naturally and elegantly incorporates domain specific information which\nsignificantly mitigates drift. We show that our approach is competitive with\nstate-of-the-art traditional methods when accurate calibration data is\navailable and can be trained to outperform them in the presence of calibration\nand synchronization errors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:34:22 GMT"}, {"version": "v2", "created": "Sun, 2 Apr 2017 17:11:53 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Clark", "Ronald", ""], ["Wang", "Sen", ""], ["Wen", "Hongkai", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1701.08378", "submitter": "Dilip K. Prasad", "authors": "D. K. Prasad, D. Rajan, C. K. Prasath, L. Rachmawati, E. Rajabaly, C.\n  Quek", "title": "MSCM-LiFe: Multi-scale cross modal linear feature for horizon detection\n  in maritime images", "comments": "5 pages, 4 figures, IEEE TENCON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for horizon detection called the multi-scale\ncross modal linear feature. This method integrates three different concepts\nrelated to the presence of horizon in maritime images to increase the accuracy\nof horizon detection. Specifically it uses the persistence of horizon in\nmulti-scale median filtering, and its detection as a linear feature commonly\ndetected by two different methods, namely the Hough transform of edgemap and\nthe intensity gradient. We demonstrate the performance of the method over 13\nvideos comprising of more than 3000 frames and show that the proposed method\ndetects horizon with small error in most of the cases, outperforming three\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:35:47 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Prasad", "D. K.", ""], ["Rajan", "D.", ""], ["Prasath", "C. K.", ""], ["Rachmawati", "L.", ""], ["Rajabaly", "E.", ""], ["Quek", "C.", ""]]}, {"id": "1701.08380", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "The HASYv2 dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the HASYv2 dataset. HASY is a publicly available, free\nof charge dataset of single symbols similar to MNIST. It contains 168233\ninstances of 369 classes. HASY contains two challenges: A classification\nchallenge with 10 pre-defined folds for 10-fold cross-validation and a\nverification challenge.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:42:14 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1701.08393", "submitter": "Shuo Yang", "authors": "Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Faceness-Net: Face Detection through Deep Facial Part Responses", "comments": "Will appear in TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1509.06451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep convolutional neural network (CNN) for face detection\nleveraging on facial attributes based supervision. We observe a phenomenon that\npart detectors emerge within CNN trained to classify attributes from uncropped\nface images, without any explicit part supervision. The observation motivates a\nnew method for finding faces through scoring facial parts responses by their\nspatial structure and arrangement. The scoring mechanism is data-driven, and\ncarefully formulated considering challenging cases where faces are only\npartially visible. This consideration allows our network to detect faces under\nsevere occlusion and unconstrained pose variations. Our method achieves\npromising performance on popular benchmarks including FDDB, PASCAL Faces, AFW,\nand WIDER FACE.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 16:11:27 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 05:48:43 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 12:36:25 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Yang", "Shuo", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1701.08398", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li", "title": "Re-ranking Person Re-identification with k-reciprocal Encoding", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering person re-identification (re-ID) as a retrieval process,\nre-ranking is a critical step to improve its accuracy. Yet in the re-ID\ncommunity, limited effort has been devoted to re-ranking, especially those\nfully automatic, unsupervised solutions. In this paper, we propose a\nk-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is\nthat if a gallery image is similar to the probe in the k-reciprocal nearest\nneighbors, it is more likely to be a true match. Specifically, given an image,\na k-reciprocal feature is calculated by encoding its k-reciprocal nearest\nneighbors into a single vector, which is used for re-ranking under the Jaccard\ndistance. The final distance is computed as the combination of the original\ndistance and the Jaccard distance. Our re-ranking method does not require any\nhuman interaction or any labeled data, so it is applicable to large-scale\ndatasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW\ndatasets confirm the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 16:31:51 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 14:53:20 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 12:57:33 GMT"}, {"version": "v4", "created": "Fri, 5 May 2017 02:46:47 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhong", "Zhun", ""], ["Zheng", "Liang", ""], ["Cao", "Donglin", ""], ["Li", "Shaozi", ""]]}, {"id": "1701.08401", "submitter": "Dimitri Van De Ville", "authors": "Dimitri Van De Ville, Robin Demesmaeker, Maria Giulia Preti", "title": "When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum", "comments": "4 pages, 5 figures, submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2704359", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of complex systems benefits from graph models and their analysis.\nIn particular, the eigendecomposition of the graph Laplacian lets emerge\nproperties of global organization from local interactions; e.g., the Fiedler\nvector has the smallest non-zero eigenvalue and plays a key role for graph\nclustering. Graph signal processing focusses on the analysis of signals that\nare attributed to the graph nodes. The eigendecomposition of the graph\nLaplacian allows to define the graph Fourier transform and extend conventional\nsignal-processing operations to graphs. Here, we introduce the design of\nSlepian graph signals, by maximizing energy concentration in a predefined\nsubgraph for a graph spectral bandlimit. We establish a novel link with\nclassical Laplacian embedding and graph clustering, which provides a meaning to\nlocalized graph frequencies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 17:11:13 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 12:53:26 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Van De Ville", "Dimitri", ""], ["Demesmaeker", "Robin", ""], ["Preti", "Maria Giulia", ""]]}, {"id": "1701.08435", "submitter": "Joost van Amersfoort", "authors": "Joost van Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur\n  Szlam, Du Tran and Soumith Chintala", "title": "Transformation-Based Models of Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a simple unsupervised approach for next frame\nprediction in video. Instead of directly predicting the pixels in a frame given\npast frames, we predict the transformations needed for generating the next\nframe in a sequence, given the transformations of the past frames. This leads\nto sharper results, while using a smaller prediction model.\n  In order to enable a fair comparison between different video frame prediction\nmodels, we also propose a new evaluation protocol. We use generated frames as\ninput to a classifier trained with ground truth sequences. This criterion\nguarantees that models scoring high are those producing sequences which\npreserve discrim- inative features, as opposed to merely penalizing any\ndeviation, plausible or not, from the ground truth. Our proposed approach\ncompares favourably against more sophisticated ones on the UCF-101 data set,\nwhile also being more efficient in terms of the number of parameters and\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 21:39:05 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 20:20:40 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["van Amersfoort", "Joost", ""], ["Kannan", "Anitha", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""], ["Tran", "Du", ""], ["Chintala", "Soumith", ""]]}, {"id": "1701.08449", "submitter": "Junaed Sattar", "authors": "Junaed Sattar and Jiawei Mo", "title": "SafeDrive: A Robust Lane Tracking System for Autonomous and Assisted\n  Driving Under Limited Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach towards robust lane tracking for assisted and\nautonomous driving, particularly under poor visibility. Autonomous detection of\nlane markers improves road safety, and purely visual tracking is desirable for\nwidespread vehicle compatibility and reducing sensor intrusion, cost, and\nenergy consumption. However, visual approaches are often ineffective because of\na number of factors, including but not limited to occlusion, poor weather\nconditions, and paint wear-off. Our method, named SafeDrive, attempts to\nimprove visual lane detection approaches in drastically degraded visual\nconditions without relying on additional active sensors. In scenarios where\nvisual lane detection algorithms are unable to detect lane markers, the\nproposed approach uses location information of the vehicle to locate and access\nalternate imagery of the road and attempts detection on this secondary image.\nSubsequently, by using a combination of feature-based and pixel-based\nalignment, an estimated location of the lane marker is found in the current\nscene. We demonstrate the effectiveness of our system on actual driving data\nfrom locations in the United States with Google Street View as the source of\nalternate imagery.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 23:17:21 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Sattar", "Junaed", ""], ["Mo", "Jiawei", ""]]}, {"id": "1701.08475", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Jie Yang, Cheng-Hao Deng", "title": "Scalable Nearest Neighbor Search based on kNN Graph", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:51:28 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 09:37:53 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Yang", "Jie", ""], ["Deng", "Cheng-Hao", ""]]}, {"id": "1701.08481", "submitter": "C.-C. Jay Kuo", "authors": "C.-C. Jay Kuo", "title": "CNN as Guided Multi-layer RECOS Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a resurging interest in developing a neural-network-based solution\nto the supervised machine learning problem. The convolutional neural network\n(CNN) will be studied in this note. To begin with, we introduce a RECOS\ntransform as a basic building block of CNNs. The \"RECOS\" is an acronym for\n\"REctified-COrrelations on a Sphere\". It consists of two main concepts: 1) data\nclustering on a sphere and 2) rectification. Afterwards, we interpret a CNN as\na network that implements the guided multi-layer RECOS transform with three\nhighlights. First, we compare the traditional single-layer and modern\nmulti-layer signal analysis approaches, point out key ingredients that enable\nthe multi-layer approach, and provide a full explanation to the operating\nprinciple of CNNs. Second, we discuss how guidance is provided by labels\nthrough backpropagation (BP) in the training. Third, we show that a trained\nnetwork can be greatly simplified in the testing stage demanding only one-bit\nrepresentation for both filter weights and inputs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 04:39:36 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 07:41:27 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 07:42:24 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Kuo", "C. -C. Jay", ""]]}, {"id": "1701.08493", "submitter": "Onur Ozyesil", "authors": "Onur Ozyesil, Vladislav Voroninski, Ronen Basri, Amit Singer", "title": "A Survey of Structure from Motion", "comments": "40 pages, 16 figures; Updated title and abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure from motion (SfM) problem in computer vision is the problem of\nrecovering the three-dimensional ($3$D) structure of a stationary scene from a\nset of projective measurements, represented as a collection of two-dimensional\n($2$D) images, via estimation of motion of the cameras corresponding to these\nimages. In essence, SfM involves the three main stages of (1) extraction of\nfeatures in images (e.g., points of interest, lines, etc.) and matching these\nfeatures between images, (2) camera motion estimation (e.g., using relative\npairwise camera positions estimated from the extracted features), and (3)\nrecovery of the $3$D structure using the estimated motion and features (e.g.,\nby minimizing the so-called reprojection error). This survey mainly focuses on\nrelatively recent developments in the literature pertaining to stages (2) and\n(3). More specifically, after touching upon the early factorization-based\ntechniques for motion and structure estimation, we provide a detailed account\nof some of the recent camera location estimation methods in the literature,\nfollowed by discussion of notable techniques for $3$D structure recovery. We\nalso cover the basics of the simultaneous localization and mapping (SLAM)\nproblem, which can be viewed as a specific case of the SfM problem. Further,\nour survey includes a review of the fundamentals of feature extraction and\nmatching (i.e., stage (1) above), various recent methods for handling\nambiguities in $3$D scenes, SfM techniques involving relatively uncommon camera\nmodels and image features, and popular sources of data and SfM software.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 06:47:44 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 01:19:41 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Ozyesil", "Onur", ""], ["Voroninski", "Vladislav", ""], ["Basri", "Ronen", ""], ["Singer", "Amit", ""]]}, {"id": "1701.08528", "submitter": "Martin J\\\"anicke", "authors": "David Bannach, Martin J\\\"anicke, Vitor F. Rey, Sven Tomforde, Bernhard\n  Sick, Paul Lukowicz", "title": "Self-Adaptation of Activity Recognition Systems to New Sensors", "comments": "26 pages, very descriptive figures, comprehensive evaluation on\n  real-life datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 10:01:38 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Bannach", "David", ""], ["J\u00e4nicke", "Martin", ""], ["Rey", "Vitor F.", ""], ["Tomforde", "Sven", ""], ["Sick", "Bernhard", ""], ["Lukowicz", "Paul", ""]]}, {"id": "1701.08608", "submitter": "Inkyu Sa", "authors": "Inkyu Sa, Chris Lehnert, Andrew English, Chris McCool, Feras Dayoub,\n  Ben Upcroft, Tristan Perez", "title": "Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting -\n  Combined Colour and 3D Information", "comments": "8 pages, 14 figures, Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2017.2651952", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a 3D visual detection method for the challenging task of\ndetecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting\nthe peduncle cleanly is one of the most difficult stages of the harvesting\nprocess, where the peduncle is the part of the crop that attaches it to the\nmain stem of the plant. Accurate peduncle detection in 3D space is therefore a\nvital step in reliable autonomous harvesting of sweet peppers, as this can lead\nto precise cutting while avoiding damage to the surrounding plant. This paper\nmakes use of both colour and geometry information acquired from an RGB-D sensor\nand utilises a supervised-learning approach for the peduncle detection task.\nThe performance of the proposed method is demonstrated and evaluated using\nqualitative and quantitative results (the Area-Under-the-Curve (AUC) of the\ndetection precision-recall curve). We are able to achieve an AUC of 0.71 for\npeduncle detection on field-grown sweet peppers. We release a set of manually\nannotated 3D sweet pepper and peduncle images to assist the research community\nin performing further research on this topic.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 14:17:59 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Sa", "Inkyu", ""], ["Lehnert", "Chris", ""], ["English", "Andrew", ""], ["McCool", "Chris", ""], ["Dayoub", "Feras", ""], ["Upcroft", "Ben", ""], ["Perez", "Tristan", ""]]}, {"id": "1701.08706", "submitter": "Saiful Islam Md", "authors": "Md. Fahad Hasan, Tasmin Afroz, Sabir Ismail and Md. Saiful Islam", "title": "Document Decomposition of Bangla Printed Text", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:54:52 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Hasan", "Md. Fahad", ""], ["Afroz", "Tasmin", ""], ["Ismail", "Sabir", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1701.08816", "submitter": "Alexey Novikov", "authors": "Alexey A. Novikov, Dimitrios Lenis, David Major, Jiri Hlad\\r{u}vka,\n  Maria Wimmer, Katja B\\\"uhler", "title": "Fully Convolutional Architectures for Multi-Class Segmentation in Chest\n  Radiographs", "comments": "Final pre-print version accepted for publication in TMI Added new\n  content: * additional evaluations * additional figures * improving the old\n  content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep convolutional neural networks on image classification and\nrecognition tasks has led to new applications in very diversified contexts,\nincluding the field of medical imaging. In this paper we investigate and\npropose neural network architectures for automated multi-class segmentation of\nanatomical organs in chest radiographs, namely for lungs, clavicles and heart.\nWe address several open challenges including model overfitting, reducing number\nof parameters and handling of severely imbalanced data in CXR by fusing recent\nconcepts in convolutional networks and adapting them to the segmentation\nproblem task in CXR. We demonstrate that our architecture combining delayed\nsubsampling, exponential linear units, highly restrictive regularization and a\nlarge number of high resolution low level abstract features outperforms\nstate-of-the-art methods on all considered organs, as well as the human\nobserver on lungs and heart. The models use a multi-class configuration with\nthree target classes and are trained and tested on the publicly available JSRT\ndatabase, consisting of 247 X-ray images the ground-truth masks for which are\navailable in the SCR database. Our best performing model, trained with the loss\nfunction based on the Dice coefficient, reached mean Jaccard overlap scores of\n95.0\\% for lungs, 86.8\\% for clavicles and 88.2\\% for heart. This architecture\noutperformed the human observer results for lungs and heart.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 20:21:57 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 14:42:02 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 13:02:51 GMT"}, {"version": "v4", "created": "Tue, 13 Feb 2018 16:12:40 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Novikov", "Alexey A.", ""], ["Lenis", "Dimitrios", ""], ["Major", "David", ""], ["Hlad\u016fvka", "Jiri", ""], ["Wimmer", "Maria", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "1701.08835", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey and A G Ramakrishnan", "title": "Language Independent Single Document Image Super-Resolution using CNN\n  for improved recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of document images have important applications in restoring old\nand classical texts. The problem involves quality improvement before passing it\nto a properly trained OCR to get accurate recognition of the text. The image\nenhancement and quality improvement constitute important steps as subsequent\nrecognition depends upon the quality of the input image. There are scenarios\nwhen high resolution images are not available and our experiments show that the\nOCR accuracy reduces significantly with decrease in the spatial resolution of\ndocument images. Thus the only option is to improve the resolution of such\ndocument images. The goal is to construct a high resolution image, given a\nsingle low resolution binary image, which constitutes the problem of single\nimage super-resolution. Most of the previous work in super-resolution deal with\nnatural images which have more information-content than the document images.\nHere, we use Convolution Neural Network to learn the mapping between low and\nthe corresponding high resolution images. We experiment with different number\nof layers, parameter settings and non-linear functions to build a fast\nend-to-end framework for document image super-resolution. Our proposed model\nshows a very good PSNR improvement of about 4 dB on 75 dpi Tamil images,\nresulting in a 3 % improvement of word level accuracy by the OCR. It takes less\ntime than the recent sparse based natural image super-resolution technique,\nmaking it useful for real-time document recognition applications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 21:37:00 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1701.08837", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Vishnu Boddeti, Marios Savvides", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theories have emerged which investigate how in- variance is generated in\nhierarchical networks through sim- ple schemes such as max and mean pooling.\nThe restriction to max/mean pooling in theoretical and empirical studies has\ndiverted attention away from a more general way of generating invariance to\nnuisance transformations. We con- jecture that hierarchically building\nselective invariance (i.e. carefully choosing the range of the transformation\nto be in- variant to at each layer of a hierarchical network) is im- portant\nfor pattern recognition. We utilize a novel pooling layer called adaptive\npooling to find linear pooling weights within networks. These networks with the\nlearnt pooling weights have performances on object categorization tasks that\nare comparable to max/mean pooling networks. In- terestingly, adaptive pooling\ncan converge to mean pooling (when initialized with random pooling weights),\nfind more general linear pooling schemes or even decide not to pool at all. We\nillustrate the general notion of selective invari- ance through object\ncategorization experiments on large- scale datasets such as SVHN and ILSVRC\n2012.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 21:44:27 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pal", "Dipan K.", ""], ["Boddeti", "Vishnu", ""], ["Savvides", "Marios", ""]]}, {"id": "1701.08869", "submitter": "Xiaqing Pan", "authors": "Xiaqing Pan, Yueru Chen, C.-C. Jay Kuo", "title": "3D Shape Retrieval via Irrelevance Filtering and Similarity Ranking\n  (IF/SR)", "comments": "arXiv admin note: text overlap with arXiv:1603.01942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel solution for the content-based 3D shape retrieval problem using an\nunsupervised clustering approach, which does not need any label information of\n3D shapes, is presented in this work. The proposed shape retrieval system\nconsists of two modules in cascade: the irrelevance filtering (IF) module and\nthe similarity ranking (SR) module. The IF module attempts to cluster gallery\nshapes that are similar to each other by examining global and local features\nsimultaneously. However, shapes that are close in the local feature space can\nbe distant in the global feature space, and vice versa. To resolve this issue,\nwe propose a joint cost function that strikes a balance between two distances.\nIrrelevant samples that are close in the local feature space but distant in the\nglobal feature space can be removed in this stage. The remaining gallery\nsamples are ranked in the SR module using the local feature. The superior\nperformance of the proposed IF/SR method is demonstrated by extensive\nexperiments conducted on the popular SHREC12 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 23:04:57 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Pan", "Xiaqing", ""], ["Chen", "Yueru", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1701.08886", "submitter": "Moustafa Alzantot", "authors": "Moustafa Alzantot, Supriyo Chakraborty, Mani B. Srivastava", "title": "SenseGen: A Deep Learning Architecture for Synthetic Sensor Data\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to synthesize sensory data that preserves specific statistical\nproperties of the real data has had tremendous implications on data privacy and\nbig data analytics. The synthetic data can be used as a substitute for\nselective real data segments,that are sensitive to the user, thus protecting\nprivacy and resulting in improved analytics.However, increasingly adversarial\nroles taken by data recipients such as mobile apps, or other cloud-based\nanalytics services, mandate that the synthetic data, in addition to preserving\nstatistical properties, should also be difficult to distinguish from the real\ndata. Typically, visual inspection has been used as a test to distinguish\nbetween datasets. But more recently, sophisticated classifier models\n(discriminators), corresponding to a set of events, have also been employed to\ndistinguish between synthesized and real data. The model operates on both\ndatasets and the respective event outputs are compared for consistency. In this\npaper, we take a step towards generating sensory data that can pass a deep\nlearning based discriminator model test, and make two specific contributions:\nfirst, we present a deep learning based architecture for synthesizing sensory\ndata. This architecture comprises of a generator model, which is a stack of\nmultiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network.\nsecond, we use another LSTM network based discriminator model for\ndistinguishing between the true and the synthesized data. Using a dataset of\naccelerometer traces, collected using smartphones of users doing their daily\nactivities, we show that the deep learning based discriminator model can only\ndistinguish between the real and synthesized traces with an accuracy in the\nneighborhood of 50%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 01:59:58 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Alzantot", "Moustafa", ""], ["Chakraborty", "Supriyo", ""], ["Srivastava", "Mani B.", ""]]}, {"id": "1701.08893", "submitter": "Connelly Barnes", "authors": "Eric Risser, Pierre Wilmot, Connelly Barnes", "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer\n  Using Histogram Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, methods have been proposed that perform texture synthesis and style\ntransfer by using convolutional neural networks (e.g. Gatys et al.\n[2015,2016]). These methods are exciting because they can in some cases create\nresults with state-of-the-art quality. However, in this paper, we show these\nmethods also have limitations in texture quality, stability, requisite\nparameter tuning, and lack of user controls. This paper presents a multiscale\nsynthesis pipeline based on convolutional neural networks that ameliorates\nthese issues. We first give a mathematical explanation of the source of\ninstabilities in many previous approaches. We then improve these instabilities\nby using histogram losses to synthesize textures that better statistically\nmatch the exemplar. We also show how to integrate localized style losses in our\nmultiscale framework. These losses can improve the quality of large features,\nimprove the separation of content and style, and offer artistic controls such\nas paint by numbers. We demonstrate that our approach offers improved quality,\nconvergence in fewer iterations, and more stability over the optimization.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:37:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:30:20 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Risser", "Eric", ""], ["Wilmot", "Pierre", ""], ["Barnes", "Connelly", ""]]}, {"id": "1701.08918", "submitter": "Padmavathi K", "authors": "Padmavathi K, Mahima Bhat and Maya V Karki", "title": "Feature Selection based on PCA and PSO for Multimodal Medical Image\n  Fusion using DTCWT", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal medical image fusion helps to increase efficiency in medical\ndiagnosis. This paper presents multimodal medical image fusion by selecting\nrelevant features using Principle Component Analysis (PCA) and Particle Swarm\nOptimization techniques (PSO). DTCWT is used for decomposition of the images\ninto low and high frequency coefficients. Fusion rules such as combination of\nminimum, maximum and simple averaging are applied to approximate and detailed\ncoefficients. The fused image is reconstructed by inverse DTCWT. Performance\nmetrics are evaluated and it shows that DTCWT-PCA performs better than\nDTCWT-PSO in terms of Structural Similarity Index Measure (SSIM) and Cross\nCorrelation (CC). Computation time and feature vector size is reduced in\nDTCWT-PCA compared to DTCWT-PSO for feature selection which proves robustness\nand storage capacity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:12:34 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["K", "Padmavathi", ""], ["Bhat", "Mahima", ""], ["Karki", "Maya V", ""]]}, {"id": "1701.08931", "submitter": "Hadar Averbuch-Elor", "authors": "Hadar Averbuch-Elor, Johannes Kopf, Tamir Hazan and Daniel Cohen-Or", "title": "Co-segmentation for Space-Time Co-located Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a co-segmentation technique for space-time co-located image\ncollections. These prevalent collections capture various dynamic events,\nusually by multiple photographers, and may contain multiple co-occurring\nobjects which are not necessarily part of the intended foreground object,\nresulting in ambiguities for traditional co-segmentation techniques. Thus, to\ndisambiguate what the common foreground object is, we introduce a\nweakly-supervised technique, where we assume only a small seed, given in the\nform of a single segmented image. We take a distributed approach, where local\nbelief models are propagated and reinforced with similar images. Our technique\nprogressively expands the foreground and background belief models across the\nentire collection. The technique exploits the power of the entire set of image\nwithout building a global model, and thus successfully overcomes large\nvariability in appearance of the common foreground object. We demonstrate that\nour method outperforms previous co-segmentation techniques on challenging\nspace-time co-located collections, including dense benchmark datasets which\nwere adapted for our novel problem setting.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 07:05:58 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Averbuch-Elor", "Hadar", ""], ["Kopf", "Johannes", ""], ["Hazan", "Tamir", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1701.08936", "submitter": "Da Zhang", "authors": "Da Zhang, Hamid Maei, Xin Wang, Yuan-Fang Wang", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a fully end-to-end approach for visual tracking in\nvideos that learns to predict the bounding box locations of a target object at\nevery frame. An important insight is that the tracking problem can be\nconsidered as a sequential decision-making process and historical semantics\nencode highly relevant information for future decisions. Based on this\nintuition, we formulate our model as a recurrent convolutional neural network\nagent that interacts with a video overtime, and our model can be trained with\nreinforcement learning (RL) algorithms to learn good tracking policies that pay\nattention to continuous, inter-frame correlation and maximize tracking\nperformance in the long run. The proposed tracking algorithm achieves\nstate-of-the-art performance in an existing tracking benchmark and operates at\nframe-rates faster than real-time. To the best of our knowledge, our tracker is\nthe first neural-network tracker that combines convolutional and recurrent\nnetworks with RL algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 07:48:56 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 20:34:43 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zhang", "Da", ""], ["Maei", "Hamid", ""], ["Wang", "Xin", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1701.08968", "submitter": "Nhan Truong", "authors": "Nhan Truong, Levin Kuhlmann, Mohammad Reza Bonyadi, Jiawei Yang,\n  Andrew Faulks, Omid Kavehei", "title": "Supervised Learning in Automatic Channel Selection for Epileptic Seizure\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting seizure using brain neuroactivations recorded by intracranial\nelectroencephalogram (iEEG) has been widely used for monitoring, diagnosing,\nand closed-loop therapy of epileptic patients, however, computational\nefficiency gains are needed if state-of-the-art methods are to be implemented\nin implanted devices. We present a novel method for automatic seizure detection\nbased on iEEG data that outperforms current state-of-the-art seizure detection\nmethods in terms of computational efficiency while maintaining the accuracy.\nThe proposed algorithm incorporates an automatic channel selection (ACS) engine\nas a pre-processing stage to the seizure detection procedure. The ACS engine\nconsists of supervised classifiers which aim to find iEEGchannelswhich\ncontribute the most to a seizure. Seizure detection stage involves feature\nextraction and classification. Feature extraction is performed in both\nfrequency and time domains where spectral power and correlation between channel\npairs are calculated. Random Forest is used in classification of interictal,\nictal and early ictal periods of iEEG signals. Seizure detection in this paper\nis retrospective and patient-specific. iEEG data is accessed via Kaggle,\nprovided by International Epilepsy Electro-physiology Portal. The dataset\nincludes a training set of 6.5 hours of interictal data and 41 minin ictal data\nand a test set of 9.14 hours. Compared to the state-of-the-art on the same\ndataset, we achieve 49.4% increase in computational efficiency and 400 mins\nbetter in average for detection delay. The proposed model is able to detect a\nseizure onset at 91.95% sensitivity and 94.05% specificity with a mean\ndetection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is\ncomparable to the current state-of-the-art with AUC of 96.29%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:01:45 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Truong", "Nhan", ""], ["Kuhlmann", "Levin", ""], ["Bonyadi", "Mohammad Reza", ""], ["Yang", "Jiawei", ""], ["Faulks", "Andrew", ""], ["Kavehei", "Omid", ""]]}, {"id": "1701.08974", "submitter": "Pedro Costa", "authors": "Pedro Costa, Adrian Galdran, Maria In\\^es Meyer, Michael David\n  Abr\\`amoff, Meindert Niemeijer, Ana Maria Mendon\\c{c}a, Aur\\'elio Campilho", "title": "Towards Adversarial Retinal Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:17:13 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Costa", "Pedro", ""], ["Galdran", "Adrian", ""], ["Meyer", "Maria In\u00eas", ""], ["Abr\u00e0moff", "Michael David", ""], ["Niemeijer", "Meindert", ""], ["Mendon\u00e7a", "Ana Maria", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1701.08985", "submitter": "Alin Popa", "authors": "Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu", "title": "Deep Multitask Architecture for Integrated 2D and 3D Human Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep multitask architecture for \\emph{fully automatic 2d and 3d\nhuman sensing} (DMHS), including \\emph{recognition and reconstruction}, in\n\\emph{monocular images}. The system computes the figure-ground segmentation,\nsemantically identifies the human body parts at pixel level, and estimates the\n2d and 3d pose of the person. The model supports the joint training of all\ncomponents by means of multi-task losses where early processing stages\nrecursively feed into advanced ones for increasingly complex calculations,\naccuracy and robustness. The design allows us to tie a complete training\nprotocol, by taking advantage of multiple datasets that would otherwise\nrestrictively cover only some of the model components: complex 2d image data\nwith no body part labeling and without associated 3d ground truth, or complex\n3d data with limited 2d background variability. In detailed experiments based\non several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we\nevaluate the sub-structures of the model, the effect of various types of\ntraining data in the multitask loss, and demonstrate that state-of-the-art\nresults can be achieved at all processing levels. We also show that in the wild\nour monocular RGB architecture is perceptually competitive to a state-of-the\nart (commercial) Kinect system based on RGB-D data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:52:48 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Popa", "Alin-Ionut", ""], ["Zanfir", "Mihai", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1701.08991", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin, J\\'er\\^ome Thevenot, Esa Rahtu, Simo Saarakkala", "title": "A novel method for automatic localization of joint area on knee plain\n  radiographs", "comments": "Accepted to Scandinavian Conference on Image Analysis (SCIA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoarthritis (OA) is a common musculoskeletal condition typically diagnosed\nfrom radiographic assessment after clinical examination. However, a visual\nevaluation made by a practitioner suffers from subjectivity and is highly\ndependent on the experience. Computer-aided diagnostics (CAD) could improve the\nobjectivity of knee radiographic examination. The first essential step of knee\nOA CAD is to automatically localize the joint area. However, according to the\nliterature this task itself remains challenging. The aim of this study was to\ndevelop novel and computationally efficient method to tackle the issue. Here,\nthree different datasets of knee radiographs were used (n = 473/93/77) to\nvalidate the overall performance of the method. Our pipeline consists of two\nparts: anatomically-based joint area proposal and their evaluation using\nHistogram of Oriented Gradients and the pre-trained Support Vector Machine\nclassifier scores. The obtained results for the used datasets show the mean\nintersection over the union equal to: 0.84, 0.79 and 0.78. Using a high-end\ncomputer, the method allows to automatically annotate conventional knee\nradiographs within 14-16ms and high resolution ones within 170ms. Our results\ndemonstrate that the developed method is suitable for large-scale analyses.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 11:06:12 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 09:23:16 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 20:05:02 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Thevenot", "J\u00e9r\u00f4me", ""], ["Rahtu", "Esa", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1701.09037", "submitter": "Hamid Shahdoosti", "authors": "Seyede Mahya Hazavei and Hamid Reza Shahdoosti", "title": "A New Method for Removing the Moire' Pattern from Images", "comments": "6 pages, 12 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, denoising methods have attracted much attention of\nresearchers. The conventional method for removing the Moire' pattern from\nimages is using notch filters in the Frequency-domain. In this paper a new\nmethod is proposed that can achieve a better performance in comparison with the\ntraditional method. Median filter is used in some part of spectrum of noisy\nimages to reduce the noise. At the second part of this paper, to demonstrate\nthe robustness of the proposed method, it is implemented for some noisy images\nthat have moire' pattern. Experiments on noisy images with different\ncharacteristics show that the proposed method increases the PSNR values\ncompared with previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 13:52:31 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Hazavei", "Seyede Mahya", ""], ["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1701.09135", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt and James Hays", "title": "DeepNav: Learning to Navigate Large Cities", "comments": "CVPR 2017 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for\nnavigating large cities using locally visible street-view images. The DeepNav\nagent learns to reach its destination quickly by making the correct navigation\ndecisions at intersections. We collect a large-scale dataset of street-view\nimages organized in a graph where nodes are connected by roads. This dataset\ncontains 10 city graphs and more than 1 million street-view images. We propose\n3 supervised learning approaches for the navigation task and show how A* search\nin the city graph can be used to generate supervision for the learning. Our\nannotation process is fully automated using publicly available mapping services\nand requires no human input. We evaluate the proposed DeepNav models on 4\nheld-out cities for navigating to 5 different types of destinations. Our\nalgorithms outperform previous work that uses hand-crafted features and Support\nVector Regression (SVR)[19].\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 17:14:24 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 22:40:26 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Hays", "James", ""]]}]