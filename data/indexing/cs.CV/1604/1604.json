[{"id": "1604.00036", "submitter": "Jos\\'e Oramas", "authors": "Jose Oramas, Tinne Tuytelaars", "title": "Modeling Visual Compatibility through Hierarchical Mid-level Elements", "comments": "29 pages, 19 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a hierarchical method to discover mid-level elements\nwith the objective of modeling visual compatibility between objects. At the\nbase-level, our method identifies patterns of CNN activations with the aim of\nmodeling different variations/styles in which objects of the classes of\ninterest may occur. At the top-level, the proposed method discovers patterns of\nco-occurring activations of base-level elements that define visual\ncompatibility between pairs of object classes. Experiments on the massive\nAmazon dataset show the strength of our method at describing object classes and\nthe characteristics that drive the compatibility between them.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 20:18:16 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1604.00066", "submitter": "Wenbin Li", "authors": "Wenbin Li, Seyedmajid Azimi, Ale\\v{s} Leonardis, Mario Fritz", "title": "To Fall Or Not To Fall: A Visual Approach to Physical Stability\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding physical phenomena is a key competence that enables humans and\nanimals to act and interact under uncertain perception in previously unseen\nenvironments containing novel object and their configurations. Developmental\npsychology has shown that such skills are acquired by infants from observations\nat a very early stage.\n  In this paper, we contrast a more traditional approach of taking a\nmodel-based route with explicit 3D representations and physical simulation by\nan end-to-end approach that directly predicts stability and related quantities\nfrom appearance. We ask the question if and to what extent and quality such a\nskill can directly be acquired in a data-driven way bypassing the need for an\nexplicit simulation.\n  We present a learning-based approach based on simulated data that predicts\nstability of towers comprised of wooden blocks under different conditions and\nquantities related to the potential fall of the towers. The evaluation is\ncarried out on synthetic data and compared to human judgments on the same\nstimuli.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 21:53:32 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Li", "Wenbin", ""], ["Azimi", "Seyedmajid", ""], ["Leonardis", "Ale\u0161", ""], ["Fritz", "Mario", ""]]}, {"id": "1604.00092", "submitter": "Paul Vernaza", "authors": "Paul Vernaza", "title": "Variational reaction-diffusion systems for semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel global energy model for multi-class semantic image segmentation is\nproposed that admits very efficient exact inference and derivative calculations\nfor learning. Inference in this model is equivalent to MAP inference in a\nparticular kind of vector-valued Gaussian Markov random field, and ultimately\nreduces to solving a linear system of linear PDEs known as a reaction-diffusion\nsystem. Solving this system can be achieved in time scaling near-linearly in\nthe number of image pixels by reducing it to sequential FFTs, after a linear\nchange of basis. The efficiency and differentiability of the model make it\nespecially well-suited for integration with convolutional neural networks, even\nallowing it to be used in interior, feature-generating layers and stacked\nmultiple times. Experimental results are shown demonstrating that the model can\nbe employed profitably in conjunction with different convolutional net\narchitectures, and that doing so compares favorably to joint training of a\nfully-connected CRF with a convolutional net.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 01:04:31 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Vernaza", "Paul", ""]]}, {"id": "1604.00133", "submitter": "Liang Zheng", "authors": "Liang Zheng, Yali Zhao, Shengjin Wang, Jingdong Wang, Qi Tian", "title": "Good Practice in CNN Feature Transfer", "comments": "9 pages. It will be submitted to an appropriate journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is the effective transfer of the Convolutional\nNeural Network (CNN) feature in image search and classification.\nSystematically, we study three facts in CNN transfer. 1) We demonstrate the\nadvantage of using images with a properly large size as input to CNN instead of\nthe conventionally resized one. 2) We benchmark the performance of different\nCNN layers improved by average/max pooling on the feature maps. Our observation\nsuggests that the Conv5 feature yields very competitive accuracy under such\npooling step. 3) We find that the simple combination of pooled features\nextracted across various CNN layers is effective in collecting evidences from\nboth low and high level descriptors. Following these good practices, we are\ncapable of improving the state of the art on a number of benchmarks to a large\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 05:31:57 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Zheng", "Liang", ""], ["Zhao", "Yali", ""], ["Wang", "Shengjin", ""], ["Wang", "Jingdong", ""], ["Tian", "Qi", ""]]}, {"id": "1604.00136", "submitter": "Pia Bideau", "authors": "Pia Bideau, Erik Learned-Miller", "title": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human ability to detect and segment moving objects works in the presence\nof multiple objects, complex background geometry, motion of the observer, and\neven camouflage. In addition to all of this, the ability to detect motion is\nnearly instantaneous. While there has been much recent progress in motion\nsegmentation, it still appears we are far from human capabilities. In this\nwork, we derive from first principles a new likelihood function for assessing\nthe probability of an optical flow vector given the 3D motion direction of an\nobject. This likelihood uses a novel combination of the angle and magnitude of\nthe optical flow to maximize the information about the true motions of objects.\nUsing this new likelihood and several innovations in initialization, we develop\na motion segmentation algorithm that beats current state-of-the-art methods by\na large margin. We compare to five state-of-the-art methods on two established\nbenchmarks, and a third new data set of camouflaged animals, which we introduce\nto push motion segmentation to the next level.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 05:37:26 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Bideau", "Pia", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1604.00147", "submitter": "Lijuan Zhou", "authors": "Lijuan Zhou, Wanqing Li, and Philip Ogunbona", "title": "Learning a Pose Lexicon for Semantic Action Recognition", "comments": "Accepted by the 2016 IEEE International Conference on Multimedia and\n  Expo (ICME 2016). 6 pages paper and 4 pages supplementary material", "journal-ref": null, "doi": "10.1109/ICME.2016.7552882", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for learning a pose lexicon comprising\nsemantic poses defined by textual instructions and their associated visual\nposes defined by visual features. The proposed method simultaneously takes two\ninput streams, semantic poses and visual pose candidates, and statistically\nlearns a mapping between them to construct the lexicon. With the learned\nlexicon, action recognition can be cast as the problem of finding the maximum\ntranslation probability of a sequence of semantic poses given a stream of\nvisual pose candidates. Experiments evaluating pre-trained and zero-shot action\nrecognition conducted on MSRC-12 gesture and WorkoutSu-10 exercise datasets\nwere used to verify the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 06:24:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhou", "Lijuan", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1604.00187", "submitter": "Sebastian Sudholt", "authors": "Sebastian Sudholt, Gernot A. Fink", "title": "PHOCNet: A Deep Convolutional Neural Network for Word Spotting in\n  Handwritten Documents", "comments": "published as conference paper at the International Conference on\n  Frontiers in Handwriting Recognition 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep convolutional neural networks have achieved state of\nthe art performance in various computer vision task such as classification,\ndetection or segmentation. Due to their outstanding performance, CNNs are more\nand more used in the field of document image analysis as well. In this work, we\npresent a CNN architecture that is trained with the recently proposed PHOC\nrepresentation. We show empirically that our CNN architecture is able to\noutperform state of the art results for various word spotting benchmarks while\nexhibiting short training and test times.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 10:11:38 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 12:11:03 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 06:58:02 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Sudholt", "Sebastian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1604.00239", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Anoop Cherian and Fatih Porikli", "title": "Tensor Representations via Kernel Linearization for Action Recognition\n  from 3D Skeletons (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore tensor representations that can compactly capture\nhigher-order relationships between skeleton joints for 3D action recognition.\nWe first define RBF kernels on 3D joint sequences, which are then linearized to\nform kernel descriptors. The higher-order outer-products of these kernel\ndescriptors form our tensor representations. We present two different kernels\nfor action recognition, namely (i) a sequence compatibility kernel that\ncaptures the spatio-temporal compatibility of joints in one sequence against\nthose in the other, and (ii) a dynamics compatibility kernel that explicitly\nmodels the action dynamics of a sequence. Tensors formed from these kernels are\nthen used to train an SVM. We present experiments on several benchmark datasets\nand demonstrate state of the art results, substantiating the effectiveness of\nour representations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 13:41:49 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 08:35:38 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Koniusz", "Piotr", ""], ["Cherian", "Anoop", ""], ["Porikli", "Fatih", ""]]}, {"id": "1604.00289", "submitter": "Brenden Lake", "authors": "Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J.\n  Gershman", "title": "Building Machines That Learn and Think Like People", "comments": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016).\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 15:37:57 GMT"}, {"version": "v2", "created": "Sat, 7 May 2016 18:03:53 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 17:26:50 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lake", "Brenden M.", ""], ["Ullman", "Tomer D.", ""], ["Tenenbaum", "Joshua B.", ""], ["Gershman", "Samuel J.", ""]]}, {"id": "1604.00312", "submitter": "S L Happy", "authors": "S L Happy, A. Dasgupta, P. Patnaik, A. Routray", "title": "Automated Alertness and Emotion Detection for Empathic Feedback During\n  E-Learning", "comments": "IEEE International Conference on Technology for Education, Kharagpur,\n  India, 2013", "journal-ref": "IEEE International Conference on Technology for Education, 2013", "doi": "10.1109/T4E.2013.19", "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of education technology, empathic interaction with the user\nand feedback by the learning system using multiple inputs such as video, voice\nand text inputs is an important area of research. In this paper, a\nnonintrusive, standalone model for intelligent assessment of alertness and\nemotional state as well as generation of appropriate feedback has been\nproposed. Using the non-intrusive visual cues, the system classifies emotion\nand alertness state of the user, and provides appropriate feedback according to\nthe detected cognitive state using facial expressions, ocular parameters,\npostures, and gestures. Assessment of alertness level using ocular parameters\nsuch as PERCLOS and saccadic parameters, emotional state from facial expression\nanalysis, and detection of both relevant cognitive and emotional states from\nupper body gestures and postures has been proposed. Integration of such a\nsystem in e-learning environment is expected to enhance students performance\nthrough interaction, feedback, and positive mood induction.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:13:05 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Happy", "S L", ""], ["Dasgupta", "A.", ""], ["Patnaik", "P.", ""], ["Routray", "A.", ""]]}, {"id": "1604.00326", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah and Rainer Stiefelhagen", "title": "How to Transfer? Zero-Shot Object Recognition via Hierarchical Transfer\n  of Semantic Attributes", "comments": "Published as a conference paper at WACV 2015, modifications include\n  new results with GoogLeNet features", "journal-ref": null, "doi": "10.1109/WACV.2015.116", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute based knowledge transfer has proven very successful in visual\nobject analysis and learning previously unseen classes. However, the common\napproach learns and transfers attributes without taking into consideration the\nembedded structure between the categories in the source set. Such information\nprovides important cues on the intra-attribute variations. We propose to\ncapture these variations in a hierarchical model that expands the knowledge\nsource with additional abstraction levels of attributes. We also provide a\nnovel transfer approach that can choose the appropriate attributes to be shared\nwith an unseen class. We evaluate our approach on three public datasets:\naPascal, Animals with Attributes and CUB-200-2011 Birds. The experiments\ndemonstrate the effectiveness of our model with significant improvement over\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:51:56 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1604.00367", "submitter": "Mengran Gou", "authors": "Mengran Gou, Xikang Zhang, Angels Rates-Borras, Sadjad\n  Asghari-Esfeden, Mario Sznaier, Octavia Camps", "title": "Person Re-identification in Appearance Impaired Scenarios", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is critical in surveillance applications. Current\napproaches rely on appearance based features extracted from a single or\nmultiple shots of the target and candidate matches. These approaches are at a\ndisadvantage when trying to distinguish between candidates dressed in similar\ncolors or when targets change their clothing. In this paper we propose a\ndynamics-based feature to overcome this limitation. The main idea is to capture\nsoft biometrics from gait and motion patterns by gathering dense short\ntrajectories (tracklets) which are Fisher vector encoded. To illustrate the\nmerits of the proposed features we introduce three new \"appearance-impaired\"\ndatasets. Our experiments on the original and the appearance impaired datasets\ndemonstrate the benefits of incorporating dynamics-based information with\nappearance-based information to re-identification algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 19:20:03 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Gou", "Mengran", ""], ["Zhang", "Xikang", ""], ["Rates-Borras", "Angels", ""], ["Asghari-Esfeden", "Sadjad", ""], ["Sznaier", "Mario", ""], ["Camps", "Octavia", ""]]}, {"id": "1604.00385", "submitter": "Stephen Plaza", "authors": "Stephen M. Plaza and Stuart E. Berg", "title": "Large-Scale Electron Microscopy Image Segmentation in Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging field of connectomics aims to unlock the mysteries of the brain\nby understanding the connectivity between neurons. To map this connectivity, we\nacquire thousands of electron microscopy (EM) images with nanometer-scale\nresolution. After aligning these images, the resulting dataset has the\npotential to reveal the shapes of neurons and the synaptic connections between\nthem. However, imaging the brain of even a tiny organism like the fruit fly\nyields terabytes of data. It can take years of manual effort to examine such\nimage volumes and trace their neuronal connections. One solution is to apply\nimage segmentation algorithms to help automate the tracing tasks. In this\npaper, we propose a novel strategy to apply such segmentation on very large\ndatasets that exceed the capacity of a single machine. Our solution is robust\nto potential segmentation errors which could otherwise severely compromise the\nquality of the overall segmentation, for example those due to poor classifier\ngeneralizability or anomalies in the image dataset. We implement our algorithms\nin a Spark application which minimizes disk I/O, and apply them to a few large\nEM datasets, revealing both their effectiveness and scalability. We hope this\nwork will encourage external contributions to EM segmentation by providing 1) a\nflexible plugin architecture that deploys easily on different cluster\nenvironments and 2) an in-memory representation of segmentation that could be\nconducive to new advances.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 19:53:30 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Plaza", "Stephen M.", ""], ["Berg", "Stuart E.", ""]]}, {"id": "1604.00409", "submitter": "Jonathan Ventura", "authors": "Jonathan Ventura", "title": "Structure from Motion on a Sphere", "comments": "in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a special case of structure from motion where the camera rotates\non a sphere. The camera's optical axis lies perpendicular to the sphere's\nsurface. In this case, the camera's pose is minimally represented by three\nrotation parameters. From analysis of the epipolar geometry we derive a novel\nand efficient solution for the essential matrix relating two images, requiring\nonly three point correspondences in the minimal case. We apply this solver in a\nstructure-from-motion pipeline that aggregates pairwise relations by rotation\naveraging followed by bundle adjustment with an inverse depth parameterization.\nOur methods enable scene modeling with an outward-facing camera and object\nscanning with an inward-facing camera.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 21:01:53 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 15:11:43 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Ventura", "Jonathan", ""]]}, {"id": "1604.00427", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Kristen Grauman", "title": "Leaving Some Stones Unturned: Dynamic Feature Prioritization for\n  Activity Detection in Streaming Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches for activity recognition often ignore constraints on\ncomputational resources: 1) they rely on extensive feature computation to\nobtain rich descriptors on all frames, and 2) they assume batch-mode access to\nthe entire test video at once. We propose a new active approach to activity\nrecognition that prioritizes \"what to compute when\" in order to make timely\npredictions. The main idea is to learn a policy that dynamically schedules the\nsequence of features to compute on selected frames of a given test video. In\ncontrast to traditional static feature selection, our approach continually\nre-prioritizes computation based on the accumulated history of observations and\naccounts for the transience of those observations in ongoing video. We develop\nvariants to handle both the batch and streaming settings. On two challenging\ndatasets, our method provides significantly better accuracy than alternative\ntechniques for a wide range of computational budgets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 22:37:28 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1604.00433", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Subhransu Maji", "title": "Adapting Models to Signal Degradation using Distillation", "comments": "BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression and knowledge distillation have been successfully applied\nfor cross-architecture and cross-domain transfer learning. However, a key\nrequirement is that training examples are in correspondence across the domains.\nWe show that in many scenarios of practical importance such aligned data can be\nsynthetically generated using computer graphics pipelines allowing domain\nadaptation through distillation. We apply this technique to learn models for\nrecognizing low-resolution images using labeled high-resolution images,\nnon-localized objects using labeled localized objects, line-drawings using\nlabeled color images, etc. Experiments on various fine-grained recognition\ndatasets demonstrate that the technique improves recognition performance on the\nlow-quality data and beats strong baselines for domain adaptation. Finally, we\npresent insights into workings of the technique through visualizations and\nrelating it to existing literature.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 23:24:17 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 17:14:25 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""]]}, {"id": "1604.00449", "submitter": "Christopher B. Choy", "authors": "Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio\n  Savarese", "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object\n  Reconstruction", "comments": "Appendix can be found at\n  http://cvgl.stanford.edu/papers/choy_16_appendix.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent success of methods that employ shape priors to achieve\nrobust 3D reconstructions, we propose a novel recurrent neural network\narchitecture that we call the 3D Recurrent Reconstruction Neural Network\n(3D-R2N2). The network learns a mapping from images of objects to their\nunderlying 3D shapes from a large collection of synthetic data. Our network\ntakes in one or more images of an object instance from arbitrary viewpoints and\noutputs a reconstruction of the object in the form of a 3D occupancy grid.\nUnlike most of the previous works, our network does not require any image\nannotations or object class labels for training or testing. Our extensive\nexperimental analysis shows that our reconstruction framework i) outperforms\nthe state-of-the-art methods for single view reconstruction, and ii) enables\nthe 3D reconstruction of objects in situations when traditional SFM/SLAM\nmethods fail (because of lack of texture and/or wide baseline).\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 01:28:27 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Choy", "Christopher B.", ""], ["Xu", "Danfei", ""], ["Gwak", "JunYoung", ""], ["Chen", "Kevin", ""], ["Savarese", "Silvio", ""]]}, {"id": "1604.00466", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed\n  Elgammal", "title": "Automatic Annotation of Structured Facts in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the application of fact-level image understanding, we present an\nautomatic method for data collection of structured visual facts from images\nwith captions. Example structured facts include attributed objects (e.g.,\n<flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man,\nwalking, dog>), and positional information (e.g., <vase, on, table>). The\ncollected annotations are in the form of fact-image pairs (e.g.,<man, walking,\ndog> and an image region containing this fact). With a language approach, the\nproposed method is able to collect hundreds of thousands of visual fact\nannotations with accuracy of 83% according to human judgment. Our method\nautomatically collected more than 380,000 visual fact annotations and more than\n110,000 unique visual facts from images with captions and localized them in\nimages in less than one day of processing time on standard CPU platforms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 06:35:45 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 18:58:10 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 00:04:22 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Cohen", "Scott", ""], ["Chang", "Walter", ""], ["Price", "Brian", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1604.00470", "submitter": "Raghvendra Kannao", "authors": "Raghvendra Kannao and Prithwijit Guha", "title": "Overlay Text Extraction From TV News Broadcast", "comments": "Published in INDICON 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text data present in overlaid bands convey brief descriptions of news\nevents in broadcast videos. The process of text extraction becomes challenging\nas overlay text is presented in widely varying formats and often with animation\neffects. We note that existing edge density based methods are well suited for\nour application on account of their simplicity and speed of operation. However,\nthese methods are sensitive to thresholds and have high false positive rates.\nIn this paper, we present a contrast enhancement based preprocessing stage for\noverlay text detection and a parameter free edge density based scheme for\nefficient text band detection. The second contribution of this paper is a novel\napproach for multiple text region tracking with a formal identification of all\npossible detection failure cases. The tracking stage enables us to establish\nthe temporal presence of text bands and their linking over time. The third\ncontribution is the adoption of Tesseract OCR for the specific task of overlay\ntext recognition using web news articles. The proposed approach is tested and\nfound superior on news videos acquired from three Indian English television\nnews channels along with benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 07:28:23 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Kannao", "Raghvendra", ""], ["Guha", "Prithwijit", ""]]}, {"id": "1604.00475", "submitter": "Bin Liu", "authors": "Yi Dai, Bin Liu", "title": "Robust video object tracking via Bayesian model averaging based feature\n  fusion", "comments": null, "journal-ref": "Opt. Eng. 55(8), 083102 (2016)", "doi": "10.1117/1.OE.55.8.083102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we are concerned with tracking an object of interest in\nvideo stream. We propose an algorithm that is robust against occlusion, the\npresence of confusing colors, abrupt changes in the object feature space and\nchanges in object size. We develop the algorithm within a Bayesian modeling\nframework. The state space model is used for capturing the temporal correlation\nin the sequence of frame images by modeling the underlying dynamics of the\ntracking system. The Bayesian model averaging (BMA) strategy is proposed for\nfusing multi-clue information in the observations. Any number of object\nfeatures are allowed to be involved in the proposed framework. Every feature\nrepresents one source of information to be fused and is associated with an\nobservation model. The state inference is performed by employing the particle\nfilter methods. In comparison with related approaches, the BMA based tracker is\nshown to have robustness, expressivity, and comprehensibility.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 08:44:20 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 02:02:23 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 12:05:57 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Dai", "Yi", ""], ["Liu", "Bin", ""]]}, {"id": "1604.00494", "submitter": "Phi Vu Tran", "authors": "Phi Vu Tran", "title": "A Fully Convolutional Neural Network for Cardiac Segmentation in\n  Short-Axis MRI", "comments": "Initial Technical Report; Include link to models and code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated cardiac segmentation from magnetic resonance imaging datasets is an\nessential step in the timely diagnosis and management of cardiac pathologies.\nWe propose to tackle the problem of automated left and right ventricle\nsegmentation through the application of a deep fully convolutional neural\nnetwork architecture. Our model is efficiently trained end-to-end in a single\nlearning stage from whole-image inputs and ground truths to make inference at\nevery pixel. To our knowledge, this is the first application of a fully\nconvolutional neural network architecture for pixel-wise labeling in cardiac\nmagnetic resonance imaging. Numerical experiments demonstrate that our model is\nrobust to outperform previous fully automated methods across multiple\nevaluation measures on a range of cardiac datasets. Moreover, our model is fast\nand can leverage commodity compute resources such as the graphics processing\nunit to enable state-of-the-art cardiac segmentation at massive scales. The\nmodels and code are available at\nhttps://github.com/vuptran/cardiac-segmentation\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 12:32:55 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 10:11:34 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 03:04:26 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Tran", "Phi Vu", ""]]}, {"id": "1604.00533", "submitter": "James  Peters Ph.D.", "authors": "R. Hettiarachchi and J.F. Peters", "title": "Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color image segmentation is a crucial step in many computer vision and\npattern recognition applications. This article introduces an adaptive and\nunsupervised clustering approach based on Voronoi regions, which can be applied\nto solve the color image segmentation problem. The proposed method performs\nregion splitting and merging within Voronoi regions of the Dirichlet\nTessellated image (also called a Voronoi diagram) , which improves the\nefficiency and the accuracy of the number of clusters and cluster centroids\nestimation process. Furthermore, the proposed method uses cluster centroid\nproximity to merge proximal clusters in order to find the final number of\nclusters and cluster centroids. In contrast to the existing adaptive\nunsupervised cluster-based image segmentation algorithms, the proposed method\nuses K-means clustering algorithm in place of the Fuzzy C-means algorithm to\nfind the final segmented image. The proposed method was evaluated on three\ndifferent unsupervised image segmentation evaluation benchmarks and its results\nwere compared with two other adaptive unsupervised cluster-based image\nsegmentation algorithms. The experimental results reported in this article\nconfirm that the proposed method outperforms the existing algorithms in terms\nof the quality of image segmentation results. Also, the proposed method results\nin the lowest average execution time per image compared to the existing methods\nreported in this article.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 17:27:24 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Hettiarachchi", "R.", ""], ["Peters", "J. F.", ""]]}, {"id": "1604.00546", "submitter": "Mukhtiar Ali Unar", "authors": "Farida Memon, Mukhtiar Ali Unar, Sheeraz Memon", "title": "Image Quality Assessment for Performance Evaluation of Focus Measure\n  Operators", "comments": "8 pages, Mehran University Research Journal of Engineering and\n  Technology, Vol. 34, No. 4, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the performance evaluation of eight focus measure\noperators namely Image CURV (Curvature), GRAE (Gradient Energy), HISE\n(Histogram Entropy), LAPM (Modified Laplacian), LAPV (Variance of Laplacian),\nLAPD (Diagonal Laplacian), LAP3 (Laplacian in 3D Window) and WAVS (Sum of\nWavelet Coefficients). Statistical matrics such as MSE (Mean Squared Error),\nPNSR (Peak Signal to Noise Ratio), SC (Structural Content), NCC (Normalized\nCross Correlation), MD (Maximum Difference) and NAE (Normalized Absolute Error)\nare used to evaluate stated focus measures in this research. . FR (Full\nReference) method of the image quality assessment is utilized in this paper.\nResults indicate that LAPD method is comparatively better than other seven\nfocus operators at typical imaging conditions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 19:28:01 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Memon", "Farida", ""], ["Unar", "Mukhtiar Ali", ""], ["Memon", "Sheeraz", ""]]}, {"id": "1604.00600", "submitter": "Anbang Yao", "authors": "Tao Kong, Anbang Yao, Yurong Chen, Fuchun Sun", "title": "HyperNet: Towards Accurate Region Proposal Generation and Joint Object\n  Detection", "comments": "Accepted as a spotlight oral paper by CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all of the current top-performing object detection networks employ\nregion proposals to guide the search for object instances. State-of-the-art\nregion proposal methods usually need several thousand proposals to get high\nrecall, thus hurting the detection efficiency. Although the latest Region\nProposal Network method gets promising detection accuracy with several hundred\nproposals, it still struggles in small-size object detection and precise\nlocalization (e.g., large IoU thresholds), mainly due to the coarseness of its\nfeature maps. In this paper, we present a deep hierarchical network, namely\nHyperNet, for handling region proposal generation and object detection jointly.\nOur HyperNet is primarily based on an elaborately designed Hyper Feature which\naggregates hierarchical feature maps first and then compresses them into a\nuniform space. The Hyper Features well incorporate deep but highly semantic,\nintermediate but really complementary, and shallow but naturally\nhigh-resolution features of the image, thus enabling us to construct HyperNet\nby sharing them both in generating proposals and detecting objects via an\nend-to-end joint training strategy. For the deep VGG16 model, our method\nachieves completely leading recall and state-of-the-art object detection\naccuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It\nruns with a speed of 5 fps (including all steps) on a GPU, thus having the\npotential for real-time processing.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 06:52:14 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Kong", "Tao", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""], ["Sun", "Fuchun", ""]]}, {"id": "1604.00606", "submitter": "Yuzhuo Ren", "authors": "Yuzhuo Ren, Chen Chen, Shangwen Li, and C.-C. Jay Kuo", "title": "GAL: A Global-Attributes Assisted Labeling System for Outdoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach that extracts global attributes from outdoor images to facilitate\ngeometric layout labeling is investigated in this work. The proposed\nGlobal-attributes Assisted Labeling (GAL) system exploits both local features\nand global attributes. First, by following a classical method, we use local\nfeatures to provide initial labels for all super-pixels. Then, we develop a set\nof techniques to extract global attributes from 2D outdoor images. They include\nsky lines, ground lines, vanishing lines, etc. Finally, we propose the GAL\nsystem that integrates global attributes in the conditional random field (CRF)\nframework to improve initial labels so as to offer a more robust labeling\nresult. The performance of the proposed GAL system is demonstrated and\nbenchmarked with several state-of-the-art algorithms against a popular outdoor\nscene layout dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 07:36:50 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Ren", "Yuzhuo", ""], ["Chen", "Chen", ""], ["Li", "Shangwen", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1604.00676", "submitter": "Hongyang Li", "authors": "Hongyang Li, Wanli Ouyang, Xiaogang Wang", "title": "Multi-Bias Non-linear Activation in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a widely used non-linear activation, Rectified Linear Unit (ReLU)\nseparates noise and signal in a feature map by learning a threshold or bias.\nHowever, we argue that the classification of noise and signal not only depends\non the magnitude of responses, but also the context of how the feature\nresponses would be used to detect more abstract patterns in higher layers. In\norder to output multiple response maps with magnitude in different ranges for a\nparticular visual pattern, existing networks employing ReLU and its variants\nhave to learn a large number of redundant filters. In this paper, we propose a\nmulti-bias non-linear activation (MBA) layer to explore the information hidden\nin the magnitudes of responses. It is placed after the convolution layer to\ndecouple the responses to a convolution kernel into multiple maps by\nmulti-thresholding magnitudes, thus generating more patterns in the feature\nspace at a low computational cost. It provides great flexibility of selecting\nresponses to different visual patterns in different magnitude ranges to form\nrich representations in higher layers. Such a simple and yet effective scheme\nachieves the state-of-the-art performance on several benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 19:31:22 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Li", "Hongyang", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1604.00730", "submitter": "Shaodi You", "authors": "Shaodi You, Robby T. Tan, Rei Kawakami, Yasuhiro Mukaigawa, Katsushi\n  Ikeuchi", "title": "Waterdrop Stereo", "comments": "12 pages, 15figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces depth estimation from water drops. The key idea is that\na single water drop adhered to window glass is totally transparent and convex,\nand thus optically acts like a fisheye lens. If we have more than one water\ndrop in a single image, then through each of them we can see the environment\nwith different view points, similar to stereo. To realize this idea, we need to\nrectify every water drop imagery to make radially distorted planar surfaces\nlook flat. For this rectification, we consider two physical properties of water\ndrops: (1) A static water drop has constant volume, and its geometric convex\nshape is determined by the balance between the tension force and gravity. This\nimplies that the 3D geometric shape can be obtained by minimizing the overall\npotential energy, which is the sum of the tension energy and the gravitational\npotential energy. (2) The imagery inside a water-drop is determined by the\nwater-drop 3D shape and total reflection at the boundary. This total reflection\ngenerates a dark band commonly observed in any adherent water drops. Hence,\nonce the 3D shape of water drops are recovered, we can rectify the water drop\nimages through backward raytracing. Subsequently, we can compute depth using\nstereo. In addition to depth estimation, we can also apply image refocusing.\nExperiments on real images and a quantitative evaluation show the effectiveness\nof our proposed method. To our best knowledge, never before have adherent water\ndrops been used to estimate depth.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 03:16:40 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["You", "Shaodi", ""], ["Tan", "Robby T.", ""], ["Kawakami", "Rei", ""], ["Mukaigawa", "Yasuhiro", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "1604.00790", "submitter": "Cheng Wang", "authors": "Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel", "title": "Image Captioning with Deep Bidirectional LSTMs", "comments": "accepted by ACMMM 2016 as full paper and oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:43:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 07:45:25 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 14:19:37 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wang", "Cheng", ""], ["Yang", "Haojin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1604.00825", "submitter": "Wojciech Samek", "authors": "Alexander Binder and Gr\\'egoire Montavon and Sebastian Bach and\n  Klaus-Robert M\\\"uller and Wojciech Samek", "title": "Layer-wise Relevance Propagation for Neural Networks with Local\n  Renormalization Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise relevance propagation is a framework which allows to decompose the\nprediction of a deep neural network computed over a sample, e.g. an image, down\nto relevance scores for the single input dimensions of the sample such as\nsubpixels of an image. While this approach can be applied directly to\ngeneralized linear mappings, product type non-linearities are not covered. This\npaper proposes an approach to extend layer-wise relevance propagation to neural\nnetworks with local renormalization layers, which is a very common product-type\nnon-linearity in convolutional neural networks. We evaluate the proposed method\nfor local renormalization layers on the CIFAR-10, Imagenet and MIT Places\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 11:52:07 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Binder", "Alexander", ""], ["Montavon", "Gr\u00e9goire", ""], ["Bach", "Sebastian", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1604.00895", "submitter": "Shuda Li", "authors": "Shuda Li, Ankur Handa, Yang Zhang, Andrew Calway", "title": "HDRFusion: HDR SLAM using a low-cost auto-exposure RGB-D sensor", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method for comparing frame appearance in a frame-to-model\n3-D mapping and tracking system using an low dynamic range (LDR) RGB-D camera\nwhich is robust to brightness changes caused by auto exposure. It is based on a\nnormalised radiance measure which is invariant to exposure changes and not only\nrobustifies the tracking under changing lighting conditions, but also enables\nthe following exposure compensation perform accurately to allow online building\nof high dynamic range (HDR) maps. The latter facilitates the frame-to-model\ntracking to minimise drift as well as better capturing light variation within\nthe scene. Results from experiments with synthetic and real data demonstrate\nthat the method provides both improved tracking and maps with far greater\ndynamic range of luminosity.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:05:27 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Li", "Shuda", ""], ["Handa", "Ankur", ""], ["Zhang", "Yang", ""], ["Calway", "Andrew", ""]]}, {"id": "1604.00906", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su and Kristen Grauman", "title": "Detecting Engagement in Egocentric Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wearable camera video, we see what the camera wearer sees. While this\nmakes it easy to know roughly what he chose to look at, it does not immediately\nreveal when he was engaged with the environment. Specifically, at what moments\ndid his focus linger, as he paused to gather more information about something\nhe saw? Knowing this answer would benefit various applications in video\nsummarization and augmented reality, yet prior work focuses solely on the\n\"what\" question (estimating saliency, gaze) without considering the \"when\"\n(engagement). We propose a learning-based approach that uses long-term\negomotion cues to detect engagement, specifically in browsing scenarios where\none frequently takes in new visual information (e.g., shopping, touring). We\nintroduce a large, richly annotated dataset for ego-engagement that is the\nfirst of its kind. Our approach outperforms a wide array of existing methods.\nWe show engagement can be detected well independent of both scene appearance\nand the camera wearer's identity.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:21:16 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1604.00970", "submitter": "Karl Granstr\\\"om", "authors": "Karl Granstrom, Marcus Baum, Stephan Reuter", "title": "Extended Object Tracking: Introduction, Overview and Applications", "comments": "30 pages, 19 figures", "journal-ref": "Journal of Advances in Information Fusion, Volume 12, Number 2,\n  Pages 139-174, December 2016, ISSN 1557-6418", "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides an elaborate overview of current research in extended\nobject tracking. We provide a clear definition of the extended object tracking\nproblem and discuss its delimitation to other types of object tracking. Next,\ndifferent aspects of extended object modelling are extensively discussed.\nSubsequently, we give a tutorial introduction to two basic and well used\nextended object tracking approaches - the random matrix approach and the Kalman\nfilter-based approach for star-convex shapes. The next part treats the tracking\nof multiple extended objects and elaborates how the large number of feasible\nassociation hypotheses can be tackled using both Random Finite Set (RFS) and\nNon-RFS multi-object trackers. The article concludes with a summary of current\napplications, where four example applications involving camera, X-band radar,\nlight detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are\nhighlighted.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 09:58:49 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 08:49:13 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 16:23:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Granstrom", "Karl", ""], ["Baum", "Marcus", ""], ["Reuter", "Stephan", ""]]}, {"id": "1604.00974", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Writer-independent Feature Learning for Offline Signature Verification\n  using Deep Convolutional Neural Networks", "comments": "Accepted as a conference paper to The International Joint Conference\n  on Neural Networks (IJCNN) 2016", "journal-ref": null, "doi": "10.1109/IJCNN.2016.7727521", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Offline Handwritten Signature Verification has been researched over\nthe last few decades from several perspectives, using insights from graphology,\ncomputer vision, signal processing, among others. In spite of the advancements\non the field, building classifiers that can separate between genuine signatures\nand skilled forgeries (forgeries made targeting a particular signature) is\nstill hard. We propose approaching the problem from a feature learning\nperspective. Our hypothesis is that, in the absence of a good model of the data\ngeneration process, it is better to learn the features from data, instead of\nusing hand-crafted features that have no resemblance to the signature\ngeneration process. To this end, we use Deep Convolutional Neural Networks to\nlearn features in a writer-independent format, and use this model to obtain a\nfeature representation on another set of users, where we train writer-dependent\nclassifiers. We tested our method in two datasets: GPDS-960 and Brazilian\nPUC-PR. Our experimental results show that the features learned in a subset of\nthe users are discriminative for the other users, including across different\ndatasets, reaching close to the state-of-the-art in the GPDS dataset, and\nimproving the state-of-the-art in the Brazilian PUC-PR dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 18:26:48 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1604.00989", "submitter": "Charles Otto", "authors": "Charles Otto, Dayong Wang, Anil K. Jain", "title": "Clustering Millions of Faces by Identity", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSU-CSE-16-3", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt to address the following problem: Given a large\nnumber of unlabeled face images, cluster them into the individual identities\npresent in this data. We consider this a relevant problem in different\napplication scenarios ranging from social media to law enforcement. In\nlarge-scale scenarios the number of faces in the collection can be of the order\nof hundreds of million, while the number of clusters can range from a few\nthousand to millions--leading to difficulties in terms of both run-time\ncomplexity and evaluating clustering and per-cluster quality. An efficient and\neffective Rank-Order clustering algorithm is developed to achieve the desired\nscalability, and better clustering accuracy than other well-known algorithms\nsuch as k-means and spectral clustering. We cluster up to 123 million face\nimages into over 10 million clusters, and analyze the results in terms of both\nexternal cluster quality measures (known face labels) and internal cluster\nquality measures (unknown face labels) and run-time. Our algorithm achieves an\nF-measure of 0.87 on a benchmark unconstrained face dataset (LFW, consisting of\n13K faces), and 0.27 on the largest dataset considered (13K images in LFW, plus\n123M distractor images). Additionally, we present preliminary work on video\nframe clustering (achieving 0.71 F-measure when clustering all frames in the\nbenchmark YouTube Faces dataset). A per-cluster quality measure is developed\nwhich can be used to rank individual clusters and to automatically identify a\nsubset of good quality clusters for manual exploration.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 18:53:12 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Otto", "Charles", ""], ["Wang", "Dayong", ""], ["Jain", "Anil K.", ""]]}, {"id": "1604.00990", "submitter": "Hatem Alismail", "authors": "Hatem Alismail, Brett Browning, Simon Lucey", "title": "Direct Visual Odometry using Bit-Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature descriptors, such as SIFT and ORB, are well-known for their\nrobustness to illumination changes, which has made them popular for\nfeature-based VSLAM\\@. However, in degraded imaging conditions such as low\nlight, low texture, blur and specular reflections, feature extraction is often\nunreliable. In contrast, direct VSLAM methods which estimate the camera pose by\nminimizing the photometric error using raw pixel intensities are often more\nrobust to low textured environments and blur. Nonetheless, at the core of\ndirect VSLAM is the reliance on a consistent photometric appearance across\nimages, otherwise known as the brightness constancy assumption. Unfortunately,\nbrightness constancy seldom holds in real world applications.\n  In this work, we overcome brightness constancy by incorporating feature\ndescriptors into a direct visual odometry framework. This combination results\nin an efficient algorithm that combines the strength of both feature-based\nalgorithms and direct methods. Namely, we achieve robustness to arbitrary\nphotometric variations while operating in low-textured and poorly lit\nenvironments. Our approach utilizes an efficient binary descriptor, which we\ncall Bit-Planes, and show how it can be used in the gradient-based optimization\nrequired by direct methods. Moreover, we show that the squared Euclidean\ndistance between Bit-Planes is equivalent to the Hamming distance. Hence, the\ndescriptor may be used in least squares optimization without sacrificing its\nphotometric invariance. Finally, we present empirical results that demonstrate\nthe robustness of the approach in poorly lit underground environments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 19:02:45 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Alismail", "Hatem", ""], ["Browning", "Brett", ""], ["Lucey", "Simon", ""]]}, {"id": "1604.00999", "submitter": "Michael Firman", "authors": "Michael Firman", "title": "RGBD Datasets: Past, Present and Future", "comments": "8 pages excluding references (CVPR style)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the launch of the Microsoft Kinect, scores of RGBD datasets have been\nreleased. These have propelled advances in areas from reconstruction to gesture\nrecognition. In this paper we explore the field, reviewing datasets across\neight categories: semantics, object pose estimation, camera tracking, scene\nreconstruction, object tracking, human actions, faces and identification. By\nextracting relevant information in each category we help researchers to find\nappropriate data for their needs, and we consider which datasets have succeeded\nin driving computer vision forward and why.\n  Finally, we examine the future of RGBD datasets. We identify key areas which\nare currently underexplored, and suggest that future directions may include\nsynthetic data and dense reconstructions of static and dynamic scenes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 19:35:56 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 09:19:44 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Firman", "Michael", ""]]}, {"id": "1604.01093", "submitter": "Angela Dai", "authors": "Angela Dai and Matthias Nie{\\ss}ner and Michael Zollh\\\"ofer and\n  Shahram Izadi and Christian Theobalt", "title": "BundleFusion: Real-time Globally Consistent 3D Reconstruction using\n  On-the-fly Surface Re-integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\nreality and robotic applications. However, scalability brings challenges of\ndrift in pose estimation, introducing significant errors in the accumulated\nmodel. Approaches often require hours of offline processing to globally correct\nmodel errors. Recent online methods demonstrate compelling results, but suffer\nfrom: (1) needing minutes to perform online correction preventing true\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\nresulting in many tracking failures; or (3) supporting only unstructured\npoint-based representations, which limit scan quality and applicability. We\nsystematically address these issues with a novel, real-time, end-to-end\nreconstruction framework. At its core is a robust pose estimation strategy,\noptimizing per frame for a global set of camera poses by considering the\ncomplete history of RGB-D input with an efficient hierarchical approach. We\nremove the heavy reliance on temporal tracking, and continually localize to the\nglobally optimized frames instead. We contribute a parallelizable optimization\nframework, which employs correspondences based on sparse features and dense\ngeometric and photometric matching. Our approach estimates globally optimized\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\nthe 3D model in real-time to ensure global consistency; all within a single\nframework. Our approach outperforms state-of-the-art online systems with\nquality on par to offline methods, but with unprecedented speed and scan\ncompleteness. Our framework leads to a comprehensive online scanning solution\nfor large indoor environments, enabling ease of use and high-quality results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 00:06:39 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 02:19:47 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 19:00:46 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""], ["Zollh\u00f6fer", "Michael", ""], ["Izadi", "Shahram", ""], ["Theobalt", "Christian", ""]]}, {"id": "1604.01109", "submitter": "Zhanning Gao", "authors": "Zhanning Gao, Gang Hua, Dongqing Zhang, Jianru Xue, Nanning Zheng", "title": "Counting Grid Aggregation for Event Retrieval and Recognition", "comments": "This paper has been withdrawn by the author because this work will be\n  part of another object which will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event retrieval and recognition in a large corpus of videos necessitates a\nholistic fixed-size visual representation at the video clip level that is\ncomprehensive, compact, and yet discriminative. It shall comprehensively\naggregate information across relevant video frames, while suppress redundant\ninformation, leading to a compact representation that can effectively\ndifferentiate among different visual events. In search for such a\nrepresentation, we propose to build a spatially consistent counting grid model\nto aggregate together deep features extracted from different video frames. The\nspatial consistency of the counting grid model is achieved by introducing a\nprior model estimated from a large corpus of video data. The counting grid\nmodel produces an intermediate tensor representation for each video, which\nautomatically identifies and removes the feature redundancy across the\ndifferent frames. The tensor representation is subsequently reduced to a\nfixed-size vector representation by averaging over the counting grid. When\ncompared to existing methods on both event retrieval and event classification\nbenchmarks, we achieve significantly better accuracy with much more compact\nrepresentation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 01:38:07 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 09:01:57 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 12:11:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Gao", "Zhanning", ""], ["Hua", "Gang", ""], ["Zhang", "Dongqing", ""], ["Xue", "Jianru", ""], ["Zheng", "Nanning", ""]]}, {"id": "1604.01146", "submitter": "Chunhua Shen", "authors": "Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Less is more: zero-shot learning from online textual documents with\n  noise suppression", "comments": "Accepted to Int. Conf. Computer Vision and Pattern Recognition (CVPR)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying a visual concept merely from its associated online textual\nsource, such as a Wikipedia article, is an attractive research topic in\nzero-shot learning because it alleviates the burden of manually collecting\nsemantic attributes. Several recent works have pursued this approach by\nexploring various ways of connecting the visual and text domains. This paper\nrevisits this idea by stepping further to consider one important factor: the\ntextual representation is usually too noisy for the zero-shot learning\napplication. This consideration motivates us to design a simple-but-effective\nzero-shot learning method capable of suppressing noise in the text.\n  More specifically, we propose an $l_{2,1}$-norm based objective function\nwhich can simultaneously suppress the noisy signal in the text and learn a\nfunction to match the text document and visual features. We also develop an\noptimization algorithm to efficiently solve the resulting problem. By\nconducting experiments on two large datasets, we demonstrate that the proposed\nmethod significantly outperforms the competing methods which rely on online\ninformation sources but without explicit noise suppression. We further make an\nin-depth analysis of the proposed method and provide insight as to what kind of\ninformation in documents is useful for zero-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 06:13:06 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Qiao", "Ruizhi", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1604.01252", "submitter": "Dong Liu", "authors": "Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li", "title": "Comparative Deep Learning of Hybrid Representations for Image\n  Recommendations", "comments": "CVPR 2016", "journal-ref": null, "doi": "10.1109/CVPR.2016.279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many image-related tasks, learning expressive and discriminative\nrepresentations of images is essential, and deep learning has been studied for\nautomating the learning of such representations. Some user-centric tasks, such\nas image recommendations, call for effective representations of not only images\nbut also preferences and intents of users over images. Such representations are\ntermed \\emph{hybrid} and addressed via a deep learning approach in this paper.\nWe design a dual-net deep network, in which the two sub-networks map input\nimages and preferences of users into a same latent semantic space, and then the\ndistances between images and users in the latent space are calculated to make\ndecisions. We further propose a comparative deep learning (CDL) method to train\nthe deep network, using a pair of images compared against one user to learn the\npattern of their relative distances. The CDL embraces much more training data\nthan naive deep learning, and thus achieves superior performance than the\nlatter, with no cost of increasing network complexity. Experimental results\nwith real-world data sets for image recommendations have shown the proposed\ndual-net network and CDL greatly outperform other state-of-the-art image\nrecommendation solutions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:34:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Lei", "Chenyi", ""], ["Liu", "Dong", ""], ["Li", "Weiping", ""], ["Zha", "Zheng-Jun", ""], ["Li", "Houqiang", ""]]}, {"id": "1604.01319", "submitter": "Lek-Heng Lim", "authors": "Ke Ye and Lek-Heng Lim", "title": "Cohomology of Cryo-Electron Microscopy", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensional\nstructure of a molecule from a collection of its 2-dimensional projected\nimages. In this article, we show that the basic premise of cryo-EM --- patching\ntogether 2-dimensional projections to reconstruct a 3-dimensional object --- is\nnaturally one of Cech cohomology with SO(2)-coefficients. We deduce that every\ncryo-EM reconstruction problem corresponds to an oriented circle bundle on a\nsimplicial complex, allowing us to classify cryo-EM problems via principal\nbundles. In practice, the 2-dimensional images are noisy and a main task in\ncryo-EM is to denoise them. We will see how the aforementioned insights can be\nused towards this end.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 16:26:47 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 10:08:39 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ye", "Ke", ""], ["Lim", "Lek-Heng", ""]]}, {"id": "1604.01325", "submitter": "Albert Gordo", "authors": "Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus", "title": "Deep Image Retrieval: Learning global representations for image search", "comments": "ECCV 2016 version + additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for instance-level image retrieval. It produces a\nglobal and compact fixed-length representation for each image by aggregating\nmany region-wise descriptors. In contrast to previous works employing\npre-trained deep networks as a black box to produce features, our method\nleverages a deep architecture trained for the specific task of image retrieval.\nOur contribution is twofold: (i) we leverage a ranking framework to learn\nconvolution and projection weights that are used to build the region features;\nand (ii) we employ a region proposal network to learn which regions should be\npooled to form the final global descriptor. We show that using clean training\ndata is key to the success of our approach. To that aim, we use a large scale\nbut noisy landmark dataset and develop an automatic cleaning approach. The\nproposed architecture produces a global image representation in a single\nforward pass. Our approach significantly outperforms previous approaches based\non global descriptors on standard datasets. It even surpasses most prior works\nbased on costly local descriptor indexing and spatial verification. Additional\nmaterial is available at www.xrce.xerox.com/Deep-Image-Retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 16:48:17 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 10:44:17 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Gordo", "Albert", ""], ["Almazan", "Jon", ""], ["Revaud", "Jerome", ""], ["Larlus", "Diane", ""]]}, {"id": "1604.01335", "submitter": "Brendan Jou", "authors": "Brendan Jou and Shih-Fu Chang", "title": "Deep Cross Residual Learning for Multitask Visual Recognition", "comments": "10 pages, 6 figures, To appear in ACM Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual learning has recently surfaced as an effective means of constructing\nvery deep neural networks for object recognition. However, current incarnations\nof residual networks do not allow for the modeling and integration of complex\nrelations between closely coupled recognition tasks or across domains. Such\nproblems are often encountered in multimedia applications involving large-scale\ncontent recognition. We propose a novel extension of residual learning for deep\nnetworks that enables intuitive learning across multiple related tasks using\ncross-connections called cross-residuals. These cross-residuals connections can\nbe viewed as a form of in-network regularization and enables greater network\ngeneralization. We show how cross-residual learning (CRL) can be integrated in\nmultitask networks to jointly train and detect visual concepts across several\ntasks. We present a single multitask cross-residual network with >40% less\nparameters that is able to achieve competitive, or even better, detection\nperformance on a visual sentiment concept detection problem normally requiring\nmultiple specialized single-task networks. The resulting multitask\ncross-residual network also achieves better detection performance by about\n10.4% over a standard multitask residual network without cross-residuals with\neven a small amount of cross-task weighting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:08:14 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 01:55:12 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1604.01345", "submitter": "Gabriel Schwartz", "authors": "Gabriel Schwartz and Ko Nishino", "title": "Integrating Local Material Recognition with Large-Scale Perceptual\n  Attribute Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material attributes have been shown to provide a discriminative intermediate\nrepresentation for recognizing materials, especially for the challenging task\nof recognition from local material appearance (i.e., regardless of object and\nscene context). In the past, however, material attributes have been recognized\nseparately preceding category recognition. In contrast, neuroscience studies on\nmaterial perception and computer vision research on object and place\nrecognition have shown that attributes are produced as a by-product during the\ncategory recognition process. Does the same hold true for material attribute\nand category recognition? In this paper, we introduce a novel material category\nrecognition network architecture to show that perceptual attributes can, in\nfact, be automatically discovered inside a local material recognition\nframework. The novel material-attribute-category convolutional neural network\n(MAC-CNN) produces perceptual material attributes from the intermediate pooling\nlayers of an end-to-end trained category recognition network using an auxiliary\nloss function that encodes human material perception. To train this model, we\nintroduce a novel large-scale database of local material appearance organized\nunder a canonical material category taxonomy and careful image patch extraction\nthat avoids unwanted object and scene context. We show that the discovered\nattributes correspond well with semantically-meaningful visual material traits\nvia Boolean algebra, and enable recognition of previously unseen material\ncategories given only a few examples. These results have strong implications in\nhow perceptually meaningful attributes can be learned in other recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:40:57 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 18:31:46 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 06:32:13 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 04:03:39 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Schwartz", "Gabriel", ""], ["Nishino", "Ko", ""]]}, {"id": "1604.01347", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Bryan Russell, Abhinav Gupta", "title": "Marr Revisited: 2D-3D Alignment via Surface Normal Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach that leverages surface normal predictions, along\nwith appearance cues, to retrieve 3D models for objects depicted in 2D still\nimages from a large CAD object library. Critical to the success of our approach\nis the ability to recover accurate surface normals for objects in the depicted\nscene. We introduce a skip-network model built on the pre-trained Oxford VGG\nconvolutional neural network (CNN) for surface normal prediction. Our model\nachieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface\nnormal prediction, and recovers fine object detail compared to previous\nmethods. Furthermore, we develop a two-stream network over the input image and\npredicted surface normals that jointly learns pose and style for CAD model\nretrieval. When using the predicted surface normals, our two-stream network\nmatches prior work using surface normals computed from RGB-D images on the task\nof pose prediction, and achieves state of the art when using RGB-D input.\nFinally, our two-stream network allows us to retrieve CAD models that better\nmatch the style and pose of a depicted object compared with baseline\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:51:39 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Bansal", "Aayush", ""], ["Russell", "Bryan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1604.01354", "submitter": "Ko Nishino", "authors": "Stephen Lombardi and Ko Nishino", "title": "Radiometric Scene Decomposition: Scene Reflectance, Illumination, and\n  Geometry from RGB-D Images", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the radiometric properties of a scene (i.e., the reflectance,\nillumination, and geometry) is a long-sought ability of computer vision that\ncan provide invaluable information for a wide range of applications.\nDeciphering the radiometric ingredients from the appearance of a real-world\nscene, as opposed to a single isolated object, is particularly challenging as\nit generally consists of various objects with different material compositions\nexhibiting complex reflectance and light interactions that are also part of the\nillumination. We introduce the first method for radiometric scene decomposition\nthat handles those intricacies. We use RGB-D images to bootstrap geometry\nrecovery and simultaneously recover the complex reflectance and natural\nillumination while refining the noisy initial geometry and segmenting the scene\ninto different material regions. Most important, we handle real-world scenes\nconsisting of multiple objects of unknown materials, which necessitates the\nmodeling of spatially-varying complex reflectance, natural illumination,\ntexture, interreflection and shadows. We systematically evaluate the\neffectiveness of our method on synthetic scenes and demonstrate its application\nto real-world scenes. The results show that rich radiometric information can be\nrecovered from RGB-D images and demonstrate a new role RGB-D sensors can play\nfor general scene understanding tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:18:41 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Lombardi", "Stephen", ""], ["Nishino", "Ko", ""]]}, {"id": "1604.01360", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav\n  Gupta", "title": "The Curious Robot: Learning Visual Representations via Physical\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the right supervisory signal to train visual representations? Current\napproaches in computer vision use category labels from datasets such as\nImageNet to train ConvNets. However, in case of biological agents, visual\nrepresentation learning does not require millions of semantic labels. We argue\nthat biological agents use physical interactions with the world to learn visual\nrepresentations unlike current vision systems which just use passive\nobservations (images and videos downloaded from web). For example, babies push\nobjects, poke them, put them in their mouth and throw them to learn\nrepresentations. Towards this goal, we build one of the first systems on a\nBaxter platform that pushes, pokes, grasps and observes objects in a tabletop\nenvironment. It uses four different types of physical interactions to collect\nmore than 130K datapoints, with each datapoint providing supervision to a\nshared ConvNet architecture allowing us to learn visual representations. We\nshow the quality of learned representations by observing neuron activations and\nperforming nearest neighbor retrieval on this learned representation.\nQuantitatively, we evaluate our learned ConvNet on image classification tasks\nand show improvements compared to learning without external data. Finally, on\nthe task of instance retrieval, our network outperforms the ImageNet network on\nrecall@1 by 3%\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:47:15 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 03:30:44 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Pinto", "Lerrel", ""], ["Gandhi", "Dhiraj", ""], ["Han", "Yuanfeng", ""], ["Park", "Yong-Lae", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1604.01420", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Reza Shoja Ghiass and Ognjen Arandjelovic", "title": "Highly accurate gaze estimation using a consumer RGB-depth sensor", "comments": "International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the direction in which a person is looking is an important\nproblem in a wide range of HCI applications. In this paper we describe a highly\naccurate algorithm that performs gaze estimation using an affordable and widely\navailable device such as Kinect. The method we propose starts by performing\naccurate head pose estimation achieved by fitting a person specific morphable\nmodel of the face to depth data. The ordinarily competing requirements of high\naccuracy and high speed are met concurrently by formulating the fitting\nobjective function as a combination of terms which excel either in accurate or\nfast fitting, and then by adaptively adjusting their relative contributions\nthroughout fitting. Following pose estimation, pose normalization is done by\nre-rendering the fitted model as a frontal face. Finally gaze estimates are\nobtained through regression from the appearance of the eyes in synthetic,\nnormalized images. Using EYEDIAP, the standard public dataset for the\nevaluation of gaze estimation algorithms from RGB-D data, we demonstrate that\nour method greatly outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 20:50:40 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ghiass", "Reza Shoja", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1604.01431", "submitter": "Wei-Chiu Ma", "authors": "Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani", "title": "Forecasting Interactive Dynamics of Pedestrians with Fictitious Play", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop predictive models of pedestrian dynamics by encoding the coupled\nnature of multi-pedestrian interaction using game theory, and deep\nlearning-based visual analysis to estimate person-specific behavior parameters.\nBuilding predictive models for multi-pedestrian interactions however, is very\nchallenging due to two reasons: (1) the dynamics of interaction are complex\ninterdependent processes, where the predicted behavior of one pedestrian can\naffect the actions taken by others and (2) dynamics are variable depending on\nan individuals physical characteristics (e.g., an older person may walk slowly\nwhile the younger person may walk faster). To address these challenges, we (1)\nutilize concepts from game theory to model the interdependent decision making\nprocess of multiple pedestrians and (2) use visual classifiers to learn a\nmapping from pedestrian appearance to behavior parameters. We evaluate our\nproposed model on several public multiple pedestrian interaction video\ndatasets. Results show that our strategic planning model explains human\ninteractions 25% better when compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 21:13:32 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 18:07:23 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 16:31:01 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Ma", "Wei-Chiu", ""], ["Huang", "De-An", ""], ["Lee", "Namhoon", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1604.01444", "submitter": "Adam Aurisano", "authors": "A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M. D. Messier, E. Niner,\n  G. Pawloski, F. Psihas, A. Sousa, P. Vahle", "title": "A Convolutional Neural Network Neutrino Event Classifier", "comments": "23 pages, 12 figures", "journal-ref": "2016 JINST 11 P09001", "doi": "10.1088/1748-0221/11/09/P09001", "report-no": "FERMILAB-PUB-16-082-ND", "categories": "hep-ex cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely applied in the computer\nvision community to solve complex problems in image recognition and analysis.\nWe describe an application of the CNN technology to the problem of identifying\nparticle interactions in sampling calorimeters used commonly in high energy\nphysics and high energy neutrino physics in particular. Following a discussion\nof the core concepts of CNNs and recent innovations in CNN architectures\nrelated to the field of deep learning, we outline a specific application to the\nNOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network)\nidentifies neutrino interactions based on their topology without the need for\ndetailed reconstruction and outperforms algorithms currently in use by the NOvA\ncollaboration.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 22:41:13 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 16:31:53 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 19:31:15 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Aurisano", "A.", ""], ["Radovic", "A.", ""], ["Rocco", "D.", ""], ["Himmel", "A.", ""], ["Messier", "M. D.", ""], ["Niner", "E.", ""], ["Pawloski", "G.", ""], ["Psihas", "F.", ""], ["Sousa", "A.", ""], ["Vahle", "P.", ""]]}, {"id": "1604.01475", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang", "title": "Learning A Deep $\\ell_\\infty$ Encoder for Hashing", "comments": "To be presented at IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the $\\ell_\\infty$-constrained representation which\ndemonstrates robustness to quantization errors, utilizing the tool of deep\nlearning. Based on the Alternating Direction Method of Multipliers (ADMM), we\nformulate the original convex minimization problem as a feed-forward neural\nnetwork, named \\textit{Deep $\\ell_\\infty$ Encoder}, by introducing the novel\nBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as\nnetwork biases. Such a structural prior acts as an effective network\nregularization, and facilitates the model initialization. We then investigate\nthe effective use of the proposed model in the application of hashing, by\ncoupling the proposed encoders under a supervised pairwise loss, to develop a\n\\textit{Deep Siamese $\\ell_\\infty$ Network}, which can be optimized from end to\nend. Extensive experiments demonstrate the impressive performances of the\nproposed model. We also provide an in-depth analysis of its behaviors against\nthe competitors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 03:54:33 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1604.01485", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski, Shuicheng Yan, Jiashi Feng", "title": "A Focused Dynamic Attention Model for Visual Question Answering", "comments": "Submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question and Answering (VQA) problems are attracting increasing\ninterest from multiple research disciplines. Solving VQA problems requires\ntechniques from both computer vision for understanding the visual contents of a\npresented image or video, as well as the ones from natural language processing\nfor understanding semantics of the question and generating the answers.\nRegarding visual content modeling, most of existing VQA methods adopt the\nstrategy of extracting global features from the image or video, which\ninevitably fails in capturing fine-grained information such as spatial\nconfiguration of multiple objects. Extracting features from auto-generated\nregions -- as some region-based image recognition methods do -- cannot\nessentially address this problem and may introduce some overwhelming irrelevant\nfeatures with the question. In this work, we propose a novel Focused Dynamic\nAttention (FDA) model to provide better aligned image content representation\nwith proposed questions. Being aware of the key words in the question, FDA\nemploys off-the-shelf object detector to identify important regions and fuse\nthe information from the regions and global features via an LSTM unit. Such\nquestion-driven representations are then combined with question representation\nand fed into a reasoning unit for generating the answers. Extensive evaluation\non a large-scale benchmark dataset, VQA, clearly demonstrate the superior\nperformance of FDA over well-established baselines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 05:16:10 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ilievski", "Ilija", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1604.01497", "submitter": "Xuefeng Liang", "authors": "Shuang Wang, Bo Yue, Xuefeng Liang, Peiyuan Ji, and Licheng Jiao", "title": "How Does the Low-Rank Matrix Decomposition Help Internal and External\n  Learnings for Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wisely utilizing the internal and external learning methods is a new\nchallenge in super-resolution problem. To address this issue, we analyze the\nattributes of two methodologies and find two observations of their recovered\ndetails: 1) they are complementary in both feature space and image plane, 2)\nthey distribute sparsely in the spatial space. These inspire us to propose a\nlow-rank solution which effectively integrates two learning methods and then\nachieves a superior result. To fit this solution, the internal learning method\nand the external learning method are tailored to produce multiple preliminary\nresults. Our theoretical analysis and experiment prove that the proposed\nlow-rank solution does not require massive inputs to guarantee the performance,\nand thereby simplifying the design of two learning methods for the solution.\nIntensive experiments show the proposed solution improves the single learning\nmethod in both qualitative and quantitative assessments. Surprisingly, it shows\nmore superior capability on noisy images and outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 06:03:02 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 03:37:19 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 04:17:10 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Wang", "Shuang", ""], ["Yue", "Bo", ""], ["Liang", "Xuefeng", ""], ["Ji", "Peiyuan", ""], ["Jiao", "Licheng", ""]]}, {"id": "1604.01500", "submitter": "Karan Sikka", "authors": "Karan Sikka, Gaurav Sharma and Marian Bartlett", "title": "LOMo: Latent Ordinal Model for Facial Analysis in Videos", "comments": "2016 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of facial analysis in videos. We propose a novel weakly\nsupervised learning method that models the video event (expression, pain etc.)\nas a sequence of automatically mined, discriminative sub-events (eg. onset and\noffset phase for smile, brow lower and cheek raise for pain). The proposed\nmodel is inspired by the recent works on Multiple Instance Learning and latent\nSVM/HCRF- it extends such frameworks to model the ordinal or temporal aspect in\nthe videos, approximately. We obtain consistent improvements over relevant\ncompetitive baselines on four challenging and publicly available video based\nfacial analysis datasets for prediction of expression, clinical pain and intent\nin dyadic conversations. In combination with complimentary features, we report\nstate-of-the-art results on these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 06:14:58 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Sikka", "Karan", ""], ["Sharma", "Gaurav", ""], ["Bartlett", "Marian", ""]]}, {"id": "1604.01545", "submitter": "German Ros", "authors": "German Ros, Simon Stent, Pablo F. Alcantarilla and Tomoki Watanabe", "title": "Training Constrained Deconvolutional Networks for Road Scene Semantic\n  Segmentation", "comments": "submitted as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the problem of road scene semantic segmentation\nusing Deconvolutional Networks (DNs). Several constraints limit the practical\nperformance of DNs in this context: firstly, the paucity of existing pixel-wise\nlabelled training data, and secondly, the memory constraints of embedded\nhardware, which rule out the practical use of state-of-the-art DN architectures\nsuch as fully convolutional networks (FCN). To address the first constraint, we\nintroduce a Multi-Domain Road Scene Semantic Segmentation (MDRS3) dataset,\naggregating data from six existing densely and sparsely labelled datasets for\ntraining our models, and two existing, separate datasets for testing their\ngeneralisation performance. We show that, while MDRS3 offers a greater volume\nand variety of data, end-to-end training of a memory efficient DN does not\nyield satisfactory performance. We propose a new training strategy to overcome\nthis, based on (i) the creation of a best-possible source network (S-Net) from\nthe aggregated data, ignoring time and memory constraints; and (ii) the\ntransfer of knowledge from S-Net to the memory-efficient target network\n(T-Net). We evaluate different techniques for S-Net creation and T-Net\ntransferral, and demonstrate that training a constrained deconvolutional\nnetwork in this manner can unlock better performance than existing training\napproaches. Specifically, we show that a target network can be trained to\nachieve improved accuracy versus an FCN despite using less than 1\\% of the\nmemory. We believe that our approach can be useful beyond automotive scenarios\nwhere labelled data is similarly scarce or fragmented and where practical\nconstraints exist on the desired model size. We make available our network\nmodels and aggregated multi-domain dataset for reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 09:02:50 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ros", "German", ""], ["Stent", "Simon", ""], ["Alcantarilla", "Pablo F.", ""], ["Watanabe", "Tomoki", ""]]}, {"id": "1604.01592", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Fast $(1+\\epsilon)$-approximation of the L\\\"owner extremal matrices of\n  high-dimensional symmetric matrices", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix data sets are common nowadays like in biomedical imaging where the\nDiffusion Tensor Magnetic Resonance Imaging (DT-MRI) modality produces data\nsets of\n  3D symmetric positive definite matrices anchored at voxel positions capturing\nthe anisotropic diffusion properties of water molecules in biological tissues.\nThe space of symmetric matrices can be partially ordered using the L\\\"owner\nordering, and computing extremal matrices dominating a given set of matrices is\na basic primitive used in matrix-valued signal processing. In this letter, we\ndesign a fast and easy-to-implement iterative algorithm to approximate\narbitrarily finely these extremal matrices. Finally, we discuss on extensions\nto matrix clustering.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 12:39:20 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1604.01655", "submitter": "Ziyan Wang", "authors": "Ziyan Wang, Jiwen Lu, Ruogu Lin, Jianjiang Feng, Jie zhou", "title": "Correlated and Individual Multi-Modal Deep Learning for RGB-D Object\n  Recognition", "comments": "11 pages, 7 figures, submitted to a conference in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new correlated and individual multi-modal deep\nlearning (CIMDL) method for RGB-D object recognition. Unlike most conventional\nRGB-D object recognition methods which extract features from the RGB and depth\nchannels individually, our CIMDL jointly learns feature representations from\nraw RGB-D data with a pair of deep neural networks, so that the sharable and\nmodal-specific information can be simultaneously exploited. Specifically, we\nconstruct a pair of deep convolutional neural networks (CNNs) for the RGB and\ndepth data, and concatenate them at the top layer of the network with a loss\nfunction which learns a new feature space where both correlated part and the\nindividual part of the RGB-D information are well modelled. The parameters of\nthe whole networks are updated by using the back-propagation criterion.\nExperimental results on two widely used RGB-D object image benchmark datasets\nclearly show that our method outperforms state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 15:06:02 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 12:08:07 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 13:56:02 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Wang", "Ziyan", ""], ["Lu", "Jiwen", ""], ["Lin", "Ruogu", ""], ["Feng", "Jianjiang", ""], ["zhou", "Jie", ""]]}, {"id": "1604.01662", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "A Survey on Bayesian Deep Learning", "comments": "Published in ACM Computing Surveys (CSUR) 2020. Constantly updating\n  project page at https://github.com/js05212/BayesianDeepLearning-Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 15:35:08 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 06:17:44 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:52:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 03:14:13 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1604.01683", "submitter": "Lakshmi Prabha Nattamai Sekar", "authors": "N. S. Lakshmiprabha", "title": "Fusing Face and Periocular biometrics using Canonical correlation\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel face and periocular biometric fusion at feature\nlevel using canonical correlation analysis. Face recognition itself has\nlimitations such as illumination, pose, expression, occlusion etc. Also,\nperiocular biometrics has spectacles, head angle, hair and expression as its\nlimitations. Unimodal biometrics cannot surmount all these limitations. The\nrecognition accuracy can be increased by fusing dual information (face and\nperiocular) from a single source (face image) using canonical correlation\nanalysis (CCA). This work also proposes a new wavelet decomposed local binary\npattern (WD-LBP) feature extractor which provides sufficient features for\nfusion. A detailed analysis on face and periocular biometrics shows that WD-LBP\nfeatures are more accurate and faster than local binary pattern (LBP) and gabor\nwavelet. The experimental results using Muct face database reveals that the\nproposed multimodal biometrics performs better than the unimodal biometrics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:23:49 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Lakshmiprabha", "N. S.", ""]]}, {"id": "1604.01684", "submitter": "Lakshmi Prabha Nattamai Sekar", "authors": "N. S. Lakshmiprabha", "title": "Face Image Analysis using AAM, Gabor, LBP and WD features for Gender,\n  Age, Expression and Ethnicity Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth in electronic transactions and human machine interactions rely on\nthe information such as gender, age, expression and ethnicity provided by the\nface image. In order to obtain these information, feature extraction plays a\nmajor role. In this paper, retrieval of age, gender, expression and race\ninformation from an individual face image is analysed using different feature\nextraction methods. The performance of four major feature extraction methods\nsuch as Active Appearance Model (AAM), Gabor wavelets, Local Binary Pattern\n(LBP) and Wavelet Decomposition (WD) are analyzed for gender recognition, age\nestimation, expression recognition and racial recognition in terms of accuracy\n(recognition rate), time for feature extraction, neural training and time to\ntest an image. Each of this recognition system is compared with four feature\nextractors on same dataset (training and validation set) to get a better\nunderstanding in its performance. Experiments carried out on FG-NET,\nCohn-Kanade, PAL face database shows that each method has its own merits and\ndemerits. Hence it is practically impossible to define a method which is best\nat all circumstances with less computational complexity. Further, a detailed\ncomparison of age estimation and age estimation using gender information is\nprovided along with a solution to overcome aging effect in case of gender\nrecognition. An attempt has been made in obtaining all (i.e. gender, age range,\nexpression and ethnicity) information from a test image in a single go.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 17:49:14 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Lakshmiprabha", "N. S.", ""]]}, {"id": "1604.01685", "submitter": "Marius Cordts", "authors": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus\n  Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "comments": "Includes supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual understanding of complex urban street scenes is an enabling factor for\na wide range of applications. Object detection has benefited enormously from\nlarge-scale datasets, especially in the context of deep learning. For semantic\nurban scene understanding, however, no current dataset adequately captures the\ncomplexity of real-world urban scenes.\n  To address this, we introduce Cityscapes, a benchmark suite and large-scale\ndataset to train and test approaches for pixel-level and instance-level\nsemantic labeling. Cityscapes is comprised of a large, diverse set of stereo\nvideo sequences recorded in streets from 50 different cities. 5000 of these\nimages have high quality pixel-level annotations; 20000 additional images have\ncoarse annotations to enable methods that leverage large volumes of\nweakly-labeled data. Crucially, our effort exceeds previous attempts in terms\nof dataset size, annotation richness, scene variability, and complexity. Our\naccompanying empirical study provides an in-depth analysis of the dataset\ncharacteristics, as well as a performance evaluation of several\nstate-of-the-art approaches based on our benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 16:34:33 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 15:39:22 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Cordts", "Marius", ""], ["Omran", "Mohamed", ""], ["Ramos", "Sebastian", ""], ["Rehfeld", "Timo", ""], ["Enzweiler", "Markus", ""], ["Benenson", "Rodrigo", ""], ["Franke", "Uwe", ""], ["Roth", "Stefan", ""], ["Schiele", "Bernt", ""]]}, {"id": "1604.01720", "submitter": "Eric Wengrowski", "authors": "Eric Wengrowski, Kristin Dana, Marco Gruteser, and Narayan Mandayam", "title": "Reading Between the Pixels: Photographic Steganography for Camera\n  Display Messaging", "comments": "16 pages with references 8 tables and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit human color metamers to send light-modulated messages less visible\nto the human eye, but recoverable by cameras. These messages are a key\ncomponent to camera-display messaging, such as handheld smartphones capturing\ninformation from electronic signage. Each color pixel in the display image is\nmodified by a particular color gradient vector. The challenge is to find the\ncolor gradient that maximizes camera response, while minimizing human response.\nThe mismatch in human spectral and camera sensitivity curves creates an\nopportunity for hidden messaging. Our approach does not require knowledge of\nthese sensitivity curves, instead we employ a data-driven method. We learn an\nellipsoidal partitioning of the six-dimensional space of colors and color\ngradients. This partitioning creates metamer sets defined by the base color at\nthe display pixel and the color gradient direction for message encoding. We\nsample from the resulting metamer sets to find color steps for each base color\nto embed a binary message into an arbitrary image with reduced visible\nartifacts. Unlike previous methods that rely on visually obtrusive intensity\nmodulation, we embed with color so that the message is more hidden. Ordinary\ndisplays and cameras are used without the need for expensive LEDs or high speed\ndevices. The primary contribution of this work is a framework to map the pixels\nin an arbitrary image to a metamer pair for steganographic photo messaging.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 18:43:18 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wengrowski", "Eric", ""], ["Dana", "Kristin", ""], ["Gruteser", "Marco", ""], ["Mandayam", "Narayan", ""]]}, {"id": "1604.01729", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, Kate\n  Saenko", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined\n  from Text", "comments": "Accepted at EMNLP 2016. Project page:\n  http://vsubhashini.github.io/language_fusion.html", "journal-ref": "Proc.EMNLP (2016) pg.1961-1966", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while modestly improving descriptive quality.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 19:01:28 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 20:37:42 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Hendricks", "Lisa Anne", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""]]}, {"id": "1604.01753", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, G\\\"ul Varol, Xiaolong Wang, Ali Farhadi, Ivan\n  Laptev, Abhinav Gupta", "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has a great potential to help our daily lives by searching\nfor lost keys, watering flowers or reminding us to take a pill. To succeed with\nsuch tasks, computer vision methods need to be trained from real and diverse\nexamples of our daily dynamic scenes. While most of such scenes are not\nparticularly exciting, they typically do not appear on YouTube, in movies or TV\nbroadcasts. So how do we collect sufficiently many diverse but boring samples\nrepresenting our lives? We propose a novel Hollywood in Homes approach to\ncollect such data. Instead of shooting videos in the lab, we ensure diversity\nby distributing and crowdsourcing the whole process of video creation from\nscript writing to video recording and annotation. Following this procedure we\ncollect a new dataset, Charades, with hundreds of people recording videos in\ntheir own homes, acting out casual everyday activities. The dataset is composed\nof 9,848 annotated videos with an average length of 30 seconds, showing\nactivities of 267 people from three continents. Each video is annotated by\nmultiple free-text descriptions, action labels, action intervals and classes of\ninteracted objects. In total, Charades provides 27,847 video descriptions,\n66,500 temporally localized intervals for 157 action classes and 41,104 labels\nfor 46 object classes. Using this rich data, we evaluate and provide baseline\nresults for several tasks including action recognition and automatic\ndescription generation. We believe that the realism, diversity, and casual\nnature of this dataset will present unique challenges and new opportunities for\ncomputer vision community.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 19:56:04 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 19:57:24 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 22:49:22 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Varol", "G\u00fcl", ""], ["Wang", "Xiaolong", ""], ["Farhadi", "Ali", ""], ["Laptev", "Ivan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1604.01787", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, Laetitia Chapel, S\\'ebastien Lef\\`evre", "title": "A Subpath Kernel for Learning Hierarchical Image Representations", "comments": "10th IAPR-TC-15 International Workshop, GbRPR 2015, Beijing, China,\n  May 13-15, 2015. Proceedings", "journal-ref": null, "doi": "10.1007/978-3-319-18224-7_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree kernels have demonstrated their ability to deal with hierarchical data,\nas the intrinsic tree structure often plays a discriminative role. While such\nkernels have been successfully applied to various domains such as nature\nlanguage processing and bioinformatics, they mostly concentrate on ordered\ntrees and whose nodes are described by symbolic data. Meanwhile, hierarchical\nrepresentations have gained increasing interest to describe image content. This\nis particularly true in remote sensing, where such representations allow for\nrevealing different objects of interest at various scales through a tree\nstructure. However, the induced trees are unordered and the nodes are equipped\nwith numerical features. In this paper, we propose a new structured kernel for\nhierarchical image representations which is built on the concept of subpath\nkernel. Experimental results on both artificial and remote sensing datasets\nshow that the proposed kernel manages to deal with the hierarchical nature of\nthe data, leading to better classification rates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:04:17 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Cui", "Yanwei", ""], ["Chapel", "Laetitia", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1604.01802", "submitter": "David Held", "authors": "David Held, Sebastian Thrun, Silvio Savarese", "title": "Learning to Track at 100 FPS with Deep Regression Networks", "comments": "To appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are often used in computer vision due to their\nability to leverage large amounts of training data to improve performance.\nUnfortunately, most generic object trackers are still trained from scratch\nonline and do not benefit from the large number of videos that are readily\navailable for offline training. We propose a method for offline training of\nneural networks that can track novel objects at test-time at 100 fps. Our\ntracker is significantly faster than previous methods that use neural networks\nfor tracking, which are typically very slow to run and not practical for\nreal-time applications. Our tracker uses a simple feed-forward network with no\nonline training required. The tracker learns a generic relationship between\nobject motion and appearance and can be used to track novel objects that do not\nappear in the training set. We test our network on a standard tracking\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\nour performance improves as we add more videos to our offline training set. To\nthe best of our knowledge, our tracker is the first neural-network tracker that\nlearns to track generic objects at 100 fps.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:39:34 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 02:34:48 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Held", "David", ""], ["Thrun", "Sebastian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1604.01818", "submitter": "Qi Wei", "authors": "Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret, Jose Bioucas-Dias,\n  Simon Godsill", "title": "R-FUSE: Robust Fast Fusion of Multi-Band Images Based on Solving a\n  Sylvester Equation", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.03121", "journal-ref": null, "doi": "10.1109/LSP.2016.2608858", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a robust fast multi-band image fusion method to merge a\nhigh-spatial low-spectral resolution image and a low-spatial high-spectral\nresolution image. Following the method recently developed in [1], the\ngeneralized Sylvester matrix equation associated with the multi-band image\nfusion problem is solved in a more robust and efficient way by exploiting the\nWoodbury formula, avoiding any permutation operation in the frequency domain as\nwell as the blurring kernel invertibility assumption required in [1]. Thanks to\nthis improvement, the proposed algorithm requires fewer computational\noperations and is also more robust with respect to the blurring kernel compared\nwith the one in [1]. The proposed new algorithm is tested with different priors\nconsidered in [1]. Our conclusion is that the proposed fusion algorithm is more\nrobust than the one in [1] with a reduced computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 21:41:16 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wei", "Qi", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["Bioucas-Dias", "Jose", ""], ["Godsill", "Simon", ""]]}, {"id": "1604.01827", "submitter": "Min Bai", "authors": "Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun", "title": "Exploiting Semantic Information and Deep Matching for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of estimating optical flow from a monocular camera in\nthe context of autonomous driving. We build on the observation that the scene\nis typically composed of a static background, as well as a relatively small\nnumber of traffic participants which move rigidly in 3D. We propose to estimate\nthe traffic participants using instance-level segmentation. For each traffic\nparticipant, we use the epipolar constraints that govern each independent\nmotion for faster and more accurate estimation. Our second contribution is a\nnew convolutional net that learns to perform flow matching, and is able to\nestimate the uncertainty of its matches. This is a core element of our flow\nestimation pipeline. We demonstrate the effectiveness of our approach in the\nchallenging KITTI 2015 flow benchmark, and show that our approach outperforms\npublished approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 22:58:53 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 03:45:41 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bai", "Min", ""], ["Luo", "Wenjie", ""], ["Kundu", "Kaustav", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1604.01841", "submitter": "Miao Sun", "authors": "Miao Sun, Tony X. Han, Zhihai He", "title": "A Classification Leveraged Object Detector", "comments": "Work in 2013, which contained some detailed algorithms for PASCAL VOC\n  2012 detection competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the state-of-the-art image classification algorithms outperform\nthe best available object detector by a big margin in terms of average\nprecision. We, therefore, propose a simple yet principled approach that allows\nus to leverage object detection through image classification on supporting\nregions specified by a preliminary object detector. Using a simple bag-of-\nwords model based image classification algorithm, we leveraged the performance\nof the deformable model objector from 35.9% to 39.5% in average precision over\n20 categories on standard PASCAL VOC 2007 detection dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 01:11:50 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Sun", "Miao", ""], ["Han", "Tony X.", ""], ["He", "Zhihai", ""]]}, {"id": "1604.01850", "submitter": "Tong Xiao", "authors": "Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang", "title": "Joint Detection and Identification Feature Learning for Person Search", "comments": "CVPR 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing person re-identification benchmarks and methods mainly focus on\nmatching cropped pedestrian images between queries and candidates. However, it\nis different from real-world scenarios where the annotations of pedestrian\nbounding boxes are unavailable and the target person needs to be searched from\na gallery of whole scene images. To close the gap, we propose a new deep\nlearning framework for person search. Instead of breaking it down into two\nseparate tasks---pedestrian detection and person re-identification, we jointly\nhandle both aspects in a single convolutional neural network. An Online\nInstance Matching (OIM) loss function is proposed to train the network\neffectively, which is scalable to datasets with numerous identities. To\nvalidate our approach, we collect and annotate a large-scale benchmark dataset\nfor person search. It contains 18,184 images, 8,432 identities, and 96,143\npedestrian bounding boxes. Experiments show that our framework outperforms\nother separate approaches, and the proposed OIM loss function converges much\nfaster and better than the conventional Softmax loss.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 02:16:26 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 09:48:19 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 01:31:08 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Xiao", "Tong", ""], ["Li", "Shuang", ""], ["Wang", "Bochao", ""], ["Lin", "Liang", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1604.01879", "submitter": "Zhichao Zhou", "authors": "Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang and Longin Jan\n  Latecki", "title": "GIFT: A Real-time and Scalable 3D Shape Search Engine", "comments": "accepted by CVPR16, achieved the first place in Shrec2016\n  competition: Large-Scale 3D Shape Retrieval under the perturbed case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projective analysis is an important solution for 3D shape retrieval, since\nhuman visual perceptions of 3D shapes rely on various 2D observations from\ndifferent view points. Although multiple informative and discriminative views\nare utilized, most projection-based retrieval systems suffer from heavy\ncomputational cost, thus cannot satisfy the basic requirement of scalability\nfor search engines. In this paper, we present a real-time 3D shape search\nengine based on the projective images of 3D shapes. The real-time property of\nour search engine results from the following aspects: (1) efficient projection\nand view feature extraction using GPU acceleration; (2) the first inverted\nfile, referred as F-IF, is utilized to speed up the procedure of multi-view\nmatching; (3) the second inverted file (S-IF), which captures a local\ndistribution of 3D shapes in the feature manifold, is adopted for efficient\ncontext-based re-ranking. As a result, for each query the retrieval task can be\nfinished within one second despite the necessary cost of IO overhead. We name\nthe proposed 3D shape search engine, which combines GPU acceleration and\nInverted File Twice, as GIFT. Besides its high efficiency, GIFT also\noutperforms the state-of-the-art methods significantly in retrieval accuracy on\nvarious shape benchmarks and competitions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 05:45:56 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 07:30:06 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Bai", "Song", ""], ["Bai", "Xiang", ""], ["Zhou", "Zhichao", ""], ["Zhang", "Zhaoxiang", ""], ["Latecki", "Longin Jan", ""]]}, {"id": "1604.01889", "submitter": "Jie Luo", "authors": "Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding and Masashi Sugiyama", "title": "Reinterpreting the Transformation Posterior in Probabilistic Image\n  Registration", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic image registration methods estimate the posterior distribution\nof transformation. The conventional way of interpreting the transformation\nposterior is to use the mode as the most likely transformation and assign its\ncorresponding intensity to the registered voxel. Meanwhile, summary statistics\nof the posterior are employed to evaluate the registration uncertainty, that is\nthe trustworthiness of the registered image. Despite the wide acceptance, this\nconvention has never been justified. In this paper, based on illustrative\nexamples, we question the correctness and usefulness of conventional methods.\nIn order to faithfully translate the transformation posterior, we propose to\nencode the variability of values into a novel data type called ensemble fields.\nEnsemble fields can serve as a complement to the registered image and a\nfoundation for developing advanced methods to characterize the uncertainty in\nregistration-based tasks. We demonstrate the potential of ensemble fields by\npilot examples\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 06:51:21 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Luo", "Jie", ""], ["Popuri", "Karteek", ""], ["Cobzas", "Dana", ""], ["Ding", "Hongyi", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1604.01891", "submitter": "Xiaohang Ren", "authors": "Xiaohang Ren, Kai Chen and Jun Sun", "title": "A CNN Based Scene Chinese Text Recognition Algorithm With Synthetic Data\n  Engine", "comments": "2 pages, DAS 2016 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition plays an important role in many computer vision\napplications. The small size of available public available scene text datasets\nis the main challenge when training a text recognition CNN model. In this\npaper, we propose a CNN based Chinese text recognition algorithm. To enlarge\nthe dataset for training the CNN model, we design a synthetic data engine for\nChinese scene character generation, which generates representative character\nimages according to the fonts use frequency of Chinese texts. As the Chinese\ntext is more complex, the English text recognition CNN architecture is modified\nfor Chinese text. To ensure the small size nature character dataset and the\nlarge size artificial character dataset are comparable in training, the CNN\nmodel are trained progressively. The proposed Chinese text recognition\nalgorithm is evaluated with two Chinese text datasets. The algorithm achieves\nbetter recognize accuracy compared to the baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 07:08:25 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ren", "Xiaohang", ""], ["Chen", "Kai", ""], ["Sun", "Jun", ""]]}, {"id": "1604.01894", "submitter": "Xiaohang Ren", "authors": "Xiaohang Ren, Kai Chen, Jun Sun", "title": "A Novel Scene Text Detection Algorithm Based On Convolutional Neural\n  Network", "comments": "5 pages, IWPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Candidate text region extraction plays a critical role in convolutional\nneural network (CNN) based text detection from natural images. In this paper,\nwe propose a CNN based scene text detection algorithm with a new text region\nextractor. The so called candidate text region extractor I-MSER is based on\nMaximally Stable Extremal Region (MSER), which can improve the independency and\ncompleteness of the extracted candidate text regions. Design of I-MSER is\nmotivated by the observation that text MSERs have high similarity and are close\nto each other. The independency of candidate text regions obtained by I-MSER is\nguaranteed by selecting the most representative regions from a MSER tree which\nis generated according to the spatial overlapping relationship among the MSERs.\nA multi-layer CNN model is trained to score the confidence value of the\nextracted regions extracted by the I-MSER for text detection. The new text\ndetection algorithm based on I-MSER is evaluated with wide-used ICDAR 2011 and\n2013 datasets and shows improved detection performance compared to the existing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 07:16:35 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ren", "Xiaohang", ""], ["Chen", "Kai", ""], ["Sun", "Jun", ""]]}, {"id": "1604.01931", "submitter": "Zhanglin Peng", "authors": "Zhanglin Peng, Ruimao Zhang, Xiaodan Liang, Xiaobai Liu, Liang Lin", "title": "Geometric Scene Parsing with Hierarchical LSTM", "comments": "To be presented at IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of geometric scene parsing, i.e.\nsimultaneously labeling geometric surfaces (e.g. sky, ground and vertical\nplane) and determining the interaction relations (e.g. layering, supporting,\nsiding and affinity) between main regions. This problem is more challenging\nthan the traditional semantic scene labeling, as recovering geometric\nstructures necessarily requires the rich and diverse contextual information. To\nachieve these goals, we propose a novel recurrent neural network model, named\nHierarchical Long Short-Term Memory (H-LSTM). It contains two coupled\nsub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM\n(MS-LSTM) for handling the surface labeling and relation prediction,\nrespectively. The two sub-networks provide complementary information to each\nother to exploit hierarchical scene contexts, and they are jointly optimized\nfor boosting the performance. Our extensive experiments show that our model is\ncapable of parsing scene geometric structures and outperforming several\nstate-of-the-art methods by large margins. In addition, we show promising 3D\nreconstruction results from the still images based on the geometric parsing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 09:20:51 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 06:21:57 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Peng", "Zhanglin", ""], ["Zhang", "Ruimao", ""], ["Liang", "Xiaodan", ""], ["Liu", "Xiaobai", ""], ["Lin", "Liang", ""]]}, {"id": "1604.01962", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Shanmuganathan Raman", "title": "Automatic Content-aware Non-Photorealistic Rendering of Images", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-photorealistic rendering techniques work on image features and often\nmanipulate a set of characteristics such as edges and texture to achieve a\ndesired depiction of the scene. Most computational photography methods\ndecompose an image using edge preserving filters and work on the resulting base\nand detail layers independently to achieve desired visual effects. We propose a\nnew approach for content-aware non-photorealistic rendering of images where we\nmanipulate the visually salient and the non-salient regions separately. We\npropose a novel content-aware framework in order to render an image for\napplications such as detail exaggeration, artificial blurring and image\nabstraction. The processed regions of the image are blended seamlessly for all\nthese applications. We demonstrate that content awareness of the proposed\nmethod leads to automatic generation of non-photorealistic rendering of the\nsame image for the different applications mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 11:31:03 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 09:33:06 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 09:27:31 GMT"}, {"version": "v4", "created": "Tue, 19 Apr 2016 12:19:59 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1604.01980", "submitter": "Emanuel Laude", "authors": "Emanuel Laude, Thomas M\\\"ollenhoff, Michael Moeller, Jan Lellmann,\n  Daniel Cremers", "title": "Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex relaxations of nonconvex multilabel problems have been demonstrated to\nproduce superior (provably optimal or near-optimal) solutions to a variety of\nclassical computer vision problems. Yet, they are of limited practical use as\nthey require a fine discretization of the label space, entailing a huge demand\nin memory and runtime. In this work, we propose the first sublabel accurate\nconvex relaxation for vectorial multilabel problems. The key idea is that we\napproximate the dataterm of the vectorial labeling problem in a piecewise\nconvex (rather than piecewise linear) manner. As a result we have a more\nfaithful approximation of the original cost function that provides a meaningful\ninterpretation for the fractional solutions of the relaxed convex problem. In\nnumerous experiments on large-displacement optical flow estimation and on color\nimage denoising we demonstrate that the computed solutions have superior\nquality while requiring much lower memory and runtime.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 12:43:07 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 16:42:55 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Laude", "Emanuel", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Moeller", "Michael", ""], ["Lellmann", "Jan", ""], ["Cremers", "Daniel", ""]]}, {"id": "1604.02013", "submitter": "Akira Kageyama", "authors": "Akira Kageyama", "title": "Keyboard Based Control of Four Dimensional Rotations", "comments": "Accepted for publication in Journal of Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at applications to the scientific visualization of three dimensional\nsimulations with time evolution, a keyboard based control method to specify\nrotations in four dimensions is proposed. It is known that four dimensional\nrotations are generally so-called double rotations, and a double rotation is a\ncombination of simultaneously applied two simple rotations. The proposed method\ncan specify both the simple and double rotations by single key typings of the\nkeyboard. The method is tested in visualizations of a regular pentachoron in\nfour dimensional space by a hyperplane slicing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 05:20:04 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Kageyama", "Akira", ""]]}, {"id": "1604.02030", "submitter": "Vivek Kumar", "authors": "Vivek Kumar, Sumit Pandey, Amrindra Pal, Sandeep Sharma", "title": "Edge Detection Based Shape Identification", "comments": "Presented in the National Conference on Emerging Trends in\n  Engineering Science & Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image recognition is the need of the hour. In order to be able to recognize\nan image, it is of immense importance that the image should be distinguishable\nfrom the background. In the present work, an approach is presented for\nautomatic detection and recognition of regular 2D shapes in low noise\nenvironments. The work has a large number of direct applications in the real\nworld. The algorithm proposed is based on locating the edges and thus, in turn\ncalculating the area of the object helps in identification of a specified\nshape. The results were simulated using MATLAB tool are encouraging and\nvalidate the proposed algorithm.\n  Index Terms: Edge Detection, Area Calculation, Shape Detection, Object\nRecognition\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:09:08 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Kumar", "Vivek", ""], ["Pandey", "Sumit", ""], ["Pal", "Amrindra", ""], ["Sharma", "Sandeep", ""]]}, {"id": "1604.02032", "submitter": "Pasquale Coscia", "authors": "Pasquale Coscia, Francesco A.N. Palmieri, Francesco Castaldo, Alberto\n  Cavallo", "title": "3-D Hand Pose Estimation from Kinect's Point Cloud Using Appearance\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel appearance-based approach for pose estimation of a human\nhand using the point clouds provided by the low-cost Microsoft Kinect sensor.\nBoth the free-hand case, in which the hand is isolated from the surrounding\nenvironment, and the hand-object case, in which the different types of\ninteractions are classified, have been considered. The hand-object case is\nclearly the most challenging task having to deal with multiple tracks. The\napproach proposed here belongs to the class of partial pose estimation where\nthe estimated pose in a frame is used for the initialization of the next one.\nThe pose estimation is obtained by applying a modified version of the Iterative\nClosest Point (ICP) algorithm to synthetic models to obtain the rigid\ntransformation that aligns each model with respect to the input data. The\nproposed framework uses a \"pure\" point cloud as provided by the Kinect sensor\nwithout any other information such as RGB values or normal vector components.\nFor this reason, the proposed method can also be applied to data obtained from\nother types of depth sensor, or RGB-D camera.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:16:17 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Coscia", "Pasquale", ""], ["Palmieri", "Francesco A. N.", ""], ["Castaldo", "Francesco", ""], ["Cavallo", "Alberto", ""]]}, {"id": "1604.02085", "submitter": "Daniel Heger", "authors": "Daniel Heger and Katharina Krischer", "title": "A robust autoassociative memory with coupled networks of Kuramoto-type\n  oscillators", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.94.022309", "report-no": null, "categories": "nlin.AO cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain recognition success, unfavorable scaling of connection complexity\nor dependence on complex external input impair the usefulness of current\noscillatory neural networks for pattern recognition or restrict technical\nrealizations to small networks. We propose a new network architecture of\ncoupled oscillators for pattern recognition which shows none of the mentioned\naws. Furthermore we illustrate the recognition process with simulation results\nand analyze the new dynamics analytically: Possible output patterns are\nisolated attractors of the system. Additionally, simple criteria for\nrecognition success are derived from a lower bound on the basins of attraction.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 17:31:13 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 06:10:29 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Heger", "Daniel", ""], ["Krischer", "Katharina", ""]]}, {"id": "1604.02115", "submitter": "Suriya Singh", "authors": "Suriya Singh, Chetan Arora, C. V. Jawahar", "title": "Trajectory Aligned Features For First Person Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Egocentric videos are characterised by their ability to have the first person\nview. With the popularity of Google Glass and GoPro, use of egocentric videos\nis on the rise. Recognizing action of the wearer from egocentric videos is an\nimportant problem. Unstructured movement of the camera due to natural head\nmotion of the wearer causes sharp changes in the visual field of the egocentric\ncamera causing many standard third person action recognition techniques to\nperform poorly on such videos. Objects present in the scene and hand gestures\nof the wearer are the most important cues for first person action recognition\nbut are difficult to segment and recognize in an egocentric video. We propose a\nnovel representation of the first person actions derived from feature\ntrajectories. The features are simple to compute using standard point tracking\nand does not assume segmentation of hand/objects or recognizing object or hand\npose unlike in many previous approaches. We train a bag of words classifier\nwith the proposed features and report a performance improvement of more than\n11% on publicly available datasets. Although not designed for the particular\ncase, we show that our technique can also recognize wearer's actions when hands\nor objects are not visible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:09:07 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Singh", "Suriya", ""], ["Arora", "Chetan", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1604.02125", "submitter": "Gordon Christie", "authors": "Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol,\n  Yash Goyal, Kevin Kochersberger, Dhruv Batra", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation &\n  Prepositional Attachment Resolution in Captioned Scenes", "comments": "*The first two authors contributed equally. Conference on Empirical\n  Methods in Natural Language Processing (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to simultaneously perform semantic segmentation and\nprepositional phrase attachment resolution for captioned images. Some\nambiguities in language cannot be resolved without simultaneously reasoning\nabout an associated image. If we consider the sentence \"I shot an elephant in\nmy pajamas\", looking at language alone (and not using common sense), it is\nunclear if it is the person or the elephant wearing the pajamas or both. Our\napproach produces a diverse set of plausible hypotheses for both semantic\nsegmentation and prepositional phrase attachment resolution that are then\njointly reranked to select the most consistent pair. We show that our semantic\nsegmentation and prepositional phrase attachment resolution modules have\ncomplementary strengths, and that joint reasoning produces more accurate\nresults than any module operating in isolation. Multiple hypotheses are also\nshown to be crucial to improved multiple-module reasoning. Our vision and\nlanguage approach significantly outperforms the Stanford Parser (De Marneffe et\nal., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two\ndifferent experiments. We also make small improvements over DeepLab-CRF (Chen\net al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:26:56 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 19:05:27 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 01:47:35 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 04:08:19 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Christie", "Gordon", ""], ["Laddha", "Ankit", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Goyal", "Yash", ""], ["Kochersberger", "Kevin", ""], ["Batra", "Dhruv", ""]]}, {"id": "1604.02129", "submitter": "Scott Workman", "authors": "Scott Workman, Menghua Zhai, Nathan Jacobs", "title": "Horizon Lines in the Wild", "comments": "British Machine Vision Conference (BMVC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The horizon line is an important contextual attribute for a wide variety of\nimage understanding tasks. As such, many methods have been proposed to estimate\nits location from a single image. These methods typically require the image to\ncontain specific cues, such as vanishing points, coplanar circles, and regular\ntextures, thus limiting their real-world applicability. We introduce a large,\nrealistic evaluation dataset, Horizon Lines in the Wild (HLW), containing\nnatural images with labeled horizon lines. Using this dataset, we investigate\nthe application of convolutional neural networks for directly estimating the\nhorizon line, without requiring any explicit geometric constraints or other\nspecial cues. An extensive evaluation shows that using our CNNs, either in\nisolation or in conjunction with a previous geometric approach, we achieve\nstate-of-the-art results on the challenging HLW dataset and two existing\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:38:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 18:48:57 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Workman", "Scott", ""], ["Zhai", "Menghua", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1604.02135", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam\n  Gross, Soumith Chintala, Piotr Doll\\'ar", "title": "A MultiPath Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent COCO object detection dataset presents several new challenges for\nobject detection. In particular, it contains objects at a broad range of\nscales, less prototypical images, and requires more precise localization. To\naddress these challenges, we test three modifications to the standard Fast\nR-CNN object detector: (1) skip connections that give the detector access to\nfeatures at multiple network layers, (2) a foveal structure to exploit object\ncontext at multiple object resolutions, and (3) an integral loss function and\ncorresponding network adjustment that improve localization. The result of these\nmodifications is that information can flow along multiple paths in our network,\nincluding through features from multiple network layers and from multiple\nobject views. We refer to our modified classifier as a \"MultiPath\" network. We\ncouple our MultiPath network with DeepMask object proposals, which are well\nsuited for localization and small objects, and adapt our pipeline to predict\nsegmentation masks in addition to bounding boxes. The combined system improves\nresults over the baseline Fast R-CNN detector with Selective Search by 66%\noverall and by 4x on small objects. It placed second in both the COCO 2015\ndetection and segmentation challenges.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:43:47 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 13:29:02 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Lerer", "Adam", ""], ["Lin", "Tsung-Yi", ""], ["Pinheiro", "Pedro O.", ""], ["Gross", "Sam", ""], ["Chintala", "Soumith", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1604.02153", "submitter": "Andreas Mang", "authors": "Andreas Mang and George Biros", "title": "A Semi-Lagrangian two-level preconditioned Newton-Krylov solver for\n  constrained diffeomorphic image registration", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, 39(6):B1064-B1101, 2017", "doi": "10.1137/16M1070475", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient numerical algorithm for the solution of diffeomorphic\nimage registration problems. We use a variational formulation constrained by a\npartial differential equation (PDE), where the constraints are a scalar\ntransport equation.\n  We use a pseudospectral discretization in space and second-order accurate\nsemi-Lagrangian time stepping scheme for the transport equations. We solve for\na stationary velocity field using a preconditioned, globalized, matrix-free\nNewton-Krylov scheme. We propose and test a two-level Hessian preconditioner.\nWe consider two strategies for inverting the preconditioner on the coarse grid:\na nested preconditioned conjugate gradient method (exact solve) and a nested\nChebyshev iterative method (inexact solve) with a fixed number of iterations.\n  We test the performance of our solver in different synthetic and real-world\ntwo-dimensional application scenarios. We study grid convergence and\ncomputational efficiency of our new scheme. We compare the performance of our\nsolver against our initial implementation that uses the same spatial\ndiscretization but a standard, explicit, second-order Runge-Kutta scheme for\nthe numerical time integration of the transport equations and a single-level\npreconditioner. Our improved scheme delivers significant speedups over our\noriginal implementation. As a highlight, we observe a 20$\\times$ speedup for a\ntwo dimensional, real world multi-subject medical image registration problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 20:00:11 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 20:16:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Mang", "Andreas", ""], ["Biros", "George", ""]]}, {"id": "1604.02182", "submitter": "Joseph Robinson", "authors": "Joseph P. Robinson, Ming Shao, Yue Wu, Yun Fu", "title": "Families in the Wild (FIW): Large-Scale Kinship Image Database and\n  Benchmarks", "comments": null, "journal-ref": "ACM MM (2016) 242-246", "doi": "10.1145/2964284.2967219", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the largest kinship recognition dataset to date, Families in the\nWild (FIW). Motivated by the lack of a single, unified dataset for kinship\nrecognition, we aim to provide a dataset that captivates the interest of the\nresearch community. With only a small team, we were able to collect, organize,\nand label over 10,000 family photos of 1,000 families with our annotation tool\ndesigned to mark complex hierarchical relationships and local label information\nin a quick and efficient manner. We include several benchmarks for two\nimage-based tasks, kinship verification and family recognition. For this, we\nincorporate several visual features and metric learning methods as baselines.\nAlso, we demonstrate that a pre-trained Convolutional Neural Network (CNN) as\nan off-the-shelf feature extractor outperforms the other feature types. Then,\nresults were further boosted by fine-tuning two deep CNNs on FIW data: (1) for\nkinship verification, a triplet loss function was learned on top of the network\nof pre-trained weights; (2) for family recognition, a family-specific softmax\nclassifier was added to the network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 21:45:53 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 03:15:48 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Robinson", "Joseph P.", ""], ["Shao", "Ming", ""], ["Wu", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "1604.02245", "submitter": "Matthias Limmer", "authors": "Matthias Limmer and Hendrik P.A. Lensch", "title": "Infrared Colorization Using Deep Convolutional Neural Networks", "comments": "8 pages, 11 figures, 1 table, submitted to ICMLA2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for transferring the RGB color spectrum to\nnear-infrared (NIR) images using deep multi-scale convolutional neural\nnetworks. A direct and integrated transfer between NIR and RGB pixels is\ntrained. The trained model does not require any user guidance or a reference\nimage database in the recall phase to produce images with a natural appearance.\nTo preserve the rich details of the NIR image, its high frequency features are\ntransferred to the estimated RGB image. The presented approach is trained and\nevaluated on a real-world dataset containing a large amount of road scene\nimages in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which\nensures a perfect pixel to pixel registration.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 07:10:47 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 13:07:59 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 09:35:51 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Limmer", "Matthias", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1604.02271", "submitter": "Liang Lin", "authors": "Liang Lin, Guangrun Wang, Rui Zhang, Ruimao Zhang, Xiaodan Liang,\n  Wangmeng Zuo", "title": "Deep Structured Scene Parsing by Learning with Image Descriptions", "comments": "Discovering a semantic object hierarchy with object interaction\n  relations (Publhised in Proceedings of IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016. (oral))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a fundamental problem of scene understanding: How to\nparse the scene image into a structured configuration (i.e., a semantic object\nhierarchy with object interaction relations) that finely accords with human\nperception. We propose a deep architecture consisting of two networks: i) a\nconvolutional neural network (CNN) extracting the image representation for\npixelwise object labeling and ii) a recursive neural network (RNN) discovering\nthe hierarchical object structure and the inter-object relations. Rather than\nrelying on elaborative user annotations (e.g., manually labeling semantic maps\nand relations), we train our deep model in a weakly-supervised manner by\nleveraging the descriptive sentences of the training images. Specifically, we\ndecompose each sentence into a semantic tree consisting of nouns and verb\nphrases, and facilitate these trees discovering the configurations of the\ntraining images. Once these scene configurations are determined, then the\nparameters of both the CNN and RNN are updated accordingly by back propagation.\nThe entire model training is accomplished through an Expectation-Maximization\nmethod. Extensive experiments suggest that our model is capable of producing\nmeaningful and structured scene configurations and achieving more favorable\nscene labeling performance on PASCAL VOC 2012 over other state-of-the-art\nweakly-supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:17:37 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 03:27:10 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 02:38:51 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Guangrun", ""], ["Zhang", "Rui", ""], ["Zhang", "Ruimao", ""], ["Liang", "Xiaodan", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1604.02275", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Thomas Mensink and Barbara Caputo", "title": "Online Open World Recognition", "comments": "keywords{Open world recognition, Open set, Incremental Learning,\n  Metric Learning, Nonparametric methods, Classification confidence}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:43:15 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["De Rosa", "Rocco", ""], ["Mensink", "Thomas", ""], ["Caputo", "Barbara", ""]]}, {"id": "1604.02292", "submitter": "Dani\\\"el Pelt", "authors": "D. M. Pelt and K. J. Batenburg", "title": "A method for locally approximating regularized iterative tomographic\n  reconstruction methods", "comments": "32 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of tomography, the acquired projections are either\nlimited in number or contain a significant amount of noise. In these cases,\nstandard reconstruction methods tend to produce artifacts that can make further\nanalysis difficult. Advanced regularized iterative methods, such as total\nvariation minimization, are often able to achieve a higher reconstruction\nquality by exploiting prior knowledge about the scanned object. In practice,\nhowever, these methods often have prohibitively long computation times or large\nmemory requirements. Furthermore, since they are based on minimizing a global\nobjective function, regularized iterative methods need to reconstruct the\nentire scanned object, even when one is only interested in a (small) region of\nthe reconstructed image.\n  In this paper, we present a method to approximate regularized iterative\nreconstruction methods inside a (small) region of the scanned object. The\nmethod only performs computations inside the region of interest, ensuring low\ncomputational requirements. Reconstruction results for different phantom images\nand types of regularization are given, showing that reconstructions of the\nproposed local method are almost identical to those of the global regularized\niterative methods that are approximated, even for relatively small regions of\ninterest. Furthermore, we show that larger regions can be reconstructed\nefficiently by reconstructing several small regions in parallel and combining\nthem into a single reconstruction afterwards.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 10:19:20 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Pelt", "D. M.", ""], ["Batenburg", "K. J.", ""]]}, {"id": "1604.02316", "submitter": "Willem Sanberg", "authors": "Willem P. Sanberg, Gijs Dubbelman, Peter H.N. de With", "title": "Free-Space Detection with Self-Supervised and Online Trained Fully\n  Convolutional Networks", "comments": "version as accepted at IS&T Electronic Imaging - Autonomous Vehicles\n  and Machines Conference (San Francisco USA, January 2017); updated with two\n  additional robustness experiments and formatted in conference style; 8 pages,\n  public data available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, vision-based Advanced Driver Assist Systems have gained broad\ninterest. In this work, we investigate free-space detection, for which we\npropose to employ a Fully Convolutional Network (FCN). We show that this FCN\ncan be trained in a self-supervised manner and achieve similar results compared\nto training on manually annotated data, thereby reducing the need for large\nmanually annotated training sets. To this end, our self-supervised training\nrelies on a stereo-vision disparity system, to automatically generate (weak)\ntraining labels for the color-based FCN. Additionally, our self-supervised\ntraining facilitates online training of the FCN instead of offline.\nConsequently, given that the applied FCN is relatively small, the free-space\nanalysis becomes highly adaptive to any traffic scene that the vehicle\nencounters. We have validated our algorithm using publicly available data and\non a new challenging benchmark dataset that is released with this paper.\nExperiments show that the online training boosts performance with 5% when\ncompared to offline training, both for Fmax and AP.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 11:54:40 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 13:59:30 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Sanberg", "Willem P.", ""], ["Dubbelman", "Gijs", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1604.02354", "submitter": "Dong Wang", "authors": "Dong Wang, Xiaoyang Tan", "title": "Bayesian Neighbourhood Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good distance metric in feature space potentially improves the\nperformance of the KNN classifier and is useful in many real-world\napplications. Many metric learning algorithms are however based on the point\nestimation of a quadratic optimization problem, which is time-consuming,\nsusceptible to overfitting, and lack a natural mechanism to reason with\nparameter uncertainty, an important property useful especially when the\ntraining set is small and/or noisy. To deal with these issues, we present a\nnovel Bayesian metric learning method, called Bayesian NCA, based on the\nwell-known Neighbourhood Component Analysis method, in which the metric\nposterior is characterized by the local label consistency constraints of\nobservations, encoded with a similarity graph instead of independent pairwise\nconstraints. For efficient Bayesian optimization, we explore the variational\nlower bound over the log-likelihood of the original NCA objective. Experiments\non several publicly available datasets demonstrate that the proposed method is\nable to learn robust metric measures from small size dataset and/or from\nchallenging training set with labels contaminated by errors. The proposed\nmethod is also shown to outperform a previous pairwise constrained Bayesian\nmetric learning method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 13:35:03 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Wang", "Dong", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1604.02376", "submitter": "Jyothi Korra", "authors": "Jyothi Korra", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision, problem of identifying or classifying the objects present\nin an image is called Object Categorization. It is a challenging problem,\nespecially when the images have clutter background, occlusions or different\nlighting conditions. Many vision features have been proposed which aid object\ncategorization even in such adverse conditions. Past research has shown that,\nemploying multiple features rather than any single features leads to better\nrecognition. Multiple Kernel Learning (MKL) framework has been developed for\nlearning an optimal combination of features for object categorization. Existing\nMKL methods use linear combination of base kernels which may not be optimal for\nobject categorization. Real-world object categorization may need to consider\ncomplex combination of kernels(non-linear) and not only linear combination.\nEvolving non-linear functions of base kernels using Genetic Programming is\nproposed in this report. Experiment results show that non-kernel generated\nusing genetic programming gives good accuracy as compared to linear combination\nof kernels.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 15:33:30 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 23:16:20 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Korra", "Jyothi", ""]]}, {"id": "1604.02388", "submitter": "Yang He", "authors": "Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz", "title": "STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven\n  Pooling", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel superpixel-based multi-view convolutional neural network\nfor semantic image segmentation. The proposed network produces a high quality\nsegmentation of a single image by leveraging information from additional views\nof the same scene. Particularly in indoor videos such as captured by robotic\nplatforms or handheld and bodyworn RGBD cameras, nearby video frames provide\ndiverse viewpoints and additional context of objects and scenes. To leverage\nsuch information, we first compute region correspondences by optical flow and\nimage boundary-based superpixels. Given these region correspondences, we\npropose a novel spatio-temporal pooling layer to aggregate information over\nspace and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D\ndatasets and compare it to various state-of-the-art single-view and multi-view\napproaches. Besides a general improvement over the state-of-the-art, we also\nshow the benefits of making use of unlabeled frames during training for\nmulti-view as well as single-view prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 16:01:34 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 19:52:02 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 13:13:02 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["He", "Yang", ""], ["Chiu", "Wei-Chen", ""], ["Keuper", "Margret", ""], ["Fritz", "Mario", ""]]}, {"id": "1604.02426", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovi\\'c, Giorgos Tolias, Ond\\v{r}ej Chum", "title": "CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard\n  Examples", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in\nmany computer vision tasks. However, this achievement is preceded by extreme\nmanual annotation in order to perform either training from scratch or\nfine-tuning for the target task. In this work, we propose to fine-tune CNN for\nimage retrieval from a large collection of unordered images in a fully\nautomated manner. We employ state-of-the-art retrieval and\nStructure-from-Motion (SfM) methods to obtain 3D models, which are used to\nguide the selection of the training data for CNN fine-tuning. We show that both\nhard positive and hard negative examples enhance the final performance in\nparticular object retrieval with compact codes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 19:04:35 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 12:29:28 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 16:46:58 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Radenovi\u0107", "Filip", ""], ["Tolias", "Giorgos", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1604.02469", "submitter": "Artem Lenskiy", "authors": "Artem Lenskiy", "title": "Image segmentation of cross-country scenes captured in IR spectrum", "comments": "Corrected version of the chapter published in Advances in Robotics\n  and Virtual Reality, Volume 26 of the series Intelligent Systems Reference\n  Library pp 227-247", "journal-ref": null, "doi": "10.1007/978-3-642-23363-0_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has become a major source of information for autonomous\nnavigation of robots of various types, self-driving cars, military robots and\nmars/lunar rovers are some examples. Nevertheless, the majority of methods\nfocus on analysing images captured in visible spectrum. In this manuscript we\nelaborate on the problem of segmenting cross-country scenes captured in IR\nspectrum. For this purpose we proposed employing salient features. Salient\nfeatures are robust to variations in scale, brightness and view angle. We\nsuggest the Speeded-Up Robust Features as a basis for our salient features for\na number of reasons discussed in the paper. We also provide a comparison of two\nSURF implementations. The SURF features are extracted from images of different\nterrain types. For every feature we estimate a terrain class membership\nfunction. The membership values are obtained by means of either the multi-layer\nperceptron or nearest neighbours. The features' class membership values and\ntheir spatial positions are then applied to estimate class membership values\nfor all pixels in the image. To decrease the effect of segmentation blinking\nthat is caused by rapid switching between different terrain types and to speed\nup segmentation, we are tracking camera position and predict features'\npositions. The comparison of the multi-layer perception and the nearest\nneighbour classifiers is presented in the paper. The error rate of the terrain\nsegmentation using the nearest neighbours obtained on the testing set is\n16.6+-9.17%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:14:46 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lenskiy", "Artem", ""]]}, {"id": "1604.02477", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Cesare Alippi", "title": "One-class classifiers based on entropic spanning graphs", "comments": "Extended and revised version of the paper \"One-Class Classification\n  Through Mutual Information Minimization\" presented at the 2016 IEEE IJCNN,\n  Vancouver, Canada", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2608983", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classifiers offer valuable tools to assess the presence of outliers\nin data. In this paper, we propose a design methodology for one-class\nclassifiers based on entropic spanning graphs. Our approach takes into account\nthe possibility to process also non-numeric data by means of an embedding\nprocedure. The spanning graph is learned on the embedded input data and the\noutcoming partition of vertices defines the classifier. The final partition is\nderived by exploiting a criterion based on mutual information minimization.\nHere, we compute the mutual information by using a convenient formulation\nprovided in terms of the $\\alpha$-Jensen difference. Once training is\ncompleted, in order to associate a confidence level with the classifier\ndecision, a graph-based fuzzy model is constructed. The fuzzification process\nis based only on topological information of the vertices of the entropic\nspanning graph. As such, the proposed one-class classifier is suitable also for\ndata characterized by complex geometric structures. We provide experiments on\nwell-known benchmarks containing both feature vectors and labeled graphs. In\naddition, we apply the method to the protein solubility recognition problem by\nconsidering several representations for the input samples. Experimental results\ndemonstrate the effectiveness and versatility of the proposed method with\nrespect to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:41:54 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 15:54:05 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 17:16:33 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 15:49:01 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""]]}, {"id": "1604.02485", "submitter": "Artem Lenskiy", "authors": "Artem A. Lenskiy, Jong-Soo Lee", "title": "Machine Learning for Visual Navigation of Unmanned Ground Vehicles", "comments": "Preprint of the chapter in the Machine Learning for Visual Navigation\n  of Unmanned Ground Vehicles 2012", "journal-ref": null, "doi": "10.4018/978-1-60960-818-7.ch418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of visual information for the navigation of unmanned ground vehicles\nin a cross-country environment recently received great attention. However,\nuntil now, the use of textural information has been somewhat less effective\nthan color or laser range information. This manuscript reviews the recent\nachievements in cross-country scene segmentation and addresses their\nshortcomings. It then describes a problem related to classification of high\ndimensional texture features. Finally, it compares three machine learning\nalgorithms aimed at resolving this problem. The experimental results for each\nmachine learning algorithm with the discussion of comparisons are given at the\nend of the manuscript.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 21:04:36 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lenskiy", "Artem A.", ""], ["Lee", "Jong-Soo", ""]]}, {"id": "1604.02488", "submitter": "Victor San Martin", "authors": "Victor Manuel San Martin and Alejandra Figliola", "title": "Application of Multifractal Analysis to Segmentation of Water Bodies in\n  Optical and Synthetic Aperture Radar Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for segmenting water bodies in optical and synthetic aperture radar\n(SAR) satellite images is proposed. It makes use of the textural features of\nthe different regions in the image for segmentation. The method consists in a\nmultiscale analysis of the images, which allows us to study the images\nregularity both, locally and globally. As results of the analysis, coarse\nmultifractal spectra of studied images and a group of images that associates\neach position (pixel) with its corresponding value of local regularity (or\nsingularity) spectrum are obtained. Thresholds are then applied to the\nmultifractal spectra of the images for the classification. These thresholds are\nselected after studying the characteristics of the spectra under the assumption\nthat water bodies have larger local regularity than other soil types.\nClassifications obtained by the multifractal method are compared quantitatively\nwith those obtained by neural networks trained to classify the pixels of the\nimages in covered against uncovered by water. In optical images, the\nclassifications are also compared with those derived using the so-called\nNormalized Differential Water Index (NDWI).\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 21:24:15 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Martin", "Victor Manuel San", ""], ["Figliola", "Alejandra", ""]]}, {"id": "1604.02531", "submitter": "Liang Zheng", "authors": "Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi\n  Yang, Qi Tian", "title": "Person Re-identification in the Wild", "comments": "accepted as spotlight to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel large-scale dataset and comprehensive baselines for\nend-to-end pedestrian detection and person recognition in raw video frames. Our\nbaselines address three issues: the performance of various combinations of\ndetectors and recognizers, mechanisms for pedestrian detection to help improve\noverall re-identification accuracy and assessing the effectiveness of different\ndetectors for re-identification. We make three distinct contributions. First, a\nnew dataset, PRW, is introduced to evaluate Person Re-identification in the\nWild, using videos acquired through six synchronized cameras. It contains 932\nidentities and 11,816 frames in which pedestrians are annotated with their\nbounding box positions and identities. Extensive benchmarking results are\npresented on this dataset. Second, we show that pedestrian detection aids\nre-identification through two simple yet effective improvements: a\ndiscriminatively trained ID-discriminative Embedding (IDE) in the person\nsubspace using convolutional neural network (CNN) features and a Confidence\nWeighted Similarity (CWS) metric that incorporates detection scores into\nsimilarity measurement. Third, we derive insights in evaluating detector\nperformance for the particular scenario of accurate person re-identification.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 06:57:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 15:02:40 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Zheng", "Liang", ""], ["Zhang", "Hengheng", ""], ["Sun", "Shaoyan", ""], ["Chandraker", "Manmohan", ""], ["Yang", "Yi", ""], ["Tian", "Qi", ""]]}, {"id": "1604.02532", "submitter": "Kai Kang", "authors": "Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang, Tong Xiao,\n  Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, Wanli Ouyang", "title": "T-CNN: Tubelets with Convolutional Neural Networks for Object Detection\n  from Videos", "comments": "ImageNet 2015 VID challenge tech report. The first two authors share\n  co-first authorship. Accepted as a Transaction paper by T-CSVT Special Issue\n  on Large Scale and Nonlinear Similarity Learning for Intelligent Video\n  Analysis", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2736553", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The state-of-the-art performance for object detection has been significantly\nimproved over the past two years. Besides the introduction of powerful deep\nneural networks such as GoogleNet and VGG, novel object detection frameworks\nsuch as R-CNN and its successors, Fast R-CNN and Faster R-CNN, play an\nessential role in improving the state-of-the-art. Despite their effectiveness\non still images, those frameworks are not specifically designed for object\ndetection from videos. Temporal and contextual information of videos are not\nfully investigated and utilized. In this work, we propose a deep learning\nframework that incorporates temporal and contextual information from tubelets\nobtained in videos, which dramatically improves the baseline performance of\nexisting still-image detection frameworks when they are applied to videos. It\nis called T-CNN, i.e. tubelets with convolutional neueral networks. The\nproposed framework won the recently introduced object-detection-from-video\n(VID) task with provided data in the ImageNet Large-Scale Visual Recognition\nChallenge 2015 (ILSVRC2015).\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 07:07:47 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 13:35:10 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 03:12:53 GMT"}, {"version": "v4", "created": "Thu, 3 Aug 2017 07:03:49 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Kang", "Kai", ""], ["Li", "Hongsheng", ""], ["Yan", "Junjie", ""], ["Zeng", "Xingyu", ""], ["Yang", "Bin", ""], ["Xiao", "Tong", ""], ["Zhang", "Cong", ""], ["Wang", "Zhe", ""], ["Wang", "Ruohui", ""], ["Wang", "Xiaogang", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1604.02546", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana and Rita Cucchiara", "title": "Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic\n  Deep Features", "comments": "ICMR 2016", "journal-ref": null, "doi": "10.1145/2911996.2912012", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel retrieval pipeline for video collections, which\naims to retrieve the most significant parts of an edited video for a given\nquery, and represent them with thumbnails which are at the same time\nsemantically meaningful and aesthetically remarkable. Videos are first\nsegmented into coherent and story-telling scenes, then a retrieval algorithm\nbased on deep learning is proposed to retrieve the most significant scenes for\na textual query. A ranking strategy based on deep features is finally used to\ntackle the problem of visualizing the best thumbnail. Qualitative and\nquantitative experiments are conducted on a collection of edited videos to\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 09:41:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1604.02619", "submitter": "Lluis Gomez", "authors": "Lluis Gomez-Bigorda and Dimosthenis Karatzas", "title": "TextProposals: a Text-specific Selective Search Algorithm for Word\n  Spotting in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the success of powerful while expensive techniques to recognize\nwords in a holistic way, object proposals techniques emerge as an alternative\nto the traditional text detectors. In this paper we introduce a novel object\nproposals method that is specifically designed for text. We rely on a\nsimilarity based region grouping algorithm that generates a hierarchy of word\nhypotheses. Over the nodes of this hierarchy it is possible to apply a holistic\nword recognition method in an efficient way.\n  Our experiments demonstrate that the presented method is superior in its\nability of producing good quality word proposals when compared with\nclass-independent algorithms. We show impressive recall rates with a few\nthousand proposals in different standard benchmarks, including focused or\nincidental text datasets, and multi-language scenarios. Moreover, the\ncombination of our object proposals with existing whole-word recognizers shows\ncompetitive performance in end-to-end word spotting, and, in some benchmarks,\noutperforms previously published results. Concretely, in the challenging\nICDAR2015 Incidental Text dataset, we overcome in more than 10 percent f-score\nthe best-performing method in the last ICDAR Robust Reading Competition. Source\ncode of the complete end-to-end system is available at\nhttps://github.com/lluisgomez/TextProposals\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 00:03:16 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 17:03:13 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 15:35:28 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Gomez-Bigorda", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1604.02646", "submitter": "Biswajit Paria", "authors": "Biswajit Paria, Vikas Reddy, Anirban Santara, Pabitra Mitra", "title": "Visualization Regularizers for Neural Network based Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks is mostly due their ability to learn\nmeaningful features from the data. Features learned in the hidden layers of\ndeep neural networks trained in computer vision tasks have been shown to be\nsimilar to mid-level vision features. We leverage this fact in this work and\npropose the visualization regularizer for image tasks. The proposed\nregularization technique enforces smoothness of the features learned by hidden\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\nhigher classification accuracy as compared to existing regularizers such as the\nL2 norm regularizer and dropout, on benchmark datasets without changing the\ntraining computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 07:02:40 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 14:38:38 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 10:07:22 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Paria", "Biswajit", ""], ["Reddy", "Vikas", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1604.02647", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Tianye Li, Hao Li", "title": "Real-Time Facial Segmentation and Performance Capture from RGB Input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of unconstrained real-time 3D facial performance\ncapture through explicit semantic segmentation in the RGB input. To ensure\nrobustness, cutting edge supervised learning approaches rely on large training\ndatasets of face images captured in the wild. While impressive tracking quality\nhas been demonstrated for faces that are largely visible, any occlusion due to\nhair, accessories, or hand-to-face gestures would result in significant visual\nartifacts and loss of tracking accuracy. The modeling of occlusions has been\nmostly avoided due to its immense space of appearance variability. To address\nthis curse of high dimensionality, we perform tracking in unconstrained images\nassuming non-face regions can be fully masked out. Along with recent\nbreakthroughs in deep learning, we demonstrate that pixel-level facial\nsegmentation is possible in real-time by repurposing convolutional neural\nnetworks designed originally for general semantic segmentation. We develop an\nefficient architecture based on a two-stream deconvolution network with\ncomplementary characteristics, and introduce carefully designed training\nsamples and data augmentation strategies for improved segmentation accuracy and\nrobustness. We adopt a state-of-the-art regression-based facial tracking\nframework with segmented face images as training, and demonstrate accurate and\nuninterrupted facial performance capture in the presence of extreme occlusion\nand even side views. Furthermore, the resulting segmentation can be directly\nused to composite partial 3D face models on the input images and enable\nseamless facial manipulation tasks, such as virtual make-up or face\nreplacement.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 07:04:47 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Saito", "Shunsuke", ""], ["Li", "Tianye", ""], ["Li", "Hao", ""]]}, {"id": "1604.02657", "submitter": "Chengde Wan Mr", "authors": "Chengde Wan, Angela Yao, Luc Van Gool", "title": "Direction matters: hand pose estimation from local surface normals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical regression framework for estimating hand joint\npositions from single depth images based on local surface normals. The\nhierarchical regression follows the tree structured topology of hand from wrist\nto finger tips. We propose a conditional regression forest, i.e., the Frame\nConditioned Regression Forest (FCRF) which uses a new normal difference\nfeature. At each stage of the regression, the frame of reference is established\nfrom either the local surface normal or previously estimated hand joints. By\nmaking the regression with respect to the local frame, the pose estimation is\nmore robust to rigid transformations. We also introduce a new efficient\napproximation to estimate surface normals. We verify the effectiveness of our\nmethod by conducting experiments on two challenging real-world datasets and\nshow consistent improvements over previous discriminative pose estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 09:16:28 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wan", "Chengde", ""], ["Yao", "Angela", ""], ["Van Gool", "Luc", ""]]}, {"id": "1604.02677", "submitter": "Hao Chen", "authors": "Hao Chen, Xiaojuan Qi, Lequan Yu, Pheng-Ann Heng", "title": "DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphology of glands has been used routinely by pathologists to assess\nthe malignancy degree of adenocarcinomas. Accurate segmentation of glands from\nhistology images is a crucial step to obtain reliable morphological statistics\nfor quantitative diagnosis. In this paper, we proposed an efficient deep\ncontour-aware network (DCAN) to solve this challenging problem under a unified\nmulti-task learning framework. In the proposed network, multi-level contextual\nfeatures from the hierarchical architecture are explored with auxiliary\nsupervision for accurate gland segmentation. When incorporated with multi-task\nregularization during the training, the discriminative capability of\nintermediate features can be further improved. Moreover, our network can not\nonly output accurate probability maps of glands, but also depict clear contours\nsimultaneously for separating clustered objects, which further boosts the gland\nsegmentation performance. This unified framework can be efficient when applied\nto large-scale histopathological data without resorting to additional steps to\ngenerate contours based on low-level cues for post-separating. Our method won\nthe 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams,\nsurpassing all the other methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 12:12:24 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Chen", "Hao", ""], ["Qi", "Xiaojuan", ""], ["Yu", "Lequan", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1604.02703", "submitter": "Wenzheng Chen", "authors": "Wenzheng Chen and Huan Wang and Yangyan Li and Hao Su and Zhenhua Wang\n  and Changhe Tu and Dani Lischinski and Daniel Cohen-Or and Baoquan Chen", "title": "Synthesizing Training Images for Boosting Human 3D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human 3D pose estimation from a single image is a challenging task with\nnumerous applications. Convolutional Neural Networks (CNNs) have recently\nachieved superior performance on the task of 2D pose estimation from a single\nimage, by training on images with 2D annotations collected by crowd sourcing.\nThis suggests that similar success could be achieved for direct estimation of\n3D poses. However, 3D poses are much harder to annotate, and the lack of\nsuitable annotated training images hinders attempts towards end-to-end\nsolutions. To address this issue, we opt to automatically synthesize training\nimages with ground truth pose annotations. Our work is a systematic study along\nthis road. We find that pose space coverage and texture diversity are the key\ningredients for the effectiveness of synthetic training data. We present a\nfully automatic, scalable approach that samples the human pose space for\nguiding the synthesis procedure and extracts clothing textures from real\nimages. Furthermore, we explore domain adaptation for bridging the gap between\nour synthetic training images and real testing photos. We demonstrate that CNNs\ntrained with our synthetic images out-perform those trained with real photos on\n3D pose estimation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 15:22:04 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 09:17:21 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2016 02:00:51 GMT"}, {"version": "v4", "created": "Fri, 16 Sep 2016 12:57:01 GMT"}, {"version": "v5", "created": "Thu, 29 Sep 2016 15:01:38 GMT"}, {"version": "v6", "created": "Thu, 5 Jan 2017 08:09:00 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Chen", "Wenzheng", ""], ["Wang", "Huan", ""], ["Li", "Yangyan", ""], ["Su", "Hao", ""], ["Wang", "Zhenhua", ""], ["Tu", "Changhe", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "1604.02715", "submitter": "Namdar Homayounfar", "authors": "Namdar Homayounfar, Sanja Fidler, Raquel Urtasun", "title": "Soccer Field Localization from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel way of efficiently localizing a soccer field\nfrom a single broadcast image of the game. Related work in this area relies on\nmanually annotating a few key frames and extending the localization to similar\nimages, or installing fixed specialized cameras in the stadium from which the\nlayout of the field can be obtained. In contrast, we formulate this problem as\na branch and bound inference in a Markov random field where an energy function\nis defined in terms of field cues such as grass, lines and circles. Moreover,\nour approach is fully automatic and depends only on single images from the\nbroadcast video of the game. We demonstrate the effectiveness of our method by\napplying it to various games and obtain promising results. Finally, we posit\nthat our approach can be applied easily to other sports such as hockey and\nbasketball.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 17:09:55 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Homayounfar", "Namdar", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1604.02748", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry\n  Goldberg, Alejandro Jaimes, Jiebo Luo", "title": "TGIF: A New Dataset and Benchmark on Animated GIF Description", "comments": "CVPR 2016 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent popularity of animated GIFs on social media, there is need\nfor ways to index them with rich metadata. To advance research on animated GIF\nunderstanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K\nanimated GIFs from Tumblr and 120K natural language descriptions obtained via\ncrowdsourcing. The motivation for this work is to develop a testbed for image\nsequence description systems, where the task is to generate natural language\ndescriptions for animated GIFs or video clips. To ensure a high quality\ndataset, we developed a series of novel quality controls to validate free-form\ntext input from crowdworkers. We show that there is unambiguous association\nbetween visual content and natural language descriptions in our dataset, making\nit an ideal benchmark for the visual content captioning task. We perform\nextensive statistical analyses to compare our dataset to existing image and\nvideo description datasets. Next, we provide baseline results on the animated\nGIF description task, using three representative techniques: nearest neighbor,\nstatistical machine translation, and recurrent neural networks. Finally, we\nshow that models fine-tuned from our animated GIF description dataset can be\nhelpful for automatic movie description.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 22:15:14 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 01:47:19 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Li", "Yuncheng", ""], ["Song", "Yale", ""], ["Cao", "Liangliang", ""], ["Tetreault", "Joel", ""], ["Goldberg", "Larry", ""], ["Jaimes", "Alejandro", ""], ["Luo", "Jiebo", ""]]}, {"id": "1604.02801", "submitter": "Ruizhe Wang", "authors": "Ruizhe Wang, Lingyu Wei, Etienne Vouga, Qixing Huang, Duygu Ceylan,\n  Gerard Medioni and Hao Li", "title": "Capturing Dynamic Textured Surfaces of Moving Targets", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end system for reconstructing complete watertight and\ntextured models of moving subjects such as clothed humans and animals, using\nonly three or four handheld sensors. The heart of our framework is a new\npairwise registration algorithm that minimizes, using a particle swarm\nstrategy, an alignment error metric based on mutual visibility and occlusion.\nWe show that this algorithm reliably registers partial scans with as little as\n15% overlap without requiring any initial correspondences, and outperforms\nalternative global registration algorithms. This registration algorithm allows\nus to reconstruct moving subjects from free-viewpoint video produced by\nconsumer-grade sensors, without extensive sensor calibration, constrained\ncapture volume, expensive arrays of cameras, or templates of the subject\ngeometry.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 06:03:09 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wang", "Ruizhe", ""], ["Wei", "Lingyu", ""], ["Vouga", "Etienne", ""], ["Huang", "Qixing", ""], ["Ceylan", "Duygu", ""], ["Medioni", "Gerard", ""], ["Li", "Hao", ""]]}, {"id": "1604.02808", "submitter": "Amir Shahroudy", "authors": "Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang", "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches in depth-based human activity analysis achieved outstanding\nperformance and proved the effectiveness of 3D representation for\nclassification of action classes. Currently available depth-based and\nRGB+D-based action recognition benchmarks have a number of limitations,\nincluding the lack of training samples, distinct class labels, camera views and\nvariety of subjects. In this paper we introduce a large-scale dataset for RGB+D\nhuman action recognition with more than 56 thousand video samples and 4 million\nframes, collected from 40 distinct subjects. Our dataset contains 60 different\naction classes including daily, mutual, and health-related actions. In\naddition, we propose a new recurrent neural network structure to model the\nlong-term temporal correlation of the features for each body part, and utilize\nthem for better action classification. Experimental results show the advantages\nof applying deep learning methods over state-of-the-art hand-crafted features\non the suggested cross-subject and cross-view evaluation criteria for our\ndataset. The introduction of this large scale dataset will enable the community\nto apply, develop and adapt various data-hungry learning techniques for the\ntask of depth-based and RGB+D-based human activity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 06:44:53 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Shahroudy", "Amir", ""], ["Liu", "Jun", ""], ["Ng", "Tian-Tsong", ""], ["Wang", "Gang", ""]]}, {"id": "1604.02815", "submitter": "Dan Rosenbaum", "authors": "Dan Rosenbaum and Yair Weiss", "title": "Beyond Brightness Constancy: Learning Noise Models for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow is typically estimated by minimizing a \"data cost\" and an\noptional regularizer. While there has been much work on different regularizers\nmany modern algorithms still use a data cost that is not very different from\nthe ones used over 30 years ago: a robust version of brightness constancy or\ngradient constancy. In this paper we leverage the recent availability of\nground-truth optical flow databases in order to learn a data cost. Specifically\nwe take a generative approach in which the data cost models the distribution of\nnoise after warping an image according to the flow and we measure the\n\"goodness\" of a data cost by how well it matches the true distribution of flow\nwarp error. Consistent with current practice, we find that robust versions of\ngradient constancy are better models than simple brightness constancy but a\nlearned GMM that models the density of patches of warp error gives a much\nbetter fit than any existing assumption of constancy. This significant\nadvantage of the GMM is due to an explicit modeling of the spatial structure of\nwarp errors, a feature which is missing from almost all existing data costs in\noptical flow. Finally, we show how a good density model of warp error patches\ncan be used for optical flow estimation on whole images. We replace the data\ncost by the expected patch log-likelihood (EPLL), and show how this cost can be\noptimized iteratively using an additional step of denoising the warp error\nimage. The results of our experiments are promising and show that patch models\nwith higher likelihood lead to better optical flow estimation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 07:23:44 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Rosenbaum", "Dan", ""], ["Weiss", "Yair", ""]]}, {"id": "1604.02855", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo and\n  Nicol\\`o Cesa-Bianchi", "title": "Active Learning for Online Recognition of Human Activities from\n  Streaming Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising human activities from streaming videos poses unique challenges to\nlearning algorithms: predictive models need to be scalable, incrementally\ntrainable, and must remain bounded in size even when the data stream is\narbitrarily long. Furthermore, as parameter tuning is problematic in a\nstreaming setting, suitable approaches should be parameterless, and make no\nassumptions on what class labels may occur in the stream. We present here an\napproach to the recognition of human actions from streaming data which meets\nall these requirements by: (1) incrementally learning a model which adaptively\ncovers the feature space with simple local classifiers; (2) employing an active\nlearning strategy to reduce annotation requests; (3) achieving promising\naccuracy within a fixed model size. Extensive experiments on standard\nbenchmarks show that our approach is competitive with state-of-the-art\nnon-incremental methods, and outperforms the existing active incremental\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 09:32:51 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["De Rosa", "Rocco", ""], ["Gori", "Ilaria", ""], ["Cuzzolin", "Fabio", ""], ["Caputo", "Barbara", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1604.02878", "submitter": "Kaipeng Zhang", "authors": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao", "title": "Joint Face Detection and Alignment using Multi-task Cascaded\n  Convolutional Networks", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2603342", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and alignment in unconstrained environment are challenging due\nto various poses, illuminations and occlusions. Recent studies show that deep\nlearning approaches can achieve impressive performance on these two tasks. In\nthis paper, we propose a deep cascaded multi-task framework which exploits the\ninherent correlation between them to boost up their performance. In particular,\nour framework adopts a cascaded structure with three stages of carefully\ndesigned deep convolutional networks that predict face and landmark location in\na coarse-to-fine manner. In addition, in the learning process, we propose a new\nonline hard sample mining strategy that can improve the performance\nautomatically without manual sample selection. Our method achieves superior\naccuracy over the state-of-the-art techniques on the challenging FDDB and WIDER\nFACE benchmark for face detection, and AFLW benchmark for face alignment, while\nkeeps real time performance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 10:47:14 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Zhang", "Kaipeng", ""], ["Zhang", "Zhanpeng", ""], ["Li", "Zhifeng", ""], ["Qiao", "Yu", ""]]}, {"id": "1604.02885", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Christian Haene, Lubor Ladicky and Marc Pollefeys", "title": "Semantic 3D Reconstruction with Continuous Regularization and Ray\n  Potentials Using a Visibility Consistency Constraint", "comments": "Accepted as a spotlight oral paper by CVPR 2016. Code at\n  https://github.com/nsavinov/ray_potentials/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for dense semantic 3D reconstruction which uses a data\nterm that is defined as potentials over viewing rays, combined with continuous\nsurface area penalization. Our formulation is a convex relaxation which we\naugment with a crucial non-convex constraint that ensures exact handling of\nvisibility. To tackle the non-convex minimization problem, we propose a\nmajorize-minimize type strategy which converges to a critical point. We\ndemonstrate the benefits of using the non-convex constraint experimentally. For\nthe geometry-only case, we set a new state of the art on two datasets of the\ncommonly used Middlebury multi-view stereo benchmark. Moreover, our\ngeneral-purpose formulation directly reconstructs thin objects, which are\nusually treated with specialized algorithms. A qualitative evaluation on the\ndense semantic 3D reconstruction task shows that we improve significantly over\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 11:12:24 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 17:27:19 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 14:53:16 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Savinov", "Nikolay", ""], ["Haene", "Christian", ""], ["Ladicky", "Lubor", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1604.02898", "submitter": "Jubin Johnson", "authors": "Jubin Johnson, Ehsan Shahrian Varnousfaderani, Hisham Cholakkal, and\n  Deepu Rajan", "title": "Sparse Coding for Alpha Matting", "comments": "To appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2555705", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing color sampling based alpha matting methods use the compositing\nequation to estimate alpha at a pixel from pairs of foreground (F) and\nbackground (B) samples. The quality of the matte depends on the selected (F,B)\npairs. In this paper, the matting problem is reinterpreted as a sparse coding\nof pixel features, wherein the sum of the codes gives the estimate of the alpha\nmatte from a set of unpaired F and B samples. A non-parametric probabilistic\nsegmentation provides a certainty measure on the pixel belonging to foreground\nor background, based on which a dictionary is formed for use in sparse coding.\nBy removing the restriction to conform to (F,B) pairs, this method allows for\nbetter alpha estimation from multiple F and B samples. The same framework is\nextended to videos, where the requirement of temporal coherence is handled\neffectively. Here, the dictionary is formed by samples from multiple frames. A\nmulti-frame graph model, as opposed to a single image as for image matting, is\nproposed that can be solved efficiently in closed form. Quantitative and\nqualitative evaluations on a benchmark dataset are provided to show that the\nproposed method outperforms current state-of-the-art in image and video\nmatting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 11:48:18 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Johnson", "Jubin", ""], ["Varnousfaderani", "Ehsan Shahrian", ""], ["Cholakkal", "Hisham", ""], ["Rajan", "Deepu", ""]]}, {"id": "1604.02902", "submitter": "Dan Rosenbaum", "authors": "Dan Rosenbaum and Yair Weiss", "title": "Statistics of RGBD Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras that can measure the depth of each pixel in addition to its color\nhave become easily available and are used in many consumer products worldwide.\nOften the depth channel is captured at lower quality compared to the RGB\nchannels and different algorithms have been proposed to improve the quality of\nthe D channel given the RGB channels. Typically these approaches work by\nassuming that edges in RGB are correlated with edges in D.\n  In this paper we approach this problem from the standpoint of natural image\nstatistics. We obtain examples of high quality RGBD images from a computer\ngraphics generated movie (MPI-Sintel) and we use these examples to compare\ndifferent probabilistic generative models of RGBD image patches. We then use\nthe generative models together with a degradation model and obtain a Bayes\nLeast Squares (BLS) estimator of the D channel given the RGB channels. Our\nresults show that learned generative models outperform the state-of-the-art in\nimproving the quality of depth channels given the color channels in natural\nimages even when training is performed on artificially generated images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 12:00:57 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Rosenbaum", "Dan", ""], ["Weiss", "Yair", ""]]}, {"id": "1604.02917", "submitter": "Stefanos Eleftheriadis", "authors": "Stefanos Eleftheriadis and Ognjen Rudovic and Marc P. Deisenroth and\n  Maja Pantic", "title": "Gaussian Process Domain Experts for Model Adaptation in Facial Behavior\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for supervised domain adaptation that is based\nupon the probabilistic framework of Gaussian processes (GPs). Specifically, we\nintroduce domain-specific GPs as local experts for facial expression\nclassification from face images. The adaptation of the classifier is\nfacilitated in probabilistic fashion by conditioning the target expert on\nmultiple source experts. Furthermore, in contrast to existing adaptation\napproaches, we also learn a target expert from available target data solely.\nThen, a single and confident classifier is obtained by combining the\npredictions from multiple experts based on their confidence. Learning of the\nmodel is efficient and requires no retraining/reweighting of the source\nclassifiers. We evaluate the proposed approach on two publicly available\ndatasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression\nclassification. To this end, we perform adaptation of two contextual factors:\n'where' (view) and 'who' (subject). We show in our experiments that the\nproposed approach consistently outperforms both source and target classifiers,\nwhile using as few as 30 target examples. It also outperforms the\nstate-of-the-art approaches for supervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 12:37:36 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 18:54:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Rudovic", "Ognjen", ""], ["Deisenroth", "Marc P.", ""], ["Pantic", "Maja", ""]]}, {"id": "1604.02946", "submitter": "David Dov", "authors": "David Dov, Ronen Talmon and Israel Cohen", "title": "Kernel-based Sensor Fusion with Application to Audio-Visual Voice\n  Activity Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2605068", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of multiple view data fusion in the\npresence of noise and interferences. Recent studies have approached this\nproblem using kernel methods, by relying particularly on a product of kernels\nconstructed separately for each view. From a graph theory point of view, we\nanalyze this fusion approach in a discrete setting. More specifically, based on\na statistical model for the connectivity between data points, we propose an\nalgorithm for the selection of the kernel bandwidth, a parameter, which, as we\nshow, has important implications on the robustness of this fusion approach to\ninterferences. Then, we consider the fusion of audio-visual speech signals\nmeasured by a single microphone and by a video camera pointed to the face of\nthe speaker. Specifically, we address the task of voice activity detection,\ni.e., the detection of speech and non-speech segments, in the presence of\nstructured interferences such as keyboard taps and office noise. We propose an\nalgorithm for voice activity detection based on the audio-visual signal.\nSimulation results show that the proposed algorithm outperforms competing\nfusion and voice activity detection approaches. In addition, we demonstrate\nthat a proper selection of the kernel bandwidth indeed leads to improved\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 13:33:43 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Dov", "David", ""], ["Talmon", "Ronen", ""], ["Cohen", "Israel", ""]]}, {"id": "1604.02975", "submitter": "Binod Bhattarai", "authors": "Binod Bhattarai, Gaurav Sharma, Frederic Jurie", "title": "CP-mtML: Coupled Projection multi-task Metric Learning for Large Scale\n  Face Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 14:30:38 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Bhattarai", "Binod", ""], ["Sharma", "Gaurav", ""], ["Jurie", "Frederic", ""]]}, {"id": "1604.03010", "submitter": "Xin Du", "authors": "Xin Du", "title": "Semi-supervised learning of local structured output predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of semi-supervised structured output\nprediction, which aims to learn predictors for structured outputs, such as\nsequences, tree nodes, vectors, etc., from a set of data points of both\ninput-output pairs and single inputs without outputs. The traditional methods\nto solve this problem usually learns one single predictor for all the data\npoints, and ignores the variety of the different data points. Different parts\nof the data set may have different local distributions, and requires different\noptimal local predictors. To overcome this disadvantage of existing methods, we\npropose to learn different local predictors for neighborhoods of different data\npoints, and the missing structured outputs simultaneously. In the neighborhood\nof each data point, we proposed to learn a linear predictor by minimizing both\nthe complexity of the predictor and the upper bound of the structured\nprediction loss. The minimization is conducted by gradient descent algorithms.\nExperiments over four benchmark data sets, including DDSM mammography medical\nimages, SUN natural image data set, Cora research paper data set, and Spanish\nnews wire article sentence data set, show the advantages of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:53:04 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Du", "Xin", ""]]}, {"id": "1604.03058", "submitter": "Xundong Wu", "authors": "Xundong Wu, Yong Wu and Yong Zhao", "title": "Binarized Neural Networks on the ImageNet Classification Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet\nILSVRC-2102 dataset classification task and achieved a good performance. With a\nmoderate size network of 13 layers, we obtained top-5 classification accuracy\nrate of 84.1 % on validation set through network distillation, much better than\nprevious published results of 73.2% on XNOR network and 69.1% on binarized\nGoogleNET. We expect networks of better performance can be obtained by\nfollowing our current strategies. We provide a detailed discussion and\npreliminary analysis on strategies used in the network training.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 18:39:33 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 03:22:37 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 15:25:06 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 00:38:03 GMT"}, {"version": "v5", "created": "Sat, 19 Nov 2016 01:37:40 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wu", "Xundong", ""], ["Wu", "Yong", ""], ["Zhao", "Yong", ""]]}, {"id": "1604.03073", "submitter": "Ashley Prater", "authors": "Ashley Prater", "title": "Reservoir computing for spatiotemporal signal classification without\n  trained output weights", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a recently introduced machine learning paradigm that\nhas been shown to be well-suited for the processing of spatiotemporal data.\nRather than training the network node connections and weights via\nbackpropagation in traditional recurrent neural networks, reservoirs instead\nhave fixed connections and weights among the `hidden layer' nodes, and\ntraditionally only the weights to the output layer of neurons are trained using\nlinear regression. We claim that for signal classification tasks one may forgo\nthe weight training step entirely and instead use a simple supervised\nclustering method based upon principal components of norms of reservoir states.\nThe proposed method is mathematically analyzed and explored through numerical\nexperiments on real-world data. The examples demonstrate that the proposed may\noutperform the traditional trained output weight approach in terms of\nclassification accuracy and sensitivity to reservoir parameters.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 19:14:05 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 13:28:17 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Prater", "Ashley", ""]]}, {"id": "1604.03075", "submitter": "Gary Huang", "authors": "Gary B. Huang, Louis K. Scheffer, Stephen M. Plaza", "title": "Fully-Automatic Synapse Prediction and Validation on a Large Data Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting a connectome from an electron microscopy (EM) data set requires\nidentification of neurons and determination of synapses between neurons. As\nmanual extraction of this information is very time-consuming, there has been\nextensive research effort to automatically segment the neurons to help guide\nand eventually replace manual tracing. Until recently, there has been\ncomparatively less research on automatically detecting the actual synapses\nbetween neurons. This discrepancy can, in part, be attributed to several\nfactors: obtaining neuronal shapes is a prerequisite first step in extracting a\nconnectome, manual tracing is much more time-consuming than annotating\nsynapses, and neuronal contact area can be used as a proxy for synapses in\ndetermining connections.\n  However, recent research has demonstrated that contact area alone is not a\nsufficient predictor of synaptic connection. Moreover, as segmentation has\nimproved, we have observed that synapse annotation is consuming a more\nsignificant fraction of overall reconstruction time. This ratio will only get\nworse as segmentation improves, gating overall possible speed-up. Therefore, we\naddress this problem by developing algorithms that automatically detect\npre-synaptic neurons and their post-synaptic partners. In particular,\npre-synaptic structures are detected using a Deep and Wide Multiscale Recursive\nNetwork, and post-synaptic partners are detected using a MLP with features\nconditioned on the local segmentation.\n  This work is novel because it requires minimal amount of training, leverages\nadvances in image segmentation directly, and provides a complete solution for\npolyadic synapse detection. We further introduce novel metrics to evaluate our\nalgorithm on connectomes of meaningful size. These metrics demonstrate that\ncomplete automatic prediction can be used to effectively characterize most\nconnectivity correctly.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 19:25:44 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Huang", "Gary B.", ""], ["Scheffer", "Louis K.", ""], ["Plaza", "Stephen M.", ""]]}, {"id": "1604.03168", "submitter": "Philipp Gysel", "authors": "Philipp Gysel, Mohammad Motamedi, Soheil Ghiasi", "title": "Hardware-oriented Approximation of Convolutional Neural Networks", "comments": "8 pages, 4 figures, Accepted as a workshop contribution at ICLR 2016.\n  Updated comparison to other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High computational complexity hinders the widespread usage of Convolutional\nNeural Networks (CNNs), especially in mobile devices. Hardware accelerators are\narguably the most promising approach for reducing both execution time and power\nconsumption. One of the most important steps in accelerator development is\nhardware-oriented model approximation. In this paper we present Ristretto, a\nmodel approximation framework that analyzes a given CNN with respect to\nnumerical resolution used in representing weights and outputs of convolutional\nand fully connected layers. Ristretto can condense models by using fixed point\narithmetic and representation instead of floating point. Moreover, Ristretto\nfine-tunes the resulting fixed point network. Given a maximum error tolerance\nof 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.\nThe code for Ristretto is available.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 22:43:21 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 17:12:47 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 15:09:39 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Gysel", "Philipp", ""], ["Motamedi", "Mohammad", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1604.03169", "submitter": "Marcel Salathe", "authors": "Sharada Prasanna Mohanty, David Hughes, Marcel Salathe", "title": "Using Deep Learning for Image-Based Plant Disease Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop diseases are a major threat to food security, but their rapid\nidentification remains difficult in many parts of the world due to the lack of\nthe necessary infrastructure. The combination of increasing global smartphone\npenetration and recent advances in computer vision made possible by deep\nlearning has paved the way for smartphone-assisted disease diagnosis. Using a\npublic dataset of 54,306 images of diseased and healthy plant leaves collected\nunder controlled conditions, we train a deep convolutional neural network to\nidentify 14 crop species and 26 diseases (or absence thereof). The trained\nmodel achieves an accuracy of 99.35% on a held-out test set, demonstrating the\nfeasibility of this approach. When testing the model on a set of images\ncollected from trusted online sources - i.e. taken under conditions different\nfrom the images used for training - the model still achieves an accuracy of\n31.4%. While this accuracy is much higher than the one based on random\nselection (2.6%), a more diverse set of training data is needed to improve the\ngeneral accuracy. Overall, the approach of training deep learning models on\nincreasingly large and publicly available image datasets presents a clear path\ntowards smartphone-assisted crop disease diagnosis on a massive global scale.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 22:44:20 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 14:05:34 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Mohanty", "Sharada Prasanna", ""], ["Hughes", "David", ""], ["Salathe", "Marcel", ""]]}, {"id": "1604.03193", "submitter": "Kye-Ryong Sin", "authors": "Sung-Ho Jong, Yong-U Ri and Kye-Ryong Sin", "title": "Application of the Second-Order Statistics for Estimation of the Pure\n  Spectra of Individual Components from the Visible Hyperspectral Images of\n  Their Mixture", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-order statistics (SOS) can be applied in estimation of the pure\nspectra of chemical components from the spectrum of their mixture, when SOS\nseems to be good at estimation of spectral patterns, but their peak directions\nare opposite in some cases. In this paper, one method for judgment of the peak\ndirection of the pure spectra was proposed, where the base line of the pure\nspectra was drawn by using their histograms and the peak directions were chosen\nso as to make all of the pure spectra located upwards over the base line.\nResults of the SOS analysis on the visible hyperspectral images of the mixture\ncomposed of two or three chemical components showed that the present method\noffered the reasonable shape and direction of the pure spectra of its\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 01:23:40 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Jong", "Sung-Ho", ""], ["Ri", "Yong-U", ""], ["Sin", "Kye-Ryong", ""]]}, {"id": "1604.03196", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, Brandon Rothrock, Charles Fleming, Hyun Jong Yang", "title": "Privacy-Preserving Human Activity Recognition from Extreme Low\n  Resolution", "comments": null, "journal-ref": "AAAI 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy protection from surreptitious video recordings is an important\nsocietal challenge. We desire a computer vision system (e.g., a robot) that can\nrecognize human activities and assist our daily life, yet ensure that it is not\nrecording video that may invade our privacy. This paper presents a fundamental\napproach to address such contradicting objectives: human activity recognition\nwhile only using extreme low-resolution (e.g., 16x12) anonymized videos. We\nintroduce the paradigm of inverse super resolution (ISR), the concept of\nlearning the optimal set of image transformations to generate multiple\nlow-resolution (LR) training videos from a single video. Our ISR learns\ndifferent types of sub-pixel transformations optimized for the activity\nclassification, allowing the classifier to best take advantage of existing\nhigh-resolution videos (e.g., YouTube videos) by creating multiple LR training\nvideos tailored for the problem. We experimentally confirm that the paradigm of\ninverse super resolution is able to benefit activity recognition from extreme\nlow-resolution videos.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 01:33:53 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 21:38:47 GMT"}, {"version": "v3", "created": "Mon, 26 Dec 2016 11:03:46 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Rothrock", "Brandon", ""], ["Fleming", "Charles", ""], ["Yang", "Hyun Jong", ""]]}, {"id": "1604.03225", "submitter": "Deepak Ghimire", "authors": "Deepak Ghimire and Joonwhoan Lee", "title": "Geometric Feature-Based Facial Expression Recognition in Image Sequences\n  Using Multi-Class AdaBoost and Support Vector Machines", "comments": "21 pages, Sensors Journal, facial expression recognition", "journal-ref": "Sensors 2013, 13, 7714-7734", "doi": "10.3390/s130607714", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial expressions are widely used in the behavioral interpretation of\nemotions, cognitive science, and social interactions. In this paper, we present\na novel method for fully automatic facial expression recognition in facial\nimage sequences. As the facial expression evolves over time facial landmarks\nare automatically tracked in consecutive video frames, using displacements\nbased on elastic bunch graph matching displacement estimation. Feature vectors\nfrom individual landmarks, as well as pairs of landmarks tracking results are\nextracted, and normalized, with respect to the first frame in the sequence. The\nprototypical expression sequence for each class of facial expression is formed,\nby taking the median of the landmark tracking results from the training facial\nexpression sequences. Multi-class AdaBoost with dynamic time warping similarity\ndistance between the feature vector of input facial expression and prototypical\nfacial expression, is used as a weak classifier to select the subset of\ndiscriminative feature vectors. Finally, two methods for facial expression\nrecognition are presented, either by using multi-class AdaBoost with dynamic\ntime warping, or by using support vector machine on the boosted feature\nvectors. The results on the Cohn-Kanade (CK+) facial expression database show a\nrecognition accuracy of 95.17% and 97.35% using multi-class AdaBoost and\nsupport vector machines, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:00:13 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Ghimire", "Deepak", ""], ["Lee", "Joonwhoan", ""]]}, {"id": "1604.03227", "submitter": "Jason Kuen", "authors": "Jason Kuen, Zhenhua Wang, Gang Wang", "title": "Recurrent Attentional Networks for Saliency Detection", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:03:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Kuen", "Jason", ""], ["Wang", "Zhenhua", ""], ["Wang", "Gang", ""]]}, {"id": "1604.03239", "submitter": "Bin Yang", "authors": "Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li", "title": "CRAFT Objects from Images", "comments": "CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental problem in image understanding. One popular\nsolution is the R-CNN framework and its fast versions. They decompose the\nobject detection problem into two cascaded easier tasks: 1) generating object\nproposals from images, 2) classifying proposals into various object categories.\nDespite that we are handling with two relatively easier tasks, they are not\nsolved perfectly and there's still room for improvement. In this paper, we push\nthe \"divide and conquer\" solution even further by dividing each task into two\nsub-tasks. We call the proposed method \"CRAFT\" (Cascade Region-proposal-network\nAnd FasT-rcnn), which tackles each task with a carefully designed network\ncascade. We show that the cascade structure helps in both tasks: in proposal\ngeneration, it provides more compact and better localized object proposals; in\nobject classification, it reduces false positives (mainly between ambiguous\ncategories) by capturing both inter- and intra-category variances. CRAFT\nachieves consistent and considerable improvement over the state-of-the-art on\nobject detection benchmarks like PASCAL VOC 07/12 and ILSVRC.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:57:48 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Yang", "Bin", ""], ["Yan", "Junjie", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1604.03247", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Categorization is a challenging problem, especially when the images\nhave clutter background, occlusions or different lighting conditions. In the\npast, many descriptors have been proposed which aid object categorization even\nin such adverse conditions. Each descriptor has its own merits and de-merits.\nSome descriptors are invariant to transformations while the others are more\ndiscriminative. Past research has shown that, employing multiple descriptors\nrather than any single descriptor leads to better recognition. The problem of\nlearning the optimal combination of the available descriptors for a particular\nclassification task is studied. Multiple Kernel Learning (MKL) framework has\nbeen developed for learning an optimal combination of descriptors for object\ncategorization. Existing MKL formulations often employ block l-1 norm\nregularization which is equivalent to selecting a single kernel from a library\nof kernels. Since essentially a single descriptor is selected, the existing\nformulations maybe sub- optimal for object categorization. A MKL formulation\nbased on block l-infinity norm regularization has been developed, which chooses\nan optimal combination of kernels as opposed to selecting a single kernel. A\nComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity\nand l-1 norm regularization has been developed. These formulations end in\nSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms for\nthese formulation have been implemented. Empirical results on benchmark\ndatasets show significant improvement using these new MKL formulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 04:56:24 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Govindaraj", "Dinesh", ""]]}, {"id": "1604.03249", "submitter": "Marcus Rohrbach", "authors": "Marcus Rohrbach", "title": "Attributes as Semantic Units between Natural Language and Visual\n  Recognition", "comments": "book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 05:23:26 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Rohrbach", "Marcus", ""]]}, {"id": "1604.03265", "submitter": "Hao Su", "authors": "Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan,\n  Leonidas J. Guibas", "title": "Volumetric and Multi-View CNNs for Object Classification on 3D Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape models are becoming widely available and easier to capture, making\navailable 3D information crucial for progress in object classification. Current\nstate-of-the-art methods rely on CNNs to address this problem. Recently, we\nwitness two types of CNNs being developed: CNNs based upon volumetric\nrepresentations versus CNNs based upon multi-view representations. Empirical\nresults from these two types of CNNs exhibit a large gap, indicating that\nexisting volumetric CNN architectures and approaches are unable to fully\nexploit the power of 3D representations. In this paper, we aim to improve both\nvolumetric CNNs and multi-view CNNs according to extensive analysis of existing\napproaches. To this end, we introduce two distinct network architectures of\nvolumetric CNNs. In addition, we examine multi-view CNNs, where we introduce\nmulti-resolution filtering in 3D. Overall, we are able to outperform current\nstate-of-the-art methods for both volumetric CNNs and multi-view CNNs. We\nprovide extensive experiments designed to evaluate underlying design choices,\nthus providing a better understanding of the space of methods available for\nobject classification on 3D data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:10:43 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 06:21:09 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Qi", "Charles R.", ""], ["Su", "Hao", ""], ["Niessner", "Matthias", ""], ["Dai", "Angela", ""], ["Yan", "Mengyuan", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1604.03286", "submitter": "Th\\'eodore Bluche", "authors": "Th\\'eodore Bluche, J\\'er\\^ome Louradour and Ronaldo Messina", "title": "Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with\n  MDLSTM Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based model for end-to-end handwriting recognition.\nOur system does not require any segmentation of the input paragraph. The model\nis inspired by the differentiable attention models presented recently for\nspeech recognition, image captioning or translation. The main difference is the\ncovert and overt attention, implemented as a multi-dimensional LSTM network.\nOur principal contribution towards handwriting recognition lies in the\nautomatic transcription without a prior segmentation into lines, which was\ncrucial in previous approaches. To the best of our knowledge this is the first\nsuccessful attempt of end-to-end multi-line handwriting recognition. We carried\nout experiments on the well-known IAM Database. The results are encouraging and\nbring hope to perform full paragraph transcription in the near future.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 08:11:20 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 11:42:54 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 08:47:49 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bluche", "Th\u00e9odore", ""], ["Louradour", "J\u00e9r\u00f4me", ""], ["Messina", "Ronaldo", ""]]}, {"id": "1604.03334", "submitter": "Shanxin Yuan", "authors": "Qi Ye, Shanxin Yuan, Tae-Kyun Kim", "title": "Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand\n  Pose Estimation", "comments": "The work is accepted by ECCV2016, Demo video:\n  https://youtu.be/2Hg0c88rHkk, Project Page:\n  https://sites.google.com/site/qiyeincv/home/eccv2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative methods often generate hand poses kinematically implausible,\nthen generative methods are used to correct (or verify) these results in a\nhybrid method. Estimating 3D hand pose in a hierarchy, where the\nhigh-dimensional output space is decomposed into smaller ones, has been shown\neffective. Existing hierarchical methods mainly focus on the decomposition of\nthe output space while the input space remains almost the same along the\nhierarchy. In this paper, a hybrid hand pose estimation method is proposed by\napplying the kinematic hierarchy strategy to the input space (as well as the\noutput space) of the discriminative method by a spatial attention mechanism and\nto the optimization of the generative method by hierarchical Particle Swarm\nOptimization (PSO). The spatial attention mechanism integrates cascaded and\nhierarchical regression into a CNN framework by transforming both the input(and\nfeature space) and the output space, which greatly reduces the viewpoint and\narticulation variations. Between the levels in the hierarchy, the hierarchical\nPSO forces the kinematic constraints to the results of the CNNs. The\nexperimental results show that our method significantly outperforms four\nstate-of-the-art methods and three baselines on three public benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 10:47:12 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 10:51:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Ye", "Qi", ""], ["Yuan", "Shanxin", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1604.03346", "submitter": "Moacir Antonelli Ponti", "authors": "Moacir Ponti and Mateus Riva", "title": "An incremental linear-time learning algorithm for the Optimum-Path\n  Forest classifier", "comments": "submitted to IPL Journal for consideration in Nov/2016", "journal-ref": null, "doi": "10.1016/j.ipl.2017.05.004", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a classification method with incremental capabilities based on the\nOptimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a\nfully-connected training graph, arc weights represent distances between two\nfeature vectors. Our algorithm includes new instances in an OPF in linear-time,\nwhile keeping similar accuracies when compared with the original quadratic-time\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:31:23 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 15:47:21 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 15:54:04 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 13:14:10 GMT"}, {"version": "v5", "created": "Wed, 23 Nov 2016 12:08:23 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ponti", "Moacir", ""], ["Riva", "Mateus", ""]]}, {"id": "1604.03351", "submitter": "Nima Sedaghat Alvar", "authors": "Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri and Thomas Brox", "title": "Orientation-boosted Voxel Nets for 3D Object Recognition", "comments": "BMVC'17 version. Added some experiments + auto-alignment of\n  Modelnet40", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown good recognition results in 3D object recognition using\n3D convolutional networks. In this paper, we show that the object orientation\nplays an important role in 3D recognition. More specifically, we argue that\nobjects induce different features in the network under rotation. Thus, we\napproach the category-level classification task as a multi-task problem, in\nwhich the network is trained to predict the pose of the object in addition to\nthe class label as a parallel task. We show that this yields significant\nimprovements in the classification results. We test our suggested architecture\non several datasets representing various 3D data sources: LiDAR data, CAD\nmodels, and RGB-D images. We report state-of-the-art results on classification\nas well as significant improvements in precision and speed over the baseline on\n3D detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:43:14 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 21:41:12 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Sedaghat", "Nima", ""], ["Zolfaghari", "Mohammadreza", ""], ["Amiri", "Ehsan", ""], ["Brox", "Thomas", ""]]}, {"id": "1604.03390", "submitter": "Marc Bola\\~nos", "authors": "\\'Alvaro Peris, Marc Bola\\~nos, Petia Radeva and Francisco Casacuberta", "title": "Video Description using Bidirectional Recurrent Neural Networks", "comments": "8 pages, 3 figures, 1 table, Submitted to International Conference on\n  Artificial Neural Networks (ICANN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although traditionally used in the machine translation field, the\nencoder-decoder framework has been recently applied for the generation of video\nand image descriptions. The combination of Convolutional and Recurrent Neural\nNetworks in these models has proven to outperform the previous state of the\nart, obtaining more accurate video descriptions. In this work we propose\npushing further this model by introducing two contributions into the encoding\nstage. First, producing richer image representations by combining object and\nlocation information from Convolutional Neural Networks and second, introducing\nBidirectional Recurrent Neural Networks for capturing both forward and backward\ntemporal relationships in the input frames.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 13:09:01 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 12:28:07 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Peris", "\u00c1lvaro", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1604.03426", "submitter": "Alireza Aghasi", "authors": "Alireza Aghasi, Barmak Heshmat, Albert Redo-Sanchez, Justin Romberg,\n  Ramesh Raskar", "title": "Sweep Distortion Removal from THz Images via Blind Demodulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy sweep distortion induced by alignments and inter-reflections of layers\nof a sample is a major burden in recovering 2D and 3D information in time\nresolved spectral imaging. This problem cannot be addressed by conventional\ndenoising and signal processing techniques as it heavily depends on the physics\nof the acquisition. Here we propose and implement an algorithmic framework\nbased on low-rank matrix recovery and alternating minimization that exploits\nthe forward model for THz acquisition. The method allows recovering the\noriginal signal in spite of the presence of temporal-spatial distortions. We\naddress a blind-demodulation problem, where based on several observations of\nthe sample texture modulated by an undesired sweep pattern, the two classes of\nsignals are separated. The performance of the method is examined in both\nsynthetic and experimental data, and the successful reconstructions are\ndemonstrated. The proposed general scheme can be implemented to advance\ninspection and imaging applications in THz and other time-resolved sensing\nmodalities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 20:01:16 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Aghasi", "Alireza", ""], ["Heshmat", "Barmak", ""], ["Redo-Sanchez", "Albert", ""], ["Romberg", "Justin", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1604.03443", "submitter": "Li Jinxing", "authors": "Jinxing Li, David Zhang, Yongcheng Li, and Jian Wu", "title": "Multi-modal Fusion for Diabetes Mellitus and Impaired Glucose Regulation\n  Detection", "comments": "9 pages, 8 figures, 30 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective and accurate diagnosis of Diabetes Mellitus (DM), as well as its\nearly stage Impaired Glucose Regulation (IGR), has attracted much attention\nrecently. Traditional Chinese Medicine (TCM) [3], [5] etc. has proved that\ntongue, face and sublingual diagnosis as a noninvasive method is a reasonable\nway for disease detection. However, most previous works only focus on a single\nmodality (tongue, face or sublingual) for diagnosis, although different\nmodalities may provide complementary information for the diagnosis of DM and\nIGR. In this paper, we propose a novel multi-modal classification method to\ndiscriminate between DM (or IGR) and healthy controls. Specially, the tongue,\nfacial and sublingual images are first collected by using a non-invasive\ncapture device. The color, texture and geometry features of these three types\nof images are then extracted, respectively. Finally, our so-called multi-modal\nsimilar and specific learning (MMSSL) approach is proposed to combine features\nof tongue, face and sublingual, which not only exploits the correlation but\nalso extracts individual components among them. Experimental results on a\ndataset consisting of 192 Healthy, 198 DM and 114 IGR samples (all samples were\nobtained from Guangdong Provincial Hospital of Traditional Chinese Medicine)\nsubstantiate the effectiveness and superiority of our proposed method for the\ndiagnosis of DM and IGR, compared to the case of using a single modality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 15:31:52 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Li", "Jinxing", ""], ["Zhang", "David", ""], ["Li", "Yongcheng", ""], ["Wu", "Jian", ""]]}, {"id": "1604.03489", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Brendan Jou and Xavier Giro-i-Nieto", "title": "From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment\n  Prediction", "comments": "Accepted for publication in Image and Vision Computing. Models and\n  source code available at https://github.com/imatge-upc/sentiment-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual multimedia have become an inseparable part of our digital social\nlives, and they often capture moments tied with deep affections. Automated\nvisual sentiment analysis tools can provide a means of extracting the rich\nfeelings and latent dispositions embedded in these media. In this work, we\nexplore how Convolutional Neural Networks (CNNs), a now de facto computational\nmachine learning tool particularly in the area of Computer Vision, can be\nspecifically applied to the task of visual sentiment prediction. We accomplish\nthis through fine-tuning experiments using a state-of-the-art CNN and via\nrigorous architecture analysis, we present several modifications that lead to\naccuracy improvements over prior art on a dataset of images from a popular\nsocial media platform. We additionally present visualizations of local patterns\nthat the network learned to associate with image sentiment for insight into how\nvisual positivity (or negativity) is perceived by the model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 17:24:39 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 18:02:16 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Campos", "Victor", ""], ["Jou", "Brendan", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1604.03498", "submitter": "Liangliang Cao", "authors": "Wenying Ma, Liangliang Cao, Lei Yu, Guoping Long, Yucheng Li", "title": "GPU-FV: Realtime Fisher Vector and Its Applications in Video Monitoring", "comments": "accepted by ICMR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher vector has been widely used in many multimedia retrieval and visual\nrecognition applications with good performance. However, the computation\ncomplexity prevents its usage in real-time video monitoring. In this work, we\nproposed and implemented GPU-FV, a fast Fisher vector extraction method with\nthe help of modern GPUs. The challenge of implementing Fisher vector on GPUs\nlies in the data dependency in feature extraction and expensive memory access\nin Fisher vector computing. To handle these challenges, we carefully designed\nGPU-FV in a way that utilizes the computing power of GPU as much as possible,\nand applied optimizations such as loop tiling to boost the performance. GPU-FV\nis about 12 times faster than the CPU version, and 50\\% faster than a\nnon-optimized GPU implementation. For standard video input (320*240), GPU-FV\ncan process each frame within 34ms on a model GPU. Our experiments show that\nGPU-FV obtains a similar recognition accuracy as traditional FV on VOC 2007 and\nCaltech 256 image sets. We also applied GPU-FV for realtime video monitoring\ntasks and found that GPU-FV outperforms a number of previous works. Especially,\nwhen the number of training examples are small, GPU-FV outperforms the recent\npopular deep CNN features borrowed from ImageNet. The code can be downloaded\nfrom the following link https://bitbucket.org/mawenjing/gpu-fv.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:22:08 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Ma", "Wenying", ""], ["Cao", "Liangliang", ""], ["Yu", "Lei", ""], ["Long", "Guoping", ""], ["Li", "Yucheng", ""]]}, {"id": "1604.03505", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R.\n  Selvaraju, Dhruv Batra, and Devi Parikh", "title": "Counting Everyday Objects in Everyday Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in counting the number of instances of object classes in\nnatural, everyday images. Previous counting approaches tackle the problem in\nrestricted domains such as counting pedestrians in surveillance videos. Counts\ncan also be estimated from outputs of other vision tasks like object detection.\nIn this work, we build dedicated models for counting designed to tackle the\nlarge variance in counts, appearances, and scales of objects found in natural\nscenes. Our approach is inspired by the phenomenon of subitizing - the ability\nof humans to make quick assessments of counts given a perceptual signal, for\nsmall count values. Given a natural scene, we employ a divide and conquer\nstrategy while incorporating context across the scene to adapt the subitizing\nidea to counting. Our approach offers consistent improvements over numerous\nbaseline approaches for counting on the PASCAL VOC 2007 and COCO datasets.\nSubsequently, we study how counting can be used to improve object detection. We\nthen show a proof of concept application of our counting methods to the task of\nVisual Question Answering, by studying the `how many?' questions in the VQA and\nCOCO-QA datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:31:43 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 17:34:20 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 03:24:40 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Vedantam", "Ramakrishna", ""], ["Selvaraju", "Ramprasaath R.", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1604.03513", "submitter": "Qifeng Chen", "authors": "Qifeng Chen and Vladlen Koltun", "title": "Full Flow: Optical Flow Estimation By Global Optimization over Regular\n  Grids", "comments": "To be presented at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a global optimization approach to optical flow estimation. The\napproach optimizes a classical optical flow objective over the full space of\nmappings between discrete grids. No descriptor matching is used. The highly\nregular structure of the space of mappings enables optimizations that reduce\nthe computational complexity of the algorithm's inner loop from quadratic to\nlinear and support efficient matching of tens of thousands of nodes to tens of\nthousands of displacements. We show that one-shot global optimization of a\nclassical Horn-Schunck-type objective over regular grids at a single resolution\nis sufficient to initialize continuous interpolation and achieve\nstate-of-the-art performance on challenging modern benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:40:47 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Chen", "Qifeng", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1604.03517", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Heesung Kwon, Archith J. Bency, and William D. Nothwang", "title": "Fast Object Localization Using a CNN Feature Map Based Multi-Scale\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object localization is an important task in computer vision but requires a\nlarge amount of computational power due mainly to an exhaustive multiscale\nsearch on the input image. In this paper, we describe a near real-time\nmultiscale search on a deep CNN feature map that does not use region proposals.\nThe proposed approach effectively exploits local semantic information preserved\nin the feature map of the outermost convolutional layer. A multi-scale search\nis performed on the feature map by processing all the sub-regions of different\nsizes using separate expert units of fully connected layers. Each expert unit\nreceives as input local semantic features only from the corresponding\nsub-regions of a specific geometric shape. Therefore, it contains more nearly\noptimal parameters tailored to the corresponding shape. This multi-scale and\nmulti-aspect ratio scanning strategy can effectively localize a potential\nobject of an arbitrary size. The proposed approach is fast and able to localize\nobjects of interest with a frame rate of 4 fps while providing improved\ndetection performance over the state-of-the art on the PASCAL VOC 12 and MSCOCO\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:44:10 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""], ["Bency", "Archith J.", ""], ["Nothwang", "William D.", ""]]}, {"id": "1604.03518", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, and William D. Nothwang", "title": "DTM: Deformable Template Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel template matching algorithm that can incorporate the concept of\ndeformable parts, is presented in this paper. Unlike the deformable part model\n(DPM) employed in object recognition, the proposed template-matching approach\ncalled Deformable Template Matching (DTM) does not require a training step.\nInstead, deformation is achieved by a set of predefined basic rules (e.g. the\nleft sub-patch cannot pass across the right patch). Experimental evaluation of\nthis new method using the PASCAL VOC 07 dataset demonstrated substantial\nperformance improvement over conventional template matching algorithms.\nAdditionally, to confirm the applicability of DTM, the concept is applied to\nthe generation of a rotation-invariant SIFT descriptor. Experimental evaluation\nemploying deformable matching of SIFT features shows an increased number of\nmatching features compared to a conventional SIFT matching.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:44:25 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""], ["Robinson", "Ryan M.", ""], ["Nothwang", "William D.", ""]]}, {"id": "1604.03519", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee and Heesung Kwon", "title": "Going Deeper with Contextual CNN for Hyperspectral Image Classification", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TIP.2017.2725580", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel deep convolutional neural network (CNN)\nthat is deeper and wider than other existing deep networks for hyperspectral\nimage classification. Unlike current state-of-the-art approaches in CNN-based\nhyperspectral image classification, the proposed network, called contextual\ndeep CNN, can optimally explore local contextual interactions by jointly\nexploiting local spatio-spectral relationships of neighboring individual pixel\nvectors. The joint exploitation of the spatio-spectral information is achieved\nby a multi-scale convolutional filter bank used as an initial component of the\nproposed CNN pipeline. The initial spatial and spectral feature maps obtained\nfrom the multi-scale filter bank are then combined together to form a joint\nspatio-spectral feature map. The joint feature map representing rich spectral\nand spatial properties of the hyperspectral image is then fed through a fully\nconvolutional network that eventually predicts the corresponding label of each\npixel vector. The proposed approach is tested on three benchmark datasets: the\nIndian Pines dataset, the Salinas dataset and the University of Pavia dataset.\nPerformance comparison shows enhanced classification performance of the\nproposed approach over the current state-of-the-art on the three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:44:34 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 19:39:52 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 14:21:21 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""]]}, {"id": "1604.03539", "submitter": "Ishan Misra", "authors": "Ishan Misra and Abhinav Shrivastava and Abhinav Gupta and Martial\n  Hebert", "title": "Cross-stitch Networks for Multi-task Learning", "comments": "To appear in CVPR 2016 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning in Convolutional Networks has displayed remarkable\nsuccess in the field of recognition. This success can be largely attributed to\nlearning shared representations from multiple supervisory tasks. However,\nexisting multi-task approaches rely on enumerating multiple network\narchitectures specific to the tasks at hand, that do not generalize. In this\npaper, we propose a principled approach to learn shared representations in\nConvNets using multi-task learning. Specifically, we propose a new sharing\nunit: \"cross-stitch\" unit. These units combine the activations from multiple\nnetworks and can be trained end-to-end. A network with cross-stitch units can\nlearn an optimal combination of shared and task-specific representations. Our\nproposed method generalizes across multiple tasks and shows dramatically\nimproved performance over baseline methods for categories with few training\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 19:43:25 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Misra", "Ishan", ""], ["Shrivastava", "Abhinav", ""], ["Gupta", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1604.03540", "submitter": "Abhinav Shrivastava", "authors": "Abhinav Shrivastava, Abhinav Gupta, Ross Girshick", "title": "Training Region-based Object Detectors with Online Hard Example Mining", "comments": "To appear in Proceedings of IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016. (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of object detection has made significant advances riding on the\nwave of region-based ConvNets, but their training procedure still includes many\nheuristics and hyperparameters that are costly to tune. We present a simple yet\nsurprisingly effective online hard example mining (OHEM) algorithm for training\nregion-based ConvNet detectors. Our motivation is the same as it has always\nbeen -- detection datasets contain an overwhelming number of easy examples and\na small number of hard examples. Automatic selection of these hard examples can\nmake training more effective and efficient. OHEM is a simple and intuitive\nalgorithm that eliminates several heuristics and hyperparameters in common use.\nBut more importantly, it yields consistent and significant boosts in detection\nperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness\nincreases as datasets become larger and more difficult, as demonstrated by the\nresults on the MS COCO dataset. Moreover, combined with complementary advances\nin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on\nPASCAL VOC 2007 and 2012 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 19:44:13 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Shrivastava", "Abhinav", ""], ["Gupta", "Abhinav", ""], ["Girshick", "Ross", ""]]}, {"id": "1604.03605", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, and Fr\\'edo\n  Durand", "title": "What do different evaluation metrics tell us about saliency models?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How best to evaluate a saliency model's ability to predict where humans look\nin images is an open research question. The choice of evaluation metric depends\non how saliency is defined and how the ground truth is represented. Metrics\ndiffer in how they rank saliency models, and this results from how false\npositives and false negatives are treated, whether viewing biases are accounted\nfor, whether spatial deviations are factored in, and how the saliency maps are\npre-processed. In this paper, we provide an analysis of 8 different evaluation\nmetrics and their properties. With the help of systematic experiments and\nvisualizations of metric computations, we add interpretability to saliency\nscores and more transparency to the evaluation of saliency models. Building off\nthe differences in metric properties and behaviors, we make recommendations for\nmetric selections under specific assumptions and for specific applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 22:16:20 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 23:46:40 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Judd", "Tilke", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1604.03628", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Devi Parikh, Dhruv Batra", "title": "Joint Unsupervised Learning of Deep Representations and Image Clusters", "comments": "19 pages, 11 figures, 14 tables, 2016 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a recurrent framework for Joint Unsupervised\nLEarning (JULE) of deep representations and image clusters. In our framework,\nsuccessive operations in a clustering algorithm are expressed as steps in a\nrecurrent process, stacked on top of representations output by a Convolutional\nNeural Network (CNN). During training, image clusters and representations are\nupdated jointly: image clustering is conducted in the forward pass, while\nrepresentation learning in the backward pass. Our key idea behind this\nframework is that good representations are beneficial to image clustering and\nclustering results provide supervisory signals to representation learning. By\nintegrating two processes into a single model with a unified weighted triplet\nloss and optimizing it end-to-end, we can obtain not only more powerful\nrepresentations, but also more precise image clusters. Extensive experiments\nshow that our method outperforms the state-of-the-art on image clustering\nacross a variety of image datasets. Moreover, the learned representations\ngeneralize well when transferred to other tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 01:24:59 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 19:45:59 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 19:56:16 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yang", "Jianwei", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1604.03629", "submitter": "William Gray Roncal", "authors": "Eva L. Dyer, William Gray Roncal, Hugo L. Fernandes, Doga G\\\"ursoy,\n  Vincent De Andrade, Rafael Vescovi, Kamel Fezzaa, Xianghui Xiao, Joshua T.\n  Vogelstein, Chris Jacobsen, Konrad P. K\\\"ording and Narayanan Kasthuri", "title": "Quantifying mesoscale neuroanatomy using X-ray microtomography", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for resolving the 3D microstructure of the brain typically start by\nthinly slicing and staining the brain, and then imaging each individual section\nwith visible light photons or electrons. In contrast, X-rays can be used to\nimage thick samples, providing a rapid approach for producing large 3D brain\nmaps without sectioning. Here we demonstrate the use of synchrotron X-ray\nmicrotomography ($\\mu$CT) for producing mesoscale $(1~\\mu m^3)$ resolution\nbrain maps from millimeter-scale volumes of mouse brain. We introduce a\npipeline for $\\mu$CT-based brain mapping that combines methods for sample\npreparation, imaging, automated segmentation of image volumes into cells and\nblood vessels, and statistical analysis of the resulting brain structures. Our\nresults demonstrate that X-ray tomography promises rapid quantification of\nlarge brain volumes, complementing other brain mapping and connectomics\nefforts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 01:46:54 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 19:56:59 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Dyer", "Eva L.", ""], ["Roncal", "William Gray", ""], ["Fernandes", "Hugo L.", ""], ["G\u00fcrsoy", "Doga", ""], ["De Andrade", "Vincent", ""], ["Vescovi", "Rafael", ""], ["Fezzaa", "Kamel", ""], ["Xiao", "Xianghui", ""], ["Vogelstein", "Joshua T.", ""], ["Jacobsen", "Chris", ""], ["K\u00f6rding", "Konrad P.", ""], ["Kasthuri", "Narayanan", ""]]}, {"id": "1604.03635", "submitter": "Anton Milan", "authors": "Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Ian Reid, Konrad\n  Schindler", "title": "Online Multi-Target Tracking Using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to online multi-target tracking based on\nrecurrent neural networks (RNNs). Tracking multiple objects in real-world\nscenes involves many challenges, including a) an a-priori unknown and\ntime-varying number of targets, b) a continuous state estimation of all present\ntargets, and c) a discrete combinatorial problem of data association. Most\nprevious methods involve complex models that require tedious tuning of\nparameters. Here, we propose for the first time, an end-to-end learning\napproach for online multi-target tracking. Existing deep learning methods are\nnot designed for the above challenges and cannot be trivially applied to the\ntask. Our solution addresses all of the above points in a principled way.\nExperiments on both synthetic and real data show promising results obtained at\n~300 Hz on a standard CPU, and pave the way towards future research in this\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 02:41:43 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:09:30 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Milan", "Anton", ""], ["Rezatofighi", "Seyed Hamid", ""], ["Dick", "Anthony", ""], ["Reid", "Ian", ""], ["Schindler", "Konrad", ""]]}, {"id": "1604.03650", "submitter": "Junyuan Xie", "authors": "Junyuan Xie, Ross Girshick, Ali Farhadi", "title": "Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D movie viewing becomes mainstream and Virtual Reality (VR) market\nemerges, the demand for 3D contents is growing rapidly. Producing 3D videos,\nhowever, remains challenging. In this paper we propose to use deep neural\nnetworks for automatically converting 2D videos and images to stereoscopic 3D\nformat. In contrast to previous automatic 2D-to-3D conversion algorithms, which\nhave separate stages and need ground truth depth map as supervision, our\napproach is trained end-to-end directly on stereo pairs extracted from 3D\nmovies. This novel training scheme makes it possible to exploit orders of\nmagnitude more data and significantly increases performance. Indeed, Deep3D\noutperforms baselines in both quantitative and human subject evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 04:35:07 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Xie", "Junyuan", ""], ["Girshick", "Ross", ""], ["Farhadi", "Ali", ""]]}, {"id": "1604.03692", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, M. S. Ryoo and Song-Chun Zhu", "title": "Learning Social Affordance for Human-Robot Interaction", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for robot learning of social affordance\nfrom human activity videos. We consider the problem in the context of\nhuman-robot interaction: Our approach learns structural representations of\nhuman-human (and human-object-human) interactions, describing how body-parts of\neach agent move with respect to each other and what spatial relations they\nshould maintain to complete each sub-event (i.e., sub-goal). This enables the\nrobot to infer its own movement in reaction to the human body motion, allowing\nit to naturally replicate such interactions.\n  We introduce the representation of social affordance and propose a generative\nmodel for its weakly supervised learning from human demonstration videos. Our\napproach discovers critical steps (i.e., latent sub-events) in an interaction\nand the typical motion associated with them, learning what body-parts should be\ninvolved and how. The experimental results demonstrate that our Markov Chain\nMonte Carlo (MCMC) based learning algorithm automatically discovers\nsemantically meaningful interactive affordance from RGB-D videos, which allows\nus to generate appropriate full body motion for an agent.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 08:40:06 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 21:02:02 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Shu", "Tianmin", ""], ["Ryoo", "M. S.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1604.03734", "submitter": "Michael Tanner", "authors": "Michael Tanner and Pedro Pinies and Lina Maria Paz and Paul Newman", "title": "DENSER Cities: A System for Dense Efficient Reconstructions of Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the efficient generation of dense, colored models of\ncity-scale environments from range data and in particular, stereo cameras.\nBetter maps make for better understanding; better understanding leads to better\nrobots, but this comes at a cost. The computational and memory requirements of\nlarge dense models can be prohibitive. We provide the theory and the system\nneeded to create city-scale dense reconstructions. To do so, we apply a\nregularizer over a compressed 3D data structure while dealing with the complex\nboundary conditions this induces during the data-fusion stage. We show that\nonly with these considerations can we swiftly create neat, large, \"well\nbehaved\" reconstructions. We evaluate our system using the KITTI dataset and\nprovide statistics for the metric errors in all surfaces created compared to\nthose measured with 3D laser. Our regularizer reduces the median error by 40%\nin 3.4 km of dense reconstructions with a median accuracy of 6 cm. For\nsubjective analysis, we provide a qualitative review of 6.1 km of our dense\nreconstructions in an attached video. These are the largest dense\nreconstructions from a single passive camera we are aware of in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 12:37:59 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Tanner", "Michael", ""], ["Pinies", "Pedro", ""], ["Paz", "Lina Maria", ""], ["Newman", "Paul", ""]]}, {"id": "1604.03755", "submitter": "Mario Fritz", "authors": "Abhishek Sharma, Oliver Grau, Mario Fritz", "title": "VConv-DAE: Deep Volumetric Shape Learning Without Object Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of affordable depth sensors, 3D capture becomes more and more\nubiquitous and already has made its way into commercial products. Yet,\ncapturing the geometry or complete shapes of everyday objects using scanning\ndevices (e.g. Kinect) still comes with several challenges that result in noise\nor even incomplete shapes. Recent success in deep learning has shown how to\nlearn complex shape distributions in a data-driven way from large scale 3D CAD\nModel collections and to utilize them for 3D processing on volumetric\nrepresentations and thereby circumventing problems of topology and\ntessellation. Prior work has shown encouraging results on problems ranging from\nshape completion to recognition. We provide an analysis of such approaches and\ndiscover that training as well as the resulting representation are strongly and\nunnecessarily tied to the notion of object labels. Thus, we propose a full\nconvolutional volumetric auto encoder that learns volumetric representation\nfrom noisy data by estimating the voxel occupancy grids. The proposed method\noutperforms prior work on challenging tasks like denoising and shape\ncompletion. We also show that the obtained deep embedding gives competitive\nperformance when used for classification and promising results for shape\ninterpolation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 13:14:53 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 10:16:33 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 20:36:36 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Sharma", "Abhishek", ""], ["Grau", "Oliver", ""], ["Fritz", "Mario", ""]]}, {"id": "1604.03832", "submitter": "Mikhail Kharinov Vyacheslavovich", "authors": "Mikhail Kharinov", "title": "Reversible Image Merging for Low-level Machine Vision", "comments": "5 pages, 3 figures, 6 formulas, submitted to the 13th International\n  Conference on Pattern Recognition and Information Processing October 3-5,\n  2016, Minsk, Belarus", "journal-ref": "Proc of the 13th International Conference on Pattern Recognition\n  and Information Processing (PRIP'2016), Oct 3-5, 2016, Minsk, Belarus,\n  pp.25-29", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a hierarchical model for pixel clustering and image\nsegmentation is developed. In the model an image is hierarchically structured.\nThe original image is treated as a set of nested images, which are capable to\nreversibly merge with each other. An object is defined as a structural element\nof an image, so that, an image is regarded as a maximal object. The simulating\nof none-hierarchical optimal pixel clustering by hierarchical clustering is\nstudied. To generate a hierarchy of optimized piecewise constant image\napproximations, estimated by the standard deviation of approximation from the\nimage, the conversion of any hierarchy of approximations into the hierarchy\ndescribed in relation to the number of intensity levels by convex sequence of\ntotal squared errors is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 15:31:24 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Kharinov", "Mikhail", ""]]}, {"id": "1604.03880", "submitter": "Hao Jiang", "authors": "Hao Jiang and Kristen Grauman", "title": "Detangling People: Individuating Multiple Close People and Their Body\n  Parts via Region Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's person detection methods work best when people are in common upright\nposes and appear reasonably well spaced out in the image. However, in many real\nimages, that's not what people do. People often appear quite close to each\nother, e.g., with limbs linked or heads touching, and their poses are often not\npedestrian-like. We propose an approach to detangle people in multi-person\nimages. We formulate the task as a region assembly problem. Starting from a\nlarge set of overlapping regions from body part semantic segmentation and\ngeneric object proposals, our optimization approach reassembles those pieces\ntogether into multiple person instances. It enforces that the composed body\npart regions of each person instance obey constraints on relative sizes, mutual\nspatial relationships, foreground coverage, and exclusive label assignments\nwhen overlapping. Since optimal region assembly is a challenging combinatorial\nproblem, we present a Lagrangian relaxation method to accelerate the lower\nbound estimation, thereby enabling a fast branch and bound solution for the\nglobal optimum. As output, our method produces a pixel-level map indicating\nboth 1) the body part labels (arm, leg, torso, and head), and 2) which parts\nbelong to which individual person. Our results on three challenging datasets\nshow our method is robust to clutter, occlusion, and complex poses. It\noutperforms a variety of competing methods, including existing detector CRF\nmethods and region CNN approaches. In addition, we demonstrate its impact on a\nproxemics recognition task, which demands a precise representation of \"whose\nbody part is where\" in crowded images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 17:35:05 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Jiang", "Hao", ""], ["Grauman", "Kristen", ""]]}, {"id": "1604.03882", "submitter": "Milind Gide", "authors": "Milind S. Gide, Samuel F. Dodge, and Lina J. Karam", "title": "The Effect of Distortions on the Prediction of Visual Attention", "comments": "14 pages, 2 column, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing saliency models have been designed and evaluated for predicting the\nsaliency in distortion-free images. However, in practice, the image quality is\naffected by a host of factors at several stages of the image processing\npipeline such as acquisition, compression and transmission. Several studies\nhave explored the effect of distortion on human visual attention; however, none\nof them have considered the performance of visual saliency models in the\npresence of distortion. Furthermore, given that one potential application of\nvisual saliency prediction is to aid pooling of objective visual quality\nmetrics, it is important to compare the performance of existing saliency models\non distorted images. In this paper, we evaluate several state-of-the-art visual\nattention models over different databases consisting of distorted images with\nvarious types of distortions such as blur, noise and compression with varying\nlevels of distortion severity. This paper also introduces new improved\nperformance evaluation metrics that are shown to overcome shortcomings in\nexisting performance metrics. We find that the performance of most models\nimproves with moderate and high levels of distortions as compared to the near\ndistortion-free case. In addition, model performance is also found to decrease\nwith an increase in image complexity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 17:37:54 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Gide", "Milind S.", ""], ["Dodge", "Samuel F.", ""], ["Karam", "Lina J.", ""]]}, {"id": "1604.03901", "submitter": "Weifeng Chen", "authors": "Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng", "title": "Single-Image Depth Perception in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies single-image depth perception in the wild, i.e.,\nrecovering depth from a single image taken in unconstrained settings. We\nintroduce a new dataset \"Depth in the Wild\" consisting of images in the wild\nannotated with relative depth between pairs of random points. We also propose a\nnew algorithm that learns to estimate metric depth using annotations of\nrelative depth. Compared to the state of the art, our algorithm is simpler and\nperforms better. Experiments show that our algorithm, combined with existing\nRGB-D data and our new relative depth annotations, significantly improves\nsingle-image depth perception in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 18:19:35 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 16:05:35 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Chen", "Weifeng", ""], ["Fu", "Zhao", ""], ["Yang", "Dawei", ""], ["Deng", "Jia", ""]]}, {"id": "1604.03915", "submitter": "Jialei Wang", "authors": "Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aurelie C. Lozano", "title": "Removing Clouds and Recovering Ground Observations in Satellite Image\n  Sequences via Temporally Contiguous Robust Matrix Completion", "comments": "To Appear In Conference on Computer Vision and Pattern Recognition\n  (CVPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of removing and replacing clouds in satellite image\nsequences, which has a wide range of applications in remote sensing. Our\napproach first detects and removes the cloud-contaminated part of the image\nsequences. It then recovers the missing scenes from the clean parts using the\nproposed \"TECROMAC\" (TEmporally Contiguous RObust MAtrix Completion) objective.\nThe objective function balances temporal smoothness with a low rank solution\nwhile staying close to the original observations. The matrix whose the rows are\npixels and columnsare days corresponding to the image, has low-rank because the\npixels reflect land-types such as vegetation, roads and lakes and there are\nrelatively few variations as a result. We provide efficient optimization\nalgorithms for TECROMAC, so we can exploit images containing millions of\npixels. Empirical results on real satellite image sequences, as well as\nsimulated data, demonstrate that our approach is able to recover underlying\nimages from heavily cloud-contaminated observations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:13:17 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Wang", "Jialei", ""], ["Olsen", "Peder A.", ""], ["Conn", "Andrew R.", ""], ["Lozano", "Aurelie C.", ""]]}, {"id": "1604.03968", "submitter": "Francis Ferraro", "authors": "Ting-Hao (Kenneth) Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan\n  Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet\n  Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende,\n  Michel Galley, Margaret Mitchell", "title": "Visual Storytelling", "comments": "to appear in NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 20:27:43 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Ting-Hao", "", "", "Kenneth"], ["Huang", "", ""], ["Ferraro", "Francis", ""], ["Mostafazadeh", "Nasrin", ""], ["Misra", "Ishan", ""], ["Agrawal", "Aishwarya", ""], ["Devlin", "Jacob", ""], ["Girshick", "Ross", ""], ["He", "Xiaodong", ""], ["Kohli", "Pushmeet", ""], ["Batra", "Dhruv", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Vanderwende", "Lucy", ""], ["Galley", "Michel", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1604.04004", "submitter": "Samuel Dodge", "authors": "Samuel Dodge and Lina Karam", "title": "Understanding How Image Quality Affects Deep Neural Networks", "comments": "Final version will appear in IEEE Xplore in the Proceedings of the\n  Conference on the Quality of Multimedia Experience (QoMEX), June 6-8, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality is an important practical challenge that is often overlooked in\nthe design of machine vision systems. Commonly, machine vision systems are\ntrained and tested on high quality image datasets, yet in practical\napplications the input images can not be assumed to be of high quality.\nRecently, deep neural networks have obtained state-of-the-art performance on\nmany machine vision tasks. In this paper we provide an evaluation of 4\nstate-of-the-art deep neural network models for image classification under\nquality distortions. We consider five types of quality distortions: blur,\nnoise, contrast, JPEG, and JPEG2000 compression. We show that the existing\nnetworks are susceptible to these quality distortions, particularly to blur and\nnoise. These results enable future work in developing deep neural networks that\nare more invariant to quality distortions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 00:47:50 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 20:44:52 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Dodge", "Samuel", ""], ["Karam", "Lina", ""]]}, {"id": "1604.04018", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, Xiang Bai", "title": "Multi-Oriented Text Detection with Fully Convolutional Networks", "comments": "Accepted by CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for text detec- tion in natural\nimages. Both local and global cues are taken into account for localizing text\nlines in a coarse-to-fine pro- cedure. First, a Fully Convolutional Network\n(FCN) model is trained to predict the salient map of text regions in a holistic\nmanner. Then, text line hypotheses are estimated by combining the salient map\nand character components. Fi- nally, another FCN classifier is used to predict\nthe centroid of each character, in order to remove the false hypotheses. The\nframework is general for handling text in multiple ori- entations, languages\nand fonts. The proposed method con- sistently achieves the state-of-the-art\nperformance on three text detection benchmarks: MSRA-TD500, ICDAR2015 and\nICDAR2013.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 02:37:05 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2016 13:22:03 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Zhang", "Zheng", ""], ["Zhang", "Chengquan", ""], ["Shen", "Wei", ""], ["Yao", "Cong", ""], ["Liu", "Wenyu", ""], ["Bai", "Xiang", ""]]}, {"id": "1604.04024", "submitter": "Eduardo Valle", "authors": "Michel Fornaciali, Micael Carvalho, Fl\\'avia Vasques Bittencourt,\n  Sandra Avila, Eduardo Valle", "title": "Towards Automated Melanoma Screening: Proper Computer Vision & Reliable\n  Results", "comments": "Minor corrections on State of the Art and Conclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we survey, analyze and criticize current art on automated\nmelanoma screening, reimplementing a baseline technique, and proposing two\nnovel ones. Melanoma, although highly curable when detected early, ends as one\nof the most dangerous types of cancer, due to delayed diagnosis and treatment.\nIts incidence is soaring, much faster than the number of trained professionals\nable to diagnose it. Automated screening appears as an alternative to make the\nmost of those professionals, focusing their time on the patients at risk while\nsafely discharging the other patients. However, the potential of automated\nmelanoma diagnosis is currently unfulfilled, due to the emphasis of current\nliterature on outdated computer vision models. Even more problematic is the\nirreproducibility of current art. We show how streamlined pipelines based upon\ncurrent Computer Vision outperform conventional models - a model based on an\nadvanced bags of words reaches an AUC of 84.6%, and a model based on deep\nneural networks reaches 89.3%, while the baseline (a classical bag of words)\nstays at 81.2%. We also initiate a dialog to improve reproducibility in our\ncommunity\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 03:26:28 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 02:45:27 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 21:00:22 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Fornaciali", "Michel", ""], ["Carvalho", "Micael", ""], ["Bittencourt", "Fl\u00e1via Vasques", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1604.04048", "submitter": "Wenqing Chu", "authors": "Wenqing Chu and Deng Cai", "title": "Deep Feature Based Contextual Model for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the most active areas in computer vision, which\nhas made significant improvement in recent years. Current state-of-the-art\nobject detection methods mostly adhere to the framework of regions with\nconvolutional neural network (R-CNN) and only use local appearance features\ninside object bounding boxes. Since these approaches ignore the contextual\ninformation around the object proposals, the outcome of these detectors may\ngenerate a semantically incoherent interpretation of the input image. In this\npaper, we propose an ensemble object detection system which incorporates the\nlocal appearance, the contextual information in term of relationships among\nobjects and the global scene based contextual feature generated by a\nconvolutional neural network. The system is formulated as a fully connected\nconditional random field (CRF) defined on object proposals and the contextual\nconstraints among object proposals are modeled as edges naturally. Furthermore,\na fast mean field approximation method is utilized to inference in this CRF\nmodel efficiently. The experimental results demonstrate that our approach\nachieves a higher mean average precision (mAP) on PASCAL VOC 2007 datasets\ncompared to the baseline algorithm Faster R-CNN.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 07:01:23 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Chu", "Wenqing", ""], ["Cai", "Deng", ""]]}, {"id": "1604.04053", "submitter": "Kai Kang", "authors": "Kai Kang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang", "title": "Object Detection from Video Tubelets with Convolutional Neural Networks", "comments": "Accepted in CVPR 2016 as a Spotlight paper", "journal-ref": "Computer Vision and Pattern Recognition (CVPR), 2016 IEEE\n  Conference on (pp. 817-825)", "doi": "10.1109/CVPR.2016.95", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Convolution Neural Networks (CNNs) have shown impressive performance in\nvarious vision tasks such as image classification, object detection and\nsemantic segmentation. For object detection, particularly in still images, the\nperformance has been significantly increased last year thanks to powerful deep\nnetworks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN\nfeatures (R-CNN)). The lately introduced ImageNet task on object detection from\nvideo (VID) brings the object detection task into the video domain, in which\nobjects' locations at each frame are required to be annotated with bounding\nboxes. In this work, we introduce a complete framework for the VID task based\non still-image object detection and general object tracking. Their relations\nand contributions in the VID task are thoroughly studied and evaluated. In\naddition, a temporal convolution network is proposed to incorporate temporal\ninformation to regularize the detection results and shows its effectiveness for\nthe task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 07:22:44 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kang", "Kai", ""], ["Ouyang", "Wanli", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1604.04112", "submitter": "Anish Shah", "authors": "Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde, Sandip Shingade", "title": "Deep Residual Networks with Exponential Linear Unit", "comments": "submitted in Vision Net 2016, Jaipur, India", "journal-ref": null, "doi": "10.1145/2983402.2983406", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional neural networks introduced new problems like\nvanishing gradient and degradation. The recent successful contributions towards\nsolving these problems are Residual and Highway Networks. These networks\nintroduce skip connections that allow the information (from the input or those\nlearned in earlier layers) to flow more into the deeper layers. These very deep\nmodels have lead to a considerable decrease in test errors, on benchmarks like\nImageNet and COCO. In this paper, we propose the use of exponential linear unit\ninstead of the combination of ReLU and Batch Normalization in Residual\nNetworks. We show that this not only speeds up learning in Residual Networks\nbut also improves the accuracy as the depth increases. It improves the test\nerror on almost all data sets, like CIFAR-10 and CIFAR-100\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 11:09:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 18:50:15 GMT"}, {"version": "v3", "created": "Sat, 20 Aug 2016 17:41:51 GMT"}, {"version": "v4", "created": "Wed, 5 Oct 2016 07:14:27 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Shah", "Anish", ""], ["Kadam", "Eashan", ""], ["Shah", "Hena", ""], ["Shinde", "Sameer", ""], ["Shingade", "Sandip", ""]]}, {"id": "1604.04125", "submitter": "Farahnaz Ahmed Wick", "authors": "Farahnaz Ahmed Wick, Michael L. Wick, Marc Pomplun", "title": "Filling in the details: Perceiving from low fidelity images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive their surroundings in great detail even though most of our\nvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input\nby the retina. In contrast, most deep learning architectures are\ncomputationally wasteful in that they consider every part of the input when\nperforming an image processing task. Yet, the human visual system is able to\nperform visual reasoning despite having only a small fovea of high visual\nacuity. With this in mind, we wish to understand the extent to which\nconnectionist architectures are able to learn from and reason with low acuity,\ndistorted inputs. Specifically, we train autoencoders to generate full-detail\nimages from low-detail \"foveations\" of those images and then measure their\nability to reconstruct the full-detail images from the foveated versions. By\nvarying the type of foveation, we can study how well the architectures can cope\nwith various types of distortion. We find that the autoencoder compensates for\nlower detail by learning increasingly global feature functions. In many cases,\nthe learnt features are suitable for reconstructing the original full-detail\nimage. For example, we find that the networks accurately perceive color in the\nperiphery, even when 75\\% of the input is achromatic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 12:10:23 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wick", "Farahnaz Ahmed", ""], ["Wick", "Michael L.", ""], ["Pomplun", "Marc", ""]]}, {"id": "1604.04142", "submitter": "Claudio Gennaro", "authors": "Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro", "title": "On Reducing the Number of Visual Words in the Bag-of-Features\n  Representation", "comments": null, "journal-ref": "VISAPP (1). 2013. p. 657-662", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of applications based on visual search engines are emerging,\nespecially on smart-phones that have evolved into powerful tools for processing\nimages and videos. The state-of-the-art algorithms for large visual content\nrecognition and content based similarity search today use the \"Bag of Features\"\n(BoF) or \"Bag of Words\" (BoW) approach. The idea, borrowed from text retrieval,\nenables the use of inverted files. A very well known issue with this approach\nis that the query images, as well as the stored data, are described with\nthousands of words. This poses obvious efficiency problems when using inverted\nfiles to perform efficient image matching. In this paper, we propose and\ncompare various techniques to reduce the number of words describing an image to\nimprove efficiency and we study the effects of this reduction on effectiveness\nin landmark recognition and retrieval scenarios. We show that very relevant\nimprovement in performance are achievable still preserving the advantages of\nthe BoF base approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 13:08:57 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""]]}, {"id": "1604.04144", "submitter": "Jason Kuen", "authors": "Jason Kuen, Kian Ming Lim, Chin Poo Lee", "title": "Self-taught learning of a deep invariant representation for visual\n  tracking via temporal slowness principle", "comments": "Pattern Recognition (Elsevier), 2015", "journal-ref": null, "doi": "10.1016/j.patcog.2015.02.012", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representation is crucial for a visual tracking method's performances.\nConventionally, visual representations adopted in visual tracking rely on\nhand-crafted computer vision descriptors. These descriptors were developed\ngenerically without considering tracking-specific information. In this paper,\nwe propose to learn complex-valued invariant representations from tracked\nsequential image patches, via strong temporal slowness constraint and stacked\nconvolutional autoencoders. The deep slow local representations are learned\noffline on unlabeled data and transferred to the observational model of our\nproposed tracker. The proposed observational model retains old training samples\nto alleviate drift, and collect negative samples which are coherent with\ntarget's motion pattern for better discriminative tracking. With the learned\nrepresentation and online training samples, a logistic regression classifier is\nadopted to distinguish target from background, and retrained online to adapt to\nappearance changes. Subsequently, the observational model is integrated into a\nparticle filter framework to peform visual tracking. Experimental results on\nvarious challenging benchmark sequences demonstrate that the proposed tracker\nperforms favourably against several state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 13:12:07 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Kuen", "Jason", ""], ["Lim", "Kian Ming", ""], ["Lee", "Chin Poo", ""]]}, {"id": "1604.04279", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Xinlei Chen, Abhinav Gupta", "title": "Learning Visual Storylines with Skipping Recurrent Neural Networks", "comments": "European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does a typical visit to Paris look like? Do people first take photos of\nthe Louvre and then the Eiffel Tower? Can we visually model a temporal event\nlike \"Paris Vacation\" using current frameworks? In this paper, we explore how\nwe can automatically learn the temporal aspects, or storylines of visual\nconcepts from web data. Previous attempts focus on consecutive image-to-image\ntransitions and are unsuccessful at recovering the long-term underlying story.\nOur novel Skipping Recurrent Neural Network (S-RNN) model does not attempt to\npredict each and every data point in the sequence, like classic RNNs. Rather,\nS-RNN uses a framework that skips through the images in the photo stream to\nexplore the space of all ordered subsets of the albums via an efficient\nsampling procedure. This approach reduces the negative impact of strong\nshort-term correlations, and recovers the latent story more accurately. We show\nhow our learned storylines can be used to analyze, predict, and summarize photo\nalbums from Flickr. Our experimental results provide strong qualitative and\nquantitative evidence that S-RNN is significantly better than other candidate\nmethods such as LSTMs on learning long-term correlations and recovering latent\nstorylines. Moreover, we show how storylines can help machines better\nunderstand and summarize photo streams by inferring a brief personalized story\nof each individual album.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 19:56:33 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 23:18:23 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Chen", "Xinlei", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1604.04293", "submitter": "Qi Wei", "authors": "Qi Wei, Marcus Chen, Jean-Yves Tourneret, Simon Godsill", "title": "Unsupervised Nonlinear Spectral Unmixing based on a Multilinear Mixing\n  Model", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2693366", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the community of remote sensing, nonlinear mixing models have recently\nreceived particular attention in hyperspectral image processing. In this paper,\nwe present a novel nonlinear spectral unmixing method following the recent\nmultilinear mixing model of [1], which includes an infinite number of terms\nrelated to interactions between different endmembers. The proposed unmixing\nmethod is unsupervised in the sense that the endmembers are estimated jointly\nwith the abundances and other parameters of interest, i.e., the transition\nprobability of undergoing further interactions. Non-negativity and sum-to one\nconstraints are imposed on abundances while only nonnegativity is considered\nfor endmembers. The resulting unmixing problem is formulated as a constrained\nnonlinear optimization problem, which is solved by a block coordinate descent\nstrategy, consisting of updating the endmembers, abundances and transition\nprobability iteratively. The proposed method is evaluated and compared with\nlinear unmixing methods for synthetic and real hyperspectral datasets acquired\nby the AVIRIS sensor. The advantage of using non-linear unmixing as opposed to\nlinear unmixing is clearly shown in these examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 20:09:22 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wei", "Qi", ""], ["Chen", "Marcus", ""], ["Tourneret", "Jean-Yves", ""], ["Godsill", "Simon", ""]]}, {"id": "1604.04326", "submitter": "Stephan Zheng", "authors": "Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow", "title": "Improving the Robustness of Deep Neural Networks via Stability Training", "comments": "Published in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the issue of output instability of deep neural\nnetworks: small perturbations in the visual input can significantly distort the\nfeature embeddings and output of a neural network. Such instability affects\nmany deep architectures with state-of-the-art performance on a wide range of\ncomputer vision tasks. We present a general stability training method to\nstabilize deep networks against small input distortions that result from\nvarious types of common image processing, such as compression, rescaling, and\ncropping. We validate our method by stabilizing the state-of-the-art Inception\narchitecture against these types of distortions. In addition, we demonstrate\nthat our stabilized model gives robust state-of-the-art performance on\nlarge-scale near-duplicate detection, similar-image ranking, and classification\non noisy datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 01:15:18 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Zheng", "Stephan", ""], ["Song", "Yang", ""], ["Leung", "Thomas", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1604.04327", "submitter": "Thusitha Chandrapala", "authors": "Thusitha N. Chandrapala and Bertram E. Shi", "title": "Invariant feature extraction from event based stimuli", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture, the event-based GASSOM for learning and\nextracting invariant representations from event streams originating from\nneuromorphic vision sensors. The framework is inspired by feed-forward cortical\nmodels for visual processing. The model, which is based on the concepts of\nsparsity and temporal slowness, is able to learn feature extractors that\nresemble neurons in the primary visual cortex. Layers of units in the proposed\nmodel can be cascaded to learn feature extractors with different levels of\ncomplexity and selectivity. We explore the applicability of the framework on\nreal world tasks by using the learned network for object recognition. The\nproposed model achieve higher classification accuracy compared to other\nstate-of-the-art event based processing methods. Our results also demonstrate\nthe generality and robustness of the method, as the recognizers for different\ndata sets and different tasks all used the same set of learned feature\ndetectors, which were trained on data collected independently of the testing\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 01:18:29 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 01:27:33 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 04:38:31 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Chandrapala", "Thusitha N.", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1604.04333", "submitter": "Miao Sun", "authors": "Miao Sun, Tony X. Han, Xun Xu, Ming-Chang Liu, Ahmad\n  Khodayari-Rostamabad", "title": "Latent Model Ensemble with Auto-localization", "comments": "International Conference on Pattern Recognition (ICPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 02:07:42 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 01:57:19 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sun", "Miao", ""], ["Han", "Tony X.", ""], ["Xu", "Xun", ""], ["Liu", "Ming-Chang", ""], ["Khodayari-Rostamabad", "Ahmad", ""]]}, {"id": "1604.04334", "submitter": "Deepak Ghimire", "authors": "Deepak Ghimire, Joonwhoan Lee, Ze-Nian Li, Sunghwan Jeong", "title": "Recognition of facial expressions based on salient geometric features\n  and support vector machines", "comments": "Facial points, Geometric features, AdaBoost, Extreme learning\n  machine, Support vector machines, Facial expression recognitions", "journal-ref": "Multimedia Tools and Applications (2016): 1-26", "doi": "10.1007/s11042-016-3428-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions convey nonverbal cues which play an important role in\ninterpersonal relations, and are widely used in behavior interpretation of\nemotions, cognitive science, and social interactions. In this paper we analyze\ndifferent ways of representing geometric feature and present a fully automatic\nfacial expression recognition (FER) system using salient geometric features. In\ngeometric feature-based FER approach, the first important step is to initialize\nand track dense set of facial points as the expression evolves over time in\nconsecutive frames. In the proposed system, facial points are initialized using\nelastic bunch graph matching (EBGM) algorithm and tracking is performed using\nKanade-Lucas-Tomaci (KLT) tracker. We extract geometric features from point,\nline and triangle composed of tracking results of facial points. The most\ndiscriminative line and triangle features are extracted using feature selective\nmulti-class AdaBoost with the help of extreme learning machine (ELM)\nclassification. Finally the geometric features for FER are extracted from the\nboosted line, and triangles composed of facial points. The recognition accuracy\nusing features from point, line and triangle are analyzed independently. The\nperformance of the proposed FER system is evaluated on three different data\nsets: namely CK+, MMI and MUG facial expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 02:18:11 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Ghimire", "Deepak", ""], ["Lee", "Joonwhoan", ""], ["Li", "Ze-Nian", ""], ["Jeong", "Sunghwan", ""]]}, {"id": "1604.04337", "submitter": "Deepak Ghimire", "authors": "Deepak Ghimire, Sunghwan Jeong, Joonwhoan Lee, Sang Hyun Park", "title": "Facial expression recognition based on local region specific features\n  and support vector machines", "comments": "Facial expressions, Local representation, Appearance features,\n  Geometric features, Support vector machines", "journal-ref": "Multimedia Tools and Applications, pp 1-19, Online: 16 March 2016", "doi": "10.1007/s11042-016-3418-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are one of the most powerful, natural and immediate means\nfor human being to communicate their emotions and intensions. Recognition of\nfacial expression has many applications including human-computer interaction,\ncognitive science, human emotion analysis, personality development etc. In this\npaper, we propose a new method for the recognition of facial expressions from\nsingle image frame that uses combination of appearance and geometric features\nwith support vector machines classification. In general, appearance features\nfor the recognition of facial expressions are computed by dividing face region\ninto regular grid (holistic representation). But, in this paper we extracted\nregion specific appearance features by dividing the whole face region into\ndomain specific local regions. Geometric features are also extracted from\ncorresponding domain specific regions. In addition, important local regions are\ndetermined by using incremental search approach which results in the reduction\nof feature dimension and improvement in recognition accuracy. The results of\nfacial expressions recognition using features from domain specific regions are\nalso compared with the results obtained using holistic representation. The\nperformance of the proposed facial expression recognition system has been\nvalidated on publicly available extended Cohn-Kanade (CK+) facial expression\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 02:27:21 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Ghimire", "Deepak", ""], ["Jeong", "Sunghwan", ""], ["Lee", "Joonwhoan", ""], ["Park", "Sang Hyun", ""]]}, {"id": "1604.04339", "submitter": "Chunhua Shen", "authors": "Zifeng Wu, Chunhua Shen, Anton van den Hengel", "title": "High-performance Semantic Segmentation Using Very Deep Fully\n  Convolutional Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for high-performance semantic image segmentation (or\nsemantic pixel labelling) based on very deep residual networks, which achieves\nthe state-of-the-art performance. A few design factors are carefully considered\nto this end.\n  We make the following contributions. (i) First, we evaluate different\nvariations of a fully convolutional residual network so as to find the best\nconfiguration, including the number of layers, the resolution of feature maps,\nand the size of field-of-view. Our experiments show that further enlarging the\nfield-of-view and increasing the resolution of feature maps are typically\nbeneficial, which however inevitably leads to a higher demand for GPU memories.\nTo walk around the limitation, we propose a new method to simulate a high\nresolution network with a low resolution network, which can be applied during\ntraining and/or testing. (ii) Second, we propose an online bootstrapping method\nfor training. We demonstrate that online bootstrapping is critically important\nfor achieving good accuracy. (iii) Third we apply the traditional dropout to\nsome of the residual blocks, which further improves the performance. (iv)\nFinally, our method achieves the currently best mean intersection-over-union\n78.3\\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset\nCityscapes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 02:52:46 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Wu", "Zifeng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1604.04372", "submitter": "Joe Kileel", "authors": "Gunnar Fl{\\o}ystad, Joe Kileel, Giorgio Ottaviani", "title": "The Chow Form of the Essential Variety in Computer Vision", "comments": "27 pages, 1 figure, 6 Macaulay2 ancillary files. v2: edits to Theorem\n  1.1, references, acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chow form of the essential variety in computer vision is calculated. Our\nderivation uses secant varieties, Ulrich sheaves and representation theory.\nNumerical experiments show that our formula can detect noisy point\ncorrespondences between two images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 06:57:09 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 18:09:12 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Fl\u00f8ystad", "Gunnar", ""], ["Kileel", "Joe", ""], ["Ottaviani", "Giorgio", ""]]}, {"id": "1604.04377", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Liang Lin, Shengyong Ding, Ya Li and Qing Wang", "title": "DARI: Distance metric And Representation Integration for Person\n  Verification", "comments": "To appear in Proceedings of AAAI Conference on Artificial\n  Intelligence (AAAI), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has witnessed the rapid development of feature representation\nlearning and distance metric learning, whereas the two steps are often\ndiscussed separately. To explore their interaction, this work proposes an\nend-to-end learning framework called DARI, i.e. Distance metric And\nRepresentation Integration, and validates the effectiveness of DARI in the\nchallenging task of person verification. Given the training images annotated\nwith the labels, we first produce a large number of triplet units, and each one\ncontains three images, i.e. one person and the matched/mismatch references. For\neach triplet unit, the distance disparity between the matched pair and the\nmismatched pair tends to be maximized. We solve this objective by building a\ndeep architecture of convolutional neural networks. In particular, the\nMahalanobis distance matrix is naturally factorized as one top fully-connected\nlayer that is seamlessly integrated with other bottom layers representing the\nimage feature. The image feature and the distance metric can be thus\nsimultaneously optimized via the one-shot backward propagation. On several\npublic datasets, DARI shows very promising performance on re-identifying\nindividuals cross cameras against various challenges, and outperforms other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 07:21:26 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Wang", "Guangrun", ""], ["Lin", "Liang", ""], ["Ding", "Shengyong", ""], ["Li", "Ya", ""], ["Wang", "Qing", ""]]}, {"id": "1604.04382", "submitter": "Chuan Li", "authors": "Chuan Li and Michael Wand", "title": "Precomputed Real-Time Texture Synthesis with Markovian Generative\n  Adversarial Networks", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Markovian Generative Adversarial Networks (MGANs), a\nmethod for training generative neural networks for efficient texture synthesis.\nWhile deep neural network approaches have recently demonstrated remarkable\nresults in terms of synthesis quality, they still come at considerable\ncomputational costs (minutes of run-time for low-res images). Our paper\naddresses this efficiency issue. Instead of a numerical deconvolution in\nprevious work, we precompute a feed-forward, strided convolutional network that\ncaptures the feature statistics of Markovian patches and is able to directly\ngenerate outputs of arbitrary dimensions. Such network can directly decode\nbrown noise to realistic texture, or photos to artistic paintings. With\nadversarial training, we obtain quality comparable to recent neural texture\nsynthesis methods. As no optimization is required any longer at generation\ntime, our run-time performance (0.25M pixel images at 25Hz) surpasses previous\nneural texture synthesizers by a significant margin (at least 500 times\nfaster). We apply this idea to texture synthesis, style transfer, and video\nstylization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 07:32:18 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Li", "Chuan", ""], ["Wand", "Michael", ""]]}, {"id": "1604.04393", "submitter": "Subhradeep Kayal", "authors": "Subhradeep Kayal", "title": "Unsupervised Image Segmentation using the Deffuant-Weisbuch Model from\n  Social Dynamics", "comments": "This paper is under consideration at Signal Image and Video\n  Processing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image segmentation algorithms aim at identifying disjoint\nhomogeneous regions in an image, and have been subject to considerable\nattention in the machine vision community. In this paper, a popular theoretical\nmodel with it's origins in statistical physics and social dynamics, known as\nthe Deffuant-Weisbuch model, is applied to the image segmentation problem. The\nDeffuant-Weisbuch model has been found to be useful in modelling the evolution\nof a closed system of interacting agents characterised by their opinions or\nbeliefs, leading to the formation of clusters of agents who share a similar\nopinion or belief at steady state. In the context of image segmentation, this\npaper considers a pixel as an agent and it's colour property as it's opinion,\nwith opinion updates as per the Deffuant-Weisbuch model. Apart from applying\nthe basic model to image segmentation, this paper incorporates adjacency and\nneighbourhood information in the model, which factors in the local similarity\nand smoothness properties of images. Convergence is reached when the number of\nunique pixel opinions, i.e., the number of colour centres, matches the\npre-specified number of clusters. Experiments are performed on a set of images\nfrom the Berkeley Image Segmentation Dataset and the results are analysed both\nqualitatively and quantitatively, which indicate that this simple and intuitive\nmethod is promising for image segmentation. To the best of the knowledge of the\nauthor, this is the first work where a theoretical model from statistical\nphysics and social dynamics has been successfully applied to image processing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 08:01:15 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 14:10:40 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 21:01:42 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Kayal", "Subhradeep", ""]]}, {"id": "1604.04397", "submitter": "Sven Puchinger", "authors": "Sven M\\\"uelich, Sven Puchinger, Martin Bossert", "title": "Low-Rank Matrix Recovery using Gabidulin Codes in Characteristic Zero", "comments": "6 pages, presented at the International Workshop on Algebraic and\n  Combinatorial Coding Theory (ACCT) 2016, submitted to Electronic Notes in\n  Discrete Mathematics (volume devoted to ACCT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach on low-rank matrix recovery (LRMR) based on\nGabidulin Codes. Since most applications of LRMR deal with matrices over\ninfinite fields, we use the recently introduced generalization of Gabidulin\ncodes to fields of characterstic zero. We show that LRMR can be reduced to\ndecoding of Gabidulin codes and discuss which field extensions can be used in\nthe code construction.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 08:19:49 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 07:30:30 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["M\u00fcelich", "Sven", ""], ["Puchinger", "Sven", ""], ["Bossert", "Martin", ""]]}, {"id": "1604.04473", "submitter": "Xiaopeng Hong", "authors": "Xiaopeng Hong, Xianbiao Qi, Guoying Zhao, Matti Pietik\\\"ainen", "title": "Probing the Intra-Component Correlations within Fisher Vector for\n  Material Classification", "comments": "It is manuscript submitted to Neurocomputing on the end of April,\n  2015 (!). One year past but no review comments we received yet!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher vector (FV) has become a popular image representation. One notable\nunderlying assumption of the FV framework is that local descriptors are well\ndecorrelated within each cluster so that the covariance matrix for each\nGaussian can be simplified to be diagonal. Though the FV usually relies on the\nPrincipal Component Analysis (PCA) to decorrelate local features, the PCA is\napplied to the entire training data and hence it only diagonalizes the\n\\textit{universal} covariance matrix, rather than those w.r.t. the local\ncomponents. As a result, the local decorrelation assumption is usually not\nsupported in practice.\n  To relax this assumption, this paper proposes a completed model of the Fisher\nvector, which is termed as the Completed Fisher vector (CFV). The CFV is a more\ngeneral framework of the FV, since it encodes not only the variances but also\nthe correlations of the whitened local descriptors. The CFV thus leads to\nimproved discriminative power. We take the task of material categorization as\nan example and experimentally show that: 1) the CFV outperforms the FV under\nall parameter settings; 2) the CFV is robust to the changes in the number of\ncomponents in the mixture; 3) even with a relatively small visual vocabulary\nthe CFV still works well on two challenging datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 12:55:00 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Hong", "Xiaopeng", ""], ["Qi", "Xianbiao", ""], ["Zhao", "Guoying", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1604.04494", "submitter": "G\\\"ul Varol", "authors": "G\\\"ul Varol, Ivan Laptev and Cordelia Schmid", "title": "Long-term Temporal Convolutions for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical human actions last several seconds and exhibit characteristic\nspatio-temporal structure. Recent methods attempt to capture this structure and\nlearn action representations with convolutional neural networks. Such\nrepresentations, however, are typically learned at the level of a few video\nframes failing to model actions at their full temporal extent. In this work we\nlearn video representations using neural networks with long-term temporal\nconvolutions (LTC). We demonstrate that LTC-CNN models with increased temporal\nextents improve the accuracy of action recognition. We also study the impact of\ndifferent low-level representations, such as raw values of video pixels and\noptical flow vector fields and demonstrate the importance of high-quality\noptical flow estimation for learning accurate action models. We report\nstate-of-the-art results on two challenging benchmarks for human action\nrecognition UCF101 (92.7%) and HMDB51 (67.2%).\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 13:33:13 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 12:08:57 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Varol", "G\u00fcl", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1604.04528", "submitter": "Youngbin Park", "authors": "Youngbin Park, Sungphill Moon and Il Hong Suh", "title": "Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks", "comments": "submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinect skeleton tracker is able to achieve considerable human body tracking\nperformance in convenient and a low-cost manner. However, The tracker often\ncaptures unnatural human poses such as discontinuous and vibrated motions when\nself-occlusions occur. A majority of approaches tackle this problem by using\nmultiple Kinect sensors in a workspace. Combination of the measurements from\ndifferent sensors is then conducted in Kalman filter framework or optimization\nproblem is formulated for sensor fusion. However, these methods usually require\nheuristics to measure reliability of measurements observed from each Kinect\nsensor. In this paper, we developed a method to improve Kinect skeleton using\nsingle Kinect sensor, in which supervised learning technique was employed to\ncorrect unnatural tracking motions. Specifically, deep recurrent neural\nnetworks were used for improving joint positions and velocities of Kinect\nskeleton, and three methods were proposed to integrate the refined positions\nand velocities for further enhancement. Moreover, we suggested a novel measure\nto evaluate naturalness of captured motions. We evaluated the proposed approach\nby comparison with the ground truth obtained using a commercial optical\nmaker-based motion capture system.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 14:55:27 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Park", "Youngbin", ""], ["Moon", "Sungphill", ""], ["Suh", "Il Hong", ""]]}, {"id": "1604.04539", "submitter": "Youdong Mao", "authors": "Jiayi Wu, Yong-Bei Ma, Charles Congdon, Bevin Brett, Shuobing Chen, Qi\n  Ouyang, Youdong Mao", "title": "Unsupervised single-particle deep clustering via statistical manifold\n  learning", "comments": "29 pages, 5 figures", "journal-ref": "PLoS ONE 12, e0182130 (2017)", "doi": "10.1371/journal.pone.0182130", "report-no": null, "categories": "physics.data-an cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Structural heterogeneity in single-particle cryo-electron\nmicroscopy (cryo-EM) data represents a major challenge for high-resolution\nstructure determination. Unsupervised classification may serve as the first\nstep in the assessment of structural heterogeneity. Traditional algorithms for\nunsupervised classification, such as K-means clustering and maximum likelihood\noptimization, may classify images into wrong classes with decreasing\nsignal-to-noise-ratio (SNR) in the image data, yet demand increased cost in\ncomputation. Overcoming these limitations requires further development on\nclustering algorithms for high-performance cryo-EM data analysis. Results: Here\nwe introduce a statistical manifold learning algorithm for unsupervised\nsingle-particle deep clustering. We show that statistical manifold learning\nimproves classification accuracy by about 40% in the absence of input\nreferences for lower SNR data. Applications to several experimental datasets\nsuggest that our deep clustering approach can detect subtle structural\ndifference among classes. Through code optimization over the Intel\nhigh-performance computing (HPC) processors, our software implementation can\ngenerate thousands of reference-free class averages within several hours from\nhundreds of thousands of single-particle cryo-EM images, which allows\nsignificant improvement in ab initio 3D reconstruction resolution and quality.\nOur approach has been successfully applied in several structural determination\nprojects. We expect that it provides a powerful computational tool in analyzing\nhighly heterogeneous structural data and assisting in computational\npurification of single-particle datasets for high-resolution reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 15:28:39 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 07:43:07 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wu", "Jiayi", ""], ["Ma", "Yong-Bei", ""], ["Congdon", "Charles", ""], ["Brett", "Bevin", ""], ["Chen", "Shuobing", ""], ["Ouyang", "Qi", ""], ["Mao", "Youdong", ""]]}, {"id": "1604.04573", "submitter": "Jiang Wang Mr.", "authors": "Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu", "title": "CNN-RNN: A Unified Framework for Multi-label Image Classification", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks (CNNs) have shown a great success in\nsingle-label image classification, it is important to note that real world\nimages generally contain multiple labels, which could correspond to different\nobjects, scenes, actions and attributes in an image. Traditional approaches to\nmulti-label image classification learn independent classifiers for each\ncategory and employ ranking or thresholding on the classification results.\nThese techniques, although working well, fail to explicitly exploit the label\ndependencies in an image. In this paper, we utilize recurrent neural networks\n(RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN\nframework learns a joint image-label embedding to characterize the semantic\nlabel dependency as well as the image-label relevance, and it can be trained\nend-to-end from scratch to integrate both information in a unified framework.\nExperimental results on public benchmark datasets demonstrate that the proposed\narchitecture achieves better performance than the state-of-the-art multi-label\nclassification model\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 17:10:54 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Wang", "Jiang", ""], ["Yang", "Yi", ""], ["Mao", "Junhua", ""], ["Huang", "Zhiheng", ""], ["Huang", "Chang", ""], ["Xu", "Wei", ""]]}, {"id": "1604.04574", "submitter": "Jonghyun Choi", "authors": "Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury,\n  Larry S. Davis", "title": "Learning Temporal Regularity in Video Sequences", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving meaningful activities in a long video sequence is a challenging\nproblem due to ambiguous definition of 'meaningfulness' as well as clutters in\nthe scene. We approach this problem by learning a generative model for regular\nmotion patterns, termed as regularity, using multiple sources with very limited\nsupervision. Specifically, we propose two methods that are built upon the\nautoencoders for their ability to work with little to no supervision. We first\nleverage the conventional handcrafted spatio-temporal local features and learn\na fully connected autoencoder on them. Second, we build a fully convolutional\nfeed-forward autoencoder to learn both the local features and the classifiers\nas an end-to-end learning framework. Our model can capture the regularities\nfrom multiple datasets. We evaluate our methods in both qualitative and\nquantitative ways - showing the learned regularity of videos in various aspects\nand demonstrating competitive performance on anomaly detection datasets as an\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 17:20:01 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Hasan", "Mahmudul", ""], ["Choi", "Jonghyun", ""], ["Neumann", "Jan", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1604.04653", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Amaia Salvador, Kevin McGuinness, Ferran Marques, Noel\n  E. O'Connor and Xavier Giro-i-Nieto", "title": "Bags of Local Convolutional Features for Scalable Instance Search", "comments": "Preprint of a short paper accepted in the ACM International\n  Conference on Multimedia Retrieval (ICMR) 2016 (New York City, NY, USA)", "journal-ref": null, "doi": "10.1145/2911996.2912061", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 22:02:22 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Mohedano", "Eva", ""], ["Salvador", "Amaia", ""], ["McGuinness", "Kevin", ""], ["Marques", "Ferran", ""], ["O'Connor", "Noel E.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1604.04673", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh, Shahryar Rahnamayan", "title": "Evolutionary Projection Selection for Radon Barcodes", "comments": "To appear in proceedings of The 2016 IEEE Congress on Evolutionary\n  Computation (IEEE CEC 2016), July 24-29, 2016, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Radon transformation has been used to generate barcodes for tagging\nmedical images. The under-sampled image is projected in certain directions, and\neach projection is binarized using a local threshold. The concatenation of the\nthresholded projections creates a barcode that can be used for tagging or\nannotating medical images. A small number of equidistant projections, e.g., 4\nor 8, is generally used to generate short barcodes. However, due to the diverse\nnature of digital images, and since we are only working with a small number of\nprojections (to keep the barcode short), taking equidistant projections may not\nbe the best course of action. In this paper, we proposed to find $n$ optimal\nprojections, whereas $n\\!<\\!180$, in order to increase the expressiveness of\nRadon barcodes. We show examples for the exhaustive search for the simple case\nwhen we attempt to find 4 best projections out of 16 equidistant projections\nand compare it with the evolutionary approach in order to establish the benefit\nof the latter when operating on a small population size as in the case of\nmicro-DE. We randomly selected 10 different classes from IRMA dataset (14,400\nx-ray images in 58 classes) and further randomly selected 5 images per class\nfor our tests.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 00:48:52 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Rahnamayan", "Shahryar", ""]]}, {"id": "1604.04675", "submitter": "Hamid Tizhoosh", "authors": "Shujin Zhu, H.R.Tizhoosh", "title": "Radon Features and Barcodes for Medical Image Retrieval via SVM", "comments": "To appear in proceedings of The 2016 IEEE International Joint\n  Conference on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than two decades, research has been performed on content-based image\nretrieval (CBIR). By combining Radon projections and the support vector\nmachines (SVM), a content-based medical image retrieval method is presented in\nthis work. The proposed approach employs the normalized Radon projections with\ncorresponding image category labels to build an SVM classifier, and the Radon\nbarcode database which encodes every image in a binary format is also generated\nsimultaneously to tag all images. To retrieve similar images when a query image\nis given, Radon projections and the barcode of the query image are generated.\nSubsequently, the k-nearest neighbor search method is applied to find the\nimages with minimum Hamming distance of the Radon barcode within the same class\npredicted by the trained SVM classifier that uses Radon features. The\nperformance of the proposed method is validated by using the IRMA 2009 dataset\nwith 14,410 x-ray images in 57 categories. The results demonstrate that our\nmethod has the capacity to retrieve similar responses for the correctly\nidentified query image and even for those mistakenly classified by SVM. The\napproach further is very fast and has low memory requirement.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 01:13:23 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Zhu", "Shujin", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1604.04676", "submitter": "Hamid Tizhoosh", "authors": "Xinran Liu, Hamid R. Tizhoosh, Jonathan Kofman", "title": "Generating Binary Tags for Fast Medical Image Retrieval Based on\n  Convolutional Nets and Radon Transform", "comments": "To appear in proceedings of The 2016 IEEE International Joint\n  Conference on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) in large medical image archives is a\nchallenging and necessary task. Generally, different feature extraction methods\nare used to assign expressive and invariant features to each image such that\nthe search for similar images comes down to feature classification and/or\nmatching. The present work introduces a new image retrieval method for medical\napplications that employs a convolutional neural network (CNN) with recently\nintroduced Radon barcodes. We combine neural codes for global classification\nwith Radon barcodes for the final retrieval. We also examine image search based\non regions of interest (ROI) matching after image retrieval. The IRMA dataset\nwith more than 14,000 x-rays images is used to evaluate the performance of our\nmethod. Experimental results show that our approach is superior to many\npublished works.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 01:30:01 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Liu", "Xinran", ""], ["Tizhoosh", "Hamid R.", ""], ["Kofman", "Jonathan", ""]]}, {"id": "1604.04678", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh, Ahmed A. Othman", "title": "Anatomy-Aware Measurement of Segmentation Accuracy", "comments": "To appear in SPIE Medical Imaging 2016", "journal-ref": null, "doi": "10.1117/12.2214869", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the accuracy of segmentation and manual delineation of organs,\ntissue types and tumors in medical images is a necessary measurement that\nsuffers from multiple problems. One major shortcoming of all accuracy measures\nis that they neglect the anatomical significance or relevance of different\nzones within a given segment. Hence, existing accuracy metrics measure the\noverlap of a given segment with a ground-truth without any anatomical\ndiscrimination inside the segment. For instance, if we understand the rectal\nwall or urethral sphincter as anatomical zones, then current accuracy measures\nignore their significance when they are applied to assess the quality of the\nprostate gland segments. In this paper, we propose an anatomy-aware measurement\nscheme for segmentation accuracy of medical images. The idea is to create a\n``master gold'' based on a consensus shape containing not just the outline of\nthe segment but also the outlines of the internal zones if existent or\nrelevant. To apply this new approach to accuracy measurement, we introduce the\nanatomy-aware extensions of both Dice coefficient and Jaccard index and\ninvestigate their effect using 500 synthetic prostate ultrasound images with 20\ndifferent segments for each image. We show that through anatomy-sensitive\ncalculation of segmentation accuracy, namely by considering relevant anatomical\nzones, not only the measurement of individual users can change but also the\nranking of users' segmentation skills may require reordering.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 01:49:22 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Othman", "Ahmed A.", ""]]}, {"id": "1604.04693", "submitter": "Yu Xiang", "authors": "Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese", "title": "Subcategory-aware Convolutional Neural Networks for Object Proposals and\n  Detection", "comments": "Published in WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In CNN-based object detection methods, region proposal becomes a bottleneck\nwhen objects exhibit significant scale variation, occlusion or truncation. In\naddition, these methods mainly focus on 2D object detection and cannot estimate\ndetailed properties of objects. In this paper, we propose subcategory-aware\nCNNs for object detection. We introduce a novel region proposal network that\nuses subcategory information to guide the proposal generating process, and a\nnew detection network for joint detection and subcategory classification. By\nusing subcategories related to object pose, we achieve state-of-the-art\nperformance on both detection and pose estimation on commonly used benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 05:05:32 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 05:47:51 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 01:19:53 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Xiang", "Yu", ""], ["Choi", "Wongun", ""], ["Lin", "Yuanqing", ""], ["Savarese", "Silvio", ""]]}, {"id": "1604.04724", "submitter": "Shanmuganathan Raman", "authors": "Sri Raghu Malireddi, Shanmuganathan Raman", "title": "Automatic Segmentation of Dynamic Objects from an Image Pair", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of objects from a single image is a challenging\nproblem which generally requires training on large number of images. We\nconsider the problem of automatically segmenting only the dynamic objects from\na given pair of images of a scene captured from different positions. We exploit\ndense correspondences along with saliency measures in order to first localize\nthe interest points on the dynamic objects from the two images. We propose a\nnovel approach based on techniques from computational geometry in order to\nautomatically segment the dynamic objects from both the images using a top-down\nsegmentation strategy. We discuss how the proposed approach is unique in\nnovelty compared to other state-of-the-art segmentation algorithms. We show\nthat the proposed approach for segmentation is efficient in handling large\nmotions and is able to achieve very good segmentation of the objects for\ndifferent scenes. We analyse the results with respect to the manually marked\nground truth segmentation masks created using our own dataset and provide key\nobservations in order to improve the work in future.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 11:00:24 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Malireddi", "Sri Raghu", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1604.04767", "submitter": "Markus Thom", "authors": "Markus Thom, Matthias Rapp, G\\\"unther Palm", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s11263-015-0799-8", "journal-ref": "International Journal of Computer Vision, vol. 114, no. 2, pp.\n  168-194, 2015", "doi": "10.1007/s11263-015-0799-8", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning dictionaries suitable for sparse coding instead of using engineered\nbases has proven effective in a variety of image processing tasks. This paper\nstudies the optimization of dictionaries on image data where the representation\nis enforced to be explicitly sparse with respect to a smooth, normalized\nsparseness measure. This involves the computation of Euclidean projections onto\nlevel sets of the sparseness measure. While previous algorithms for this\noptimization problem had at least quasi-linear time complexity, here the first\nalgorithm with linear time complexity and constant space complexity is\nproposed. The key for this is the mathematically rigorous derivation of a\ncharacterization of the projection's result based on a soft-shrinkage function.\nThis theory is applied in an original algorithm called Easy Dictionary Learning\n(EZDL), which learns dictionaries with a simple and fast-to-compute\nHebbian-like learning rule. The new algorithm is efficient, expressive and\nparticularly simple to implement. It is demonstrated that despite its\nsimplicity, the proposed learning algorithm is able to generate a rich variety\nof dictionaries, in particular a topographic organization of atoms or separable\natoms. Further, the dictionaries are as expressive as those of benchmark\nlearning algorithms in terms of the reproduction quality on entire images, and\nresult in an equivalent denoising performance. EZDL learns approximately 30 %\nfaster than the already very efficient Online Dictionary Learning algorithm,\nand is therefore eligible for rapid data set analysis and problems with vast\nquantities of learning samples.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 15:42:12 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thom", "Markus", ""], ["Rapp", "Matthias", ""], ["Palm", "G\u00fcnther", ""]]}, {"id": "1604.04784", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Chen Sun, Ram Nevatia", "title": "ACD: Action Concept Discovery from Image-Sentence Corpora", "comments": "8 pages, accepted by ICMR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action classification in still images is an important task in computer\nvision. It is challenging as the appearances of ac- tions may vary depending on\ntheir context (e.g. associated objects). Manually labeling of context\ninformation would be time consuming and difficult to scale up. To address this\nchallenge, we propose a method to automatically discover and cluster action\nconcepts, and learn their classifiers from weakly supervised image-sentence\ncorpora. It obtains candidate action concepts by extracting verb-object pairs\nfrom sentences and verifies their visualness with the associated images.\nCandidate action concepts are then clustered by using a multi-modal\nrepresentation with image embeddings from deep convolutional networks and text\nembeddings from word2vec. More than one hundred human action concept\nclassifiers are learned from the Flickr 30k dataset with no additional human\neffort and promising classification results are obtained. We further apply the\nAdaBoost algorithm to automatically select and combine relevant action concepts\ngiven an action query. Promising results have been shown on the PASCAL VOC 2012\naction classification benchmark, which has zero overlap with Flickr30k.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 18:26:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Gao", "Jiyang", ""], ["Sun", "Chen", ""], ["Nevatia", "Ram", ""]]}, {"id": "1604.04808", "submitter": "Arun Mallya", "authors": "Arun Mallya and Svetlana Lazebnik", "title": "Learning Models for Actions and Person-Object Interactions with Transfer\n  to Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes deep convolutional network models that utilize local and\nglobal context to make human activity label predictions in still images,\nachieving state-of-the-art performance on two recent datasets with hundreds of\nlabels each. We use multiple instance learning to handle the lack of\nsupervision on the level of individual person instances, and weighted loss to\nhandle unbalanced training data. Further, we show how specialized features\ntrained on these datasets can be used to improve accuracy on the Visual\nQuestion Answering (VQA) task, in the form of multiple choice fill-in-the-blank\nquestions (Visual Madlibs). Specifically, we tackle two types of questions on\nperson activity and person-object relationship and show improvements over\ngeneric features trained on the ImageNet classification task.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 22:54:05 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 04:44:36 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Mallya", "Arun", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1604.04825", "submitter": "Sourya Roy", "authors": "Sourya Roy, Pabitra Mitra", "title": "Visual saliency detection: a Kalman filter based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a Kalman filter aided saliency detection model which\nis based on the conjecture that salient regions are considerably different from\nour \"visual expectation\" or they are \"visually surprising\" in nature. In this\nwork, we have structured our model with an immediate objective to predict\nsaliency in static images. However, the proposed model can be easily extended\nfor space-time saliency prediction. Our approach was evaluated using two\npublicly available benchmark data sets and results have been compared with\nother existing saliency models. The results clearly illustrate the superior\nperformance of the proposed model over other approaches.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 03:48:45 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Roy", "Sourya", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1604.04842", "submitter": "Chao-Yeh Chen", "authors": "Chao-Yeh Chen and Kristen Grauman", "title": "Subjects and Their Objects: Localizing Interactees for a Person-Centric\n  View of Importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding images with people often entails understanding their\n\\emph{interactions} with other objects or people. As such, given a novel image,\na vision system ought to infer which other objects/people play an important\nrole in a given person's activity. However, existing methods are limited to\nlearning action-specific interactions (e.g., how the pose of a tennis player\nrelates to the position of his racquet when serving the ball) for improved\nrecognition, making them unequipped to reason about novel interactions with\nactions or objects unobserved in the training data.\n  We propose to predict the \"interactee\" in novel images---that is, to localize\nthe \\emph{object} of a person's action. Given an arbitrary image with a\ndetected person, the goal is to produce a saliency map indicating the most\nlikely positions and scales where that person's interactee would be found. To\nthat end, we explore ways to learn the generic, action-independent connections\nbetween (a) representations of a person's pose, gaze, and scene cues and (b)\nthe interactee object's position and scale. We provide results on a newly\ncollected UT Interactee dataset spanning more than 10,000 images from SUN,\nPASCAL, and COCO. We show that the proposed interaction-informed saliency\nmetric has practical utility for four tasks: contextual object detection, image\nretargeting, predicting object importance, and data-driven natural language\nscene description. All four scenarios reveal the value in linking the subject\nto its object in order to understand the story of an image.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 08:26:31 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Chen", "Chao-Yeh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1604.04848", "submitter": "Gil Ben-Artzi", "authors": "Gil Ben-Artzi, Tavi Halperin, Michael Werman, Shmuel Peleg", "title": "Epipolar Geometry Based On Line Similarity", "comments": "ICPR 2016, Cancun, Dec 2016", "journal-ref": "ICPR'16, Cancun, Dec. 2016, pp. 1865-1870", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that epipolar geometry can be computed from three epipolar line\ncorrespondences but this computation is rarely used in practice since there are\nno simple methods to find corresponding lines. Instead, methods for finding\ncorresponding points are widely used. This paper proposes a similarity measure\nbetween lines that indicates whether two lines are corresponding epipolar lines\nand enables finding epipolar line correspondences as needed for the computation\nof epipolar geometry.\n  A similarity measure between two lines, suitable for video sequences of a\ndynamic scene, has been previously described. This paper suggests a stereo\nmatching similarity measure suitable for images. It is based on the quality of\nstereo matching between the two lines, as corresponding epipolar lines yield a\ngood stereo correspondence.\n  Instead of an exhaustive search over all possible pairs of lines, the search\nspace is substantially reduced when two corresponding point pairs are given.\n  We validate the proposed method using real-world images and compare it to\nstate-of-the-art methods. We found this method to be more accurate by a factor\nof five compared to the standard method using seven corresponding points and\ncomparable to the 8-points algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 09:14:22 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 14:39:32 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 08:07:36 GMT"}, {"version": "v4", "created": "Sun, 8 Jan 2017 00:11:14 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Ben-Artzi", "Gil", ""], ["Halperin", "Tavi", ""], ["Werman", "Michael", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1604.04906", "submitter": "Johannes Stegmaier", "authors": "Johannes Stegmaier, Julian Arz, Benjamin Schott, Jens C. Otte, Andrei\n  Kobitski, G. Ulrich Nienhaus, Uwe Str\\\"ahle, Peter Sanders, Ralf Mikut", "title": "Generating Semi-Synthetic Validation Benchmarks for Embryomics", "comments": "Accepted publication at IEEE International Symposium on Biomedical\n  Imaging: From Nano to Macro (ISBI), 2016", "journal-ref": null, "doi": "10.1109/ISBI.2016.7493359", "report-no": null, "categories": "cs.CV q-bio.CB q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic validation is an essential part of algorithm development. The\nenormous dataset sizes and the complexity observed in many recent time-resolved\n3D fluorescence microscopy imaging experiments, however, prohibit a\ncomprehensive manual ground truth generation. Moreover, existing simulated\nbenchmarks in this field are often too simple or too specialized to\nsufficiently validate the observed image analysis problems. We present a new\nsemi-synthetic approach to generate realistic 3D+t benchmarks that combines\nchallenging cellular movement dynamics of real embryos with simulated\nfluorescent nuclei and artificial image distortions including various\nparametrizable options like cell numbers, acquisition deficiencies or multiview\nsimulations. We successfully applied the approach to simulate the development\nof a zebrafish embryo with thousands of cells over 14 hours of its early\nexistence.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 18:29:48 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Stegmaier", "Johannes", ""], ["Arz", "Julian", ""], ["Schott", "Benjamin", ""], ["Otte", "Jens C.", ""], ["Kobitski", "Andrei", ""], ["Nienhaus", "G. Ulrich", ""], ["Str\u00e4hle", "Uwe", ""], ["Sanders", "Peter", ""], ["Mikut", "Ralf", ""]]}, {"id": "1604.04926", "submitter": "Ramin Zabih", "authors": "Ramin Zabih", "title": "Some medical applications of example-based super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example-based super-resolution (EBSR) reconstructs a high-resolution image\nfrom a low-resolution image, given a training set of high-resolution images. In\nthis note I propose some applications of EBSR to medical imaging. A particular\ninteresting application, which I call \"x-ray voxelization\", approximates the\nresult of a CT scan from an x-ray image.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 20:48:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Zabih", "Ramin", ""]]}, {"id": "1604.04953", "submitter": "Lianwen Jin", "authors": "Zecheng Xie, Zenghui Sun, Lianwen Jin, Ziyong Feng, Shuye Zhang", "title": "Fully Convolutional Recurrent Network for Handwritten Chinese Text\n  Recognition", "comments": "6 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end framework, namely fully convolutional\nrecurrent network (FCRN) for handwritten Chinese text recognition (HCTR).\nUnlike traditional methods that rely heavily on segmentation, our FCRN is\ntrained with online text data directly and learns to associate the pen-tip\ntrajectory with a sequence of characters. FCRN consists of four parts: a\npath-signature layer to extract signature features from the input pen-tip\ntrajectory, a fully convolutional network to learn informative representation,\na sequence modeling layer to make per-frame predictions on the input sequence\nand a transcription layer to translate the predictions into a label sequence.\nThe FCRN is end-to-end trainable in contrast to conventional methods whose\ncomponents are separately trained and tuned. We also present a refined beam\nsearch method that efficiently integrates the language model to decode the FCRN\nand significantly improve the recognition results.\n  We evaluate the performance of the proposed method on the test sets from the\ndatabases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognition\ncompetition, and both achieve state-of-the-art performance with correct rates\nof 96.40% and 95.00%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 01:28:28 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Xie", "Zecheng", ""], ["Sun", "Zenghui", ""], ["Jin", "Lianwen", ""], ["Feng", "Ziyong", ""], ["Zhang", "Shuye", ""]]}, {"id": "1604.04970", "submitter": "Yueying Kao", "authors": "Yueying Kao, Ran He, Kaiqi Huang", "title": "Deep Aesthetic Quality Assessment with Semantic Information", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TIP.2017.2651399", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings often assess the aesthetic quality of an image coupled with the\nidentification of the image's semantic content. This paper addresses the\ncorrelation issue between automatic aesthetic quality assessment and semantic\nrecognition. We cast the assessment problem as the main task among a multi-task\ndeep model, and argue that semantic recognition task offers the key to address\nthis problem. Based on convolutional neural networks, we employ a single and\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\nand semantic labels. A correlation item between these two tasks is further\nintroduced to the framework by incorporating the inter-task relationship\nlearning. This item not only provides some useful insight about the correlation\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\neffective strategy is developed to keep a balance between the two tasks, which\nfacilitates to optimize the parameters of the framework. Extensive experiments\non the challenging AVA dataset and Photo.net dataset validate the importance of\nsemantic recognition in aesthetic quality assessment, and demonstrate that\nmulti-task deep models can discover an effective aesthetic representation to\nachieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 03:16:56 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 14:09:48 GMT"}, {"version": "v3", "created": "Fri, 21 Oct 2016 07:46:54 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kao", "Yueying", ""], ["He", "Ran", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1604.04994", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei and Jian-Hao Luo and Jianxin Wu and Zhi-Hua Zhou", "title": "Selective Convolutional Descriptor Aggregation for Fine-Grained Image\n  Retrieval", "comments": "IEEE Transactions on Image Processing (TIP), 2017, 26(6): 2868-2881", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network models pre-trained for the ImageNet\nclassification task have been successfully adopted to tasks in other domains,\nsuch as texture description and object proposal generation, but these tasks\nrequire annotations for images in the new domain. In this paper, we focus on a\nnovel and challenging task in the pure unsupervised setting: fine-grained image\nretrieval. Even with image labels, fine-grained images are difficult to\nclassify, let alone the unsupervised retrieval task. We propose the Selective\nConvolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the\nmain object in fine-grained images, a step that discards the noisy background\nand keeps useful deep descriptors. The selected descriptors are then aggregated\nand dimensionality reduced into a short feature vector using the best practices\nwe found. SCDA is unsupervised, using no image label or bounding box\nannotation. Experiments on six fine-grained datasets confirm the effectiveness\nof SCDA for fine-grained image retrieval. Besides, visualization of the SCDA\nfeatures shows that they correspond to visual attributes (even subtle ones),\nwhich might explain SCDA's high mean average precision in fine-grained\nretrieval. Moreover, on general image retrieval datasets, SCDA achieves\ncomparable retrieval results with state-of-the-art general image retrieval\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 05:39:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 02:38:16 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Luo", "Jian-Hao", ""], ["Wu", "Jianxin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1604.05000", "submitter": "Zhen Li", "authors": "Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui Cheng and Liang Lin", "title": "LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene\n  Labeling", "comments": "17 pages, accepted by ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labeling of RGB-D scenes is crucial to many intelligent applications\nincluding perceptual robotics. It generates pixelwise and fine-grained label\nmaps from simultaneously sensed photometric (RGB) and depth channels. This\npaper addresses this problem by i) developing a novel Long Short-Term Memorized\nContext Fusion (LSTM-CF) Model that captures and fuses contextual information\nfrom multiple channels of photometric and depth data, and ii) incorporating\nthis model into deep convolutional neural networks (CNNs) for end-to-end\ntraining. Specifically, contexts in photometric and depth channels are,\nrespectively, captured by stacking several convolutional layers and a long\nshort-term memory layer; the memory layer encodes both short-range and\nlong-range spatial dependencies in an image along the vertical direction.\nAnother long short-term memorized fusion layer is set up to integrate the\ncontexts along the vertical direction from different channels, and perform\nbi-directional propagation of the fused vertical contexts along the horizontal\ndirection to obtain true 2D global contexts. At last, the fused contextual\nrepresentation is concatenated with the convolutional features extracted from\nthe photometric channels in order to improve the accuracy of fine-scale\nsemantic labeling. Our proposed model has set a new state of the art, i.e.,\n48.1% and 49.4% average class accuracy over 37 categories (2.2% and 5.4%\nimprovement) on the large-scale SUNRGBD dataset and the NYUDv2dataset,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 05:59:50 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 08:15:19 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 16:46:43 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Li", "Zhen", ""], ["Gan", "Yukang", ""], ["Liang", "Xiaodan", ""], ["Yu", "Yizhou", ""], ["Cheng", "Hui", ""], ["Lin", "Liang", ""]]}, {"id": "1604.05027", "submitter": "Line K\\\"uhnel", "authors": "Line K\\\"uhnel, Stefan Sommer, Akshay Pai and Lars Lau Raket", "title": "Most Likely Separation of Intensity and Warping Effects in Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a class of mixed-effects models for joint modeling of\nspatially correlated intensity variation and warping variation in 2D images.\nSpatially correlated intensity variation and warp variation are modeled as\nrandom effects, resulting in a nonlinear mixed-effects model that enables\nsimultaneous estimation of template and model parameters by optimization of the\nlikelihood function. We propose an algorithm for fitting the model which\nalternates estimation of variance parameters and image registration. This\napproach avoids the potential estimation bias in the template estimate that\narises when treating registration as a preprocessing step. We apply the model\nto datasets of facial images and 2D brain magnetic resonance images to\nillustrate the simultaneous estimation and prediction of intensity and warp\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 08:15:27 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 10:54:43 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["K\u00fchnel", "Line", ""], ["Sommer", "Stefan", ""], ["Pai", "Akshay", ""], ["Raket", "Lars Lau", ""]]}, {"id": "1604.05091", "submitter": "Peter Ondruska", "authors": "Peter Ondruska, Julie Dequaire, Dominic Zeng Wang, Ingmar Posner", "title": "End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel end-to-end framework for tracking and\nclassifying a robot's surroundings in complex, dynamic and only partially\nobservable real-world environments. The approach deploys a recurrent neural\nnetwork to filter an input stream of raw laser measurements in order to\ndirectly infer object locations, along with their identity in both visible and\noccluded areas. To achieve this we first train the network using unsupervised\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\noccupancy prediction. We show that by learning to track on a large amount of\nunsupervised data, the network creates a rich internal representation of its\nenvironment which we in turn exploit through the principle of inductive\ntransfer of knowledge to perform the task of it's semantic classification. As a\nresult, we show that only a small amount of labelled data suffices to steer the\nnetwork towards mastering this additional task. Furthermore we propose a novel\nrecurrent neural network architecture specifically tailored to tracking and\nsemantic classification in real-world robotics applications. We demonstrate the\ntracking and classification performance of the method on real-world data\ncollected at a busy road junction. Our evaluation shows that the proposed\nend-to-end framework compares favourably to a state-of-the-art, model-free\ntracking solution and that it outperforms a conventional one-shot training\nscheme for semantic classification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 11:15:56 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 14:09:26 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ondruska", "Peter", ""], ["Dequaire", "Julie", ""], ["Wang", "Dominic Zeng", ""], ["Posner", "Ingmar", ""]]}, {"id": "1604.05096", "submitter": "Jonas Uhrig", "authors": "Jonas Uhrig, Marius Cordts, Uwe Franke, Thomas Brox", "title": "Pixel-level Encoding and Depth Layering for Instance-level Semantic\n  Labeling", "comments": "Accepted at GCPR 2016. Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for instance-aware semantic labeling have augmented\nconvolutional neural networks (CNNs) with complex multi-task architectures or\ncomputationally expensive graphical models. We present a method that leverages\na fully convolutional network (FCN) to predict semantic labels, depth and an\ninstance-based encoding using each pixel's direction towards its corresponding\ninstance center. Subsequently, we apply low-level computer vision techniques to\ngenerate state-of-the-art instance segmentation on the street scene datasets\nKITTI and Cityscapes. Our approach outperforms existing works by a large margin\nand can additionally predict absolute distances of individual instances from a\nmonocular image as well as a pixel-level semantic labeling.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 11:24:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 14:46:35 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Uhrig", "Jonas", ""], ["Cordts", "Marius", ""], ["Franke", "Uwe", ""], ["Brox", "Thomas", ""]]}, {"id": "1604.05132", "submitter": "Christian Mostegel", "authors": "Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer and Horst\n  Bischof", "title": "Using Self-Contradiction to Learn Confidence Measures in Stereo Vision", "comments": "This paper was accepted to the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016. The copyright was transfered to IEEE\n  (https://www.ieee.org). The official version of the paper will be made\n  available on IEEE Xplore (R) (http://ieeexplore.ieee.org). This version of\n  the paper also contains the supplementary material, which will not appear\n  IEEE Xplore (R)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned confidence measures gain increasing importance for outlier removal\nand quality improvement in stereo vision. However, acquiring the necessary\ntraining data is typically a tedious and time consuming task that involves\nmanual interaction, active sensing devices and/or synthetic scenes. To overcome\nthis problem, we propose a new, flexible, and scalable way for generating\ntraining data that only requires a set of stereo images as input. The key idea\nof our approach is to use different view points for reasoning about\ncontradictions and consistencies between multiple depth maps generated with the\nsame stereo algorithm. This enables us to generate a huge amount of training\ndata in a fully automated manner. Among other experiments, we demonstrate the\npotential of our approach by boosting the performance of three learned\nconfidence measures on the KITTI2012 dataset by simply training them on a vast\namount of automatically generated training data rather than a limited amount of\nlaser ground truth data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 13:26:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Mostegel", "Christian", ""], ["Rumpler", "Markus", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1604.05144", "submitter": "Jifeng Dai", "authors": "Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun", "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic\n  Segmentation", "comments": "accepted by CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale data is of crucial importance for learning semantic segmentation\nmodels, but annotating per-pixel masks is a tedious and inefficient procedure.\nWe note that for the topic of interactive image segmentation, scribbles are\nvery widely used in academic research and commercial software, and are\nrecognized as one of the most user-friendly ways of interacting. In this paper,\nwe propose to use scribbles to annotate images, and develop an algorithm to\ntrain convolutional networks for semantic segmentation supervised by scribbles.\nOur algorithm is based on a graphical model that jointly propagates information\nfrom scribbles to unmarked pixels and learns network parameters. We present\ncompetitive object semantic segmentation results on the PASCAL VOC dataset by\nusing scribbles as annotations. Scribbles are also favored for annotating stuff\n(e.g., water, sky, grass) that has no well-defined shape, and our method shows\nexcellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive\nscribble annotations. Our scribble annotations on PASCAL VOC are available at\nhttp://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 13:46:23 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Lin", "Di", ""], ["Dai", "Jifeng", ""], ["Jia", "Jiaya", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1604.05210", "submitter": "Benjamin Irving", "authors": "Benjamin Irving, James M Franklin, Bartlomiej W Papiez, Ewan M\n  Anderson, Ricky A Sharma, Fergus V Gleeson, Sir Michael Brady and Julia A\n  Schnabel", "title": "Pieces-of-parts for supervoxel segmentation with global context:\n  Application to DCE-MRI tumour delineation", "comments": "accepted for publication in the Journal of Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2016.03.002", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rectal tumour segmentation in dynamic contrast-enhanced MRI (DCE-MRI) is a\nchallenging task, and an automated and consistent method would be highly\ndesirable to improve the modelling and prediction of patient outcomes from\ntissue contrast enhancement characteristics - particularly in routine clinical\npractice. A framework is developed to automate DCE-MRI tumour segmentation, by\nintroducing: perfusion-supervoxels to over-segment and classify DCE-MRI volumes\nusing the dynamic contrast enhancement characteristics; and the pieces-of-parts\ngraphical model, which adds global (anatomic) constraints that further refine\nthe supervoxel components that comprise the tumour. The framework was evaluated\non 23 DCE-MRI scans of patients with rectal adenocarcinomas, and achieved a\nvoxelwise area-under the receiver operating characteristic curve (AUC) of 0.97\ncompared to expert delineations. Creating a binary tumour segmentation, 21 of\nthe 23 cases were segmented correctly with a median Dice similarity coefficient\n(DSC) of 0.63, which is close to the inter-rater variability of this\nchallenging task. A sec- ond study is also included to demonstrate the method's\ngeneralisability and achieved a DSC of 0.71. The framework achieves promising\nresults for the underexplored area of rectal tumour segmentation in DCE-MRI,\nand the methods have potential to be applied to other DCE-MRI and supervoxel\nsegmentation problems\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 15:31:22 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Irving", "Benjamin", ""], ["Franklin", "James M", ""], ["Papiez", "Bartlomiej W", ""], ["Anderson", "Ewan M", ""], ["Sharma", "Ricky A", ""], ["Gleeson", "Fergus V", ""], ["Brady", "Sir Michael", ""], ["Schnabel", "Julia A", ""]]}, {"id": "1604.05213", "submitter": "Robert Amelard", "authors": "Robert Amelard, Richard L Hughson, Danielle K Greaves, Kaylen J\n  Pfisterer, Jason Leung, David A Clausi, Alexander Wong", "title": "Non-contact hemodynamic imaging reveals the jugular venous pulse\n  waveform", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular monitoring is important to prevent diseases from progressing.\nThe jugular venous pulse (JVP) waveform offers important clinical information\nabout cardiac health, but is not routinely examined due to its invasive\ncatheterisation procedure. Here, we demonstrate for the first time that the JVP\ncan be consistently observed in a non-contact manner using a novel light-based\nphotoplethysmographic imaging system, coded hemodynamic imaging (CHI). While\ntraditional monitoring methods measure the JVP at a single location, CHI's\nwide-field imaging capabilities were able to observe the jugular venous pulse's\nspatial flow profile for the first time. The important inflection points in the\nJVP were observed, meaning that cardiac abnormalities can be assessed through\nJVP distortions. CHI provides a new way to assess cardiac health through\nnon-contact light-based JVP monitoring, and can be used in non-surgical\nenvironments for cardiac assessment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 19:21:35 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 14:50:01 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Amelard", "Robert", ""], ["Hughson", "Richard L", ""], ["Greaves", "Danielle K", ""], ["Pfisterer", "Kaylen J", ""], ["Leung", "Jason", ""], ["Clausi", "David A", ""], ["Wong", "Alexander", ""]]}, {"id": "1604.05225", "submitter": "Jiren Jin", "authors": "Jiren Jin, Hideki Nakayama", "title": "Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length\n  Image Tagging", "comments": "International Conference on Pattern Recognition (ICPR) 2016, Cancun,\n  Mexico (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation has been an important research topic in\nfacilitating large scale image management and retrieval. Existing methods focus\non learning image-tag correlation or correlation between tags to improve\nannotation accuracy. However, most of these methods evaluate their performance\nusing top-k retrieval performance, where k is fixed. Although such setting\ngives convenience for comparing different methods, it is not the natural way\nthat humans annotate images. The number of annotated tags should depend on\nimage contents. Inspired by the recent progress in machine translation and\nimage captioning, we propose a novel Recurrent Image Annotator (RIA) model that\nforms image annotation task as a sequence generation problem so that RIA can\nnatively predict the proper length of tags according to image contents. We\nevaluate the proposed model on various image annotation datasets. In addition\nto comparing our model with existing methods using the conventional top-k\nevaluation measures, we also provide our model as a high quality baseline for\nthe arbitrary length image tagging task. Moreover, the results of our\nexperiments show that the order of tags in training phase has a great impact on\nthe final annotation performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 16:09:04 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 08:30:48 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 19:57:02 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Jin", "Jiren", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1604.05242", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj", "title": "Can Boosting with SVM as Week Learners Help?", "comments": "Work done in 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in images involves identifying objects with partial\nocclusions, viewpoint changes, varying illumination, cluttered backgrounds.\nRecent work in object recognition uses machine learning techniques SVM-KNN,\nLocal Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, we\nwant to utilize SVM as week learners in AdaBoost. Experiments are done with\nclassifiers like near- est neighbor, k-nearest neighbor, Support vector\nmachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariant\ndescriptors and Pyramid his- togram of gradient descriptors. AdaBoost is\ntrained with set of week classifier as SVMs, each with kernel distance function\non different descriptors. Results shows AdaBoost with SVM outperform other\nmethods for Object Categorization dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:05:00 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 23:03:27 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Govindaraj", "Dinesh", ""]]}, {"id": "1604.05383", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou, Philipp Kr\\\"ahenb\\\"uhl, Mathieu Aubry, Qixing Huang,\n  Alexei A. Efros", "title": "Learning Dense Correspondence via 3D-guided Cycle Consistency", "comments": "To appear in CVPR 2016 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative deep learning approaches have shown impressive results for\nproblems where human-labeled ground truth is plentiful, but what about tasks\nwhere labels are difficult or impossible to obtain? This paper tackles one such\nproblem: establishing dense visual correspondence across different object\ninstances. For this task, although we do not know what the ground-truth is, we\nknow it should be consistent across instances of that category. We exploit this\nconsistency as a supervisory signal to train a convolutional neural network to\npredict cross-instance correspondences between pairs of images depicting\nobjects of the same category. For each pair of training images we find an\nappropriate 3D CAD model and render two synthetic views to link in with the\npair, establishing a correspondence flow 4-cycle. We use ground-truth\nsynthetic-to-synthetic correspondences, provided by the rendering engine, to\ntrain a ConvNet to predict synthetic-to-real, real-to-real and\nreal-to-synthetic correspondences that are cycle-consistent with the\nground-truth. At test time, no CAD models are required. We demonstrate that our\nend-to-end trained ConvNet supervised by cycle-consistency outperforms\nstate-of-the-art pairwise matching methods in correspondence-related tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 23:50:40 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Zhou", "Tinghui", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Aubry", "Mathieu", ""], ["Huang", "Qixing", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1604.05413", "submitter": "Hariharan Ramasangu Dr", "authors": "Hariharan Ramasangu, Neelam Sinha", "title": "Cognitive state classification using transformed fMRI data", "comments": "5 pages, Conference-SPCOM14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach, for understanding human brain functioning, is to analyze the\nchanges in the brain while performing cognitive tasks. Towards this, Functional\nMagnetic Resonance (fMR) images of subjects performing well-defined tasks are\nwidely utilized for task-specific analyses. In this work, we propose a\nprocedure to enable classification between two chosen cognitive tasks, using\ntheir respective fMR image sequences. The time series of expert-marked\nanatomically-mapped relevant voxels are processed and fed as input to the\nclassical Naive Bayesian and SVM classifiers. The processing involves use of\nrandom sieve function, phase information in the data transformed using Fourier\nand Hilbert transformations. This processing results in improved\nclassification, as against using the voxel intensities directly, as\nillustrated. The novelty of the proposed method lies in utilizing the phase\ninformation in the transformed domain, for classifying between the cognitive\ntasks along with random sieve function chosen with a particular probability\ndistribution. The proposed classification procedure is applied on a publicly\navailable dataset, StarPlus data, with 6 subjects performing the two distinct\ncognitive tasks of watching either a picture or a sentence. The classification\naccuracy stands at an average of 65.6%(using Naive Bayes classifier) and\n76.4%(using SVM classifier) for raw data. The corresponding classification\naccuracy stands at 96.8% and 97.5% for Fourier transformed data. For Hilbert\ntransformed data, it is 93.7% and 99%, for 6 subjects, on 2 cognitive tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 02:52:31 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ramasangu", "Hariharan", ""], ["Sinha", "Neelam", ""]]}, {"id": "1604.05417", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa", "title": "Triplet Probabilistic Embedding for Face Verification and Clustering", "comments": "Oral Paper in BTAS 2016; NVIDIA Best paper Award\n  (http://ieee-biometrics.org/btas2016/awards.html)", "journal-ref": null, "doi": "10.1109/BTAS.2016.7791205", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress made over the past twenty five years,\nunconstrained face verification remains a challenging problem. This paper\nproposes an approach that couples a deep CNN-based approach with a\nlow-dimensional discriminative embedding learned using triplet probability\nconstraints to solve the unconstrained face verification problem. Aside from\nyielding performance improvements, this embedding provides significant\nadvantages in terms of memory and for post-processing operations like subject\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\nproposed algorithm performs comparably or better than the state of the art\nmethods in verification and identification metrics, while requiring much less\ntraining data and training time. The superior performance of the proposed\nmethod on the CFP dataset shows that the representation learned by our deep CNN\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\nof the deep features to challenges including age, pose, blur and clutter by\nperforming simple clustering experiments on both IJB-A and LFW datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 03:29:56 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 16:04:02 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 03:10:44 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Alavi", "Azadeh", ""], ["Castillo", "Carlos", ""], ["Chellappa", "Rama", ""]]}, {"id": "1604.05442", "submitter": "Binqi Zhang", "authors": "Binqi Zhang, Chen Wang, Bing Bing Zhou, Albert Y. Zomaya", "title": "Improving Raw Image Storage Efficiency by Exploiting Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the temporal and spatial storage efficiency, researchers have\nintensively studied various techniques, including compression and\ndeduplication. Through our evaluation, we find that methods such as photo tags\nor local features help to identify the content-based similar- ity between raw\nimages. The images can then be com- pressed more efficiently to get better\nstorage space sav- ings. Furthermore, storing similar raw images together\nenables rapid data sorting, searching and retrieval if the images are stored in\na distributed and large-scale envi- ronment by reducing fragmentation. In this\npaper, we evaluated the compressibility by designing experiments and observing\nthe results. We found that on a statistical basis the higher similarity photos\nhave, the better com- pression results are. This research helps provide a clue\nfor future large-scale storage system design.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 06:29:36 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Zhang", "Binqi", ""], ["Wang", "Chen", ""], ["Zhou", "Bing Bing", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1604.05451", "submitter": "Dacheng Tao", "authors": "Yunhe Wang, Chang Xu, Shan You, Dacheng Tao and Chao Xu", "title": "Parts for the Whole: The DCT Norm for Extreme Visual Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study the extreme visual recovery problem, in which over 90\\% of\npixel values in a given image are missing. Existing low rank-based algorithms\nare only effective for recovering data with at most 90\\% missing values. Thus,\nwe exploit visual data's smoothness property to help solve this challenging\nextreme visual recovery problem. Based on the Discrete Cosine Transformation\n(DCT), we propose a novel DCT norm that involves all pixels and produces smooth\nestimations in any view. Our theoretical analysis shows that the total\nvariation (TV) norm, which only achieves local smoothness, is a special case of\nthe proposed DCT norm. We also develop a new visual recovery algorithm by\nminimizing the DCT and nuclear norms to achieve a more visually pleasing\nestimation. Experimental results on a benchmark image dataset demonstrate that\nthe proposed approach is superior to state-of-the-art methods in terms of peak\nsignal-to-noise ratio and structural similarity.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:13:50 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["You", "Shan", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1604.05495", "submitter": "Gayoung Lee", "authors": "Gayoung Lee, Yu-Wing Tai, Junmo Kim", "title": "Deep Saliency with Encoded Low level Distance Map and High Level\n  Features", "comments": "Accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition(CVPR) 2016. Project page:\n  https://github.com/gylee1103/SaliencyELD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in saliency detection have utilized deep learning to obtain\nhigh level features to detect salient regions in a scene. These advances have\ndemonstrated superior results over previous works that utilize hand-crafted low\nlevel features for saliency detection. In this paper, we demonstrate that\nhand-crafted features can provide complementary information to enhance\nperformance of saliency detection that utilizes only high level features. Our\nmethod utilizes both high level and low level features for saliency detection\nunder a unified deep learning framework. The high level features are extracted\nusing the VGG-net, and the low level features are compared with other parts of\nan image to form a low level distance map. The low level distance map is then\nencoded using a convolutional neural network(CNN) with multiple 1X1\nconvolutional and ReLU layers. We concatenate the encoded low level distance\nmap and the high level features, and connect them to a fully connected neural\nnetwork classifier to evaluate the saliency of a query region. Our experiments\nshow that our method can further improve the performance of state-of-the-art\ndeep learning-based saliency detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 10:07:16 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Lee", "Gayoung", ""], ["Tai", "Yu-Wing", ""], ["Kim", "Junmo", ""]]}, {"id": "1604.05576", "submitter": "Claudio Gennaro", "authors": "Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Claudio Gennaro,\n  Lucia Vadicamo", "title": "Using Apache Lucene to Search Vector of Locally Aggregated Descriptors", "comments": "In Proceedings of the 11th Joint Conference on Computer Vision,\n  Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2016) -\n  Volume 4: VISAPP, p. 383-392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate Text Representation (STR) is a profitable solution to efficient\nsimilarity search on metric space using conventional text search engines, such\nas Apache Lucene. This technique is based on comparing the permutations of some\nreference objects in place of the original metric distance. However, the\nAchilles heel of STR approach is the need to reorder the result set of the\nsearch according to the metric distance. This forces to use a support database\nto store the original objects, which requires efficient random I/O on a fast\nsecondary memory (such as flash-based storages). In this paper, we propose to\nextend the Surrogate Text Representation to specifically address a class of\nvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).\nThis approach is based on representing the individual sub-vectors forming the\nVLAD vector with the STR, providing a finer representation of the vector and\nenabling us to get rid of the reordering phase. The experiments on a publicly\navailable dataset show that the extended STR outperforms the baseline STR\nachieving satisfactory performance near to the one obtained with the original\nVLAD vectors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:08:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Amato", "Giuseppe", ""], ["Bolettieri", "Paolo", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Vadicamo", "Lucia", ""]]}, {"id": "1604.05592", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa and David W. Jacobs and Manmohan Chandraker", "title": "WarpNet: Weakly Supervised Matching for Single-view Reconstruction", "comments": "to appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to matching images of objects in fine-grained datasets\nwithout using part annotations, with an application to the challenging problem\nof weakly supervised single-view reconstruction. This is in contrast to prior\nworks that require part annotations, since matching objects across class and\npose variations is challenging with appearance features alone. We overcome this\nchallenge through a novel deep learning architecture, WarpNet, that aligns an\nobject in one image with a different object in another. We exploit the\nstructure of the fine-grained dataset to create artificial data for training\nthis network in an unsupervised-discriminative learning approach. The output of\nthe network acts as a spatial prior that allows generalization at test time to\nmatch real images across variations in appearance, viewpoint and articulation.\nOn the CUB-200-2011 dataset of bird categories, we improve the AP over an\nappearance-only network by 13.6%. We further demonstrate that our WarpNet\nmatches, together with the structure of fine-grained datasets, allow\nsingle-view reconstructions with quality comparable to using annotated point\ncorrespondences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:28:42 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 09:40:46 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Jacobs", "David W.", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1604.05605", "submitter": "Andrei Polzounov", "authors": "Andrei Polzounov, Ilmira Terpugova, Deividas Skiparis, Andrei Mihai", "title": "Right whale recognition using convolutional neural networks", "comments": "10 pages + appendix, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We studied the feasibility of recognizing individual right whales (Eubalaena\nglacialis) using convolutional neural networks. Prior studies have shown that\nCNNs can be used in wide range of classification and categorization tasks such\nas automated human face recognition. To test applicability of deep learning to\nwhale recognition we have developed several models based on best practices from\nliterature. Here, we describe the performance of the models. We conclude that\nmachine recognition of whales is feasible and comment on the difficulty of the\nproblem\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:46:30 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Polzounov", "Andrei", ""], ["Terpugova", "Ilmira", ""], ["Skiparis", "Deividas", ""], ["Mihai", "Andrei", ""]]}, {"id": "1604.05633", "submitter": "Yanghao Li", "authors": "Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan and\n  Jiaying Liu", "title": "Online Human Action Detection using Joint Classification-Regression\n  Recurrent Neural Networks", "comments": "2016 ECCV Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition from well-segmented 3D skeleton data has been\nintensively studied and has been attracting an increasing attention. Online\naction detection goes one step further and is more challenging, which\nidentifies the action type and localizes the action positions on the fly from\nthe untrimmed stream data. In this paper, we study the problem of online action\ndetection from streaming skeleton data. We propose a multi-task end-to-end\nJoint Classification-Regression Recurrent Neural Network to better explore the\naction type and temporal localization information. By employing a joint\nclassification and regression optimization objective, this network is capable\nof automatically localizing the start and end points of actions more\naccurately. Specifically, by leveraging the merits of the deep Long Short-Term\nMemory (LSTM) subnetwork, the proposed model automatically captures the complex\nlong-range temporal dynamics, which naturally avoids the typical sliding window\ndesign and thus ensures high computational efficiency. Furthermore, the subtask\nof regression optimization provides the ability to forecast the action prior to\nits occurrence. To evaluate our proposed model, we build a large streaming\nvideo dataset with annotations. Experimental results on our dataset and the\npublic G3D dataset both demonstrate very promising performance of our scheme.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 15:58:56 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 15:54:07 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Li", "Yanghao", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Yuan", "Chunfeng", ""], ["Liu", "Jiaying", ""]]}, {"id": "1604.05766", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee", "title": "Track and Transfer: Watching Videos to Simulate Strong Human Supervision\n  for Weakly-Supervised Object Detection", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The status quo approach to training object detectors requires expensive\nbounding box annotations. Our framework takes a markedly different direction:\nwe transfer tracked object boxes from weakly-labeled videos to weakly-labeled\nimages to automatically generate pseudo ground-truth boxes, which replace\nmanually annotated bounding boxes. We first mine discriminative regions in the\nweakly-labeled image collection that frequently/rarely appear in the\npositive/negative images. We then match those regions to videos and retrieve\nthe corresponding tracked object boxes. Finally, we design a hough transform\nalgorithm to vote for the best box to serve as the pseudo GT for each image,\nand use them to train an object detector. Together, these lead to\nstate-of-the-art weakly-supervised detection results on the PASCAL 2007 and\n2010 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 22:23:29 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1604.05813", "submitter": "Ruining He", "authors": "Ruining He, Chunbin Lin, Jianguo Wang, Julian McAuley", "title": "Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class\n  Collaborative Filtering", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building successful recommender systems requires uncovering the underlying\ndimensions that describe the properties of items as well as users' preferences\ntoward them. In domains like clothing recommendation, explaining users'\npreferences requires modeling the visual appearance of the items in question.\nThis makes recommendation especially challenging, due to both the complexity\nand subtlety of people's 'visual preferences,' as well as the scale and\ndimensionality of the data and features involved. Ultimately, a successful\nmodel should be capable of capturing considerable variance across different\ncategories and styles, while still modeling the commonalities explained by\n`global' structures in order to combat the sparsity (e.g. cold-start),\nvariability, and scale of real-world datasets. Here, we address these\nchallenges by building such structures to model the visual dimensions across\ndifferent product categories. With a novel hierarchical embedding architecture,\nour method accounts for both high-level (colorfulness, darkness, etc.) and\nsubtle (e.g. casualness) visual characteristics simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:36:57 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["He", "Ruining", ""], ["Lin", "Chunbin", ""], ["Wang", "Jianguo", ""], ["McAuley", "Julian", ""]]}, {"id": "1604.05816", "submitter": "Hongwei Li", "authors": "Hongwei Li, Wei-Shi Zheng and Jianguo Zhang", "title": "Deep CNNs for HEp-2 Cells Classification : A Cross-specimen Analysis", "comments": "rejected by MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic classification of Human Epithelial Type-2 (HEp-2) cells staining\npatterns is an important and yet a challenging problem. Although both shallow\nand deep methods have been applied, the study of deep convolutional networks\n(CNNs) on this topic is shallow to date, thus failed to claim its top position\nfor this problem. In this paper, we propose a novel study of using CNNs for\nHEp-2 cells classification focusing on cross-specimen analysis, a key\nevaluation for generalization. For the first time, our study reveals several\nkey factors of using CNNs for HEp-2 cells classification. Our proposed system\nachieves state-of-the-art classification accuracy on public benchmark dataset.\nComparative experiments on different training data reveals that adding\ndifferent specimens,rather than increasing in numbers by affine\ntransformations, helps to train a good deep model. This opens a new avenue for\nadopting deep CNNs to HEp-2 cells classification.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:43:20 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 21:37:20 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 16:29:32 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Li", "Hongwei", ""], ["Zheng", "Wei-Shi", ""], ["Zhang", "Jianguo", ""]]}, {"id": "1604.05817", "submitter": "Hongyang Xue", "authors": "Hongyang Xue and Shengming Zhang and Deng Cai", "title": "Depth Image Inpainting: Improving Low Rank Matrix Completion with Low\n  Gradient Regularization", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2718183", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the case of inpainting single depth images. Without corresponding\ncolor images, previous or next frames, depth image inpainting is quite\nchallenging. One natural solution is to regard the image as a matrix and adopt\nthe low rank regularization just as inpainting color images. However, the low\nrank assumption does not make full use of the properties of depth images.\n  A shallow observation may inspire us to penalize the non-zero gradients by\nsparse gradient regularization. However, statistics show that though most\npixels have zero gradients, there is still a non-ignorable part of pixels whose\ngradients are equal to 1. Based on this specific property of depth images , we\npropose a low gradient regularization method in which we reduce the penalty for\ngradient 1 while penalizing the non-zero gradients to allow for gradual depth\nchanges. The proposed low gradient regularization is integrated with the low\nrank regularization into the low rank low gradient approach for depth image\ninpainting. We compare our proposed low gradient regularization with sparse\ngradient regularization. The experimental results show the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:56:06 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Xue", "Hongyang", ""], ["Zhang", "Shengming", ""], ["Cai", "Deng", ""]]}, {"id": "1604.05848", "submitter": "Bing Shuai", "authors": "Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang", "title": "Scene Parsing with Integration of Parametric and Non-parametric Models", "comments": "13 Pages, 6 figures, IEEE Transactions on Image Processing (T-IP)\n  2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2533862", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopt Convolutional Neural Networks (CNNs) to be our parametric model to\nlearn discriminative features and classifiers for local patch classification.\nBased on the occurrence frequency distribution of classes, an ensemble of CNNs\n(CNN-Ensemble) are learned, in which each CNN component focuses on learning\ndifferent and complementary visual patterns. The local beliefs of pixels are\noutput by CNN-Ensemble. Considering that visually similar pixels are\nindistinguishable under local context, we leverage the global scene semantics\nto alleviate the local ambiguity. The global scene constraint is mathematically\nachieved by adding a global energy term to the labeling energy function, and it\nis practically estimated in a non-parametric framework. A large margin based\nCNN metric learning method is also proposed for better global belief\nestimation. In the end, the integration of local and global beliefs gives rise\nto the class likelihood of pixels, based on which maximum marginal inference is\nperformed to generate the label prediction maps. Even without any\npost-processing, we achieve state-of-the-art results on the challenging\nSiftFlow and Barcelona benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 07:38:15 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Shuai", "Bing", ""], ["Zuo", "Zhen", ""], ["Wang", "Gang", ""], ["Wang", "Bing", ""]]}, {"id": "1604.05865", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Haitham Bou Ammar, Luis Puig, Eric Eaton\n  and Antonio Liotta", "title": "Estimating 3D Trajectories from 2D Projections via Disjunctive Factored\n  Four-Way Conditional Restricted Boltzmann Machines", "comments": "Pattern Recognition, ISSN 0031-3203, Elsevier, 2017", "journal-ref": null, "doi": "10.1016/j.patcog.2017.04.017", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation, recognition, and near-future prediction of 3D trajectories based\non their two dimensional projections available from one camera source is an\nexceptionally difficult problem due to uncertainty in the trajectories and\nenvironment, high dimensionality of the specific trajectory states, lack of\nenough labeled data and so on. In this article, we propose a solution to solve\nthis problem based on a novel deep learning model dubbed Disjunctive Factored\nFour-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our method\nimproves state-of-the-art deep learning techniques for high dimensional\ntime-series modeling by introducing a novel tensor factorization capable of\ndriving forth order Boltzmann machines to considerably lower energy levels, at\nno computational costs. DFFW-CRBMs are capable of accurately estimating,\nrecognizing, and performing near-future prediction of three-dimensional\ntrajectories from their 2D projections while requiring limited amount of\nlabeled data. We evaluate our method on both simulated and real-world data,\nshowing its effectiveness in predicting and classifying complex ball\ntrajectories and human activities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:08:50 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 06:11:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Ammar", "Haitham Bou", ""], ["Puig", "Luis", ""], ["Eaton", "Eric", ""], ["Liotta", "Antonio", ""]]}, {"id": "1604.05907", "submitter": "Sounak Dey", "authors": "Sounak Dey, Anguelos Nicolaou, Josep Llados, and Umapada Pal", "title": "Local Binary Pattern for Word Spotting in Handwritten Historical\n  Document", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital libraries store images which can be highly degraded and to index this\nkind of images we resort to word spot- ting as our information retrieval\nsystem. Information retrieval for handwritten document images is more\nchallenging due to the difficulties in complex layout analysis, large\nvariations of writing styles, and degradation or low quality of historical\nmanuscripts. This paper presents a simple innovative learning-free method for\nword spotting from large scale historical documents combining Local Binary\nPattern (LBP) and spatial sampling. This method offers three advantages:\nfirstly, it operates in completely learning free paradigm which is very\ndifferent from unsupervised learning methods, secondly, the computational time\nis significantly low because of the LBP features which are very fast to\ncompute, and thirdly, the method can be used in scenarios where annotations are\nnot available. Finally we compare the results of our proposed retrieval method\nwith the other methods in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 11:58:36 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 09:50:47 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Dey", "Sounak", ""], ["Nicolaou", "Anguelos", ""], ["Llados", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "1604.05921", "submitter": "Alexandre De Siqueira", "authors": "Alexandre Fioravante de Siqueira and Fl\\'avio Camargo Cabrera and\n  Wagner Massayuki Nakasuga and Aylton Pagamisse and Aldo Eloizo Job", "title": "Jansen-MIDAS: a multi-level photomicrograph segmentation software based\n  on isotropic undecimated wavelets", "comments": "arXiv version: 25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation, the process of separating the elements within an image,\nis frequently used for obtaining information from photomicrographs. However,\nsegmentation methods should be used with reservations: incorrect segmentation\ncan mislead when interpreting regions of interest (ROI), thus decreasing the\nsuccess rate of additional procedures. Multi-Level Starlet Segmentation (MLSS)\nand Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to address\nthe photomicrograph segmentation deficiency on general tools. These methods\ngave rise to Jansen-MIDAS, an open-source software which a scientist can use to\nobtain a multi-level threshold segmentation of his/hers photomicrographs. This\nsoftware is presented in two versions: a text-based version, for GNU Octave,\nand a graphical user interface (GUI) version, for MathWorks MATLAB. It can be\nused to process several types of images, becoming a reliable alternative to the\nscientist.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 12:40:50 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 10:21:28 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Cabrera", "Fl\u00e1vio Camargo", ""], ["Nakasuga", "Wagner Massayuki", ""], ["Pagamisse", "Aylton", ""], ["Job", "Aldo Eloizo", ""]]}, {"id": "1604.05933", "submitter": "Jochen Gast", "authors": "Jochen Gast and Anita Sellent and Stefan Roth", "title": "Parametric Object Motion from Blur", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition 2016. Includes supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur can adversely affect a number of vision tasks, hence it is\ngenerally considered a nuisance. We instead treat motion blur as a useful\nsignal that allows to compute the motion of objects from a single image.\nDrawing on the success of joint segmentation and parametric motion models in\nthe context of optical flow estimation, we propose a parametric object motion\nmodel combined with a segmentation mask to exploit localized, non-uniform\nmotion blur. Our parametric image formation model is differentiable w.r.t. the\nmotion parameters, which enables us to generalize marginal-likelihood\ntechniques from uniform blind deblurring to localized, non-uniform blur. A\ntwo-stage pipeline, first in derivative space and then in image space, allows\nto estimate both parametric object motion as well as a motion segmentation from\na single image alone. Our experiments demonstrate its ability to cope with very\nchallenging cases of object motion blur.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 13:00:30 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Gast", "Jochen", ""], ["Sellent", "Anita", ""], ["Roth", "Stefan", ""]]}, {"id": "1604.05966", "submitter": "Tharindu Rathnayake", "authors": "Tharindu Rathnayake, Reza Hoseinnezhad, Ruwan Tennakoon and Alireza\n  Bab-Hadiashar", "title": "Labeled Multi-Bernoulli Tracking for Industrial Mobile Platform Safety", "comments": "The conference which this paper was submitted, has rejected this\n  paper. Thus, we are in the process of enhancing the content of the paper and\n  submit it to another conference/journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a track-before-detect labeled multi-Bernoulli filter\ntailored for industrial mobile platform safety applications. We derive two\napplication specific separable likelihood functions that capture the geometric\nshape and colour information of the human targets who are wearing a high\nvisible vest. These likelihoods are then used in a labeled multi-Bernoulli\nfilter with a novel two step Bayesian update. Preliminary simulation results\nshow that the proposed solution can successfully track human workers wearing a\nluminous yellow colour vest in an industrial environment.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 14:00:09 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 00:00:09 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Rathnayake", "Tharindu", ""], ["Hoseinnezhad", "Reza", ""], ["Tennakoon", "Ruwan", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "1604.06057", "submitter": "Karthik Narasimhan", "authors": "Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B.\n  Tenenbaum", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma's Revenge'.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:47:48 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 14:45:58 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Narasimhan", "Karthik R.", ""], ["Saeedi", "Ardavan", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1604.06079", "submitter": "Guilin Liu", "authors": "Guilin Liu and Chao Yang and Zimo Li and Duygu Ceylan and Qixing Huang", "title": "Symmetry-aware Depth Estimation using Deep Neural Networks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the abundance of 2D product images from the Internet, developing\nefficient and scalable algorithms to recover the missing depth information is\ncentral to many applications. Recent works have addressed the single-view depth\nestimation problem by utilizing convolutional neural networks. In this paper,\nwe show that exploring symmetry information, which is ubiquitous in man made\nobjects, can significantly boost the quality of such depth predictions.\nSpecifically, we propose a new convolutional neural network architecture to\nfirst estimate dense symmetric correspondences in a product image and then\npropose an optimization which utilizes this information explicitly to\nsignificantly improve the quality of single-view depth estimations. We have\nevaluated our approach extensively, and experimental results show that this\napproach outperforms state-of-the-art depth estimation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 19:50:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 22:39:49 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Liu", "Guilin", ""], ["Yang", "Chao", ""], ["Li", "Zimo", ""], ["Ceylan", "Duygu", ""], ["Huang", "Qixing", ""]]}, {"id": "1604.06083", "submitter": "Bernardete Ribeiro Prof", "authors": "Gon\\c{c}alo Oliveira, Xavier Fraz\\~ao, Andr\\'e Pimentel, Bernardete\n  Ribeiro", "title": "Automatic Graphic Logo Detection via Fast Region-based Convolutional\n  Networks", "comments": "7 pages, 9 figures, IJCNN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brand recognition is a very challenging topic with many useful applications\nin localization recognition, advertisement and marketing. In this paper we\npresent an automatic graphic logo detection system that robustly handles\nunconstrained imaging conditions. Our approach is based on Fast Region-based\nConvolutional Networks (FRCN) proposed by Ross Girshick, which have shown\nstate-of-the-art performance in several generic object recognition tasks\n(PASCAL Visual Object Classes challenges). In particular, we use two CNN models\npre-trained with the ILSVRC ImageNet dataset and we look at the selective\nsearch of windows `proposals' in the pre-processing stage and data augmentation\nto enhance the logo recognition rate. The novelty lies in the use of transfer\nlearning to leverage powerful Convolutional Neural Network models trained with\nlarge-scale datasets and repurpose them in the context of graphic logo\ndetection. Another benefit of this framework is that it allows for multiple\ndetections of graphic logos using regions that are likely to have an object.\nExperimental results with the FlickrLogos-32 dataset show not only the\npromising performance of our developed models with respect to noise and other\ntransformations a graphic logo can be subject to, but also its superiority over\nstate-of-the-art systems with hand-crafted models and features.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 19:54:01 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Oliveira", "Gon\u00e7alo", ""], ["Fraz\u00e3o", "Xavier", ""], ["Pimentel", "Andr\u00e9", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "1604.06119", "submitter": "Karim Ahmed", "authors": "Karim Ahmed, Mohammad Haris Baig, and Lorenzo Torresani", "title": "Network of Experts for Large-Scale Image Categorization", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tree-structured network architecture for large scale image\nclassification. The trunk of the network contains convolutional layers\noptimized over all classes. At a given depth, the trunk splits into separate\nbranches, each dedicated to discriminate a different subset of classes. Each\nbranch acts as an expert classifying a set of categories that are difficult to\ntell apart, while the trunk provides common knowledge to all experts in the\nform of shared features. The training of our \"network of experts\" is completely\nend-to-end: the partition of categories into disjoint subsets is learned\nsimultaneously with the parameters of the network trunk and the experts are\ntrained jointly by minimizing a single learning objective over all classes. The\nproposed structure can be built from any existing convolutional neural network\n(CNN). We demonstrate its generality by adapting 4 popular CNNs for image\ncategorization into the form of networks of experts. Our experiments on\nCIFAR100 and ImageNet show that in every case our method yields a substantial\nimprovement in accuracy over the base CNN, and gives the best result achieved\nso far on CIFAR100. Finally, the improvement in accuracy comes at little\nadditional cost: compared to the base network, the training time is only\nmoderately increased and the number of parameters is comparable or in some\ncases even lower.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 20:34:44 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 02:47:32 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 16:25:14 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Ahmed", "Karim", ""], ["Baig", "Mohammad Haris", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1604.06154", "submitter": "Xichuan Zhou", "authors": "Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong\n  Hu, Shujun Liu, Zhi Lin", "title": "Deep Adaptive Network: An Efficient Deep Neural Network with Sparse\n  Binary Connections", "comments": "10 pages, extended and submitted to IEEE Transactions of Systems,\n  Man, and Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are state-of-the-art models for understanding the\ncontent of images, video and raw input data. However, implementing a deep\nneural network in embedded systems is a challenging task, because a typical\ndeep neural network, such as a Deep Belief Network using 128x128 images as\ninput, could exhaust Giga bytes of memory and result in bandwidth and computing\nbottleneck. To address this challenge, this paper presents a hardware-oriented\ndeep learning algorithm, named as the Deep Adaptive Network, which attempts to\nexploit the sparsity in the neural connections. The proposed method adaptively\nreduces the weights associated with negligible features to zero, leading to\nsparse feedforward network architecture. Furthermore, since the small\nproportion of important weights are significantly larger than zero, they can be\nrobustly thresholded and represented using single-bit integers (-1 and +1),\nleading to implementations of deep neural networks with sparse and binary\nconnections. Our experiments showed that, for the application of recognizing\nMNIST handwritten digits, the features extracted by a two-layer Deep Adaptive\nNetwork with about 25% reserved important connections achieved 97.2%\nclassification accuracy, which was almost the same with the standard Deep\nBelief Network (97.3%). Furthermore, for efficient hardware implementations,\nthe sparse-and-binary-weighted deep neural network could save about 99.3%\nmemory and 99.9% computation units without significant loss of classification\naccuracy for pattern recognition applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 01:47:33 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Zhou", "Xichuan", ""], ["Li", "Shengli", ""], ["Qin", "Kai", ""], ["Li", "Kunping", ""], ["Tang", "Fang", ""], ["Hu", "Shengdong", ""], ["Liu", "Shujun", ""], ["Lin", "Zhi", ""]]}, {"id": "1604.06182", "submitter": "Haroon Idrees", "authors": "Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev,\n  Rahul Sukthankar, Mubarak Shah", "title": "The THUMOS Challenge on Action Recognition for Videos \"in the Wild\"", "comments": "Preprint submitted to Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2016.10.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically recognizing and localizing wide ranges of human actions has\ncrucial importance for video understanding. Towards this goal, the THUMOS\nchallenge was introduced in 2013 to serve as a benchmark for action\nrecognition. Until then, video action recognition, including THUMOS challenge,\nhad focused primarily on the classification of pre-segmented (i.e., trimmed)\nvideos, which is an artificial task. In THUMOS 2014, we elevated action\nrecognition to a more practical level by introducing temporally untrimmed\nvideos. These also include `background videos' which share similar scenes and\nbackgrounds as action videos, but are devoid of the specific actions. The three\neditions of the challenge organized in 2013--2015 have made THUMOS a common\nbenchmark for action classification and detection and the annual challenge is\nwidely attended by teams from around the world.\n  In this paper we describe the THUMOS benchmark in detail and give an overview\nof data collection and annotation procedures. We present the evaluation\nprotocols used to quantify results in the two THUMOS tasks of action\nclassification and temporal detection. We also present results of submissions\nto the THUMOS 2015 challenge and review the participating approaches.\nAdditionally, we include a comprehensive empirical study evaluating the\ndifferences in action recognition between trimmed and untrimmed videos, and how\nwell methods trained on trimmed videos generalize to untrimmed videos. We\nconclude by proposing several directions and improvements for future THUMOS\nchallenges.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 05:08:59 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Idrees", "Haroon", ""], ["Zamir", "Amir R.", ""], ["Jiang", "Yu-Gang", ""], ["Gorban", "Alex", ""], ["Laptev", "Ivan", ""], ["Sukthankar", "Rahul", ""], ["Shah", "Mubarak", ""]]}, {"id": "1604.06195", "submitter": "Emad Barsoum", "authors": "Emad Barsoum", "title": "Articulated Hand Pose Estimation Review", "comments": "Review of state of the art articulated hand pose estimation since\n  2007, this paper was written in May 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase number of companies focusing on commercializing Augmented\nReality (AR), Virtual Reality (VR) and wearable devices, the need for a hand\nbased input mechanism is becoming essential in order to make the experience\nnatural, seamless and immersive. Hand pose estimation has progressed\ndrastically in recent years due to the introduction of commodity depth cameras.\n  Hand pose estimation based on vision is still a challenging problem due to\nits complexity from self-occlusion (between fingers), close similarity between\nfingers, dexterity of the hands, speed of the pose and the high dimension of\nthe hand kinematic parameters. Articulated hand pose estimation is still an\nopen problem and under intensive research from both academia and industry.\n  The 2 approaches used for hand pose estimation are: discriminative and\ngenerative. Generative approach is a model based that tries to fit a hand model\nto the observed data. Discriminative approach is appearance based, usually\nimplemented with machine learning (ML) and require a large amount of training\ndata. Recent hand pose estimation uses hybrid approach by combining both\ndiscriminative and generative methods into a single hand pipeline.\n  In this paper, we focus on reviewing recent progress of hand pose estimation\nfrom depth sensor. We will survey discriminative methods, generative methods\nand hybrid methods. This paper is not a comprehensive review of all hand pose\nestimation techniques, it is a subset of some of the recent state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 06:55:42 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Barsoum", "Emad", ""]]}, {"id": "1604.06232", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni and Matteo Matteucci", "title": "Incremental Reconstruction of Urban Environments by Edge-Points Delaunay\n  Triangulation", "comments": "Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International\n  Conference on (IROS) 2015. http://hdl.handle.net/11311/972021", "journal-ref": null, "doi": "10.1109/IROS.2015.7354012", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban reconstruction from a video captured by a surveying vehicle constitutes\na core module of automated mapping. When computational power represents a\nlimited resource and, a detailed map is not the primary goal, the\nreconstruction can be performed incrementally, from a monocular video, carving\na 3D Delaunay triangulation of sparse points; this allows online incremental\nmapping for tasks such as traversability analysis or obstacle avoidance. To\nexploit the sharp edges of urban landscape, we propose to use a Delaunay\ntriangulation of Edge-Points, which are the 3D points corresponding to image\nedges. These points constrain the edges of the 3D Delaunay triangulation to\nreal-world edges. Besides the use of the Edge-Points, a second contribution of\nthis paper is the Inverse Cone Heuristic that preemptively avoids the creation\nof artifacts in the reconstructed manifold surface. We force the reconstruction\nof a manifold surface since it makes it possible to apply computer graphics or\nphotometric refinement algorithms to the output mesh. We evaluated our approach\non four real sequences of the public available KITTI dataset by comparing the\nincremental reconstruction against Velodyne measurements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 09:59:42 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 13:11:03 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1604.06242", "submitter": "Nomi Vinokurov", "authors": "Nomi Vinokurov and Daphna Weinshall", "title": "Novelty Detection in MultiClass Scenarios with Incomplete Set of Class\n  Labels", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of novelty detection in multiclass scenarios where\nsome class labels are missing from the training set. Our method is based on the\ninitial assignment of confidence values, which measure the affinity between a\nnew test point and each known class. We first compare the values of the two top\nelements in this vector of confidence values. In the heart of our method lies\nthe training of an ensemble of classifiers, each trained to discriminate known\nfrom novel classes based on some partition of the training data into\npresumed-known and presumednovel classes. Our final novelty score is derived\nfrom the output of this ensemble of classifiers. We evaluated our method on two\ndatasets of images containing a relatively large number of classes - the\nCaltech-256 and Cifar-100 datasets. We compared our method to 3 alternative\nmethods which represent commonly used approaches, including the one-class SVM,\nnovelty based on k-NN, novelty based on maximal confidence, and the recent\nKNFST method. The results show a very clear and marked advantage for our method\nover all alternative methods, in an experimental setup where class labels are\nmissing during training.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 10:18:26 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 16:44:15 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Vinokurov", "Nomi", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1604.06243", "submitter": "Sounak Dey", "authors": "Sounak Dey, Anguelos Nicolaou, Josep Llados, and Umapada Pal", "title": "Evaluation of the Effect of Improper Segmentation on Word Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word spotting is an important recognition task in historical document\nanalysis. In most cases methods are developed and evaluated assuming perfect\nword segmentations. In this paper we propose an experimental framework to\nquantify the effect of goodness of word segmentation has on the performance\nachieved by word spotting methods in identical unbiased conditions. The\nframework consists of generating systematic distortions on segmentation and\nretrieving the original queries from the distorted dataset. We apply the\nframework on the George Washington and Barcelona Marriage Dataset and on\nseveral established and state-of-the-art methods. The experiments allow for an\nestimate of the end-to-end performance of word spotting methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 10:20:12 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Dey", "Sounak", ""], ["Nicolaou", "Anguelos", ""], ["Llados", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "1604.06258", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni and Ama\\\"el Delaunoy and Marc Pollefeys and Matteo\n  Matteucci", "title": "Automatic 3D Reconstruction of Manifold Meshes via Delaunay\n  Triangulation and Mesh Sweeping", "comments": "in IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach to incrementally initialize a\nmanifold surface for automatic 3D reconstruction from images. More precisely we\nfocus on the automatic initialization of a 3D mesh as close as possible to the\nfinal solution; indeed many approaches require a good initial solution for\nfurther refinement via multi-view stereo techniques. Our novel algorithm\nautomatically estimates an initial manifold mesh for surface evolving\nmulti-view stereo algorithms, where the manifold property needs to be enforced.\nIt bootstraps from 3D points extracted via Structure from Motion, then iterates\nbetween a state-of-the-art manifold reconstruction step and a novel mesh\nsweeping algorithm that looks for new 3D points in the neighborhood of the\nreconstructed manifold to be added in the manifold reconstruction. The\nexperimental results show quantitatively that the mesh sweeping improves the\nresolution and the accuracy of the manifold reconstruction, allowing a better\nconvergence of state-of-the-art surface evolution multi-view stereo algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 11:10:19 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Romanoni", "Andrea", ""], ["Delaunoy", "Ama\u00ebl", ""], ["Pollefeys", "Marc", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1604.06318", "submitter": "Nikolay Savinov", "authors": "Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, Marc Pollefeys", "title": "TI-POOLING: transformation-invariant pooling for feature learning in\n  Convolutional Neural Networks", "comments": "Accepted at CVPR 2016. The first two authors assert equal\n  contribution and joint first authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deep neural network topology that incorporates a\nsimple to implement transformation invariant pooling operator (TI-POOLING).\nThis operator is able to efficiently handle prior knowledge on nuisance\nvariations in the data, such as rotation or scale changes. Most current methods\nusually make use of dataset augmentation to address this issue, but this\nrequires larger number of model parameters and more training data, and results\nin significantly increased training time and larger chance of under- or\noverfitting. The main reason for these drawbacks is that the learned model\nneeds to capture adequate features for all the possible transformations of the\ninput. On the other hand, we formulate features in convolutional neural\nnetworks to be transformation-invariant. We achieve that using parallel siamese\narchitectures for the considered transformation set and applying the TI-POOLING\noperator on their outputs before the fully-connected layers. We show that this\ntopology internally finds the most optimal \"canonical\" instance of the input\nimage for training and therefore limits the redundancy in learned features.\nThis more efficient use of training data results in better performance on\npopular benchmark datasets with smaller number of parameters when comparing to\nstandard convolutional neural networks with dataset augmentation and to other\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 14:17:05 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 14:42:28 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Laptev", "Dmitry", ""], ["Savinov", "Nikolay", ""], ["Buhmann", "Joachim M.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1604.06397", "submitter": "Yang Wang", "authors": "Yang Wang and Minh Hoai", "title": "Improving Human Action Recognition by Non-action Classification", "comments": "appears in CVPR16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the task of recognizing human actions in realistic\nvideo where human actions are dominated by irrelevant factors. We first study\nthe benefits of removing non-action video segments, which are the ones that do\nnot portray any human action. We then learn a non-action classifier and use it\nto down-weight irrelevant video segments. The non-action classifier is trained\nusing ActionThread, a dataset with shot-level annotation for the occurrence or\nabsence of a human action. The non-action classifier can be used to identify\nnon-action shots with high precision and subsequently used to improve the\nperformance of action recognition systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 17:46:25 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 02:50:12 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Wang", "Yang", ""], ["Hoai", "Minh", ""]]}, {"id": "1604.06427", "submitter": "Uche Nnolim", "authors": "Uche A. Nnolim", "title": "Analysis of the Entropy-guided Switching Trimmed Mean Deviation-based\n  Anisotropic Diffusion filter", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.25.4.043001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the experimental analysis of a proposed switching\nfilter-anisotropic diffusion hybrid for the filtering of the fixed value (salt\nand pepper) impulse noise (FVIN). The filter works well at both low and high\nnoise densities though it was specifically designed for high noise density\nlevels. The filter combines the switching mechanism of decision-based filters\nand the partial differential equation-based formulation to yield a powerful\nsystem capable of recovering the image signals at very high noise levels.\nExperimental results indicate that the filter surpasses other filters,\nespecially at very high noise levels. Additionally, its adaptive nature ensures\nthat the performance is guided by the metrics obtained from the noisy input\nimage. The filter algorithm is of both global and local nature, where the\nformer is chosen to reduce computation time and complexity, while the latter is\nused for best results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 19:06:59 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:15:50 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Nnolim", "Uche A.", ""]]}, {"id": "1604.06433", "submitter": "Jing Wang", "authors": "Jing Wang, Yu Cheng, Rogerio Schmidt Feris", "title": "Walk and Learn: Facial Attribute Representation Learning from Egocentric\n  Video and Contextual Data", "comments": "Paper accepted by CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way people look in terms of facial attributes (ethnicity, hair color,\nfacial hair, etc.) and the clothes or accessories they wear (sunglasses, hat,\nhoodies, etc.) is highly dependent on geo-location and weather condition,\nrespectively. This work explores, for the first time, the use of this\ncontextual information, as people with wearable cameras walk across different\nneighborhoods of a city, in order to learn a rich feature representation for\nfacial attribute classification, without the costly manual annotation required\nby previous methods. By tracking the faces of casual walkers on more than 40\nhours of egocentric video, we are able to cover tens of thousands of different\nidentities and automatically extract nearly 5 million pairs of images connected\nby or from different face tracks, along with their weather and location\ncontext, under pose and lighting variations. These image pairs are then fed\ninto a deep network that preserves similarity of images connected by the same\ntrack, in order to capture identity-related attribute features, and optimizes\nfor location and weather prediction to capture additional facial attribute\nfeatures. Finally, the network is fine-tuned with manually annotated samples.\nWe perform an extensive experimental analysis on wearable data and two standard\nbenchmark datasets based on web images (LFWA and CelebA). Our method\noutperforms by a large margin a network trained from scratch. Moreover, even\nwithout using manually annotated identity labels for pre-training as in\nprevious methods, our approach achieves results that are better than the state\nof the art.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 19:21:55 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 17:07:33 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 20:51:33 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Wang", "Jing", ""], ["Cheng", "Yu", ""], ["Feris", "Rogerio Schmidt", ""]]}, {"id": "1604.06480", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David\n  A. Shamma", "title": "LOH and behold: Web-scale visual search, recommendation and clustering\n  using Locally Optimized Hashing", "comments": "Accepted for publication at the 4th Workshop on Web-scale Vision and\n  Social Media (VSM), ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hashing-based matching scheme, called Locally Optimized\nHashing (LOH), based on a state-of-the-art quantization algorithm that can be\nused for efficient, large-scale search, recommendation, clustering, and\ndeduplication. We show that matching with LOH only requires set intersections\nand summations to compute and so is easily implemented in generic distributed\ncomputing systems. We further show application of LOH to: a) large-scale search\ntasks where performance is on par with other state-of-the-art hashing\napproaches; b) large-scale recommendation where queries consisting of thousands\nof images can be used to generate accurate recommendations from collections of\nhundreds of millions of images; and c) efficient clustering with a graph-based\nalgorithm that can be scaled to massive collections in a distributed\nenvironment or can be used for deduplication for small collections, like search\nresults, performing better than traditional hashing approaches while only\nrequiring a few milliseconds to run. In this paper we experiment on datasets of\nup to 100 million images, but in practice our system can scale to larger\ncollections and can be used for other types of data that have a vector\nrepresentation in a Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:23:55 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 02:34:52 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Kennedy", "Lyndon", ""], ["Nguyen", "Huy", ""], ["Mellina", "Clayton", ""], ["Shamma", "David A.", ""]]}, {"id": "1604.06481", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Ayman Farahat, Lyndon Kennedy, Ricardo Baeza-Yates,\n  David A. Shamma", "title": "Visual Congruent Ads for Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of user experience online is affected by the relevance and\nplacement of advertisements. We propose a new system for selecting and\ndisplaying visual advertisements in image search result sets. Our method\ncompares the visual similarity of candidate ads to the image search results and\nselects the most visually similar ad to be displayed. The method further\nselects an appropriate location in the displayed image grid to minimize the\nperceptual visual differences between the ad and its neighbors. We conduct an\nexperiment with about 900 users and find that our proposed method provides\nsignificant improvement in the users' overall satisfaction with the image\nsearch experience, without diminishing the users' ability to see the ad or\nrecall the advertised brand.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:23:58 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Farahat", "Ayman", ""], ["Kennedy", "Lyndon", ""], ["Baeza-Yates", "Ricardo", ""], ["Shamma", "David A.", ""]]}, {"id": "1604.06486", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh,\n  Timoth\\'ee Masquelier", "title": "Humans and deep networks largely agree on which kinds of variation make\n  object recognition harder", "comments": null, "journal-ref": "Frontiers in Computational Neuroscience (2016) 10:92", "doi": "10.3389/fncom.2016.00092", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View-invariant object recognition is a challenging problem, which has\nattracted much attention among the psychology, neuroscience, and computer\nvision communities. Humans are notoriously good at it, even if some variations\nare presumably more difficult to handle than others (e.g. 3D rotations). Humans\nare thought to solve the problem through hierarchical processing along the\nventral stream, which progressively extracts more and more invariant visual\nfeatures. This feed-forward architecture has inspired a new generation of\nbio-inspired computer vision systems called deep convolutional neural networks\n(DCNN), which are currently the best algorithms for object recognition in\nnatural images. Here, for the first time, we systematically compared human\nfeed-forward vision and DCNNs at view-invariant object recognition using the\nsame images and controlling for both the kinds of transformation as well as\ntheir magnitude. We used four object categories and images were rendered from\n3D computer models. In total, 89 human subjects participated in 10 experiments\nin which they had to discriminate between two or four categories after rapid\npresentation with backward masking. We also tested two recent DCNNs on the same\ntasks. We found that humans and DCNNs largely agreed on the relative\ndifficulties of each kind of variation: rotation in depth is by far the hardest\ntransformation to handle, followed by scale, then rotation in plane, and\nfinally position. This suggests that humans recognize objects mainly through 2D\ntemplate matching, rather than by constructing 3D object models, and that DCNNs\nare not too unreasonable models of human feed-forward vision. Also, our results\nshow that the variation levels in rotation in depth and scale strongly modulate\nboth humans' and DCNNs' recognition performances. We thus argue that these\nvariations should be controlled in the image datasets used in vision research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:53:00 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ghodrati", "Masoud", ""], ["Ganjtabesh", "Mohammad", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1604.06506", "submitter": "Roeland De Geest", "authors": "Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees\n  Snoek, Tinne Tuytelaars", "title": "Online Action Detection", "comments": "Project page:\n  http://homes.esat.kuleuven.be/~rdegeest/OnlineActionDetection.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online action detection, the goal is to detect the start of an action in a\nvideo stream as soon as it happens. For instance, if a child is chasing a ball,\nan autonomous car should recognize what is going on and respond immediately.\nThis is a very challenging problem for four reasons. First, only partial\nactions are observed. Second, there is a large variability in negative data.\nThird, the start of the action is unknown, so it is unclear over what time\nwindow the information should be integrated. Finally, in real world data, large\nwithin-class variability exists. This problem has been addressed before, but\nonly to some extent. Our contributions to online action detection are\nthreefold. First, we introduce a realistic dataset composed of 27 episodes from\n6 popular TV series. The dataset spans over 16 hours of footage annotated with\n30 action classes, totaling 6,231 action instances. Second, we analyze and\ncompare various baseline methods, showing this is a challenging problem for\nwhich none of the methods provides a good solution. Third, we analyze the\nchange in performance when there is a variation in viewpoint, occlusion,\ntruncation, etc. We introduce an evaluation protocol for fair comparison. The\ndataset, the baselines and the models will all be made publicly available to\nencourage (much needed) further research on online action detection on\nrealistic data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 22:02:50 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 09:29:39 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["De Geest", "Roeland", ""], ["Gavves", "Efstratios", ""], ["Ghodrati", "Amir", ""], ["Li", "Zhenyang", ""], ["Snoek", "Cees", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1604.06525", "submitter": "Matthias Nie{\\ss}ner", "authors": "Zachary DeVito, Michael Mara, Michael Zollh\\\"ofer, Gilbert Bernstein,\n  Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher,\n  Matthias Nie{\\ss}ner", "title": "Opt: A Domain Specific Language for Non-linear Least Squares\n  Optimization in Graphics and Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graphics and vision problems can be expressed as non-linear least\nsquares optimizations of objective functions over visual data, such as images\nand meshes. The mathematical descriptions of these functions are extremely\nconcise, but their implementation in real code is tedious, especially when\noptimized for real-time performance on modern GPUs in interactive applications.\nIn this work, we propose a new language, Opt (available under\nhttp://optlang.org), for writing these objective functions over image- or\ngraph-structured unknowns concisely and at a high level. Our compiler\nautomatically transforms these specifications into state-of-the-art GPU solvers\nbased on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate\ndifferent variations of the solver, so users can easily explore tradeoffs in\nnumerical precision, matrix-free methods, and solver approaches. In our\nresults, we implement a variety of real-world graphics and vision applications.\nTheir energy functions are expressible in tens of lines of code, and produce\nhighly-optimized GPU solver implementations. These solver have performance\ncompetitive with the best published hand-tuned, application-specific GPU\nsolvers, and orders of magnitude beyond a general-purpose auto-generated\nsolver.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 03:02:59 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 00:19:31 GMT"}, {"version": "v3", "created": "Sat, 9 Sep 2017 13:23:33 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["DeVito", "Zachary", ""], ["Mara", "Michael", ""], ["Zollh\u00f6fer", "Michael", ""], ["Bernstein", "Gilbert", ""], ["Ragan-Kelley", "Jonathan", ""], ["Theobalt", "Christian", ""], ["Hanrahan", "Pat", ""], ["Fisher", "Matthew", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1604.06570", "submitter": "Jubin Johnson", "authors": "Hisham Cholakkal, Jubin Johnson and Deepu Rajan", "title": "A Classifier-guided Approach for Top-down Salient Object Detection", "comments": "To appear in Signal Processing: Image Communication, Elsevier.\n  Available online from April 2016", "journal-ref": null, "doi": "10.1016/j.image.2016.04.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for top-down salient object detection that\nincorporates a tightly coupled image classification module. The classifier is\ntrained on novel category-aware sparse codes computed on object dictionaries\nused for saliency modeling. A misclassification indicates that the\ncorresponding saliency model is inaccurate. Hence, the classifier selects\nimages for which the saliency models need to be updated. The category-aware\nsparse coding produces better image classification accuracy as compared to\nconventional sparse coding with a reduced computational complexity. A\nsaliency-weighted max-pooling is proposed to improve image classification,\nwhich is further used to refine the saliency maps. Experimental results on\nGraz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salient\nobject detection. Although the role of the classifier is to support salient\nobject detection, we evaluate its performance in image classification and also\nillustrate the utility of thresholded saliency maps for image segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 08:43:34 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Cholakkal", "Hisham", ""], ["Johnson", "Jubin", ""], ["Rajan", "Deepu", ""]]}, {"id": "1604.06573", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman", "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition", "comments": "in Proc. CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications of Convolutional Neural Networks (ConvNets) for human\naction recognition in videos have proposed different solutions for\nincorporating the appearance and motion information. We study a number of ways\nof fusing ConvNet towers both spatially and temporally in order to best take\nadvantage of this spatio-temporal information. We make the following findings:\n(i) that rather than fusing at the softmax layer, a spatial and temporal\nnetwork can be fused at a convolution layer without loss of performance, but\nwith a substantial saving in parameters; (ii) that it is better to fuse such\nnetworks spatially at the last convolutional layer than earlier, and that\nadditionally fusing at the class prediction layer can boost accuracy; finally\n(iii) that pooling of abstract convolutional features over spatiotemporal\nneighbourhoods further boosts performance. Based on these studies we propose a\nnew ConvNet architecture for spatiotemporal fusion of video snippets, and\nevaluate its performance on standard benchmarks where this architecture\nachieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 08:51:17 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 10:47:21 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Pinz", "Axel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1604.06582", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza, Andrea Zunino, Marco San Biagio and Vittorio Murino", "title": "Kernelized Covariance for Action Recognition", "comments": "Accepted paper at the 23rd International Conference on Pattern\n  Recognition (ICPR), Cancun, Mexico, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim at increasing the descriptive power of the covariance\nmatrix, limited in capturing linear mutual dependencies between variables only.\nWe present a rigorous and principled mathematical pipeline to recover the\nkernel trick for computing the covariance matrix, enhancing it to model more\ncomplex, non-linear relationships conveyed by the raw data. To this end, we\npropose Kernelized-COV, which generalizes the original covariance\nrepresentation without compromising the efficiency of the computation. In the\nexperiments, we validate the proposed framework against many previous\napproaches in the literature, scoring on par or superior with respect to the\nstate of the art on benchmark datasets for 3D action recognition.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 09:16:22 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 12:05:09 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Zunino", "Andrea", ""], ["Biagio", "Marco San", ""], ["Murino", "Vittorio", ""]]}, {"id": "1604.06620", "submitter": "Jiandong Meng", "authors": "Ru-Ze Liang and Lihui Shi and Haoxiang Wang and Jiandong Meng and Jim\n  Jing-Yan Wang and Qingquan Sun and Yi Gu", "title": "Optimizing Top Precision Performance Measure of Content-Based Image\n  Retrieval by Learning Similarity Function", "comments": "Pattern Recognition (ICPR), 2016 23st International Conference on", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of content-based image retrieval. In this\nproblem, the most popular performance measure is the top precision measure, and\nthe most important component of a retrieval system is the similarity function\nused to compare a query image against a database image. However, up to now,\nthere is no existing similarity learning method proposed to optimize the top\nprecision measure. To fill this gap, in this paper, we propose a novel\nsimilarity learning method to maximize the top precision measure. We model this\nproblem as a minimization problem with an objective function as the combination\nof the losses of the relevant images ranked behind the top-ranked irrelevant\nimage, and the squared Frobenius norm of the similarity function parameter.\nThis minimization problem is solved as a quadratic programming problem. The\nexperiments over two benchmark data sets show the advantages of the proposed\nmethod over other similarity learning methods when the top precision is used as\nthe performance measure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:15:56 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 15:02:46 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 06:31:46 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 05:42:26 GMT"}, {"version": "v5", "created": "Sat, 20 Aug 2016 06:15:22 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Liang", "Ru-Ze", ""], ["Shi", "Lihui", ""], ["Wang", "Haoxiang", ""], ["Meng", "Jiandong", ""], ["Wang", "Jim Jing-Yan", ""], ["Sun", "Qingquan", ""], ["Gu", "Yi", ""]]}, {"id": "1604.06626", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "The Mean Partition Theorem of Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To devise efficient solutions for approximating a mean partition in consensus\nclustering, Dimitriadou et al. [3] presented a necessary condition of\noptimality for a consensus function based on least square distances. We show\nthat their result is pivotal for deriving interesting properties of consensus\nclustering beyond optimization. For this, we present the necessary condition of\noptimality in a slightly stronger form in terms of the Mean Partition Theorem\nand extend it to the Expected Partition Theorem. To underpin its versatility,\nwe show three examples that apply the Mean Partition Theorem: (i) equivalence\nof the mean partition and optimal multiple alignment, (ii) construction of\nprofiles and motifs, and (iii) relationship between consensus clustering and\ncluster stability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:32:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 06:55:06 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1604.06646", "submitter": "Ankush Gupta", "authors": "Ankush Gupta and Andrea Vedaldi and Andrew Zisserman", "title": "Synthetic Data for Text Localisation in Natural Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 13:23:08 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Gupta", "Ankush", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1604.06665", "submitter": "Leonie Zeune", "authors": "Leonie Zeune, Guus van Dalum, Leon W.M.M. Terstappen, S.A. van Gils,\n  Christoph Brune", "title": "Multiscale Segmentation via Bregman Distances and Nonlinear Spectral\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical imaging reliable segmentation of objects (e.g. from small cells\nup to large organs) is of fundamental importance for automated medical\ndiagnosis. New approaches for multi-scale segmentation can considerably improve\nperformance in case of natural variations in intensity, size and shape. This\npaper aims at segmenting objects of interest based on shape contours and\nautomatically finding multiple objects with different scales. The overall\nstrategy of this work is to combine nonlinear segmentation with scales spaces\nand spectral decompositions recently introduced in literature. For this we\ngeneralize a variational segmentation model based on total variation using\nBregman distances to construct an inverse scale space. This offers the new\nmodel to be accomplished by a scale analysis approach based on a spectral\ndecomposition of the total variation. As a result we obtain a very efficient,\n(nearly) parameter-free multiscale segmentation method that comes with an\nadaptive regularization parameter choice. The added benefit of our method is\ndemonstrated by systematic synthetic tests and its usage in a new biomedical\ntoolbox for identifying and classifying circulating tumor cells. Due to the\nnature of nonlinear diffusion underlying, the mathematical concepts in this\nwork offer promising extensions to nonlocal classification problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 14:06:42 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 09:43:09 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zeune", "Leonie", ""], ["van Dalum", "Guus", ""], ["Terstappen", "Leon W. M. M.", ""], ["van Gils", "S. A.", ""], ["Brune", "Christoph", ""]]}, {"id": "1604.06720", "submitter": "Diego Marcos", "authors": "Diego Marcos, Michele Volpi, Devis Tuia", "title": "Learning rotation invariant convolutional filters for texture\n  classification", "comments": "6 pages, in ICPR 2016", "journal-ref": null, "doi": "10.1109/ICPR.2016.7899932", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning discriminative filters using a shallow\nConvolutional Neural Network (CNN). We encode rotation invariance directly in\nthe model by tying the weights of groups of filters to several rotated versions\nof the canonical filter in the group. These filters can be used to extract\nrotation invariant features well-suited for image classification. We test this\nlearning procedure on a texture classification benchmark, where the\norientations of the training images differ from those of the test images. We\nobtain results comparable to the state-of-the-art. Compared to standard shallow\nCNNs, the proposed method obtains higher classification performance while\nreducing by an order of magnitude the number of parameters to be learned.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:55:37 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 09:41:48 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Marcos", "Diego", ""], ["Volpi", "Michele", ""], ["Tuia", "Devis", ""]]}, {"id": "1604.06832", "submitter": "S Shankar", "authors": "Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi,\n  Roberto Cipolla", "title": "Refining Architectures of Deep Convolutional Neural Networks", "comments": "9 pages, 6 figures, CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have recently evinced immense\nsuccess for various image recognition tasks. However, a question of paramount\nimportance is somewhat unanswered in deep learning research - is the selected\nCNN optimal for the dataset in terms of accuracy and model size? In this paper,\nwe intend to answer this question and introduce a novel strategy that alters\nthe architecture of a given CNN for a specified dataset, to potentially enhance\nthe original accuracy while possibly reducing the model size. We use two\noperations for architecture refinement, viz. stretching and symmetrical\nsplitting. Our procedure starts with a pre-trained CNN for a given dataset, and\noptimally decides the stretch and split factors across the network to refine\nthe architecture. We empirically demonstrate the necessity of the two\noperations. We evaluate our approach on two natural scenes attributes datasets,\nSUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that\nare quite contrasting in their construction. We justify our choice of datasets,\nand show that they are interestingly distinct from each other, and together\npose a challenge to our architectural refinement algorithm. Our results\nsubstantiate the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 22:39:55 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Shankar", "Sukrit", ""], ["Robertson", "Duncan", ""], ["Ioannou", "Yani", ""], ["Criminisi", "Antonio", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1604.06838", "submitter": "Xirong Li", "authors": "Jianfeng Dong and Xirong Li and Cees G. M. Snoek", "title": "Word2VisualVec: Image and Video to Sentence Matching by Visual Feature\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to find the sentence best describing the content of an\nimage or video. Different from existing works, which rely on a joint subspace\nfor image / video to sentence matching, we propose to do so in a visual space\nonly. We contribute Word2VisualVec, a deep neural network architecture that\nlearns to predict a deep visual encoding of textual input based on sentence\nvectorization and a multi-layer perceptron. We thoroughly analyze its\narchitectural design, by varying the sentence vectorization strategy, network\ndepth and the deep feature to predict for image to sentence matching. We also\ngeneralize Word2VisualVec for matching a video to a sentence, by extending the\npredictive abilities to 3-D ConvNet features as well as a visual-audio\nrepresentation. Experiments on four challenging image and video benchmarks\ndetail Word2VisualVec's properties, capabilities for image and video to\nsentence matching, and on all datasets its state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 00:28:17 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 06:06:31 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1604.06852", "submitter": "Sin Yonghak", "authors": "Changyong Ri, Duho Pak, Cholryong Choe, Suhyang Kim, Yonghak Sin", "title": "Contextual object categorization with energy-based model", "comments": "13 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object categorization is a hot issue of an image mining. Contextual\ninformation between objects is one of the important semantic knowledge of an\nimage. However, the previous researches for an object categorization have not\nmade full use of the contextual information, especially the spatial relations\nbetween objects. In addition, the object categorization methods, which\ngenerally use the probabilistic graphical models to implement the incorporation\nof contextual information with appearance of objects, are almost inevitable to\nevaluate the intractable partition function for normalization. In this work, we\nintroduced fully-connected fuzzy spatial relations including directional,\ndistance and topological relations between object regions, so the spatial\nrelational information could be fully utilized. Then, the spatial relations\nwere considered as well as co-occurrence and appearance of objects by using\nenergy-based model, where the energy function was defined as the region-object\nassociation potential and the configuration potential of objects. Minimizing\nthe energy function of whole image arrangement, we obtained the optimal label\nset about the image regions and addressed the evaluation of intractable\npartition function in conditional random fields. Experimental results show the\nvalidity and reliability of this proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 02:49:11 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Ri", "Changyong", ""], ["Pak", "Duho", ""], ["Choe", "Cholryong", ""], ["Kim", "Suhyang", ""], ["Sin", "Yonghak", ""]]}, {"id": "1604.06877", "submitter": "Shangxuan Tian", "authors": "Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, and Chew\n  Lim Tan", "title": "Text Flow: A Unified Text Detection System in Natural Scene Images", "comments": "9 pages, ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalent scene text detection approach follows four sequential steps\ncomprising character candidate detection, false character candidate removal,\ntext line extraction, and text line verification. However, errors occur and\naccumulate throughout each of these sequential steps which often lead to low\ndetection performance. To address these issues, we propose a unified scene text\ndetection system, namely Text Flow, by utilizing the minimum cost (min-cost)\nflow network model. With character candidates detected by cascade boosting, the\nmin-cost flow network model integrates the last three sequential steps into a\nsingle process which solves the error accumulation problem at both character\nlevel and text line level effectively. The proposed technique has been tested\non three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a\nmultilingual dataset and it outperforms the state-of-the-art methods on all\nthree datasets with much higher recall and F-score. The good performance on the\nmultilingual dataset shows that the proposed technique can be used for the\ndetection of texts in different languages.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 08:11:17 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Tian", "Shangxuan", ""], ["Pan", "Yifeng", ""], ["Huang", "Chang", ""], ["Lu", "Shijian", ""], ["Yu", "Kai", ""], ["Tan", "Chew Lim", ""]]}, {"id": "1604.06939", "submitter": "Rudrasis Chakraborty Mr", "authors": "Rudrasis Chakraborty, Monami Banerjee, Victoria Crawford and Baba C.\n  Vemuri", "title": "An information theoretic formulation of the Dictionary Learning and\n  Sparse Coding Problems on Statistical Manifolds", "comments": "This paper has been withdrawn by the author due to major change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel information theoretic framework for\ndictionary learning (DL) and sparse coding (SC) on a statistical manifold (the\nmanifold of probability distributions). Unlike the traditional DL and SC\nframework, our new formulation {\\it does not explicitly incorporate any\nsparsity inducing norm in the cost function but yet yields SCs}. Moreover, we\nextend this framework to the manifold of symmetric positive definite matrices,\n$\\mathcal{P}_n$. Our algorithm approximates the data points, which are\nprobability distributions, by the weighted Kullback-Leibeler center (KL-center)\nof the dictionary atoms. The KL-center is the minimizer of the maximum\nKL-divergence between the unknown center and members of the set whose center is\nbeing sought. Further, {\\it we proved that this KL-center is a sparse\ncombination of the dictionary atoms}. Since, the data reside on a statistical\nmanifold, the data fidelity term can not be as simple as in the case of the\nvector-space data. We therefore employ the geodesic distance between the data\nand a sparse approximation of the data element. This cost function is minimized\nusing an acceleterated gradient descent algorithm. An extensive set of\nexperimental results show the effectiveness of our proposed framework. We\npresent several experiments involving a variety of classification problems in\nComputer Vision applications. Further, we demonstrate the performance of our\nalgorithm by comparing it to several state-of-the-art methods both in terms of\nclassification accuracy and sparsity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 19:15:42 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 12:28:52 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Banerjee", "Monami", ""], ["Crawford", "Victoria", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1604.06970", "submitter": "Clayton Morrison", "authors": "Ernesto Brau, Colin Dawson, Alfredo Carrillo, David Sidi and Clayton\n  T. Morrison", "title": "Bayesian Inference of Recursive Sequences of Group Activities from\n  Tracks", "comments": "10 pages, 6 figures, in Proceedings of the 30th AAAI Conference on\n  Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic generative model for inferring a description of\ncoordinated, recursively structured group activities at multiple levels of\ntemporal granularity based on observations of individuals' trajectories. The\nmodel accommodates: (1) hierarchically structured groups, (2) activities that\nare temporally and compositionally recursive, (3) component roles assigning\ndifferent subactivity dynamics to subgroups of participants, and (4) a\nnonparametric Gaussian Process model of trajectories. We present an MCMC\nsampling framework for performing joint inference over recursive activity\ndescriptions and assignment of trajectories to groups, integrating out\ncontinuous parameters. We demonstrate the model's expressive power in several\nsimulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event\nvideo data sets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 00:55:27 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Brau", "Ernesto", ""], ["Dawson", "Colin", ""], ["Carrillo", "Alfredo", ""], ["Sidi", "David", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1604.06979", "submitter": "V S R Veeravasarapu", "authors": "V S R Veeravasarapu, Jayanthi Sivaswamy, Vishanji Karani", "title": "Cardiac Motion Analysis by Temporal Flow Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cardiac motion analysis from B-mode ultrasound sequence is a key task in\nassessing the health of the heart. The paper proposes a new methodology for\ncardiac motion analysis based on the temporal behaviour of points of interest\non the myocardium. We define a new signal called the Temporal Flow Graph (TFG)\nwhich depicts the movement of a point of interest over time. It is a graphical\nrepresentation derived from a flow field and describes the temporal evolution\nof a point. We prove that TFG for an object undergoing periodic motion is also\nperiodic. This principle can be utilized to derive both global and local\ninformation from a given sequence. We demonstrate this for detecting motion\nirregularities at the sequence, as well as regional levels on real and\nsynthetic data. A coarse localisation of anatomical landmarks such as centres\nof left/right cavities and valve points is also demonstrated using TFGs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 03:35:15 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Veeravasarapu", "V S R", ""], ["Sivaswamy", "Jayanthi", ""], ["Karani", "Vishanji", ""]]}, {"id": "1604.07043", "submitter": "Mengchen Liu", "authors": "Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, and Shixia\n  Liu", "title": "Towards Better Analysis of Deep Convolutional Neural Networks", "comments": "Submitted to VIS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved breakthrough\nperformance in many pattern recognition tasks such as image classification.\nHowever, the development of high-quality deep models typically relies on a\nsubstantial amount of trial-and-error, as there is still no clear understanding\nof when and why a deep model works. In this paper, we present a visual\nanalytics approach for better understanding, diagnosing, and refining deep\nCNNs. We formulate a deep CNN as a directed acyclic graph. Based on this\nformulation, a hybrid visualization is developed to disclose the multiple\nfacets of each neuron and the interactions between them. In particular, we\nintroduce a hierarchical rectangle packing algorithm and a matrix reordering\nalgorithm to show the derived features of a neuron cluster. We also propose a\nbiclustering-based edge bundling method to reduce visual clutter caused by a\nlarge number of connections between neurons. We evaluated our method on a set\nof CNNs and the results are generally favorable.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 15:53:22 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 07:27:07 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 08:17:19 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Liu", "Mengchen", ""], ["Shi", "Jiaxin", ""], ["Li", "Zhen", ""], ["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Liu", "Shixia", ""]]}, {"id": "1604.07045", "submitter": "Mario Valerio Giuffrida", "authors": "Mario Valerio Giuffrida and Sotirios A. Tsaftaris", "title": "Rotation-Invariant Restricted Boltzmann Machine Using Shared Gradient\n  Filters", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1007/978-3-319-44781-0_57", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding suitable features has been an essential problem in computer vision.\nWe focus on Restricted Boltzmann Machines (RBMs), which, despite their\nversatility, cannot accommodate transformations that may occur in the scene. As\na result, several approaches have been proposed that consider a set of\ntransformations, which are used to either augment the training set or transform\nthe actual learned filters. In this paper, we propose the Explicit\nRotation-Invariant Restricted Boltzmann Machine, which exploits prior\ninformation coming from the dominant orientation of images. Our model extends\nthe standard RBM, by adding a suitable number of weight matrices, associated\nwith each dominant gradient. We show that our approach is able to learn\nrotation-invariant features, comparing it with the classic formulation of RBM\non the MNIST benchmark dataset. Overall, requiring less hidden units, our\nmethod learns compact features, which are robust to rotations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 15:56:18 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 09:59:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Giuffrida", "Mario Valerio", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1604.07057", "submitter": "Cheng Yaw Low", "authors": "Cheng Yaw Low, Andrew Beng Jin Teoh, Cong Jie Ng", "title": "Multi-Fold Gabor, PCA and ICA Filter Convolution Descriptor for Face\n  Recognition", "comments": "14 pages, 10 figures, 10 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2761829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper devises a new means of filter diversification, dubbed multi-fold\nfilter convolution (M-FFC), for face recognition. On the assumption that M-FFC\nreceives single-scale Gabor filters of varying orientations as input, these\nfilters are self-cross convolved by M-fold to instantiate a filter offspring\nset. The M-FFC flexibility also permits cross convolution amongst Gabor filters\nand other filter banks of profoundly dissimilar traits, e.g., principal\ncomponent analysis (PCA) filters, and independent component analysis (ICA)\nfilters. The 2-FFC of Gabor, PCA and ICA filters thus yields three offspring\nsets: (1) Gabor filters solely, (2) Gabor-PCA filters, and (3) Gabor-ICA\nfilters, to render the learning-free and the learning-based 2-FFC descriptors.\nTo facilitate a sensible Gabor filter selection for M-FFC, the 40 multi-scale,\nmulti-orientation Gabor filters are condensed into 8 elementary filters. Aside\nfrom that, an average histogram pooling operator is employed to leverage the\n2-FFC histogram features, prior to the final whitening PCA compression. The\nempirical results substantiate that the 2-FFC descriptors prevail over, or on\npar with, other face descriptors on both identification and verification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 17:30:34 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 08:05:18 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 06:39:45 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Low", "Cheng Yaw", ""], ["Teoh", "Andrew Beng Jin", ""], ["Ng", "Cong Jie", ""]]}, {"id": "1604.07060", "submitter": "Hamid Tizhoosh", "authors": "Antonio Sze-To, Hamid R. Tizhoosh, Andrew K.C. Wong", "title": "Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders", "comments": "To appear in proceedings of The 2016 International Joint Conference\n  on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Content-Based Image Retrieval (CBIR) system which identifies similar\nmedical images based on a query image can assist clinicians for more accurate\ndiagnosis. The recent CBIR research trend favors the construction and use of\nbinary codes to represent images. Deep architectures could learn the non-linear\nrelationship among image pixels adaptively, allowing the automatic learning of\nhigh-level features from raw pixels. However, most of them require class\nlabels, which are expensive to obtain, particularly for medical images. The\nmethods which do not need class labels utilize a deep autoencoder for binary\nhashing, but the code construction involves a specific training algorithm and\nan ad-hoc regularization technique. In this study, we explored using a deep\nde-noising autoencoder (DDA), with a new unsupervised training scheme using\nonly backpropagation and dropout, to hash images into binary codes. We\nconducted experiments on more than 14,000 x-ray images. By using class labels\nonly for evaluating the retrieval results, we constructed a 16-bit DDA and a\n512-bit DDA independently. Comparing to other unsupervised methods, we\nsucceeded to obtain the lowest total error by using the 512-bit codes for\nretrieval via exhaustive search, and speed up 9.27 times with the use of the\n16-bit codes while keeping a comparable total error. We found that our new\ntraining scheme could reduce the total retrieval error significantly by 21.9%.\nTo further boost the image retrieval performance, we developed Radon\nAutoencoder Barcode (RABC) which are learned from the Radon projections of\nimages using a de-noising autoencoder. Experimental results demonstrated its\nsuperior performance in retrieval when it was combined with DDA binary codes.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 17:44:30 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Sze-To", "Antonio", ""], ["Tizhoosh", "Hamid R.", ""], ["Wong", "Andrew K. C.", ""]]}, {"id": "1604.07090", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, Xuelong Li", "title": "A Review of Co-saliency Detection Technique: Fundamentals, Applications,\n  and Challenges", "comments": "28 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Co-saliency detection is a newly emerging and rapidly growing research area\nin computer vision community. As a novel branch of visual saliency, co-saliency\ndetection refers to the discovery of common and salient foregrounds from two or\nmore relevant images, and can be widely used in many computer vision tasks. The\nexisting co-saliency detection algorithms mainly consist of three components:\nextracting effective features to represent the image regions, exploring the\ninformative cues or factors to characterize co-saliency, and designing\neffective computational frameworks to formulate co-saliency. Although numerous\nmethods have been developed, the literature is still lacking a deep review and\nevaluation of co-saliency detection techniques. In this paper, we aim at\nproviding a comprehensive review of the fundamentals, challenges, and\napplications of co-saliency detection. Specifically, we provide an overview of\nsome related computer vision works, review the history of co-saliency\ndetection, summarize and categorize the major algorithms in this research area,\ndiscuss some open issues in this area, present the potential applications of\nco-saliency detection, and finally point out some unsolved challenges and\npromising future works. We expect this review to be beneficial to both fresh\nand senior researchers in this field, and give insights to researchers in other\nrelated areas regarding the utility of co-saliency detection algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 22:36:38 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 14:09:33 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 20:55:33 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 13:36:24 GMT"}, {"version": "v5", "created": "Sun, 9 Jul 2017 15:44:10 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhang", "Dingwen", ""], ["Fu", "Huazhu", ""], ["Han", "Junwei", ""], ["Borji", "Ali", ""], ["Li", "Xuelong", ""]]}, {"id": "1604.07093", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Leonid Sigal", "title": "Semi-supervised Vocabulary-informed Learning", "comments": "10 pages, Accepted by CVPR 2016 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in object categorization, in recent years, a\nnumber of important challenges remain, mainly, ability to learn from limited\nlabeled data and ability to recognize object classes within large, potentially\nopen, set of labels. Zero-shot learning is one way of addressing these\nchallenges, but it has only been shown to work with limited sized class\nvocabularies and typically requires separation between supervised and\nunsupervised classes, allowing former to inform the latter but not vice versa.\nWe propose the notion of semi-supervised vocabulary-informed learning to\nalleviate the above mentioned challenges and address problems of supervised,\nzero-shot and open set recognition using a unified framework. Specifically, we\npropose a maximum margin framework for semantic manifold-based recognition that\nincorporates distance constraints from (both supervised and unsupervised)\nvocabulary atoms, ensuring that labeled samples are projected closest to their\ncorrect prototypes, in the embedding space, than to others. We show that\nresulting model shows improvements in supervised, zero-shot, and large open set\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 23:36:36 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Fu", "Yanwei", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.07102", "submitter": "Wei Wang", "authors": "Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao", "title": "Makeup like a superstar: Deep Localized Makeup Transfer Network", "comments": "7pages, 11 figures, to appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Deep Localized Makeup Transfer Network to\nautomatically recommend the most suitable makeup for a female and synthesis the\nmakeup on her face. Given a before-makeup face, her most suitable makeup is\ndetermined automatically. Then, both the beforemakeup and the reference faces\nare fed into the proposed Deep Transfer Network to generate the after-makeup\nface. Our end-to-end makeup transfer network have several nice properties\nincluding: (1) with complete functions: including foundation, lip gloss, and\neye shadow transfer; (2) cosmetic specific: different cosmetics are transferred\nin different manners; (3) localized: different cosmetics are applied on\ndifferent facial regions; (4) producing naturally looking results without\nobvious artifacts; (5) controllable makeup lightness: various results from\nlight makeup to heavy makeup can be generated. Qualitative and quantitative\nexperiments show that our network performs much better than the methods of [Guo\nand Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 01:01:51 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Liu", "Si", ""], ["Ou", "Xinyu", ""], ["Qian", "Ruihe", ""], ["Wang", "Wei", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1604.07279", "submitter": "Limin Wang", "authors": "Limin Wang, Yu Qiao, Xiaoou Tang, Luc Van Gool", "title": "Actionness Estimation Using Hybrid Fully Convolutional Networks", "comments": "accepted by CVPR16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actionness was introduced to quantify the likelihood of containing a generic\naction instance at a specific location. Accurate and efficient estimation of\nactionness is important in video analysis and may benefit other relevant tasks\nsuch as action recognition and action detection. This paper presents a new deep\narchitecture for actionness estimation, called hybrid fully convolutional\nnetwork (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN\n(M-FCN). These two FCNs leverage the strong capacity of deep models to estimate\nactionness maps from the perspectives of static appearance and dynamic motion,\nrespectively. In addition, the fully convolutional nature of H-FCN allows it to\nefficiently process videos with arbitrary sizes. Experiments are conducted on\nthe challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the\neffectiveness of H-FCN on actionness estimation, which demonstrate that our\nmethod achieves superior performance to previous ones. Moreover, we apply the\nestimated actionness maps on action proposal generation and action detection.\nOur actionness maps advance the current state-of-the-art performance of these\ntasks substantially.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 14:32:28 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Wang", "Limin", ""], ["Qiao", "Yu", ""], ["Tang", "Xiaoou", ""], ["Van Gool", "Luc", ""]]}, {"id": "1604.07316", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard\n  Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs\n  Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba", "title": "End to End Learning for Self-Driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a convolutional neural network (CNN) to map raw pixels from a\nsingle front-facing camera directly to steering commands. This end-to-end\napproach proved surprisingly powerful. With minimum training data from humans\nthe system learns to drive in traffic on local roads with or without lane\nmarkings and on highways. It also operates in areas with unclear visual\nguidance such as in parking lots and on unpaved roads.\n  The system automatically learns internal representations of the necessary\nprocessing steps such as detecting useful road features with only the human\nsteering angle as the training signal. We never explicitly trained it to\ndetect, for example, the outline of roads.\n  Compared to explicit decomposition of the problem, such as lane marking\ndetection, path planning, and control, our end-to-end system optimizes all\nprocessing steps simultaneously. We argue that this will eventually lead to\nbetter performance and smaller systems. Better performance will result because\nthe internal components self-optimize to maximize overall system performance,\ninstead of optimizing human-selected intermediate criteria, e.g., lane\ndetection. Such criteria understandably are selected for ease of human\ninterpretation which doesn't automatically guarantee maximum system\nperformance. Smaller networks are possible because the system learns to solve\nthe problem with the minimal number of processing steps.\n  We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX\nself-driving car computer also running Torch 7 for determining where to drive.\nThe system operates at 30 frames per second (FPS).\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 16:03:56 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Del Testa", "Davide", ""], ["Dworakowski", "Daniel", ""], ["Firner", "Bernhard", ""], ["Flepp", "Beat", ""], ["Goyal", "Prasoon", ""], ["Jackel", "Lawrence D.", ""], ["Monfort", "Mathew", ""], ["Muller", "Urs", ""], ["Zhang", "Jiakai", ""], ["Zhang", "Xin", ""], ["Zhao", "Jake", ""], ["Zieba", "Karol", ""]]}, {"id": "1604.07319", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Safaa M.A. Bedawi, Ali Ghodsi, Fakhri Karray", "title": "Semi-supervised Dictionary Learning Based on Hilbert-Schmidt\n  Independence Criterion", "comments": "Accepted at International conference on Image analysis and\n  Recognition (ICIAR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel semi-supervised dictionary learning and sparse\nrepresentation (SS-DLSR) is proposed. The proposed method benefits from the\nsupervisory information by learning the dictionary in a space where the\ndependency between the data and class labels is maximized. This maximization is\nperformed using Hilbert-Schmidt independence criterion (HSIC). On the other\nhand, the global distribution of the underlying manifolds were learned from the\nunlabeled data by minimizing the distances between the unlabeled data and the\ncorresponding nearest labeled data in the space of the dictionary learned. The\nproposed SS-DLSR algorithm has closed-form solutions for both the dictionary\nand sparse coefficients, and therefore does not have to learn the two\niteratively and alternately as is common in the literature of the DLSR. This\nmakes the solution for the proposed algorithm very fast. The experiments\nconfirm the improvement in classification performance on benchmark datasets by\nincluding the information from both labeled and unlabeled data, particularly\nwhen there are many unlabeled data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 16:25:38 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Bedawi", "Safaa M. A.", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""]]}, {"id": "1604.07335", "submitter": "Bahadir Ozdemir", "authors": "Bahadir Ozdemir and Larry S. Davis", "title": "Scalable Gaussian Processes for Supervised Hashing", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible procedure for large-scale image search by hash\nfunctions with kernels. Our method treats binary codes and pairwise semantic\nsimilarity as latent and observed variables, respectively, in a probabilistic\nmodel based on Gaussian processes for binary classification. We present an\nefficient inference algorithm with the sparse pseudo-input Gaussian process\n(SPGP) model and parallelization. Experiments on three large-scale image\ndataset demonstrate the effectiveness of the proposed hashing method, Gaussian\nProcess Hashing (GPH), for short binary codes and the datasets without\npredefined classes in comparison to the state-of-the-art supervised hashing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 17:30:20 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Ozdemir", "Bahadir", ""], ["Davis", "Larry S.", ""]]}, {"id": "1604.07342", "submitter": "Mahyar Najibi", "authors": "Bahadir Ozdemir and Mahyar Najibi and Larry S. Davis", "title": "Supervised Incremental Hashing", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an incremental strategy for learning hash functions with kernels\nfor large-scale image search. Our method is based on a two-stage classification\nframework that treats binary codes as intermediate variables between the\nfeature space and the semantic space. In the first stage of classification,\nbinary codes are considered as class labels by a set of binary SVMs; each\ncorresponds to one bit. In the second stage, binary codes become the input\nspace of a multi-class SVM. Hash functions are learned by an efficient\nalgorithm where the NP-hard problem of finding optimal binary codes is solved\nvia cyclic coordinate descent and SVMs are trained in a parallelized\nincremental manner. For modifications like adding images from a previously\nunseen class, we describe an incremental procedure for effective and efficient\nupdates to the previous hash functions. Experiments on three large-scale image\ndatasets demonstrate the effectiveness of the proposed hashing method,\nSupervised Incremental Hashing (SIH), over the state-of-the-art supervised\nhashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 17:50:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 17:24:25 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Ozdemir", "Bahadir", ""], ["Najibi", "Mahyar", ""], ["Davis", "Larry S.", ""]]}, {"id": "1604.07360", "submitter": "Emily Hand", "authors": "Emily M. Hand and Rama Chellappa", "title": "Attributes for Improved Attributes: A Multi-Task Network for Attribute\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes, or semantic features, have gained popularity in the past few\nyears in domains ranging from activity recognition in video to face\nverification. Improving the accuracy of attribute classifiers is an important\nfirst step in any application which uses these attributes. In most works to\ndate, attributes have been considered to be independent. However, we know this\nnot to be the case. Many attributes are very strongly related, such as heavy\nmakeup and wearing lipstick. We propose to take advantage of attribute\nrelationships in three ways: by using a multi-task deep convolutional neural\nnetwork (MCNN) sharing the lowest layers amongst all attributes, sharing the\nhigher layers for related attributes, and by building an auxiliary network on\ntop of the MCNN which utilizes the scores from all attributes to improve the\nfinal classification of each attribute. We demonstrate the effectiveness of our\nmethod by producing results on two challenging publicly available datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:49:55 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Hand", "Emily M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1604.07361", "submitter": "Martin Brooks", "authors": "Martin Brooks", "title": "Persistence Lenses: Segmentation, Simplification, Vectorization, Scale\n  Space and Fractal Analysis of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A persistence lens is a hierarchy of disjoint upper and lower level sets of a\ncontinuous luminance image's Reeb graph. The boundary components of a\npersistence lens's interior components are Jordan curves that serve as a\nhierarchical segmentation of the image, and may be rendered as vector graphics.\nA persistence lens determines a varilet basis for the luminance image, in which\nimage simplification is a realized by subspace projection. Image scale space,\nand image fractal analysis, result from applying a scale measure to each basis\nfunction.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:50:19 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 01:23:45 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 17:18:40 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Brooks", "Martin", ""]]}, {"id": "1604.07379", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell,\n  Alexei A. Efros", "title": "Context Encoders: Feature Learning by Inpainting", "comments": "New results on ImageNet Generation", "journal-ref": "CVPR 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised visual feature learning algorithm driven by\ncontext-based pixel prediction. By analogy with auto-encoders, we propose\nContext Encoders -- a convolutional neural network trained to generate the\ncontents of an arbitrary image region conditioned on its surroundings. In order\nto succeed at this task, context encoders need to both understand the content\nof the entire image, as well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experimented with both a\nstandard pixel-wise reconstruction loss, as well as a reconstruction plus an\nadversarial loss. The latter produces much sharper results because it can\nbetter handle multiple modes in the output. We found that a context encoder\nlearns a representation that captures not just appearance but also the\nsemantics of visual structures. We quantitatively demonstrate the effectiveness\nof our learned features for CNN pre-training on classification, detection, and\nsegmentation tasks. Furthermore, context encoders can be used for semantic\ninpainting tasks, either stand-alone or as initialization for non-parametric\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:42:46 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 20:56:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Krahenbuhl", "Philipp", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1604.07429", "submitter": "Yale Song", "authors": "Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny", "title": "Balancing Appearance and Context in Sketch Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a sketch interpretation system that detects and classifies clock\nnumerals created by subjects taking the Clock Drawing Test, a clinical tool\nwidely used to screen for cognitive impairments (e.g., dementia). We describe\nhow it balances appearance and context, and document its performance on some\n2,000 drawings (about 24K clock numerals) produced by a wide spectrum of\npatients. We calibrate the utility of different forms of context, describing\nexperiments with Conditional Random Fields trained and tested using a variety\nof features. We identify context that contributes to interpreting otherwise\nambiguous or incomprehensible strokes. We describe ST-slices, a novel\nrepresentation that enables \"unpeeling\" the layers of ink that result when\npeople overwrite, which often produces ink impossible to analyze if only the\nfinal drawing is examined. We characterize when ST-slices work, calibrate their\nimpact on performance, and consider their breadth of applicability.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 20:14:35 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Song", "Yale", ""], ["Davis", "Randall", ""], ["Ma", "Kaichen", ""], ["Penny", "Dana L.", ""]]}, {"id": "1604.07457", "submitter": "Panqu Wang", "authors": "Panqu Wang, Garrison Cottrell", "title": "Modeling the Contribution of Central Versus Peripheral Vision in Scene,\n  Object, and Face Recognition", "comments": "CogSci 2016 Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly believed that the central visual field is important for\nrecognizing objects and faces, and the peripheral region is useful for scene\nrecognition. However, the relative importance of central versus peripheral\ninformation for object, scene, and face recognition is unclear. In a behavioral\nstudy, Larson and Loschky (2009) investigated this question by measuring the\nscene recognition accuracy as a function of visual angle, and demonstrated that\nperipheral vision was indeed more useful in recognizing scenes than central\nvision. In this work, we modeled and replicated the result of Larson and\nLoschky (2009), using deep convolutional neural networks. Having fit the data\nfor scenes, we used the model to predict future data for large-scale scene\nrecognition as well as for objects and faces. Our results suggest that the\nrelative order of importance of using central visual field information is face\nrecognition>object recognition>scene recognition, and vice-versa for peripheral\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:01:50 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Wang", "Panqu", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1604.07468", "submitter": "Shoou-I Yu", "authors": "Shoou-I Yu, Yi Yang, Xuanchong Li, and Alexander G. Hauptmann", "title": "Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person tracking plays a critical role in the analysis of surveillance\nvideo. However, most existing work focus on shorter-term (e.g. minute-long or\nhour-long) video sequences. Therefore, we propose a multi-person tracking\nalgorithm for very long-term (e.g. month-long) multi-camera surveillance\nscenarios. Long-term tracking is challenging because 1) the apparel/appearance\nof the same person will vary greatly over multiple days and 2) a person will\nleave and re-enter the scene numerous times. To tackle these challenges, we\nleverage face recognition information, which is robust to apparel change, to\nautomatically reinitialize our tracker over multiple days of recordings.\nUnfortunately, recognized faces are unavailable oftentimes. Therefore, our\ntracker propagates identity information to frames without recognized faces by\nuncovering the appearance and spatial manifold formed by person detections. We\ntested our algorithm on a 23-day 15-camera data set (4,935 hours total), and we\nwere able to localize a person 53.2% of the time with 69.8% precision. We\nfurther performed video summarization experiments based on our tracking output.\nResults on 116.25 hours of video showed that we were able to generate a\nreasonable visual diary (i.e. a summary of what a person did) for different\npeople, thus potentially opening the door to automatic summarization of the\nvast amount of surveillance video generated every day.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:53:55 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 01:21:20 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Yu", "Shoou-I", ""], ["Yang", "Yi", ""], ["Li", "Xuanchong", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1604.07480", "submitter": "Arsalan Mousavian", "authors": "Arsalan Mousavian, Hamed Pirsiavash, Jana Kosecka", "title": "Joint Semantic Segmentation and Depth Estimation with Deep Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale deep CNNs have been used successfully for problems mapping each\npixel to a label, such as depth estimation and semantic segmentation. It has\nalso been shown that such architectures are reusable and can be used for\nmultiple tasks. These networks are typically trained independently for each\ntask by varying the output layer(s) and training objective. In this work we\npresent a new model for simultaneous depth estimation and semantic segmentation\nfrom a single RGB image. Our approach demonstrates the feasibility of training\nparts of the model for each task and then fine tuning the full, combined model\non both tasks simultaneously using a single loss function. Furthermore we\ncouple the deep CNN with fully connected CRF, which captures the contextual\nrelationships and interactions between the semantic and depth cues improving\nthe accuracy of the final results. The proposed model is trained and evaluated\non NYUDepth V2 dataset outperforming the state of the art methods on semantic\nsegmentation and achieving comparable results on the task of depth estimation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 23:58:00 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 15:10:54 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 21:57:28 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Pirsiavash", "Hamed", ""], ["Kosecka", "Jana", ""]]}, {"id": "1604.07499", "submitter": "Rizhen Qin", "authors": "Rizhen Qin, Wei Gao, Huarong Xu, Zhanyi Hu", "title": "Modern Physiognomy: An Investigation on Predicting Personality Traits\n  and Intelligence from the Human Face", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human behavior of evaluating other individuals with respect to their\npersonality traits and intelligence by evaluating their faces plays a crucial\nrole in human relations. These trait judgments might influence important social\noutcomes in our lives such as elections and court sentences. Previous studies\nhave reported that human can make valid inferences for at least four\npersonality traits. In addition, some studies have demonstrated that facial\ntrait evaluation can be learned using machine learning methods accurately. In\nthis work, we experimentally explore whether self-reported personality traits\nand intelligence can be predicted reliably from a facial image. More\nspecifically, the prediction problem is separately cast in two parts: a\nclassification task and a regression task. A facial structural feature is\nconstructed from the relations among facial salient points, and an appearance\nfeature is built by five texture descriptors. In addition, a minutia-based\nfingerprint feature from a fingerprint image is also explored. The\nclassification results show that the personality traits \"Rule-consciousness\"\nand \"Vigilance\" can be predicted reliably, and that the traits of females can\nbe predicted more accurately than those of male. However, the regression\nexperiments show that it is difficult to predict scores for individual\npersonality traits and intelligence. The residual plots and the correlation\nresults indicate no evident linear correlation between the measured scores and\nthe predicted scores. Both the classification and the regression results reveal\nthat \"Rule-consciousness\" and \"Tension\" can be reliably predicted from the\nfacial features, while \"Social boldness\" gets the worst prediction results. The\nexperiments results show that it is difficult to predict intelligence from\neither the facial features or the fingerprint feature, a finding that is in\nagreement with previous studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 02:58:08 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Qin", "Rizhen", ""], ["Gao", "Wei", ""], ["Xu", "Huarong", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1604.07507", "submitter": "Kai Chen", "authors": "Kai Chen and Wenbing Tao", "title": "Once for All: a Two-flow Convolutional Neural Network for Visual\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main challenges of visual object tracking comes from the arbitrary\nappearance of objects. Most existing algorithms try to resolve this problem as\nan object-specific task, i.e., the model is trained to regenerate or classify a\nspecific object. As a result, the model need to be initialized and retrained\nfor different objects. In this paper, we propose a more generic approach\nutilizing a novel two-flow convolutional neural network (named YCNN). The YCNN\ntakes two inputs (one is object image patch, the other is search image patch),\nthen outputs a response map which predicts how likely the object appears in a\nspecific location. Unlike those object-specific approach, the YCNN is trained\nto measure the similarity between two image patches. Thus it will not be\nconfined to any specific object. Furthermore the network can be end-to-end\ntrained to extract both shallow and deep convolutional features which are\ndedicated for visual tracking. And once properly trained, the YCNN can be\napplied to track all kinds of objects without further training and updating.\nBenefiting from the once-for-all model, our algorithm is able to run at a very\nhigh speed of 45 frames-per-second. The experiments on 51 sequences also show\nthat our algorithm achieves an outstanding performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 03:34:39 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Chen", "Kai", ""], ["Tao", "Wenbing", ""]]}, {"id": "1604.07513", "submitter": "Hirokatsu Kataoka", "authors": "Teppei Suzuki, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Yutaka\n  Satoh, Hirokatsu Kataoka", "title": "Semantic Change Detection with Hypermaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is the study of detecting changes between two different\nimages of a scene taken at different times. By the detected change areas,\nhowever, a human cannot understand how different the two images. Therefore, a\nsemantic understanding is required in the change detection research such as\ndisaster investigation. The paper proposes the concept of semantic change\ndetection, which involves intuitively inserting semantic meaning into detected\nchange areas. We mainly focus on the novel semantic segmentation in addition to\na conventional change detection approach. In order to solve this problem and\nobtain a high-level of performance, we propose an improvement to the\nhypercolumns representation, hereafter known as hypermaps, which effectively\nuses convolutional maps obtained from convolutional neural networks (CNNs). We\nalso employ multi-scale feature representation captured by different image\npatches. We applied our method to the TSUNAMI Panoramic Change Detection\ndataset, and re-annotated the changed areas of the dataset via semantic\nclasses. The results show that our multi-scale hypermaps provided outstanding\nperformance on the re-annotated TSUNAMI dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 04:31:31 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 01:46:37 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Suzuki", "Teppei", ""], ["Shirakabe", "Soma", ""], ["Miyashita", "Yudai", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1604.07528", "submitter": "Tong Xiao", "authors": "Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang", "title": "Learning Deep Feature Representations with Domain Guided Dropout for\n  Person Re-identification", "comments": "To appear in CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning generic and robust feature representations with data from multiple\ndomains for the same problem is of great value, especially for the problems\nthat have multiple datasets but none of them are large enough to provide\nabundant data variations. In this work, we present a pipeline for learning deep\nfeature representations from multiple domains with Convolutional Neural\nNetworks (CNNs). When training a CNN with data from all the domains, some\nneurons learn representations shared across several domains, while some others\nare effective only for a specific one. Based on this important observation, we\npropose a Domain Guided Dropout algorithm to improve the feature learning\nprocedure. Experiments show the effectiveness of our pipeline and the proposed\nalgorithm. Our methods on the person re-identification problem outperform\nstate-of-the-art methods on multiple datasets by large margins.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 05:39:53 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Xiao", "Tong", ""], ["Li", "Hongsheng", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1604.07547", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell", "title": "Towards Miss Universe Automatic Prediction: The Evening Gown Competition", "comments": null, "journal-ref": "International Conference on Pattern Recognition, 2016", "doi": "10.1109/ICPR.2016.7899781", "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we predict the winner of Miss Universe after watching how they stride\ndown the catwalk during the evening gown competition? Fashion gurus say they\ncan! In our work, we study this question from the perspective of computer\nvision. In particular, we want to understand whether existing computer vision\napproaches can be used to automatically extract the qualities exhibited by the\nMiss Universe winners during their catwalk. This study can pave the way towards\nnew vision-based applications for the fashion industry. To this end, we propose\na novel video dataset, called the Miss Universe dataset, comprising 10 years of\nthe evening gown competition selected between 1996-2010. We further propose two\nranking-related problems: (1) Miss Universe Listwise Ranking and (2) Miss\nUniverse Pairwise Ranking. In addition, we also develop an approach that\nsimultaneously addresses the two proposed problems. To describe the videos we\nemploy the recently proposed Stacked Fisher Vectors in conjunction with robust\nlocal spatio-temporal features. From our evaluation we found that although the\naddressed problems are extremely challenging, the proposed system is able to\nrank the winner in the top 3 best predicted scores for 5 out of 10 Miss\nUniverse competitions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 07:02:16 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 03:22:01 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Carvajal", "Johanna", ""], ["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian", ""]]}, {"id": "1604.07554", "submitter": "Muhammad Yousefnezhad", "authors": "Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian", "title": "A New Approach in Persian Handwritten Letters Recognition Using Error\n  Correcting Output Coding", "comments": "Journal of Advances in Computer Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification Ensemble, which uses the weighed polling of outputs, is the\nart of combining a set of basic classifiers for generating high-performance,\nrobust and more stable results. This study aims to improve the results of\nidentifying the Persian handwritten letters using Error Correcting Output\nCoding (ECOC) ensemble method. Furthermore, the feature selection is used to\nreduce the costs of errors in our proposed method. ECOC is a method for\ndecomposing a multi-way classification problem into many binary classification\ntasks; and then combining the results of the subtasks into a hypothesized\nsolution to the original problem. Firstly, the image features are extracted by\nPrincipal Components Analysis (PCA). After that, ECOC is used for\nidentification the Persian handwritten letters which it uses Support Vector\nMachine (SVM) as the base classifier. The empirical results of applying this\nensemble method using 10 real-world data sets of Persian handwritten letters\nindicate that this method has better results in identifying the Persian\nhandwritten letters than other ensemble methods and also single\nclassifications. Moreover, by testing a number of different features, this\npaper found that we can reduce the additional cost in feature selection stage\nby using this method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 07:43:59 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kazemi", "Maziar", ""], ["Yousefnezhad", "Muhammad", ""], ["Nourian", "Saber", ""]]}, {"id": "1604.07602", "submitter": "Pascal Mettes", "authors": "Pascal Mettes and Jan C. van Gemert and Cees G. M. Snoek", "title": "Spot On: Action Localization from Pointly-Supervised Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": "ECCV/2016/10", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We strive for spatio-temporal localization of actions in videos. The\nstate-of-the-art relies on action proposals at test time and selects the best\none with a classifier trained on carefully annotated box annotations.\nAnnotating action boxes in video is cumbersome, tedious, and error prone.\nRather than annotating boxes, we propose to annotate actions in video with\npoints on a sparse subset of frames only. We introduce an overlap measure\nbetween action proposals and points and incorporate them all into the objective\nof a non-convex Multiple Instance Learning optimization. Experimental\nevaluation on the UCF Sports and UCF 101 datasets shows that (i)\nspatio-temporal proposals can be used to train classifiers while retaining the\nlocalization performance, (ii) point annotations yield results comparable to\nbox annotations while being significantly faster to annotate, (iii) with a\nminimum amount of supervision our approach is competitive to the\nstate-of-the-art. Finally, we introduce spatio-temporal action annotations on\nthe train and test videos of Hollywood2, resulting in Hollywood2Tubes,\navailable at http://tinyurl.com/hollywood2tubes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 10:09:18 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 20:45:18 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Mettes", "Pascal", ""], ["van Gemert", "Jan C.", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1604.07666", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Bernard Ghanem", "title": "$\\ell_p$-Box ADMM: A Versatile Framework for Integer Programming", "comments": "both authors share first-authorship. Integer programming, Lp-box\n  intersection, ADMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the integer programming (IP) problem, which plays a\nfundamental role in many computer vision and machine learning applications. The\nliterature abounds with many seminal works that address this problem, some\nfocusing on continuous approaches (e.g. linear program relaxation) while others\non discrete ones (e.g., min-cut). However, a limited number of them are\ndesigned to handle the general IP form and even these methods cannot adequately\nsatisfy the simultaneous requirements of accuracy, feasibility, and\nscalability. To this end, we propose a novel and versatile framework called\n$\\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint is\nequivalently replaced by the intersection of a box and a $(n-1)$-dimensional\nsphere (defined through the $\\ell_p$ norm). (2) We infuse this equivalence into\nthe ADMM (Alternating Direction Method of Multipliers) framework to handle\nthese continuous constraints separately and to harness its attractive\nproperties. More importantly, the ADMM update steps can lead to manageable\nsub-problems in the continuous domain. To demonstrate its efficacy, we consider\nan instance of the framework, namely $\\ell_2$-box ADMM applied to binary\nquadratic programming (BQP). Here, the ADMM steps are simple, computationally\nefficient, and theoretically guaranteed to converge to a KKT point. We\ndemonstrate the applicability of $\\ell_2$-box ADMM on three important\napplications: MRF energy minimization, graph matching, and clustering. Results\nclearly show that it significantly outperforms existing generic IP solvers both\nin runtime and objective. It also achieves very competitive performance vs.\nstate-of-the-art methods specific to these applications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 13:15:17 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 08:24:32 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2016 12:53:02 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wu", "Baoyuan", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1604.07669", "submitter": "Limin Wang", "authors": "Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang", "title": "Real-time Action Recognition with Enhanced Motion Vector CNNs", "comments": "accepted by CVPR16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep two-stream architecture exhibited excellent performance on video\nbased action recognition. The most computationally expensive step in this\napproach comes from the calculation of optical flow which prevents it to be\nreal-time. This paper accelerates this architecture by replacing optical flow\nwith motion vector which can be obtained directly from compressed videos\nwithout extra calculation. However, motion vector lacks fine structures, and\ncontains noisy and inaccurate motion patterns, leading to the evident\ndegradation of recognition performance. Our key insight for relieving this\nproblem is that optical flow and motion vector are inherent correlated.\nTransferring the knowledge learned with optical flow CNN to motion vector CNN\ncan significantly boost the performance of the latter. Specifically, we\nintroduce three strategies for this, initialization transfer, supervision\ntransfer and their combination. Experimental results show that our method\nachieves comparable recognition performance to the state-of-the-art, while our\nmethod can process 390.7 frames per second, which is 27 times faster than the\noriginal two-stream method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 13:21:37 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Zhang", "Bowen", ""], ["Wang", "Limin", ""], ["Wang", "Zhe", ""], ["Qiao", "Yu", ""], ["Wang", "Hanli", ""]]}, {"id": "1604.07681", "submitter": "Youngjung Kim", "authors": "Youngjung Kim, Dongbo Min, Bumsub Ham, and Kwanghoon Sohn", "title": "Efficient Splitting-based Method for Global Image Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-preserving smoothing (EPS) can be formulated as minimizing an objective\nfunction that consists of data and prior terms. This global EPS approach shows\nbetter smoothing performance than a local one that typically has a form of\nweighted averaging, at the price of high computational cost. In this paper, we\nintroduce a highly efficient splitting-based method for global EPS that\nminimizes the objective function of ${l_2}$ data and prior terms (possibly\nnon-smooth and non-convex) in linear time. Different from previous\nsplitting-based methods that require solving a large linear system, our\napproach solves an equivalent constrained optimization problem, resulting in a\nsequence of 1D sub-problems. This enables linear time solvers for\nweighted-least squares and -total variation problems. Our solver converges\nquickly, and its runtime is even comparable to state-of-the-art local EPS\napproaches. We also propose a family of fast iteratively re-weighted algorithms\nusing a non-convex prior term. Experimental results demonstrate the\neffectiveness and flexibility of our approach in a range of computer vision and\nimage processing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 14:05:41 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kim", "Youngjung", ""], ["Min", "Dongbo", ""], ["Ham", "Bumsub", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1604.07741", "submitter": "Tavi Halperin", "authors": "Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg", "title": "EgoSampling: Wide View Hyperlapse from Egocentric Videos", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2651051", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility of sharing one's point of view makes use of wearable cameras\ncompelling. These videos are often long, boring and coupled with extreme shake,\nas the camera is worn on a moving person. Fast forwarding (i.e. frame sampling)\nis a natural choice for quick video browsing. However, this accentuates the\nshake caused by natural head motion in an egocentric video, making the fast\nforwarded video useless. We propose EgoSampling, an adaptive frame sampling\nthat gives stable, fast forwarded, hyperlapse videos. Adaptive frame sampling\nis formulated as an energy minimization problem, whose optimal solution can be\nfound in polynomial time. We further turn the camera shake from a drawback into\na feature, enabling the increase in field-of-view of the output video. This is\nobtained when each output frame is mosaiced from several input frames. The\nproposed technique also enables the generation of a single hyperlapse video\nfrom multiple egocentric videos, allowing even faster video consumption.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 16:25:24 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 15:12:19 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Halperin", "Tavi", ""], ["Poleg", "Yair", ""], ["Arora", "Chetan", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1604.07751", "submitter": "Rafal Kotynski", "authors": "David Pastor-Calle, Anna Pastuszczak, Michal Mikolajczyk and Rafal\n  Kotynski", "title": "Compressive phase-only filtering at extreme compression rates", "comments": null, "journal-ref": "Opt. Commun. vol. 383, pp. 446-452, (2017)", "doi": "10.1016/j.optcom.2016.09.024", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient method for the reconstruction of the correlation\nbetween a compressively measured image and a phase-only filter. The proposed\nmethod is based on two properties of phase-only filtering: such filtering is a\nunitary circulant transform, and the correlation plane it produces is usually\nsparse. Thanks to these properties, phase-only filters are perfectly compatible\nwith the framework of compressive sensing. Moreover, the lasso-based recovery\nalgorithm is very fast when phase-only filtering is used as the compression\nmatrix. The proposed method can be seen as a generalisation of the\ncorrelation-based pattern recognition technique, which is hereby applied\ndirectly to non-adaptively acquired compressed data. At the time of\nmeasurement, any prior knowledge of the target object for which the data will\nbe scanned is not required. We show that images measured at extremely high\ncompression rates may still contain sufficient information for target\nclassification and localization, even if the compression rate is high enough,\nthat visual recognition of the target in the reconstructed image is no longer\npossible. The method has been applied by us to highly undersampled measurements\nobtained from a single-pixel camera, with sampling based on randomly chosen\nWalsh-Hadamard patterns.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 16:49:58 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 13:45:38 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 09:46:50 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 12:33:18 GMT"}, {"version": "v5", "created": "Thu, 29 Sep 2016 06:43:39 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Pastor-Calle", "David", ""], ["Pastuszczak", "Anna", ""], ["Mikolajczyk", "Michal", ""], ["Kotynski", "Rafal", ""]]}, {"id": "1604.07788", "submitter": "Dong Zhang", "authors": "Dong Zhang and Mubarak Shah", "title": "A Framework for Human Pose Estimation in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to estimate a sequence of human poses in\nunconstrained videos. We aim to demonstrate that by using temporal information,\nthe human pose estimation results can be improved over image based pose\nestimation methods. In contrast to the commonly employed graph optimization\nformulation, which is NP-hard and needs approximate solutions, we formulate\nthis problem into a unified two stage tree-based optimization problem for which\nan efficient and exact solution exists. Although the proposed method finds an\nexact solution, it does not sacrifice the ability to model the spatial and\ntemporal constraints between body parts in the frames; in fact it models the\n{\\em symmetric} parts better than the existing methods. The proposed method is\nbased on two main ideas: `Abstraction' and `Association' to enforce the intra-\nand inter-frame body part constraints without inducing extra computational\ncomplexity to the polynomial time solution. Using the idea of `Abstraction', a\nnew concept of `abstract body part' is introduced to conceptually combine the\nsymmetric body parts and model them in the tree based body part structure.\nUsing the idea of `Association', the optimal tracklets are generated for each\nabstract body part, in order to enforce the spatiotemporal constraints between\nbody parts in adjacent frames. A sequence of the best poses is inferred from\nthe abstract body part tracklets through the tree-based optimization. Finally,\nthe poses are refined by limb alignment and refinement schemes. We evaluated\nthe proposed method on three publicly available video based human pose\nestimation datasets, and obtained dramatically improved performance compared to\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 18:45:25 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Zhang", "Dong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1604.07807", "submitter": "Shangxuan Wu", "authors": "Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, and\n  Wei-Shi Zheng", "title": "An Enhanced Deep Feature Representation for Person Re-identification", "comments": "Citation for this paper: Shangxuan Wu, Ying-Cong Chen, Xiang Li,\n  An-Cong Wu, Jin-Jie You, and Wei-Shi Zheng. An Enhanced Deep Feature\n  Representation for Person Re-identification. In IEEE WACV, 2016", "journal-ref": null, "doi": "10.1109/WACV.2016.7477681", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representation and metric learning are two critical components in\nperson re-identification models. In this paper, we focus on the feature\nrepresentation and claim that hand-crafted histogram features can be\ncomplementary to Convolutional Neural Network (CNN) features. We propose a\nnovel feature extraction model called Feature Fusion Net (FFN) for pedestrian\nimage representation. In FFN, back propagation makes CNN features constrained\nby the handcrafted features. Utilizing color histogram features (RGB, HSV,\nYCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientation\nGabor features), we get a new deep feature representation that is more\ndiscriminative and compact. Experiments on three challenging datasets (VIPeR,\nCUHK01, PRID450s) validates the effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 19:27:50 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 08:17:35 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wu", "Shangxuan", ""], ["Chen", "Ying-Cong", ""], ["Li", "Xiang", ""], ["Wu", "An-Cong", ""], ["You", "Jin-Jie", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1604.07866", "submitter": "Cristian Canton Ferrer", "authors": "Laura Leal-Taix\\'e, Cristian Canton Ferrer, Konrad Schindler", "title": "Learning by tracking: Siamese CNN for robust target association", "comments": null, "journal-ref": "Computer Vision and Pattern Recognition Conference Workshops\n  (CVPRW). DeepVision: Deep Learning for Computer Vision. 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to the task of data association within\nthe context of pedestrian tracking, by introducing a two-stage learning scheme\nto match pairs of detections. First, a Siamese convolutional neural network\n(CNN) is trained to learn descriptors encoding local spatio-temporal structures\nbetween the two input image patches, aggregating pixel values and optical flow\ninformation. Second, a set of contextual features derived from the position and\nsize of the compared input patches are combined with the CNN output by means of\na gradient boosting classifier to generate the final matching probability. This\nlearning approach is validated by using a linear programming based multi-person\ntracker showing that even a simple and efficient tracker may outperform much\nmore complex models when fed with our learned matching probabilities. Results\non publicly available sequences show that our method meets state-of-the-art\nstandards in multiple people tracking.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 21:42:51 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 16:20:16 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 15:01:36 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Leal-Taix\u00e9", "Laura", ""], ["Ferrer", "Cristian Canton", ""], ["Schindler", "Konrad", ""]]}, {"id": "1604.07872", "submitter": "Panqu Wang", "authors": "Panqu Wang, Isabel Gauthier, Garrison Cottrell", "title": "Are Face and Object Recognition Independent? A Neurocomputational\n  Modeling Exploration", "comments": null, "journal-ref": "Journal of Cognitive Neuroscience, 28(4):558-574. 2016", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are face and object recognition abilities independent? Although it is\ncommonly believed that they are, Gauthier et al.(2014) recently showed that\nthese abilities become more correlated as experience with nonface categories\nincreases. They argued that there is a single underlying visual ability, v,\nthat is expressed in performance with both face and nonface categories as\nexperience grows. Using the Cambridge Face Memory Test and the Vanderbilt\nExpertise Test, they showed that the shared variance between Cambridge Face\nMemory Test and Vanderbilt Expertise Test performance increases monotonically\nas experience increases. Here, we address why a shared resource across\ndifferent visual domains does not lead to competition and to an inverse\ncorrelation in abilities? We explain this conundrum using our\nneurocomputational model of face and object processing (The Model, TM). Our\nresults show that, as in the behavioral data, the correlation between\nsubordinate level face and object recognition accuracy increases as experience\ngrows. We suggest that different domains do not compete for resources because\nthe relevant features are shared between faces and objects. The essential power\nof experience is to generate a \"spreading transform\" for faces that generalizes\nto objects that must be individuated. Interestingly, when the task of the\nnetwork is basic level categorization, no increase in the correlation between\ndomains is observed. Hence, our model predicts that it is the type of\nexperience that matters and that the source of the correlation is in the\nfusiform face area, rather than in cortical areas that subserve basic level\ncategorization. This result is consistent with our previous modeling\nelucidating why the FFA is recruited for novel domains of expertise (Tong et\nal., 2008).\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 22:04:22 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Wang", "Panqu", ""], ["Gauthier", "Isabel", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1604.07904", "submitter": "Ruck Thawonmas", "authors": "Tung Nguyen, Kazuki Mori, and Ruck Thawonmas", "title": "Image Colorization Using a Deep Convolutional Neural Network", "comments": null, "journal-ref": "Proc. of ASIAGRAPH 2016, Toyama, Japan, pp. 49-50, Mar. 5-6, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach that uses deep learning techniques\nfor colorizing grayscale images. By utilizing a pre-trained convolutional\nneural network, which is originally designed for image classification, we are\nable to separate content and style of different images and recombine them into\na single image. We then propose a method that can add colors to a grayscale\nimage by combining its content with style of a color image having semantic\nsimilarity with the grayscale one. As an application, to our knowledge the\nfirst of its kind, we use the proposed method to colorize images of ukiyo-e a\ngenre of Japanese painting?and obtain interesting results, showing the\npotential of this method in the growing field of computer assisted art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 02:16:43 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Nguyen", "Tung", ""], ["Mori", "Kazuki", ""], ["Thawonmas", "Ruck", ""]]}, {"id": "1604.07944", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N. Do, Kwanghoon Sohn", "title": "DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral\n  Correspondence Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing dense correspondences between multiple images is a fundamental\ntask in many applications. However, finding a reliable correspondence in\nmulti-modal or multi-spectral images still remains unsolved due to their\nchallenging photometric and geometric variations. In this paper, we propose a\nnovel dense descriptor, called dense adaptive self-correlation (DASC), to\nestimate multi-modal and multi-spectral dense correspondences. Based on an\nobservation that self-similarity existing within images is robust to imaging\nmodality variations, we define the descriptor with a series of an adaptive\nself-correlation similarity measure between patches sampled by a randomized\nreceptive field pooling, in which a sampling pattern is obtained using a\ndiscriminative learning. The computational redundancy of dense descriptors is\ndramatically reduced by applying fast edge-aware filtering. Furthermore, in\norder to address geometric variations including scale and rotation, we propose\na geometry-invariant DASC (GI-DASC) descriptor that effectively leverages the\nDASC through a superpixel-based representation. For a quantitative evaluation\nof the GI-DASC, we build a novel multi-modal benchmark as varying photometric\nand geometric conditions. Experimental results demonstrate the outstanding\nperformance of the DASC and GI-DASC in many cases of multi-modal and\nmulti-spectral dense correspondences.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 06:35:13 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Ham", "Bumsub", ""], ["Do", "Minh N.", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1604.07948", "submitter": "Jiahao Pang", "authors": "Jiahao Pang, Gene Cheung", "title": "Graph Laplacian Regularization for Image Denoising: Analysis in the\n  Continuous Domain", "comments": "More discussions and results are provided", "journal-ref": null, "doi": "10.1109/TIP.2017.2651400", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse imaging problems are inherently under-determined, and hence it is\nimportant to employ appropriate image priors for regularization. One recent\npopular prior---the graph Laplacian regularizer---assumes that the target pixel\npatch is smooth with respect to an appropriately chosen graph. However, the\nmechanisms and implications of imposing the graph Laplacian regularizer on the\noriginal inverse problem are not well understood. To address this problem, in\nthis paper we interpret neighborhood graphs of pixel patches as discrete\ncounterparts of Riemannian manifolds and perform analysis in the continuous\ndomain, providing insights into several fundamental aspects of graph Laplacian\nregularization for image denoising. Specifically, we first show the convergence\nof the graph Laplacian regularizer to a continuous-domain functional,\nintegrating a norm measured in a locally adaptive metric space. Focusing on\nimage denoising, we derive an optimal metric space assuming non-local\nself-similarity of pixel patches, leading to an optimal graph Laplacian\nregularizer for denoising in the discrete domain. We then interpret graph\nLaplacian regularization as an anisotropic diffusion scheme to explain its\nbehavior during iterations, e.g., its tendency to promote piecewise smooth\nsignals under certain settings. To verify our analysis, an iterative image\ndenoising algorithm is developed. Experimental results show that our algorithm\nperforms competitively with state-of-the-art denoising methods such as BM3D for\nnatural images, and outperforms them significantly for piecewise smooth images.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 06:53:12 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 12:11:26 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Pang", "Jiahao", ""], ["Cheung", "Gene", ""]]}, {"id": "1604.07952", "submitter": "Rene Grzeszick", "authors": "Rene Grzeszick, Gernot A. Fink", "title": "Zero-shot object prediction using semantic scene knowledge", "comments": "This version extends the related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the semantic relations between scenes and objects for\nvisual object recognition. Semantic knowledge can be a powerful source of\ninformation especially in scenarios with few or no annotated training samples.\nThese scenarios are referred to as zero-shot or few-shot recognition and often\nbuild on visual attributes. Here, instead of relying on various visual\nattributes, a more direct way is pursued: after recognizing the scene that is\ndepicted in an image, semantic relations between scenes and objects are used\nfor predicting the presence of objects in an unsupervised manner. Most\nimportantly, relations between scenes and objects can easily be obtained from\nexternal sources such as large scale text corpora from the web and, therefore,\ndo not require tremendous manual labeling efforts. It will be shown that in\ncluttered scenes, where visual recognition is difficult, scene knowledge is an\nimportant cue for predicting objects.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 07:16:56 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 11:45:10 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 12:30:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Grzeszick", "Rene", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1604.07953", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos and Petia Radeva", "title": "Simultaneous Food Localization and Recognition", "comments": "6 pages, 6 figures, 2 tables. International Conference on Pattern\n  Recognition (ICPR) 2016 (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of automatic nutrition diaries, which would allow to keep\ntrack objectively of everything we eat, could enable a whole new world of\npossibilities for people concerned about their nutrition patterns. With this\npurpose, in this paper we propose the first method for simultaneous food\nlocalization and recognition. Our method is based on two main steps, which\nconsist in, first, produce a food activation map on the input image (i.e. heat\nmap of probabilities) for generating bounding boxes proposals and, second,\nrecognize each of the food types or food-related objects present in each\nbounding box. We demonstrate that our proposal, compared to the most similar\nproblem nowadays - object localization, is able to obtain high precision and\nreasonable recall levels with only a few bounding boxes. Furthermore, we show\nthat it is applicable to both conventional and egocentric images.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 07:20:55 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 12:49:11 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1604.08010", "submitter": "Souad Chaabouni", "authors": "Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar", "title": "Deep Learning for Saliency Prediction in Natural Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 10:34:21 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Chaabouni", "Souad", ""], ["Benois-Pineau", "Jenny", ""], ["Hadar", "Ofer", ""], ["Amar", "Chokri Ben", ""]]}, {"id": "1604.08088", "submitter": "Xirong Li", "authors": "Xirong Li and Yujia Huo and Jieping Xu and Qin Jin", "title": "Detecting Violence in Video using Subclasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of violence detection in videos.\nDifferent from existing works focusing on combining multi-modal features, we go\none step further by adding and exploiting subclasses visually related to\nviolence. We enrich the MediaEval 2015 violence dataset by \\emph{manually}\nlabeling violence videos with respect to the subclasses. Such fine-grained\nannotations not only help understand what have impeded previous efforts on\nlearning to fuse the multi-modal features, but also enhance the generalization\nability of the learned fusion to novel test data. The new subclass based\nsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,\noutperforms several state-of-the-art alternatives. Notice that our solution\ndoes not require fine-grained annotations on the test set, so it can be\ndirectly applied on novel and fully unlabeled videos. Interestingly, our study\nshows that motion related features, though being essential part in previous\nsystems, are dispensable.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:32:16 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Li", "Xirong", ""], ["Huo", "Yujia", ""], ["Xu", "Jieping", ""], ["Jin", "Qin", ""]]}, {"id": "1604.08145", "submitter": "Farnoud Kazemzadeh", "authors": "Farnoud Kazemzadeh and Alexander Wong", "title": "Laser light-field fusion for wide-field lensfree on-chip phase contrast\n  nanoscopy", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-field lensfree on-chip microscopy, which leverages holography principles\nto capture interferometric light-field encodings without lenses, is an emerging\nimaging modality with widespread interest given the large field-of-view\ncompared to lens-based techniques. In this study, we introduce the idea of\nlaser light-field fusion for lensfree on-chip phase contrast nanoscopy, where\ninterferometric laser light-field encodings acquired using an on-chip setup\nwith laser pulsations at different wavelengths are fused to produce marker-free\nphase contrast images of superior quality with resolving power more than five\ntimes below the pixel pitch of the sensor array and more than 40% beyond the\ndiffraction limit. As a proof of concept, we demonstrate, for the first time, a\nwide-field lensfree on-chip instrument successfully detecting 300 nm particles,\nresulting in a numerical aperture of 1.1, across a large field-of-view of\n$\\sim$ 30 mm$^2$ without any specialized or intricate sample preparation, or\nthe use of synthetic aperture- or shift-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 17:06:17 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 19:56:34 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Kazemzadeh", "Farnoud", ""], ["Wong", "Alexander", ""]]}, {"id": "1604.08164", "submitter": "Edgar Simo-Serra", "authors": "Edgar Simo-Serra", "title": "Understanding Human-Centric Images: From Geometry to Fashion", "comments": "PhD Thesis, May 2015. BarcelonaTech. 169 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding humans from photographs has always been a fundamental goal of\ncomputer vision. In this thesis we have developed a hierarchy of tools that\ncover a wide range of topics with the objective of understanding humans from\nmonocular RGB image: from low level feature point descriptors to high level\nfashion-aware conditional random fields models. In order to build these high\nlevel models it is paramount to have a battery of robust and reliable low and\nmid level cues. Along these lines, we have proposed two low-level keypoint\ndescriptors: one based on the theory of the heat diffusion on images, and the\nother that uses a convolutional neural network to learn discriminative image\npatch representations. We also introduce distinct low-level generative models\nfor representing human pose: in particular we present a discrete model based on\na directed acyclic graph and a continuous model that consists of poses\nclustered on a Riemannian manifold. As mid level cues we propose two 3D human\npose estimation algorithms: one that estimates the 3D pose given a noisy 2D\nestimation, and an approach that simultaneously estimates both the 2D and 3D\npose. Finally, we formulate higher level models built upon low and mid level\ncues for understanding humans from single images. Concretely, we focus on two\ndifferent tasks in the context of fashion: semantic segmentation of clothing,\nand predicting the fashionability from images with metadata to ultimately\nprovide fashion advice to the user. For all presented approaches we present\nextensive results and comparisons against the state-of-the-art and show\nsignificant improvements on the entire variety of tasks we tackle.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 03:15:14 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Simo-Serra", "Edgar", ""]]}, {"id": "1604.08182", "submitter": "Wei Zhu", "authors": "Wei Zhu, Victoria Chayes, Alexandre Tiard, Stephanie Sanchez, Devin\n  Dahlberg, Andrea L. Bertozzi, Stanley Osher, Dominique Zosso, and Da Kuang", "title": "Unsupervised Classification in Hyperspectral Imagery with Nonlocal Total\n  Variation and Primal-Dual Hybrid Gradient Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2654486", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a graph-based nonlocal total variation method (NLTV) is\nproposed for unsupervised classification of hyperspectral images (HSI). The\nvariational problem is solved by the primal-dual hybrid gradient (PDHG)\nalgorithm. By squaring the labeling function and using a stable simplex\nclustering routine, an unsupervised clustering method with random\ninitialization can be implemented. The effectiveness of this proposed algorithm\nis illustrated on both synthetic and real-world HSI, and numerical results show\nthat the proposed algorithm outperforms other standard unsupervised clustering\nmethods such as spherical K-means, nonnegative matrix factorization (NMF), and\nthe graph-based Merriman-Bence-Osher (MBO) scheme.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 19:11:10 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 21:45:19 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Zhu", "Wei", ""], ["Chayes", "Victoria", ""], ["Tiard", "Alexandre", ""], ["Sanchez", "Stephanie", ""], ["Dahlberg", "Devin", ""], ["Bertozzi", "Andrea L.", ""], ["Osher", "Stanley", ""], ["Zosso", "Dominique", ""], ["Kuang", "Da", ""]]}, {"id": "1604.08202", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Amodal Instance Segmentation", "comments": "23 pages, 14 figures; European Conference on Computer Vision (ECCV),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of amodal instance segmentation, the objective of\nwhich is to predict the region encompassing both visible and occluded parts of\neach object. Thus far, the lack of publicly available amodal segmentation\nannotations has stymied the development of amodal segmentation methods. In this\npaper, we sidestep this issue by relying solely on standard modal instance\nsegmentation annotations to train our model. The result is a new method for\namodal instance segmentation, which represents the first such method to the\nbest of our knowledge. We demonstrate the proposed method's effectiveness both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 19:56:11 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 19:13:04 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1604.08220", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan, Baoxin Li", "title": "Diving deeper into mentee networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision is all about the possession of powerful image\nrepresentations. Deeper and deeper convolutional neural networks have been\nbuilt using larger and larger datasets and are made publicly available. A large\nswath of computer vision scientists use these pre-trained networks with varying\ndegrees of successes in various tasks. Even though there is tremendous success\nin copying these networks, the representational space is not learnt from the\ntarget dataset in a traditional manner. One of the reasons for opting to use a\npre-trained network over a network learnt from scratch is that small datasets\nprovide less supervision and require meticulous regularization, smaller and\ncareful tweaking of learning rates to even achieve stable learning without\nweight explosion. It is often the case that large deep networks are not\nportable, which necessitates the ability to learn mid-sized networks from\nscratch.\n  In this article, we dive deeper into training these mid-sized networks on\nsmall datasets from scratch by drawing additional supervision from a large\npre-trained network. Such learning also provides better generalization\naccuracies than networks trained with common regularization techniques such as\nl2, l1 and dropouts. We show that features learnt thus, are more general than\nthose learnt independently. We studied various characteristics of such networks\nand found some interesting behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 20:05:45 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Li", "Baoxin", ""]]}, {"id": "1604.08256", "submitter": "Ricardo Fabbri", "authors": "Ricardo Fabbri and Benjamin Kimia", "title": "Multiview Differential Geometry of Curves", "comments": "International Journal of Computer Vision Final Accepted version.\n  International Journal of Computer Vision, 2016. The final publication is\n  available at Springer via http://dx.doi.org/10.1007/s11263-016-0912-7", "journal-ref": null, "doi": "10.1007/s11263-016-0912-7", "report-no": null, "categories": "cs.CV cs.CG cs.GR math.DG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of multiple view geometry has seen tremendous progress in\nreconstruction and calibration due to methods for extracting reliable point\nfeatures and key developments in projective geometry. Point features, however,\nare not available in certain applications and result in unstructured point\ncloud reconstructions. General image curves provide a complementary feature\nwhen keypoints are scarce, and result in 3D curve geometry, but face challenges\nnot addressed by the usual projective geometry of points and algebraic curves.\nWe address these challenges by laying the theoretical foundations of a\nframework based on the differential geometry of general curves, including\nstationary curves, occluding contours, and non-rigid curves, aiming at stereo\ncorrespondence, camera estimation (including calibration, pose, and multiview\nepipolar geometry), and 3D reconstruction given measured image curves. By\ngathering previous results into a cohesive theory, novel results were made\npossible, yielding three contributions. First we derive the differential\ngeometry of an image curve (tangent, curvature, curvature derivative) from that\nof the underlying space curve (tangent, curvature, curvature derivative,\ntorsion). Second, we derive the differential geometry of a space curve from\nthat of two corresponding image curves. Third, the differential motion of an\nimage curve is derived from camera motion and the differential geometry and\nmotion of the space curve. The availability of such a theory enables novel\ncurve-based multiview reconstruction and camera estimation systems to augment\nexisting point-based approaches. This theory has been used to reconstruct a \"3D\ncurve sketch\", to determine camera pose from local curve geometry, and\ntracking; other developments are underway.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 21:55:39 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Kimia", "Benjamin", ""]]}, {"id": "1604.08269", "submitter": "Pritish Mohapatra", "authors": "Pritish Mohapatra, Michal Rolinek, C.V. Jawahar, Vladimir Kolmogorov,\n  M. Pawan Kumar", "title": "Efficient Optimization for Rank-based Loss Functions", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of information retrieval systems is often measured using complex\nloss functions such as the average precision (AP) or the normalized discounted\ncumulative gain (NDCG). Given a set of positive and negative samples, the\nparameters of a retrieval system can be estimated by minimizing these loss\nfunctions. However, the non-differentiability and non-decomposability of these\nloss functions does not allow for simple gradient based optimization\nalgorithms. This issue is generally circumvented by either optimizing a\nstructured hinge-loss upper bound to the loss function or by using asymptotic\nmethods like the direct-loss minimization framework. Yet, the high\ncomputational complexity of loss-augmented inference, which is necessary for\nboth the frameworks, prohibits its use in large training data sets. To\nalleviate this deficiency, we present a novel quicksort flavored algorithm for\na large class of non-decomposable loss functions. We provide a complete\ncharacterization of the loss functions that are amenable to our algorithm, and\nshow that it includes both AP and NDCG based loss functions. Furthermore, we\nprove that no comparison based algorithm can improve upon the computational\ncomplexity of our approach asymptotically. We demonstrate the effectiveness of\nour approach in the context of optimizing the structured hinge loss upper bound\nof AP and NDCG loss for learning models for a variety of vision tasks. We show\nthat our approach provides significantly better results than simpler\ndecomposable loss functions, while requiring a comparable training time.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 23:33:19 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:37:57 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 09:27:30 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Mohapatra", "Pritish", ""], ["Rolinek", "Michal", ""], ["Jawahar", "C. V.", ""], ["Kolmogorov", "Vladimir", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1604.08352", "submitter": "Th\\'eodore Bluche", "authors": "Th\\'eodore Bluche", "title": "Joint Line Segmentation and Transcription for End-to-End Handwritten\n  Paragraph Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwriting recognition systems require cropped text line images for\nboth training and recognition. On the one hand, the annotation of position and\ntranscript at line level is costly to obtain. On the other hand, automatic line\nsegmentation algorithms are prone to errors, compromising the subsequent\nrecognition. In this paper, we propose a modification of the popular and\nefficient multi-dimensional long short-term memory recurrent neural networks\n(MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More\nparticularly, we replace the collapse layer transforming the two-dimensional\nrepresentation into a sequence of predictions by a recurrent version which can\nrecognize one line at a time. In the proposed model, a neural network performs\na kind of implicit line segmentation by computing attention weights on the\nimage representation. The experiments on paragraphs of Rimes and IAM database\nyield results that are competitive with those of networks trained at line\nlevel, and constitute a significant step towards end-to-end transcription of\nfull documents.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 09:08:30 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Bluche", "Th\u00e9odore", ""]]}, {"id": "1604.08426", "submitter": "Cheng Chen", "authors": "Cheng Chen, Xilin Zhang, Yizhou Wang, Fang Fang", "title": "A Novel Method to Study Bottom-up Visual Saliency and its Neural\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel method to measure bottom-up saliency maps\nof natural images. In order to eliminate the influence of top-down signals,\nbackward masking is used to make stimuli (natural images) subjectively\ninvisible to subjects, however, the bottom-up saliency can still orient the\nsubjects attention. To measure this orientation/attention effect, we adopt the\ncueing effect paradigm by deploying discrimination tasks at each location of an\nimage, and measure the discrimination performance variation across the image as\nthe attentional effect of the bottom-up saliency. Such attentional effects are\ncombined to construct a final bottomup saliency map. Based on the proposed\nmethod, we introduce a new bottom-up saliency map dataset of natural images to\nbenchmark computational models. We compare several state-of-the-art saliency\nmodels on the dataset. Moreover, the proposed paradigm is applied to\ninvestigate the neural basis of the bottom-up visual saliency map by analyzing\npsychophysical and fMRI experimental results. Our findings suggest that the\nbottom-up saliency maps of natural images are constructed in V1. It provides a\nstrong scientific evidence to resolve the long standing dispute in neuroscience\nabout where the bottom-up saliency map is constructed in human brain.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 12:14:31 GMT"}], "update_date": "2016-04-30", "authors_parsed": [["Chen", "Cheng", ""], ["Zhang", "Xilin", ""], ["Wang", "Yizhou", ""], ["Fang", "Fang", ""]]}, {"id": "1604.08524", "submitter": "Andres G. Abad", "authors": "Andres G. Abad and Luis I. Reyes Castro", "title": "A Probabilistic Adaptive Search System for Exploring the Face Space", "comments": "6 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recall is a basic human cognitive process performed routinely, e.g.,\nwhen meeting someone and determining if we have met that person before.\nAssisting a subject during face recall by suggesting candidate faces can be\nchallenging. One of the reasons is that the search space - the face space - is\nquite large and lacks structure. A commercial application of face recall is\nfacial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where a\nwitness searches for an image of a face that resembles his memory of a\nparticular offender. The inherent uncertainty and cost in the evaluation of the\nobjective function, the large size and lack of structure of the search space,\nand the unavailability of the gradient concept makes this problem inappropriate\nfor traditional optimization methods. In this paper we propose a novel\nevolutionary approach for searching the face space that can be used as a facial\ncomposite system. The approach is inspired by methods of Bayesian optimization\nand differs from other applications in the use of the skew-normal distribution\nas its acquisition function. This choice of acquisition function provides\ngreater granularity, with regularized, conservative, and realistic results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 17:23:42 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Abad", "Andres G.", ""], ["Castro", "Luis I. Reyes", ""]]}, {"id": "1604.08610", "submitter": "Manuel Ruder", "authors": "Manuel Ruder, Alexey Dosovitskiy, Thomas Brox", "title": "Artistic style transfer for videos", "comments": "final version appeared in GCPR-2016; minor changes to improve the\n  clarity", "journal-ref": "German Conference on Pattern Recognition (GCPR), LNCS 9796, pp.\n  26-36 (2016)", "doi": "10.1007/978-3-319-45886-1_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past, manually re-drawing an image in a certain artistic style\nrequired a professional artist and a long time. Doing this for a video sequence\nsingle-handed was beyond imagination. Nowadays computers provide new\npossibilities. We present an approach that transfers the style from one image\n(for example, a painting) to a whole video sequence. We make use of recent\nadvances in style transfer in still images and propose new initializations and\nloss functions applicable to videos. This allows us to generate consistent and\nstable stylized video sequences, even in cases with large motion and strong\nocclusion. We show that the proposed method clearly outperforms simpler\nbaselines both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 20:23:15 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 20:01:09 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Ruder", "Manuel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1604.08660", "submitter": "Chunhua Shen", "authors": "Biyun Sheng, Chunhua Shen, Guosheng Lin, Jun Li, Wankou Yang, Changyin\n  Sun", "title": "Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is an important task in computer vision, which has many\napplications in video surveillance. Although the regression-based framework has\nachieved great improvements for crowd counting, how to improve the\ndiscriminative power of image representation is still an open problem.\nConventional holistic features used in crowd counting often fail to capture\nsemantic attributes and spatial cues of the image. In this paper, we propose\nintegrating semantic information into learning locality-aware feature sets for\naccurate crowd counting. First, with the help of convolutional neural network\n(CNN), the original pixel space is mapped onto a dense attribute feature map,\nwhere each dimension of the pixel-wise feature indicates the probabilistic\nstrength of a certain semantic class. Then, locality-aware features (LAF) built\non the idea of spatial pyramids on neighboring patches are proposed to explore\nmore spatial context and local information. Finally, the traditional VLAD\nencoding method is extended to a more generalized form in which diverse\ncoefficient weights are taken into consideration. Experimental results validate\nthe effectiveness of our presented method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 00:55:41 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Sheng", "Biyun", ""], ["Shen", "Chunhua", ""], ["Lin", "Guosheng", ""], ["Li", "Jun", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""]]}, {"id": "1604.08671", "submitter": "Wenhan Yang", "authors": "Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu,\n  Zongming Guo, Shuicheng Yan", "title": "Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2750403", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the image super-resolution (SR) problem. The main\nchallenge of image SR is to recover high-frequency details of a low-resolution\n(LR) image that are important for human perception. To address this essentially\nill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE)\nnetwork to progressively recover the high-frequency details. Different from\nmost of existing methods that aim at predicting high-resolution (HR) images\ndirectly, DEGREE investigates an alternative route to recover the difference\nbetween a pair of LR and HR images by recurrent residual learning. DEGREE\nfurther augments the SR process with edge-preserving capability, namely the LR\nimage and its edge map can jointly infer the sharp edge details of the HR image\nduring the recurrent recovery process. To speed up its training convergence\nrate, by-pass connections across multiple layers of DEGREE are constructed. In\naddition, we offer an understanding on DEGREE from the view-point of sub-band\nfrequency decomposition on image signal and experimentally demonstrate how\nDEGREE can recover different frequency bands separately. Extensive experiments\non three benchmark datasets clearly demonstrate the superiority of DEGREE over\nwell-established baselines and DEGREE also provides new state-of-the-arts on\nthese datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 02:33:17 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 03:38:35 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Yang", "Wenhan", ""], ["Feng", "Jiashi", ""], ["Yang", "Jianchao", ""], ["Zhao", "Fang", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1604.08683", "submitter": "Jinjie You", "authors": "Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng", "title": "Top-push Video-based Person Re-identification", "comments": "In IEEE CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (re-id) models focus on matching still\nperson images across disjoint camera views. Since only limited information can\nbe exploited from still images, it is hard (if not impossible) to overcome the\nocclusion, pose and camera-view change, and lighting variation problems. In\ncomparison, video-based re-id methods can utilize extra space-time information,\nwhich contains much more rich cues for matching to overcome the mentioned\nproblems. However, we find that when using video-based representation, some\ninter-class difference can be much more obscure than the one when using\nstill-image based representation, because different people could not only have\nsimilar appearance but also have similar motions and actions which are hard to\nalign. To solve this problem, we propose a top-push distance learning model\n(TDL), in which we integrate a top-push constrain for matching video features\nof persons. The top-push constraint enforces the optimization on top-rank\nmatching in re-id, so as to make the matching model more effective towards\nselecting more discriminative features to distinguish different persons. Our\nexperiments show that the proposed video-based re-id framework outperforms the\nstate-of-the-art video-based re-id methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 04:14:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 15:10:37 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["You", "Jinjie", ""], ["Wu", "Ancong", ""], ["Li", "Xiang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1604.08685", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B.\n  Tenenbaum, Antonio Torralba, William T. Freeman", "title": "Single Image 3D Interpreter Network", "comments": "ECCV 2016 (oral). The first two authors contributed equally to this\n  work", "journal-ref": null, "doi": "10.1007/978-3-319-46466-4_22", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding 3D object structure from a single image is an important but\ndifficult task in computer vision, mostly due to the lack of 3D object\nannotations in real images. Previous work tackles this problem by either\nsolving an optimization task given 2D keypoint positions, or training on\nsynthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Network (3D-INN), an end-to-end framework which sequentially\nestimates 2D keypoint heatmaps and 3D object structure, trained on both real\n2D-annotated images and synthetic 3D data. This is made possible mainly by two\ntechnical innovations. First, we propose a Projection Layer, which projects\nestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D\nstructural parameters supervised by 2D annotations on real images. Second,\nheatmaps of keypoints serve as an intermediate representation connecting real\nand synthetic data, enabling 3D-INN to benefit from the variation and abundance\nof synthetic 3D objects, without suffering from the difference between the\nstatistics of real and synthesized images due to imperfect rendering. The\nnetwork achieves state-of-the-art performance on both 2D keypoint estimation\nand 3D structure recovery. We also show that the recovered 3D information can\nbe used in other vision applications, such as 3D rendering and image retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 04:52:46 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 19:35:54 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Wu", "Jiajun", ""], ["Xue", "Tianfan", ""], ["Lim", "Joseph J.", ""], ["Tian", "Yuandong", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""], ["Freeman", "William T.", ""]]}, {"id": "1604.08772", "submitter": "Frederic Besse", "authors": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka\n  and Daan Wierstra", "title": "Towards Conceptual Compression", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple recurrent variational auto-encoder architecture that\nsignificantly improves image modeling. The system represents the\nstate-of-the-art in latent variable models for both the ImageNet and Omniglot\ndatasets. We show that it naturally separates global conceptual information\nfrom lower level details, thus addressing one of the fundamentally desired\nproperties of unsupervised learning. Furthermore, the possibility of\nrestricting ourselves to storing only global information about an image allows\nus to achieve high quality 'conceptual compression'.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 11:02:52 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gregor", "Karol", ""], ["Besse", "Frederic", ""], ["Rezende", "Danilo Jimenez", ""], ["Danihelka", "Ivo", ""], ["Wierstra", "Daan", ""]]}, {"id": "1604.08789", "submitter": "Chourmouzios Tsiotsios Dr", "authors": "Chourmouzios Tsiotsios, Maria E. Angelopoulou, Andrew J. Davison,\n  Tae-Kyun Kim", "title": "Effective Backscatter Approximation for Photometry in Murky Water", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shading-based approaches like Photometric Stereo assume that the image\nformation model can be effectively optimized for the scene normals. However, in\nmurky water this is a very challenging problem. The light from artificial\nsources is not only reflected by the scene but it is also scattered by the\nmedium particles, yielding the backscatter component. Backscatter corresponds\nto a complex term with several unknown variables, and makes the problem of\nnormal estimation hard. In this work, we show that instead of trying to\noptimize the complex backscatter model or use previous unrealistic\nsimplifications, we can approximate the per-pixel backscatter signal directly\nfrom the captured images. Our method is based on the observation that\nbackscatter is saturated beyond a certain distance, i.e. it becomes scene-depth\nindependent, and finally corresponds to a smoothly varying signal which depends\nstrongly on the light position with respect to each pixel. Our backscatter\napproximation method facilitates imaging and scene reconstruction in murky\nwater when the illumination is artificial as in Photometric Stereo.\nSpecifically, we show that it allows accurate scene normal estimation and\noffers potentials like single image restoration. We evaluate our approach using\nnumerical simulations and real experiments within both the controlled\nenvironment of a big water-tank and real murky port-waters.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 12:14:10 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Tsiotsios", "Chourmouzios", ""], ["Angelopoulou", "Maria E.", ""], ["Davison", "Andrew J.", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1604.08806", "submitter": "Xinyu Lin", "authors": "Xinyu Lin, Ce Zhu, Yipeng Liu", "title": "Mesh Interest Point Detection Based on Geometric Measures and Sparse\n  Refinement", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional (3D) interest point detection plays a fundamental role in\n3D computer vision and graphics. In this paper, we introduce a new method for\ndetecting mesh interest points based on geometric measures and sparse\nrefinement (GMSR). The key point of our approach is to calculate the 3D\ninterest point response function using two intuitive and effective geometric\nproperties of the local surface on a 3D mesh model, namely Euclidean distances\nbetween the neighborhood vertices to the tangent plane of a vertex and the\nangles of normal vectors of them. The response function is defined in\nmulti-scale space and can be utilized to effectively distinguish 3D interest\npoints from edges and flat areas. Those points with local maximal 3D interest\npoint response value are selected as the candidates of 3D interest points.\nFinally, we utilize an $\\ell_0$ norm based optimization method to refine the\ncandidates of 3D interest points by constraining its quality and quantity.\nNumerical experiments demonstrate that our proposed GMSR based 3D interest\npoint detector outperforms current several state-of-the-art methods for\ndifferent kinds of 3D mesh models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 12:48:43 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 03:05:43 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 12:22:47 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lin", "Xinyu", ""], ["Zhu", "Ce", ""], ["Liu", "Yipeng", ""]]}, {"id": "1604.08826", "submitter": "Katsunori Ohnishi", "authors": "Katsunori Ohnishi, Masatoshi Hidaka, Tatsuya Harada", "title": "Improved Dense Trajectory with Cross Streams", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improved dense trajectories (iDT) have shown great performance in action\nrecognition, and their combination with the two-stream approach has achieved\nstate-of-the-art performance. It is, however, difficult for iDT to completely\nremove background trajectories from video with camera shaking. Trajectories in\nless discriminative regions should be given modest weights in order to create\nmore discriminative local descriptors for action recognition. In addition, the\ntwo-stream approach, which learns appearance and motion information separately,\ncannot focus on motion in important regions when extracting features from\nspatial convolutional layers of the appearance network, and vice versa. In\norder to address the above mentioned problems, we propose a new local\ndescriptor that pools a new convolutional layer obtained from crossing two\nnetworks along iDT. This new descriptor is calculated by applying\ndiscriminative weights learned from one network to a convolutional layer of the\nother network. Our method has achieved state-of-the-art performance on ordinal\naction recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 13:39:40 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Ohnishi", "Katsunori", ""], ["Hidaka", "Masatoshi", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1604.08865", "submitter": "Pouya Samangouei", "authors": "Pouya Samangouei and Rama Chellappa", "title": "Convolutional Neural Networks for Attribute-based Active Authentication\n  on Mobile Devices", "comments": "Accepted in BTAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a Deep Convolutional Neural Network (DCNN) architecture for the\ntask of continuous authentication on mobile devices. To deal with the limited\nresources of these devices, we reduce the complexity of the networks by\nlearning intermediate features such as gender and hair color instead of\nidentities. We present a multi-task, part-based DCNN architecture for attribute\ndetection that performs better than the state-of-the-art methods in terms of\naccuracy. As a byproduct of the proposed architecture, we are able to explore\nthe embedding space of the attributes extracted from different facial parts,\nsuch as mouth and eyes, to discover new attributes. Furthermore, through\nextensive experimentation, we show that the attribute features extracted by our\nmethod outperform the previously presented attribute-based method and a\nbaseline LBP method for the task of active authentication. Lastly, we\ndemonstrate the effectiveness of the proposed architecture in terms of speed\nand power consumption by deploying it on an actual mobile device.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 15:03:09 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 13:11:31 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Samangouei", "Pouya", ""], ["Chellappa", "Rama", ""]]}, {"id": "1604.08893", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Amaia Salvador, Xavier Giro-i-Nieto, Ferran Marques and Shin'ichi\n  Satoh", "title": "Faster R-CNN Features for Instance Search", "comments": "DeepVision Workshop in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representations derived from pre-trained Convolutional Neural Networks\n(CNNs) have become the new state of the art in computer vision tasks such as\ninstance retrieval. This work explores the suitability for instance retrieval\nof image- and region-wise representations pooled from an object detection CNN\nsuch as Faster R-CNN. We take advantage of the object proposals learned by a\nRegion Proposal Network (RPN) and their associated CNN features to build an\ninstance search pipeline composed of a first filtering stage followed by a\nspatial reranking. We further investigate the suitability of Faster R-CNN\nfeatures when the network is fine-tuned for the same objects one wants to\nretrieve. We assess the performance of our proposed system with the Oxford\nBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,\nachieving competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 16:00:24 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Salvador", "Amaia", ""], ["Giro-i-Nieto", "Xavier", ""], ["Marques", "Ferran", ""], ["Satoh", "Shin'ichi", ""]]}]