[{"id": "1107.0169", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Colin Ponce, Bart Selman, Ashutosh Saxena", "title": "Unstructured Human Activity Detection from RGBD Images", "comments": "2012 IEEE International Conference on Robotics and Automation (A\n  preliminary version of this work was presented at AAAI workshop on Pattern,\n  Activity and Intent Recognition, 2011)", "journal-ref": null, "doi": "10.1109/ICRA.2012.6224591", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to detect and recognize human activities is essential for several\napplications, including personal assistive robotics. In this paper, we perform\ndetection and recognition of unstructured human activity in unstructured\nenvironments. We use a RGBD sensor (Microsoft Kinect) as the input sensor, and\ncompute a set of features based on human pose and motion, as well as based on\nimage and pointcloud information. Our algorithm is based on a hierarchical\nmaximum entropy Markov model (MEMM), which considers a person's activity as\ncomposed of a set of sub-activities. We infer the two-layered graph structure\nusing a dynamic programming approach. We test our algorithm on detecting and\nrecognizing twelve different activities performed by four people in different\nenvironments, such as a kitchen, a living room, an office, etc., and achieve\ngood performance even when the person was not seen before in the training set.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 09:41:12 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2012 05:20:59 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Sung", "Jaeyong", ""], ["Ponce", "Colin", ""], ["Selman", "Bart", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1107.0399", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser, Vladimir Voronov", "title": "Vision-Based Navigation I: A navigation filter for fusing\n  DTM/correspondence updates", "comments": "26 pages, 3 figures, in English and in Russian. arXiv admin note:\n  substantial text overlap with arXiv:1106.6341, arXiv:1107.1470", "journal-ref": "Proceedings of the IEEE International Conference on Robotics and\n  Biomimetics (ROBIO), 2011 , Page(s): 1591 - 1596", "doi": "10.1109/ROBIO.2011.6181516", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for pose and motion estimation using corresponding features in\nimages and a digital terrain map is proposed. Using a Digital Terrain (or\nDigital Elevation) Map (DTM/DEM) as a global reference enables recovering the\nabsolute position and orientation of the camera. In order to do this, the DTM\nis used to formulate a constraint between corresponding features in two\nconsecutive frames. The utilization of data is shown to improve the robustness\nand accuracy of the inertial navigation algorithm. Extended Kalman filter was\nused to combine results of inertial navigation algorithm and proposed\nvision-based navigation algorithm. The feasibility of this algorithms is\nestablished through numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2011 17:34:30 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 20:02:53 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2012 21:37:28 GMT"}, {"version": "v4", "created": "Tue, 10 Jul 2012 18:40:53 GMT"}], "update_date": "2012-11-11", "authors_parsed": [["Kupervasser", "Oleg", ""], ["Voronov", "Vladimir", ""]]}, {"id": "1107.0550", "submitter": "Nicolas Brodu", "authors": "Nicolas Brodu and Dimitri Lague", "title": "3D Terrestrial lidar data classification of complex natural scenes using\n  a multi-scale dimensionality criterion: applications in geomorphology", "comments": "Free/Libre software implementation is available at\n  http://nicolas.brodu.numerimoire.net, as well as data sets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point clouds of natural environments relevant to problems in geomorphology\noften require classification of the data into elementary relevant classes. A\ntypical example is the separation of riparian vegetation from ground in fluvial\nenvironments, the distinction between fresh surfaces and rockfall in cliff\nenvironments, or more generally the classification of surfaces according to\ntheir morphology. Natural surfaces are heterogeneous and their distinctive\nproperties are seldom defined at a unique scale, prompting the use of\nmulti-scale criteria to achieve a high degree of classification success. We\nhave thus defined a multi-scale measure of the point cloud dimensionality\naround each point, which characterizes the local 3D organization. We can thus\nmonitor how the local cloud geometry behaves across scales. We present the\ntechnique and illustrate its efficiency in separating riparian vegetation from\nground and classifying a mountain stream as vegetation, rock, gravel or water\nsurface. In these two cases, separating the vegetation from ground or other\nclasses achieve accuracy larger than 98 %. Comparison with a single scale\napproach shows the superiority of the multi-scale analysis in enhancing class\nseparability and spatial resolution. The technique is robust to missing data,\nshadow zones and changes in point density within the scene. The classification\nis fast and accurate and can account for some degree of intra-class\nmorphological variability such as different vegetation types. A probabilistic\nconfidence in the classification result is given at each point, allowing the\nuser to remove the points for which the classification is uncertain. The\nprocess can be both fully automated, but also fully customized by the user\nincluding a graphical definition of the classifiers. Although developed for\nfully 3D data, the method can be readily applied to 2.5D airborne lidar data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 07:36:45 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2011 04:34:57 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2012 21:19:28 GMT"}], "update_date": "2012-01-25", "authors_parsed": [["Brodu", "Nicolas", ""], ["Lague", "Dimitri", ""]]}, {"id": "1107.0845", "submitter": "Sparisoma Viridi", "authors": "Suprijadi, Thomas Muliawan, Sparisoma Viridi", "title": "Automatic Road Lighting System (ARLS) Model Based on Image Processing of\n  Moving Object", "comments": "5 pages, 8 figures, 1 table, submitted to ARPN Journal of Science and\n  Technology", "journal-ref": "ARPN Journal of Science and Technology 3 (12), 1105-1109 (2013)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Using a vehicle toy (in next future called vehicle) as a moving object an\nautomatic road lighting system (ARLS) model is constructed. A digital video\ncamera with 25 fps is used to capture the vehicle motion as it moves in the\ntest segment of the road. Captured images are then processed to calculate\nvehicle speed. This information of the speed together with position of vehicle\nis then used to control the lighting system along the path that passes by the\nvehicle. Length of the road test segment is 1 m, the video camera is positioned\nabout 1.1 m above the test segment, and the vehicle toy dimension is 13 cm\n\\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about\n1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 11:06:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 00:26:49 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2011 06:32:43 GMT"}, {"version": "v4", "created": "Wed, 27 Nov 2013 03:39:17 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Suprijadi", "", ""], ["Muliawan", "Thomas", ""], ["Viridi", "Sparisoma", ""]]}, {"id": "1107.1058", "submitter": "Yongquan Lai", "authors": "Ranch Y.Q. Lai", "title": "Online Vehicle Detection For Estimating Traffic Status", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a traffic congestion estimation system based on unsupervised\non-line learning algorithm. The system does not rely on background extraction\nor motion detection. It extracts local features inside detection regions of\nvariable size which are drawn on lanes in advance. The extracted features are\nthen clustered into two classes using K-means and Gaussian Mixture Models(GMM).\nA Bayes classifier is used to detect vehicles according to the previous cluster\ninformation which keeps updated whenever system is running by on-line EM\nalgorithm. Experimental result shows that our system can be adapted to various\ntraffic scenes for estimating traffic status.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 08:43:38 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Lai", "Ranch Y. Q.", ""]]}, {"id": "1107.1081", "submitter": "Gururaj Mukarambi", "authors": "B.V. Dhandra, Mallikarjun Hangarge and Gururaj Mukarambi", "title": "Spatial Features for Multi-Font/Multi-Size Kannada Numerals and Vowels\n  Recognition", "comments": "4 pages, 4 Figures, 4 Tables, \"International Conference on\n  Communication, Computation, Control and Nanotechnology (2010)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents multi-font/multi-size Kannada numerals and vowels\nrecognition based on spatial features. Directional spatial features viz stroke\ndensity, stroke length and the number of stokes in an image are employed as\npotential features to characterize the printed Kannada numerals and vowels.\nBased on these features 1100 numerals and 1400 vowels are classified with\nMulti-class Support Vector Machines (SVM). The proposed system achieves the\nrecognition accuracy as 98.45% and 90.64% for numerals and vowels respectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 10:02:42 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Dhandra", "B. V.", ""], ["Hangarge", "Mallikarjun", ""], ["Mukarambi", "Gururaj", ""]]}, {"id": "1107.1119", "submitter": "Christoph Hertzberg", "authors": "Christoph Hertzberg, Ren\\'e Wagner, Udo Frese, Lutz Schr\\\"oder", "title": "Integrating Generic Sensor Fusion Algorithms with Sound State\n  Representations through Encapsulation of Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common estimation algorithms, such as least squares estimation or the Kalman\nfilter, operate on a state in a state space S that is represented as a\nreal-valued vector. However, for many quantities, most notably orientations in\n3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a\nvector space locally but has a more complex global topological structure. For\nintegrating these quantities, several ad-hoc approaches have been proposed.\n  Here, we present a principled solution to this problem where the structure of\nthe manifold S is encapsulated by two operators, state displacement [+]:S x R^n\n--> S and its inverse [-]: S x S --> R^n. These operators provide a local\nvector-space view \\delta; --> x [+] \\delta; around a given state x. Generic\nestimation algorithms can then work on the manifold S mainly by replacing +/-\nwith [+]/[-] where appropriate. We analyze these operators axiomatically, and\ndemonstrate their use in least-squares estimation and the Unscented Kalman\nFilter. Moreover, we exploit the idea of encapsulation from a software\nengineering perspective in the Manifold Toolkit, where the [+]/[-] operators\nmediate between a \"flat-vector\" view for the generic algorithm and a\n\"named-members\" view for the problem specific functions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 13:04:47 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Hertzberg", "Christoph", ""], ["Wagner", "Ren\u00e9", ""], ["Frese", "Udo", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "1107.1257", "submitter": "Muhammad Asim Mubeen", "authors": "M. Asim Mubeen, Kevin H. Knuth", "title": "Evidence-Based Filters for Signal Detection: Application to Evoked Brain\n  Responses", "comments": "8 Pages, 3 figures, 7 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-based signal detection most often relies on computing a correlation,\nor a dot product, between an incoming data stream and a signal template. Such a\ncorrelation results in an ongoing estimate of the magnitude of the signal in\nthe data stream. However, it does not directly indicate the presence or absence\nof the signal. The problem is really one of model-testing, and the relevant\nquantity is the Bayesian evidence (marginal likelihood) of the signal model.\nGiven a signal template and an ongoing data stream, we have developed an\nevidence-based filter that computes the Bayesian evidence that a signal is\npresent in the data. We demonstrate this algorithm by applying it to\nbrain-machine interface (BMI) data obtained by recording human brain electrical\nactivity, or electroencephalography (EEG). A very popular and effective\nparadigm in EEG-based BMI is based on the detection of the P300 evoked brain\nresponse which is generated in response to particular sensory stimuli. The goal\nis to detect the presence of a P300 signal in ongoing EEG activity as\naccurately and as fast as possible. Our algorithm uses a subject-specific P300\ntemplate to compute the Bayesian evidence that a applying window of EEG data\ncontains the signal. The efficacy of this algorithm is demonstrated by\ncomparing receiver operating characteristic (ROC) curves of the evidence-based\nfilter to the usual correlation method. Our results show a significant\nimprovement in single-trial P300 detection. The evidence-based filter promises\nto improve the accuracy and speed of the detection of evoked brain responses in\nBMI applications as well the detection of template signals in more general\nsignal processing applications\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 21:02:13 GMT"}], "update_date": "2011-07-08", "authors_parsed": [["Mubeen", "M. Asim", ""], ["Knuth", "Kevin H.", ""]]}, {"id": "1107.1470", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser, Ronen Lerner, Ehud Rivlin, Hector Rotstein", "title": "Vision-Based Navigation II: Error Analysis for a Navigation Algorithm\n  based on Optical-Flow and a Digital Terrain Map", "comments": "10 pages,12 figures, 2 tables", "journal-ref": "Proceedings of the 2008 IEEE/ION Position, Location and Navigation\n  Symposium, P.1203-1212", "doi": "10.1109/PLANS.2008.4570040", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the error analysis of a navigation algorithm that uses\nas input a sequence of images acquired by a moving camera and a Digital Terrain\nMap (DTM) of the region been imaged by the camera during the motion. The main\nsources of error are more or less straightforward to identify: camera\nresolution, structure of the observed terrain and DTM accuracy, field of view\nand camera trajectory. After characterizing and modeling these error sources in\nthe framework of the CDTM algorithm, a closed form expression for their effect\non the pose and motion errors of the camera can be found. The analytic\nexpression provides a priori measurements for the accuracy in terms of the\nparameters mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 18:09:55 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 23:14:05 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Kupervasser", "Oleg", ""], ["Lerner", "Ronen", ""], ["Rivlin", "Ehud", ""], ["Rotstein", "Hector", ""]]}, {"id": "1107.1561", "submitter": "Zhouchen Lin", "authors": "Wei Siming, Lin Zhouchen", "title": "Analysis and Improvement of Low Rank Representation for Subspace\n  segmentation", "comments": "Disclosed as Microsoft technical report on Auguat 25, 2010", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2010-177", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze and improve low rank representation (LRR), the state-of-the-art\nalgorithm for subspace segmentation of data. We prove that for the noiseless\ncase, the optimization model of LRR has a unique solution, which is the shape\ninteraction matrix (SIM) of the data matrix. So in essence LRR is equivalent to\nfactorization methods. We also prove that the minimum value of the optimization\nmodel of LRR is equal to the rank of the data matrix. For the noisy case, we\nshow that LRR can be approximated as a factorization method that combines noise\nremoval by column sparse robust PCA. We further propose an improved version of\nLRR, called Robust Shape Interaction (RSI), which uses the corrected data as\nthe dictionary instead of the noisy data. RSI is more robust than LRR when the\ncorruption in data is heavy. Experiments on both synthetic and real data\ntestify to the improved robustness of RSI.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 05:44:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Siming", "Wei", ""], ["Zhouchen", "Lin", ""]]}, {"id": "1107.1644", "submitter": "Jocelyne Troccaz", "authors": "Michael Baumann (TIMC), Pierre Mozer, Vincent Daanen, Jocelyne Troccaz\n  (TIMC)", "title": "Prostate biopsy tracking with deformation estimation", "comments": "Medical Image Analysis (2011) epub ahead of print", "journal-ref": null, "doi": "10.1016/j.media.2011.01.008", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transrectal biopsies under 2D ultrasound (US) control are the current\nclinical standard for prostate cancer diagnosis. The isoechogenic nature of\nprostate carcinoma makes it necessary to sample the gland systematically,\nresulting in a low sensitivity. Also, it is difficult for the clinician to\nfollow the sampling protocol accurately under 2D US control and the exact\nanatomical location of the biopsy cores is unknown after the intervention.\nTracking systems for prostate biopsies make it possible to generate biopsy\ndistribution maps for intra- and post-interventional quality control and 3D\nvisualisation of histological results for diagnosis and treatment planning.\nThey can also guide the clinician toward non-ultrasound targets. In this paper,\na volume-swept 3D US based tracking system for fast and accurate estimation of\nprostate tissue motion is proposed. The entirely image-based system solves the\npatient motion problem with an a priori model of rectal probe kinematics.\nProstate deformations are estimated with elastic registration to maximize\naccuracy. The system is robust with only 17 registration failures out of 786\n(2%) biopsy volumes acquired from 47 patients during biopsy sessions. Accuracy\nwas evaluated to 0.76$\\pm$0.52mm using manually segmented fiducials on 687\nregistered volumes stemming from 40 patients. A clinical protocol for assisted\nbiopsy acquisition was designed and implemented as a biopsy assistance system,\nwhich allows to overcome the draw-backs of the standard biopsy procedure.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 06:31:05 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Baumann", "Michael", "", "TIMC"], ["Mozer", "Pierre", "", "TIMC"], ["Daanen", "Vincent", "", "TIMC"], ["Troccaz", "Jocelyne", "", "TIMC"]]}, {"id": "1107.1837", "submitter": "Baogang Hu", "authors": "Bao-Gang Hu, Ran He, XiaoTong Yuan", "title": "Information-Theoretic Measures for Objective Evaluation of\n  Classifications", "comments": "25 Pages, 1 Figure, 10 Tables", "journal-ref": "Acta Automatica Sinica, 38(7): 1169-1182, 2012", "doi": "10.3724/SP.J.1004.2012.01169", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a systematic study of objective evaluations of abstaining\nclassifications using Information-Theoretic Measures (ITMs). First, we define\nobjective measures for which they do not depend on any free parameter. This\ndefinition provides technical simplicity for examining \"objectivity\" or\n\"subjectivity\" directly to classification evaluations. Second, we propose\ntwenty four normalized ITMs, derived from either mutual information,\ndivergence, or cross-entropy, for investigation. Contrary to conventional\nperformance measures that apply empirical formulas based on users' intuitions\nor preferences, the ITMs are theoretically more sound for realizing objective\nevaluations of classifications. We apply them to distinguish \"error types\" and\n\"reject types\" in binary classifications without the need for input data of\ncost terms. Third, to better understand and select the ITMs, we suggest three\ndesirable features for classification assessment measures, which appear more\ncrucial and appealing from the viewpoint of classification applications. Using\nthese features as \"meta-measures\", we can reveal the advantages and limitations\nof ITMs from a higher level of evaluation knowledge. Numerical examples are\ngiven to corroborate our claims and compare the differences among the proposed\nmeasures. The best measure is selected in terms of the meta-measures, and its\nspecific properties regarding error types and reject types are analytically\nderived.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2011 05:43:09 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Hu", "Bao-Gang", ""], ["He", "Ran", ""], ["Yuan", "XiaoTong", ""]]}, {"id": "1107.1987", "submitter": "Atanas Atanassov", "authors": "Atanas Marinov Atanassov", "title": "Median Algorithm for Sector Spectra Calculation from Images Registered\n  by the Spectral Airglow Temperature Imager", "comments": "6 pages, 5 figures, Sixth Scientific Conference \"Space Ecology\n  Safety\" 2-4 November 2010, Sofia, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Spectral Airglow Temperature Imager is an instrument, specially designed\nfor investigation of the wave processes in the Mesosphere-Lower Thermosphere.\nIn order to determine the kinematic parameters of a wave, the values of a\nphysical quantity in different space points and their changes in the time\nshould be known. As a result of the possibilities of the SATI instrument for\nspace scanning, different parts of the images (sectors of spectrograms)\ncorrespond to the respective mesopause areas (where the radiation is\ngenerated). An approach is proposed for sector spectra determination from SATI\nimages based on ordered statistics instead of meaning. Comparative results are\nshown.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 10:25:54 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Atanassov", "Atanas Marinov", ""]]}, {"id": "1107.2085", "submitter": "Taras Slipets", "authors": "Oleg Chertov, Taras Slipets", "title": "Kunchenko's Polynomials for Template Matching", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews Kunchenko's polynomials using as template matching method\nto recognize template in one-dimensional input signal. Kunchenko's polynomials\nmethod is compared with classical methods - cross-correlation and sum of\nsquared differences according to numerical statistical example.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:44:17 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Chertov", "Oleg", ""], ["Slipets", "Taras", ""]]}, {"id": "1107.2336", "submitter": "Nikolaos Nikolaidis", "authors": "N. S. Nikolaidis, I. N. Nikolaidis and C. C. Tsouros", "title": "A Variation of the Box-Counting Algorithm Applied to Colour Images", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The box counting method for fractal dimension estimation had not been applied\nto large or colour images thus far due to the processing time required. In this\nletter we present a fast, easy to implement and very easily expandable to any\nnumber of dimensions variation, the box merging method. It is applied here in\nRGB images which are considered as sets in 5-D space.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 16:21:06 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Nikolaidis", "N. S.", ""], ["Nikolaidis", "I. N.", ""], ["Tsouros", "C. C.", ""]]}, {"id": "1107.2347", "submitter": "Gautam Pendse", "authors": "Gautam V. Pendse", "title": "BSVM: A Banded Suport Vector Machine", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel binary classification technique called Banded SVM\n(B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the\ndecision rule is encouraged to lie in the interval [1, \\infty]. The new B-SVM\nobjective function contains a penalty term that encourages the decision rule to\nlie in a user specified range [\\rho_1, \\rho_2]. In addition to the standard set\nof support vectors (SVs) near the class boundaries, B-SVM results in a second\nset of SVs in the interior of each class.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 16:56:14 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Pendse", "Gautam V.", ""]]}, {"id": "1107.2553", "submitter": "Toufiq Parag", "authors": "Toufiq Parag and Vladimir Pavlovic and Ahmed Elgammal", "title": "Learning Hypergraph Labeling for Feature Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study poses the feature correspondence problem as a hypergraph node\nlabeling problem. Candidate feature matches and their subsets (usually of size\nlarger than two) are considered to be the nodes and hyperedges of a hypergraph.\nA hypergraph labeling algorithm, which models the subset-wise interaction by an\nundirected graphical model, is applied to label the nodes (feature\ncorrespondences) as correct or incorrect. We describe a method to learn the\ncost function of this labeling algorithm from labeled examples using a\ngraphical model training algorithm. The proposed feature matching algorithm is\ndifferent from the most of the existing learning point matching methods in\nterms of the form of the objective function, the cost function to be learned\nand the optimization method applied to minimize it. The results on standard\ndatasets demonstrate how learning over a hypergraph improves the matching\nperformance over existing algorithms, notably one that also uses higher order\ninformation without learning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 14:01:50 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Parag", "Toufiq", ""], ["Pavlovic", "Vladimir", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1107.2693", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin", "title": "A Fuzzy View on k-Means Based Signal Quantization with Application in\n  Iris Segmentation", "comments": "4, pages, 3 figures, 17th Telecommunications Forum TELFOR 2009,\n  Belgrade, Serbia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that the k-means quantization of a signal can be interpreted\nboth as a crisp indicator function and as a fuzzy membership assignment\ndescribing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy\nindicator functions are defined here as natural generalizations of the ordinary\ncrisp and fuzzy indicator functions, respectively. An application to iris\nsegmentation is presented together with a demo program.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 22:46:58 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""]]}, {"id": "1107.2696", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin", "title": "Exploring New Directions in Iris Recognition", "comments": "8 pages, 10 figures, 11th Int. Symp. on Symbolic and Numeric\n  Algorithms for Scientific Computing, 2009", "journal-ref": "Proc. 11th Int. Symp. on Symbolic and Numeric Algorithms for\n  Scientific Computing (2009), CPS-IEEE Computer Society, pp. 384-391, DOI:\n  10.1109/SYNASC.2009.45", "doi": "10.1109/SYNASC.2009.45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach in iris recognition based on Circular Fuzzy Iris Segmentation\n(CFIS) and Gabor Analytic Iris Texture Binary Encoder (GAITBE) is proposed and\ntested here. CFIS procedure is designed to guarantee that similar iris segments\nwill be obtained for similar eye images, despite the fact that the degree of\nocclusion may vary from one image to another. Its result is a circular iris\nring (concentric with the pupil) which approximates the actual iris. GAITBE\nproves better encoding of statistical independence between the iris codes\nextracted from different irides using Hilbert Transform. Irides from University\nof Bath Iris Database are binary encoded on two different lengths (768 / 192\nbytes) and tested in both single-enrollment and multi-enrollment identification\nscenarios. All cases illustrate the capacity of the newly proposed methodology\nto narrow down the distribution of inter-class matching scores, and\nconsequently, to guarantee a steeper descent of the False Accept Rate.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 23:18:57 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""]]}, {"id": "1107.2723", "submitter": "Soumen Bag", "authors": "Soumen Bag, Gaurav Harit", "title": "Topographic Feature Extraction for Bengali and Hindi Character Images", "comments": null, "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ),\n  vol.2, no.2, pp. 181-196, June 2011", "doi": "10.5121/sipij.2011.2215", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection and extraction plays an important role in different\nclassification based problems such as face recognition, signature verification,\noptical character recognition (OCR) etc. The performance of OCR highly depends\non the proper selection and extraction of feature set. In this paper, we\npresent novel features based on the topography of a character as visible from\ndifferent viewing directions on a 2D plane. By topography of a character we\nmean the structural features of the strokes and their spatial relations. In\nthis work we develop topographic features of strokes visible with respect to\nviews from different directions (e.g. North, South, East, and West). We\nconsider three types of topographic features: closed region, convexity of\nstrokes, and straight line strokes. These features are represented as a\nshape-based graph which acts as an invariant feature set for discriminating\nvery similar type characters efficiently. We have tested the proposed method on\nprinted and handwritten Bengali and Hindi character images. Initial results\ndemonstrate the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 04:05:23 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Bag", "Soumen", ""], ["Harit", "Gaurav", ""]]}, {"id": "1107.2781", "submitter": "Rami C.", "authors": "Rami Cohen", "title": "Face Recognition using Curvelet Transform", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Face recognition has been studied extensively for more than 20 years now.\nSince the beginning of 90s the subject has became a major issue. This\ntechnology is used in many important real-world applications, such as video\nsurveillance, smart cards, database security, internet and intranet access.\nThis report reviews recent two algorithms for face recognition which take\nadvantage of a relatively new multiscale geometric analysis tool - Curvelet\ntransform, for facial processing and feature extraction. This transform proves\nto be efficient especially due to its good ability to detect curves and lines,\nwhich characterize the human's face. An algorithm which is based on the two\nalgorithms mentioned above is proposed, and its performance is evaluated on\nthree data bases of faces: AT&T (ORL), Essex Grimace and Georgia-Tech.\nk-nearest neighbour (k-NN) and Support vector machine (SVM) classifiers are\nused, along with Principal Component Analysis (PCA) for dimensionality\nreduction. This algorithm shows good results, and it even outperforms other\nalgorithms in some cases.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 10:44:01 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Cohen", "Rami", ""]]}, {"id": "1107.2782", "submitter": "Rami Cohen", "authors": "Rami Cohen", "title": "The Chan-Vese Algorithm", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Segmentation is the process of partitioning a digital image into multiple\nsegments (sets of pixels). Such common segmentation tasks including segmenting\nwritten text or segmenting tumors from healthy brain tissue in an MRI image,\netc. Chan-Vese model for active contours is a powerful and flexible method\nwhich is able to segment many types of images, including some that would be\nquite difficult to segment in means of \"classical\" segmentation - i.e., using\nthresholding or gradient based methods. This model is based on the Mumford-Shah\nfunctional for segmentation, and is used widely in the medical imaging field,\nespecially for the segmentation of the brain, heart and trachea. The model is\nbased on an energy minimization problem, which can be reformulated in the level\nset formulation, leading to an easier way to solve the problem. In this\nproject, the model will be presented (there is an extension to color\n(vector-valued) images, but it will not be considered here), and Matlab code\nthat implements it will be introduced.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 10:44:36 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2011 10:08:37 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Cohen", "Rami", ""]]}, {"id": "1107.2807", "submitter": "Boris Flach", "authors": "Boris Flach and Dmitrij Schlesinger", "title": "Modelling Distributed Shape Priors by Gibbs Random Fields of Second\n  Order", "comments": "17 pages, 8 figures", "journal-ref": "Control Systems and Computers, (2) 2011, pp 14-24", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the potential of Gibbs Random Fields for shape prior modelling. We\nshow that the expressive power of second order GRFs is already sufficient to\nexpress simple shapes and spatial relations between them simultaneously. This\nallows to model and recognise complex shapes as spatial compositions of simpler\nparts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 12:51:10 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Flach", "Boris", ""], ["Schlesinger", "Dmitrij", ""]]}, {"id": "1107.2859", "submitter": "Jinhui Tang", "authors": "Jinhui Tang, Shuicheng Yan, Tat-Seng Chua and Ramesh Jain", "title": "Label-Specific Training Set Construction from Web Resource for Image\n  Annotation", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many research efforts have been devoted to image annotation by\nleveraging on the associated tags/keywords of web images as training labels. A\nkey issue to resolve is the relatively low accuracy of the tags. In this paper,\nwe propose a novel semi-automatic framework to construct a more accurate and\neffective training set from these web media resources for each label that we\nwant to learn. Experiments conducted on a real-world dataset demonstrate that\nthe constructed training set can result in higher accuracy for image\nannotation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 15:52:21 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Chua", "Tat-Seng", ""], ["Jain", "Ramesh", ""]]}, {"id": "1107.2875", "submitter": "Chris Aholt", "authors": "Chris Aholt, Bernd Sturmfels, Rekha Thomas", "title": "A Hilbert Scheme in Computer Vision", "comments": "26 pages", "journal-ref": "Can. J. Math.-J. Can. Math. 65 (2013) 961-988", "doi": "10.4153/CJM-2012-023-2", "report-no": "Mittag-Leffler-2011spring", "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview geometry is the study of two-dimensional images of\nthree-dimensional scenes, a foundational subject in computer vision. We\ndetermine a universal Groebner basis for the multiview ideal of n generic\ncameras. As the cameras move, the multiview varieties vary in a family of\ndimension 11n-15. This family is the distinguished component of a multigraded\nHilbert scheme with a unique Borel-fixed point. We present a combinatorial\nstudy of ideals lying on that Hilbert scheme.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 17:36:59 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Aholt", "Chris", ""], ["Sturmfels", "Bernd", ""], ["Thomas", "Rekha", ""]]}, {"id": "1107.3194", "submitter": "Thai Le", "authors": "Le Hoang Thai and Ha Nhat Tam", "title": "Fingerprint recognition using standardized fingerprint model", "comments": "7 pages, 16 figures, 3 tables, IJCSI International Journal of\n  Computer Science Issues, Vol. 7, Issue 3, No 7, May 2010", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 7,\n  Issue 3, No 7, May 2010, ISSN (Online): 1694-0784, ISSN (Print): 1694-0814", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition is one of most popular and accuracy Biometric\ntechnologies. Nowadays, it is used in many real applications. However,\nrecognizing fingerprints in poor quality images is still a very complex\nproblem. In recent years, many algorithms, models...are given to improve the\naccuracy of recognition system. This paper discusses on the standardized\nfingerprint model which is used to synthesize the template of fingerprints. In\nthis model, after pre-processing step, we find the transformation between\ntemplates, adjust parameters, synthesize fingerprint, and reduce noises. Then,\nwe use the final fingerprint to match with others in FVC2004 fingerprint\ndatabase (DB4) to show the capability of the model.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2011 03:04:22 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Thai", "Le Hoang", ""], ["Tam", "Ha Nhat", ""]]}, {"id": "1107.3195", "submitter": "Thai Le", "authors": "Thai Le, Phat Tat and Hai Tran", "title": "Facial Expression Classification Based on Multi Artificial Neural\n  Network and Two Dimensional Principal Component Analysis", "comments": "8 pages, 16 figures, IJCSI International Journal of Computer Science\n  Issues, Vol. 8, Issue 3, No. 1, May 2011", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 1, May 2011, ISSN (Online): 1694-0814, www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression classification is a kind of image classification and it has\nreceived much attention, in recent years. There are many approaches to solve\nthese problems with aiming to increase efficient classification. One of famous\nsuggestions is described as first step, project image to different spaces;\nsecond step, in each of these spaces, images are classified into responsive\nclass and the last step, combine the above classified results into the final\nresult. The advantages of this approach are to reflect fulfill and multiform of\nimage classified. In this paper, we use 2D-PCA and its variants to project the\npattern or image into different spaces with different grouping strategies. Then\nwe develop a model which combines many Neural Networks applied for the last\nstep. This model evaluates the reliability of each space and gives the final\nclassification conclusion. Our model links many Neural Networks together, so we\ncall it Multi Artificial Neural Network (MANN). We apply our proposal model for\n6 basic facial expressions on JAFFE database consisting 213 images posed by 10\nJapanese female models.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2011 03:15:40 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Le", "Thai", ""], ["Tat", "Phat", ""], ["Tran", "Hai", ""]]}, {"id": "1107.3348", "submitter": "Firouz Wassai", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar, Ali A. Al-Zuky", "title": "Arithmetic and Frequency Filtering Methods of Pixel-Based Image Fusion\n  Techniques", "comments": "Image Fusion, Pixel-Based Fusion, Brovey Transform, Color Normalized,\n  High-Pass Filter, Modulation, Wavelet transform", "journal-ref": "Journal-ref: International Journal of Advanced Research in\n  Computer Science,Volume 2, No. 5, Sept-Oct 2011,www.ijarcs.info", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing, image fusion technique is a useful tool used to fuse high\nspatial resolution panchromatic images (PAN) with lower spatial resolution\nmultispectral images (MS) to create a high spatial resolution multispectral of\nimage fusion (F) while preserving the spectral information in the multispectral\nimage (MS).There are many PAN sharpening techniques or Pixel-Based image fusion\ntechniques that have been developed to try to enhance the spatial resolution\nand the spectral property preservation of the MS. This paper attempts to\nundertake the study of image fusion, by using two types of pixel-based image\nfusion techniques i.e. Arithmetic Combination and Frequency Filtering Methods\nof Pixel-Based Image Fusion Techniques. The first type includes Brovey\nTransform (BT), Color Normalized Transformation (CN) and Multiplicative Method\n(MLT). The second type include High-Pass Filter Additive Method (HPFA),\nHigh-Frequency-Addition Method (HFA) High Frequency Modulation Method (HFM) and\nThe Wavelet transform-based fusion method (WT). This paper also devotes to\nconcentrate on the analytical techniques for evaluating the quality of image\nfusion (F) by using various methods including Standard Deviation (SD),\nEntropy(En), Correlation Coefficient (CC), Signal-to Noise Ratio (SNR),\nNormalization Root Mean Square Error (NRMSE) and Deviation Index (DI) to\nestimate the quality and degree of information improvement of a fused image\nquantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 01:41:37 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2011 05:20:59 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""], ["Al-Zuky", "Ali A.", ""]]}, {"id": "1107.3499", "submitter": "Robert Corrie", "authors": "Robert Corrie, Yoshiki Ninomiya, and Jonathan Aitchison", "title": "Applying Advanced Spaceborne Thermal Emission and Reflection Radiometer\n  (ASTER) spectral indices for geological mapping and mineral identification on\n  the Tibetan Plateau", "comments": "6 pages, 4 figures, 2 tables, Published in the International Archives\n  of the Photogrammetry, Remote Sensing, and Spatial Information Science,\n  Volume XXXVIII, pp. 464-469. For associated web page, see\n  http://www.isprs.org/proceedings/XXXVIII/part8/headline/PS-1%20Interactive%20PresentationWG%20VIII5.html", "journal-ref": "International Archives of the Photogrammetry, Remote Sensing, and\n  Spatial Information Science, XXXVIII (2010) 464-469", "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tibetan Plateau holds clues to understanding the dynamics and mechanisms\nassociated with continental growth. Part of the region is characterized by\nzones of ophiolitic melange believed to represent the remnants of ancient\noceanic crust and underlying upper mantle emplaced during oceanic closures.\nHowever, due to the remoteness of the region and the inhospitable terrain many\nareas have not received detailed investigation. Increased spatial and spectral\nresolution of satellite sensors have made it possible to map in greater detail\nthe mineralogy and lithology than in the past. Recent work by Yoshiki Ninomiya\nof the Geological Survey of Japan has pioneered the use of several spectral\nindices for the mapping of quartzose, carbonate, and silicate rocks using\nAdvanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) thermal\ninfrared (TIR) data. In this study, ASTER TIR indices have been applied to a\nregion in western-central Tibet for the purposes of assessing their\neffectiveness for differentiating ophiolites and other lithologies. The results\nagree well with existing geological maps and other published data. The study\narea was chosen due to its diverse range of rock types, including an ophiolitic\nmelange, associated with the Bangong-Nujiang suture (BNS) that crops out on the\nnorthern shores of Lagkor Tso and Dong Tso (\"Tso\" is Tibetan for lake). The\ntechniques highlighted in this paper could be applied to other geographical\nregions where similar geological questions need to be resolved. The results of\nthis study aim to show the utility of ASTER TIR imagery for geological mapping\nin semi-arid and sparsely vegetated areas on the Tibetan Plateau.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 16:59:30 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Corrie", "Robert", ""], ["Ninomiya", "Yoshiki", ""], ["Aitchison", "Jonathan", ""]]}, {"id": "1107.3680", "submitter": "Raj Kishen Moloo Mr", "authors": "Raj Kishen Moloo, Muhammad Ajmal Sheik Dawood, Abu Salmaan Auleear", "title": "3-Phase Recognition Approach to Pseudo 3D Building Generation from 2D\n  Floor Plan", "comments": "15 pages,12 figures, 2 tables, International Journal of Computer\n  Graphics & Animation (IJCGA) Vol.1, No.2, June 2011", "journal-ref": "International Journal of Computer Graphics & Animation (IJCGA)\n  Vol.1, No.2, June 2011", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays three dimension (3D) architectural visualisation has become a\npowerful tool in the conceptualisation, design and presentation of\narchitectural products in the construction industry, providing realistic\ninteraction and walkthrough on engineering products. Traditional ways of\nimplementing 3D models involves the use of specialised 3D authoring tools along\nwith skilled 3D designers with blueprints of the model and this is a slow and\nlaborious process. The aim of this paper is to automate this process by simply\nanalyzing the blueprint document and generating the 3D scene automatically. For\nthis purpose we have devised a 3-Phase recognition approach to pseudo 3D\nbuilding generation from 2D floor plan and developed a software accordingly.\nOur 3-phased 3D building system has been implemented using C, C++ and OpenCV\nlibrary [24] for the Image Processing module; The Save Module generated an XML\nfile for storing the processed floor plan objects attributes; while the\nIrrlitch [14] game engine was used to implement the Interactive 3D module.\nThough still at its infancy, our proposed system gave commendable results. We\ntested our system on 6 floor plans with complexities ranging from low to high\nand the results seems to be very promising with an average processing time of\naround 3s and a 3D generation in 4s. In addition the system provides an\ninteractive walk-though and allows users to modify components.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 10:50:31 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Moloo", "Raj Kishen", ""], ["Dawood", "Muhammad Ajmal Sheik", ""], ["Auleear", "Abu Salmaan", ""]]}, {"id": "1107.3823", "submitter": "Nicolas Le Roux", "authors": "Nicolas Heess (Informatics), Nicolas Le Roux (INRIA Paris -\n  Rocquencourt), John Winn", "title": "Weakly Supervised Learning of Foreground-Background Segmentation using\n  Masked RBMs", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (2011)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the Restricted Boltzmann Machine (RBM) that allows\nthe joint shape and appearance of foreground objects in cluttered images to be\nmodeled independently of the background. We present a learning scheme that\nlearns this representation directly from cluttered images with only very weak\nsupervision. The model generates plausible samples and performs\nforeground-background segmentation. We demonstrate that representing foreground\nobjects independently of the background can be beneficial in recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 19:43:10 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Heess", "Nicolas", "", "Informatics"], ["Roux", "Nicolas Le", "", "INRIA Paris -\n  Rocquencourt"], ["Winn", "John", ""]]}, {"id": "1107.4396", "submitter": "Firouz Wassai", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar, Ali A. Al-Zuky", "title": "The IHS Transformations Based Image Fusion", "comments": "Image Fusion, Color Models, IHS, HSV, HSL, YIQ, transformations", "journal-ref": "Journal-ref: International Journal of Advanced Research in\n  Computer Science,Volume 2, No. 5, Sept-Oct 2011,www.ijarcs.info", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IHS sharpening technique is one of the most commonly used techniques for\nsharpening. Different transformations have been developed to transfer a color\nimage from the RGB space to the IHS space. Through literature, it appears that,\nvarious scientists proposed alternative IHS transformations and many papers\nhave reported good results whereas others show bad ones as will as not those\nobtained which the formula of IHS transformation were used. In addition to\nthat, many papers show different formulas of transformation matrix such as IHS\ntransformation. This leads to confusion what is the exact formula of the IHS\ntransformation?. Therefore, the main purpose of this work is to explore\ndifferent IHS transformation techniques and experiment it as IHS based image\nfusion. The image fusion performance was evaluated, in this study, using\nvarious methods to estimate the quality and degree of information improvement\nof a fused image quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 06:18:56 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2011 01:08:40 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""], ["Al-Zuky", "Ali A.", ""]]}, {"id": "1107.4617", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal Narayan Chaudhury", "title": "Constant-time filtering using shiftable kernels", "comments": "Accepted in IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Processing Letters, vol. 18(11), pp. 651 - 654, 2011", "doi": "10.1109/LSP.2011.2167967", "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently demonstrated in [5] that the non-linear bilateral filter [14]\ncan be efficiently implemented using a constant-time or O(1) algorithm. At the\nheart of this algorithm was the idea of approximating the Gaussian range kernel\nof the bilateral filter using trigonometric functions. In this letter, we\nexplain how the idea in [5] can be extended to few other linear and non-linear\nfilters [14, 17, 2]. While some of these filters have received a lot of\nattention in recent years, they are known to be computationally intensive. To\nextend the idea in [5], we identify a central property of trigonometric\nfunctions, called shiftability, that allows us to exploit the redundancy\ninherent in the filtering operations. In particular, using shiftable kernels,\nwe show how certain complex filtering can be reduced to simply that of\ncomputing the moving sum of a stack of images. Each image in the stack is\nobtained through an elementary pointwise transform of the input image. This has\na two-fold advantage. First, we can use fast recursive algorithms for computing\nthe moving sum [15, 6], and, secondly, we can use parallel computation to\nfurther speed up the computation. We also show how shiftable kernels can also\nbe used to approximate the (non-shiftable) Gaussian kernel that is ubiquitously\nused in image filtering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 20:18:16 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 13:52:16 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2011 15:55:45 GMT"}, {"version": "v4", "created": "Thu, 8 Sep 2011 16:11:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chaudhury", "Kunal Narayan", ""]]}, {"id": "1107.4619", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal Narayan Chaudhury and Michael Unser", "title": "On the Hilbert transform of wavelets", "comments": "Appears in IEEE Transactions on Signal Processing, vol. 59, no. 4,\n  pp. 1890-1894, 2011", "journal-ref": "IEEE Transactions on Signal Processing, vol. 19(11), pp. 1890 -\n  1894, 2011", "doi": null, "report-no": null, "categories": "math.FA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wavelet is a localized function having a prescribed number of vanishing\nmoments. In this correspondence, we provide precise arguments as to why the\nHilbert transform of a wavelet is again a wavelet. In particular, we provide\nsharp estimates of the localization, vanishing moments, and smoothness of the\ntransformed wavelet. We work in the general setting of non-compactly supported\nwavelets. Our main result is that, in the presence of some minimal smoothness\nand decay, the Hilbert transform of a wavelet is again as smooth and\noscillating as the original wavelet, whereas its localization is controlled by\nthe number of vanishing moments of the original wavelet. We motivate our\nresults using concrete examples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 20:33:13 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Chaudhury", "Kunal Narayan", ""], ["Unser", "Michael", ""]]}, {"id": "1107.4637", "submitter": "George Papandreou", "authors": "George Papandreou and Alan Yuille", "title": "Efficient variational inference in large-scale Bayesian compressed\n  sensing", "comments": "8 pages, 3 figures, appears in Proc. IEEE Workshop on Information\n  Theory in Computer Vision and Pattern Recognition (in conjunction with\n  ICCV-11), Barcelona, Spain, Nov. 2011", "journal-ref": "Proc. IEEE Workshop on Information Theory in Computer Vision and\n  Pattern Recognition (in conjunction with ICCV-11), pp. 1332-1339, Barcelona,\n  Spain, Nov. 2011", "doi": "10.1109/ICCVW.2011.6130406", "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study linear models under heavy-tailed priors from a probabilistic\nviewpoint. Instead of computing a single sparse most probable (MAP) solution as\nin standard deterministic approaches, the focus in the Bayesian compressed\nsensing framework shifts towards capturing the full posterior distribution on\nthe latent variables, which allows quantifying the estimation uncertainty and\nlearning model parameters using maximum likelihood. The exact posterior\ndistribution under the sparse linear model is intractable and we concentrate on\nvariational Bayesian techniques to approximate it. Repeatedly computing\nGaussian variances turns out to be a key requisite and constitutes the main\ncomputational bottleneck in applying variational techniques in large-scale\nproblems. We leverage on the recently proposed Perturb-and-MAP algorithm for\ndrawing exact samples from Gaussian Markov random fields (GMRF). The main\ntechnical contribution of our paper is to show that estimating Gaussian\nvariances using a relatively small number of such efficiently drawn random\nsamples is much more effective than alternative general-purpose variance\nestimation techniques. By reducing the problem of variance estimation to\nstandard optimization primitives, the resulting variational algorithms are\nfully scalable and parallelizable, allowing Bayesian computations in extremely\nlarge-scale problems with the same memory and time complexity requirements as\nconventional point estimation techniques. We illustrate these ideas with\nexperiments in image deblurring.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 22:10:38 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2011 02:04:00 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Papandreou", "George", ""], ["Yuille", "Alan", ""]]}, {"id": "1107.4667", "submitter": "Vijayaraghavan Thirumalai", "authors": "Vijayaraghavan Thirumalai and Pascal Frossard", "title": "Correlation Estimation from Compressed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of correlation estimation in sets of\ncompressed images. We consider a framework where images are represented under\nthe form of linear measurements due to low complexity sensing or security\nrequirements. We assume that the images are correlated through the displacement\nof visual objects due to motion or viewpoint change and the correlation is\neffectively represented by optical flow or motion field models. The correlation\nis estimated in the compressed domain by jointly processing the linear\nmeasurements. We first show that the correlated images can be efficiently\nrelated using a linear operator. Using this linear relationship we then\ndescribe the dependencies between images in the compressed domain. We further\ncast a regularized optimization problem where the correlation is estimated in\norder to satisfy both data consistency and motion smoothness objectives with a\nGraph Cut algorithm. We analyze in detail the correlation estimation\nperformance and quantify the penalty due to image compression. Extensive\nexperiments in stereo and video imaging applications show that our novel\nsolution stays competitive with methods that implement complex image\nreconstruction steps prior to correlation estimation. We finally use the\nestimated correlation in a novel joint image reconstruction scheme that is\nbased on an optimization problem with sparsity priors on the reconstructed\nimages. Additional experiments show that our correlation estimation algorithm\nleads to an effective reconstruction of pairs of images in distributed image\ncoding schemes that outperform independent reconstruction algorithms by 2 to 4\ndB.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2011 08:51:17 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2011 18:28:47 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Thirumalai", "Vijayaraghavan", ""], ["Frossard", "Pascal", ""]]}, {"id": "1107.4763", "submitter": "ANqi Qiu DR", "authors": "Jia Du, Alvina Goh, Anqi Qiu", "title": "Diffeomorphic Metric Mapping of High Angular Resolution Diffusion\n  Imaging based on Riemannian Structure of Orientation Distribution Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel large deformation diffeomorphic\nregistration algorithm to align high angular resolution diffusion images\n(HARDI) characterized by orientation distribution functions (ODFs). Our\nproposed algorithm seeks an optimal diffeomorphism of large deformation between\ntwo ODF fields in a spatial volume domain and at the same time, locally\nreorients an ODF in a manner such that it remains consistent with the\nsurrounding anatomical structure. To this end, we first review the Riemannian\nmanifold of ODFs. We then define the reorientation of an ODF when an affine\ntransformation is applied and subsequently, define the diffeomorphic group\naction to be applied on the ODF based on this reorientation. We incorporate the\nRiemannian metric of ODFs for quantifying the similarity of two HARDI images\ninto a variational problem defined under the large deformation diffeomorphic\nmetric mapping (LDDMM) framework. We finally derive the gradient of the cost\nfunction in both Riemannian spaces of diffeomorphisms and the ODFs, and present\nits numerical implementation. Both synthetic and real brain HARDI data are used\nto illustrate the performance of our registration algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2011 15:34:27 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Du", "Jia", ""], ["Goh", "Alvina", ""], ["Qiu", "Anqi", ""]]}, {"id": "1107.4958", "submitter": "Elhanan Elboher", "authors": "Elhanan Elboher and Michael Werman", "title": "Efficient and Accurate Gaussian Image Filtering Using Running Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple and efficient method to convolve an image with a\nGaussian kernel. The computation is performed in a constant number of\noperations per pixel using running sums along the image rows and columns. We\ninvestigate the error function used for kernel approximation and its relation\nto the properties of the input signal. Based on natural image statistics we\npropose a quadratic form kernel error function so that the output image l2\nerror is minimized. We apply the proposed approach to approximate the Gaussian\nkernel by linear combination of constant functions. This results in very\nefficient Gaussian filtering method. Our experiments show that the proposed\ntechnique is faster than state of the art methods while preserving a similar\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 14:20:34 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Elboher", "Elhanan", ""], ["Werman", "Michael", ""]]}, {"id": "1107.4985", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Gaussian Process Dynamical Systems", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional time series are endemic in applications of machine learning\nsuch as robotics (sensor data), computational biology (gene expression data),\nvision (video sequences) and graphics (motion capture data). Practical\nnonlinear probabilistic approaches to this data are required. In this paper we\nintroduce the variational Gaussian process dynamical system. Our work builds on\nrecent variational approximations for Gaussian process latent variable models\nto allow for nonlinear dimensionality reduction simultaneously with learning a\ndynamical prior in the latent space. The approach also allows for the\nappropriate dimensionality of the latent space to be automatically determined.\nWe demonstrate the model on a human motion capture data set and a series of\nhigh resolution video sequences.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:54:05 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1107.5000", "submitter": "Fabricio Martins Lopes", "authors": "Fabr\\'icio Martins Lopes and David C. Martins-Jr and Junior Barrera\n  and Roberto M. Cesar-Jr", "title": "An iterative feature selection method for GRNs inference by exploring\n  topological properties", "comments": "10 pages, 5 figures, SFFS search method based on scale-free network\n  topology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT math.IT q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in bioinformatics is the inference of gene regulatory\nnetworks (GRN) from temporal expression profiles. In general, the main\nlimitations faced by GRN inference methods is the small number of samples with\nhuge dimensionalities and the noisy nature of the expression measurements. In\nface of these limitations, alternatives are needed to get better accuracy on\nthe GRNs inference problem. This work addresses this problem by presenting an\nalternative feature selection method that applies prior knowledge on its search\nstrategy, called SFFS-BA. The proposed search strategy is based on the\nSequential Floating Forward Selection (SFFS) algorithm, with the inclusion of a\nscale-free (Barab\\'asi-Albert) topology information in order to guide the\nsearch process to improve inference. The proposed algorithm explores the\nscale-free property by pruning the search space and using a power law as a\nweight for reducing it. In this way, the search space traversed by the SFFS-BA\nmethod combines a breadth-first search when the number of combinations is small\n(<k> <= 2) with a depth-first search when the number of combinations becomes\nexplosive (<k> >= 3), being guided by the scale-free prior information.\nExperimental results show that the SFFS-BA provides a better inference\nsimilarities than SFS and SFFS, keeping the robustness of the SFS and SFFS\nmethods, thus presenting very good results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 17:04:33 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Lopes", "Fabr\u00edcio Martins", ""], ["Martins-Jr", "David C.", ""], ["Barrera", "Junior", ""], ["Cesar-Jr", "Roberto M.", ""]]}, {"id": "1107.5186", "submitter": "Preben Gr{\\aa}berg Nes", "authors": "Preben Gr{\\aa}berg Nes", "title": "Fast multi-scale edge-detection in medical ultrasound signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we suggest a fast multi-scale edge-detection scheme for\nmedical ultrasound signals. The edge-detector is based on well-known properties\nof the continuous wavelet trans- form. To achieve both good localization of\nedges and detect only significant edges, we study the maxima-lines of the\nwavelet transform. One can obtain the maxima-lines between two scales by\ncomputing the wavelet transform at several intermediate scales. To reduce\ncomputational effort and time we suggest a time-scale filtering procedure which\nuses only few scales to connect modulus-maxima across time-scale plane. The\ndesign of this procedure is based on a study of maxima-lines corresponding to\nedges typical for medical ultrasound signals. This study allows us to construct\nan algorithm for medical ultrasound signals which meets the demand for speed,\nbut not on expense of reliability. The edge-detection algorithm has been\napplied to a large class of medical ultrasound sig- nals including tumour-,\nliver- and artery-images. Our results show that the proposed algorithm\neffectively detects major features in such signals, including edges with low\ncontrast.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 11:59:40 GMT"}], "update_date": "2011-07-27", "authors_parsed": [["Nes", "Preben Gr\u00e5berg", ""]]}, {"id": "1107.5349", "submitter": "Luca Pinello", "authors": "Luca Pinello", "title": "Multi Layer Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents a new methodology to analyze one-dimensional signals\ntrough a new approach called Multi Layer Analysis, for short MLA. It also\nprovides some new insights on the relationship between one-dimensional signals\nprocessed by MLA and tree kernels, test of randomness and signal processing\ntechniques. The MLA approach has a wide range of application to the fields of\npattern discovery and matching, computational biology and many other areas of\ncomputer science and signal processing. This thesis includes also some\napplications of this approach to real problems in biology and seismology.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 22:29:35 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Pinello", "Luca", ""]]}, {"id": "1107.5850", "submitter": "Ibrahim Saygin Topkaya", "authors": "Ibrahim Saygin Topkaya and Hakan Erdogan", "title": "Confidence-Based Dynamic Classifier Combination For Mean-Shift Tracking", "comments": "This paper has been withdrawn by the author due to an implementation\n  issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel tracking technique which uses dynamic confidence-based\nfusion of two different information sources for robust and efficient tracking\nof visual objects. Mean-shift tracking is a popular and well known method used\nin object tracking problems. Originally, the algorithm uses a similarity\nmeasure which is optimized by shifting a search area to the center of a\ngenerated weight image to track objects. Recent improvements on the original\nmean-shift algorithm involves using a classifier that differentiates the object\nfrom its surroundings. We adopt this classifier-based approach and propose an\napplication of a classifier fusion technique within this classifier-based\ncontext in this work. We use two different classifiers, where one comes from a\nbackground modeling method, to generate the weight image and we calculate\ncontributions of the classifiers dynamically using their confidences to\ngenerate a final weight image to be used in tracking. The contributions of the\nclassifiers are calculated by using correlations between histograms of their\nweight images and histogram of a defined ideal weight image in the previous\nframe. We show with experiments that our dynamic combination scheme selects\ngood contributions for classifiers for different cases and improves tracking\naccuracy significantly.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 01:08:52 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 15:35:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Topkaya", "Ibrahim Saygin", ""], ["Erdogan", "Hakan", ""]]}]