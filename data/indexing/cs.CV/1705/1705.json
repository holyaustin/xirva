[{"id": "1705.00002", "submitter": "Danil Kuzin", "authors": "Danil Kuzin, Olga Isupova, Lyudmila Mihaylova", "title": "Compressive Sensing Approaches for Autonomous Object Detection in Video\n  Sequences", "comments": "SDF 2015", "journal-ref": null, "doi": "10.1109/SDF.2015.7347706", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video analytics requires operating with large amounts of data. Compressive\nsensing allows to reduce the number of measurements required to represent the\nvideo using the prior knowledge of sparsity of the original signal, but it\nimposes certain conditions on the design matrix. The Bayesian compressive\nsensing approach relaxes the limitations of the conventional approach using the\nprobabilistic reasoning and allows to include different prior knowledge about\nthe signal structure. This paper presents two Bayesian compressive sensing\nmethods for autonomous object detection in a video sequence from a static\ncamera. Their performance is compared on the real datasets with the\nnon-Bayesian greedy algorithm. It is shown that the Bayesian methods can\nprovide the same accuracy as the greedy algorithm but much faster; or if the\ncomputational time is not critical they can provide more accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 20:24:33 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kuzin", "Danil", ""], ["Isupova", "Olga", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1705.00027", "submitter": "Jo\\~ao Carvalho", "authors": "Jo\\~ao Carvalho, Manuel Marques, Jo\\~ao P. Costeira", "title": "Understanding People Flow in Transportation Hubs", "comments": "10 pages, 19 figure, 1 table", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems ( Volume:\n  19 , Issue: 10 , Oct. 2018 )", "doi": "10.1109/TITS.2017.2775285", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to monitor the flow of people in large public\ninfrastructures. We propose an unsupervised methodology to cluster people flow\npatterns into the most typical and meaningful configurations. By processing 3D\nimages from a network of depth cameras, we build a descriptor for the flow\npattern. We define a data-irregularity measure that assesses how well each\ndescriptor fits a data model. This allows us to rank flow patterns from highly\ndistinctive (outliers) to very common ones. By discarding outliers, we obtain\nmore reliable key configurations (classes). Synthetic experiments show that the\nproposed method is superior to standard clustering methods. We applied it in an\noperational scenario during 14 days in the X-ray screening area of an\ninternational airport. Results show that our methodology is able to\nsuccessfully summarize the representative patterns for such a long observation\nperiod, providing relevant information for airport management. Beyond regular\nflows, our method identifies a set of rare events corresponding to uncommon\nactivities (cleaning, special security and circulating staff).\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 18:24:38 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 16:48:58 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Carvalho", "Jo\u00e3o", ""], ["Marques", "Manuel", ""], ["Costeira", "Jo\u00e3o P.", ""]]}, {"id": "1705.00034", "submitter": "Sara Bahaadini", "authors": "Sara Bahaadini, Neda Rohani, Scott Coughlin, Michael Zevin, Vicky\n  Kalogera, and Aggelos K Katsaggelos", "title": "Deep Multi-view Models for Glitch Classification", "comments": "Accepted to the 42nd IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-cosmic, non-Gaussian disturbances known as \"glitches\", show up in\ngravitational-wave data of the Advanced Laser Interferometer Gravitational-wave\nObservatory, or aLIGO. In this paper, we propose a deep multi-view\nconvolutional neural network to classify glitches automatically. The primary\npurpose of classifying glitches is to understand their characteristics and\norigin, which facilitates their removal from the data or from the detector\nentirely. We visualize glitches as spectrograms and leverage the\nstate-of-the-art image classification techniques in our model. The suggested\nclassifier is a multi-view deep neural network that exploits four different\nviews for classification. The experimental results demonstrate that the\nproposed model improves the overall accuracy of the classification compared to\ntraditional single view algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 18:45:57 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Bahaadini", "Sara", ""], ["Rohani", "Neda", ""], ["Coughlin", "Scott", ""], ["Zevin", "Michael", ""], ["Kalogera", "Vicky", ""], ["Katsaggelos", "Aggelos K", ""]]}, {"id": "1705.00053", "submitter": "Jacob Walker", "authors": "Jacob Walker, Kenneth Marino, Abhinav Gupta, Martial Hebert", "title": "The Pose Knows: Video Forecasting by Generating Pose Futures", "comments": "Project Website: http://www.cs.cmu.edu/~jcwalker/POS/POS.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches in video forecasting attempt to generate videos directly\nin pixel space using Generative Adversarial Networks (GANs) or Variational\nAutoencoders (VAEs). However, since these approaches try to model all the\nstructure and scene dynamics at once, in unconstrained settings they often\ngenerate uninterpretable results. Our insight is to model the forecasting\nproblem at a higher level of abstraction. Specifically, we exploit human pose\ndetectors as a free source of supervision and break the video forecasting\nproblem into two discrete steps. First we explicitly model the high level\nstructure of active objects in the scene---humans---and use a VAE to model the\npossible future movements of humans in the pose space. We then use the future\nposes generated as conditional information to a GAN to predict the future\nframes of the video in pixel space. By using the structured space of pose as an\nintermediate representation, we sidestep the problems that GANs have in\ngenerating video pixels directly. We show through quantitative and qualitative\nevaluation that our method outperforms state-of-the-art methods for video\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 19:46:47 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Walker", "Jacob", ""], ["Marino", "Kenneth", ""], ["Gupta", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1705.00086", "submitter": "Jihua Zhu", "authors": "Minmin Xu, Siyu Xu, Jihua Zhu, Yaochen Li, Jun Wang, Huimin Lu", "title": "Effective scaling registration approach by imposing the emphasis on the\n  scale factor", "comments": "22 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes an effective approach for the scaling registration of\n$m$-D point sets. Different from the rigid transformation, the scaling\nregistration can not be formulated into the common least square function due to\nthe ill-posed problem caused by the scale factor. Therefore, this paper designs\na novel objective function for the scaling registration problem. The appearance\nof this objective function is a rational fraction, where the numerator item is\nthe least square error and the denominator item is the square of the scale\nfactor. By imposing the emphasis on scale factor, the ill-posed problem can be\navoided in the scaling registration. Subsequently, the new objective function\ncan be solved by the proposed scaling iterative closest point (ICP) algorithm,\nwhich can obtain the optimal scaling transformation. For the practical\napplications, the scaling ICP algorithm is further extended to align partially\noverlapping point sets. Finally, the proposed approach is tested on public data\nsets and applied to merging grid maps of different resolutions. Experimental\nresults demonstrate its superiority over previous approaches on efficiency and\nrobustness.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 22:21:47 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 12:02:24 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Xu", "Minmin", ""], ["Xu", "Siyu", ""], ["Zhu", "Jihua", ""], ["Li", "Yaochen", ""], ["Wang", "Jun", ""], ["Lu", "Huimin", ""]]}, {"id": "1705.00268", "submitter": "Amin Zheng", "authors": "Amin Zheng and Gene Cheung and Dinei Florencio", "title": "Joint Denoising / Compression of Image Contours via Shape Prior and\n  Context Tree", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2816818", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of depth sensing technologies, the extraction of object\ncontours in images---a common and important pre-processing step for later\nhigher-level computer vision tasks like object detection and human action\nrecognition---has become easier. However, acquisition noise in captured depth\nimages means that detected contours suffer from unavoidable errors. In this\npaper, we propose to jointly denoise and compress detected contours in an image\nfor bandwidth-constrained transmission to a client, who can then carry out\naforementioned application-specific tasks using the decoded contours as input.\nWe first prove theoretically that in general a joint denoising / compression\napproach can outperform a separate two-stage approach that first denoises then\nencodes contours lossily. Adopting a joint approach, we first propose a burst\nerror model that models typical errors encountered in an observed string y of\ndirectional edges. We then formulate a rate-constrained maximum a posteriori\n(MAP) problem that trades off the posterior probability p(x'|y) of an estimated\nstring x' given y with its code rate R(x'). We design a dynamic programming\n(DP) algorithm that solves the posed problem optimally, and propose a compact\ncontext representation called total suffix tree (TST) that can reduce\ncomplexity of the algorithm dramatically. Experimental results show that our\njoint denoising / compression scheme outperformed a competing separate scheme\nin rate-distortion performance noticeably.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 04:27:07 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zheng", "Amin", ""], ["Cheung", "Gene", ""], ["Florencio", "Dinei", ""]]}, {"id": "1705.00274", "submitter": "Sibel Tari", "authors": "Asli Genctav, Yusuf Sahillioglu, and Sibel Tari", "title": "Topologically Robust 3D Shape Matching via Gradual Deflation and\n  Inflation", "comments": "Section 2 replaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being vastly ignored in the literature, coping with topological noise\nis an issue of increasing importance, especially as a consequence of the\nincreasing number and diversity of 3D polygonal models that are captured by\ndevices of different qualities or synthesized by algorithms of different\nstabilities. One approach for matching 3D shapes under topological noise is to\nreplace the topology-sensitive geodesic distance with distances that are less\nsensitive to topological changes. We propose an alternative approach utilising\ngradual deflation (or inflation) of the shape volume, of which purpose is to\nbring the pair of shapes to be matched to a \\emph{comparable} topology before\nthe search for correspondences. Illustrative experiments using different\ndatasets demonstrate that as the level of topological noise increases, our\napproach outperforms the other methods in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 06:40:18 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 21:48:29 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Genctav", "Asli", ""], ["Sahillioglu", "Yusuf", ""], ["Tari", "Sibel", ""]]}, {"id": "1705.00279", "submitter": "Luanzheng Guo", "authors": "Luanzheng Guo, Jun Chu", "title": "Indoor Frame Recovery from Refined Line Segments", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important yet challenging problem in understanding indoor scene is\nrecovering indoor frame structure from a monocular image. It is more difficult\nwhen occlusions and illumination vary, and object boundaries are weak. To\novercome these difficulties, a new approach based on line segment refinement\nwith two constraints is proposed. First, the line segments are refined by four\nconsecutive operations, i.e., reclassifying, connecting, fitting, and voting.\nSpecifically, misclassified line segments are revised by the reclassifying\noperation, some short line segments are joined by the connecting operation, the\nundetected key line segments are recovered by the fitting operation with the\nhelp of the vanishing points, the line segments converging on the frame are\nselected by the voting operation. Second, we construct four frame models\naccording to four classes of possible shooting angles of the monocular image,\nthe natures of all frame models are introduced via enforcing the cross ratio\nand depth constraints. The indoor frame is then constructed by fitting those\nrefined line segments with related frame model under the two constraints, which\njointly advance the accuracy of the frame. Experimental results on a collection\nof over 300 indoor images indicate that our algorithm has the capability of\nrecovering the frame from complex indoor scenes.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 07:16:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Guo", "Luanzheng", ""], ["Chu", "Jun", ""]]}, {"id": "1705.00301", "submitter": "Marei Algarni Mr.", "authors": "Marei Algarni and Ganesh Sundaramoorthi", "title": "SurfCut: Surfaces of Minimal Paths From Topological Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SurfCut, an algorithm for extracting a smooth, simple surface with\nan unknown 3D curve boundary from a noisy 3D image and a seed point. Our method\nis built on the novel observation that certain ridge curves of a function\ndefined on a front propagated using the Fast Marching algorithm lie on the\nsurface. Our method extracts and cuts these ridges to form the surface\nboundary. Our surface extraction algorithm is built on the novel observation\nthat the surface lies in a valley of the distance from Fast Marching. We show\nthat the resulting surface is a collection of minimal paths. Using the\nframework of cubical complexes and Morse theory, we design algorithms to\nextract these critical structures robustly. Experiments on three 3D datasets\nshow the robustness of our method, and that it achieves higher accuracy with\nlower computational cost than state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 11:56:51 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Algarni", "Marei", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1705.00322", "submitter": "Zaidao Wen", "authors": "Zaidao Wen, Biao Hou, Licheng Jiao", "title": "Discriminative Nonlinear Analysis Operator Learning: When Cosparse Model\n  Meets Image Classification", "comments": "IEEE TIP Accepted", "journal-ref": null, "doi": "10.1109/TIP.2017.2700761", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear synthesis model based dictionary learning framework has achieved\nremarkable performances in image classification in the last decade. Behaved as\na generative feature model, it however suffers from some intrinsic\ndeficiencies. In this paper, we propose a novel parametric nonlinear analysis\ncosparse model (NACM) with which a unique feature vector will be much more\nefficiently extracted. Additionally, we derive a deep insight to demonstrate\nthat NACM is capable of simultaneously learning the task adapted feature\ntransformation and regularization to encode our preferences, domain prior\nknowledge and task oriented supervised information into the features. The\nproposed NACM is devoted to the classification task as a discriminative feature\nmodel and yield a novel discriminative nonlinear analysis operator learning\nframework (DNAOL). The theoretical analysis and experimental performances\nclearly demonstrate that DNAOL will not only achieve the better or at least\ncompetitive classification accuracies than the state-of-the-art algorithms but\nit can also dramatically reduce the time complexities in both training and\ntesting phases.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:09:22 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Wen", "Zaidao", ""], ["Hou", "Biao", ""], ["Jiao", "Licheng", ""]]}, {"id": "1705.00346", "submitter": "Andre Luckow", "authors": "Andre Luckow and Matthew Cook and Nathan Ashcraft and Edwin Weill and\n  Emil Djerekarov and Bennie Vorster", "title": "Deep Learning in the Automotive Industry: Applications and Tools", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/BigData.2016.7841045", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning refers to a set of machine learning techniques that utilize\nneural networks with many hidden layers for tasks, such as image\nclassification, speech recognition, language understanding. Deep learning has\nbeen proven to be very effective in these domains and is pervasively used by\nmany Internet services. In this paper, we describe different automotive uses\ncases for deep learning in particular in the domain of computer vision. We\nsurveys the current state-of-the-art in libraries, tools and infrastructures\n(e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural\nnetworks. We particularly focus on convolutional neural networks and computer\nvision use cases, such as the visual inspection process in manufacturing plants\nand the analysis of social media data. To train neural networks, curated and\nlabeled datasets are essential. In particular, both the availability and scope\nof such datasets is typically very limited. A main contribution of this paper\nis the creation of an automotive dataset, that allows us to learn and\nautomatically recognize different vehicle properties. We describe an end-to-end\ndeep learning application utilizing a mobile app for data collection and\nprocess support, and an Amazon-based cloud backend for storage and training.\nFor training we evaluate the use of cloud and on-premises infrastructures\n(including multiple GPUs) in conjunction with different neural network\narchitectures and frameworks. We assess both the training times as well as the\naccuracy of the classifier. Finally, we demonstrate the effectiveness of the\ntrained classifier in a real world setting during manufacturing process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 17:17:44 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Luckow", "Andre", ""], ["Cook", "Matthew", ""], ["Ashcraft", "Nathan", ""], ["Weill", "Edwin", ""], ["Djerekarov", "Emil", ""], ["Vorster", "Bennie", ""]]}, {"id": "1705.00360", "submitter": "Xuebin Qin", "authors": "Xuebin Qin, Shida He, Camilo Perez Quintero, Abhineet Singh, Masood\n  Dehghan and Martin Jagersand", "title": "Real-Time Salient Closed Boundary Tracking via Line Segments Perceptual\n  Grouping", "comments": "7 pages, 8 figures, The 2017 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2017) submission ID 1034", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel real-time method for tracking salient closed\nboundaries from video image sequences. This method operates on a set of\nstraight line segments that are produced by line detection. The tracking scheme\nis coherently integrated into a perceptual grouping framework in which the\nvisual tracking problem is tackled by identifying a subset of these line\nsegments and connecting them sequentially to form a closed boundary with the\nlargest saliency and a certain similarity to the previous one. Specifically, we\ndefine a new tracking criterion which combines a grouping cost and an area\nsimilarity constraint. The proposed criterion makes the resulting boundary\ntracking more robust to local minima. To achieve real-time tracking\nperformance, we use Delaunay Triangulation to build a graph model with the\ndetected line segments and then reduce the tracking problem to finding the\noptimal cycle in this graph. This is solved by our newly proposed closed\nboundary candidates searching algorithm called \"Bidirectional Shortest Path\n(BDSP)\". The efficiency and robustness of the proposed method are tested on\nreal video sequences as well as during a robot arm pouring experiment.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 19:01:07 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 23:44:36 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Qin", "Xuebin", ""], ["He", "Shida", ""], ["Quintero", "Camilo Perez", ""], ["Singh", "Abhineet", ""], ["Dehghan", "Masood", ""], ["Jagersand", "Martin", ""]]}, {"id": "1705.00366", "submitter": "Danna Gurari", "authors": "Danna Gurari and Kun He and Bo Xiong and Jianming Zhang and Mehrnoosh\n  Sameki and Suyog Dutt Jain and Stan Sclaroff and Margrit Betke and Kristen\n  Grauman", "title": "Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the\n  Segmentation(s)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the ambiguity problem for the foreground object segmentation task\nand motivate the importance of estimating and accounting for this ambiguity\nwhen designing vision systems. Specifically, we distinguish between images\nwhich lead multiple annotators to segment different foreground objects\n(ambiguous) versus minor inter-annotator differences of the same object. Taking\nimages from eight widely used datasets, we crowdsource labeling the images as\n\"ambiguous\" or \"not ambiguous\" to segment in order to construct a new dataset\nwe call STATIC. Using STATIC, we develop a system that automatically predicts\nwhich images are ambiguous. Experiments demonstrate the advantage of our\nprediction system over existing saliency-based methods on images from vision\nbenchmarks and images taken by blind people who are trying to recognize objects\nin their environment. Finally, we introduce a crowdsourcing system to achieve\ncost savings for collecting the diversity of all valid \"ground truth\"\nforeground object segmentations by collecting extra segmentations only when\nambiguity is expected. Experiments show our system eliminates up to 47% of\nhuman effort compared to existing crowdsourcing methods with no loss in\ncapturing the diversity of ground truths.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 19:27:30 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Gurari", "Danna", ""], ["He", "Kun", ""], ["Xiong", "Bo", ""], ["Zhang", "Jianming", ""], ["Sameki", "Mehrnoosh", ""], ["Jain", "Suyog Dutt", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""], ["Grauman", "Kristen", ""]]}, {"id": "1705.00389", "submitter": "Chunhua Shen", "authors": "Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang", "title": "Adversarial PoseNet: A Structure-aware Convolutional Network for Human\n  Pose Estimation", "comments": "Fixed typos. 14 pages. Demonstration videos are\n  http://v.qq.com/x/page/c039862eira.html,\n  http://v.qq.com/x/page/f0398zcvkl5.html,\n  http://v.qq.com/x/page/w0398ei9m1r.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human pose estimation in monocular images, joint occlusions and\noverlapping upon human bodies often result in deviated pose predictions. Under\nthese circumstances, biologically implausible pose predictions may be produced.\nIn contrast, human vision is able to predict poses by exploiting geometric\nconstraints of joint inter-connectivity. To address the problem by\nincorporating priors about the structure of human bodies, we propose a novel\nstructure-aware convolutional network to implicitly take such priors into\naccount during training of the deep network. Explicit learning of such\nconstraints is typically challenging. Instead, we design discriminators to\ndistinguish the real poses from the fake ones (such as biologically implausible\nones). If the pose generator (G) generates results that the discriminator fails\nto distinguish from real ones, the network successfully learns the priors.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 23:54:43 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 03:54:39 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Chen", "Yu", ""], ["Shen", "Chunhua", ""], ["Wei", "Xiu-Shen", ""], ["Liu", "Lingqiao", ""], ["Yang", "Jian", ""]]}, {"id": "1705.00393", "submitter": "Aaron Nech", "authors": "Aaron Nech, Ira Kemelmacher-Shlizerman", "title": "Level Playing Field for Million Scale Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has the perception of a solved problem, however when tested\nat the million-scale exhibits dramatic variation in accuracies across the\ndifferent algorithms. Are the algorithms very different? Is access to good/big\ntraining data their secret weapon? Where should face recognition improve? To\naddress those questions, we created a benchmark, MF2, that requires all\nalgorithms to be trained on same data, and tested at the million scale. MF2 is\na public large-scale set with 672K identities and 4.7M photos created with the\ngoal to level playing field for large scale face recognition. We contrast our\nresults with findings from the other two large-scale benchmarks MegaFace\nChallenge and MS-Celebs-1M where groups were allowed to train on any\nprivate/public/big/small set. Some key discoveries: 1) algorithms, trained on\nMF2, were able to achieve state of the art and comparable results to algorithms\ntrained on massive private sets, 2) some outperformed themselves once trained\non MF2, 3) invariance to aging suffers from low accuracies as in MegaFace,\nidentifying the need for larger age variations possibly within identities or\nadjustment of algorithms in future testings.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:04:53 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Nech", "Aaron", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1705.00430", "submitter": "Hassan Foroosh", "authors": "Vildan Atalay Aydin and Hassan Foroosh", "title": "Sub-Pixel Registration of Wavelet-Encoded Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sub-pixel registration is a crucial step for applications such as\nsuper-resolution in remote sensing, motion compensation in magnetic resonance\nimaging, and non-destructive testing in manufacturing, to name a few. Recently,\nthese technologies have been trending towards wavelet encoded imaging and\nsparse/compressive sensing. The former plays a crucial role in reducing imaging\nartifacts, while the latter significantly increases the acquisition speed. In\nview of these new emerging needs for applications of wavelet encoded imaging,\nwe propose a sub-pixel registration method that can achieve direct wavelet\ndomain registration from a sparse set of coefficients. We make the following\ncontributions: (i) We devise a method of decoupling scale, rotation, and\ntranslation parameters in the Haar wavelet domain, (ii) We derive explicit\nmathematical expressions that define in-band sub-pixel registration in terms of\nwavelet coefficients, (iii) Using the derived expressions, we propose an\napproach to achieve in-band subpixel registration, avoiding back and forth\ntransformations. (iv) Our solution remains highly accurate even when a sparse\nset of coefficients are used, which is due to localization of signals in a\nsparse set of wavelet coefficients. We demonstrate the accuracy of our method,\nand show that it outperforms the state-of-the-art on simulated and real data,\neven when the data is sparse.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 07:27:04 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Aydin", "Vildan Atalay", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.00432", "submitter": "Akshay Pai", "authors": "Akshay Pai, Stefan Sommer, Lars Lau Raket, Line K\\\"uhnel, Sune\n  Darkner, Lauge S{\\o}rensen, Mads Nielsen", "title": "A Statistical Model for Simultaneous Template Estimation, Bias\n  Correction, and Registration of 3D Brain Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template estimation plays a crucial role in computational anatomy since it\nprovides reference frames for performing statistical analysis of the underlying\nanatomical population variability. While building models for template\nestimation, variability in sites and image acquisition protocols need to be\naccounted for. To account for such variability, we propose a generative\ntemplate estimation model that makes simultaneous inference of both bias fields\nin individual images, deformations for image registration, and variance\nhyperparameters. In contrast, existing maximum a posterori based methods need\nto rely on either bias-invariant similarity measures or robust image\nnormalization. Results on synthetic and real brain MRI images demonstrate the\ncapability of the model to capture heterogeneity in intensities and provide a\nreliable template estimation from registration.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 07:31:45 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Pai", "Akshay", ""], ["Sommer", "Stefan", ""], ["Raket", "Lars Lau", ""], ["K\u00fchnel", "Line", ""], ["Darkner", "Sune", ""], ["S\u00f8rensen", "Lauge", ""], ["Nielsen", "Mads", ""]]}, {"id": "1705.00451", "submitter": "Ziyi Liu", "authors": "Ziyi Liu, Siyu Yu, Xiao Wang and Nanning Zheng", "title": "Detecting Drivable Area for Self-driving Cars: An Unsupervised Approach", "comments": "6 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been well recognized that detecting drivable area is central to\nself-driving cars. Most of existing methods attempt to locate road surface by\nusing lane line, thereby restricting to drivable area on which have a clear\nlane mark. This paper proposes an unsupervised approach for detecting drivable\narea utilizing both image data from a monocular camera and point cloud data\nfrom a 3D-LIDAR scanner. Our approach locates initial drivable areas based on a\n\"direction ray map\" obtained by image-LIDAR data fusion. Besides, a fusion of\nthe feature level is also applied for more robust performance. Once the initial\ndrivable areas are described by different features, the feature fusion problem\nis formulated as a Markov network and a belief propagation algorithm is\ndeveloped to perform the model inference. Our approach is unsupervised and\navoids common hypothesis, yet gets state-of-the-art results on ROAD-KITTI\nbenchmark. Experiments show that our unsupervised approach is efficient and\nrobust for detecting drivable area for self-driving cars.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 09:14:29 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Liu", "Ziyi", ""], ["Yu", "Siyu", ""], ["Wang", "Xiao", ""], ["Zheng", "Nanning", ""]]}, {"id": "1705.00463", "submitter": "Jackie Ma", "authors": "Jackie Ma and Maximilian M\\\"arz and Stephanie Funk and Jeanette\n  Schulz-Menger and Gitta Kutyniok and Tobias Schaeffter and Christoph\n  Kolbitsch", "title": "Shearlet-based compressed sensing for fast 3D cardiac MR imaging using\n  iterative reweighting", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/aaea04", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution three-dimensional (3D) cardiovascular magnetic resonance\n(CMR) is a valuable medical imaging technique, but its widespread application\nin clinical practice is hampered by long acquisition times. Here we present a\nnovel compressed sensing (CS) reconstruction approach using shearlets as a\nsparsifying transform allowing for fast 3D CMR (3DShearCS). Shearlets are\nmathematically optimal for a simplified model of natural images and have been\nproven to be more efficient than classical systems such as wavelets. Data is\nacquired with a 3D Radial Phase Encoding (RPE) trajectory and an iterative\nreweighting scheme is used during image reconstruction to ensure fast\nconvergence and high image quality. In our in-vivo cardiac MRI experiments we\nshow that the proposed method 3DShearCS has lower relative errors and higher\nstructural similarity compared to the other reconstruction techniques\nespecially for high undersampling factors, i.e. short scan times. In this\npaper, we further show that 3DShearCS provides improved depiction of cardiac\nanatomy (measured by assessing the sharpness of coronary arteries) and two\nclinical experts qualitatively analyzed the image quality.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 10:43:17 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Ma", "Jackie", ""], ["M\u00e4rz", "Maximilian", ""], ["Funk", "Stephanie", ""], ["Schulz-Menger", "Jeanette", ""], ["Kutyniok", "Gitta", ""], ["Schaeffter", "Tobias", ""], ["Kolbitsch", "Christoph", ""]]}, {"id": "1705.00464", "submitter": "Ted Zhang", "authors": "Ted Zhang, Dengxin Dai, Tinne Tuytelaars, Marie-Francine Moens, Luc\n  Van Gool", "title": "Speech-Based Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces speech-based visual question answering (VQA), the task\nof generating an answer given an image and a spoken question. Two methods are\nstudied: an end-to-end, deep neural network that directly uses audio waveforms\nas input versus a pipelined approach that performs ASR (Automatic Speech\nRecognition) on the question, followed by text-based visual question answering.\nFurthermore, we investigate the robustness of both methods by injecting various\nlevels of noise into the spoken question and find both methods to be tolerate\nnoise at similar levels.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 10:43:28 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 03:43:20 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Zhang", "Ted", ""], ["Dai", "Dengxin", ""], ["Tuytelaars", "Tinne", ""], ["Moens", "Marie-Francine", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.00487", "submitter": "Marcel Simon", "authors": "Marcel Simon, Yang Gao, Trevor Darrell, Joachim Denzler, Erik Rodner", "title": "Generalized orderless pooling performs implicit salient matching", "comments": "Published at International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent CNN architectures use average pooling as a final feature encoding\nstep. In the field of fine-grained recognition, however, recent global\nrepresentations like bilinear pooling offer improved performance. In this\npaper, we generalize average and bilinear pooling to \"alpha-pooling\", allowing\nfor learning the pooling strategy during training. In addition, we present a\nnovel way to visualize decisions made by these approaches. We identify parts of\ntraining images having the highest influence on the prediction of a given test\nimage. It allows for justifying decisions to users and also for analyzing the\ninfluence of semantic parts. For example, we can show that the higher capacity\nVGG16 model focuses much more on the bird's head than, e.g., the lower-capacity\nVGG-M model when recognizing fine-grained bird categories. Both contributions\nallow us to analyze the difference when moving between average and bilinear\npooling. In addition, experiments show that our generalized approach can\noutperform both across a variety of standard datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 12:00:49 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:57:38 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 13:59:36 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Simon", "Marcel", ""], ["Gao", "Yang", ""], ["Darrell", "Trevor", ""], ["Denzler", "Joachim", ""], ["Rodner", "Erik", ""]]}, {"id": "1705.00522", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary\n  learning approach", "comments": "To be presented at SPARS 2017, Lisbon, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Residual Quantization (RQ) framework is revisited where the quantization\ndistortion is being successively reduced in multi-layers. Inspired by the\nreverse-water-filling paradigm in rate-distortion theory, an efficient\nregularization on the variances of the codewords is introduced which allows to\nextend the RQ for very large numbers of layers and also for high dimensional\ndata, without getting over-trained. The proposed Regularized Residual\nQuantization (RRQ) results in multi-layer dictionaries which are additionally\nsparse, thanks to the soft-thresholding nature of the regularization when\napplied to variance-decaying data which can arise from de-correlating\ntransformations applied to correlated data. Furthermore, we also propose a\ngeneral-purpose pre-processing for natural images which makes them suitable for\nsuch quantization. The RRQ framework is first tested on synthetic\nvariance-decaying data to show its efficiency in quantization of\nhigh-dimensional data. Next, we use the RRQ in super-resolution of a database\nof facial images where it is shown that low-resolution facial images from the\ntest set quantized with codebooks trained on high-resolution images from the\ntraining set show relevant high-frequency content when reconstructed with those\ncodebooks.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 13:59:04 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1705.00534", "submitter": "Bo Li", "authors": "Bo Li, Yuchao Dai, Huahui Chen, Mingyi He", "title": "Single image depth estimation by dilated deep residual convolutional\n  neural network and soft-weight-sum inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new residual convolutional neural network (CNN)\narchitecture for single image depth estimation. Compared with existing deep CNN\nbased methods, our method achieves much better results with fewer training\nexamples and model parameters. The advantages of our method come from the usage\nof dilated convolution, skip connection architecture and soft-weight-sum\ninference. Experimental evaluation on the NYU Depth V2 dataset shows that our\nmethod outperforms other state-of-the-art methods by a margin.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:07:05 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Li", "Bo", ""], ["Dai", "Yuchao", ""], ["Chen", "Huahui", ""], ["He", "Mingyi", ""]]}, {"id": "1705.00540", "submitter": "Ayan Chaudhury", "authors": "Ayan Chaudhury, Christopher Ward, Ali Talasaz, Alexander G. Ivanov,\n  Mark Brophy, Bernard Grodzinski, Norman P.A. Huner, Rajni V. Patel and John\n  L. Barron", "title": "Machine Vision System for 3D Plant Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision for plant phenotyping is an emerging research area for\nproducing high throughput in agriculture and crop science applications. Since\n2D based approaches have their inherent limitations, 3D plant analysis is\nbecoming state of the art for current phenotyping technologies. We present an\nautomated system for analyzing plant growth in indoor conditions. A gantry\nrobot system is used to perform scanning tasks in an automated manner\nthroughout the lifetime of the plant. A 3D laser scanner mounted as the robot's\npayload captures the surface point cloud data of the plant from multiple views.\nThe plant is monitored from the vegetative to reproductive stages in light/dark\ncycles inside a controllable growth chamber. An efficient 3D reconstruction\nalgorithm is used, by which multiple scans are aligned together to obtain a 3D\nmesh of the plant, followed by surface area and volume computations. The whole\nsystem, including the programmable growth chamber, robot, scanner, data\ntransfer and analysis is fully automated in such a way that a naive user can,\nin theory, start the system with a mouse click and get back the growth analysis\nresults at the end of the lifetime of the plant with no intermediate\nintervention. As evidence of its functionality, we show and analyze\nquantitative results of the rhythmic growth patterns of the dicot Arabidopsis\nthaliana(L.), and the monocot barley (Hordeum vulgare L.) plants under their\ndiurnal light/dark cycles.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 01:02:12 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Chaudhury", "Ayan", ""], ["Ward", "Christopher", ""], ["Talasaz", "Ali", ""], ["Ivanov", "Alexander G.", ""], ["Brophy", "Mark", ""], ["Grodzinski", "Bernard", ""], ["Huner", "Norman P. A.", ""], ["Patel", "Rajni V.", ""], ["Barron", "John L.", ""]]}, {"id": "1705.00581", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin and Luc Van Gool", "title": "Query-adaptive Video Summarization via Quality-aware Relevance\n  Estimation", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the problem of automatic video summarization has recently received a\nlot of attention, the problem of creating a video summary that also highlights\nelements relevant to a search query has been less studied. We address this\nproblem by posing query-relevant summarization as a video frame subset\nselection problem, which lets us optimise for summaries which are\nsimultaneously diverse, representative of the entire video, and relevant to a\ntext query. We quantify relevance by measuring the distance between frames and\nqueries in a common textual-visual semantic embedding space induced by a neural\nnetwork. In addition, we extend the model to capture query-independent\nproperties, such as frame quality. We compare our method against previous state\nof the art on textual-visual embeddings for thumbnail selection and show that\nour model outperforms them on relevance prediction. Furthermore, we introduce a\nnew dataset, annotated with diversity and query-specific relevance labels. On\nthis dataset, we train and test our complete model for video summarization and\nshow that it outperforms standard baselines such as Maximal Marginal Relevance.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:28:18 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:18:56 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Gygli", "Michael", ""], ["Volokitin", "Anna", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.00601", "submitter": "Akrit Mohapatra", "authors": "Aroma Mahendru, Viraj Prabhu, Akrit Mohapatra, Dhruv Batra, Stefan Lee", "title": "The Promise of Premise: Harnessing Question Premises in Visual Question\n  Answering", "comments": "Published at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we make a simple observation that questions about images often\ncontain premises - objects and relationships implied by the question - and that\nreasoning about premises can help Visual Question Answering (VQA) models\nrespond more intelligently to irrelevant or previously unseen questions. When\npresented with a question that is irrelevant to an image, state-of-the-art VQA\nmodels will still answer purely based on learned language biases, resulting in\nnon-sensical or even misleading answers. We note that a visual question is\nirrelevant to an image if at least one of its premises is false (i.e. not\ndepicted in the image). We leverage this observation to construct a dataset for\nQuestion Relevance Prediction and Explanation (QRPE) by searching for false\npremises. We train novel question relevance detection models and show that\nmodels that reason about premises consistently outperform models that do not.\nWe also find that forcing standard VQA models to reason about premises during\ntraining can lead to improvements on tasks requiring compositional reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:41:37 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 18:12:18 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mahendru", "Aroma", ""], ["Prabhu", "Viraj", ""], ["Mohapatra", "Akrit", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "1705.00604", "submitter": "Joel Brogan Joel R Brogan", "authors": "Joel Brogan, Paolo Bestagini, Aparna Bharati, Allan Pinto, Daniel\n  Moreira, Kevin Bowyer, Patrick Flynn, Anderson Rocha, and Walter Scheirer", "title": "Spotting the Difference: Context Retrieval and Analysis for Improved\n  Forgery Detection and Localization", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As image tampering becomes ever more sophisticated and commonplace, the need\nfor image forensics algorithms that can accurately and quickly detect forgeries\ngrows. In this paper, we revisit the ideas of image querying and retrieval to\nprovide clues to better localize forgeries. We propose a method to perform\nlarge-scale image forensics on the order of one million images using the help\nof an image search algorithm and database to gather contextual clues as to\nwhere tampering may have taken place. In this vein, we introduce five new\nstrongly invariant image comparison methods and test their effectiveness under\nheavy noise, rotation, and color space changes. Lastly, we show the\neffectiveness of these methods compared to passive image forensics using Nimble\n[https://www.nist.gov/itl/iad/mig/nimble-challenge], a new, state-of-the-art\ndataset from the National Institute of Standards and Technology (NIST).\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:43:49 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Brogan", "Joel", ""], ["Bestagini", "Paolo", ""], ["Bharati", "Aparna", ""], ["Pinto", "Allan", ""], ["Moreira", "Daniel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter", ""]]}, {"id": "1705.00609", "submitter": "Hongliang Yan", "authors": "Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, Wangmeng\n  Zuo", "title": "Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for\n  Unsupervised Domain Adaptation", "comments": "10 pages, 5 figures, accepted by CVPR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted\nas a discrepancy metric between the distributions of source and target domains.\nHowever, existing MMD-based domain adaptation methods generally ignore the\nchanges of class prior distributions, i.e., class weight bias across domains.\nThis remains an open problem but ubiquitous for domain adaptation, which can be\ncaused by changes in sample selection criteria and application scenarios. We\nshow that MMD cannot account for class weight bias and results in degraded\ndomain adaptation performance. To address this issue, a weighted MMD model is\nproposed in this paper. Specifically, we introduce class-specific auxiliary\nweights into the original MMD for exploiting the class prior probability on\nsource and target domains, whose challenge lies in the fact that the class\nlabel in target domain is unavailable. To account for it, our proposed weighted\nMMD model is defined by introducing an auxiliary weight for each class in the\nsource domain, and a classification EM algorithm is suggested by alternating\nbetween assigning the pseudo-labels, estimating auxiliary weights and updating\nmodel parameters. Extensive experiments demonstrate the superiority of our\nweighted MMD over conventional MMD for domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:54:53 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yan", "Hongliang", ""], ["Ding", "Yukang", ""], ["Li", "Peihua", ""], ["Wang", "Qilong", ""], ["Xu", "Yong", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1705.00664", "submitter": "Ryutaro Tanno", "authors": "Ryutaro Tanno, Daniel E. Worrall, Aurobrata Ghosh, Enrico Kaden,\n  Stamatios N. Sotiropoulos, Antonio Criminisi, Daniel C. Alexander", "title": "Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI\n  Super-Resolution", "comments": "Accepted paper at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the value of uncertainty modeling in 3D\nsuper-resolution with convolutional neural networks (CNNs). Deep learning has\nshown success in a plethora of medical image transformation problems, such as\nsuper-resolution (SR) and image synthesis. However, the highly ill-posed nature\nof such problems results in inevitable ambiguity in the learning of networks.\nWe propose to account for intrinsic uncertainty through a per-patch\nheteroscedastic noise model and for parameter uncertainty through approximate\nBayesian inference in the form of variational dropout. We show that the\ncombined benefits of both lead to the state-of-the-art performance SR of\ndiffusion MR brain images in terms of errors compared to ground truth. We\nfurther show that the reduced error scores produce tangible benefits in\ndownstream tractography. In addition, the probabilistic nature of the methods\nnaturally confers a mechanism to quantify uncertainty over the super-resolved\noutput. We demonstrate through experiments on both healthy and pathological\nbrains the potential utility of such an uncertainty measure in the risk\nassessment of the super-resolved images for subsequent clinical use.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:56:22 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 09:37:57 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Tanno", "Ryutaro", ""], ["Worrall", "Daniel E.", ""], ["Ghosh", "Aurobrata", ""], ["Kaden", "Enrico", ""], ["Sotiropoulos", "Stamatios N.", ""], ["Criminisi", "Antonio", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1705.00678", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "comments": "Published in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many similarity-based clustering methods work in two separate steps including\nsimilarity matrix computation and subsequent spectral clustering. However,\nsimilarity measurement is challenging because it is usually impacted by many\nfactors, e.g., the choice of similarity metric, neighborhood size, scale of\ndata, noise and outliers. Thus the learned similarity matrix is often not\nsuitable, let alone optimal, for the subsequent clustering. In addition,\nnonlinear similarity often exists in many real world data which, however, has\nnot been effectively considered by most existing methods. To tackle these two\nchallenges, we propose a model to simultaneously learn cluster indicator matrix\nand similarity information in kernel spaces in a principled way. We show\ntheoretical relationships to kernel k-means, k-means, and spectral clustering\nmethods. Then, to address the practical issue of how to select the most\nsuitable kernel for a particular clustering task, we further extend our model\nwith a multiple kernel learning ability. With this joint model, we can\nautomatically accomplish three subtasks of finding the best cluster indicator\nmatrix, the most accurate similarity relations and the optimal combination of\nmultiple kernels. By leveraging the interactions between these three subtasks\nin a joint framework, each subtask can be iteratively boosted by using the\nresults of the others towards an overall optimal solution. Extensive\nexperiments are performed to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:33:27 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 00:29:13 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1705.00703", "submitter": "Mike Roberts", "authors": "Mike Roberts, Debadeepta Dey, Anh Truong, Sudipta Sinha, Shital Shah,\n  Ashish Kapoor, Pat Hanrahan, Neel Joshi", "title": "Submodular Trajectory Optimization for Aerial 3D Scanning", "comments": "Accepted for publication at the International Conference on Computer\n  Vision (ICCV) 2017; Supplementary video:\n  http://www.youtube.com/watch?v=89fFmfVZSO8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones equipped with cameras are emerging as a powerful tool for large-scale\naerial 3D scanning, but existing automatic flight planners do not exploit all\navailable information about the scene, and can therefore produce inaccurate and\nincomplete 3D models. We present an automatic method to generate drone\ntrajectories, such that the imagery acquired during the flight will later\nproduce a high-fidelity 3D model. Our method uses a coarse estimate of the\nscene geometry to plan camera trajectories that: (1) cover the scene as\nthoroughly as possible; (2) encourage observations of scene geometry from a\ndiverse set of viewing angles; (3) avoid obstacles; and (4) respect a\nuser-specified flight time budget. Our method relies on a mathematical model of\nscene coverage that exhibits an intuitive diminishing returns property known as\nsubmodularity. We leverage this property extensively to design a trajectory\nplanning algorithm that reasons globally about the non-additive coverage reward\nobtained across a trajectory, jointly with the cost of traveling between views.\nWe evaluate our method by using it to scan three large outdoor scenes, and we\nperform a quantitative evaluation using a photorealistic video game simulator.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 20:32:43 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 07:42:58 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 05:22:24 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Roberts", "Mike", ""], ["Dey", "Debadeepta", ""], ["Truong", "Anh", ""], ["Sinha", "Sudipta", ""], ["Shah", "Shital", ""], ["Kapoor", "Ashish", ""], ["Hanrahan", "Pat", ""], ["Joshi", "Neel", ""]]}, {"id": "1705.00727", "submitter": "John Paisley", "authors": "Xiangyong Cao, Feng Zhou, Lin Xu, Deyu Meng, Zongben Xu, John Paisley", "title": "Hyperspectral Image Classification with Markov Random Fields and a\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2799324", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new supervised classification algorithm for remotely\nsensed hyperspectral image (HSI) which integrates spectral and spatial\ninformation in a unified Bayesian framework. First, we formulate the HSI\nclassification problem from a Bayesian perspective. Then, we adopt a\nconvolutional neural network (CNN) to learn the posterior class distributions\nusing a patch-wise training strategy to better use the spatial information.\nNext, spatial information is further considered by placing a spatial smoothness\nprior on the labels. Finally, we iteratively update the CNN parameters using\nstochastic gradient decent (SGD) and update the class labels of all pixel\nvectors using an alpha-expansion min-cut-based algorithm. Compared with other\nstate-of-the-art methods, the proposed classification method achieves better\nperformance on one synthetic dataset and two benchmark HSI datasets in a number\nof experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 22:15:11 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 15:48:49 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Cao", "Xiangyong", ""], ["Zhou", "Feng", ""], ["Xu", "Lin", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""], ["Paisley", "John", ""]]}, {"id": "1705.00744", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan,\n  Baoxin Li", "title": "A Strategy for an Uncompromising Incremental Learner", "comments": "Under review at IEEE Transactions of Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class supervised learning systems require the knowledge of the entire\nrange of labels they predict. Often when learnt incrementally, they suffer from\ncatastrophic forgetting. To avoid this, generous leeways have to be made to the\nphilosophy of incremental learning that either forces a part of the machine to\nnot learn, or to retrain the machine again with a selection of the historic\ndata. While these hacks work to various degrees, they do not adhere to the\nspirit of incremental learning. In this article, we redefine incremental\nlearning with stringent conditions that do not allow for any undesirable\nrelaxations and assumptions. We design a strategy involving generative models\nand the distillation of dark knowledge as a means of hallucinating data along\nwith appropriate targets from past distributions. We call this technique,\nphantom sampling.We show that phantom sampling helps avoid catastrophic\nforgetting during incremental learning. Using an implementation based on deep\nneural networks, we demonstrate that phantom sampling dramatically avoids\ncatastrophic forgetting. We apply these strategies to competitive multi-class\nincremental learning of deep neural networks. Using various benchmark datasets\nand through our strategy, we demonstrate that strict incremental learning could\nbe achieved. We further put our strategy to test on challenging cases,\nincluding cross-domain increments and incrementing on a novel label space. We\nalso propose a trivial extension to unbounded-continual learning and identify\npotential for future development.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 00:17:54 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 07:30:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Venkateswara", "Hemanth", ""], ["Panchanathan", "Sethuraman", ""], ["Li", "Baoxin", ""]]}, {"id": "1705.00754", "submitter": "Ranjay Krishna", "authors": "Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos\n  Niebles", "title": "Dense-Captioning Events in Videos", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most natural videos contain numerous events. For example, in a video of a\n\"man playing a piano\", the video might also contain \"another man dancing\" or \"a\ncrowd clapping\". We introduce the task of dense-captioning events, which\ninvolves both detecting and describing events in a video. We propose a new\nmodel that is able to identify all events in a single pass of the video while\nsimultaneously describing the detected events with natural language. Our model\nintroduces a variant of an existing proposal module that is designed to capture\nboth short as well as long events that span minutes. To capture the\ndependencies between the events in a video, our model introduces a new\ncaptioning module that uses contextual information from past and future events\nto jointly describe all events. We also introduce ActivityNet Captions, a\nlarge-scale benchmark for dense-captioning events. ActivityNet Captions\ncontains 20k videos amounting to 849 video hours with 100k total descriptions,\neach with it's unique start and end time. Finally, we report performances of\nour model for dense-captioning events, video retrieval and localization.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 01:21:58 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Krishna", "Ranjay", ""], ["Hata", "Kenji", ""], ["Ren", "Frederic", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1705.00771", "submitter": "Yehui Yang", "authors": "Yehui Yang, Tao Li, Wensi Li, Haishan Wu, Wei Fan, Wensheng Zhang", "title": "Lesion detection and Grading of Diabetic Retinopathy via Two-stages Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic diabetic retinopathy (DR) analysis algorithm based on\ntwo-stages deep convolutional neural networks (DCNN). Compared to existing\nDCNN-based DR detection methods, the proposed algorithm have the following\nadvantages: (1) Our method can point out the location and type of lesions in\nthe fundus images, as well as giving the severity grades of DR. Moreover, since\nretina lesions and DR severity appear with different scales in fundus images,\nthe integration of both local and global networks learn more complete and\nspecific features for DR analysis. (2) By introducing imbalanced weighting map,\nmore attentions will be given to lesion patches for DR grading, which\nsignificantly improve the performance of the proposed algorithm. In this study,\nwe label 12,206 lesion patches and re-annotate the DR grades of 23,595 fundus\nimages from Kaggle competition dataset. Under the guidance of clinical\nophthalmologists, the experimental results show that our local lesion detection\nnet achieve comparable performance with trained human observers, and the\nproposed imbalanced weighted scheme also be proved to significantly improve the\ncapability of our DCNN-based DR grading algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 02:44:39 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yang", "Yehui", ""], ["Li", "Tao", ""], ["Li", "Wensi", ""], ["Wu", "Haishan", ""], ["Fan", "Wei", ""], ["Zhang", "Wensheng", ""]]}, {"id": "1705.00794", "submitter": "Jino P J", "authors": "Jino P J, Kannan Balakrishnan", "title": "Offline Handwritten Recognition of Malayalam District Name - A Holistic\n  Approach", "comments": "8 pages, IJET 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various machine learning methods for writer independent recognition of\nMalayalam handwritten district names are discussed in this paper. Data\ncollected from 56 different writers are used for the experiments. The proposed\nwork can be used for the recognition of district in the address written in\nMalayalam. Different methods for Dimensionality reduction are discussed.\nFeatures consider for the recognition are Histogram of Oriented Gradient\ndescriptor, Number of Black Pixels in the upper half and lower half, length of\nimage. Classifiers used in this work are Neural Network, SVM and RandomForest.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 04:51:47 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["J", "Jino P", ""], ["Balakrishnan", "Kannan", ""]]}, {"id": "1705.00821", "submitter": "Naushad Ansari", "authors": "Naushad Ansari and Anubha Gupta", "title": "Statistical learning of rational wavelet transform for natural images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated with the concept of transform learning and the utility of rational\nwavelet transform in audio and speech processing, this paper proposes Rational\nWavelet Transform Learning in Statistical sense (RWLS) for natural images. The\nproposed RWLS design is carried out via lifting framework and is shown to have\na closed form solution. The efficacy of the learned transform is demonstrated\nin the application of compressed sensing (CS) based reconstruction. The learned\nRWLS is observed to perform better than the existing standard dyadic wavelet\ntransforms.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 06:51:20 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ansari", "Naushad", ""], ["Gupta", "Anubha", ""]]}, {"id": "1705.00823", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Yutaro Shigeto, Akikazu Takeuchi", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption\n  Dataset", "comments": "Accepted as ACL2017 short paper. 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, automatic generation of image descriptions (captions), that\nis, image captioning, has attracted a great deal of attention. In this paper,\nwe particularly consider generating Japanese captions for images. Since most\navailable caption datasets have been constructed for English language, there\nare few datasets for Japanese. To tackle this problem, we construct a\nlarge-scale Japanese image caption dataset based on images from MS-COCO, which\nis called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions\nfor 164,062 images. In the experiment, we show that a neural network trained\nusing STAIR Captions can generate more natural and better Japanese captions,\ncompared to those generated using English-Japanese machine translation after\ngenerating English captions.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:07:55 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Shigeto", "Yutaro", ""], ["Takeuchi", "Akikazu", ""]]}, {"id": "1705.00835", "submitter": "Pichao Wang", "authors": "Zewei Ding and Pichao Wang and Philip O. Ogunbona and Wanqing Li", "title": "Investigation of Different Skeleton Features for CNN-based 3D Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques are being used in skeleton based action recognition\ntasks and outstanding performance has been reported. Compared with RNN based\nmethods which tend to overemphasize temporal information, CNN-based approaches\ncan jointly capture spatio-temporal information from texture color images\nencoded from skeleton sequences. There are several skeleton-based features that\nhave proven effective in RNN-based and handcrafted-feature-based methods.\nHowever, it remains unknown whether they are suitable for CNN-based approaches.\nThis paper proposes to encode five spatial skeleton features into images with\ndifferent encoding methods. In addition, the performance implication of\ndifferent joints used for feature extraction is studied. The proposed method\nachieved state-of-the-art performance on NTU RGB+D dataset for 3D human action\nanalysis. An accuracy of 75.32\\% was achieved in Large Scale 3D Human Activity\nAnalysis Challenge in Depth Videos.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:42:35 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ding", "Zewei", ""], ["Wang", "Pichao", ""], ["Ogunbona", "Philip O.", ""], ["Li", "Wanqing", ""]]}, {"id": "1705.00873", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Parthipan Siva, Tao Xiang", "title": "Transfer Learning by Ranking for Weakly Supervised Object Annotation", "comments": "BMVC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to training object detectors rely on fully\nsupervised learning, which requires the tedious manual annotation of object\nlocation in a training set. Recently there has been an increasing interest in\ndeveloping weakly supervised approach to detector training where the object\nlocation is not manually annotated but automatically determined based on binary\n(weak) labels indicating if a training image contains the object. This is a\nchallenging problem because each image can contain many candidate object\nlocations which partially overlaps the object of interest. Existing approaches\nfocus on how to best utilise the binary labels for object location annotation.\nIn this paper we propose to solve this problem from a very different\nperspective by casting it as a transfer learning problem. Specifically, we\nformulate a novel transfer learning based on learning to rank, which\neffectively transfers a model for automatic annotation of object location from\nan auxiliary dataset to a target dataset with completely unrelated object\ncategories. We show that our approach outperforms existing state-of-the-art\nweakly supervised approach to annotating objects in the challenging VOC\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 09:23:27 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Siva", "Parthipan", ""], ["Xiang", "Tao", ""]]}, {"id": "1705.00930", "submitter": "Tseng-Hung Chen", "authors": "Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting Hsu,\n  Jianlong Fu, Min Sun", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image\n  Captioner", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive image captioning results are achieved in domains with plenty of\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\ntarget domain with significant domain shifts but no paired training data\n(referred to as cross-domain image captioning) remains largely unexplored. We\npropose a novel adversarial training procedure to leverage unpaired data in the\ntarget domain. Two critic networks are introduced to guide the captioner,\nnamely domain critic and multi-modal critic. The domain critic assesses whether\nthe generated sentences are indistinguishable from sentences in the target\ndomain. The multi-modal critic assesses whether an image and its generated\nsentence are a valid pair. During training, the critics and captioner act as\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\ncritics aim at distinguishing them. The assessment improves the captioner\nthrough policy gradient updates. During inference, we further propose a novel\ncritic-based planning method to select high-quality sentences without\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\nas the target domains. Our method consistently performs well on all datasets.\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\nadaptation. Utilizing critics during inference further gives another 4.5%\nboost.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 12:06:54 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:54:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Tseng-Hung", ""], ["Liao", "Yuan-Hong", ""], ["Chuang", "Ching-Yao", ""], ["Hsu", "Wan-Ting", ""], ["Fu", "Jianlong", ""], ["Sun", "Min", ""]]}, {"id": "1705.00938", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Sailesh Conjeti, Debdoot Sheet, Amin Katouzian,\n  Nassir Navab, Christian Wachinger", "title": "Error Corrective Boosting for Learning Fully Convolutional Networks with\n  Limited Data", "comments": "Accepted at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep fully convolutional neural networks (F-CNNs) for semantic image\nsegmentation requires access to abundant labeled data. While large datasets of\nunlabeled image data are available in medical applications, access to manually\nlabeled data is very limited. We propose to automatically create auxiliary\nlabels on initially unlabeled data with existing tools and to use them for\npre-training. For the subsequent fine-tuning of the network with manually\nlabeled data, we introduce error corrective boosting (ECB), which emphasizes\nparameter updates on classes with lower accuracy. Furthermore, we introduce\nSkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that\ncombines skip connections with the unpooling strategy for upsampling. The\nSD-Net addresses challenges of severe class imbalance and errors along\nboundaries. With application to whole-brain MRI T1 scan segmentation, we\ngenerate auxiliary labels on a large dataset with FreeSurfer and fine-tune on\ntwo datasets with manual annotations. Our results show that the inclusion of\nauxiliary labels and ECB yields significant improvements. SD-Net segments a 3D\nscan in 7 secs in comparison to 30 hours for the closest multi-atlas\nsegmentation method, while reaching similar performance. It also outperforms\nthe latest state-of-the-art F-CNN models.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 12:28:24 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 07:58:41 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Conjeti", "Sailesh", ""], ["Sheet", "Debdoot", ""], ["Katouzian", "Amin", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1705.00949", "submitter": "Christian Mostegel", "authors": "Christian Mostegel and Rudolf Prettenthaler and Friedrich Fraundorfer\n  and Horst Bischof", "title": "Scalable Surface Reconstruction from Point Clouds with Extreme Scale and\n  Density Diversity", "comments": "This paper was accepted to the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2017. The copyright was transfered to IEEE\n  (ieee.org). The official version of the paper will be made available on IEEE\n  Xplore (R) (ieeexplore.ieee.org). This version of the paper also contains the\n  supplementary material, which will not appear IEEE Xplore (R)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a scalable approach for robustly computing a 3D\nsurface mesh from multi-scale multi-view stereo point clouds that can handle\nextreme jumps of point density (in our experiments three orders of magnitude).\nThe backbone of our approach is a combination of octree data partitioning,\nlocal Delaunay tetrahedralization and graph cut optimization. Graph cut\noptimization is used twice, once to extract surface hypotheses from local\nDelaunay tetrahedralizations and once to merge overlapping surface hypotheses\neven when the local tetrahedralizations do not share the same topology.This\nformulation allows us to obtain a constant memory consumption per sub-problem\nwhile at the same time retaining the density independent interpolation\nproperties of the Delaunay-based optimization. On multiple public datasets, we\ndemonstrate that our approach is highly competitive with the state-of-the-art\nin terms of accuracy, completeness and outlier resilience. Further, we\ndemonstrate the multi-scale potential of our approach by processing a newly\nrecorded dataset with 2 billion points and a point density variation of more\nthan four orders of magnitude - requiring less than 9GB of RAM per process.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:13:47 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mostegel", "Christian", ""], ["Prettenthaler", "Rudolf", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1705.01010", "submitter": "Rui Huang", "authors": "Rui Huang (1,3), Danping Zou (2), Richard Vaughan (1), Ping Tan (1)\n  ((1) Simon Fraser University, (2) Shanghai Jiao Tong University, (3) Alibaba\n  AI Labs)", "title": "Active Image-based Modeling with a Toy Drone", "comments": "To be published on International Conference on Robotics and\n  Automation 2018, Brisbane, Australia. Project Page:\n  https://huangrui815.github.io/active-image-based-modeling/ The author's\n  personal page: http://www.sfu.ca/~rha55/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based modeling techniques can now generate photo-realistic 3D models\nfrom images. But it is up to users to provide high quality images with good\ncoverage and view overlap, which makes the data capturing process tedious and\ntime consuming. We seek to automate data capturing for image-based modeling.\nThe core of our system is an iterative linear method to solve the multi-view\nstereo (MVS) problem quickly and plan the Next-Best-View (NBV) effectively. Our\nfast MVS algorithm enables online model reconstruction and quality assessment\nto determine the NBVs on the fly. We test our system with a toy unmanned aerial\nvehicle (UAV) in simulated, indoor and outdoor experiments. Results show that\nour system improves the efficiency of data acquisition and ensures the\ncompleteness of the final model.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:06:36 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 10:31:15 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 09:52:14 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Huang", "Rui", ""], ["Zou", "Danping", ""], ["Vaughan", "Richard", ""], ["Tan", "Ping", ""]]}, {"id": "1705.01013", "submitter": "Wen Jiang", "authors": "Zichang He and Wen Jiang", "title": "Quantum Mechanical Approach to Modelling Reliability of Sensor Reports", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer evidence theory is wildly applied in multi-sensor data\nfusion. However, lots of uncertainty and interference exist in practical\nsituation, especially in the battle field. It is still an open issue to model\nthe reliability of sensor reports. Many methods are proposed based on the\nrelationship among collected data. In this letter, we proposed a quantum\nmechanical approach to evaluate the reliability of sensor reports, which is\nbased on the properties of a sensor itself. The proposed method is used to\nmodify the combining of evidences.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 01:22:15 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["He", "Zichang", ""], ["Jiang", "Wen", ""]]}, {"id": "1705.01088", "submitter": "Jing Liao", "authors": "Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, Sing Bing Kang", "title": "Visual Attribute Transfer through Deep Image Analogy", "comments": "Accepted by SIGGRAPH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique for visual attribute transfer across images that\nmay have very different appearance but have perceptually similar semantic\nstructure. By visual attribute transfer, we mean transfer of visual information\n(such as color, tone, texture, and style) from one image to another. For\nexample, one image could be that of a painting or a sketch while the other is a\nphoto of a real scene, and both depict the same type of scene.\n  Our technique finds semantically-meaningful dense correspondences between two\ninput images. To accomplish this, it adapts the notion of \"image analogy\" with\nfeatures extracted from a Deep Convolutional Neutral Network for matching; we\ncall our technique Deep Image Analogy. A coarse-to-fine strategy is used to\ncompute the nearest-neighbor field for generating the results. We validate the\neffectiveness of our proposed method in a variety of cases, including\nstyle/texture transfer, color/style swap, sketch/painting to photo, and time\nlapse.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 17:44:01 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 15:16:19 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Liao", "Jing", ""], ["Yao", "Yuan", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1705.01148", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw K{\\l}opotek", "title": "Recovery of structure of looped jointed objects from multiframes", "comments": null, "journal-ref": "a preliminary version for Machine Graphics and Vision, Vol. 3 No.\n  4, pp. 645-656, 1995", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method to recover structural parameters of looped jointed objects from\nmultiframes is being developed. Each rigid part of the jointed body needs only\nto be traced at two (that is at junction) points.\n  This method has been linearized for 4-part loops, with recovery from at least\n19 frames.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:21:51 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw", ""]]}, {"id": "1705.01152", "submitter": "Zijian Wang", "authors": "Eric Cristofalo and Zijian Wang", "title": "Out-of-focus: Learning Depth from Image Bokeh for Robotic Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we propose a novel approach for estimating depth from RGB\nimages. Traditionally, most work uses a single RGB image to estimate depth,\nwhich is inherently difficult and generally results in poor performance, even\nwith thousands of data examples. In this work, we alternatively use multiple\nRGB images that were captured while changing the focus of the camera's lens.\nThis method leverages the natural depth information correlated to the different\npatterns of clarity/blur in the sequence of focal images, which helps\ndistinguish objects at different depths. Since no such data set exists for\nlearning this mapping, we collect our own data set using customized hardware.\nWe then use a convolutional neural network for learning the depth from the\nstacked focal images. Comparative studies were conducted on both a standard\nRGBD data set and our own data set (learning from both single and multiple\nimages), and results verified that stacked focal images yield better depth\nestimation than using just single RGB image.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:30:51 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Cristofalo", "Eric", ""], ["Wang", "Zijian", ""]]}, {"id": "1705.01156", "submitter": "Balazs Kovacs", "authors": "Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala", "title": "Shading Annotations in the Wild", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding shading effects in images is critical for a variety of vision\nand graphics problems, including intrinsic image decomposition, shadow removal,\nimage relighting, and inverse rendering. As is the case with other vision\ntasks, machine learning is a promising approach to understanding shading - but\nthere is little ground truth shading data available for real-world images. We\nintroduce Shading Annotations in the Wild (SAW), a new large-scale, public\ndataset of shading annotations in indoor scenes, comprised of multiple forms of\nshading judgments obtained via crowdsourcing, along with shading annotations\nautomatically generated from RGB-D imagery. We use this data to train a\nconvolutional neural network to predict per-pixel shading information in an\nimage. We demonstrate the value of our data and network in an application to\nintrinsic images, where we can reduce decomposition artifacts produced by\nexisting algorithms. Our database is available at\nhttp://opensurfaces.cs.cornell.edu/saw/.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:54:31 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kovacs", "Balazs", ""], ["Bell", "Sean", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""]]}, {"id": "1705.01180", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Zhenheng Yang, Ram Nevatia", "title": "Cascaded Boundary Regression for Temporal Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection in long videos is an important problem.\nState-of-the-art methods address this problem by applying action classifiers on\nsliding windows. Although sliding windows may contain an identifiable portion\nof the actions, they may not necessarily cover the entire action instance,\nwhich would lead to inferior performance. We adapt a two-stage temporal action\ndetection pipeline with Cascaded Boundary Regression (CBR) model.\nClass-agnostic proposals and specific actions are detected respectively in the\nfirst and the second stage. CBR uses temporal coordinate regression to refine\nthe temporal boundaries of the sliding windows. The salient aspect of the\nrefinement process is that, inside each stage, the temporal boundaries are\nadjusted in a cascaded way by feeding the refined windows back to the system\nfor further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and\nachieve state-of-the-art performance on both datasets. The performance gain is\nespecially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14\nis improved from 19.0% to 31.0%.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 21:45:21 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Gao", "Jiyang", ""], ["Yang", "Zhenheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "1705.01217", "submitter": "Zheng Cao", "authors": "Zheng Cao, Shujian Yu, Bing Ouyang, Fraser Dalgleish, Anni\n  Vuorenkoski, Gabriel Alsenas, Jose Principe", "title": "Marine Animal Classification with Correntropy Loss Based Multi-view\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze marine animals behavior, seasonal distribution and abundance,\ndigital imagery can be acquired by visual or Lidar camera. Depending on the\nquantity and properties of acquired imagery, the animals are characterized as\neither features (shape, color, texture, etc.), or dissimilarity matrices\nderived from different shape analysis methods (shape context, internal distance\nshape context, etc.). For both cases, multi-view learning is critical in\nintegrating more than one set of feature or dissimilarity matrix for higher\nclassification accuracy. This paper adopts correntropy loss as cost function in\nmulti-view learning, which has favorable statistical properties for rejecting\nnoise. For the case of features, the correntropy loss-based multi-view learning\nand its entrywise variation are developed based on the multi-view intact space\nlearning algorithm. For the case of dissimilarity matrices, the robust\nEuclidean embedding algorithm is extended to its multi-view form with the\ncorrentropy loss function. Results from simulated data and real-world marine\nanimal imagery show that the proposed algorithms can effectively enhance\nclassification rate, as well as suppress noise under different noise\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:26:24 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Cao", "Zheng", ""], ["Yu", "Shujian", ""], ["Ouyang", "Bing", ""], ["Dalgleish", "Fraser", ""], ["Vuorenkoski", "Anni", ""], ["Alsenas", "Gabriel", ""], ["Principe", "Jose", ""]]}, {"id": "1705.01247", "submitter": "Jian Xu", "authors": "Jian Xu, Cunzhao Shi, Chengzuo Qi, Chunheng Wang, Baihua Xiao", "title": "Unsupervised Part-based Weighting Aggregation of Deep Convolutional\n  Features for Image Retrieval", "comments": "8 pages, 4 figures. Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple but effective semantic part-based\nweighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the\ndiscriminative filters of deep convolutional layers as part detectors.\nMoreover, we propose the effective unsupervised strategy to select some part\ndetectors to generate the \"probabilistic proposals\", which highlight certain\ndiscriminative parts of objects and suppress the noise of background. The final\nglobal PWA representation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. We conduct comprehensive experiments\non four standard datasets and show that our unsupervised PWA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods. Code is\navailable at https://github.com/XJhaoren/PWA.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 03:54:02 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 02:33:03 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 08:53:36 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Xu", "Jian", ""], ["Shi", "Cunzhao", ""], ["Qi", "Chengzuo", ""], ["Wang", "Chunheng", ""], ["Xiao", "Baihua", ""]]}, {"id": "1705.01253", "submitter": "Hongyang Xue", "authors": "Hongyang Xue, Zhou Zhao, Deng Cai", "title": "The Forgettable-Watcher Model for Video Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of visual question answering approaches have been proposed recently,\naiming at understanding the visual scenes by answering the natural language\nquestions. While the image question answering has drawn significant attention,\nvideo question answering is largely unexplored.\n  Video-QA is different from Image-QA since the information and the events are\nscattered among multiple frames. In order to better utilize the temporal\nstructure of the videos and the phrasal structures of the answers, we propose\ntwo mechanisms: the re-watching and the re-reading mechanisms and combine them\ninto the forgettable-watcher model. Then we propose a TGIF-QA dataset for video\nquestion answering with the help of automatic question generation. Finally, we\nevaluate the models on our dataset. The experimental results show the\neffectiveness of our proposed models.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 04:46:33 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Xue", "Hongyang", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""]]}, {"id": "1705.01258", "submitter": "Hassan Foroosh", "authors": "Vildan Atalay Aydin and Hassan Foroosh", "title": "Super-Resolution of Wavelet-Encoded Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview super-resolution image reconstruction (SRIR) is often cast as a\nresampling problem by merging non-redundant data from multiple low-resolution\n(LR) images on a finer high-resolution (HR) grid, while inverting the effect of\nthe camera point spread function (PSF). One main problem with multiview methods\nis that resampling from nonuniform samples (provided by LR images) and the\ninversion of the PSF are highly nonlinear and ill-posed problems. Non-linearity\nand ill-posedness are typically overcome by linearization and regularization,\noften through an iterative optimization process, which essentially trade off\nthe very same information (i.e. high frequency) that we want to recover. We\npropose a novel point of view for multiview SRIR: Unlike existing multiview\nmethods that reconstruct the entire spectrum of the HR image from the multiple\ngiven LR images, we derive explicit expressions that show how the\nhigh-frequency spectra of the unknown HR image are related to the spectra of\nthe LR images. Therefore, by taking any of the LR images as the reference to\nrepresent the low-frequency spectra of the HR image, one can reconstruct the\nsuper-resolution image by focusing only on the reconstruction of the\nhigh-frequency spectra. This is very much like single-image methods, which\nextrapolate the spectrum of one image, except that we rely on information\nprovided by all other views, rather than by prior constraints as in\nsingle-image methods (which may not be an accurate source of information). This\nis made possible by deriving and applying explicit closed-form expressions that\ndefine how the local high frequency information that we aim to recover for the\nreference high resolution image is related to the local low frequency\ninformation in the sequence of views. Results and comparisons with recently\npublished state-of-the-art methods show the superiority of the proposed\nsolution.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 05:42:14 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Aydin", "Vildan Atalay", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.01262", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Learning to segment with image-level supervision", "comments": "Published in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have achieved the state-of-the-art for semantic\nimage segmentation tasks. However, training these networks requires access to\ndensely labeled images, which are known to be very expensive to obtain. On the\nother hand, the web provides an almost unlimited source of images annotated at\nthe image level. How can one utilize this much larger weakly annotated set for\ntasks that require dense labeling? Prior work often relied on localization\ncues, such as saliency maps, objectness priors, bounding boxes etc., to address\nthis challenging problem. In this paper, we propose a model that generates\nauxiliary labels for each image, while simultaneously forcing the output of the\nCNN to satisfy the mean-field constraints imposed by a conditional random\nfield. We show that one can enforce the CRF constraints by forcing the\ndistribution at each pixel to be close to the distribution of its neighbors.\nThis is in stark contrast with methods that compute a recursive expansion of\nthe mean-field distribution using a recurrent architecture and train the\nresultant distribution. Instead, the proposed model adds an extra loss term to\nthe output of the CNN, and hence, is faster than recursive implementations. We\nachieve the state-of-the-art for weakly supervised semantic image segmentation\non VOC 2012 dataset, assuming no manually labeled pixel level information is\navailable. Furthermore, the incorporation of conditional random fields in CNN\nincurs little extra time during training.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 06:02:32 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 17:56:16 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1705.01314", "submitter": "Yu-Ying Yeh", "authors": "Yen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Sheng-De Wang, Wei-Chen\n  Chiu, Yu-Chiang Frank Wang", "title": "Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation", "comments": "CVPR 2018 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While representation learning aims to derive interpretable features for\ndescribing visual data, representation disentanglement further results in such\nfeatures so that particular image attributes can be identified and manipulated.\nHowever, one cannot easily address this task without observing ground truth\nannotation for the training data. To address this problem, we propose a novel\ndeep learning model of Cross-Domain Representation Disentangler (CDRD). By\nobserving fully annotated source-domain data and unlabeled target-domain data\nof interest, our model bridges the information across data domains and\ntransfers the attribute information accordingly. Thus, cross-domain joint\nfeature disentanglement and adaptation can be jointly performed. In the\nexperiments, we provide qualitative results to verify our disentanglement\ncapability. Moreover, we further confirm that our model can be applied for\nsolving classification tasks of unsupervised domain adaptation, and performs\nfavorably against state-of-the-art image disentanglement and translation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 09:04:21 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 05:09:21 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 15:40:35 GMT"}, {"version": "v4", "created": "Tue, 1 May 2018 09:00:52 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Liu", "Yen-Cheng", ""], ["Yeh", "Yu-Ying", ""], ["Fu", "Tzu-Chien", ""], ["Wang", "Sheng-De", ""], ["Chiu", "Wei-Chen", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1705.01352", "submitter": "Jonas Wulff", "authors": "Jonas Wulff, Laura Sevilla-Lara, Michael J. Black", "title": "Optical Flow in Mostly Rigid Scenes", "comments": "15 pages, 10 figures; accepted for publication at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical flow of natural scenes is a combination of the motion of the\nobserver and the independent motion of objects. Existing algorithms typically\nfocus on either recovering motion and structure under the assumption of a\npurely static world or optical flow for general unconstrained scenes. We\ncombine these approaches in an optical flow algorithm that estimates an\nexplicit segmentation of moving objects from appearance and physical\nconstraints. In static regions we take advantage of strong constraints to\njointly estimate the camera motion and the 3D structure of the scene over\nmultiple frames. This allows us to also regularize the structure instead of the\nmotion. Our formulation uses a Plane+Parallax framework, which works even under\nsmall baselines, and reduces the motion estimation to a one-dimensional search\nproblem, resulting in more accurate estimation. In moving regions the flow is\ntreated as unconstrained, and computed with an existing optical flow method.\nThe resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art\nresults on both the MPI-Sintel and KITTI-2015 benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:48:21 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Wulff", "Jonas", ""], ["Sevilla-Lara", "Laura", ""], ["Black", "Michael J.", ""]]}, {"id": "1705.01359", "submitter": "Moin Nabi", "authors": "Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot,\n  Moin Nabi, Enver Sangineto, Raffaella Bernardi", "title": "FOIL it! Find One mismatch between Image and Language caption", "comments": "To appear at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1024", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand whether current language and vision\n(LaVi) models truly grasp the interaction between the two modalities. To this\nend, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates\nimages with both correct and \"foil\" captions, that is, descriptions of the\nimage that are highly similar to the original ones, but contain one single\nmistake (\"foil word\"). We show that current LaVi models fall into the traps of\nthis data and perform badly on three tasks: a) caption classification (correct\nvs. foil); b) foil word detection; c) foil word correction. Humans, in\ncontrast, have near-perfect performance on those tasks. We demonstrate that\nmerely utilising language cues is not enough to model FOIL-COCO and that it\nchallenges the state-of-the-art by requiring a fine-grained understanding of\nthe relation between text and image.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:07:13 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shekhar", "Ravi", ""], ["Pezzelle", "Sandro", ""], ["Klimovich", "Yauhen", ""], ["Herbelot", "Aurelie", ""], ["Nabi", "Moin", ""], ["Sangineto", "Enver", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1705.01362", "submitter": "Anders Eriksson", "authors": "Anders Eriksson, Carl Olsson, Fredrik Kahl and Tat-Jun Chin", "title": "Rotation Averaging and Strong Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the role of duality principles within the problem of\nrotation averaging, a fundamental task in a wide range of computer vision\napplications. In its conventional form, rotation averaging is stated as a\nminimization over multiple rotation constraints. As these constraints are\nnon-convex, this problem is generally considered challenging to solve globally.\nWe show how to circumvent this difficulty through the use of Lagrangian\nduality. While such an approach is well-known it is normally not guaranteed to\nprovide a tight relaxation. Based on spectral graph theory, we analytically\nprove that in many cases there is no duality gap unless the noise levels are\nsevere. This allows us to obtain certifiably global solutions to a class of\nimportant non-convex problems in polynomial time.\n  We also propose an efficient, scalable algorithm that out-performs general\npurpose numerical solvers and is able to handle the large problem instances\ncommonly occurring in structure from motion settings. The potential of this\nproposed method is demonstrated on a number of different problems, consisting\nof both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:17:18 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 00:49:18 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Eriksson", "Anders", ""], ["Olsson", "Carl", ""], ["Kahl", "Fredrik", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1705.01371", "submitter": "Fanyi Xiao", "authors": "Fanyi Xiao, Leonid Sigal, Yong Jae Lee", "title": "Weakly-supervised Visual Grounding of Phrases with Linguistic Structures", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly-supervised approach that takes image-sentence pairs as\ninput and learns to visually ground (i.e., localize) arbitrary linguistic\nphrases, in the form of spatial attention masks. Specifically, the model is\ntrained with images and their associated image-level captions, without any\nexplicit region-to-phrase correspondence annotations. To this end, we introduce\nan end-to-end model which learns visual groundings of phrases with two types of\ncarefully designed loss functions. In addition to the standard discriminative\nloss, which enforces that attended image regions and phrases are consistently\nencoded, we propose a novel structural loss which makes use of the parse tree\nstructures induced by the sentences. In particular, we ensure complementarity\namong the attention masks that correspond to sibling noun phrases, and\ncompositionality of attention masks among the children and parent phrases, as\ndefined by the sentence parse tree. We validate the effectiveness of our\napproach on the Microsoft COCO and Visual Genome datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:53:33 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Xiao", "Fanyi", ""], ["Sigal", "Leonid", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1705.01389", "submitter": "Christian Zimmermann", "authors": "Christian Zimmermann and Thomas Brox", "title": "Learning to Estimate 3D Hand Pose from Single RGB Images", "comments": "Accepted to ICCV 2017. Code and dataset is released:\n  https://lmb.informatik.uni-freiburg.de/projects/hand3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-cost consumer depth cameras and deep learning have enabled reasonable 3D\nhand pose estimation from single depth images. In this paper, we present an\napproach that estimates 3D hand pose from regular RGB images. This task has far\nmore ambiguities due to the missing depth information. To this end, we propose\na deep network that learns a network-implicit 3D articulation prior. Together\nwith detected keypoints in the images, this network yields good estimates of\nthe 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic\nhand models for training the involved networks. Experiments on a variety of\ntest sets, including one on sign language recognition, demonstrate the\nfeasibility of 3D hand pose estimation on single color images.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 12:50:18 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:13:56 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 15:52:51 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zimmermann", "Christian", ""], ["Brox", "Thomas", ""]]}, {"id": "1705.01450", "submitter": "Shangzhen Luan", "authors": "Shangzhen Luan, Baochang Zhang, Chen Chen, Xianbin Cao, Jungong Han,\n  Jianzhuang Liu", "title": "Gabor Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2835143", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steerable properties dominate the design of traditional filters, e.g., Gabor\nfilters, and endow features the capability of dealing with spatial\ntransformations. However, such excellent properties have not been well explored\nin the popular deep convolutional neural networks (DCNNs). In this paper, we\npropose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor\nCNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of\ndeep learned features to the orientation and scale changes. By only\nmanipulating the basic element of DCNNs based on Gabor filters, i.e., the\nconvolution operator, GCNs can be easily implemented and are compatible with\nany popular deep learning architecture. Experimental results demonstrate the\nsuper capability of our algorithm in recognizing objects, where the scale and\nrotation changes occur frequently. The proposed GCNs have much fewer learnable\nnetwork parameters, and thus is easier to train with an end-to-end pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 14:37:55 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 13:11:51 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 01:45:41 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Luan", "Shangzhen", ""], ["Zhang", "Baochang", ""], ["Chen", "Chen", ""], ["Cao", "Xianbin", ""], ["Han", "Jungong", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "1705.01567", "submitter": "Manuel G\\\"unther", "authors": "Manuel G\\\"unther, Steve Cruz, Ethan M. Rudd, Terrance E. Boult", "title": "Toward Open-Set Face Recognition", "comments": "Accepted for Publication in CVPR 2017 Biometrics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has been conducted on both face identification and face\nverification, with greater focus on the latter. Research on face identification\nhas mostly focused on using closed-set protocols, which assume that all probe\nimages used in evaluation contain identities of subjects that are enrolled in\nthe gallery. Real systems, however, where only a fraction of probe sample\nidentities are enrolled in the gallery, cannot make this closed-set assumption.\nInstead, they must assume an open set of probe samples and be able to\nreject/ignore those that correspond to unknown identities. In this paper, we\naddress the widespread misconception that thresholding verification-like scores\nis a good way to solve the open-set face identification problem, by formulating\nan open-set face identification protocol and evaluating different strategies\nfor assessing similarity. Our open-set identification protocol is based on the\ncanonical labeled faces in the wild (LFW) dataset. Additionally to the known\nidentities, we introduce the concepts of known unknowns (known, but\nuninteresting persons) and unknown unknowns (people never seen before) to the\nbiometric community. We compare three algorithms for assessing similarity in a\ndeep feature space under an open-set protocol: thresholded verification-like\nscores, linear discriminant analysis (LDA) scores, and an extreme value machine\n(EVM) probabilities. Our findings suggest that thresholding EVM probabilities,\nwhich are open-set by design, outperforms thresholding verification-like\nscores.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 18:10:09 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 00:24:43 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["G\u00fcnther", "Manuel", ""], ["Cruz", "Steve", ""], ["Rudd", "Ethan M.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1705.01583", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin,\n  Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, Christian\n  Theobalt", "title": "VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera", "comments": "Accepted to SIGGRAPH 2017", "journal-ref": null, "doi": "10.1145/3072959.3073596", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first real-time method to capture the full global 3D skeletal\npose of a human in a stable, temporally consistent manner using a single RGB\ncamera. Our method combines a new convolutional neural network (CNN) based pose\nregressor with kinematic skeleton fitting. Our novel fully-convolutional pose\nformulation regresses 2D and 3D joint positions jointly in real time and does\nnot require tightly cropped input frames. A real-time kinematic skeleton\nfitting method uses the CNN output to yield temporally stable 3D global pose\nreconstructions on the basis of a coherent kinematic skeleton. This makes our\napproach the first monocular RGB method usable in real-time applications such\nas 3D character control---thus far, the only monocular methods for such\napplications employed specialized RGB-D cameras. Our method's accuracy is\nquantitatively on par with the best offline 3D monocular RGB pose estimation\nmethods. Our results are qualitatively comparable to, and sometimes better\nthan, results from monocular RGB-D approaches, such as the Kinect. However, we\nshow that our approach is more broadly applicable than RGB-D solutions, i.e. it\nworks for outdoor scenes, community videos, and low quality commodity RGB\ncameras.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 19:13:23 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mehta", "Dushyant", ""], ["Sridhar", "Srinath", ""], ["Sotnychenko", "Oleksandr", ""], ["Rhodin", "Helge", ""], ["Shafiei", "Mohammad", ""], ["Seidel", "Hans-Peter", ""], ["Xu", "Weipeng", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "1705.01707", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Federico Monti, Michael M. Bronstein", "title": "Generative Convolutional Networks for Latent Fingerprint Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of fingerprint recognition depends heavily on the extraction of\nminutiae points. Enhancement of the fingerprint ridge pattern is thus an\nessential pre-processing step that noticeably reduces false positive and\nnegative detection rates. A particularly challenging setting is when the\nfingerprint images are corrupted or partially missing. In this work, we apply\ngenerative convolutional networks to denoise visible minutiae and predict the\nmissing parts of the ridge pattern. The proposed enhancement approach is tested\nas a pre-processing step in combination with several standard feature\nextraction methods such as MINDTCT, followed by biometric comparison using MCC\nand BOZORTH3. We evaluate our method on several publicly available latent\nfingerprint datasets captured using different sensors.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 05:29:23 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Svoboda", "Jan", ""], ["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1705.01734", "submitter": "Berkan Demirel", "authors": "Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis", "title": "Attributes2Classname: A discriminative model for attribute-based\n  unsupervised zero-shot learning", "comments": "To appear at IEEE Int. Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for unsupervised zero-shot learning (ZSL) of\nclasses based on their names. Most existing unsupervised ZSL methods aim to\nlearn a model for directly comparing image features and class names. However,\nthis proves to be a difficult task due to dominance of non-visual semantics in\nunderlying vector-space embeddings of class names. To address this issue, we\ndiscriminatively learn a word representation such that the similarities between\nclass and combination of attribute names fall in line with the visual\nsimilarity. Contrary to the traditional zero-shot learning approaches that are\nbuilt upon attribute presence, our approach bypasses the laborious\nattribute-class relation annotations for unseen classes. In addition, our\nproposed approach renders text-only training possible, hence, the training can\nbe augmented without the need to collect additional image data. The\nexperimental results show that our method yields state-of-the-art results for\nunsupervised ZSL in three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:28:44 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 21:51:45 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Demirel", "Berkan", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1705.01759", "submitter": "Hou-Ning Hu", "authors": "Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju\n  Chang, Min Sun", "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360{\\deg}\n  Sports Video", "comments": "13 pages, 8 figures, To appear in CVPR 2017 as an Oral paper. The\n  first two authors contributed equally to this work.\n  https://aliensunmin.github.io/project/360video/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching a 360{\\deg} sports video requires a viewer to continuously select a\nviewing angle, either through a sequence of mouse clicks or head movements. To\nrelieve the viewer from this \"360 piloting\" task, we propose \"deep 360 pilot\"\n-- a deep learning-based agent for piloting through 360{\\deg} sports videos\nautomatically. At each frame, the agent observes a panoramic image and has the\nknowledge of previously selected viewing angles. The task of the agent is to\nshift the current viewing angle (i.e. action) to the next preferred one (i.e.,\ngoal). We propose to directly learn an online policy of the agent from data. We\nuse the policy gradient technique to jointly train our pipeline: by minimizing\n(1) a regression loss measuring the distance between the selected and ground\ntruth viewing angles, (2) a smoothness loss encouraging smooth transition in\nviewing angle, and (3) maximizing an expected reward of focusing on a\nforeground object. To evaluate our method, we build a new 360-Sports video\ndataset consisting of five sports domains. We train domain-specific agents and\nachieve the best performance on viewing angle selection accuracy and transition\nsmoothness compared to [51] and other baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 09:26:58 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Hu", "Hou-Ning", ""], ["Lin", "Yen-Chen", ""], ["Liu", "Ming-Yu", ""], ["Cheng", "Hsien-Tzu", ""], ["Chang", "Yung-Ju", ""], ["Sun", "Min", ""]]}, {"id": "1705.01781", "submitter": "Tiberio Uricchio", "authors": "Federico Becattini, Tiberio Uricchio, Lorenzo Seidenari, Lamberto\n  Ballan, Alberto Del Bimbo", "title": "Am I Done? Predicting Action Progress in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the problem of predicting action progress in\nvideos. We argue that this is an extremely important task since it can be\nvaluable for a wide range of interaction applications. To this end we introduce\na novel approach, named ProgressNet, capable of predicting when an action takes\nplace in a video, where it is located within the frames, and how far it has\nprogressed during its execution. To provide a general definition of action\nprogress, we ground our work in the linguistics literature, borrowing terms and\nconcepts to understand which actions can be the subject of progress estimation.\nAs a result, we define a categorization of actions and their phases. Motivated\nby the recent success obtained from the interaction of Convolutional and\nRecurrent Neural Networks, our model is based on a combination of the Faster\nR-CNN framework, to make frame-wise predictions, and LSTM networks, to estimate\naction progress through time. After introducing two evaluation protocols for\nthe task at hand, we demonstrate the capability of our model to effectively\npredict action progress on the UCF-101 and J-HMDB datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 10:28:21 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 19:11:15 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 13:48:02 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 01:43:40 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Becattini", "Federico", ""], ["Uricchio", "Tiberio", ""], ["Seidenari", "Lorenzo", ""], ["Ballan", "Lamberto", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1705.01782", "submitter": "Li Liu", "authors": "Yang Long, Li Liu, Ling Shao, Fumin Shen, Guiguang Ding, Jungong Han", "title": "From Zero-shot Learning to Conventional Supervised Classification:\n  Unseen Visual Data Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object recognition systems usually rely on powerful feature extraction\nmechanisms from a large number of real images. However, in many realistic\napplications, collecting sufficient images for ever-growing new classes is\nunattainable. In this paper, we propose a new Zero-shot learning (ZSL)\nframework that can synthesise visual features for unseen classes without\nacquiring real images. Using the proposed Unseen Visual Data Synthesis (UVDS)\nalgorithm, semantic attributes are effectively utilised as an intermediate clue\nto synthesise unseen visual features at the training stage. Hereafter, ZSL\nrecognition is converted into the conventional supervised problem, i.e. the\nsynthesised visual features can be straightforwardly fed to typical classifiers\nsuch as SVM. On four benchmark datasets, we demonstrate the benefit of using\nsynthesised unseen data. Extensive experimental results suggest that our\nproposed approach significantly improve the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 10:28:37 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Long", "Yang", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""], ["Shen", "Fumin", ""], ["Ding", "Guiguang", ""], ["Han", "Jungong", ""]]}, {"id": "1705.01809", "submitter": "Parth Sane", "authors": "Parth Sane, Ravindra Agrawal", "title": "Pixel Normalization from Numeric Data as Input to Neural Networks", "comments": "IEEE WiSPNET 2017 conference in Chennai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text to image transformation for input to neural networks requires\nintermediate steps. This paper attempts to present a new approach to pixel\nnormalization so as to convert textual data into image, suitable as input for\nneural networks. This method can be further improved by its Graphics Processing\nUnit (GPU) implementation to provide significant speedup in computational time.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:20:56 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Sane", "Parth", ""], ["Agrawal", "Ravindra", ""]]}, {"id": "1705.01813", "submitter": "Cheng-Hao Deng", "authors": "Cheng-Hao Deng and Wan-Lei Zhao", "title": "Fast k-means based on KNN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, k-means clustering has been widely adopted as a basic\nprocessing tool in various contexts. However, its computational cost could be\nprohibitively high as the data size and the cluster number are large. It is\nwell known that the processing bottleneck of k-means lies in the operation of\nseeking closest centroid in each iteration. In this paper, a novel solution\ntowards the scalability issue of k-means is presented. In the proposal, k-means\nis supported by an approximate k-nearest neighbors graph. In the k-means\niteration, each data sample is only compared to clusters that its nearest\nneighbors reside. Since the number of nearest neighbors we consider is much\nless than k, the processing cost in this step becomes minor and irrelevant to\nk. The processing bottleneck is therefore overcome. The most interesting thing\nis that k-nearest neighbor graph is constructed by iteratively calling the fast\n$k$-means itself. Comparing with existing fast k-means variants, the proposed\nalgorithm achieves hundreds to thousands times speed-up while maintaining high\nclustering quality. As it is tested on 10 million 512-dimensional data, it\ntakes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the\nsame scale of clustering, it would take 3 years for traditional k-means.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:27:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Deng", "Cheng-Hao", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1705.01842", "submitter": "Ran Breuer", "authors": "Ran Breuer and Ron Kimmel", "title": "A Deep Learning Perspective on the Origin of Facial Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions play a significant role in human communication and\nbehavior. Psychologists have long studied the relationship between facial\nexpressions and emotions. Paul Ekman et al., devised the Facial Action Coding\nSystem (FACS) to taxonomize human facial expressions and model their behavior.\nThe ability to recognize facial expressions automatically, enables novel\napplications in fields like human-computer interaction, social gaming, and\npsychological research. There has been a tremendously active research in this\nfield, with several recent papers utilizing convolutional neural networks (CNN)\nfor feature extraction and inference. In this paper, we employ CNN\nunderstanding methods to study the relation between the features these\ncomputational networks are using, the FACS and Action Units (AU). We verify our\nfindings on the Extended Cohn-Kanade (CK+), NovaEmotions and FER2013 datasets.\nWe apply these models to various tasks and tests using transfer learning,\nincluding cross-dataset validation and cross-task performance. Finally, we\nexploit the nature of the FER based CNN models for the detection of\nmicro-expressions and achieve state-of-the-art accuracy using a simple\nlong-short-term-memory (LSTM) recurrent neural network (RNN).\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 13:59:07 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 13:05:00 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Breuer", "Ran", ""], ["Kimmel", "Ron", ""]]}, {"id": "1705.01861", "submitter": "Vicky Kalogeiton", "authors": "Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, Cordelia\n  Schmid", "title": "Action Tubelet Detector for Spatio-Temporal Action Localization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches for spatio-temporal action localization\nrely on detections at the frame level that are then linked or tracked across\ntime. In this paper, we leverage the temporal continuity of videos instead of\noperating at the frame level. We propose the ACtion Tubelet detector\n(ACT-detector) that takes as input a sequence of frames and outputs tubelets,\ni.e., sequences of bounding boxes with associated scores. The same way\nstate-of-the-art object detectors rely on anchor boxes, our ACT-detector is\nbased on anchor cuboids. We build upon the SSD framework. Convolutional\nfeatures are extracted for each frame, while scores and regressions are based\non the temporal stacking of these features, thus exploiting information from a\nsequence. Our experimental results show that leveraging sequences of frames\nsignificantly improves detection performance over using individual frames. The\ngain of our tubelet detector can be explained by both more accurate scores and\nmore precise localization. Our ACT-detector outperforms the state-of-the-art\nmethods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in\nparticular at high overlap thresholds.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 14:41:56 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 21:02:35 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 13:54:34 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Kalogeiton", "Vicky", ""], ["Weinzaepfel", "Philippe", ""], ["Ferrari", "Vittorio", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1705.01906", "submitter": "Tobias B\\\"ottger", "authors": "Tobias B\\\"ottger and Dominik Gutermuth", "title": "Derivate-based Component-Trees for Multi-Channel Image Segmentation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of derivate-based component-trees for images with an\narbitrary number of channels. The approach is a natural extension of the\nclassical component-tree devoted to gray-scale images. The similar structure\nenables the translation of many gray-level image processing techniques based on\nthe component-tree to hyperspectral and color images. As an example\napplication, we present an image segmentation approach that extracts Maximally\nStable Homogeneous Regions (MSHR). The approach very similar to MSER but can be\napplied to images with an arbitrary number of channels. As opposed to MSER, our\napproach implicitly segments regions with are both lighter and darker than\ntheir background for gray-scale images and can be used in OCR applications\nwhere MSER will fail. We introduce a local flooding-based immersion for the\nderivate-based component-tree construction which is linear in the number of\npixels. In the experiments, we show that the runtime scales favorably with an\nincreasing number of channels and may improve algorithms which build on MSER.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 16:51:33 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:41:32 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["B\u00f6ttger", "Tobias", ""], ["Gutermuth", "Dominik", ""]]}, {"id": "1705.01908", "submitter": "Yifan Liu", "authors": "Yifan Liu, Zengchang Qin, Zhenbo Luo and Hua Wang", "title": "Auto-painter: Cartoon Image Generation from Sketch by Using Conditional\n  Generative Adversarial Networks", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, realistic image generation using deep neural networks has become a\nhot topic in machine learning and computer vision. Images can be generated at\nthe pixel level by learning from a large collection of images. Learning to\ngenerate colorful cartoon images from black-and-white sketches is not only an\ninteresting research problem, but also a potential application in digital\nentertainment. In this paper, we investigate the sketch-to-image synthesis\nproblem by using conditional generative adversarial networks (cGAN). We propose\nthe auto-painter model which can automatically generate compatible colors for a\nsketch. The new model is not only capable of painting hand-draw sketch with\nproper colors, but also allowing users to indicate preferred colors.\nExperimental results on two sketch datasets show that the auto-painter performs\nbetter that existing image-to-image methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 17:04:28 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 03:40:05 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Liu", "Yifan", ""], ["Qin", "Zengchang", ""], ["Luo", "Zhenbo", ""], ["Wang", "Hua", ""]]}, {"id": "1705.01921", "submitter": "Liliang Ren", "authors": "Liliang Ren", "title": "Recurrent Soft Attention Model for Common Object Recognition", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Recurrent Soft Attention Model, which integrates the visual\nattention from the original image to a LSTM memory cell through a down-sample\nnetwork. The model recurrently transmits visual attention to the memory cells\nfor glimpse mask generation, which is a more natural way for attention\nintegration and exploitation in general object detection and recognition\nproblem. We test our model under the metric of the top-1 accuracy on the\nCIFAR-10 dataset. The experiment shows that our down-sample network and\nfeedback mechanism plays an effective role among the whole network structure.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 17:27:42 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:02:52 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Ren", "Liliang", ""]]}, {"id": "1705.02045", "submitter": "Teresa Heiss", "authors": "Teresa Heiss and Hubert Wagner", "title": "Streaming Algorithm for Euler Characteristic Curves of Multidimensional\n  Images", "comments": null, "journal-ref": "In: Computer Analysis of Images and Patterns. CAIP 2017. Lecture\n  Notes in Computer Science, vol 10424. Springer International Publishing,\n  Cham, 2017, pp. 397-409", "doi": "10.1007/978-3-319-64689-3_32", "report-no": null, "categories": "cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm to compute Euler characteristic curves of\ngray scale images of arbitrary dimension. In various applications the Euler\ncharacteristic curve is used as a descriptor of an image.\n  Our algorithm is the first streaming algorithm for Euler characteristic\ncurves. The usage of streaming removes the necessity to store the entire image\nin RAM. Experiments show that our implementation handles terabyte scale images\non commodity hardware. Due to lock-free parallelism, it scales well with the\nnumber of processor cores. Our software---CHUNKYEuler---is available as open\nsource on Bitbucket.\n  Additionally, we put the concept of the Euler characteristic curve in the\nwider context of computational topology. In particular, we explain the\nconnection with persistence diagrams.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 23:14:45 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 14:06:49 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 14:36:57 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Heiss", "Teresa", ""], ["Wagner", "Hubert", ""]]}, {"id": "1705.02082", "submitter": "Katerina Fragkiadaki", "authors": "Katerina Fragkiadaki, Jonathan Huang, Alex Alemi, Sudheendra\n  Vijayanarasimhan, Susanna Ricco, Rahul Sukthankar", "title": "Motion Prediction Under Multimodality with Conditional Stochastic\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a visual history, multiple future outcomes for a video scene are\nequally probable, in other words, the distribution of future outcomes has\nmultiple modes. Multimodality is notoriously hard to handle by standard\nregressors or classifiers: the former regress to the mean and the latter\ndiscretize a continuous high dimensional output space. In this work, we present\nstochastic neural network architectures that handle such multimodality through\nstochasticity: future trajectories of objects, body joints or frames are\nrepresented as deep, non-linear transformations of random (as opposed to\ndeterministic) variables. Such random variables are sampled from simple\nGaussian distributions whose means and variances are parametrized by the output\nof convolutional encoders over the visual history. We introduce novel\nconvolutional architectures for predicting future body joint trajectories that\noutperform fully connected alternatives \\cite{DBLP:journals/corr/WalkerDGH16}.\nWe introduce stochastic spatial transformers through optical flow warping for\npredicting future frames, which outperform their deterministic equivalents\n\\cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves\nan intractable marginalization over stochastic variables. We compare various\ntraining schemes that handle such marginalization through a) straightforward\nsampling from the prior, b) conditional variational autoencoders\n\\cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed\nK-best-sample loss that penalizes the best prediction under a fixed \"prediction\nbudget\". We show experimental results on object trajectory prediction, human\nbody joint trajectory prediction and video prediction under varying future\nuncertainty, validating quantitatively and qualitatively our architectural\nchoices and training schemes.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 04:19:40 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Fragkiadaki", "Katerina", ""], ["Huang", "Jonathan", ""], ["Alemi", "Alex", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Ricco", "Susanna", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1705.02090", "submitter": "Kai Xu", "authors": "Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, Leonidas\n  Guibas", "title": "GRASS: Generative Recursive Autoencoders for Shape Structures", "comments": "Corresponding author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2017) 36, 4, Article 52", "doi": "10.1145/3072959.3073613", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel neural network architecture for encoding and synthesis\nof 3D shapes, particularly their structures. Our key insight is that 3D shapes\nare effectively characterized by their hierarchical organization of parts,\nwhich reflects fundamental intra-shape relationships such as adjacency and\nsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map a\nflat, unlabeled, arbitrary part layout to a compact code. The code effectively\ncaptures hierarchical structures of man-made 3D objects of varying structural\ncomplexities despite being fixed-dimensional: an associated decoder maps a code\nback to a full hierarchy. The learned bidirectional mapping is further tuned\nusing an adversarial setup to yield a generative model of plausible structures,\nfrom which novel structures can be sampled. Finally, our structure synthesis\nframework is augmented by a second trained module that produces fine-grained\npart geometry, conditioned on global and local structural context, leading to a\nfull generative pipeline for 3D shapes. We demonstrate that without\nsupervision, our network learns meaningful structural hierarchies adhering to\nperceptual grouping principles, produces compact codes which enable\napplications such as shape classification and partial matching, and supports\nshape synthesis and interpolation with significant variations in topology and\ngeometry.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:45:10 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 04:49:23 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Jun", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yumer", "Ersin", ""], ["Zhang", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1705.02092", "submitter": "Justin Johnson", "authors": "Agrim Gupta, Justin Johnson, Alexandre Alahi, and Li Fei-Fei", "title": "Characterizing and Improving Stability in Neural Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in style transfer on images has focused on improving the\nquality of stylized images and speed of methods. However, real-time methods are\nhighly unstable resulting in visible flickering when applied to videos. In this\nwork we characterize the instability of these methods by examining the solution\nset of the style transfer objective. We show that the trace of the Gram matrix\nrepresenting style is inversely related to the stability of the method. Then,\nwe present a recurrent convolutional network for real-time video style transfer\nwhich incorporates a temporal consistency loss and overcomes the instability of\nprior methods. Our networks can be applied at any resolution, do not re- quire\noptical flow at test time, and produce high quality, temporally consistent\nstylized videos in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:54:05 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Gupta", "Agrim", ""], ["Johnson", "Justin", ""], ["Alahi", "Alexandre", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1705.02101", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Chen Sun, Zhenheng Yang, Ram Nevatia", "title": "TALL: Temporal Activity Localization via Language Query", "comments": "ICCV 2017 camera ready (with supplemental material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on temporal localization of actions in untrimmed videos.\nExisting methods typically train classifiers for a pre-defined list of actions\nand apply them in a sliding window fashion. However, activities in the wild\nconsist of a wide combination of actors, actions and objects; it is difficult\nto design a proper activity list that meets users' needs. We propose to\nlocalize activities by natural language queries. Temporal Activity Localization\nvia Language (TALL) is challenging as it requires: (1) suitable design of text\nand video representations to allow cross-modal matching of actions and language\nqueries; (2) ability to locate actions accurately given features from sliding\nwindows of limited granularity. We propose a novel Cross-modal Temporal\nRegression Localizer (CTRL) to jointly model text query and video clips, output\nalignment scores and action boundary regression results for candidate clips.\nFor evaluation, we adopt TaCoS dataset, and build a new dataset for this task\non top of Charades by adding sentence temporal annotations, called\nCharades-STA. We also build complex sentence queries in Charades-STA for test.\nExperimental results show that CTRL outperforms previous methods significantly\non both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 06:54:41 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 21:31:07 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Gao", "Jiyang", ""], ["Sun", "Chen", ""], ["Yang", "Zhenheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "1705.02102", "submitter": "Mahdi Alavi", "authors": "Seyed Mohammad Mahdi Alavi, and Yunyan Zhang", "title": "Phase Congruency Parameter Optimization for Enhanced Detection of Image\n  Features for both Natural and Medical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the presentation and proof of the hypothesis that image features\nare particularly perceived at points where the Fourier components are maximally\nin phase, the concept of phase congruency (PC) is introduced. Subsequently, a\ntwo-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has\nbeen an important tool for detecting and evaluation of image features. However,\nthe 2D-MSPC requires many parameters to be appropriately tuned for optimal\nimage features detection. In this paper, we defined a criterion for parameter\noptimization of the 2D-MSPC, which is a function of its maximum and minimum\nmoments. We formulated the problem in various optimal and suboptimal\nframeworks, and discussed the conditions and features of the suboptimal\nsolutions. The effectiveness of the proposed method was verified through\nseveral examples, ranging from natural objects to medical images from patients\nwith a neurological disease, multiple sclerosis.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 06:58:03 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Alavi", "Seyed Mohammad Mahdi", ""], ["Zhang", "Yunyan", ""]]}, {"id": "1705.02139", "submitter": "Antonio D'Innocente", "authors": "Antonio D'Innocente, Fabio Maria Carlucci, Mirco Colosi, Barbara\n  Caputo", "title": "Bridging between Computer and Robot Vision through Data Augmentation: a\n  Case Study on Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive progress brought by deep network in visual object\nrecognition, robot vision is still far from being a solved problem. The most\nsuccessful convolutional architectures are developed starting from ImageNet, a\nlarge scale collection of images of object categories downloaded from the Web.\nThis kind of images is very different from the situated and embodied visual\nexperience of robots deployed in unconstrained settings. To reduce the gap\nbetween these two visual experiences, this paper proposes a simple yet\neffective data augmentation layer that zooms on the object of interest and\nsimulates the object detection outcome of a robot vision system. The layer,\nthat can be used with any convolutional deep architecture, brings to an\nincrease in object recognition performance of up to 7\\%, in experiments\nperformed over three different benchmark databases. Upon acceptance of the\npaper, our robot data augmentation layer will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 09:05:11 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["D'Innocente", "Antonio", ""], ["Carlucci", "Fabio Maria", ""], ["Colosi", "Mirco", ""], ["Caputo", "Barbara", ""]]}, {"id": "1705.02145", "submitter": "Fu-Qing Zhu", "authors": "Fuqing Zhu, Xiangwei Kong, Liang Zheng, Haiyan Fu, Qi Tian", "title": "Part-based Deep Hashing for Large-scale Person Re-identification", "comments": "12 pages, 4 figures. IEEE Transactions on Image Processing, 2017", "journal-ref": null, "doi": "10.1109/TIP.2017.2695101", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale is a trend in person re-identification (re-id). It is important\nthat real-time search be performed in a large gallery. While previous methods\nmostly focus on discriminative learning, this paper makes the attempt in\nintegrating deep learning and hashing into one framework to evaluate the\nefficiency and accuracy for large-scale person re-id. We integrate spatial\ninformation for discriminative visual representation by partitioning the\npedestrian image into horizontal parts. Specifically, Part-based Deep Hashing\n(PDH) is proposed, in which batches of triplet samples are employed as the\ninput of the deep hashing architecture. Each triplet sample contains two\npedestrian images (or parts) with the same identity and one pedestrian image\n(or part) of the different identity. A triplet loss function is employed with a\nconstraint that the Hamming distance of pedestrian images (or parts) with the\nsame identity is smaller than ones with the different identity. In the\nexperiment, we show that the proposed Part-based Deep Hashing method yields\nvery competitive re-id accuracy on the large-scale Market-1501 and\nMarket-1501+500K datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 09:24:13 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhu", "Fuqing", ""], ["Kong", "Xiangwei", ""], ["Zheng", "Liang", ""], ["Fu", "Haiyan", ""], ["Tian", "Qi", ""]]}, {"id": "1705.02148", "submitter": "Noureldien Hussein", "authors": "Noureldien Hussein, Efstratios Gavves and Arnold W.M. Smeulders", "title": "Unified Embedding and Metric Learning for Zero-Exemplar Event Detection", "comments": "IEEE CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection in unconstrained videos is conceived as a content-based video\nretrieval with two modalities: textual and visual. Given a text describing a\nnovel event, the goal is to rank related videos accordingly. This task is\nzero-exemplar, no video examples are given to the novel event.\n  Related works train a bank of concept detectors on external data sources.\nThese detectors predict confidence scores for test videos, which are ranked and\nretrieved accordingly. In contrast, we learn a joint space in which the visual\nand textual representations are embedded. The space casts a novel event as a\nprobability of pre-defined events. Also, it learns to measure the distance\nbetween an event and its related videos.\n  Our model is trained end-to-end on publicly available EventNet. When applied\nto TRECVID Multimedia Event Detection dataset, it outperforms the\nstate-of-the-art by a considerable margin.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 09:45:58 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Hussein", "Noureldien", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1705.02193", "submitter": "Hakan Bilen", "authors": "James Thewlis, Hakan Bilen, Andrea Vedaldi", "title": "Unsupervised learning of object landmarks by factorized spatial\n  embeddings", "comments": "To be published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning automatically the structure of object categories remains an\nimportant open problem in computer vision. In this paper, we propose a novel\nunsupervised approach that can discover and learn landmarks in object\ncategories, thus characterizing their structure. Our approach is based on\nfactorizing image deformations, as induced by a viewpoint change or an object\ndeformation, by learning a deep neural network that detects landmarks\nconsistently with such visual effects. Furthermore, we show that the learned\nlandmarks establish meaningful correspondences between different object\ninstances in a category without having to impose this requirement explicitly.\nWe assess the method qualitatively on a variety of object types, natural and\nman-made. We also show that our unsupervised landmarks are highly predictive of\nmanually-annotated landmarks in face benchmark datasets, and can be used to\nregress these with a high degree of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 12:45:35 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 08:54:26 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Thewlis", "James", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1705.02224", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Detecting Adversarial Samples Using Density Ratio Estimates", "comments": "Updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially based on deep architectures are used in\neveryday applications ranging from self driving cars to medical diagnostics. It\nhas been shown that such models are dangerously susceptible to adversarial\nsamples, indistinguishable from real samples to human eye, adversarial samples\nlead to incorrect classifications with high confidence. Impact of adversarial\nsamples is far-reaching and their efficient detection remains an open problem.\nWe propose to use direct density ratio estimation as an efficient model\nagnostic measure to detect adversarial samples. Our proposed method works\nequally well with single and multi-channel samples, and with different\nadversarial sample generation methods. We also propose a method to use density\nratio estimates for generating adversarial samples with an added constraint of\npreserving density ratio.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:28:59 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:23:57 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 21:22:00 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 16:17:18 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1705.02233", "submitter": "Minne Li", "authors": "Minne Li, Zhaoning Zhang, Hao Yu, Xinyuan Chen, Dongsheng Li", "title": "S-OHEM: Stratified Online Hard Example Mining for Object Detection", "comments": "9 pages, 3 figures, accepted by CCCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in object detection is to propose detectors with\nhighly accurate localization of objects. The online sampling of high-loss\nregion proposals (hard examples) uses the multitask loss with equal weight\nsettings across all loss types (e.g, classification and localization, rigid and\nnon-rigid categories) and ignores the influence of different loss distributions\nthroughout the training process, which we find essential to the training\nefficacy. In this paper, we present the Stratified Online Hard Example Mining\n(S-OHEM) algorithm for training higher efficiency and accuracy detectors.\nS-OHEM exploits OHEM with stratified sampling, a widely-adopted sampling\ntechnique, to choose the training examples according to this influence during\nhard example mining, and thus enhance the performance of object detectors. We\nshow through systematic experiments that S-OHEM yields an average precision\n(AP) improvement of 0.5% on rigid categories of PASCAL VOC 2007 for both the\nIoU threshold of 0.6 and 0.7. For KITTI 2012, both results of the same metric\nare 1.6%. Regarding the mean average precision (mAP), a relative increase of\n0.3% and 0.5% (1% and 0.5%) is observed for VOC07 (KITTI12) using the same set\nof IoU threshold. Also, S-OHEM is easy to integrate with existing region-based\ndetectors and is capable of acting with post-recognition level regressors.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 14:13:17 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 08:18:45 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 08:30:59 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 08:41:43 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Li", "Minne", ""], ["Zhang", "Zhaoning", ""], ["Yu", "Hao", ""], ["Chen", "Xinyuan", ""], ["Li", "Dongsheng", ""]]}, {"id": "1705.02315", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri and\n  Ronald M. Summers", "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "comments": "CVPR 2017 spotlight;V1: CVPR submission+supplementary; V2: Statistics\n  and benchmark results on published ChestX-ray14 dataset are updated in\n  Appendix B V3: Minor correction V4: new data download link upated:\n  https://nihcc.app.box.com/v/ChestXray-NIHCC V5: Update benchmark results on\n  the published data split in the appendix", "journal-ref": "IEEE CVPR 2017, pp. 2097-2106 (2017)", "doi": "10.1109/CVPR.2017.369", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:31:12 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 17:45:07 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 19:12:50 GMT"}, {"version": "v4", "created": "Wed, 27 Sep 2017 14:33:36 GMT"}, {"version": "v5", "created": "Thu, 14 Dec 2017 19:35:31 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wang", "Xiaosong", ""], ["Peng", "Yifan", ""], ["Lu", "Le", ""], ["Lu", "Zhiyong", ""], ["Bagheri", "Mohammadhadi", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1705.02402", "submitter": "Zhenhua Feng", "authors": "Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber and\n  Xiao-Jun Wu", "title": "Face Detection, Bounding Box Aggregation and Pose Estimation for Robust\n  Facial Landmark Localisation in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for robust face detection and landmark localisation of\nfaces in the wild, which has been evaluated as part of `the 2nd Facial Landmark\nLocalisation Competition'. The framework has four stages: face detection,\nbounding box aggregation, pose estimation and landmark localisation. To achieve\na high detection rate, we use two publicly available CNN-based face detectors\nand two proprietary detectors. We aggregate the detected face bounding boxes of\neach input image to reduce false positives and improve face detection accuracy.\nA cascaded shape regressor, trained using faces with a variety of pose\nvariations, is then employed for pose estimation and image pre-processing.\nLast, we train the final cascaded shape regressor for fine-grained landmark\nlocalisation, using a large number of training samples with limited pose\nvariations. The experimental results obtained on the 300W and Menpo benchmarks\ndemonstrate the superiority of our framework over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 21:28:27 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 10:28:51 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Feng", "Zhen-Hua", ""], ["Kittler", "Josef", ""], ["Awais", "Muhammad", ""], ["Huber", "Patrik", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1705.02406", "submitter": "Tejas Borkar", "authors": "Tejas Borkar and Lina Karam", "title": "DeepCorrect: Correcting DNN models against Image Distortions", "comments": "Accepted to IEEE Transactions on Image Processing, April 2019. For\n  associated code, see https://github.com/tsborkar/DeepCorrect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the widespread use of deep neural networks (DNNs) has\nfacilitated great improvements in performance for computer vision tasks like\nimage classification and object recognition. In most realistic computer vision\napplications, an input image undergoes some form of image distortion such as\nblur and additive noise during image acquisition or transmission. Deep networks\ntrained on pristine images perform poorly when tested on such distortions. In\nthis paper, we evaluate the effect of image distortions like Gaussian blur and\nadditive noise on the activations of pre-trained convolutional filters. We\npropose a metric to identify the most noise susceptible convolutional filters\nand rank them in order of the highest gain in classification accuracy upon\ncorrection. In our proposed approach called DeepCorrect, we apply small stacks\nof convolutional layers with residual connections, at the output of these\nranked filters and train them to correct the worst distortion affected filter\nactivations, whilst leaving the rest of the pre-trained filter outputs in the\nnetwork unchanged. Performance results show that applying DeepCorrect models\nfor common vision tasks like image classification (ImageNet), object\nrecognition (Caltech-101, Caltech-256) and scene classification (SUN-397),\nsignificantly improves the robustness of DNNs against distorted images and\noutperforms other alternative approaches..\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 21:52:49 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 06:56:39 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 10:06:28 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 02:13:57 GMT"}, {"version": "v5", "created": "Tue, 9 Apr 2019 01:22:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Borkar", "Tejas", ""], ["Karam", "Lina", ""]]}, {"id": "1705.02407", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Zhi Zhang, Zhihai He", "title": "Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation", "comments": "13 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:1609.01743, arXiv:1702.07432, arXiv:1602.00134 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Human pose estimation using deep neural networks aims to map input images\nwith large variations into multiple body keypoints which must satisfy a set of\ngeometric constraints and inter-dependency imposed by the human body model.\nThis is a very challenging nonlinear manifold learning process in a very high\ndimensional feature space. We believe that the deep neural network, which is\ninherently an algebraic computation system, is not the most effecient way to\ncapture highly sophisticated human knowledge, for example those highly coupled\ngeometric characteristics and interdependence between keypoints in human poses.\nIn this work, we propose to explore how external knowledge can be effectively\nrepresented and injected into the deep neural networks to guide its training\nprocess using learned projections that impose proper prior. Specifically, we\nuse the stacked hourglass design and inception-resnet module to construct a\nfractal network to regress human pose images into heatmaps with no explicit\ngraphical modeling. We encode external knowledge with visual features which are\nable to characterize the constraints of human body models and evaluate the\nfitness of intermediate network output. We then inject these external features\ninto the neural network using a projection matrix learned using an auxiliary\ncost function. The effectiveness of the proposed inception-resnet module and\nthe benefit in guided learning with knowledge projection is evaluated on two\nwidely used benchmarks. Our approach achieves state-of-the-art performance on\nboth datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:06:55 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 21:05:15 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ning", "Guanghan", ""], ["Zhang", "Zhi", ""], ["He", "Zhihai", ""]]}, {"id": "1705.02429", "submitter": "Peng Tang", "authors": "Peng Tang, Xinggang Wang, Zilong Huang, Xiang Bai, Wenyu Liu", "title": "Deep Patch Learning for Weakly Supervised Object Classification and\n  Discovery", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-level image representation is very important for object classification\nand detection, since it is robust to spatial transformation, scale variation,\nand cluttered background. Many existing methods usually require fine-grained\nsupervisions (e.g., bounding-box annotations) to learn patch features, which\nrequires a great effort to label images may limit their potential applications.\nIn this paper, we propose to learn patch features via weak supervisions, i.e.,\nonly image-level supervisions. To achieve this goal, we treat images as bags\nand patches as instances to integrate the weakly supervised multiple instance\nlearning constraints into deep neural networks. Also, our method integrates the\ntraditional multiple stages of weakly supervised object classification and\ndiscovery into a unified deep convolutional neural network and optimizes the\nnetwork in an end-to-end way. The network processes the two tasks object\nclassification and discovery jointly, and shares hierarchical deep features.\nThrough this jointly learning strategy, weakly supervised object classification\nand discovery are beneficial to each other. We test the proposed method on the\nchallenging PASCAL VOC datasets. The results show that our method can obtain\nstate-of-the-art performance on object classification, and very competitive\nresults on object discovery, with faster testing speed than competitors.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 02:05:38 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Xinggang", ""], ["Huang", "Zilong", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""]]}, {"id": "1705.02431", "submitter": "He Zhang", "authors": "He Zhang, Vishal M.Patel", "title": "Sparse Representation-based Open Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized Sparse Representation- based Classification (SRC)\nalgorithm for open set recognition where not all classes presented during\ntesting are known during training. The SRC algorithm uses class reconstruction\nerrors for classification. As most of the discriminative information for open\nset recognition is hidden in the tail part of the matched and sum of\nnon-matched reconstruction error distributions, we model the tail of those two\nerror distributions using the statistical Extreme Value Theory (EVT). Then we\nsimplify the open set recognition problem into a set of hypothesis testing\nproblems. The confidence scores corresponding to the tail distributions of a\nnovel test sample are then fused to determine its identity. The effectiveness\nof the proposed method is demonstrated using four publicly available image and\nobject classification datasets and it is shown that this method can perform\nsignificantly better than many competitive open set recognition algorithms.\nCode is public available: https://github.com/hezhangsprinter/SROSR\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 02:16:48 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1705.02445", "submitter": "Julieta Martinez", "authors": "Julieta Martinez, Michael J. Black, Javier Romero", "title": "On human motion prediction using recurrent neural networks", "comments": "Accepted at CVPR 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion modelling is a classical problem at the intersection of graphics\nand computer vision, with applications spanning human-computer interaction,\nmotion synthesis, and motion prediction for virtual and augmented reality.\nFollowing the success of deep learning methods in several computer vision\ntasks, recent work has focused on using deep recurrent neural networks (RNNs)\nto model human motion, with the goal of learning time-dependent representations\nthat perform tasks such as short-term motion prediction and long-term human\nmotion synthesis. We examine recent work, with a focus on the evaluation\nmethodologies commonly used in the literature, and show that, surprisingly,\nstate-of-the-art performance can be achieved by a simple baseline that does not\nattempt to model motion at all. We investigate this result, and analyze recent\nRNN methods by looking at the architectures, loss functions, and training\nprocedures used in state-of-the-art approaches. We propose three changes to the\nstandard RNN models typically used for human motion, which result in a simple\nand scalable RNN architecture that obtains state-of-the-art performance on\nhuman motion prediction.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 05:08:05 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Martinez", "Julieta", ""], ["Black", "Michael J.", ""], ["Romero", "Javier", ""]]}, {"id": "1705.02460", "submitter": "Hassan Foroosh", "authors": "Amara Tariq and Hassan Foroosh", "title": "Image Annotation using Multi-Layer Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic annotation of images with descriptive words is a challenging\nproblem with vast applications in the areas of image search and retrieval. This\nproblem can be viewed as a label-assignment problem by a classifier dealing\nwith a very large set of labels, i.e., the vocabulary set. We propose a novel\nannotation method that employs two layers of sparse coding and performs\ncoarse-to-fine labeling. Themes extracted from the training data are treated as\ncoarse labels. Each theme is a set of training images that share a common\nsubject in their visual and textual contents. Our system extracts coarse labels\nfor training and test images without requiring any prior knowledge. Vocabulary\nwords are the fine labels to be associated with images. Most of the annotation\nmethods achieve low recall due to the large number of available fine labels,\ni.e., vocabulary words. These systems also tend to achieve high precision for\nhighly frequent words only while relatively rare words are more important for\nsearch and retrieval purposes. Our system not only outperforms various\npreviously proposed annotation systems, but also achieves symmetric response in\nterms of precision and recall. Our system scores and maintains high precision\nfor words with a wide range of frequencies. Such behavior is achieved by\nintelligently reducing the number of available fine labels or words for each\nimage based on coarse labels assigned to it.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 08:15:18 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tariq", "Amara", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.02498", "submitter": "Samuel Dodge", "authors": "Samuel Dodge, Lina Karam", "title": "A Study and Comparison of Human and Deep Learning Recognition\n  Performance Under Visual Distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) achieve excellent performance on standard\nclassification tasks. However, under image quality distortions such as blur and\nnoise, classification accuracy becomes poor. In this work, we compare the\nperformance of DNNs with human subjects on distorted images. We show that,\nalthough DNNs perform better than or on par with humans on good quality images,\nDNN performance is still much lower than human performance on distorted images.\nWe additionally find that there is little correlation in errors between DNNs\nand human subjects. This could be an indication that the internal\nrepresentation of images are different between DNNs and the human visual\nsystem. These comparisons with human performance could be used to guide future\ndevelopment of more robust DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 16:16:11 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Dodge", "Samuel", ""], ["Karam", "Lina", ""]]}, {"id": "1705.02503", "submitter": "Lamberto Ballan", "authors": "Federico Bartoli, Giuseppe Lisanti, Lamberto Ballan, Alberto Del Bimbo", "title": "Context-Aware Trajectory Prediction", "comments": "Submitted to BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion and behaviour in crowded spaces is influenced by several\nfactors, such as the dynamics of other moving agents in the scene, as well as\nthe static elements that might be perceived as points of attraction or\nobstacles. In this work, we present a new model for human trajectory prediction\nwhich is able to take advantage of both human-human and human-space\ninteractions. The future trajectory of humans, are generated by observing their\npast positions and interactions with the surroundings. To this end, we propose\na \"context-aware\" recurrent neural network LSTM model, which can learn and\npredict human motion in crowded spaces such as a sidewalk, a museum or a\nshopping mall. We evaluate our model on a public pedestrian datasets, and we\ncontribute a new challenging dataset that collects videos of humans that\nnavigate in a (real) crowded space such as a big museum. Results show that our\napproach can predict human trajectories better when compared to previous\nstate-of-the-art forecasting models.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 16:36:32 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bartoli", "Federico", ""], ["Lisanti", "Giuseppe", ""], ["Ballan", "Lamberto", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1705.02544", "submitter": "Wenguan Wang", "authors": "Wenguan Wang and Jianbing Shen", "title": "Deep Visual Attention Prediction", "comments": "W. Wang and J. Shen. Deep visual attention prediction. IEEE TIP,\n  27(5):2368-2378,2018. Code and results can be found in\n  https://github.com/wenguanwang/deepattention", "journal-ref": "IEEE Transactions on Image Processing, Vol. 27, No. 5, pp\n  2368-2378, 2018", "doi": "10.1109/TIP.2017.2787612", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we aim to predict human eye fixation with view-free scenes\nbased on an end-to-end deep learning architecture. Although Convolutional\nNeural Networks (CNNs) have made substantial improvement on human attention\nprediction, it is still needed to improve CNN based attention models by\nefficiently leveraging multi-scale features. Our visual attention network is\nproposed to capture hierarchical saliency information from deep, coarse layers\nwith global saliency information to shallow, fine layers with local saliency\nresponse. Our model is based on a skip-layer network structure, which predicts\nhuman attention from multiple convolutional layers with various reception\nfields. Final saliency prediction is achieved via the cooperation of those\nglobal and local predictions. Our model is learned in a deep supervision\nmanner, where supervision is directly fed into multi-level layers, instead of\nprevious approaches of providing supervision only at the output layer and\npropagating this supervision back to earlier layers. Our model thus\nincorporates multi-level saliency predictions within a single network, which\nsignificantly decreases the redundancy of previous approaches of learning\nmultiple network streams with different input scales. Extensive experimental\nanalysis on various challenging benchmark datasets demonstrate our method\nyields state-of-the-art performance with competitive inference time.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 01:53:17 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 03:41:34 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 22:59:02 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""]]}, {"id": "1705.02596", "submitter": "Yawen Huang", "authors": "Yawen Huang, Ling Shao, Alejandro F. Frangi", "title": "Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical\n  Images using Weakly-Supervised Joint Convolutional Sparse Coding", "comments": "10 pages, 6 figures. Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) offers high-resolution \\emph{in vivo}\nimaging and rich functional and anatomical multimodality tissue contrast. In\npractice, however, there are challenges associated with considerations of\nscanning costs, patient comfort, and scanning time that constrain how much data\ncan be acquired in clinical or research studies. In this paper, we explore the\npossibility of generating high-resolution and multimodal images from\nlow-resolution single-modality imagery. We propose the weakly-supervised joint\nconvolutional sparse coding to simultaneously solve the problems of\nsuper-resolution (SR) and cross-modality image synthesis. The learning process\nrequires only a few registered multimodal image pairs as the training set.\nAdditionally, the quality of the joint dictionary learning can be improved\nusing a larger set of unpaired images. To combine unpaired data from different\nimage resolutions/modalities, a hetero-domain image alignment term is proposed.\nLocal image neighborhoods are naturally preserved by operating on the whole\nimage domain (as opposed to image patches) and using joint convolutional sparse\ncoding. The paired images are enhanced in the joint learning process with\nunpaired data and an additional maximum mean discrepancy term, which minimizes\nthe dissimilarity between their feature distributions. Experiments show that\nthe proposed method outperforms state-of-the-art techniques on both SR\nreconstruction and simultaneous SR and cross-modality synthesis.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 10:55:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Huang", "Yawen", ""], ["Shao", "Ling", ""], ["Frangi", "Alejandro F.", ""]]}, {"id": "1705.02636", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Erico N de Souza, Ahmad Pesaranghader, Baifan Hu, Daniel\n  L. Silver and Stan Matwin", "title": "TrajectoryNet: An Embedded GPS Trajectory Representation for Point-based\n  Classification Using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and discovering knowledge from GPS (Global Positioning System)\ntraces of human activities is an essential topic in mobility-based urban\ncomputing. We propose TrajectoryNet-a neural network architecture for\npoint-based trajectory classification to infer real world human transportation\nmodes from GPS traces. To overcome the challenge of capturing the underlying\nlatent factors in the low-dimensional and heterogeneous feature space imposed\nby GPS data, we develop a novel representation that embeds the original feature\nspace into another space that can be understood as a form of basis expansion.\nWe also enrich the feature space via segment-based information and use Maxout\nactivations to improve the predictive power of Recurrent Neural Networks\n(RNNs). We achieve over 98% classification accuracy when detecting four types\nof transportation modes, outperforming existing models without additional\nsensory data or location-based prior knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 15:40:08 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 15:06:43 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Jiang", "Xiang", ""], ["de Souza", "Erico N", ""], ["Pesaranghader", "Ahmad", ""], ["Hu", "Baifan", ""], ["Silver", "Daniel L.", ""], ["Matwin", "Stan", ""]]}, {"id": "1705.02653", "submitter": "Reinhard Moratz", "authors": "Christopher H. Dorr, Reinhard Moratz", "title": "Towards Applying the OPRA Theory to Shape Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation for using qualitative shape descriptions is as follows:\nqualitative shape descriptions can implicitly act as a schema for measuring the\nsimilarity of shapes, which has the potential to be cognitively adequate. Then,\nshapes which are similar to each other would also be similar for a pattern\nrecognition algorithm. There is substantial work in pattern recognition and\ncomputer vision dealing with shape similarity. Here with our approach to\nqualitative shape descriptions and shape similarity, the focus is on achieving\na representation using only simple predicates that a human could even apply\nwithout computer support.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 16:37:27 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Dorr", "Christopher H.", ""], ["Moratz", "Reinhard", ""]]}, {"id": "1705.02678", "submitter": "Yi Gao", "authors": "Naiyun Zhou, Andrey Fedorov, Fiona Fennessy, Ron Kikinis, Yi Gao", "title": "Large scale digital prostate pathology image analysis combining feature\n  extraction and deep neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological assessments, including surgical resection and core needle\nbiopsy, are the standard procedures in the diagnosis of the prostate cancer.\nCurrent interpretation of the histopathology images includes the determination\nof the tumor area, Gleason grading, and identification of certain\nprognosis-critical features. Such a process is not only tedious, but also prune\nto intra/inter-observe variabilities. Recently, FDA cleared the marketing of\nthe first whole slide imaging system for digital pathology. This opens a new\nera for the computer aided prostate image analysis and feature extraction based\non the digital histopathology images. In this work, we present an analysis\npipeline that includes localization of the cancer region, grading, area ratio\nof different Gleason grades, and cytological/architectural feature extraction.\nThe proposed algorithm combines the human engineered feature extraction as well\nas those learned by the deep neural network. Moreover, the entire pipeline is\nimplemented to directly operate on the whole slide images produced by the\ndigital scanners and is therefore potentially easy to translate into clinical\npractices. The algorithm is tested on 368 whole slide images from the TCGA data\nset and achieves an overall accuracy of 75% in differentiating Gleason 3+4 with\n4+3 slides.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 18:45:04 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 19:30:23 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Zhou", "Naiyun", ""], ["Fedorov", "Andrey", ""], ["Fennessy", "Fiona", ""], ["Kikinis", "Ron", ""], ["Gao", "Yi", ""]]}, {"id": "1705.02680", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, and Vijayan K. Asari", "title": "Handwritten Bangla Digit Recognition Using Deep Learning", "comments": "12 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 18:49:27 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Sidike", "Paheding", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1705.02689", "submitter": "Seyed Sajjadi", "authors": "Seyed A Sajjadi, Danial Moazen, Ani Nahapetian", "title": "AirDraw: Leveraging Smart Watch Motion Sensors for Mobile Human Computer\n  Interactions", "comments": "6 pages, AirDraw, Leveraging Smart Watch Motion Sensors for Mobile\n  Human Computer Interactions : IEEE, CCNC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable computing is one of the fastest growing technologies today. Smart\nwatches are poised to take over at least of half the wearable devices market in\nthe near future. Smart watch screen size, however, is a limiting factor for\ngrowth, as it restricts practical text input. On the other hand, wearable\ndevices have some features, such as consistent user interaction and hands-free,\nheads-up operations, which pave the way for gesture recognition methods of text\nentry. This paper proposes a new text input method for smart watches, which\nutilizes motion sensor data and machine learning approaches to detect letters\nwritten in the air by a user. This method is less computationally intensive and\nless expensive when compared to computer vision approaches. It is also not\naffected by lighting factors, which limit computer vision solutions. The\nAirDraw system prototype developed to test this approach is presented.\nAdditionally, experimental results close to 71% accuracy are presented.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:58:54 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed A", ""], ["Moazen", "Danial", ""], ["Nahapetian", "Ani", ""]]}, {"id": "1705.02694", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan and Gerald M Knapp", "title": "Multimodal Affect Analysis for Product Feedback Assessment", "comments": "10 pages, ISERC 2013, IIE Annual Conference. Proceedings. Institute\n  of Industrial Engineers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers often react expressively to products such as food samples, perfume,\njewelry, sunglasses, and clothing accessories. This research discusses a\nmultimodal affect recognition system developed to classify whether a consumer\nlikes or dislikes a product tested at a counter or kiosk, by analyzing the\nconsumer's facial expression, body posture, hand gestures, and voice after\ntesting the product. A depth-capable camera and microphone system - Kinect for\nWindows - is utilized. An emotion identification engine has been developed to\nanalyze the images and voice to determine affective state of the customer. The\nimage is segmented using skin color and adaptive threshold. Face, body and\nhands are detected using the Haar cascade classifier. Canny edges are\nidentified and the lip, body and hand contours are extracted using spatial\nfiltering. Edge count and orientation around the mouth, cheeks, eyes,\nshoulders, fingers and the location of the edges are used as features.\nClassification is done by an emotion template mapping algorithm and training a\nclassifier using support vector machines. The real-time performance, accuracy\nand feasibility for multimodal affect recognition in feedback assessment are\nevaluated.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 20:39:35 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1705.02727", "submitter": "Jhony Heriberto Giraldo Zuluaga", "authors": "Jhony-Heriberto Giraldo-Zuluaga, Augusto Salazar, Alexander Gomez and\n  Ang\\'elica Diaz-Pulido", "title": "Automatic Recognition of Mammal Genera on Camera-Trap Images using\n  Multi-Layer Robust Principal Component Analysis and Mixture Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation and classification of animals from camera-trap images is due\nto the conditions under which the images are taken, a difficult task. This work\npresents a method for classifying and segmenting mammal genera from camera-trap\nimages. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA)\nfor segmenting, Convolutional Neural Networks (CNNs) for extracting features,\nLeast Absolute Shrinkage and Selection Operator (LASSO) for selecting features,\nand Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for\nclassifying mammal genera present in the Colombian forest. We evaluated our\nmethod with the camera-trap images from the Alexander von Humboldt Biological\nResources Research Institute. We obtained an accuracy of 92.65% classifying 8\nmammal genera and a False Positive (FP) class, using automatic-segmented\nimages. On the other hand, we reached 90.32% of accuracy classifying 10 mammal\ngenera, using ground-truth images only. Unlike almost all previous works, we\nconfront the animal segmentation and genera classification in the camera-trap\nrecognition. This method shows a new approach toward a fully-automatic\ndetection of animals from camera-trap images.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 02:50:06 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Giraldo-Zuluaga", "Jhony-Heriberto", ""], ["Salazar", "Augusto", ""], ["Gomez", "Alexander", ""], ["Diaz-Pulido", "Ang\u00e9lica", ""]]}, {"id": "1705.02743", "submitter": "Xin Chen", "authors": "Xin Chen and Yu Zhu, Hua Zhou and Liang Diao and Dongyan Wang", "title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition", "comments": "8 pages, 5 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new and challenging large-scale food image\ndataset called \"ChineseFoodNet\", which aims to automatically recognizing\npictured Chinese dishes. Most of the existing food image datasets collected\nfood images either from recipe pictures or selfie. In our dataset, images of\neach food category of our dataset consists of not only web recipe and menu\npictures but photos taken from real dishes, recipe and menu as well.\nChineseFoodNet contains over 180,000 food photos of 208 categories, with each\ncategory covering a large variations in presentations of same Chinese food. We\npresent our efforts to build this large-scale image dataset, including food\ncategory selection, data collection, and data clean and label, in particular\nhow to use machine learning methods to reduce manual labeling work that is an\nexpensive process. We share a detailed benchmark of several state-of-the-art\ndeep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose\na novel two-step data fusion approach referred as \"TastyNet\", which combines\nprediction results from different CNNs with voting method. Our proposed\napproach achieves top-1 accuracies of 81.43% on the validation set and 81.55%\non the test set, respectively. The latest dataset is public available for\nresearch and can be achieved at https://sites.google.com/view/chinesefoodnet.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 05:16:51 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 01:07:35 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 17:58:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Chen", "Xin", ""], ["Zhu", "Yu", ""], ["Zhou", "Hua", ""], ["Diao", "Liang", ""], ["Wang", "Dongyan", ""]]}, {"id": "1705.02751", "submitter": "Afsheen Rafaqat Ali", "authors": "Afsheen Rafaqat Ali, Usman Shahid, Mohsen Ali and Jeffrey Ho", "title": "High-Level Concepts for Affective Understanding of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to bridge the affective gap between image content and the\nemotional response of the viewer it elicits by using High-Level Concepts\n(HLCs). In contrast to previous work that relied solely on low-level features\nor used convolutional neural network (CNN) as a black-box, we use HLCs\ngenerated by pretrained CNNs in an explicit way to investigate the\nrelations/associations between these HLCs and a (small) set of Ekman's\nemotional classes. As a proof-of-concept, we first propose a linear admixture\nmodel for modeling these relations, and the resulting computational framework\nallows us to determine the associations between each emotion class and certain\nHLCs (objects and places). This linear model is further extended to a nonlinear\nmodel using support vector regression (SVR) that aims to predict the viewer's\nemotional response using both low-level image features and HLCs extracted from\nimages. These class-specific regressors are then assembled into a regressor\nensemble that provide a flexible and effective predictor for predicting\nviewer's emotional responses from images. Experimental results have\ndemonstrated that our results are comparable to existing methods, with a clear\nview of the association between HLCs and emotional classes that is ostensibly\nmissing in most existing work.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 05:58:05 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Ali", "Afsheen Rafaqat", ""], ["Shahid", "Usman", ""], ["Ali", "Mohsen", ""], ["Ho", "Jeffrey", ""]]}, {"id": "1705.02757", "submitter": "Jiayuan Mao", "authors": "Jiayuan Mao, Tete Xiao, Yuning Jiang, Zhimin Cao", "title": "What Can Help Pedestrian Detection?", "comments": "Accepted to IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating extra features has been considered as an effective approach to\nboost traditional pedestrian detection methods. However, there is still a lack\nof studies on whether and how CNN-based pedestrian detectors can benefit from\nthese extra features. The first contribution of this paper is exploring this\nissue by aggregating extra features into CNN-based pedestrian detection\nframework. Through extensive experiments, we evaluate the effects of different\nkinds of extra features quantitatively. Moreover, we propose a novel network\narchitecture, namely HyperLearner, to jointly learn pedestrian detection as\nwell as the given extra feature. By multi-task training, HyperLearner is able\nto utilize the information of given features and improve detection performance\nwithout extra inputs in inference. The experimental results on multiple\npedestrian benchmarks validate the effectiveness of the proposed HyperLearner.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 06:43:20 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mao", "Jiayuan", ""], ["Xiao", "Tete", ""], ["Jiang", "Yuning", ""], ["Cao", "Zhimin", ""]]}, {"id": "1705.02758", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie, Jianxin Wu,\n  Chunhua Shen, Zhi-Hua Zhou", "title": "Deep Descriptor Transforming for Image Co-Localization", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusable model design becomes desirable with the rapid expansion of machine\nlearning applications. In this paper, we focus on the reusability of\npre-trained deep convolutional models. Specifically, different from treating\npre-trained models as feature extractors, we reveal more treasures beneath\nconvolutional layers, i.e., the convolutional activations could act as a\ndetector for the common object in the image co-localization problem. We propose\na simple but effective method, named Deep Descriptor Transforming (DDT), for\nevaluating the correlations of descriptors and then obtaining the\ncategory-consistent regions, which can accurately locate the common object in a\nset of images. Empirical studies validate the effectiveness of the proposed DDT\nmethod. On benchmark image co-localization datasets, DDT consistently\noutperforms existing state-of-the-art methods by a large margin. Moreover, DDT\nalso demonstrates good generalization ability for unseen categories and\nrobustness for dealing with noisy data.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 06:52:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Zhang", "Chen-Lin", ""], ["Li", "Yao", ""], ["Xie", "Chen-Wei", ""], ["Wu", "Jianxin", ""], ["Shen", "Chunhua", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1705.02772", "submitter": "Toshiki Nakamura", "authors": "Toshiki Nakamura, Anna Zhu, Keiji Yanai and Seiichi Uchida", "title": "Scene Text Eraser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The character information in natural scene images contains various personal\ninformation, such as telephone numbers, home addresses, etc. It is a high risk\nof leakage the information if they are published. In this paper, we proposed a\nscene text erasing method to properly hide the information via an inpainting\nconvolutional neural network (CNN) model. The input is a scene text image, and\nthe output is expected to be text erased image with all the character regions\nfilled up the colors of the surrounding background pixels. This work is\naccomplished by a CNN model through convolution to deconvolution with\ninterconnection process. The training samples and the corresponding inpainting\nimages are considered as teaching signals for training. To evaluate the text\nerasing performance, the output images are detected by a novel scene text\ndetection method. Subsequently, the same measurement on text detection is\nutilized for testing the images in benchmark dataset ICDAR2013. Compared with\ndirect text detection way, the scene text erasing process demonstrates a\ndrastically decrease on the precision, recall and f-score. That proves the\neffectiveness of proposed method for erasing the text in natural scene images.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 08:28:34 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Nakamura", "Toshiki", ""], ["Zhu", "Anna", ""], ["Yanai", "Keiji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1705.02782", "submitter": "Fares Jalled", "authors": "Fares Jalled", "title": "Face Recognition Machine Vision System Using Eigenfaces", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face Recognition is a common problem in Machine Learning. This technology has\nalready been widely used in our lives. For example, Facebook can automatically\ntag people's faces in images, and also some mobile devices use face recognition\nto protect private security. Face images comes with different background,\nvariant illumination, different facial expression and occlusion. There are a\nlarge number of approaches for the face recognition. Different approaches for\nface recognition have been experimented with specific databases which consist\nof single type, format and composition of image. Doing so, these approaches\ndon't suit with different face databases. One of the basic face recognition\ntechniques is eigenface which is quite simple, efficient, and yields generally\ngood results in controlled circumstances. So, this paper presents an\nexperimental performance comparison of face recognition using Principal\nComponent Analysis (PCA) and Normalized Principal Component Analysis (NPCA).\nThe experiments are carried out on the ORL (ATT) and Indian face database (IFD)\nwhich contain variability in expression, pose, and facial details. The results\nobtained for the two methods have been compared by varying the number of\ntraining images. MATLAB is used for implementing algorithms also.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 08:53:30 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Jalled", "Fares", ""]]}, {"id": "1705.02854", "submitter": "Alessandro Niccolai", "authors": "Stefano Frassinelli, Alessandro Niccolai, Riccardo E. Zich", "title": "Video Processing for Barycenter Trajectory Identification in Diving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to show a procedure for identify the barycentre of a\ndiver by means of video processing. This procedure is aimed to introduce\nquantitative analysis tools and diving performance measurement and therefore in\ndiving training. Sport performance analysis is a trend that is growing\nexponentially for all level athletes: it has been applied extensively in some\nsports such as cycling. Sport performance analysis has been applied mainly for\nhigh level athletes; in order to be used also for middle or low level athletes\nthe proposed technique has to be flexible and low cost. Video processing is\nsuitable to fulfil both these requirements. In diving, the first analysis that\nhas to be done is the barycentre trajectory tracking.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 13:09:09 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Frassinelli", "Stefano", ""], ["Niccolai", "Alessandro", ""], ["Zich", "Riccardo E.", ""]]}, {"id": "1705.02883", "submitter": "Umar Iqbal", "authors": "Umar Iqbal, Andreas Doering, Hashim Yasin, Bj\\\"orn Kr\\\"uger, Andreas\n  Weber, Juergen Gall", "title": "A Dual-Source Approach for 3D Human Pose Estimation from a Single Image", "comments": "under consideration at Computer Vision and Image Understanding.\n  Extended version of CVPR-2016 paper, arXiv:1509.06720", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the challenging problem of 3D human pose estimation\nfrom single images. Recent approaches learn deep neural networks to regress 3D\npose directly from images. One major challenge for such methods, however, is\nthe collection of training data. Specifically, collecting large amounts of\ntraining data containing unconstrained images annotated with accurate 3D poses\nis infeasible. We therefore propose to use two independent training sources.\nThe first source consists of accurate 3D motion capture data, and the second\nsource consists of unconstrained images with annotated 2D poses. To integrate\nboth sources, we propose a dual-source approach that combines 2D pose\nestimation with efficient 3D pose retrieval. To this end, we first convert the\nmotion capture data into a normalized 2D pose space, and separately learn a 2D\npose estimation model from the image data. During inference, we estimate the 2D\npose and efficiently retrieve the nearest 3D poses. We then jointly estimate a\nmapping from the 3D pose space to the image and reconstruct the 3D pose. We\nprovide a comprehensive evaluation of the proposed method and experimentally\ndemonstrate the effectiveness of our approach, even when the skeleton\nstructures of the two sources differ substantially.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:03:48 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 13:24:52 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Iqbal", "Umar", ""], ["Doering", "Andreas", ""], ["Yasin", "Hashim", ""], ["Kr\u00fcger", "Bj\u00f6rn", ""], ["Weber", "Andreas", ""], ["Gall", "Juergen", ""]]}, {"id": "1705.02887", "submitter": "Qiangeng Xu", "authors": "Qiangeng Xu, Zengchang Qin and Tao Wan", "title": "Generative Cooperative Net for Image Generation and Data Augmentation", "comments": "12 pages, 8 figures", "journal-ref": "The International Symposium on Integrated Uncertainty in Knowledge\n  Modelling and Decision Making (IUKM) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to build a good model for image generation given an abstract concept is a\nfundamental problem in computer vision. In this paper, we explore a generative\nmodel for the task of generating unseen images with desired features. We\npropose the Generative Cooperative Net (GCN) for image generation. The idea is\nsimilar to generative adversarial networks except that the generators and\ndiscriminators are trained to work accordingly. Our experiments on hand-written\ndigit generation and facial expression generation show that GCN's two\ncooperative counterparts (the generator and the classifier) can work together\nnicely and achieve promising results. We also discovered a usage of such\ngenerative model as an data-augmentation tool. Our experiment of applying this\nmethod on a recognition task shows that it is very effective comparing to other\nexisting methods. It is easy to set up and could help generate a very large\nsynthesized dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:10:07 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 02:36:51 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 15:29:28 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Xu", "Qiangeng", ""], ["Qin", "Zengchang", ""], ["Wan", "Tao", ""]]}, {"id": "1705.02893", "submitter": "Yilin Song", "authors": "Yilin Song, Jonathan Viventi, Yao Wang", "title": "Multi Resolution LSTM For Long Term Prediction In Neural Activity Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epileptic seizures are caused by abnormal, overly syn- chronized, electrical\nactivity in the brain. The abnor- mal electrical activity manifests as waves,\npropagating across the brain. Accurate prediction of the propagation velocity\nand direction of these waves could enable real- time responsive brain\nstimulation to suppress or prevent the seizures entirely. However, this problem\nis very chal- lenging because the algorithm must be able to predict the neural\nsignals in a sufficiently long time horizon to allow enough time for medical\nintervention. We consider how to accomplish long term prediction using a LSTM\nnetwork. To alleviate the vanishing gradient problem, we propose two\nencoder-decoder-predictor structures, both using multi-resolution\nrepresentation. The novel LSTM structure with multi-resolution layers could\nsignificantly outperform the single-resolution benchmark with similar number of\nparameters. To overcome the blurring effect associated with video prediction in\nthe pixel domain using standard mean square error (MSE) loss, we use energy-\nbased adversarial training to improve the long-term pre- diction. We\ndemonstrate and analyze how a discriminative model with an encoder-decoder\nstructure using 3D CNN model improves long term prediction.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:32:22 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 02:50:09 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Song", "Yilin", ""], ["Viventi", "Jonathan", ""], ["Wang", "Yao", ""]]}, {"id": "1705.02894", "submitter": "Jong Chul Ye", "authors": "Jae Hyun Lim and Jong Chul Ye", "title": "Geometric GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) represent an important milestone for\neffective generative models, which has inspired numerous variants seemingly\ndifferent from each other. One of the main contributions of this paper is to\nreveal a unified geometric structure in GAN and its variants. Specifically, we\nshow that the adversarial generative model training can be decomposed into\nthree geometric steps: separating hyperplane search, discriminator parameter\nupdate away from the separating hyperplane, and the generator update along the\nnormal vector direction of the separating hyperplane. This geometric intuition\nreveals the limitations of the existing approaches and leads us to propose a\nnew formulation called geometric GAN using SVM separating hyperplane that\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\nconverges to a Nash equilibrium between the discriminator and generator. In\naddition, extensive numerical results show that the superior performance of\ngeometric GAN.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:32:33 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 01:12:28 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lim", "Jae Hyun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1705.02900", "submitter": "Nilaksh Das", "authors": "Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen,\n  Michael E. Kounavis, Duen Horng Chau", "title": "Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with\n  JPEG Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved great success in solving a variety\nof machine learning (ML) problems, especially in the domain of image\nrecognition. However, recent research showed that DNNs can be highly vulnerable\nto adversarially generated instances, which look seemingly normal to human\nobservers, but completely confuse DNNs. These adversarial samples are crafted\nby adding small perturbations to normal, benign images. Such perturbations,\nwhile imperceptible to the human eye, are picked up by DNNs and cause them to\nmisclassify the manipulated instances with high confidence. In this work, we\nexplore and demonstrate how systematic JPEG compression can work as an\neffective pre-processing step in the classification pipeline to counter\nadversarial attacks and dramatically reduce their effects (e.g., Fast Gradient\nSign Method, DeepFool). An important component of JPEG compression is its\nability to remove high frequency signal components, inside square blocks of an\nimage. Such an operation is equivalent to selective blurring of the image,\nhelping remove additive perturbations. Further, we propose an ensemble-based\ntechnique that can be constructed quickly from a given well-performing DNN, and\nempirically show how such an ensemble that leverages JPEG compression can\nprotect a model from multiple types of adversarial attacks, without requiring\nknowledge about the model.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:55:32 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Das", "Nilaksh", ""], ["Shanbhogue", "Madhuri", ""], ["Chen", "Shang-Tse", ""], ["Hohman", "Fred", ""], ["Chen", "Li", ""], ["Kounavis", "Michael E.", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1705.02928", "submitter": "Yuantao Gu", "authors": "Xiudong Wang and Yuantao Gu", "title": "Cross-label Suppression: A Discriminative and Fast Dictionary Learning\n  with Group Regularization", "comments": "36 pages, 12 figures, 11 tables", "journal-ref": null, "doi": "10.1109/TIP.2017.2703101", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses image classification through learning a compact and\ndiscriminative dictionary efficiently. Given a structured dictionary with each\natom (columns in the dictionary matrix) related to some label, we propose\ncross-label suppression constraint to enlarge the difference among\nrepresentations for different classes. Meanwhile, we introduce group\nregularization to enforce representations to preserve label properties of\noriginal samples, meaning the representations for the same class are encouraged\nto be similar. Upon the cross-label suppression, we don't resort to\nfrequently-used $\\ell_0$-norm or $\\ell_1$-norm for coding, and obtain\ncomputational efficiency without losing the discriminative power for\ncategorization. Moreover, two simple classification schemes are also developed\nto take full advantage of the learnt dictionary. Extensive experiments on six\ndata sets including face recognition, object categorization, scene\nclassification, texture recognition and sport action categorization are\nconducted, and the results show that the proposed approach can outperform lots\nof recently presented dictionary algorithms on both recognition accuracy and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:49:43 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Wang", "Xiudong", ""], ["Gu", "Yuantao", ""]]}, {"id": "1705.02949", "submitter": "Kumar Sankar Ray", "authors": "Kumar S. Ray, Vijayan K. Asari and Soma Chakraborty", "title": "Object Detection by Spatio-Temporal Analysis and Tracking of the\n  Detected Objects in a Video with Variable Background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach for detecting and tracking objects\nin videos with variable background i.e. videos captured by moving cameras\nwithout any additional sensor. In a video captured by a moving camera, both the\nbackground and foreground are changing in each frame of the image sequence. So\nfor these videos, modeling a single background with traditional background\nmodeling methods is infeasible and thus the detection of actual moving object\nin a variable background is a challenging task. To detect actual moving object\nin this work, spatio-temporal blobs have been generated in each frame by\nspatio-temporal analysis of the image sequence using a three-dimensional Gabor\nfilter. Then individual blobs, which are parts of one object are merged using\nMinimum Spanning Tree to form the moving object in the variable background. The\nheight, width and four-bin gray-value histogram of the object are calculated as\nits features and an object is tracked in each frame using these features to\ngenerate the trajectories of the object through the video sequence. In this\nwork, problem of data association during tracking is solved by Linear\nAssignment Problem and occlusion is handled by the application of kalman\nfilter. The major advantage of our method over most of the existing tracking\nalgorithms is that, the proposed method does not require initialization in the\nfirst frame or training on sample data to perform. Performance of the algorithm\nhas been tested on benchmark videos and very satisfactory result has been\nachieved. The performance of the algorithm is also comparable and superior with\nrespect to some benchmark algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 09:32:06 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Ray", "Kumar S.", ""], ["Asari", "Vijayan K.", ""], ["Chakraborty", "Soma", ""]]}, {"id": "1705.02950", "submitter": "Jan Hosang", "authors": "Jan Hosang, Rodrigo Benenson, Bernt Schiele", "title": "Learning non-maximum suppression", "comments": "Added \"Supplementary material\" title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors have hugely profited from moving towards an end-to-end\nlearning paradigm: proposals, features, and the classifier becoming one neural\nnetwork improved results two-fold on general object detection. One\nindispensable component is non-maximum suppression (NMS), a post-processing\nalgorithm responsible for merging all detections that belong to the same\nobject. The de facto standard NMS algorithm is still fully hand-crafted,\nsuspiciously simple, and -- being based on greedy clustering with a fixed\ndistance threshold -- forces a trade-off between recall and precision. We\npropose a new network architecture designed to perform NMS, using only boxes\nand their score. We report experiments for person detection on PETS and for\ngeneral object categories on the COCO dataset. Our approach shows promise\nproviding improved localization and occlusion handling.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:16:28 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 12:52:04 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Hosang", "Jan", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1705.02953", "submitter": "Limin Wang", "authors": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang,\n  Luc Van Gool", "title": "Temporal Segment Networks for Action Recognition in Videos", "comments": "14 pages. An extension of submission at\n  https://arxiv.org/abs/1608.00859", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have achieved great success for image\nrecognition. However, for action recognition in videos, their advantage over\ntraditional methods is not so evident. We present a general and flexible\nvideo-level framework for learning action models in videos. This method, called\ntemporal segment network (TSN), aims to model long-range temporal structures\nwith a new segment-based sampling and aggregation module. This unique design\nenables our TSN to efficiently learn action models by using the whole action\nvideos. The learned models could be easily adapted for action recognition in\nboth trimmed and untrimmed videos with simple average pooling and multi-scale\ntemporal window integration, respectively. We also study a series of good\npractices for the instantiation of TSN framework given limited training\nsamples. Our approach obtains the state-the-of-art performance on four\nchallenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%),\nTHUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB\ndifference for motion models, our method can still achieve competitive accuracy\non UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal\nsegment networks, we won the video classification track at the ActivityNet\nchallenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and\nthe proposed good practices.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:21:26 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wang", "Limin", ""], ["Xiong", "Yuanjun", ""], ["Wang", "Zhe", ""], ["Qiao", "Yu", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.02962", "submitter": "R\\\"udiger Alshut", "authors": "R\\\"udiger Alshut", "title": "Konzept f\\\"ur Bildanalysen in Hochdurchsatz-Systemen am Beispiel des\n  Zebrab\\\"arblings", "comments": null, "journal-ref": "Ph.D. Thesis, Karlsruhe Institute of Technology, KIT Scientific\n  Publishing, 2016", "doi": "10.5445/IR/1000061759", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With image-based high-throughput experiments, new challenges arise in both,\nthe design of experiments and the automated analysis. To be able to handle the\nmassive number of single experiments and the corresponding amount of data, a\ncomprehensive concept for the design of experiments and a new evaluation method\nis needed. This work proposes a new method for an optimized experiment layout\nthat enables the determination of parameters, adapted for the needs of\nautomated image analysis. Furthermore, a catalogue of new image analysis\nmodules, especially developed for zebrafish analysis, is presented. The\ncombination of both parts offers the user, usually a biologist, an approach for\nhigh-throughput zebrafish image analysis, which enables the extraction of new\nsignals and optimizes the design of experiments. The result is a reduction of\ndata amount, redundant information and workload as well as classification\nerrors.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:48:49 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Alshut", "R\u00fcdiger", ""]]}, {"id": "1705.02966", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Amir Jamaludin, Andrew Zisserman", "title": "You said that?", "comments": "https://youtu.be/LeufDSb15Kc British Machine Vision Conference\n  (BMVC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for generating a video of a talking face. The method\ntakes as inputs: (i) still images of the target face, and (ii) an audio speech\nsegment; and outputs a video of the target face lip synched with the audio. The\nmethod runs in real time and is applicable to faces and audio not seen at\ntraining time.\n  To achieve this we propose an encoder-decoder CNN model that uses a joint\nembedding of the face and audio to generate synthesised talking face video\nframes. The model is trained on tens of hours of unlabelled videos.\n  We also show results of re-dubbing videos using speech from a different\nperson.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:44:46 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 14:58:55 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chung", "Joon Son", ""], ["Jamaludin", "Amir", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.02997", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Jun-Yan Zhu, Nima Khademi Kalantari, Alexei A. Efros,\n  Ravi Ramamoorthi", "title": "Light Field Video Capture Using a Learning-Based Hybrid Imaging System", "comments": "ACM Transactions on Graphics (Proceedings of SIGGRAPH 2017)", "journal-ref": null, "doi": "10.1145/3072959.3073614", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras have many advantages over traditional cameras, as they\nallow the user to change various camera settings after capture. However,\ncapturing light fields requires a huge bandwidth to record the data: a modern\nlight field camera can only take three images per second. This prevents current\nconsumer light field cameras from capturing light field videos. Temporal\ninterpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible\nas too much information will be entirely missing between adjacent frames.\nInstead, we develop a hybrid imaging system, adding another standard video\ncamera to capture the temporal information. Given a 3 fps light field sequence\nand a standard 30 fps 2D video, our system can then generate a full light field\nvideo at 30 fps. We adopt a learning-based approach, which can be decomposed\ninto two steps: spatio-temporal flow estimation and appearance estimation. The\nflow estimation propagates the angular information from the light field\nsequence to the 2D video, so we can warp input images to the target view. The\nappearance estimation then combines these warped images to output the final\npixels. The whole process is trained end-to-end using convolutional neural\nnetworks. Experimental results demonstrate that our algorithm outperforms\ncurrent video interpolation methods, enabling consumer light field videography,\nand making applications such as refocusing and parallax view generation\nachievable on videos for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:56:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Zhu", "Jun-Yan", ""], ["Kalantari", "Nima Khademi", ""], ["Efros", "Alexei A.", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1705.02999", "submitter": "Richard Zhang", "authors": "Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S.\n  Lin, Tianhe Yu, Alexei A. Efros", "title": "Real-Time User-Guided Image Colorization with Learned Deep Priors", "comments": "Accepted to SIGGRAPH 2017. Project page:\n  https://richzhang.github.io/ideepcolor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning approach for user-guided image colorization. The\nsystem directly maps a grayscale image, along with sparse, local user \"hints\"\nto an output colorization with a Convolutional Neural Network (CNN). Rather\nthan using hand-defined rules, the network propagates user edits by fusing\nlow-level cues along with high-level semantic information, learned from\nlarge-scale data. We train on a million images, with simulated user inputs. To\nguide the user towards efficient input selection, the system recommends likely\ncolors based on the input image and current user inputs. The colorization is\nperformed in a single feed-forward pass, enabling real-time use. Even with\nrandomly simulated user inputs, we show that the proposed system helps novice\nusers quickly create realistic colorizations, and offers large improvements in\ncolorization quality with just a minute of use. In addition, we demonstrate\nthat the framework can incorporate other user \"hints\" to the desired\ncolorization, showing an application to color histogram transfer. Our code and\nmodels are available at https://richzhang.github.io/ideepcolor.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:58:11 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Isola", "Phillip", ""], ["Geng", "Xinyang", ""], ["Lin", "Angela S.", ""], ["Yu", "Tianhe", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1705.03004", "submitter": "Hussam Qassim Mr.", "authors": "Hussam Qassim, David Feinzimer, and Abhishek Verma", "title": "Residual Squeeze VGG16", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has given way to a new era of machine learning, apart from\ncomputer vision. Convolutional neural networks have been implemented in image\nclassification, segmentation and object detection. Despite recent advancements,\nwe are still in the very early stages and have yet to settle on best practices\nfor network architecture in terms of deep design, small in size and a short\ntraining time. In this work, we propose a very deep neural network comprised of\n16 Convolutional layers compressed with the Fire Module adapted from the\nSQUEEZENET model. We also call for the addition of residual connections to help\nsuppress degradation. This model can be implemented on almost every neural\nnetwork model with fully incorporated residual learning. This proposed model\nResidual-Squeeze-VGG16 (ResSquVGG16) trained on the large-scale MIT\nPlaces365-Standard scene dataset. In our tests, the model performed with\naccuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation\naccuracy while also enjoying a 23.86% reduction in training time and an 88.4%\nreduction in size. In our tests, this model was trained from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 23:46:26 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Qassim", "Hussam", ""], ["Feinzimer", "David", ""], ["Verma", "Abhishek", ""]]}, {"id": "1705.03098", "submitter": "Julieta Martinez", "authors": "Julieta Martinez, Rayat Hossain, Javier Romero, James J. Little", "title": "A simple yet effective baseline for 3d human pose estimation", "comments": "Accepted to ICCV 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success of deep convolutional networks, state-of-the-art\nmethods for 3d human pose estimation have focused on deep end-to-end systems\nthat predict 3d joint locations given raw image pixels. Despite their excellent\nperformance, it is often not easy to understand whether their remaining error\nstems from a limited 2d pose (visual) understanding, or from a failure to map\n2d poses into 3-dimensional positions. With the goal of understanding these\nsources of error, we set out to build a system that given 2d joint locations\npredicts 3d positions. Much to our surprise, we have found that, with current\ntechnology, \"lifting\" ground truth 2d joint locations to 3d space is a task\nthat can be solved with a remarkably low error rate: a relatively simple deep\nfeed-forward network outperforms the best reported result by about 30\\% on\nHuman3.6M, the largest publicly available 3d pose estimation benchmark.\nFurthermore, training our system on the output of an off-the-shelf\nstate-of-the-art 2d detector (\\ie, using images as input) yields state of the\nart results -- this includes an array of systems that have been trained\nend-to-end specifically for this task. Our results indicate that a large\nportion of the error of modern deep 3d pose estimation systems stems from their\nvisual analysis, and suggests directions to further advance the state of the\nart in 3d human pose estimation.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 21:48:37 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 18:36:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Martinez", "Julieta", ""], ["Hossain", "Rayat", ""], ["Romero", "Javier", ""], ["Little", "James J.", ""]]}, {"id": "1705.03111", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Slobodan Ilic", "title": "CAD Priors for Accurate and Flexible Instance Reconstruction", "comments": "Published at International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and automatic approach for accurate reconstruction of\ninstances of big 3D objects from multiple, unorganized and unstructured point\nclouds, in presence of dynamic clutter and occlusions. In contrast to\nconventional scanning, where the background is assumed to be rather static, we\naim at handling dynamic clutter where background drastically changes during the\nobject scanning. Currently, it is tedious to solve this with available methods\nunless the object of interest is first segmented out from the rest of the\nscene. We address the problem by assuming the availability of a prior CAD\nmodel, roughly resembling the object to be reconstructed. This assumption\nalmost always holds in applications such as industrial inspection or reverse\nengineering. With aid of this prior acting as a proxy, we propose a fully\nenhanced pipeline, capable of automatically detecting and segmenting the object\nof interest from scenes and creating a pose graph, online, with linear\ncomplexity. This allows initial scan alignment to the CAD model space, which is\nthen refined without the CAD constraint to fully recover a high fidelity 3D\nreconstruction, accurate up to the sensor noise level. We also contribute a\nnovel object detection method, local implicit shape models (LISM) and give a\nfast verification scheme. We evaluate our method on multiple datasets,\ndemonstrating the ability to accurately reconstruct objects from small sizes up\nto $125m^3$.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 22:39:17 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 21:38:53 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1705.03146", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Jeremy S. Smith, Wenjin Lu and Bailing Zhang", "title": "CHAM: action recognition using convolutional hierarchical attention\n  model", "comments": "accepted by ICIP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the soft attention mechanism, which was originally proposed in\nlanguage processing, has been applied in computer vision tasks like image\ncaptioning. This paper presents improvements to the soft attention model by\ncombining a convolutional LSTM with a hierarchical system architecture to\nrecognize action categories in videos. We call this model the Convolutional\nHierarchical Attention Model (CHAM). The model applies a convolutional\noperation inside the LSTM cell and an attention map generation process to\nrecognize actions. The hierarchical architecture of this model is able to\nexplicitly reason on multi-granularities of action categories. The proposed\narchitecture achieved improved results on three publicly available datasets:\nthe UCF sports dataset, the Olympic sports dataset and the HMDB51 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:27:37 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 06:11:26 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Yan", "Shiyang", ""], ["Smith", "Jeremy S.", ""], ["Lu", "Wenjin", ""], ["Zhang", "Bailing", ""]]}, {"id": "1705.03148", "submitter": "Chen Chen", "authors": "Ce Li, Chen Chen, Baochang Zhang, Qixiang Ye, Jungong Han, Rongrong Ji", "title": "Deep Spatio-temporal Manifold Network for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data such as videos are often sampled from complex manifold. We\npropose leveraging the manifold structure to constrain the deep action feature\nlearning, thereby minimizing the intra-class variations in the feature space\nand alleviating the over-fitting problem. Considering that manifold can be\ntransferred, layer by layer, from the data domain to the deep features, the\nmanifold priori is posed from the top layer into the back propagation learning\nprocedure of convolutional neural network (CNN). The resulting algorithm\n--Spatio-Temporal Manifold Network-- is solved with the efficient Alternating\nDirection Method of Multipliers and Backward Propagation (ADMM-BP). We\ntheoretically show that STMN recasts the problem as projection over the\nmanifold via an embedding method. The proposed approach is evaluated on two\nbenchmark datasets, showing significant improvements to the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:37:30 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Li", "Ce", ""], ["Chen", "Chen", ""], ["Zhang", "Baochang", ""], ["Ye", "Qixiang", ""], ["Han", "Jungong", ""], ["Ji", "Rongrong", ""]]}, {"id": "1705.03159", "submitter": "Li Shen", "authors": "Teck Wee Chua and Li Shen", "title": "Contour Detection from Deep Patch-level Boundary Prediction", "comments": "IEEE International Conference on Signal and Image Processing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for contour detection with\nConvolutional Neural Networks. A multi-scale CNN learning framework is designed\nto automatically learn the most relevant features for contour patch detection.\nOur method uses patch-level measurements to create contour maps with\noverlapping patches. We show the proposed CNN is able to to detect large-scale\ncontours in an image efficienly. We further propose a guided filtering method\nto refine the contour maps produced from large-scale contours. Experimental\nresults on the major contour benchmark databases demonstrate the effectiveness\nof the proposed technique. We show our method can achieve good detection of\nboth fine-scale and large-scale contours.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 03:12:02 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Chua", "Teck Wee", ""], ["Shen", "Li", ""]]}, {"id": "1705.03212", "submitter": "San Jiang", "authors": "San Jiang, Wanshou Jiang", "title": "Efficient Structure from Motion for Oblique UAV Images Based on Maximal\n  Spanning Tree Expansions", "comments": "33 pages, 66 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary contribution of this paper is an efficient Structure from Motion\n(SfM) solution for oblique unmanned aerial vehicle (UAV) images. First, an\nalgorithm, considering spatial relationship constrains between image\nfootprints, is designed for match pair selection with assistant of UAV flight\ncontrol data and oblique camera mounting angles. Second, a topological\nconnection network (TCN), represented by an undirected weighted graph, is\nconstructed from initial match pairs, which encodes overlap area and\nintersection angle into edge weights. Then, an algorithm, termed MST-Expansion,\nis proposed to extract the match graph from the TCN where the TCN is firstly\nsimplified by a maximum spanning tree (MST). By further analysis of local\nstructure in the MST, expansion operations are performed on the nodes of the\nMST for match graph enhancement, which is achieved by introducing critical\nconnections in two expansion directions. Finally, guided by the match graph, an\nefficient SfM solution is proposed, and its validation is verified through\ncomprehensive analysis and comparison using three UAV datasets captured with\ndifferent oblique multi-camera systems. Experiment results demonstrate that the\nefficiency of image matching is improved with a speedup ratio ranging from 19\nto 35, and competitive orientation accuracy is achieved from both relative\nbundle adjustment (BA) without GCPs (Ground Control Points) and absolute BA\nwith GCPs. At the same time, images in the three datasets are successfully\noriented. For orientation of oblique UAV images, the proposed method can be a\nmore efficient solution.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 07:22:23 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Jiang", "San", ""], ["Jiang", "Wanshou", ""]]}, {"id": "1705.03239", "submitter": "Vardan Papyan", "authors": "Vardan Papyan, Yaniv Romano, Jeremias Sulam and Michael Elad", "title": "Convolutional Dictionary Learning via Local Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Sparse Coding (CSC) is an increasingly popular model in the\nsignal and image processing communities, tackling some of the limitations of\ntraditional patch-based sparse representations. Although several works have\naddressed the dictionary learning problem under this model, these relied on an\nADMM formulation in the Fourier domain, losing the sense of locality and the\nrelation to the traditional patch-based sparse pursuit. A recent work suggested\na novel theoretical analysis of this global model, providing guarantees that\nrely on a localized sparsity measure. Herein, we extend this local-global\nrelation by showing how one can efficiently solve the convolutional sparse\npursuit problem and train the filters involved, while operating locally on\nimage patches. Our approach provides an intuitive algorithm that can leverage\nstandard techniques from the sparse representations field. The proposed method\nis fast to train, simple to implement, and flexible enough that it can be\neasily deployed in a variety of applications. We demonstrate the proposed\ntraining scheme for image inpainting and image separation, while achieving\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 09:07:59 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Papyan", "Vardan", ""], ["Romano", "Yaniv", ""], ["Sulam", "Jeremias", ""], ["Elad", "Michael", ""]]}, {"id": "1705.03255", "submitter": "Alessandro Niccolai", "authors": "Stefano Frassinelli, Alessandro Niccolai, Riccardo E. Zich", "title": "Diving Performance Assessment by means of Video Processing", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.02854", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a procedure for video analysis applied in\nan innovative way to diving performance assessment. Sport performance analysis\nis a trend that is growing exponentially for all level athletes. The technique\nhere shown is based on two important requirements: flexibility and low cost.\nThese two requirements lead to many problems in the video processing that have\nbeen faced and solved in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:05:07 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Frassinelli", "Stefano", ""], ["Niccolai", "Alessandro", ""], ["Zich", "Riccardo E.", ""]]}, {"id": "1705.03281", "submitter": "Mohamed Elgharib", "authors": "Ahmed Hassanien, Mohamed Elgharib, Ahmed Selim, Sung-Ho Bae, Mohamed\n  Hefeeda and Wojciech Matusik", "title": "Large-scale, Fast and Accurate Shot Boundary Detection through\n  Spatio-temporal Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shot boundary detection (SBD) is an important pre-processing step for video\nmanipulation. Here, each segment of frames is classified as either sharp,\ngradual or no transition. Current SBD techniques analyze hand-crafted features\nand attempt to optimize both detection accuracy and processing speed. However,\nthe heavy computations of optical flow prevents this. To achieve this aim, we\npresent an SBD technique based on spatio-temporal Convolutional Neural Networks\n(CNN). Since current datasets are not large enough to train an accurate SBD\nCNN, we present a new dataset containing more than 3.5 million frames of sharp\nand gradual transitions. The transitions are generated synthetically using\nimage compositing models. Our dataset contain additional 70,000 frames of\nimportant hard-negative no transitions. We perform the largest evaluation to\ndate for one SBD algorithm, on real and synthetic data, containing more than\n4.85 million frames. In comparison to the state of the art, we outperform\ndissolve gradual detection, generate competitive performance for sharp\ndetections and produce significant improvement in wipes. In addition, we are up\nto 11 times faster than the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 11:37:25 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 07:10:33 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Hassanien", "Ahmed", ""], ["Elgharib", "Mohamed", ""], ["Selim", "Ahmed", ""], ["Bae", "Sung-Ho", ""], ["Hefeeda", "Mohamed", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1705.03311", "submitter": "Tobias Gr\\\"uning", "authors": "Tobias Gr\\\"uning (1), Roger Labahn (1), Markus Diem (2), Florian\n  Kleber (2), Stefan Fiel (2) ((1) University of Rostock - CITlab, (2) TU Wien\n  - Computer Vision Lab)", "title": "READ-BAD: A New Dataset and Evaluation Scheme for Baseline Detection in\n  Archival Documents", "comments": "Submitted to DAS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text line detection is crucial for any application associated with Automatic\nText Recognition or Keyword Spotting. Modern algorithms perform good on\nwell-established datasets since they either comprise clean data or\nsimple/homogeneous page layouts. We have collected and annotated 2036 archival\ndocument images from different locations and time periods. The dataset contains\nvarying page layouts and degradations that challenge text line segmentation\nmethods. Well established text line segmentation evaluation schemes such as the\nDetection Rate or Recognition Accuracy demand for binarized data that is\nannotated on a pixel level. Producing ground truth by these means is laborious\nand not needed to determine a method's quality. In this paper we propose a new\nevaluation scheme that is based on baselines. The proposed scheme has no need\nfor binarization and it can handle skewed as well as rotated text lines. The\nICDAR 2017 Competition on Baseline Detection and the ICDAR 2017 Competition on\nLayout Analysis for Challenging Medieval Manuscripts used this evaluation\nscheme. Finally, we present results achieved by a recently published text line\ndetection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 13:19:39 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 08:15:20 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Gr\u00fcning", "Tobias", ""], ["Labahn", "Roger", ""], ["Diem", "Markus", ""], ["Kleber", "Florian", ""], ["Fiel", "Stefan", ""]]}, {"id": "1705.03332", "submitter": "Haibo Jin", "authors": "Haibo Jin, Xiaobo Wang, Shengcai Liao, Stan Z. Li", "title": "Deep Person Re-Identification with Improved Embedding and Efficient\n  Training", "comments": "Accepted by IJCB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification task has been greatly boosted by deep convolutional\nneural networks (CNNs) in recent years. The core of which is to enlarge the\ninter-class distinction as well as reduce the intra-class variance. However, to\nachieve this, existing deep models prefer to adopt image pairs or triplets to\nform verification loss, which is inefficient and unstable since the number of\ntraining pairs or triplets grows rapidly as the number of training data grows.\nMoreover, their performance is limited since they ignore the fact that\ndifferent dimension of embedding may play different importance. In this paper,\nwe propose to employ identification loss with center loss to train a deep model\nfor person re-identification. The training process is efficient since it does\nnot require image pairs or triplets for training while the inter-class\ndistinction and intra-class variance are well handled. To boost the\nperformance, a new feature reweighting (FRW) layer is designed to explicitly\nemphasize the importance of each embedding dimension, thus leading to an\nimproved embedding. Experiments on several benchmark datasets have shown the\nsuperiority of our method over the state-of-the-art alternatives on both\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 13:47:59 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 06:21:06 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 11:05:17 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Jin", "Haibo", ""], ["Wang", "Xiaobo", ""], ["Liao", "Shengcai", ""], ["Li", "Stan Z.", ""]]}, {"id": "1705.03338", "submitter": "Atul Dhingra", "authors": "Atul Dhingra", "title": "Model Complexity-Accuracy Trade-off for a Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNN) has had a great success in the recent\npast, because of the advent of faster GPUs and memory access. CNNs are really\npowerful as they learn the features from data in layers such that they exhibit\nthe structure of the V-1 features of the human brain. A huge bottleneck, in\nthis case, is that CNNs are very large and have a very high memory footprint,\nand hence they cannot be employed on devices with limited storage such as\nmobile phone, IoT etc. In this work, we study the model complexity versus\naccuracy trade-off on MNSIT dataset, and give a concrete framework for handling\nsuch a problem, given the worst case accuracy that a system can tolerate. In\nour work, we reduce the model complexity by 236 times, and memory footprint by\n19.5 times compared to the base model while achieving worst case accuracy\nthreshold.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 14:04:45 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Dhingra", "Atul", ""]]}, {"id": "1705.03350", "submitter": "Ja-Keoung Koo", "authors": "Byung-Woo Hong, Ja-Keoung Koo, Martin Burger and Stefano Soatto", "title": "Adaptive Regularization of Some Inverse Problems in Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive regularization scheme for optimizing composite energy\nfunctionals arising in image analysis problems. The scheme automatically trades\noff data fidelity and regularization depending on the current data fit during\nthe iterative optimization, so that regularization is strongest initially, and\nwanes as data fidelity improves, with the weight of the regularizer being\nminimized at convergence. We also introduce the use of a Huber loss function in\nboth data fidelity and regularization terms, and present an efficient convex\noptimization algorithm based on the alternating direction method of multipliers\n(ADMM) using the equivalent relation between the Huber function and the\nproximal operator of the one-norm. We illustrate and validate our adaptive\nHuber-Huber model on synthetic and real images in segmentation, motion\nestimation, and denoising problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 14:27:58 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Hong", "Byung-Woo", ""], ["Koo", "Ja-Keoung", ""], ["Burger", "Martin", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.03360", "submitter": "Balazs Harangi Ph.D", "authors": "Balazs Harangi", "title": "Skin lesion detection based on an ensemble of deep convolutional neural\n  network", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbi.2018.08.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is a major public health problem, with over 5 million newly\ndiagnosed cases in the United States each year. Melanoma is the deadliest form\nof skin cancer, responsible for over 9,000 deaths each year. In this paper, we\npropose an ensemble of deep convolutional neural networks to classify\ndermoscopy images into three classes. To achieve the highest classification\naccuracy, we fuse the outputs of the softmax layers of four different neural\narchitectures. For aggregation, we consider the individual accuracies of the\nnetworks weighted by the confidence values provided by their final softmax\nlayers. This fusion-based approach outperformed all the individual neural\nnetworks regarding classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 14:43:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Harangi", "Balazs", ""]]}, {"id": "1705.03372", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang", "title": "Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation", "comments": "iccv 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of localisation of objects as bounding boxes in images\nwith weak labels. This weakly supervised object localisation problem has been\ntackled in the past using discriminative models where each object class is\nlocalised independently from other classes. We propose a novel framework based\non Bayesian joint topic modelling. Our framework has three distinctive\nadvantages over previous works: (1) All object classes and image backgrounds\nare modelled jointly together in a single generative model so that \"explaining\naway\" inference can resolve ambiguity and lead to better learning and\nlocalisation. (2) The Bayesian formulation of the model enables easy\nintegration of prior knowledge about object appearance to compensate for\nlimited supervision. (3) Our model can be learned with a mixture of weakly\nlabelled and unlabelled data, allowing the large volume of unlabelled images on\nthe Internet to be exploited for learning. Extensive experiments on the\nchallenging VOC dataset demonstrate that our approach outperforms the\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:00:07 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1705.03386", "submitter": "Saad Ullah Akram", "authors": "Saad Ullah Akram, Juho Kannala, Lauri Eklund, Janne Heikkil\\\"a", "title": "Cell Tracking via Proposal Generation and Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy imaging plays a vital role in understanding many biological\nprocesses in development and disease. The recent advances in automation of\nmicroscopes and development of methods and markers for live cell imaging has\nled to rapid growth in the amount of image data being captured. To efficiently\nand reliably extract useful insights from these captured sequences, automated\ncell tracking is essential. This is a challenging problem due to large\nvariation in the appearance and shapes of cells depending on many factors\nincluding imaging methodology, biological characteristics of cells, cell matrix\ncomposition, labeling methodology, etc. Often cell tracking methods require a\nsequence-specific segmentation method and manual tuning of many tracking\nparameters, which limits their applicability to sequences other than those they\nare designed for. In this paper, we propose 1) a deep learning based cell\nproposal method, which proposes candidates for cells along with their scores,\nand 2) a cell tracking method, which links proposals in adjacent frames in a\ngraphical model using edges representing different cellular events and poses\njoint cell detection and tracking as the selection of a subset of cell and edge\nproposals. Our method is completely automated and given enough training data\ncan be applied to a wide variety of microscopy sequences. We evaluate our\nmethod on multiple fluorescence and phase contrast microscopy sequences\ncontaining cells of various shapes and appearances from ISBI cell tracking\nchallenge, and show that our method outperforms existing cell tracking methods.\n  Code is available at: https://github.com/SaadUllahAkram/CellTracker\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:30:39 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Akram", "Saad Ullah", ""], ["Kannala", "Juho", ""], ["Eklund", "Lauri", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1705.03419", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Matthew Nokleby and Xuewen Chen", "title": "Learning Deep Networks from Noisy Labels with Dropout Regularization", "comments": "Published at 2016 IEEE 16th International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets often have unreliable labels-such as those obtained from\nAmazon's Mechanical Turk or social media platforms-and classifiers trained on\nmislabeled datasets often exhibit poor performance. We present a simple,\neffective technique for accounting for label noise when training deep neural\nnetworks. We augment a standard deep network with a softmax layer that models\nthe label noise statistics. Then, we train the deep network and noise model\njointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)\ndataset. The augmented model is overdetermined, so in order to encourage the\nlearning of a non-trivial noise model, we apply dropout regularization to the\nweights of the noise model during training. Numerical experiments on noisy\nversions of the CIFAR-10 and MNIST datasets show that the proposed dropout\ntechnique outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:42:32 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Jindal", "Ishan", ""], ["Nokleby", "Matthew", ""], ["Chen", "Xuewen", ""]]}, {"id": "1705.03428", "submitter": "Felix J\\\"aremo Lawin", "authors": "Felix J\\\"aremo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat,\n  Fahad Shahbaz Khan, Michael Felsberg", "title": "Deep Projective 3D Semantic Segmentation", "comments": "Submitted to CAIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of 3D point clouds is a challenging problem with\nnumerous real-world applications. While deep learning has revolutionized the\nfield of image semantic segmentation, its impact on point cloud data has been\nlimited so far. Recent attempts, based on 3D deep learning approaches\n(3D-CNNs), have achieved below-expected results. Such methods require\nvoxelizations of the underlying point cloud data, leading to decreased spatial\nresolution and increased memory consumption. Additionally, 3D-CNNs greatly\nsuffer from the limited availability of annotated datasets.\n  In this paper, we propose an alternative framework that avoids the\nlimitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first\nproject the point cloud onto a set of synthetic 2D-images. These images are\nthen used as input to a 2D-CNN, designed for semantic segmentation. Finally,\nthe obtained prediction scores are re-projected to the point cloud to obtain\nthe segmentation results. We further investigate the impact of multiple\nmodalities, such as color, depth and surface normals, in a multi-stream network\narchitecture. Experiments are performed on the recent Semantic3D dataset. Our\napproach sets a new state-of-the-art by achieving a relative gain of 7.9 %,\ncompared to the previous best approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:59:41 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lawin", "Felix J\u00e4remo", ""], ["Danelljan", "Martin", ""], ["Tosteberg", "Patrik", ""], ["Bhat", "Goutam", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1705.03493", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev, Alexander Malyshev", "title": "Signal reconstruction via operator guiding", "comments": "5 pages, 8 figures. To appear in Proceedings of SampTA 2017: Sampling\n  Theory and Applications, 12th International Conference, July 3-7, 2017,\n  Tallinn, Estonia", "journal-ref": "IEEE Xplore: 2017 International Conference on Sampling Theory and\n  Applications (SampTA), Tallin, Estonia, 2017, pp. 630-634", "doi": "10.1109/SAMPTA.2017.8024424", "report-no": "MERL TR2017-087", "categories": "cs.CV cs.GR cs.IT cs.NA math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal reconstruction from a sample using an orthogonal projector onto a\nguiding subspace is theoretically well justified, but may be difficult to\npractically implement. We propose more general guiding operators, which\nincrease signal components in the guiding subspace relative to those in a\ncomplementary subspace, e.g., iterative low-pass edge-preserving filters for\nsuper-resolution of images. Two examples of super-resolution illustrate our\ntechnology: a no-flash RGB photo guided using a high resolution flash RGB\nphoto, and a depth image guided using a high resolution RGB photo.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 19:06:16 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Knyazev", "Andrew", ""], ["Malyshev", "Alexander", ""]]}, {"id": "1705.03524", "submitter": "Mahdieh Poostchi", "authors": "Mahdieh Poostchi, Ali Shafiekhani, Kannappan Palaniappan, Guna\n  Seetharaman", "title": "Multi-Scale Spatially Weighted Local Histograms in O(1)", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighting pixel contribution considering its location is a key feature in\nmany fundamental image processing tasks including filtering, object modeling\nand distance matching. Several techniques have been proposed that incorporate\nSpatial information to increase the accuracy and boost the performance of\ndetection, tracking and recognition systems at the cost of speed. But, it is\nstill not clear how to efficiently ex- tract weighted local histograms in\nconstant time using integral histogram. This paper presents a novel algorithm\nto compute accurately multi-scale Spatially weighted local histograms in\nconstant time using Weighted Integral Histogram (SWIH) for fast search. We\napplied our spatially weighted integral histogram approach for fast tracking\nand obtained more accurate and robust target localization result in comparison\nwith using plain histogram.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:08:11 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Poostchi", "Mahdieh", ""], ["Shafiekhani", "Ali", ""], ["Palaniappan", "Kannappan", ""], ["Seetharaman", "Guna", ""]]}, {"id": "1705.03550", "submitter": "Vincenzo Lomonaco", "authors": "Vincenzo Lomonaco and Davide Maltoni", "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous/Lifelong learning of high-dimensional data streams is a\nchallenging research problem. In fact, fully retraining models each time new\ndata become available is infeasible, due to computational and storage issues,\nwhile na\\\"ive incremental strategies have been shown to suffer from\ncatastrophic forgetting. In the context of real-world object recognition\napplications (e.g., robotic vision), where continuous learning is crucial, very\nfew datasets and benchmarks are available to evaluate and compare emerging\ntechniques. In this work we propose a new dataset and benchmark CORe50,\nspecifically designed for continuous object recognition, and introduce baseline\napproaches for different continuous learning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 21:32:19 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""]]}, {"id": "1705.03572", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery\n  for Pathologically-Proven Lung Cancer Detection", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While lung cancer is the second most diagnosed form of cancer in men and\nwomen, a sufficiently early diagnosis can be pivotal in patient survival rates.\nImaging-based, or radiomics-driven, detection methods have been developed to\naid diagnosticians, but largely rely on hand-crafted features which may not\nfully encapsulate the differences between cancerous and healthy tissue.\nRecently, the concept of discovery radiomics was introduced, where custom\nabstract features are discovered from readily available imaging data. We\npropose a novel evolutionary deep radiomic sequencer discovery approach based\non evolutionary deep intelligence. Motivated by patient privacy concerns and\nthe idea of operational artificial intelligence, the evolutionary deep radiomic\nsequencer discovery approach organically evolves increasingly more efficient\ndeep radiomic sequencers that produce significantly more compact yet similarly\ndescriptive radiomic sequences over multiple generations. As a result, this\nframework improves operational efficiency and enables diagnosis to be run\nlocally at the radiologist's computer while maintaining detection accuracy. We\nevaluated the evolved deep radiomic sequencer (EDRS) discovered via the\nproposed evolutionary deep radiomic sequencer discovery framework against\nstate-of-the-art radiomics-driven and discovery radiomics methods using\nclinical lung CT data with pathologically-proven diagnostic data from the\nLIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved\nsensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%)\nrelative to previous radiomics approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 00:20:23 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 00:01:27 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1705.03595", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Kaori Abe, Akio Nakamura, Yutaka Satoh", "title": "Collaborative Descriptors: Convolutional Maps for Preprocessing", "comments": "CVPR 2017 Workshop Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel concept for collaborative descriptors between\ndeeply learned and hand-crafted features. To achieve this concept, we apply\nconvolutional maps for pre-processing, namely the convovlutional maps are used\nas input of hand-crafted features. We recorded an increase in the performance\nrate of +17.06 % (multi-class object recognition) and +24.71 % (car detection)\nfrom grayscale input to convolutional maps. Although the framework is\nstraight-forward, the concept should be inherited for an improved\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:04:48 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Abe", "Kaori", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1705.03607", "submitter": "Riku Shigematsu", "authors": "Riku Shigematsu, David Feng, Shaodi You, Nick Barnes", "title": "Learning RGB-D Salient Object Detection using background enclosure,\n  depth contrast, and top-down features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep Convolutional Neural Networks (CNN) have demonstrated strong\nperformance on RGB salient object detection. Although, depth information can\nhelp improve detection results, the exploration of CNNs for RGB-D salient\nobject detection remains limited. Here we propose a novel deep CNN architecture\nfor RGB-D salient object detection that exploits high-level, mid-level, and low\nlevel features. Further, we present novel depth features that capture the ideas\nof background enclosure and depth contrast that are suitable for a learned\napproach. We show improved results compared to state-of-the-art RGB-D salient\nobject detection methods. We also show that the low-level and mid-level depth\nfeatures both contribute to improvements in the results. Especially, F-Score of\nour method is 0.848 on RGBD1000 dataset, which is 10.7% better than the second\nplace.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 05:12:45 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Shigematsu", "Riku", ""], ["Feng", "David", ""], ["You", "Shaodi", ""], ["Barnes", "Nick", ""]]}, {"id": "1705.03633", "submitter": "Justin Johnson", "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy\n  Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick", "title": "Inferring and Executing Programs for Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:08:23 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Johnson", "Justin", ""], ["Hariharan", "Bharath", ""], ["van der Maaten", "Laurens", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""], ["Zitnick", "C. Lawrence", ""], ["Girshick", "Ross", ""]]}, {"id": "1705.03634", "submitter": "Shirui Li", "authors": "Shirui Li, Alper Yilmaz, Changlin Xiao, Hua Li", "title": "4d isip: 4d implicit surface interest point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to detect 4D spatiotemporal interest\npoints though an implicit surface, we refer to as the 4D-ISIP. We use a 3D\nvolume which has a truncated signed distance function(TSDF) for every voxel to\nrepresent our 3D object model. The TSDF represents the distance between the\nspatial points and object surface points which is an implicit surface\nrepresentation. Our novelty is to detect the points where the local\nneighborhood has significant variations along both spatial and temporal\ndirections. We established a system to acquire 3D human motion dataset using\nonly one Kinect. Experimental results show that our method can detect 4D-ISIP\nfor different human actions.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:12:31 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 07:18:54 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Li", "Shirui", ""], ["Yilmaz", "Alper", ""], ["Xiao", "Changlin", ""], ["Li", "Hua", ""]]}, {"id": "1705.03678", "submitter": "Babak Ehteshami Bejnordi", "authors": "Babak Ehteshami Bejnordi, Guido Zuidhof, Maschenka Balkenhol, Meyke\n  Hermsen, Peter Bult, Bram van Ginneken, Nico Karssemeijer, Geert Litjens,\n  Jeroen van der Laak", "title": "Context-aware stacked convolutional neural networks for classification\n  of breast carcinomas in whole-slide histopathology images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated classification of histopathological whole-slide images (WSI) of\nbreast tissue requires analysis at very high resolutions with a large\ncontextual area. In this paper, we present context-aware stacked convolutional\nneural networks (CNN) for classification of breast WSIs into normal/benign,\nductal carcinoma in situ (DCIS), and invasive ductal carcinoma (IDC). We first\ntrain a CNN using high pixel resolution patches to capture cellular level\ninformation. The feature responses generated by this model are then fed as\ninput to a second CNN, stacked on top of the first. Training of this stacked\narchitecture with large input patches enables learning of fine-grained\n(cellular) details and global interdependence of tissue structures. Our system\nis trained and evaluated on a dataset containing 221 WSIs of H&E stained breast\ntissue specimens. The system achieves an AUC of 0.962 for the binary\nclassification of non-malignant and malignant slides and obtains a three class\naccuracy of 81.3% for classification of WSIs into normal/benign, DCIS, and IDC,\ndemonstrating its potentials for routine diagnostics.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:05:06 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Bejnordi", "Babak Ehteshami", ""], ["Zuidhof", "Guido", ""], ["Balkenhol", "Maschenka", ""], ["Hermsen", "Meyke", ""], ["Bult", "Peter", ""], ["van Ginneken", "Bram", ""], ["Karssemeijer", "Nico", ""], ["Litjens", "Geert", ""], ["van der Laak", "Jeroen", ""]]}, {"id": "1705.03737", "submitter": "Sung-Ho Bae", "authors": "Sung-Ho Bae, Mohamed Elgharib, Mohamed Hefeeda, Wojciech Matusik", "title": "Efficient and Scalable View Generation from a Single Image using Fully\n  Convolutional Networks", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image-based view generation (SIVG) is important for producing 3D\nstereoscopic content. Here, handling different spatial resolutions as input and\noptimizing both reconstruction accuracy and processing speed is desirable.\nLatest approaches are based on convolutional neural network (CNN), and they\ngenerate promising results. However, their use of fully connected layers as\nwell as pre-trained VGG forces a compromise between reconstruction accuracy and\nprocessing speed. In addition, this approach is limited to the use of a\nspecific spatial resolution. To remedy these problems, we propose exploiting\nfully convolutional networks (FCN) for SIVG. We present two FCN architectures\nfor SIVG. The first one is based on combination of an FCN and a view-rendering\nnetwork called DeepView$_{ren}$. The second one consists of decoupled networks\nfor luminance and chrominance signals, denoted by DeepView$_{dec}$. To train\nour solutions we present a large dataset of 2M stereoscopic images. Results\nshow that both of our architectures improve accuracy and speed over the state\nof the art. DeepView$_{ren}$ generates competitive accuracy to the state of the\nart, however, with the fastest processing speed of all. That is x5 times faster\nspeed and x24 times lower memory consumption compared to the state of the art.\nDeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and\nx12 times lower memory consumption. We evaluated our approach with both\nobjective and subjective studies.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:02:24 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 06:28:05 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 23:30:05 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bae", "Sung-Ho", ""], ["Elgharib", "Mohamed", ""], ["Hefeeda", "Mohamed", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1705.03820", "submitter": "Hao Dong", "authors": "Hao Dong, Guang Yang, Fangde Liu, Yuanhan Mo, Yike Guo", "title": "Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully\n  Convolutional Networks", "comments": "Medical Image Understanding and Analysis (MIUA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in brain tumor treatment planning and quantitative\nevaluation is determination of the tumor extent. The noninvasive magnetic\nresonance imaging (MRI) technique has emerged as a front-line diagnostic tool\nfor brain tumors without ionizing radiation. Manual segmentation of brain tumor\nextent from 3D MRI volumes is a very time-consuming task and the performance is\nhighly relied on operator's experience. In this context, a reliable fully\nautomatic segmentation method for the brain tumor segmentation is necessary for\nan efficient measurement of the tumor extent. In this study, we propose a fully\nautomatic method for brain tumor segmentation, which is developed using U-Net\nbased deep convolutional networks. Our method was evaluated on Multimodal Brain\nTumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade\nbrain tumor and 54 low-grade tumor cases. Cross-validation has shown that our\nmethod can obtain promising segmentation efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:29:52 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 13:03:43 GMT"}, {"version": "v3", "created": "Sat, 3 Jun 2017 22:47:47 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Dong", "Hao", ""], ["Yang", "Guang", ""], ["Liu", "Fangde", ""], ["Mo", "Yuanhan", ""], ["Guo", "Yike", ""]]}, {"id": "1705.03854", "submitter": "Davide Abati", "authors": "Andrea Palazzi, Davide Abati, Simone Calderara, Francesco Solera, Rita\n  Cucchiara", "title": "Predicting the Driver's Focus of Attention: the DR(eye)VE Project", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we aim to predict the driver's focus of attention. The goal is\nto estimate what a person would pay attention to while driving, and which part\nof the scene around the vehicle is more critical for the task. To this end we\npropose a new computer vision model based on a multi-branch deep architecture\nthat integrates three sources of information: raw video, motion and scene\nsemantics. We also introduce DR(eye)VE, the largest dataset of driving scenes\nfor which eye-tracking annotations are available. This dataset features more\nthan 500,000 registered frames, matching ego-centric views (from glasses worn\nby drivers) and car-centric views (from roof-mounted camera), further enriched\nby other sensors measurements. Results highlight that several attention\npatterns are shared across drivers and can be reproduced to some extent. The\nindication of which elements in the scene are likely to capture the driver's\nattention may benefit several applications in the context of human-vehicle\ninteraction and driver attention analysis.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:00:14 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 15:17:59 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 08:54:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Palazzi", "Andrea", ""], ["Abati", "Davide", ""], ["Calderara", "Simone", ""], ["Solera", "Francesco", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1705.03865", "submitter": "Akshay Gupta", "authors": "Akshay Kumar Gupta", "title": "Survey of Visual Question Answering: Datasets and Techniques", "comments": "10 pages, 3 figures, 3 tables Added references, corrected typos, made\n  references less wordy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:30:17 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 06:46:52 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Gupta", "Akshay Kumar", ""]]}, {"id": "1705.03933", "submitter": "Angana Chakraborty", "authors": "Angana Chakraborty and Sanghamitra Bandyopadhyay", "title": "An Improved Video Analysis using Context based Extension of LSH", "comments": "The work us regarding a conceptual phenomenon which is found no\n  longer valid in Computer Vision (we are not continuing this work). Rather we\n  want to apply it to Bioinformatics (different domain ). Therefore, it will\n  not be a replacement of this submission. As, a significant portion of this\n  submission is not valid and fullproof, we want to withdraw this submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality Sensitive Hashing (LSH) based algorithms have already shown their\npromise in finding approximate nearest neighbors in high dimen- sional data\nspace. However, there are certain scenarios, as in sequential data, where the\nproximity of a pair of points cannot be captured without considering their\nsurroundings or context. In videos, as for example, a particular frame is\nmeaningful only when it is seen in the context of its preceding and following\nframes. LSH has no mechanism to handle the con- texts of the data points. In\nthis article, a novel scheme of Context based Locality Sensitive Hashing\n(conLSH) has been introduced, in which points are hashed together not only\nbased on their closeness, but also because of similar context. The contribution\nmade in this article is three fold. First, conLSH is integrated with a recently\nproposed fast optimal sequence alignment algorithm (FOGSAA) using a layered\napproach. The resultant method is applied to video retrieval for extracting\nsimilar sequences. The pro- posed algorithm yields more than 80% accuracy on an\naverage in different datasets. It has been found to save 36.3% of the total\ntime, consumed by the exhaustive search. conLSH reduces the search space to\napproximately 42% of the entire dataset, when compared with an exhaustive\nsearch by the aforementioned FOGSAA, Bag of Words method and the standard LSH\nimplementations. Secondly, the effectiveness of conLSH is demon- strated in\naction recognition of the video clips, which yields an average gain of 12.83%\nin terms of classification accuracy over the state of the art methods using\nSTIP descriptors. The last but of great significance is that this article\nprovides a way of automatically annotating long and composite real life videos.\nThe source code of conLSH is made available at\nhttp://www.isical.ac.in/~bioinfo_miu/conLSH/conLSH.html\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 19:42:51 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 13:49:03 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Chakraborty", "Angana", ""], ["Bandyopadhyay", "Sanghamitra", ""]]}, {"id": "1705.03951", "submitter": "David Novotn\\'y", "authors": "David Novotny, Diane Larlus, Andrea Vedaldi", "title": "Learning 3D Object Categories by Looking Around Them", "comments": "Proceedings of the International Conference on Computer Vision, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for learning 3D object categories use either synthetic\ndata or manual supervision. In this paper, we propose a method which does not\nrequire manual annotations and is instead cued by observing objects from a\nmoving vantage point. Our system builds on two innovations: a Siamese viewpoint\nfactorization network that robustly aligns different videos together without\nexplicitly comparing 3D shapes; and a 3D shape completion network that can\nextract the full shape of an object from partial observations. We also\ndemonstrate the benefits of configuring networks to perform probabilistic\npredictions as well as of geometry-aware data augmentation schemes. We obtain\nstate-of-the-art results on publicly-available benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 21:01:39 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 20:21:31 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Novotny", "David", ""], ["Larlus", "Diane", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1705.03986", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Distribution of degrees of freedom over structure and motion of rigid\n  bodies", "comments": "20 pages, 7 figures", "journal-ref": "Machine Graphics and Vision, 1995, Vol. 4, No 1-2 (preliminary\n  version)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with recovery of motion and structure parameters from\nmultiframes under orthogonal projection when only points are traced. The main\nquestion is how many points and/or how many frames are necessary for the task.\nIt is demonstrated that 3 frames and 3 points are the absolute minimum.\nClosed-form solution is presented. Furthermore, it is shown that the task may\nbe linearized if either four points or four frames are available. It is\ndemonstrated that no increase in the number of points may lead to recovery of\nstructure and motion parameters from two frames only. It is shown that instead\nthe increase in the number of points may support the task of tracing the points\nfrom frame to frame.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 01:53:18 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1705.04043", "submitter": "Kai Han", "authors": "Kai Han, Rafael S. Rezende, Bumsub Ham, Kwan-Yee K. Wong, Minsu Cho,\n  Cordelia Schmid, Jean Ponce", "title": "SCNet: Learning Semantic Correspondence", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of establishing semantic correspondences\nbetween images depicting different instances of the same object or scene\ncategory. Previous approaches focus on either combining a spatial regularizer\nwith hand-crafted features, or learning a correspondence model for appearance\nonly. We propose instead a convolutional neural network architecture, called\nSCNet, for learning a geometrically plausible model for semantic\ncorrespondence. SCNet uses region proposals as matching primitives, and\nexplicitly incorporates geometric consistency in its loss function. It is\ntrained on image pairs obtained from the PASCAL VOC 2007 keypoint dataset, and\na comparative evaluation on several standard benchmarks demonstrates that the\nproposed approach substantially outperforms both recent deep learning\narchitectures and previous methods based on hand-crafted features.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 07:30:36 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 12:42:03 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 13:25:25 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Han", "Kai", ""], ["Rezende", "Rafael S.", ""], ["Ham", "Bumsub", ""], ["Wong", "Kwan-Yee K.", ""], ["Cho", "Minsu", ""], ["Schmid", "Cordelia", ""], ["Ponce", "Jean", ""]]}, {"id": "1705.04058", "submitter": "Yongcheng Jing", "authors": "Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu,\n  Mingli Song", "title": "Neural Style Transfer: A Review", "comments": "Project page: https://github.com/ycjing/Neural-Style-Transfer-Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 08:08:44 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:21:36 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 13:25:07 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 12:35:19 GMT"}, {"version": "v5", "created": "Wed, 16 May 2018 11:59:51 GMT"}, {"version": "v6", "created": "Sun, 17 Jun 2018 08:40:41 GMT"}, {"version": "v7", "created": "Tue, 30 Oct 2018 09:48:05 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jing", "Yongcheng", ""], ["Yang", "Yezhou", ""], ["Feng", "Zunlei", ""], ["Ye", "Jingwen", ""], ["Yu", "Yizhou", ""], ["Song", "Mingli", ""]]}, {"id": "1705.04085", "submitter": "Jorge Beltr\\'an", "authors": "Carlos Guindel, Jorge Beltr\\'an, David Mart\\'in and Fernando Garc\\'ia", "title": "Automatic Extrinsic Calibration for Lidar-Stereo Vehicle Sensor Setups", "comments": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems 2017 (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor setups consisting of a combination of 3D range scanner lasers and\nstereo vision systems are becoming a popular choice for on-board perception\nsystems in vehicles; however, the combined use of both sources of information\nimplies a tedious calibration process. We present a method for extrinsic\ncalibration of lidar-stereo camera pairs without user intervention. Our\ncalibration approach is aimed to cope with the constraints commonly found in\nautomotive setups, such as low-resolution and specific sensor poses. To\ndemonstrate the performance of our method, we also introduce a novel approach\nfor the quantitative assessment of the calibration results, based on a\nsimulation environment. Tests using real devices have been conducted as well,\nproving the usability of the system and the improvement over the existing\napproaches. Code is available at http://wiki.ros.org/velo2cam_calibration\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 09:27:59 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 08:28:51 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 13:54:46 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Guindel", "Carlos", ""], ["Beltr\u00e1n", "Jorge", ""], ["Mart\u00edn", "David", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "1705.04098", "submitter": "Christoph Lassner", "authors": "Christoph Lassner and Gerard Pons-Moll and Peter V. Gehler", "title": "A Generative Model of People in Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first image-based generative model of people in clothing for\nthe full body. We sidestep the commonly used complex graphics rendering\npipeline and the need for high-quality 3D scans of dressed people. Instead, we\nlearn generative models from a large image database. The main challenge is to\ncope with the high variance in human pose, shape and appearance. For this\nreason, pure image-based approaches have not been considered so far. We show\nthat this challenge can be overcome by splitting the generating process in two\nparts. First, we learn to generate a semantic segmentation of the body and\nclothing. Second, we learn a conditional model on the resulting segments that\ncreates realistic images. The full model is differentiable and can be\nconditioned on pose, shape or color. The result are samples of people in\ndifferent clothing items and styles. The proposed model can generate entirely\nnew people with realistic clothing. In several experiments we present\nencouraging results that suggest an entirely data-driven approach to people\ngeneration is possible.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 10:07:09 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 14:21:09 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 16:09:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lassner", "Christoph", ""], ["Pons-Moll", "Gerard", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1705.04114", "submitter": "Akkas Uddin Haque", "authors": "Akkas Uddin Haque and Ashkan Nejadpak", "title": "Obstacle Avoidance Using Stereo Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel method for obstacle avoidance using the\nstereo camera. The conventional obstacle avoidance methods and their\nlimitations are discussed. A new algorithm is developed for the real-time\nobstacle avoidance which responds faster to unexpected obstacles. In this\napproach the depth map is divided into optimized number of regions and the\nminimum depth at each section is assigned as the depth of that region. A fuzzy\ncontroller is designed to create the drive commands for the robot/quadcopter.\nThe system was tested on multiple paths with different obstacles and the\nresults demonstrated the high accuracy of the developed system.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 11:34:27 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 16:46:57 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Haque", "Akkas Uddin", ""], ["Nejadpak", "Ashkan", ""]]}, {"id": "1705.04228", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, John K. Tsotsos", "title": "Incremental Learning Through Deep Adaptation", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an existing trained neural network, it is often desirable to learn new\ncapabilities without hindering performance of those already learned. Existing\napproaches either learn sub-optimal solutions, require joint training, or incur\na substantial increment in the number of parameters for each added domain,\ntypically as many as the original network. We propose a method called\n\\emph{Deep Adaptation Networks} (DAN) that constrains newly learned filters to\nbe linear combinations of existing ones. DANs precisely preserve performance on\nthe original domain, require a fraction (typically 13\\%, dependent on network\narchitecture) of the number of parameters compared to standard fine-tuning\nprocedures and converge in less cycles of training to a comparable or better\nlevel of performance. When coupled with standard network quantization\ntechniques, we further reduce the parameter cost to around 3\\% of the original\nwith negligible or no loss in accuracy. The learned architecture can be\ncontrolled to switch between various learned representations, enabling a single\nnetwork to solve a task from multiple different domains. We conduct extensive\nexperiments showing the effectiveness of our method on a range of image\nclassification tasks and explore different aspects of its behavior.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 15:04:10 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 19:41:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1705.04258", "submitter": "Alexander Kolesnikov", "authors": "Amelie Royer, Alexander Kolesnikov, Christoph H. Lampert", "title": "Probabilistic Image Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic technique for colorizing grayscale natural images.\nIn light of the intrinsic uncertainty of this task, the proposed probabilistic\nframework has numerous desirable properties. In particular, our model is able\nto produce multiple plausible and vivid colorizations for a given grayscale\nimage and is one of the first colorization models to provide a proper\nstochastic sampling scheme. Moreover, our training procedure is supported by a\nrigorous theoretical framework that does not require any ad hoc heuristics and\nallows for efficient modeling and learning of the joint pixel color\ndistribution. We demonstrate strong quantitative and qualitative experimental\nresults on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 16:09:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Royer", "Amelie", ""], ["Kolesnikov", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1705.04267", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kyungsang Kim, Georges El Fakhri, and Quanzheng Li", "title": "A Cascaded Convolutional Neural Network for X-ray Low-dose CT Image\n  Denoising", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising techniques are essential to reducing noise levels and\nenhancing diagnosis reliability in low-dose computed tomography (CT). Machine\nlearning based denoising methods have shown great potential in removing the\ncomplex and spatial-variant noises in CT images. However, some residue\nartifacts would appear in the denoised image due to complexity of noises. A\ncascaded training network was proposed in this work, where the trained CNN was\napplied on the training dataset to initiate new trainings and remove artifacts\ninduced by denoising. A cascades of convolutional neural networks (CNN) were\nbuilt iteratively to achieve better performance with simple CNN structures.\nExperiments were carried out on 2016 Low-dose CT Grand Challenge datasets to\nevaluate the method's performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 16:32:55 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 13:48:08 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wu", "Dufan", ""], ["Kim", "Kyungsang", ""], ["Fakhri", "Georges El", ""], ["Li", "Quanzheng", ""]]}, {"id": "1705.04272", "submitter": "Uche Nnolim", "authors": "U. A. Nnolim", "title": "Improved underwater image enhancement algorithms based on partial\n  differential equations (PDEs)", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental results of improved underwater image enhancement algorithms\nbased on partial differential equations (PDEs) are presented in this report.\nThis second work extends the study of previous work and incorporating several\nimprovements into the revised algorithm. Experiments show the evidence of the\nimprovements when compared to previously proposed approaches and other\nconventional algorithms found in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 11:14:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Nnolim", "U. A.", ""]]}, {"id": "1705.04279", "submitter": "Boyce Griffith", "authors": "Ali Hasan, Ebrahim M. Kolahdouz, Andinet Enquobahrie, Thomas G.\n  Caranasos, John P. Vavalle, Boyce E. Griffith", "title": "Image-based immersed boundary model of the aortic root", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, approximately 300,000 heart valve repair or replacement procedures\nare performed worldwide, including approximately 70,000 aortic valve\nreplacement surgeries in the United States alone. This paper describes progress\nin constructing anatomically and physiologically realistic immersed boundary\n(IB) models of the dynamics of the aortic root and ascending aorta. This work\nbuilds on earlier IB models of fluid-structure interaction (FSI) in the aortic\nroot, which previously achieved realistic hemodynamics over multiple cardiac\ncycles, but which also were limited to simplified aortic geometries and\nidealized descriptions of the biomechanics of the aortic valve cusps. By\ncontrast, the model described herein uses an anatomical geometry reconstructed\nfrom patient-specific computed tomography angiography (CTA) data, and employs a\ndescription of the elasticity of the aortic valve leaflets based on a\nfiber-reinforced constitutive model fit to experimental tensile test data.\nNumerical tests show that the model is able to resolve the leaflet biomechanics\nin diastole and early systole at practical grid spacings. The model is also\nused to examine differences in the mechanics and fluid dynamics yielded by\nfresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in\nbioprosthetic heart valves. Although there are large differences in the leaflet\ndeformations during diastole, the differences in the open configurations of the\nvalve models are relatively small, and nearly identical hemodynamics are\nobtained in all cases considered.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 06:34:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Hasan", "Ali", ""], ["Kolahdouz", "Ebrahim M.", ""], ["Enquobahrie", "Andinet", ""], ["Caranasos", "Thomas G.", ""], ["Vavalle", "John P.", ""], ["Griffith", "Boyce E.", ""]]}, {"id": "1705.04281", "submitter": "Ulugbek Kamilov", "authors": "Hsiou-Yuan Liu, Dehong Liu, Hassan Mansour, Petros T. Boufounos, Laura\n  Waller and Ulugbek S. Kamilov", "title": "SEAGLE: Sparsity-Driven Image Reconstruction under Multiple Scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple scattering of an electromagnetic wave as it passes through an object\nis a fundamental problem that limits the performance of current imaging\nsystems. In this paper, we describe a new technique-called Series Expansion\nwith Accelerated Gradient Descent on Lippmann-Schwinger Equation (SEAGLE)-for\nrobust imaging under multiple scattering based on a combination of a new\nnonlinear forward model and a total variation (TV) regularizer. The proposed\nforward model can account for multiple scattering, which makes it advantageous\nin applications where linear models are inaccurate. Specifically, it\ncorresponds to a series expansion of the scattered wave with an\naccelerated-gradient method. This expansion guarantees the convergence even for\nstrongly scattering objects. One of our key insights is that it is possible to\nobtain an explicit formula for computing the gradient of our nonlinear forward\nmodel with respect to the unknown object, thus enabling fast image\nreconstruction with the state-of-the-art fast iterative shrinkage/thresholding\nalgorithm (FISTA). The proposed method is validated on both simulated and\nexperimentally measured data.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:05:26 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Liu", "Hsiou-Yuan", ""], ["Liu", "Dehong", ""], ["Mansour", "Hassan", ""], ["Boufounos", "Petros T.", ""], ["Waller", "Laura", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1705.04282", "submitter": "Amanda Song", "authors": "Amanda Song, Linjie Li, Chad Atalla, Garrison Cottrell", "title": "Learning to see people like people", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make complex inferences on faces, ranging from objective properties\n(gender, ethnicity, expression, age, identity, etc) to subjective judgments\n(facial attractiveness, trustworthiness, sociability, friendliness, etc). While\nthe objective aspects of face perception have been extensively studied,\nrelatively fewer computational models have been developed for the social\nimpressions of faces. Bridging this gap, we develop a method to predict human\nimpressions of faces in 40 subjective social dimensions, using deep\nrepresentations from state-of-the-art neural networks. We find that model\nperformance grows as the human consensus on a face trait increases, and that\nmodel predictions outperform human groups in correlation with human averages.\nThis illustrates the learnability of subjective social perception of faces,\nespecially when there is high human consensus. Our system can be used to decide\nwhich photographs from a personal collection will make the best impression. The\nresults are significant for the field of social robotics, demonstrating that\nrobots can learn the subjective judgments defining the underlying fabric of\nhuman interaction.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:47:15 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Song", "Amanda", ""], ["Li", "Linjie", ""], ["Atalla", "Chad", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1705.04286", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Yibo Zhang, Harun Gunaydin, Da Teng, Aydogan Ozcan", "title": "Phase recovery and holographic image reconstruction using deep learning\n  in neural networks", "comments": null, "journal-ref": "Light: Science and Applications, 7, e17141 (2018)", "doi": "10.1038/lsa.2017.141", "report-no": null, "categories": "cs.CV cs.IR cs.LG physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase recovery from intensity-only measurements forms the heart of coherent\nimaging techniques and holography. Here we demonstrate that a neural network\ncan learn to perform phase recovery and holographic image reconstruction after\nappropriate training. This deep learning-based approach provides an entirely\nnew framework to conduct holographic imaging by rapidly eliminating twin-image\nand self-interference related spatial artifacts. Compared to existing\napproaches, this neural network based method is significantly faster to\ncompute, and reconstructs improved phase and amplitude images of the objects\nusing only one hologram, i.e., requires less number of measurements in addition\nto being computationally faster. We validated this method by reconstructing\nphase and amplitude images of various samples, including blood and Pap smears,\nand tissue sections. These results are broadly applicable to any phase recovery\nproblem, and highlight that through machine learning challenging problems in\nimaging science can be overcome, providing new avenues to design powerful\ncomputational imaging systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:26:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Rivenson", "Yair", ""], ["Zhang", "Yibo", ""], ["Gunaydin", "Harun", ""], ["Teng", "Da", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1705.04300", "submitter": "Nan Yang", "authors": "Nan Yang, Rui Wang, Xiang Gao, Daniel Cremers", "title": "Challenges in Monocular Visual Odometry: Photometric Calibration, Motion\n  Bias and Rolling Shutter Effect", "comments": "Accepted by IEEE Robotics and Automation Letters (RA-L), 2018. The\n  first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular visual odometry (VO) and simultaneous localization and mapping\n(SLAM) have seen tremendous improvements in accuracy, robustness and\nefficiency, and have gained increasing popularity over recent years.\nNevertheless, not so many discussions have been carried out to reveal the\ninfluences of three very influential yet easily overlooked aspects: photometric\ncalibration, motion bias and rolling shutter effect. In this work, we evaluate\nthese three aspects quantitatively on the state of the art of direct,\nfeature-based and semi-direct methods, providing the community with useful\npractical knowledge both for better applying existing methods and developing\nnew algorithms of VO and SLAM. Conclusions (some of which are\ncounter-intuitive) are drawn with both technical and empirical analyses to all\nof our experiments. Possible improvements on existing methods are directed or\nproposed, such as a sub-pixel accuracy refinement of ORB-SLAM which boosts its\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:36:43 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 11:25:45 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 13:21:30 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 11:46:59 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Yang", "Nan", ""], ["Wang", "Rui", ""], ["Gao", "Xiang", ""], ["Cremers", "Daniel", ""]]}, {"id": "1705.04301", "submitter": "Akilan Thangarajah Mr", "authors": "Thangarajah Akilan (1), Q.M. Jonathan Wu (1), Wei Jiang (2) ((1)\n  Department of Electrical and Computer Engineering, University of Windsor,\n  Canada, (2) Department of Control Science and Engineering, Zhejiang\n  University, China)", "title": "A Feature Embedding Strategy for High-level CNN representations from\n  Multiple ConvNets", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the rapidly growing digital image usage, automatic image\ncategorization has become preeminent research area. It has broaden and adopted\nmany algorithms from time to time, whereby multi-feature (generally,\nhand-engineered features) based image characterization comes handy to improve\naccuracy. Recently, in machine learning, pre-trained deep convolutional neural\nnetworks (DCNNs or ConvNets) have been that the features extracted through such\nDCNN can improve classification accuracy. Thence, in this paper, we further\ninvestigate a feature embedding strategy to exploit cues from multiple DCNNs.\nWe derive a generalized feature space by embedding three different DCNN\nbottleneck features with weights respect to their Softmax cross-entropy loss.\nTest outcomes on six different object classification data-sets and an action\nclassification data-set show that regardless of variation in image statistics\nand tasks the proposed multi-DCNN bottleneck feature fusion is well suited to\nimage classification tasks and an effective complement of DCNN. The comparisons\nto existing fusion-based image classification approaches prove that the\nproposed method surmounts the state-of-the-art methods and produces competitive\nresults with fully trained DCNNs as well.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:38:12 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Akilan", "Thangarajah", ""], ["Wu", "Q. M. Jonathan", ""], ["Jiang", "Wei", ""]]}, {"id": "1705.04336", "submitter": "Alice Bates", "authors": "Alice P. Bates, Zubair Khalid, Jason D. McEwen and Rodney A. Kennedy", "title": "An Optimal Dimensionality Multi-shell Sampling Scheme with Accurate and\n  Efficient Transforms for Diffusion MRI", "comments": "4 pages, 4 figures presented at ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-shell sampling scheme and corresponding\ntransforms for the accurate reconstruction of the diffusion signal in diffusion\nMRI by expansion in the spherical polar Fourier (SPF) basis. The sampling\nscheme uses an optimal number of samples, equal to the degrees of freedom of\nthe band-limited diffusion signal in the SPF domain, and allows for\ncomputationally efficient reconstruction. We use synthetic data sets to\ndemonstrate that the proposed scheme allows for greater reconstruction accuracy\nof the diffusion signal than the multi-shell sampling schemes obtained using\nthe generalised electrostatic energy minimisation (gEEM) method used in the\nHuman Connectome Project. We also demonstrate that the proposed sampling scheme\nallows for increased angular discrimination and improved rotational invariance\nof reconstruction accuracy than the gEEM schemes.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 22:08:45 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Bates", "Alice P.", ""], ["Khalid", "Zubair", ""], ["McEwen", "Jason D.", ""], ["Kennedy", "Rodney A.", ""]]}, {"id": "1705.04350", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, \\'Akos K\\'ad\\'ar", "title": "Imagination improves Multimodal Translation", "comments": "Clarified main contributions, minor correction to Equation 8,\n  additional comparisons in Table 2, added more related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 18:49:17 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 09:18:55 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Elliott", "Desmond", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""]]}, {"id": "1705.04352", "submitter": "Mark Buckler", "authors": "Mark Buckler, Suren Jayasuriya, Adrian Sampson", "title": "Reconfiguring the Imaging Pipeline for Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in deep learning have ignited an explosion of research on\nefficient hardware for embedded computer vision. Hardware vision acceleration,\nhowever, does not address the cost of capturing and processing the image data\nthat feeds these algorithms. We examine the role of the image signal processing\n(ISP) pipeline in computer vision to identify opportunities to reduce\ncomputation and save energy. The key insight is that imaging pipelines should\nbe designed to be configurable: to switch between a traditional photography\nmode and a low-power vision mode that produces lower-quality image data\nsuitable only for computer vision. We use eight computer vision algorithms and\na reversible pipeline simulation tool to study the imaging system's impact on\nvision performance. For both CNN-based and classical vision algorithms, we\nobserve that only two ISP stages, demosaicing and gamma compression, are\ncritical for task performance. We propose a new image sensor design that can\ncompensate for skipping these stages. The sensor design features an adjustable\nresolution and tunable analog-to-digital converters (ADCs). Our proposed\nimaging system's vision mode disables the ISP entirely and configures the\nsensor to produce subsampled, lower-precision image data. This vision mode can\nsave ~75% of the average energy of a baseline photography mode while having\nonly a small impact on vision task accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 18:57:01 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 19:09:48 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 18:04:40 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Buckler", "Mark", ""], ["Jayasuriya", "Suren", ""], ["Sampson", "Adrian", ""]]}, {"id": "1705.04358", "submitter": "Syed Ashar Javed", "authors": "Syed Ashar Javed and Anil Kumar Nelakanti", "title": "Object-Level Context Modeling For Scene Classification with Context-CNN", "comments": "Scene Understanding workshop (SUNw), CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been used extensively for computer\nvision tasks and produce rich feature representation for objects or parts of an\nimage. But reasoning about scenes requires integration between the low-level\nfeature representations and the high-level semantic information. We propose a\ndeep network architecture which models the semantic context of scenes by\ncapturing object-level information. We use Long Short Term Memory(LSTM) units\nin conjunction with object proposals to incorporate object-object relationship\nand object-scene relationship in an end-to-end trainable manner. We evaluate\nour model on the LSUN dataset and achieve results comparable to the\nstate-of-art. We further show visualization of the learned features and analyze\nthe model with experiments to verify our model's ability to model context.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 19:25:37 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 05:29:20 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Javed", "Syed Ashar", ""], ["Nelakanti", "Anil Kumar", ""]]}, {"id": "1705.04396", "submitter": "Jing Zhang", "authors": "Jing Zhang and Wanqing Li and Philip Ogunbona and Dong Xu", "title": "Recent Advances in Transfer Learning for Cross-Dataset Visual\n  Recognition: A Problem-Oriented Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes a problem-oriented perspective and presents a comprehensive\nreview of transfer learning methods, both shallow and deep, for cross-dataset\nvisual recognition. Specifically, it categorises the cross-dataset recognition\ninto seventeen problems based on a set of carefully chosen data and label\nattributes. Such a problem-oriented taxonomy has allowed us to examine how\ndifferent transfer learning approaches tackle each problem and how well each\nproblem has been researched to date. The comprehensive problem-oriented review\nof the advances in transfer learning with respect to the problem has not only\nrevealed the challenges in transfer learning for visual recognition, but also\nthe problems (e.g. eight of the seventeen problems) that have been scarcely\nstudied. This survey not only presents an up-to-date technical review for\nresearchers, but also a systematic approach and a reference for a machine\nlearning practitioner to categorise a real problem and to look up for a\npossible solution accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 23:24:37 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 07:08:00 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 03:14:21 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""], ["Xu", "Dong", ""]]}, {"id": "1705.04402", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Negative Results in Computer Vision: A Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A negative result is when the outcome of an experiment or a model is not what\nis expected or when a hypothesis does not hold. Despite being often overlooked\nin the scientific community, negative results are results and they carry value.\nWhile this topic has been extensively discussed in other fields such as social\nsciences and biosciences, less attention has been paid to it in the computer\nvision community. The unique characteristics of computer vision, particularly\nits experimental aspect, call for a special treatment of this matter. In this\npaper, I will address what makes negative results important, how they should be\ndisseminated and incentivized, and what lessons can be learned from cognitive\nvision research in this regard. Further, I will discuss issues such as computer\nvision and human vision interaction, experimental design and statistical\nhypothesis testing, explanatory versus predictive modeling, performance\nevaluation, model comparison, as well as computer vision research culture.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 23:39:18 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 17:01:54 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 23:23:28 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "1705.04407", "submitter": "Brendt Wohlberg", "authors": "Brendt Wohlberg", "title": "Convolutional Sparse Representations with Gradient Penalties", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2018.8462151", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional sparse representations enjoy a number of useful\nproperties, they have received limited attention for image reconstruction\nproblems. The present paper compares the performance of block-based and\nconvolutional sparse representations in the removal of Gaussian white noise.\nWhile the usual formulation of the convolutional sparse coding problem is\nslightly inferior to the block-based representations in this problem, the\nperformance of the convolutional form can be boosted beyond that of the\nblock-based form by the inclusion of suitable penalties on the gradients of the\ncoefficient maps.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 00:33:05 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 03:47:27 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wohlberg", "Brendt", ""]]}, {"id": "1705.04433", "submitter": "Hassan Foroosh", "authors": "Sina Lotfian and Hassan Foroosh", "title": "View-Invariant Template Matching Using Homography Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change in viewpoint is one of the major factors for variation in object\nappearance across different images. Thus, view-invariant object recognition is\na challenging and important image understanding task. In this paper, we propose\na method that can match objects in images taken under different viewpoints.\nUnlike most methods in the literature, no restriction on camera orientations or\ninternal camera parameters are imposed and no prior knowledge of 3D structure\nof the object is required. We prove that when two cameras take pictures of the\nsame object from two different viewing angels, the relationship between every\nquadruple of points reduces to the special case of homography with two equal\neigenvalues. Based on this property, we formulate the problem as an error\nfunction that indicates how likely two sets of 2D points are projections of the\nsame set of 3D points under two different cameras. Comprehensive set of\nexperiments were conducted to prove the robustness of the method to noise, and\nevaluate its performance on real-world applications, such as face and object\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 03:40:02 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Lotfian", "Sina", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.04442", "submitter": "Yuqi Han", "authors": "Yuqi Han, Chenwei Deng, Zengshuo Zhang, Jiatong Li, Baojun Zhao", "title": "Adaptive Feature Representation for Visual Tracking", "comments": "4 pages, ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust feature representation plays significant role in visual tracking.\nHowever, it remains a challenging issue, since many factors may affect the\nexperimental performance. The existing method which combine different features\nby setting them equally with the fixed weight could hardly solve the issues,\ndue to the different statistical properties of different features across\nvarious of scenarios and attributes. In this paper, by exploiting the internal\nrelationship among these features, we develop a robust method to construct a\nmore stable feature representation. More specifically, we utilize a co-training\nparadigm to formulate the intrinsic complementary information of multi-feature\ntemplate into the efficient correlation filter framework. We test our approach\non challenging se- quences with illumination variation, scale variation,\ndeformation etc. Experimental results demonstrate that the proposed method\noutperforms state-of-the-art methods favorably.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 05:04:41 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Han", "Yuqi", ""], ["Deng", "Chenwei", ""], ["Zhang", "Zengshuo", ""], ["Li", "Jiatong", ""], ["Zhao", "Baojun", ""]]}, {"id": "1705.04451", "submitter": "Anza Shakeel", "authors": "Anza Shakeel, Mohsen Ali", "title": "Using Satellite Imagery for Good: Detecting Communities in Desert and\n  Mapping Vaccination Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have outperformed existing object\nrecognition and detection algorithms. On the other hand satellite imagery\ncaptures scenes that are diverse. This paper describes a deep learning approach\nthat analyzes a geo referenced satellite image and efficiently detects built\nstructures in it. A Fully Convolution Network (FCN) is trained on low\nresolution Google earth satellite imagery in order to achieve end result. The\ndetected built communities are then correlated with the vaccination activity\nthat has furnished some useful statistics.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 07:23:06 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Shakeel", "Anza", ""], ["Ali", "Mohsen", ""]]}, {"id": "1705.04456", "submitter": "Yahui Liu", "authors": "Yahui Liu, Jian Yao, Li Li, Xiaohu Lu and Jing Han", "title": "Learning to Refine Object Contours with a Top-Down Fully Convolutional\n  Encoder-Decoder Network", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "1705.04456", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel deep contour detection algorithm with a top-down fully\nconvolutional encoder-decoder network. Our proposed method, named TD-CEDN,\nsolves two important issues in this low-level vision problem: (1) learning\nmulti-scale and multi-level features; and (2) applying an effective top-down\nrefined approach in the networks. TD-CEDN performs the pixel-wise prediction by\nmeans of leveraging features at all layers of the net. Unlike skip connections\nand previous encoder-decoder methods, we first learn a coarse feature map after\nthe encoder stage in a feedforward pass, and then refine this feature map in a\ntop-down strategy during the decoder stage utilizing features at successively\nlower layers. Therefore, the deconvolutional process is conducted stepwise,\nwhich is guided by Deeply-Supervision Net providing the integrated direct\nsupervision. The above proposed technologies lead to a more precise and clearer\nprediction. Our proposed algorithm achieved the state-of-the-art on the BSDS500\ndataset (ODS F-score of 0.788), the PASCAL VOC2012 dataset (ODS F-score of\n0.588), and and the NYU Depth dataset (ODS F-score of 0.735).\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 07:52:27 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Liu", "Yahui", ""], ["Yao", "Jian", ""], ["Li", "Li", ""], ["Lu", "Xiaohu", ""], ["Han", "Jing", ""]]}, {"id": "1705.04469", "submitter": "Luka \\v{C}ehovin Zajc", "authors": "Luka \\v{C}ehovin", "title": "TraX: The visual Tracking eXchange Protocol and Library", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2017.02.036", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of developing on-line visual tracking\nalgorithms. We present a specialized communication protocol that serves as a\nbridge between a tracker implementation and utilizing application. It decouples\ndevelopment of algorithms and application, encouraging re-usability. The\nprimary use case is algorithm evaluation where the protocol facilitates more\ncomplex evaluation scenarios that are used nowadays thus pushing forward the\nfield of visual tracking. We present a reference implementation of the protocol\nthat makes it easy to use in several popular programming languages and discuss\nwhere the protocol is already used and some usage scenarios that we envision\nfor the future.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 08:33:20 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["\u010cehovin", "Luka", ""]]}, {"id": "1705.04505", "submitter": "Jun Xu", "authors": "Jun Xu, Lei Zhang, David Zhang", "title": "External Prior Guided Internal Prior Learning for Real-World Noisy Image\n  Denoising", "comments": "14 pages, 13figures, IEEE Trans. Image Processing 27(6): 2996-3010\n  (2018)", "journal-ref": null, "doi": "10.1109/TIP.2018.2811546", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing image denoising methods learn image priors from either\nexternal data or the noisy image itself to remove noise. However, priors\nlearned from external data may not be adaptive to the image to be denoised,\nwhile priors learned from the given noisy image may not be accurate due to the\ninterference of corrupted noise. Meanwhile, the noise in real-world noisy\nimages is very complex, which is hard to be described by simple distributions\nsuch as Gaussian distribution, making real-world noisy image denoising a very\nchallenging problem. We propose to exploit the information in both external\ndata and the given noisy image, and develop an external prior guided internal\nprior learning method for real-world noisy image denoising. We first learn\nexternal priors from an independent set of clean natural images. With the aid\nof learned external priors, we then learn internal priors from the given noisy\nimage to refine the prior model. The external and internal priors are\nformulated as a set of orthogonal dictionaries to efficiently reconstruct the\ndesired image. Extensive experiments are performed on several real-world noisy\nimage datasets. The proposed method demonstrates highly competitive denoising\nperformance, outperforming state-of-the-art denoising methods including those\ndesigned for real-world noisy images.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 10:49:33 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 08:53:12 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Xu", "Jun", ""], ["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1705.04515", "submitter": "Tong Zhang", "authors": "Tong Zhang (1 and 2), Wenming Zheng (2), Zhen Cui (2), Yuan Zong (2)\n  and Yang Li (1 and 2) ((1) the Department of Information Science and\n  Engineering, Southeast University, Nanjing, China (2) the Key Laboratory of\n  Child Development and Learning Science of Ministry of Education, Research\n  Center for Learning Science, Southeast University, Nanjing, China)", "title": "Spatial-Temporal Recurrent Neural Network for Emotion Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2017.2788081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion analysis is a crucial problem to endow artifact machines with real\nintelligence in many large potential applications. As external appearances of\nhuman emotions, electroencephalogram (EEG) signals and video face signals are\nwidely used to track and analyze human's affective information. According to\ntheir common characteristics of spatial-temporal volumes, in this paper we\npropose a novel deep learning framework named spatial-temporal recurrent neural\nnetwork (STRNN) to unify the learning of two different signal sources into a\nspatial-temporal dependency model. In STRNN, to capture those spatially\ncooccurrent variations of human emotions, a multi-directional recurrent neural\nnetwork (RNN) layer is employed to capture longrange contextual cues by\ntraversing the spatial region of each time slice from multiple angles. Then a\nbi-directional temporal RNN layer is further used to learn discriminative\ntemporal dependencies from the sequences concatenating spatial features of each\ntime slice produced from the spatial RNN layer. To further select those salient\nregions of emotion representation, we impose sparse projection onto those\nhidden states of spatial and temporal domains, which actually also increases\nthe model discriminant ability because of this global consideration.\nConsequently, such a two-layer RNN model builds spatial dependencies as well as\ntemporal dependencies of the input signals. Experimental results on the public\nemotion datasets of EEG and facial expression demonstrate the proposed STRNN\nmethod is more competitive over those state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:23:07 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Zhang", "Tong", "", "1 and 2"], ["Zheng", "Wenming", "", "1 and 2"], ["Cui", "Zhen", "", "1 and 2"], ["Zong", "Yuan", "", "1 and 2"], ["Li", "Yang", "", "1 and 2"]]}, {"id": "1705.04519", "submitter": "Andjela Draganic", "authors": "Zoja Vulaj, Milos Brajovic, Andjela Draganic, Irena Orovic", "title": "Detection of irregular QRS complexes using Hermite Transform and Support\n  Vector Machine", "comments": "submitted to 59th International Symposium ELMAR-2017, Zadar, Croatia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer based recognition and detection of abnormalities in ECG signals is\nproposed. For this purpose, the Support Vector Machines (SVM) are combined with\nthe advantages of Hermite transform representation. SVM represent a special\ntype of classification techniques commonly used in medical applications.\nAutomatic classification of ECG could make the work of cardiologic departments\nfaster and more efficient. It would also reduce the number of false diagnosis\nand, as a result, save lives. The working principle of the SVM is based on\ntranslating the data into a high dimensional feature space and separating it\nusing a linear classificator. In order to provide an optimal representation for\nSVM application, the Hermite transform domain is used. This domain is proved to\nbe suitable because of the similarity of the QRS complex with Hermite basis\nfunctions. The maximal signal information is obtained using a small set of\nfeatures that are used for detection of irregular QRS complexes. The aim of the\npaper is to show that these features can be employed for automatic ECG signal\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:40:02 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Vulaj", "Zoja", ""], ["Brajovic", "Milos", ""], ["Draganic", "Andjela", ""], ["Orovic", "Irena", ""]]}, {"id": "1705.04528", "submitter": "Byeongyong Ahn", "authors": "Byeongyong Ahn, and Nam Ik Cho", "title": "Self-Committee Approach for Image Restoration Problems using\n  Convolutional Neural Network", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many discriminative learning methods using convolutional\nneural networks (CNN) for several image restoration problems, which learn the\nmapping function from a degraded input to the clean output. In this letter, we\npropose a self-committee method that can find enhanced restoration results from\nthe multiple trial of a trained CNN with different but related inputs.\nSpecifically, it is noted that the CNN sometimes finds different mapping\nfunctions when the input is transformed by a reversible transform and thus\nproduces different but related outputs with the original. Hence averaging the\noutputs for several different transformed inputs can enhance the results as\nevidenced by the network committee methods. Unlike the conventional committee\napproaches that require several networks, the proposed method needs only a\nsingle network. Experimental results show that adding an additional transform\nas a committee always brings additional gain on image denoising and single\nimage supre-resolution problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 12:10:52 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 01:16:05 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ahn", "Byeongyong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1705.04608", "submitter": "Lucas Beyer", "authors": "Lucas Beyer, Stefan Breuers, Vitaly Kurin, Bastian Leibe", "title": "Towards a Principled Integration of Multi-Camera Re-Identification and\n  Tracking through Optimal Bayes Filters", "comments": "First two authors have equal contribution. This is initial work into\n  a new direction, not a benchmark-beating method. v2 only adds\n  acknowledgements and fixes a typo in e-mail", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of end-to-end learning through deep learning, person detectors\nand re-identification (ReID) models have recently become very strong.\nMulti-camera multi-target (MCMT) tracking has not fully gone through this\ntransformation yet. We intend to take another step in this direction by\npresenting a theoretically principled way of integrating ReID with tracking\nformulated as an optimal Bayes filter. This conveniently side-steps the need\nfor data-association and opens up a direct path from full images to the core of\nthe tracker. While the results are still sub-par, we believe that this new,\ntight integration opens many interesting research opportunities and leads the\nway towards full end-to-end tracking from raw pixels.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 14:50:15 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 10:07:56 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Beyer", "Lucas", ""], ["Breuers", "Stefan", ""], ["Kurin", "Vitaly", ""], ["Leibe", "Bastian", ""]]}, {"id": "1705.04641", "submitter": "Hassan Foroosh", "authors": "Marjaneh Safaei and Hassan Foroosh", "title": "Single Image Action Recognition by Predicting Space-Time Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach based on deep Convolutional Neural Networks (CNN)\nto recognize human actions in still images by predicting the future motion, and\ndetecting the shape and location of the salient parts of the image. We make the\nfollowing major contributions to this important area of research: (i) We use\nthe predicted future motion in the static image (Walker et al., 2015) as a\nmeans of compensating for the missing temporal information, while using the\nsaliency map to represent the the spatial information in the form of location\nand shape of what is predicted as significant. (ii) We cast action\nclassification in static images as a domain adaptation problem by transfer\nlearning. We first map the input static image to a new domain that we refer to\nas the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tune\nthe layers of a deep CNN model trained on classifying the ImageNet dataset to\nperform action classification in the POF-SM domain. (iii) We tested our method\non the popular Willow dataset. But unlike existing methods, we also tested on a\nmore realistic and challenging dataset of over 2M still images that we\ncollected and labeled by taking random frames from the UCF-101 video dataset.\nWe call our dataset the UCF Still Image dataset or UCFSI-101 in short. Our\nresults outperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 16:03:33 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Safaei", "Marjaneh", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.04709", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Zoltan Gorocs, Harun Gunaydin, Yibo Zhang, Hongda Wang,\n  Aydogan Ozcan", "title": "Deep Learning Microscopy", "comments": null, "journal-ref": "Optica, Vol. 4, Issue 11, pp. 1437-1443 (2017)", "doi": "10.1364/OPTICA.4.001437", "report-no": null, "categories": "cs.LG cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a deep neural network can significantly improve optical\nmicroscopy, enhancing its spatial resolution over a large field-of-view and\ndepth-of-field. After its training, the only input to this network is an image\nacquired using a regular optical microscope, without any changes to its design.\nWe blindly tested this deep learning approach using various tissue samples that\nare imaged with low-resolution and wide-field systems, where the network\nrapidly outputs an image with remarkably better resolution, matching the\nperformance of higher numerical aperture lenses, also significantly surpassing\ntheir limited field-of-view and depth-of-field. These results are\ntransformative for various fields that use microscopy tools, including e.g.,\nlife sciences, where optical microscopy is considered as one of the most widely\nused and deployed techniques. Beyond such applications, our presented approach\nis broadly applicable to other imaging modalities, also spanning different\nparts of the electromagnetic spectrum, and can be used to design computational\nimagers that get better and better as they continue to image specimen and\nestablish new transformations among different modes of imaging.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 18:22:54 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Rivenson", "Yair", ""], ["Gorocs", "Zoltan", ""], ["Gunaydin", "Harun", ""], ["Zhang", "Yibo", ""], ["Wang", "Hongda", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1705.04724", "submitter": "Wei Li", "authors": "Wei Li, Xiatian Zhu, Shaogang Gong", "title": "Person Re-Identification by Deep Joint Learning of Multi-Loss\n  Classification", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods rely mostly on either\nlocalised or global feature representation alone. This ignores their joint\nbenefit and mutual complementary effects. In this work, we show the advantages\nof jointly learning local and global features in a Convolutional Neural Network\n(CNN) by aiming to discover correlated local and global features in different\ncontext. Specifically, we formulate a method for joint learning of local and\nglobal feature selection losses designed to optimise person re-id when using\nonly generic matching metrics such as the L2 distance. We design a novel CNN\narchitecture for Jointly Learning Multi-Loss (JLML) of local and global\ndiscriminative feature optimisation subject concurrently to the same re-id\nlabelled information. Extensive comparative evaluations demonstrate the\nadvantages of this new JLML model for person re-id over a wide range of\nstate-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03,\nMarket-1501).\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 19:18:07 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 01:04:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Li", "Wei", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1705.04748", "submitter": "Syed Shakib Sarwar", "authors": "Syed Shakib Sarwar, Priyadarshini Panda, Kaushik Roy", "title": "Gabor Filter Assisted Energy Efficient Fast Learning Convolutional\n  Neural Networks", "comments": "Accepted in ISLPED 2017", "journal-ref": "EEE/ACM International Symposium on Low Power Electronics and\n  Design (ISLPED), Taipei, 2017, pp. 1-6", "doi": "10.1109/ISLPED.2017.8009202", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) are being increasingly used in computer\nvision for a wide range of classification and recognition problems. However,\ntraining these large networks demands high computational time and energy\nrequirements; hence, their energy-efficient implementation is of great\ninterest. In this work, we reduce the training complexity of CNNs by replacing\ncertain weight kernels of a CNN with Gabor filters. The convolutional layers\nuse the Gabor filters as fixed weight kernels, which extracts intrinsic\nfeatures, with regular trainable weight kernels. This combination creates a\nbalanced system that gives better training performance in terms of energy and\ntime, compared to the standalone CNN (without any Gabor kernels), in exchange\nfor tolerable accuracy degradation. We show that the accuracy degradation can\nbe mitigated by partially training the Gabor kernels, for a small fraction of\nthe total training cycles. We evaluated the proposed approach on 4 benchmark\napplications. Simple tasks like face detection and character recognition (MNIST\nand TiCH), were implemented using LeNet architecture. While a more complex task\nof object recognition (CIFAR10) was implemented on a state of the art deep CNN\n(Network in Network) architecture. The proposed approach yields 1.31-1.53x\nimprovement in training energy in comparison to conventional CNN\nimplementation. We also obtain improvement up to 1.4x in training time, up to\n2.23x in storage requirements, and up to 2.2x in memory access energy. The\naccuracy degradation suffered by the approximate implementations is within 0-3%\nof the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 20:50:51 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sarwar", "Syed Shakib", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1705.04823", "submitter": "EL-Hachemi Guerrout", "authors": "EL-Hachemi Guerrout, Samy Ait-Aoudia, Dominique Michelucci, Ramdane\n  Mahiou", "title": "Combination of Hidden Markov Random Field and Conjugate Gradient for\n  Brain Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning the image into significant\nregions easier to analyze. Nowadays, segmentation has become a necessity in\nmany practical medical imaging methods as locating tumors and diseases. Hidden\nMarkov Random Field model is one of several techniques used in image\nsegmentation. It provides an elegant way to model the segmentation process.\nThis modeling leads to the minimization of an objective function. Conjugate\nGradient algorithm (CG) is one of the best known optimization techniques. This\npaper proposes the use of the Conjugate Gradient algorithm (CG) for image\nsegmentation, based on the Hidden Markov Random Field. Since derivatives are\nnot available for this expression, finite differences are used in the CG\nalgorithm to approximate the first derivative. The approach is evaluated using\na number of publicly available images, where ground truth is known. The Dice\nCoefficient is used as an objective criterion to measure the quality of\nsegmentation. The results show that the proposed CG approach compares favorably\nwith other variants of Hidden Markov Random Field segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 13:34:09 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 20:23:37 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 14:43:00 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 09:36:07 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Guerrout", "EL-Hachemi", ""], ["Ait-Aoudia", "Samy", ""], ["Michelucci", "Dominique", ""], ["Mahiou", "Ramdane", ""]]}, {"id": "1705.04824", "submitter": "Yao Yao", "authors": "Yao Yao, Jialv He, Jinbao Zhang and Yatao Zhang", "title": "Extracting urban impervious surface from GF-1 imagery using one-class\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impervious surface area is a direct consequence of the urbanization, which\nalso plays an important role in urban planning and environmental management.\nWith the rapidly technical development of remote sensing, monitoring urban\nimpervious surface via high spatial resolution (HSR) images has attracted\nunprecedented attention recently. Traditional multi-classes models are\ninefficient for impervious surface extraction because it requires labeling all\nneeded and unneeded classes that occur in the image exhaustively. Therefore, we\nneed to find a reliable one-class model to classify one specific land cover\ntype without labeling other classes. In this study, we investigate several\none-class classifiers, such as Presence and Background Learning (PBL), Positive\nUnlabeled Learning (PUL), OCSVM, BSVM and MAXENT, to extract urban impervious\nsurface area using high spatial resolution imagery of GF-1, China's new\ngeneration of high spatial remote sensing satellite, and evaluate the\nclassification accuracy based on artificial interpretation results. Compared to\ntraditional multi-classes classifiers (ANN and SVM), the experimental results\nindicate that PBL and PUL provide higher classification accuracy, which is\nsimilar to the accuracy provided by ANN model. Meanwhile, PBL and PUL\noutperforms OCSVM, BSVM, MAXENT and SVM models. Hence, the one-class\nclassifiers only need a small set of specific samples to train models without\nlosing predictive accuracy, which is supposed to gain more attention on urban\nimpervious surface extraction or other one specific land cover type.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 13:39:42 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Yao", "Yao", ""], ["He", "Jialv", ""], ["Zhang", "Jinbao", ""], ["Zhang", "Yatao", ""]]}, {"id": "1705.04828", "submitter": "Yiluan Guo", "authors": "Yiluan Guo, Hossein Nejati, Ngai-Man Cheung", "title": "Deep neural networks on graph signals for brain imaging analysis", "comments": "Accepted by ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain imaging data such as EEG or MEG are high-dimensional spatiotemporal\ndata often degraded by complex, non-Gaussian noise. For reliable analysis of\nbrain imaging data, it is important to extract discriminative, low-dimensional\nintrinsic representation of the recorded data. This work proposes a new method\nto learn the low-dimensional representations from the noise-degraded\nmeasurements. In particular, our work proposes a new deep neural network design\nthat integrates graph information such as brain connectivity with\nfully-connected layers. Our work leverages efficient graph filter design using\nChebyshev polynomial and recent work on convolutional nets on graph-structured\ndata. Our approach exploits graph structure as the prior side information,\nlocalized graph filter for feature extraction and neural networks for high\ncapacity learning. Experiments on real MEG datasets show that our approach can\nextract more discriminative representations, leading to improved accuracy in a\nsupervised classification task.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 13:50:47 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Guo", "Yiluan", ""], ["Nejati", "Hossein", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1705.04838", "submitter": "Nam Vo", "authors": "Nam Vo, Nathan Jacobs and James Hays", "title": "Revisiting IM2GPS in the Deep Learning Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image geolocalization, inferring the geographic location of an image, is a\nchallenging computer vision problem with many potential applications. The\nrecent state-of-the-art approach to this problem is a deep image classification\napproach in which the world is spatially divided into cells and a deep network\nis trained to predict the correct cell for a given image. We propose to combine\nthis approach with the original Im2GPS approach in which a query image is\nmatched against a database of geotagged images and the location is inferred\nfrom the retrieved set. We estimate the geographic location of a query image by\napplying kernel density estimation to the locations of its nearest neighbors in\nthe reference database. Interestingly, we find that the best features for our\nretrieval task are derived from networks trained with classification loss even\nthough we do not use a classification approach at test time. Training with\nclassification loss outperforms several deep feature learning methods (e.g.\nSiamese networks with contrastive of triplet loss) more typical for retrieval\napplications. Our simple approach achieves state-of-the-art geolocalization\naccuracy while also requiring significantly less training data.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 14:43:02 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Vo", "Nam", ""], ["Jacobs", "Nathan", ""], ["Hays", "James", ""]]}, {"id": "1705.04916", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar, Yuchao Dai, Hongdong Li", "title": "Spatial-Temporal Union of Subspaces for Multi-body Non-rigid\n  Structure-from-Motion", "comments": "Author version of this paper has been accepted by Pattern Recognition\n  Journal in the special issue on Articulated Motion and Deformable Objects.\n  This work was originally submitted to ACCV 16 conference on 27th May 2016 for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid structure-from-motion (NRSfM) has so far been mostly studied for\nrecovering 3D structure of a single non-rigid/deforming object. To handle the\nreal world challenging multiple deforming objects scenarios, existing methods\neither pre-segment different objects in the scene or treat multiple non-rigid\nobjects as a whole to obtain the 3D non-rigid reconstruction. However, these\nmethods fail to exploit the inherent structure in the problem as the solution\nof segmentation and the solution of reconstruction could not benefit each\nother. In this paper, we propose a unified framework to jointly segment and\nreconstruct multiple non-rigid objects. To compactly represent complex\nmulti-body non-rigid scenes, we propose to exploit the structure of the scenes\nalong both temporal direction and spatial direction, thus achieving a\nspatio-temporal representation. Specifically, we represent the 3D non-rigid\ndeformations as lying in a union of subspaces along the temporal direction and\nrepresent the 3D trajectories as lying in the union of subspaces along the\nspatial direction. This spatio-temporal representation not only provides\ncompetitive 3D reconstruction but also outputs robust segmentation of multiple\nnon-rigid objects. The resultant optimization problem is solved efficiently\nusing the Alternating Direction Method of Multipliers (ADMM). Extensive\nexperimental results on both synthetic and real multi-body NRSfM datasets\ndemonstrate the superior performance of our proposed framework compared with\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 05:59:51 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kumar", "Suryansh", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1705.04919", "submitter": "Shinjini Kundu", "authors": "Shinjini Kundu, Soheil Kolouri, Kirk I Erickson, Arthur F Kramer,\n  Edward McAuley, Gustavo K Rohde", "title": "Discovery and visualization of structural biomarkers from MRI using\n  transport-based morphometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease in the brain is often associated with subtle, spatially diffuse, or\ncomplex tissue changes that may lie beneath the level of gross visual\ninspection, even on magnetic resonance imaging (MRI). Unfortunately, current\ncomputer-assisted approaches that examine pre-specified features, whether\nanatomically-defined (i.e. thalamic volume, cortical thickness) or based on\npixelwise comparison (i.e. deformation-based methods), are prone to missing a\nvast array of physical changes that are not well-encapsulated by these metrics.\nIn this paper, we have developed a technique for automated pattern analysis\nthat can fully determine the relationship between brain structure and\nobservable phenotype without requiring any a priori features. Our technique,\ncalled transport-based morphometry (TBM), is an image transformation that maps\nbrain images losslessly to a domain where they become much more separable. The\nnew approach is validated on structural brain images of healthy older adult\nsubjects where even linear models for discrimination, regression, and blind\nsource separation enable TBM to independently discover the characteristic\nchanges of aging and highlight potential mechanisms by which aerobic fitness\nmay mediate brain health later in life. TBM is a generative approach that can\nprovide visualization of physically meaningful shifts in tissue distribution\nthrough inverse transformation. The proposed framework is a powerful technique\nthat can potentially elucidate genotype-structural-behavioral associations in\nmyriad diseases.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 06:29:34 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kundu", "Shinjini", ""], ["Kolouri", "Soheil", ""], ["Erickson", "Kirk I", ""], ["Kramer", "Arthur F", ""], ["McAuley", "Edward", ""], ["Rohde", "Gustavo K", ""]]}, {"id": "1705.04924", "submitter": "Salman Siddiique Khan", "authors": "Rohith AP, Salman S. Khan, Kumar Anubhav, Angshuman Paul", "title": "Gland Segmentation in Histopathology Images Using Random Forest Guided\n  Boundary Construction", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grading of cancer is important to know the extent of its spread. Prior to\ngrading, segmentation of glandular structures is important. Manual segmentation\nis a time consuming process and is subject to observer bias. Hence, an\nautomated process is required to segment the gland structures. These glands\nshow a large variation in shape size and texture. This makes the task\nchallenging as the glands cannot be segmented using mere morphological\noperations and conventional segmentation mechanisms. In this project we propose\na method which detects the boundary epithelial cells of glands and then a novel\napproach is used to construct the complete gland boundary. The region enclosed\nwithin the boundary can then be obtained to get the segmented gland regions.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 07:01:08 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 05:35:30 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 17:57:52 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["AP", "Rohith", ""], ["Khan", "Salman S.", ""], ["Anubhav", "Kumar", ""], ["Paul", "Angshuman", ""]]}, {"id": "1705.04927", "submitter": "Hassan Foroosh", "authors": "Mais Alnasser and Hassan Foroosh", "title": "A Closed-Form Model for Image-Based Distant Lighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new mathematical foundation for image-based\nlighting. Using a simple manipulation of the local coordinate system, we derive\na closed-form solution to the light integral equation under distant environment\nillumination. We derive our solution for different BRDF's such as lambertian\nand Phong-like. The method is free of noise, and provides the possibility of\nusing the full spectrum of frequencies captured by images taken from the\nenvironment. This allows for the color of the rendered object to be toned\naccording to the color of the light in the environment. Experimental results\nalso show that one can gain an order of magnitude or higher in rendering time\ncompared to Monte Carlo quadrature methods and spherical harmonics.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 08:39:26 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Alnasser", "Mais", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.04932", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He, Weiran\n  He", "title": "GeneGAN: Learning Object Transfiguration and Attribute Subspace from\n  Unpaired Data", "comments": "Github: https://github.com/Prinsphield/GeneGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Transfiguration replaces an object in an image with another object\nfrom a second image. For example it can perform tasks like \"putting exactly\nthose eyeglasses from image A on the nose of the person in image B\". Usage of\nexemplar images allows more precise specification of desired modifications and\nimproves the diversity of conditional image generation. However, previous\nmethods that rely on feature space operations, require paired data and/or\nappearance models for training or disentangling objects from background. In\nthis work, we propose a model that can learn object transfiguration from two\nunpaired sets of images: one set containing images that \"have\" that kind of\nobject, and the other set being the opposite, with the mild constraint that the\nobjects be located approximately at the same place. For example, the training\ndata can be one set of reference face images that have eyeglasses, and another\nset of images that have not, both of which spatially aligned by face landmarks.\nDespite the weak 0/1 labels, our model can learn an \"eyeglasses\" subspace that\ncontain multiple representatives of different types of glasses. Consequently,\nwe can perform fine-grained control of generated images, like swapping the\nglasses in two images by swapping the projected components in the \"eyeglasses\"\nsubspace, to create novel images of people wearing eyeglasses.\n  Overall, our deterministic generative model learns disentangled attribute\nsubspaces from weakly labeled data by adversarial training. Experiments on\nCelebA and Multi-PIE datasets validate the effectiveness of the proposed model\non real world data, in generating images with specified eyeglasses, smiling,\nhair styles, and lighting conditions etc. The code is available online.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 08:59:36 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhou", "Shuchang", ""], ["Xiao", "Taihong", ""], ["Yang", "Yi", ""], ["Feng", "Dieqiao", ""], ["He", "Qinyao", ""], ["He", "Weiran", ""]]}, {"id": "1705.04964", "submitter": "Balint Daroczy", "authors": "B\\'alint Zolt\\'an Dar\\'oczy", "title": "Machine learning methods for multimedia information retrieval", "comments": "doctoral thesis, 2016", "journal-ref": null, "doi": "10.15476/ELTE.2016.086", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we examined several multimodal feature extraction and learning\nmethods for retrieval and classification purposes. We reread briefly some\ntheoretical results of learning in Section 2 and reviewed several generative\nand discriminative models in Section 3 while we described the similarity kernel\nin Section 4. We examined different aspects of the multimodal image retrieval\nand classification in Section 5 and suggested methods for identifying quality\nassessments of Web documents in Section 6. In our last problem we proposed\nsimilarity kernel for time-series based classification. The experiments were\ncarried over publicly available datasets and source codes for the most\nessential parts are either open source or released. Since the used similarity\ngraphs (Section 4.2) are greatly constrained for computational purposes, we\nwould like to continue work with more complex, evolving and capable graphs and\napply for different problems such as capturing the rapid change in the\ndistribution (e.g. session based recommendation) or complex graphs of the\nliterature work. The similarity kernel with the proper metrics reaches and in\nmany cases improves over the state-of-the-art. Hence we may conclude generative\nmodels based on instance similarities with multiple modes is a generally\napplicable model for classification and regression tasks ranging over various\ndomains, including but not limited to the ones presented in this thesis. More\ngenerally, the Fisher kernel is not only unique in many ways but one of the\nmost powerful kernel functions. Therefore we may exploit the Fisher kernel in\nthe future over widely used generative models, such as Boltzmann Machines\n[Hinton et al., 1984], a particular subset, the Restricted Boltzmann Machines\nand Deep Belief Networks [Hinton et al., 2006]), Latent Dirichlet Allocation\n[Blei et al., 2003] or Hidden Markov Models [Baum and Petrie, 1966] to name a\nfew.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 14:10:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Dar\u00f3czy", "B\u00e1lint Zolt\u00e1n", ""]]}, {"id": "1705.05016", "submitter": "Yong Khoo", "authors": "Yong Khoo", "title": "A Correspondence Relaxation Approach for 3D Shape Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for 3D shape reconstruction based on two\nexisting methods. A 3D reconstruction from a single photograph is introduced by\nboth papers: the first one uses a photograph and a set of existing 3D model to\ngenerate the 3D object in the photograph, while the second one uses a\nphotograph and a selected similar model to create the 3D object in the\nphotograph. According to their difference, we propose a relaxation based method\nfor more accurate correspondence establishment and shape recovery. The\nexperiment demonstrates promising results compared to the state-of-the-art work\non 3D shape estimation.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 19:02:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Khoo", "Yong", ""]]}, {"id": "1705.05065", "submitter": "Shital Shah", "authors": "Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor", "title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous\n  Vehicles", "comments": "Accepted for Field and Service Robotics conference 2017 (FSR 2017)", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-9", "categories": "cs.RO cs.AI cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing and testing algorithms for autonomous vehicles in real world is an\nexpensive and time consuming process. Also, in order to utilize recent advances\nin machine intelligence and deep learning we need to collect a large amount of\nannotated training data in a variety of conditions and environments. We present\na new simulator built on Unreal Engine that offers physically and visually\nrealistic simulations for both of these goals. Our simulator includes a physics\nengine that can operate at a high frequency for real-time hardware-in-the-loop\n(HITL) simulations with support for popular protocols (e.g. MavLink). The\nsimulator is designed from the ground up to be extensible to accommodate new\ntypes of vehicles, hardware platforms and software protocols. In addition, the\nmodular design enables various components to be easily usable independently in\nother projects. We demonstrate the simulator by first implementing a quadrotor\nas an autonomous vehicle and then experimentally comparing the software\ncomponents with real-world flights.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 04:06:22 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 05:30:28 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Shah", "Shital", ""], ["Dey", "Debadeepta", ""], ["Lovett", "Chris", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1705.05084", "submitter": "Bolun Cai", "authors": "Xiaoyi Jia, Xiangmin Xu, Bolun Cai, Kailing Guo", "title": "Single Image Super-Resolution Using Multi-Scale Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on convolutional neural network (CNN) have demonstrated\ntremendous improvements on single image super-resolution. However, the previous\nmethods mainly restore images from one single area in the low resolution (LR)\ninput, which limits the flexibility of models to infer various scales of\ndetails for high resolution (HR) output. Moreover, most of them train a\nspecific model for each up-scale factor. In this paper, we propose a\nmulti-scale super resolution (MSSR) network. Our network consists of\nmulti-scale paths to make the HR inference, which can learn to synthesize\nfeatures from different scales. This property helps reconstruct various kinds\nof regions in HR images. In addition, only one single model is needed for\nmultiple up-scale factors, which is more efficient without loss of restoration\nquality. Experiments on four public datasets demonstrate that the proposed\nmethod achieved state-of-the-art performance with fast speed.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 06:38:04 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Jia", "Xiaoyi", ""], ["Xu", "Xiangmin", ""], ["Cai", "Bolun", ""], ["Guo", "Kailing", ""]]}, {"id": "1705.05102", "submitter": "Hassan Foroosh", "authors": "Amara Tariq and Hassan Foroosh", "title": "Learning Semantics for Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image search and retrieval engines rely heavily on textual annotation in\norder to match word queries to a set of candidate images. A system that can\nautomatically annotate images with meaningful text can be highly beneficial for\nsuch engines. Currently, the approaches to develop such systems try to\nestablish relationships between keywords and visual features of images. In this\npaper, We make three main contributions to this area: (i) We transform this\nproblem from the low-level keyword space to the high-level semantics space that\nwe refer to as the \"{\\em image theme}\", (ii) Instead of treating each possible\nkeyword independently, we use latent Dirichlet allocation to learn image themes\nfrom the associated texts in a training phase. Images are then annotated with\nimage themes rather than keywords, using a modified continuous relevance model,\nwhich takes into account the spatial coherence and the visual continuity among\nimages of common theme. (iii) To achieve more coherent annotations among images\nof common theme, we have integrated ConceptNet in learning the semantics of\nimages, and hence augment image descriptions beyond annotations provided by\nhumans. Images are thus further annotated by a few most significant words of\nthe prominent image theme. Our extensive experiments show that a coherent\ntheme-based image annotation using high-level semantics results in improved\nprecision and recall as compared with equivalent classical keyword annotation\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:45:10 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Tariq", "Amara", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.05108", "submitter": "Liangli Zhen", "authors": "Liangli Zhen, Dezhong Peng, Wei Wang, Xin Yao", "title": "Kernel Truncated Regression Representation for Robust Subspace\n  Clustering", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering aims to group data points into multiple clusters of which\neach corresponds to one subspace. Most existing subspace clustering approaches\nassume that input data lie on linear subspaces. In practice, however, this\nassumption usually does not hold. To achieve nonlinear subspace clustering, we\npropose a novel method, called kernel truncated regression representation. Our\nmethod consists of the following four steps: 1) projecting the input data into\na hidden space, where each data point can be linearly represented by other data\npoints; 2) calculating the linear representation coefficients of the data\nrepresentations in the hidden space; 3) truncating the trivial coefficients to\nachieve robustness and block-diagonality; and 4) executing the graph cutting\noperation on the coefficient matrix by solving a graph Laplacian problem. Our\nmethod has the advantages of a closed-form solution and the capacity of\nclustering data points that lie on nonlinear subspaces. The first advantage\nmakes our method efficient in handling large-scale datasets, and the second one\nenables the proposed method to conquer the nonlinear subspace clustering\nchallenge. Extensive experiments on six benchmarks demonstrate the\neffectiveness and the efficiency of the proposed method in comparison with\ncurrent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 08:16:34 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 15:33:56 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 09:16:24 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhen", "Liangli", ""], ["Peng", "Dezhong", ""], ["Wang", "Wei", ""], ["Yao", "Xin", ""]]}, {"id": "1705.05116", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Peter I. Corke", "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination", "comments": "2 pages, to appear in the Deep Learning for Robotic Vision (DLRV)\n  Workshop in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an end-to-end fine-tuning method to improve hand-eye\ncoordination in modular deep visuo-motor policies (modular networks) where each\nmodule is trained independently. Benefiting from weighted losses, the\nfine-tuning method significantly improves the performance of the policies for a\nrobotic planar reaching task.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 08:57:27 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Corke", "Peter I.", ""]]}, {"id": "1705.05126", "submitter": "Qingbo Wu", "authors": "Qingbo Wu and Hongliang Li and Fanman Meng and King N. Ngan", "title": "A Perceptually Weighted Rank Correlation Indicator for Objective Image\n  Quality Assessment", "comments": "This paper has been submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2799331", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of objective image quality assessment (IQA), the Spearman's\n$\\rho$ and Kendall's $\\tau$ are two most popular rank correlation indicators,\nwhich straightforwardly assign uniform weight to all quality levels and assume\neach pair of images are sortable. They are successful for measuring the average\naccuracy of an IQA metric in ranking multiple processed images. However, two\nimportant perceptual properties are ignored by them as well. Firstly, the\nsorting accuracy (SA) of high quality images are usually more important than\nthe poor quality ones in many real world applications, where only the\ntop-ranked images would be pushed to the users. Secondly, due to the subjective\nuncertainty in making judgement, two perceptually similar images are usually\nhardly sortable, whose ranks do not contribute to the evaluation of an IQA\nmetric. To more accurately compare different IQA algorithms, we explore a\nperceptually weighted rank correlation indicator in this paper, which rewards\nthe capability of correctly ranking high quality images, and suppresses the\nattention towards insensitive rank mistakes. More specifically, we focus on\nactivating `valid' pairwise comparison towards image quality, whose difference\nexceeds a given sensory threshold (ST). Meanwhile, each image pair is assigned\nan unique weight, which is determined by both the quality level and rank\ndeviation. By modifying the perception threshold, we can illustrate the sorting\naccuracy with a more sophisticated SA-ST curve, rather than a single rank\ncorrelation coefficient. The proposed indicator offers a new insight for\ninterpreting visual perception behaviors. Furthermore, the applicability of our\nindicator is validated in recommending robust IQA metrics for both the degraded\nand enhanced image data.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 09:24:05 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wu", "Qingbo", ""], ["Li", "Hongliang", ""], ["Meng", "Fanman", ""], ["Ngan", "King N.", ""]]}, {"id": "1705.05207", "submitter": "Lianwen Jin", "authors": "Xuefeng Xiao, Yafeng Yang, Tasweer Ahmad, Lianwen Jin and Tianhai\n  Chang", "title": "Design of a Very Compact CNN Classifier for Online Handwritten Chinese\n  Character Recognition Using DropWeight and Global Pooling", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, owing to the ubiquity of mobile devices, online handwritten\nChinese character recognition (HCCR) has become one of the suitable choice for\nfeeding input to cell phones and tablet devices. Over the past few years,\nlarger and deeper convolutional neural networks (CNNs) have extensively been\nemployed for improving character recognition performance. However, its\nsubstantial storage requirement is a significant obstacle in deploying such\nnetworks into portable electronic devices. To circumvent this problem, we\npropose a novel technique called DropWeight for pruning redundant connections\nin the CNN architecture. It is revealed that the proposed method not only\ntreats streamlined architectures such as AlexNet and VGGNet well but also\nexhibits remarkable performance for deep residual network and inception\nnetwork. We also demonstrate that global pooling is a better choice for\nbuilding very compact online HCCR systems. Experiments were performed on the\nICDAR-2013 online HCCR competition dataset using our proposed network, and it\nis found that the proposed approach requires only 0.57 MB for storage, whereas\nstate-of-the-art CNN-based methods require up to 135 MB; meanwhile the\nperformance is decreased only by 0.91%.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 13:18:38 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Xiao", "Xuefeng", ""], ["Yang", "Yafeng", ""], ["Ahmad", "Tasweer", ""], ["Jin", "Lianwen", ""], ["Chang", "Tianhai", ""]]}, {"id": "1705.05273", "submitter": "Ebenezer Isaac", "authors": "Ebenezer Isaac, Susan Elias, Srinivasan Rajagopalan, K.S. Easwarakumar", "title": "View-invariant Gait Recognition through Genetic Template Segmentation", "comments": "Published in IEEE Signal Processing Letters. Received April 24, 2017,\n  revised June 6, 2017, accepted June 10, 2017, published June 14, 2017", "journal-ref": "IEEE Signal Processing Letters, vol. 24, no. 8, pp. 1188-1192,\n  2017", "doi": "10.1109/LSP.2017.2715179", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-based model-free approach provides by far the most successful\nsolution to the gait recognition problem in literature. Recent work discusses\nhow isolating the head and leg portion of the template increase the performance\nof a gait recognition system making it robust against covariates like clothing\nand carrying conditions. However, most involve a manual definition of the\nboundaries. The method we propose, the genetic template segmentation (GTS),\nemploys the genetic algorithm to automate the boundary selection process. This\nmethod was tested on the GEI, GEnI and AEI templates. GEI seems to exhibit the\nbest result when segmented with our approach. Experimental results depict that\nour approach significantly outperforms the existing implementations of\nview-invariant gait recognition.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:44:44 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 06:32:39 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 08:46:36 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Isaac", "Ebenezer", ""], ["Elias", "Susan", ""], ["Rajagopalan", "Srinivasan", ""], ["Easwarakumar", "K. S.", ""]]}, {"id": "1705.05301", "submitter": "Paschalis Panteleris", "authors": "Paschalis Panteleris (1) and Antonis Argyros (1 and 2) ((1) Institute\n  of Computer Science, FORTH, (2) Computer Science Department, University of\n  Crete)", "title": "Back to RGB: 3D tracking of hands and hand-object interactions based on\n  short-baseline stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel solution to the problem of 3D tracking of the articulated\nmotion of human hand(s), possibly in interaction with other objects. The vast\nmajority of contemporary relevant work capitalizes on depth information\nprovided by RGBD cameras. In this work, we show that accurate and efficient 3D\nhand tracking is possible, even for the case of RGB stereo. A straightforward\napproach for solving the problem based on such input would be to first recover\ndepth and then apply a state of the art depth-based 3D hand tracking method.\nUnfortunately, this does not work well in practice because the stereo-based,\ndense 3D reconstruction of hands is far less accurate than the one obtained by\nRGBD cameras. Our approach bypasses 3D reconstruction and follows a completely\ndifferent route: 3D hand tracking is formulated as an optimization problem\nwhose solution is the hand configuration that maximizes the color consistency\nbetween the two views of the hand. We demonstrate the applicability of our\nmethod for real time tracking of a single hand, of a hand manipulating an\nobject and of two interacting hands. The method has been evaluated\nquantitatively on standard datasets and in comparison to relevant, state of the\nart RGBD-based approaches. The obtained results demonstrate that the proposed\nstereo-based method performs equally well to its RGBD-based competitors, and in\nsome cases, it even outperforms them.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 15:38:56 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Panteleris", "Paschalis", "", "1 and 2"], ["Argyros", "Antonis", "", "1 and 2"]]}, {"id": "1705.05363", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "comments": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent's ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:56:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1705.05435", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yasin Almalioglu, Ender Konukoglu, Metin Sitti", "title": "A Deep Learning Based 6 Degree-of-Freedom Localization Method for\n  Endoscopic Capsule Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust deep learning based 6 degrees-of-freedom (DoF)\nlocalization system for endoscopic capsule robots. Our system mainly focuses on\nlocalization of endoscopic capsule robots inside the GI tract using only visual\ninformation captured by a mono camera integrated to the robot. The proposed\nsystem is a 23-layer deep convolutional neural network (CNN) that is capable to\nestimate the pose of the robot in real time using a standard CPU. The dataset\nfor the evaluation of the system was recorded inside a surgical human stomach\nmodel with realistic surface texture, softness, and surface liquid properties\nso that the pre-trained CNN architecture can be transferred confidently into a\nreal endoscopic scenario. An average error of 7:1% and 3:4% for translation and\nrotation has been obtained, respectively. The results accomplished from the\nexperiments demonstrate that a CNN pre-trained with raw 2D endoscopic images\nperforms accurately inside the GI tract and is robust to various challenges\nposed by reflection distortions, lens imperfections, vignetting, noise, motion\nblur, low resolution, and lack of unique landmarks to track.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 20:33:37 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Turan", "Mehmet", ""], ["Almalioglu", "Yasin", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1705.05444", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yasin Almalioglu, Helder Araujo, Ender Konukoglu, Metin\n  Sitti", "title": "A Non-Rigid Map Fusion-Based RGB-Depth SLAM Method for Endoscopic\n  Capsule Robots", "comments": null, "journal-ref": null, "doi": "10.1007/s41315-017-0036-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is considered as a minimally invasive novel diagnostic\ntechnology to inspect the entire GI tract and to diagnose various diseases and\npathologies. Since the development of this technology, medical device companies\nand many groups have made significant progress to turn such passive capsule\nendoscopes into robotic active capsule endoscopes to achieve almost all\nfunctions of current active flexible endoscopes. However, the use of robotic\ncapsule endoscopy still has some challenges. One such challenge is the precise\nlocalization of such active devices in 3D world, which is essential for a\nprecise three-dimensional (3D) mapping of the inner organ. A reliable 3D map of\nthe explored inner organ could assist the doctors to make more intuitive and\ncorrect diagnosis. In this paper, we propose to our knowledge for the first\ntime in literature a visual simultaneous localization and mapping (SLAM) method\nspecifically developed for endoscopic capsule robots. The proposed RGB-Depth\nSLAM method is capable of capturing comprehensive dense globally consistent\nsurfel-based maps of the inner organs explored by an endoscopic capsule robot\nin real time. This is achieved by using dense frame-to-model camera tracking\nand windowed surfelbased fusion coupled with frequent model refinement through\nnon-rigid surface deformations.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 20:42:29 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Turan", "Mehmet", ""], ["Almalioglu", "Yasin", ""], ["Araujo", "Helder", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1705.05455", "submitter": "Saad Bin Ahmed", "authors": "Saad Bin Ahmed, Saeeda Naz, Salahuddin Swati, Muhammad Imran Razzak", "title": "Handwritten Urdu Character Recognition using 1-Dimensional BLSTM\n  Classifier", "comments": "10 pages, Accepted in NCA for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of cursive script is regarded as a subtle task in optical\ncharacter recognition due to its varied representation. Every cursive script\nhas different nature and associated challenges. As Urdu is one of cursive\nlanguage that is derived from Arabic script, thats why it nearly shares the\nsame challenges and difficulties even more harder. We can categorized Urdu and\nArabic language on basis of its script they use. Urdu is mostly written in\nNastaliq style whereas, Arabic follows Naskh style of writing. This paper\npresents new and comprehensive Urdu handwritten offline database name\nUrdu-Nastaliq Handwritten Dataset (UNHD). Currently, there is no standard and\ncomprehensive Urdu handwritten dataset available publicly for researchers. The\nacquired dataset covers commonly used ligatures that were written by 500\nwriters with their natural handwriting on A4 size paper. We performed\nexperiments using recurrent neural networks and reported a significant accuracy\nfor handwritten Urdu character recognition.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 21:13:08 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ahmed", "Saad Bin", ""], ["Naz", "Saeeda", ""], ["Swati", "Salahuddin", ""], ["Razzak", "Muhammad Imran", ""]]}, {"id": "1705.05483", "submitter": "Andrei Polzounov", "authors": "Andrei Polzounov, Artsiom Ablavatski, Sergio Escalera, Shijian Lu,\n  Jianfei Cai", "title": "WordFence: Text Detection in Natural Images with Border Awareness", "comments": "5 pages, 2 figures, ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, text recognition has achieved remarkable success in\nrecognizing scanned document text. However, word recognition in natural images\nis still an open problem, which generally requires time consuming\npost-processing steps. We present a novel architecture for individual word\ndetection in scene images based on semantic segmentation. Our contributions are\ntwofold: the concept of WordFence, which detects border areas surrounding each\nindividual word and a novel pixelwise weighted softmax loss function which\npenalizes background and emphasizes small text regions. WordFence ensures that\neach word is detected individually, and the new loss function provides a strong\ntraining signal to both text and word border localization. The proposed\ntechnique avoids intensive post-processing, producing an end-to-end word\ndetection system. We achieve superior localization recall on common benchmark\ndatasets - 92% recall on ICDAR11 and ICDAR13 and 63% recall on SVT.\nFurthermore, our end-to-end word recognition system achieves state-of-the-art\n86% F-Score on ICDAR13.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 23:42:59 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Polzounov", "Andrei", ""], ["Ablavatski", "Artsiom", ""], ["Escalera", "Sergio", ""], ["Lu", "Shijian", ""], ["Cai", "Jianfei", ""]]}, {"id": "1705.05498", "submitter": "Jing Zhang", "authors": "Jing Zhang and Wanqing Li and Philip Ogunbona", "title": "Joint Geometrical and Statistical Alignment for Visual Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unsupervised domain adaptation method for\ncross-domain visual recognition. We propose a unified framework that reduces\nthe shift between domains both statistically and geometrically, referred to as\nJoint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two\ncoupled projections that project the source domain and target domain data into\nlow dimensional subspaces where the geometrical shift and distribution shift\nare reduced simultaneously. The objective function can be solved efficiently in\na closed form. Extensive experiments have verified that the proposed method\nsignificantly outperforms several state-of-the-art domain adaptation methods on\na synthetic dataset and three different real world cross-domain visual\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 01:35:58 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1705.05508", "submitter": "Yong Khoo", "authors": "Yong Khoo, Sang Chung", "title": "Automated Body Structure Extraction from Arbitrary 3D Mesh", "comments": null, "journal-ref": "Imaging and Graphics, 2017", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated method for 3D character skeleton extraction\nthat can be applied for generic 3D shapes. Our work is motivated by the\nskeleton-based prior work on automatic rigging focused on skeleton extraction\nand can automatically aligns the extracted structure to fit the 3D shape of the\ngiven 3D mesh. The body mesh can be subsequently skinned based on the extracted\nskeleton and thus enables rigging process. In the experiment, we apply public\ndataset to drive the estimated skeleton from different body shapes, as well as\nthe real data obtained from 3D scanning systems. Satisfactory results are\nobtained compared to the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 02:58:44 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Khoo", "Yong", ""], ["Chung", "Sang", ""]]}, {"id": "1705.05512", "submitter": "Tanmay Batra", "authors": "Tanmay Batra, Devi Parikh", "title": "Cooperative Learning with Visual Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning paradigms involving varying levels of supervision have received a\nlot of interest within the computer vision and machine learning communities.\nThe supervisory information is typically considered to come from a human\nsupervisor -- a \"teacher\" figure. In this paper, we consider an alternate\nsource of supervision -- a \"peer\" -- i.e. a different machine. We introduce\ncooperative learning, where two agents trying to learn the same visual\nconcepts, but in potentially different environments using different sources of\ndata (sensors), communicate their current knowledge of these concepts to each\nother. Given the distinct sources of data in both agents, the mode of\ncommunication between the two agents is not obvious. We propose the use of\nvisual attributes -- semantic mid-level visual properties such as furry,\nwooden, etc.-- as the mode of communication between the agents. Our experiments\nin three domains -- objects, scenes, and animals -- demonstrate that our\nproposed cooperative learning approach improves the performance of both agents\nas compared to their performance if they were to learn in isolation. Our\napproach is particularly applicable in scenarios where privacy, security and/or\nbandwidth constraints restrict the amount and type of information the two\nagents can exchange.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 03:04:55 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Batra", "Tanmay", ""], ["Parikh", "Devi", ""]]}, {"id": "1705.05548", "submitter": "Leonid Keselman", "authors": "Leonid Keselman, John Iselin Woodfill, Anders Grunnet-Jepsen, Achintya\n  Bhowmik", "title": "Intel RealSense Stereoscopic Depth Cameras", "comments": "Accepted to CCD 2017, a CVPR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive overview of the stereoscopic Intel RealSense RGBD\nimaging systems. We discuss these systems' mode-of-operation, functional\nbehavior and include models of their expected performance, shortcomings, and\nlimitations. We provide information about the systems' optical characteristics,\ntheir correlation algorithms, and how these properties can affect different\napplications, including 3D reconstruction and gesture recognition. Our\ndiscussion covers the Intel RealSense R200 and the Intel RealSense D400\n(formally RS400).\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:36:11 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 05:31:14 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Keselman", "Leonid", ""], ["Woodfill", "John Iselin", ""], ["Grunnet-Jepsen", "Anders", ""], ["Bhowmik", "Achintya", ""]]}, {"id": "1705.05552", "submitter": "Yanchun Xie", "authors": "Jimin Xiao, Yanchun Xie, Tammam Tillo, Kaizhu Huang, Yunchao Wei,\n  Jiashi Feng", "title": "IAN: The Individual Aggregation Network for Person Search", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search in real-world scenarios is a new challenging computer version\ntask with many meaningful applications. The challenge of this task mainly comes\nfrom: (1) unavailable bounding boxes for pedestrians and the model needs to\nsearch for the person over the whole gallery images; (2) huge variance of\nvisual appearance of a particular person owing to varying poses, lighting\nconditions, and occlusions. To address these two critical issues in modern\nperson search applications, we propose a novel Individual Aggregation Network\n(IAN) that can accurately localize persons by learning to minimize intra-person\nfeature variations. IAN is built upon the state-of-the-art object detection\nframework, i.e., faster R-CNN, so that high-quality region proposals for\npedestrians can be produced in an online manner. In addition, to relieve the\nnegative effect caused by varying visual appearances of the same individual,\nIAN introduces a novel center loss that can increase the intra-class\ncompactness of feature representations. The engaged center loss encourages\npersons with the same identity to have similar feature characteristics.\nExtensive experimental results on two benchmarks, i.e., CUHK-SYSU and PRW, well\ndemonstrate the superiority of the proposed model. In particular, IAN achieves\n77.23% mAP and 80.45% top-1 accuracy on CUHK-SYSU, which outperform the\nstate-of-the-art by 1.7% and 1.85%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:55:02 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Xiao", "Jimin", ""], ["Xie", "Yanchun", ""], ["Tillo", "Tammam", ""], ["Huang", "Kaizhu", ""], ["Wei", "Yunchao", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.05619", "submitter": "Jiang Hao", "authors": "Hao Jiang", "title": "Research on Bi-mode Biometrics Based on Deep Learning", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In view of the fact that biological characteristics have excellent\nindependent distinguishing characteristics,biometric identification technology\ninvolves almost all the relevant areas of human distinction. Fingerprints,\niris, face, voice-print and other biological features have been widely used in\nthe public security departments to detect detection, mobile equipment unlock,\ntarget tracking and other fields. With the use of electronic devices more and\nmore widely and the frequency is getting higher and higher. Only the Biometrics\nidentification technology with excellent recognition rate can guarantee the\nlong-term development of these fields.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:55:05 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Jiang", "Hao", ""]]}, {"id": "1705.05627", "submitter": "Ryan Henderson", "authors": "Ryan Henderson and Rasmus Rothe", "title": "Picasso: A Modular Framework for Visualizing the Learning Process of\n  Neural Network Image Classifiers", "comments": "9 pages, submission to the Journal of Open Research Software,\n  github.com/merantix/picasso", "journal-ref": "Journal of Open Research Software. 5(1), p.22 (2017)", "doi": "10.5334/jors.178", "report-no": null, "categories": "cs.CV cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Picasso is a free open-source (Eclipse Public License) web application\nwritten in Python for rendering standard visualizations useful for analyzing\nconvolutional neural networks. Picasso ships with occlusion maps and saliency\nmaps, two visualizations which help reveal issues that evaluation metrics like\nloss and accuracy might hide: for example, learning a proxy classification\ntask. Picasso works with the Tensorflow deep learning framework, and Keras\n(when the model can be loaded into the Tensorflow backend). Picasso can be used\nwith minimal configuration by deep learning researchers and engineers alike\nacross various neural network architectures. Adding new visualizations is\nsimple: the user can specify their visualization code and HTML template\nseparately from the application code.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 10:06:19 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 16:22:49 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 12:35:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Henderson", "Ryan", ""], ["Rothe", "Rasmus", ""]]}, {"id": "1705.05640", "submitter": "Limin Wang", "authors": "Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, Jesse Berent, Abhinav\n  Gupta, Rahul Sukthankar, Luc Van Gool", "title": "WebVision Challenge: Visual Learning and Understanding With Web Data", "comments": "project page: http://www.vision.ee.ethz.ch/webvision/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2017 WebVision Challenge, a public image recognition challenge\ndesigned for deep learning based on web images without instance-level human\nannotation. Following the spirit of previous vision challenges, such as ILSVRC,\nPlaces2 and PASCAL VOC, which have played critical roles in the development of\ncomputer vision by contributing to the community with large scale annotated\ndata for model designing and standardized benchmarking, we contribute with this\nchallenge a large scale web images dataset, and a public competition with a\nworkshop co-located with CVPR 2017. The WebVision dataset contains more than\n$2.4$ million web images crawled from the Internet by using queries generated\nfrom the $1,000$ semantic concepts of the benchmark ILSVRC 2012 dataset. Meta\ninformation is also included. A validation set and test set containing human\nannotated images are also provided to facilitate algorithmic development. The\n2017 WebVision challenge consists of two tracks, the image classification task\non WebVision test set, and the transfer learning task on PASCAL VOC 2012\ndataset. In this paper, we describe the details of data collection and\nannotation, highlight the characteristics of the dataset, and introduce the\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 10:59:23 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Li", "Wen", ""], ["Wang", "Limin", ""], ["Li", "Wei", ""], ["Agustsson", "Eirikur", ""], ["Berent", "Jesse", ""], ["Gupta", "Abhinav", ""], ["Sukthankar", "Rahul", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.05665", "submitter": "Yao Lu", "authors": "Yao Lu, Zhirong Yang, Juho Kannala, Samuel Kaski", "title": "Learning Image Relations with Contrast Association Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the relations between two images is an important class of tasks in\ncomputer vision. Examples of such tasks include computing optical flow and\nstereo disparity. We treat the relation inference tasks as a machine learning\nproblem and tackle it with neural networks. A key to the problem is learning a\nrepresentation of relations. We propose a new neural network module, contrast\nassociation unit (CAU), which explicitly models the relations between two sets\nof input variables. Due to the non-negativity of the weights in CAU, we adopt a\nmultiplicative update algorithm for learning these weights. Experiments show\nthat neural networks with CAUs are more effective in learning five fundamental\nimage transformations than conventional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:09:44 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 21:44:39 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lu", "Yao", ""], ["Yang", "Zhirong", ""], ["Kannala", "Juho", ""], ["Kaski", "Samuel", ""]]}, {"id": "1705.05685", "submitter": "Yulong Wu", "authors": "Yulong Wu and John Tsotsos", "title": "Active Control of Camera Parameters for Object Detection Algorithms", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera parameters not only play an important role in determining the visual\nquality of perceived images, but also affect the performance of vision\nalgorithms, for a vision-guided robot. By quantitatively evaluating four object\ndetection algorithms, with respect to varying ambient illumination, shutter\nspeed and voltage gain, it is observed that the performance of the algorithms\nis highly dependent on these variables. From this observation, a novel active\ncontrol of camera parameters method is proposed, to make robot vision more\nrobust under different light conditions. Experimental results demonstrate the\neffectiveness of our proposed approach, which improves the performance of\nobject detection algorithms, compared with the conventional auto-exposure\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:47:47 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Wu", "Yulong", ""], ["Tsotsos", "John", ""]]}, {"id": "1705.05741", "submitter": "Hassan Foroosh", "authors": "Vildan Atalay Aydin and Hassan Foroosh", "title": "Motion-Compensated Temporal Filtering for Critically-Sampled\n  Wavelet-Encoded Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.04433,\n  arXiv:1705.04641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel motion estimation/compensation (ME/MC) method for\nwavelet-based (in-band) motion compensated temporal filtering (MCTF), with\napplication to low-bitrate video coding. Unlike the conventional in-band MCTF\nalgorithms, which require redundancy to overcome the shift-variance problem of\ncritically sampled (complete) discrete wavelet transforms (DWT), we perform\nME/MC steps directly on DWT coefficients by avoiding the need of\nshift-invariance. We omit upsampling, the inverse-DWT (IDWT), and the\ncalculation of redundant DWT coefficients, while achieving arbitrary subpixel\naccuracy without interpolation, and high video quality even at very\nlow-bitrates, by deriving the exact relationships between DWT subbands of input\nimage sequences. Experimental results demonstrate the accuracy of the proposed\nmethod, confirming that our model for ME/MC effectively improves video coding\nquality.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 07:41:37 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Aydin", "Vildan Atalay", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.05745", "submitter": "Hassan Foroosh", "authors": "Vildan Atalay Aydin and Hassan Foroosh", "title": "Volumetric Super-Resolution of Multispectral Data", "comments": "arXiv admin note: text overlap with arXiv:1705.01258", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most multispectral remote sensors (e.g. QuickBird, IKONOS, and Landsat 7\nETM+) provide low-spatial high-spectral resolution multispectral (MS) or\nhigh-spatial low-spectral resolution panchromatic (PAN) images, separately. In\norder to reconstruct a high-spatial/high-spectral resolution multispectral\nimage volume, either the information in MS and PAN images are fused (i.e.\npansharpening) or super-resolution reconstruction (SRR) is used with only MS\nimages captured on different dates. Existing methods do not utilize temporal\ninformation of MS and high spatial resolution of PAN images together to improve\nthe resolution. In this paper, we propose a multiframe SRR algorithm using\npansharpened MS images, taking advantage of both temporal and spatial\ninformation available in multispectral imagery, in order to exceed spatial\nresolution of given PAN images. We first apply pansharpening to a set of\nmultispectral images and their corresponding PAN images captured on different\ndates. Then, we use the pansharpened multispectral images as input to the\nproposed wavelet-based multiframe SRR method to yield full volumetric SRR. The\nproposed SRR method is obtained by deriving the subband relations between\nmultitemporal MS volumes. We demonstrate the results on Landsat 7 ETM+ images\ncomparing our method to conventional techniques.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 03:53:16 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Aydin", "Vildan Atalay", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.05787", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Learning Features for Offline Handwritten Signature Verification using\n  Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.05.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying the identity of a person using handwritten signatures is\nchallenging in the presence of skilled forgeries, where a forger has access to\na person's signature and deliberately attempt to imitate it. In offline\n(static) signature verification, the dynamic information of the signature\nwriting process is lost, and it is difficult to design good feature extractors\nthat can distinguish genuine signatures and skilled forgeries. This reflects in\na relatively poor performance, with verification errors around 7% in the best\nsystems in the literature. To address both the difficulty of obtaining good\nfeatures, as well as improve system performance, we propose learning the\nrepresentations from signature images, in a Writer-Independent format, using\nConvolutional Neural Networks. In particular, we propose a novel formulation of\nthe problem that includes knowledge of skilled forgeries from a subset of users\nin the feature learning process, that aims to capture visual cues that\ndistinguish genuine signatures and forgeries regardless of the user. Extensive\nexperiments were conducted on four datasets: GPDS, MCYT, CEDAR and Brazilian\nPUC-PR datasets. On GPDS-160, we obtained a large improvement in\nstate-of-the-art performance, achieving 1.72% Equal Error Rate, compared to\n6.97% in the literature. We also verified that the features generalize beyond\nthe GPDS dataset, surpassing the state-of-the-art performance in the other\ndatasets, without requiring the representation to be fine-tuned to each\nparticular dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:08:09 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1705.05804", "submitter": "Vamsi Ithapu", "authors": "Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, Vikas Singh", "title": "The Incremental Multiresolution Matrix Factorization Algorithm", "comments": "Computer Vision and Pattern Recognition (CVPR) 2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution analysis and matrix factorization are foundational tools in\ncomputer vision. In this work, we study the interface between these two\ndistinct topics and obtain techniques to uncover hierarchical block structure\nin symmetric matrices -- an important aspect in the success of many vision\nproblems. Our new algorithm, the incremental multiresolution matrix\nfactorization, uncovers such structure one feature at a time, and hence scales\nwell to large matrices. We describe how this multiscale analysis goes much\nfarther than what a direct global factorization of the data can identify. We\nevaluate the efficacy of the resulting factorizations for relative leveraging\nwithin regression tasks using medical imaging data. We also use the\nfactorization on representations learned by popular deep networks, providing\nevidence of their ability to infer semantic relationships even when they are\nnot explicitly trained to do so. We show that this algorithm can be used as an\nexploratory tool to improve the network architecture, and within numerous other\nsettings in vision.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:10:05 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ithapu", "Vamsi K.", ""], ["Kondor", "Risi", ""], ["Johnson", "Sterling C.", ""], ["Singh", "Vikas", ""]]}, {"id": "1705.05823", "submitter": "Oren Rippel", "authors": "Oren Rippel, Lubomir Bourdev", "title": "Real-Time Adaptive Image Compression", "comments": "Published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning-based approach to lossy image compression which\noutperforms all existing codecs, while running in real-time.\n  Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG\n2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of\ngeneric images across all quality levels. At the same time, our codec is\ndesigned to be lightweight and deployable: for example, it can encode or decode\nthe Kodak dataset in around 10ms per image on GPU.\n  Our architecture is an autoencoder featuring pyramidal analysis, an adaptive\ncoding module, and regularization of the expected codelength. We also\nsupplement our approach with adversarial training specialized towards use in a\ncompression setting: this enables us to produce visually pleasing\nreconstructions for very low bitrates.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:51:07 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Rippel", "Oren", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1705.05884", "submitter": "Babak Toghiani-Rizi", "authors": "Babak Toghiani-Rizi, Christofer Lind, Maria Svensson, Marcus Windmark", "title": "Static Gesture Recognition using Leap Motion", "comments": "Results based on a study conducted during the course Intelligent\n  Interactive Systems at Uppsala University, spring 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, an automated bartender system was developed for making orders\nin a bar using hand gestures. The gesture recognition of the system was\ndeveloped using Machine Learning techniques, where the model was trained to\nclassify gestures using collected data. The final model used in the system\nreached an average accuracy of 95%. The system raised ethical concerns both in\nterms of user interaction and having such a system in a real world scenario,\nbut it could initially work as a complement to a real bartender.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:38:20 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Toghiani-Rizi", "Babak", ""], ["Lind", "Christofer", ""], ["Svensson", "Maria", ""], ["Windmark", "Marcus", ""]]}, {"id": "1705.05885", "submitter": "Benjamin Kunsberg", "authors": "Daniel Niels Holtmann-Rice, Benjamin S. Kunsberg, Steven W. Zucker", "title": "What's In A Patch, I: Tensors, Differential Geometry and Statistical\n  Shading Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a linear algebraic framework for the shape-from-shading problem,\nbecause tensors arise when scalar (e.g. image) and vector (e.g. surface normal)\nfields are differentiated multiple times. The work is in two parts. In this\nfirst part we investigate when image derivatives exhibit invariance to changing\nillumination by calculating the statistics of image derivatives under general\ndistributions on the light source. We computationally validate the hypothesis\nthat image orientations (derivatives) provide increased invariance to\nillumination by showing (for a Lambertian model) that a shape-from-shading\nalgorithm matching gradients instead of intensities provides more accurate\nreconstructions when illumination is incorrectly estimated under a flatness\nprior.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:39:52 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Holtmann-Rice", "Daniel Niels", ""], ["Kunsberg", "Benjamin S.", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1705.05902", "submitter": "Benjamin Kunsberg", "authors": "Daniel Niels Holtmann-Rice, Benjamin S. Kunsberg, Steven W. Zucker", "title": "Tensors, Differential Geometry and Statistical Shading Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.05885", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a linear algebraic framework for the shape-from-shading problem,\nbecause tensors arise when scalar (e.g. image) and vector (e.g. surface normal)\nfields are differentiated multiple times. Using this framework, we first\ninvestigate when image derivatives exhibit invariance to changing illumination\nby calculating the statistics of image derivatives under general distributions\non the light source. Second, we apply that framework to develop Taylor-like\nexpansions, and build a boot-strapping algorithm to find the polynomial surface\nsolutions (under any light source) consistent with a given patch to arbitrary\norder. A generic constraint on the light source restricts these solutions to a\n2-D subspace, plus an unknown rotation matrix. It is this unknown matrix that\nencapsulates the ambiguity in the problem. Finally, we use the framework to\ncomputationally validate the hypothesis that image orientations (derivatives)\nprovide increased invariance to illumination by showing (for a Lambertian\nmodel) that a shape-from-shading algorithm matching gradients instead of\nintensities provides more accurate reconstructions when illumination is\nincorrectly estimated under a flatness prior.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 20:25:21 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 15:23:02 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Holtmann-Rice", "Daniel Niels", ""], ["Kunsberg", "Benjamin S.", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1705.05922", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Gokce Dane and Byeongkeun Kang and Vasudev\n  Bhaskaran and Truong Nguyen", "title": "LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object\n  Detection in Embedded Systems", "comments": "Embedded Vision Workshop in CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional Neural Networks (CNN) are the state-of-the-art performers\nfor object detection task. It is well known that object detection requires more\ncomputation and memory than image classification. Thus the consolidation of a\nCNN-based object detection for an embedded system is more challenging. In this\nwork, we propose LCDet, a fully-convolutional neural network for generic object\ndetection that aims to work in embedded systems. We design and develop an\nend-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit\nquantization on the learned weights. We use face detection as a use case. Our\nTF-Slim based network can predict different faces of different shapes and sizes\nin a single forward pass. Our experimental results show that the proposed\nmethod achieves comparative accuracy comparing with state-of-the-art CNN-based\nface detection methods, while reducing the model size by 3x and memory-BW by\n~4x comparing with one of the best real-time CNN-based object detector such as\nYOLO. TF 8-bit quantized model provides additional 4x memory reduction while\nkeeping the accuracy as good as the floating point model. The proposed model\nthus becomes amenable for embedded implementations.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 21:05:49 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Tripathi", "Subarna", ""], ["Dane", "Gokce", ""], ["Kang", "Byeongkeun", ""], ["Bhaskaran", "Vasudev", ""], ["Nguyen", "Truong", ""]]}, {"id": "1705.05994", "submitter": "Shikun Liu", "authors": "Shikun Liu, C. Lee Giles, Alexander G. Ororbia II", "title": "Learning a Hierarchical Latent-Variable Model of 3D Shapes", "comments": "Accepted as oral presentation at International Conference on 3D\n  Vision (3DV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Variational Shape Learner (VSL), a generative model that\nlearns the underlying structure of voxelized 3D shapes in an unsupervised\nfashion. Through the use of skip-connections, our model can successfully learn\nand infer a latent, hierarchical representation of objects. Furthermore,\nrealistic 3D objects can be easily generated by sampling the VSL's latent\nprobabilistic manifold. We show that our generative model can be trained\nend-to-end from 2D images to perform single image 3D model retrieval.\nExperiments show, both quantitatively and qualitatively, the improved\ngeneralization of our proposed model over a range of tasks, performing better\nor comparable to various state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 03:04:34 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 00:55:34 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 18:15:18 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 07:17:28 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Liu", "Shikun", ""], ["Giles", "C. Lee", ""], ["Ororbia", "Alexander G.", "II"]]}, {"id": "1705.05998", "submitter": "Tao Xiong", "authors": "Dong Yang, Tao Xiong, Daguang Xu, Qiangui Huang, David Liu, S.Kevin\n  Zhou, Zhoubing Xu, JinHyeong Park, Mingqing Chen, Trac D. Tran, Sang Peter\n  Chin, Dimitris Metaxas, Dorin Comaniciu", "title": "Automatic Vertebra Labeling in Large-Scale 3D CT using Deep\n  Image-to-Image Network with Message Passing and Sparsity Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic localization and labeling of vertebra in 3D medical images plays an\nimportant role in many clinical tasks, including pathological diagnosis,\nsurgical planning and postoperative assessment. However, the unusual conditions\nof pathological cases, such as the abnormal spine curvature, bright visual\nimaging artifacts caused by metal implants, and the limited field of view,\nincrease the difficulties of accurate localization. In this paper, we propose\nan automatic and fast algorithm to localize and label the vertebra centroids in\n3D CT volumes. First, we deploy a deep image-to-image network (DI2IN) to\ninitialize vertebra locations, employing the convolutional encoder-decoder\narchitecture together with multi-level feature concatenation and deep\nsupervision. Next, the centroid probability maps from DI2IN are iteratively\nevolved with the message passing schemes based on the mutual relation of\nvertebra centroids. Finally, the localization results are refined with sparsity\nregularization. The proposed method is evaluated on a public dataset of 302\nspine CT volumes with various pathologies. Our method outperforms other\nstate-of-the-art methods in terms of localization accuracy. The run time is\naround 3 seconds on average per case. To further boost the performance, we\nretrain the DI2IN on additional 1000+ 3D CT volumes from different patients. To\nthe best of our knowledge, this is the first time more than 1000 3D CT volumes\nwith expert annotation are adopted in experiments for the anatomic landmark\ndetection tasks. Our experimental results show that training with such a large\ndataset significantly improves the performance and the overall identification\nrate, for the first time by our knowledge, reaches 90 %.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 03:56:14 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Yang", "Dong", ""], ["Xiong", "Tao", ""], ["Xu", "Daguang", ""], ["Huang", "Qiangui", ""], ["Liu", "David", ""], ["Zhou", "S. Kevin", ""], ["Xu", "Zhoubing", ""], ["Park", "JinHyeong", ""], ["Chen", "Mingqing", ""], ["Tran", "Trac D.", ""], ["Chin", "Sang Peter", ""], ["Metaxas", "Dimitris", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1705.06000", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "One Shot Joint Colocalization and Cosegmentation", "comments": "8 pages, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework in which image cosegmentation and\ncolocalization are cast into a single optimization problem that integrates\ninformation from low level appearance cues with that of high level localization\ncues in a very weakly supervised manner. In contrast to multi-task learning\nparadigm that learns similar tasks using a shared representation, the proposed\nframework leverages two representations at different levels and simultaneously\ndiscriminates between foreground and background at the bounding box and\nsuperpixel level using discriminative clustering. We show empirically that\nconstraining the two problems at different scales enables the transfer of\nsemantic localization cues to improve cosegmentation output whereas local\nappearance based segmentation cues help colocalization. The unified framework\noutperforms strong baseline approaches, of learning the two problems\nseparately, by a large margin on four benchmark datasets. Furthermore, it\nobtains competitive results compared to the state of the art for cosegmentation\non two benchmark datasets and second best result for colocalization on Pascal\nVOC 2007.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 04:18:19 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "1705.06011", "submitter": "Yeong-Jun Cho", "authors": "Yeong-Jun Cho and Kuk-Jin Yoon", "title": "PaMM: Pose-aware Multi-shot Matching for Improving Person\n  Re-identification", "comments": "12 pages, 12 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TIP.2018.2815840", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is the problem of recognizing people across\ndifferent images or videos with non-overlapping views. Although there has been\nmuch progress in person re-identification over the last decade, it remains a\nchallenging task because appearances of people can seem extremely different\nacross diverse camera viewpoints and person poses. In this paper, we propose a\nnovel framework for person re-identification by analyzing camera viewpoints and\nperson poses in a so-called Pose-aware Multi-shot Matching (PaMM), which\nrobustly estimates people's poses and efficiently conducts multi-shot matching\nbased on pose information. Experimental results using public person\nre-identification datasets show that the proposed methods outperform\nstate-of-the-art methods and are promising for person re-identification from\ndiverse viewpoints and pose variances.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 05:16:42 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Cho", "Yeong-Jun", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1705.06057", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (Palaiseau, OBELIX), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Joint Learning from Earth Observation and OpenStreetMap Data to Get\n  Faster Better Semantic Maps", "comments": null, "journal-ref": "EARTHVISION 2017 IEEE/ISPRS CVPR Workshop. Large Scale Computer\n  Vision for Remote Sensing Imagery, Jul 2017, Honolulu, United States. 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the use of OpenStreetMap data for semantic\nlabeling of Earth Observation images. Deep neural networks have been used in\nthe past for remote sensing data classification from various sensors, including\nmultispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has\nalready been used as ground truth data for training such networks, this\nabundant data source remains rarely exploited as an input information layer. In\nthis paper, we study different use cases and deep network architectures to\nleverage OpenStreetMap data for semantic labeling of aerial and satellite\nimages. Especially , we look into fusion based architectures and coarse-to-fine\nsegmentation to include the OpenStreetMap layer into multispectral-based deep\nfully convolutional networks. We illustrate how these methods can be\nsuccessfully used on two public datasets: ISPRS Potsdam and DFC2017. We show\nthat OpenStreetMap data can efficiently be integrated into the vision-based\ndeep learning models and that it significantly improves both the accuracy\nperformance and the convergence speed of the networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 09:07:08 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Audebert", "Nicolas", "", "Palaiseau, OBELIX"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1705.06091", "submitter": "Mairead Grogan", "authors": "Mair\\'ead Grogan and Rozenn Dahyot", "title": "Robust Registration of Gaussian Mixtures for Colour Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible approach to colour transfer inspired by techniques\nrecently proposed for shape registration. Colour distributions of the palette\nand target images are modelled with Gaussian Mixture Models (GMMs) that are\nrobustly registered to infer a non linear parametric transfer function. We show\nexperimentally that our approach compares well to current techniques both\nquantitatively and qualitatively. Moreover, our technique is computationally\nthe fastest and can take efficient advantage of parallel processing\narchitectures for recolouring images and videos. Our transfer function is\nparametric and hence can be stored in memory for later usage and also combined\nwith other computed transfer functions to create interesting visual effects.\nOverall this paper provides a fast user friendly approach to recolouring of\nimage and video materials.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 11:11:14 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Grogan", "Mair\u00e9ad", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1705.06196", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yasin Almalioglu, Hunter Gilbert, Helder Araujo, Ender\n  Konukoglu, Metin Sitti", "title": "Magnetic-Visual Sensor Fusion based Medical SLAM for Endoscopic Capsule\n  Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable, real-time simultaneous localization and mapping (SLAM) method is\ncrucial for the navigation of actively controlled capsule endoscopy robots.\nThese robots are an emerging, minimally invasive diagnostic and therapeutic\ntechnology for use in the gastrointestinal (GI) tract. In this study, we\npropose a dense, non-rigidly deformable, and real-time map fusion approach for\nactively controlled endoscopic capsule robot applications. The method combines\nmagnetic and vision based localization, and makes use of frame-to-model fusion\nand model-to-model loop closure. The performance of the method is demonstrated\nusing an ex-vivo porcine stomach model. Across four trajectories of varying\nspeed and complexity, and across three cameras, the root mean square\nlocalization errors range from 0.42 to 1.92 cm, and the root mean square\nsurface reconstruction errors range from 1.23 to 2.39 cm.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:01:37 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 03:57:54 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Turan", "Mehmet", ""], ["Almalioglu", "Yasin", ""], ["Gilbert", "Hunter", ""], ["Araujo", "Helder", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1705.06260", "submitter": "Min Tang", "authors": "Min Tang, Sepehr Valipour, Zichen Vincent Zhang, Dana Cobzas, and\n  MartinJagersand", "title": "A deep level set method for image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel image segmentation approachthat integrates fully\nconvolutional networks (FCNs) with a level setmodel. Compared with a FCN, the\nintegrated method can incorporatesmoothing and prior information to achieve an\naccurate segmentation.Furthermore, different than using the level set model as\na post-processingtool, we integrate it into the training phase to fine-tune the\nFCN. Thisallows the use of unlabeled data during training in a\nsemi-supervisedsetting. Using two types of medical imaging data (liver CT and\nleft ven-tricle MRI data), we show that the integrated method achieves\ngoodperformance even when little training data is available, outperformingthe\nFCN or the level set model alone.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:01:16 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 16:14:14 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Tang", "Min", ""], ["Valipour", "Sepehr", ""], ["Zhang", "Zichen Vincent", ""], ["Cobzas", "Dana", ""], ["MartinJagersand", "", ""]]}, {"id": "1705.06264", "submitter": "Stanislav Filippov", "authors": "Stanislav Filippov, Arsenii Moiseev and Andronenko Andrey", "title": "Deep Diagnostics: Applying Convolutional Neural Networks for Vessels\n  Defects Detection", "comments": "Complaint to the article due to low research quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary angiography is considered to be a safe tool for the evaluation of\ncoronary artery disease and perform in approximately 12 million patients each\nyear worldwide. [1] In most cases, angiograms are manually analyzed by a\ncardiologist. Actually, there are no clinical practice algorithms which could\nimprove and automate this work. Neural networks show high efficiency in tasks\nof image analysis and they can be used for the analysis of angiograms and\nfacilitate diagnostics. We have developed an algorithm based on Convolutional\nNeural Network and Neural Network U-Net [2] for vessels segmentation and\ndefects detection such as stenosis. For our research we used anonymized\nangiography data obtained from one of the city's hospitals and augmented them\nto improve learning efficiency. U-Net usage provided high quality segmentation\nand the combination of our algorithm with an ensemble of classifiers shows a\ngood accuracy in the task of ischemia evaluation on test data. Subsequently,\nthis approach can be served as a basis for the creation of an analytical system\nthat could speed up the diagnosis of cardiovascular diseases and greatly\nfacilitate the work of a specialist.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:17:07 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 16:56:23 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Filippov", "Stanislav", ""], ["Moiseev", "Arsenii", ""], ["Andrey", "Andronenko", ""]]}, {"id": "1705.06300", "submitter": "Rui Chen", "authors": "Rui Chen, Huizhu Jia, Xiange Wen and Xiaodong Xie", "title": "Bayer Demosaicking Using Optimized Mean Curvature over RGB channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color artifacts of demosaicked images are often found at contours due to\ninterpolation across edges and cross-channel aliasing. To tackle this problem,\nwe propose a novel demosaicking method to reliably reconstruct color channels\nof a Bayer image based on two different optimized mean-curvature (MC) models.\nThe missing pixel values in green (G) channel are first estimated by minimizing\na variational MC model. The curvatures of restored G-image surface are\napproximated as a linear MC model which guides the initial reconstruction of\nred (R) and blue (B) channels. Then a refinement process is performed to\ninterpolate accurate full-resolution R and B images. Experiments on benchmark\nimages have testified to the superiority of the proposed method in terms of\nboth the objective and subjective quality.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 18:17:17 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Chen", "Rui", ""], ["Jia", "Huizhu", ""], ["Wen", "Xiange", ""], ["Xie", "Xiaodong", ""]]}, {"id": "1705.06333", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi, Rashed Karim, Kawal Rhode, Jeremy Burt, Ulas Bagci", "title": "CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins\n  from MRI Using Multi-View CNN", "comments": "The paper is accepted by MICCAI 2017 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical and biophysical modeling of left atrium (LA) and proximal\npulmonary veins (PPVs) is important for clinical management of several cardiac\ndiseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA\nand PPVs through visualization. However, there is a strong need for an advanced\nimage segmentation method to be applied to cardiac MRI for quantitative\nanalysis of LA and PPVs. In this study, we address this unmet clinical need by\nexploring a new deep learning-based segmentation strategy for quantification of\nLA and PPVs with high accuracy and heightened efficiency. Our approach is based\non a multi-view convolutional neural network (CNN) with an adaptive fusion\nstrategy and a new loss function that allows fast and more accurate convergence\nof the backpropagation based optimization. After training our network from\nscratch by using more than 60K 2D MRI images (slices), we have evaluated our\nsegmentation strategy to the STACOM 2013 cardiac segmentation challenge\nbenchmark. Qualitative and quantitative evaluations, obtained from the\nsegmentation challenge, indicate that the proposed method achieved the\nstate-of-the-art sensitivity (90%), specificity (99%), precision (94%), and\nefficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 20:18:32 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 15:57:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Karim", "Rashed", ""], ["Rhode", "Kawal", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1705.06362", "submitter": "Darvin Yi", "authors": "Darvin Yi, Rebecca Lynn Sawyer, David Cohn III, Jared Dunnmon, Carson\n  Lam, Xuerong Xiao, and Daniel Rubin", "title": "Optimizing and Visualizing Deep Learning for Benign/Malignant\n  Classification in Breast Tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer has the highest incidence and second highest mortality rate for\nwomen in the US. Our study aims to utilize deep learning for benign/malignant\nclassification of mammogram tumors using a subset of cases from the Digital\nDatabase of Screening Mammography (DDSM). Though it was a small dataset from\nthe view of Deep Learning (about 1000 patients), we show that currently state\nof the art architectures of deep learning can find a robust signal, even when\ntrained from scratch. Using convolutional neural networks (CNNs), we are able\nto achieve an accuracy of 85% and an ROC AUC of 0.91, while leading\nhand-crafted feature based methods are only able to achieve an accuracy of 71%.\nWe investigate an amalgamation of architectures to show that our best result is\nreached with an ensemble of the lightweight GoogLe Nets tasked with\ninterpreting both the coronal caudal view and the mediolateral oblique view,\nsimply averaging the probability scores of both views to make the final\nprediction. In addition, we have created a novel method to visualize what\nfeatures the neural network detects for the benign/malignant classification,\nand have correlated those features with well known radiological features, such\nas spiculation. Our algorithm significantly improves existing classification\nmethods for mammography lesions and identifies features that correlate with\nestablished clinical markers.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 22:35:28 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Yi", "Darvin", ""], ["Sawyer", "Rebecca Lynn", ""], ["Cohn", "David", "III"], ["Dunnmon", "Jared", ""], ["Lam", "Carson", ""], ["Xiao", "Xuerong", ""], ["Rubin", "Daniel", ""]]}, {"id": "1705.06368", "submitter": "Daniel Gordon", "authors": "Daniel Gordon, Ali Farhadi, Dieter Fox", "title": "Re3 : Real-Time Recurrent Regression Networks for Visual Tracking of\n  Generic Objects", "comments": "Presented at ICRA 2018", "journal-ref": "IEEE Robotics and Automation Letters 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object tracking requires knowledge and understanding of the object\nbeing tracked: its appearance, its motion, and how it changes over time. A\ntracker must be able to modify its underlying model and adapt to new\nobservations. We present Re3, a real-time deep object tracker capable of\nincorporating temporal information into its model. Rather than focusing on a\nlimited set of objects or training a model at test-time to track a specific\ninstance, we pretrain our generic tracker on a large variety of objects and\nefficiently update on the fly; Re3 simultaneously tracks and updates the\nappearance model with a single forward pass. This lightweight model is capable\nof tracking objects at 150 FPS, while attaining competitive results on\nchallenging benchmarks. We also show that our method handles temporary\nocclusion better than other comparable trackers using experiments that directly\nmeasure performance on sequences with occlusion.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:20:28 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 01:50:47 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 19:21:12 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Gordon", "Daniel", ""], ["Farhadi", "Ali", ""], ["Fox", "Dieter", ""]]}, {"id": "1705.06394", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Rainer Stiefelhagen, Kristen Grauman", "title": "Fashion Forward: Forecasting Visual Style in Fashion", "comments": "ICCV 2017. Project page:\n  https://cvhci.anthropomatik.kit.edu/~zalhalah/prj_fashion_forecast.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the future of fashion? Tackling this question from a data-driven\nvision perspective, we propose to forecast visual style trends before they\noccur. We introduce the first approach to predict the future popularity of\nstyles discovered from fashion images in an unsupervised manner. Using these\nstyles as a basis, we train a forecasting model to represent their trends over\ntime. The resulting model can hypothesize new mixtures of styles that will\nbecome popular in the future, discover style dynamics (trendy vs. classic), and\nname the key visual attributes that will dominate tomorrow's fashion. We\ndemonstrate our idea applied to three datasets encapsulating 80,000 fashion\nproducts sold across six years on Amazon. Results indicate that fashion\nforecasting benefits greatly from visual analysis, much more than textual or\nmeta-data cues surrounding products.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 02:26:52 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 22:50:54 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 03:06:15 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Stiefelhagen", "Rainer", ""], ["Grauman", "Kristen", ""]]}, {"id": "1705.06516", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F. Proen\\c{c}a and Yang Gao", "title": "Probabilistic Combination of Noisy Points and Planes for RGB-D Odometry", "comments": "Accepted to TAROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a visual odometry method that combines points and plane\nprimitives, extracted from a noisy depth camera. Depth measurement uncertainty\nis modelled and propagated through the extraction of geometric primitives to\nthe frame-to-frame motion estimation, where pose is optimized by weighting the\nresiduals of 3D point and planes matches, according to their uncertainties.\nResults on an RGB-D dataset show that the combination of points and planes,\nthrough the proposed method, is able to perform well in poorly textured\nenvironments, where point-based odometry is bound to fail.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 10:53:51 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Proen\u00e7a", "Pedro F.", ""], ["Gao", "Yang", ""]]}, {"id": "1705.06524", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yusuf Yigit Pilavci, Redhwan Jamiruddin, Helder Araujo,\n  Ender Konukoglu, Metin Sitti", "title": "A fully dense and globally consistent 3D map reconstruction approach for\n  GI tract to enhance therapeutic relevance of the endoscopic capsule robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is emerging as a novel, minimally invasive diagnostic\ntechnology for inspection of the GI tract and diagnosis of a wide range of\ndiseases and pathologies. Since the development of this technology, medical\ndevice companies and many research groups have made substantial progress in\nconverting passive capsule endoscopes to robotic active capsule endoscopes with\nmost of the functionality of current active flexible endoscopes. However,\nrobotic capsule endoscopy still has some challenges. In particular, the use of\nsuch devices to generate a precise three-dimensional (3D) mapping of the entire\ninner organ remains an unsolved problem. Such global 3D maps of inner organs\nwould help doctors to detect the location and size of diseased areas more\naccurately and intuitively, thus permitting more reliable diagnoses. To our\nknowledge, this paper presents the first complete pipeline for a complete 3D\nvisual map reconstruction of the stomach. The proposed pipeline is modular and\nincludes a preprocessing module, an image registration module, and a final\nshape-from-shading-based 3D reconstruction module; the 3D map is primarily\ngenerated by a combination of image stitching and shape-from-shading\ntechniques, and is updated in a frame-by-frame iterative fashion via capsule\nmotion inside the stomach. A comprehensive quantitative analysis of the\nproposed 3D reconstruction method is performed using an esophagus gastro\nduodenoscopy simulator, three different endoscopic cameras, and a 3D optical\nscanner.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 11:12:32 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Turan", "Mehmet", ""], ["Pilavci", "Yusuf Yigit", ""], ["Jamiruddin", "Redhwan", ""], ["Araujo", "Helder", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1705.06560", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, Min\n  Sun", "title": "Agent-Centric Risk Assessment: Accident Anticipation and Risky Region\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For survival, a living agent must have the ability to assess risk (1) by\ntemporally anticipating accidents before they occur, and (2) by spatially\nlocalizing risky regions in the environment to move away from threats. In this\npaper, we take an agent-centric approach to study the accident anticipation and\nrisky region localization tasks. We propose a novel soft-attention Recurrent\nNeural Network (RNN) which explicitly models both spatial and appearance-wise\nnon-linear interaction between the agent triggering the event and another agent\nor static-region involved. In order to test our proposed method, we introduce\nthe Epic Fail (EF) dataset consisting of 3000 viral videos capturing various\naccidents. In the experiments, we evaluate the risk assessment accuracy both in\nthe temporal domain (accident anticipation) and spatial domain (risky region\nlocalization) on our EF dataset and the Street Accident (SA) dataset. Our\nmethod consistently outperforms other baselines on both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 12:56:20 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chou", "Shih-Han", ""], ["Chan", "Fu-Hsiang", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1705.06566", "submitter": "Nikolay Jetchev", "authors": "Urs Bergmann, Nikolay Jetchev, Roland Vollgraf", "title": "Learning Texture Manifolds with the Periodic Spatial GAN", "comments": "Proceedings of the 34th International Conference on Machine Learning,\n  Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to texture synthesis based on\ngenerative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the\nstructure of the input noise distribution by constructing tensors with\ndifferent types of dimensions. We call this technique Periodic Spatial GAN\n(PSGAN). The PSGAN has several novel abilities which surpass the current state\nof the art in texture synthesis. First, we can learn multiple textures from\ndatasets of one or more complex large images. Second, we show that the image\ngeneration with PSGANs has properties of a texture manifold: we can smoothly\ninterpolate between samples in the structured noise space and generate novel\nsamples, which lie perceptually between the textures of the original dataset.\nIn addition, we can also accurately learn periodical textures. We make multiple\nexperiments which show that PSGANs can flexibly handle diverse texture and\nimage data sources. Our method is highly scalable and it can generate output\nimages of arbitrary large size.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:09:45 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 21:26:03 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bergmann", "Urs", ""], ["Jetchev", "Nikolay", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1705.06599", "submitter": "Boyue Wang", "authors": "Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun and Baocai Yin", "title": "Localized LRR on Grassmann Manifolds: An Extrinsic View", "comments": "IEEE Transactions on Circuits and Systems for Video Technology with\n  Minor Revisions. arXiv admin note: text overlap with arXiv:1504.01807", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace data representation has recently become a common practice in many\ncomputer vision tasks. It demands generalizing classical machine learning\nalgorithms for subspace data. Low-Rank Representation (LRR) is one of the most\nsuccessful models for clustering vectorial data according to their subspace\nstructures. This paper explores the possibility of extending LRR for subspace\ndata on Grassmann manifolds. Rather than directly embedding the Grassmann\nmanifolds into the symmetric matrix space, an extrinsic view is taken to build\nthe LRR self-representation in the local area of the tangent space at each\nGrassmannian point, resulting in a localized LRR method on Grassmann manifolds.\nA novel algorithm for solving the proposed model is investigated and\nimplemented. The performance of the new clustering algorithm is assessed\nthrough experiments on several real-world datasets including MNIST handwritten\ndigits, ballet video clips, SKIG action clips, DynTex++ dataset and highway\ntraffic video clips. The experimental results show the new method outperforms a\nnumber of state-of-the-art clustering methods\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 03:04:43 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1705.06628", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Simon J. Julier, Nicolai Marquardt and Nadia\n  Bianchi-Berthouze", "title": "Robust tracking of respiratory rate in high-dynamic range scenes using\n  mobile thermal imaging", "comments": "Vol. 8, No. 10, 1 Oct 2017, Biomedical Optics Express 4480 - Full\n  abstract can be found in this journal article (due to limited word counts of\n  'arXiv abstract')", "journal-ref": "Biomedical Optics Express, 2017", "doi": "10.1364/BOE.8.004480", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to monitor respiratory rate is extremely important for medical\ntreatment, healthcare and fitness sectors. In many situations, mobile methods,\nwhich allow users to undertake every day activities, are required. However,\ncurrent monitoring systems can be obtrusive, requiring users to wear\nrespiration belts or nasal probes. Recent advances in thermographic systems\nhave shrunk their size, weight and cost, to the point where it is possible to\ncreate smart-phone based respiration rate monitoring devices that are not\naffected by lighting conditions. However, mobile thermal imaging is challenged\nin scenes with high thermal dynamic ranges. This challenge is further amplified\nby general problems such as motion artifacts and low spatial resolution,\nleading to unreliable breathing signals. In this paper, we propose a novel and\nrobust approach for respiration tracking which compensates for the negative\neffects of variations in the ambient temperature and motion artifacts and can\naccurately extract breathing rates in highly dynamic thermal scenes. It has\nthree main contributions. The first is a novel Optimal Quantization technique\nwhich adaptively constructs a color mapping of absolute temperature to improve\nsegmentation, classification and tracking. The second is the Thermal Gradient\nFlow method that computes thermal gradient magnitude maps to enhance accuracy\nof the nostril region tracking. Finally, we introduce the Thermal Voxel method\nto increase the reliability of the captured respiration signals compared to the\ntraditional averaging method. We demonstrate the extreme robustness of our\nsystem to track the nostril-region and measure the respiratory rate in high\ndynamic range scenes.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:49:03 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 21:06:05 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Cho", "Youngjun", ""], ["Julier", "Simon J.", ""], ["Marquardt", "Nicolai", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1705.06676", "submitter": "Hedi Ben-Younes", "authors": "Hedi Ben-younes, R\\'emi Cadene, Matthieu Cord, Nicolas Thome", "title": "MUTAN: Multimodal Tucker Fusion for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear models provide an appealing framework for mixing and merging\ninformation in Visual Question Answering (VQA) tasks. They help to learn high\nlevel associations between question meaning and visual concepts in the image,\nbut they suffer from huge dimensionality issues. We introduce MUTAN, a\nmultimodal tensor-based Tucker decomposition to efficiently parametrize\nbilinear interactions between visual and textual representations. Additionally\nto the Tucker framework, we design a low-rank matrix-based decomposition to\nexplicitly constrain the interaction rank. With MUTAN, we control the\ncomplexity of the merging scheme while keeping nice interpretable fusion\nrelations. We show how our MUTAN model generalizes some of the latest VQA\narchitectures, providing state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:23:22 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ben-younes", "Hedi", ""], ["Cadene", "R\u00e9mi", ""], ["Cord", "Matthieu", ""], ["Thome", "Nicolas", ""]]}, {"id": "1705.06687", "submitter": "Nick Johnston", "authors": "Michele Covell, Nick Johnston, David Minnen, Sung Jin Hwang, Joel\n  Shor, Saurabh Singh, Damien Vincent, George Toderici", "title": "Target-Quality Image Compression with Recurrent, Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stop-code tolerant (SCT) approach to training recurrent\nconvolutional neural networks for lossy image compression. Our methods\nintroduce a multi-pass training method to combine the training goals of\nhigh-quality reconstructions in areas around stop-code masking as well as in\nhighly-detailed areas. These methods lead to lower true bitrates for a given\nrecursion count, both pre- and post-entropy coding, even using unstructured\nLZ77 code compression. The pre-LZ77 gains are achieved by trimming stop codes.\nThe post-LZ77 gains are due to the highly unequal distributions of 0/1 codes\nfrom the SCT architectures. With these code compressions, the SCT architecture\nmaintains or exceeds the image quality at all compression rates compared to\nJPEG and to RNN auto-encoders across the Kodak dataset. In addition, the SCT\ncoding results in lower variance in image quality across the extent of the\nimage, a characteristic that has been shown to be important in human ratings of\nimage quality\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:44:31 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Covell", "Michele", ""], ["Johnston", "Nick", ""], ["Minnen", "David", ""], ["Hwang", "Sung Jin", ""], ["Shor", "Joel", ""], ["Singh", "Saurabh", ""], ["Vincent", "Damien", ""], ["Toderici", "George", ""]]}, {"id": "1705.06709", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Viktor Rozgic, Sancar Adali", "title": "Learning Spatiotemporal Features for Infrared Action Recognition with 3D\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) imaging has the potential to enable more robust action\nrecognition systems compared to visible spectrum cameras due to lower\nsensitivity to lighting conditions and appearance variability. While the action\nrecognition task on videos collected from visible spectrum imaging has received\nmuch attention, action recognition in IR videos is significantly less explored.\nOur objective is to exploit imaging data in this modality for the action\nrecognition task. In this work, we propose a novel two-stream 3D convolutional\nneural network (CNN) architecture by introducing the discriminative code layer\nand the corresponding discriminative code loss function. The proposed network\nprocesses IR image and the IR-based optical flow field sequences. We pretrain\nthe 3D CNN model on the visible spectrum Sports-1M action dataset and finetune\nit on the Infrared Action Recognition (InfAR) dataset. To our best knowledge,\nthis is the first application of the 3D CNN to action recognition in the IR\ndomain. We conduct an elaborate analysis of different fusion schemes (weighted\naverage, single and double-layer neural nets) applied to different 3D CNN\noutputs. Experimental results demonstrate that our approach can achieve\nstate-of-the-art average precision (AP) performances on the InfAR dataset: (1)\nthe proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our\n3D CNN model applied to the optical flow fields achieves the best reported\nsingle stream 75.42% AP.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:26:34 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Rozgic", "Viktor", ""], ["Adali", "Sancar", ""]]}, {"id": "1705.06712", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Guillaume Pernelle, Lauren Barber, Steve Pieper, Dirk\n  Fortmeier, Sandy Wells, Heinz Handels, Tina Kapur", "title": "Model-based Catheter Segmentation in MRI-images", "comments": "MICCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable segmentation of catheters in MR-guided interventions\nremains a challenge, and a step of critical importance in clinical workflows.\nIn this work, under reasonable assumptions, mechanical model based heuristics\nguide the segmentation process allows correct catheter identification rates\ngreater than 98% (error 2.88 mm), and reduction in outliers to one-fourth\ncompared to the state of the art. Given distal tips, searching towards the\nproximal ends of the catheters is guided by mechanical models that are\nestimated on a per-catheter basis. Their bending characteristics are used to\nconstrain the image feature based candidate points. The final catheter\ntrajectories are hybrid sequences of individual points, each derived from model\nand image features. We evaluate the method on a database of 10 patient MRI\nscans including 101 manually segmented catheters. The mean errors were 1.40 mm\nand the median errors were 1.05 mm. The number of outliers deviating more than\n2 mm from the gold standard is 7, and the number of outliers deviating more\nthan 3 mm from the gold standard is just 2.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:28:53 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 07:55:13 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Pernelle", "Guillaume", ""], ["Barber", "Lauren", ""], ["Pieper", "Steve", ""], ["Fortmeier", "Dirk", ""], ["Wells", "Sandy", ""], ["Handels", "Heinz", ""], ["Kapur", "Tina", ""]]}, {"id": "1705.06755", "submitter": "Yao Wang", "authors": "Xi'ai Chen, Zhi Han, Yao Wang, Qian Zhao, Deyu Meng, Lin Lin, Yandong\n  Tang", "title": "A General Model for Robust Tensor Factorization with Unknown Noise", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the limitations of matrix factorization, such as losing spatial\nstructure information, the concept of low-rank tensor factorization (LRTF) has\nbeen applied for the recovery of a low dimensional subspace from high\ndimensional visual data. The low-rank tensor recovery is generally achieved by\nminimizing the loss function between the observed data and the factorization\nrepresentation. The loss function is designed in various forms under different\nnoise distribution assumptions, like $L_1$ norm for Laplacian distribution and\n$L_2$ norm for Gaussian distribution. However, they often fail to tackle the\nreal data which are corrupted by the noise with unknown distribution. In this\npaper, we propose a generalized weighted low-rank tensor factorization method\n(GWLRTF) integrated with the idea of noise modelling. This procedure treats the\ntarget data as high-order tensor directly and models the noise by a Mixture of\nGaussians, which is called MoG GWLRTF. The parameters in the model are\nestimated under the EM framework and through a new developed algorithm of\nweighted low-rank tensor factorization. We provide two versions of the\nalgorithm with different tensor factorization operations, i.e., CP\nfactorization and Tucker factorization. Extensive experiments indicate the\nrespective advantages of this two versions in different applications and also\ndemonstrate the effectiveness of MoG GWLRTF compared with other competing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 18:04:15 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Chen", "Xi'ai", ""], ["Han", "Zhi", ""], ["Wang", "Yao", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Lin", "Lin", ""], ["Tang", "Yandong", ""]]}, {"id": "1705.06778", "submitter": "Martin Mundt", "authors": "Martin Mundt, Tobias Weis, Kishore Konda and Visvanathan Ramesh", "title": "Building effective deep neural network architectures one feature at a\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful training of convolutional neural networks is often associated with\nsufficiently deep architectures composed of high amounts of features. These\nnetworks typically rely on a variety of regularization and pruning techniques\nto converge to less redundant states. We introduce a novel bottom-up approach\nto expand representations in fixed-depth architectures. These architectures\nstart from just a single feature per layer and greedily increase width of\nindividual layers to attain effective representational capacities needed for a\nspecific task. While network growth can rely on a family of metrics, we propose\na computationally efficient version based on feature time evolution and\ndemonstrate its potency in determining feature importance and a networks'\neffective capacity. We demonstrate how automatically expanded architectures\nconverge to similar topologies that benefit from lesser amount of parameters or\nimproved accuracy and exhibit systematic correspondence in representational\ncomplexity with the specified task. In contrast to conventional design patterns\nwith a typical monotonic increase in the amount of features with increased\ndepth, we observe that CNNs perform better when there is more learnable\nparameters in intermediate, with falloffs to earlier and later layers.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:40:37 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 21:59:52 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Mundt", "Martin", ""], ["Weis", "Tobias", ""], ["Konda", "Kishore", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1705.06820", "submitter": "Hongyang Gao", "authors": "Hongyang Gao and Hao Yuan and Zhengyang Wang and Shuiwang Ji", "title": "Pixel Deconvolutional Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolutional layers have been widely used in a variety of deep models for\nup-sampling, including encoder-decoder networks for semantic segmentation and\ndeep generative models for unsupervised learning. One of the key limitations of\ndeconvolutional operations is that they result in the so-called checkerboard\nproblem. This is caused by the fact that no direct relationship exists among\nadjacent pixels on the output feature map. To address this problem, we propose\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\namong adjacent pixels on the up-sampled feature map. Our method is based on a\nfresh interpretation of the regular deconvolution operation. The resulting\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\nmanner without compromising the fully trainable capabilities of original\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\nthis can be overcome by an implementation trick. Experimental results on\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\nsuch as edges and shapes and yields more accurate segmentation outputs than\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\nlargely overcome the checkerboard problem suffered by regular deconvolution\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:31:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 17:17:07 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 18:17:31 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 02:53:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gao", "Hongyang", ""], ["Yuan", "Hao", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06821", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Hao Yuan, Shuiwang Ji", "title": "Spatial Variational Auto-Encoding via Matrix-Variate Normal\n  Distributions", "comments": "Accepted by SDM2019. Code is publicly available at\n  https://github.com/divelab/svae", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of variational auto-encoders (VAEs) resembles that of\ntraditional auto-encoder models in which spatial information is supposed to be\nexplicitly encoded in the latent space. However, the latent variables in VAEs\nare vectors, which can be interpreted as multiple feature maps of size 1x1.\nSuch representations can only convey spatial information implicitly when\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\nfeature maps of larger size as latent variables to explicitly capture spatial\ninformation. This is achieved by allowing the latent variables to be sampled\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\nfrom the encoder network. To increase dependencies among locations on latent\nfeature maps and reduce the number of parameters, we further propose spatial\nVAEs via low-rank MVN distributions. Experimental results show that the\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\nspatial information.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:32:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 17:48:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Zhengyang", ""], ["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06830", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin,\n  Jonathon Shlens", "title": "Exploring the structure of a real-time, arbitrary neural artistic\n  stylization network", "comments": "Accepted as an oral presentation at British Machine Vision Conference\n  (BMVC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method which combines the flexibility of the\nneural algorithm of artistic style with the speed of fast style transfer\nnetworks to allow real-time stylization using any content/style image pair. We\nbuild upon recent work leveraging conditional instance normalization for\nmulti-style transfer networks by learning to predict the conditional instance\nnormalization parameters directly from a style image. The model is successfully\ntrained on a corpus of roughly 80,000 paintings and is able to generalize to\npaintings previously unobserved. We demonstrate that the learned embedding\nspace is smooth and contains a rich structure and organizes semantic\ninformation associated with paintings in an entirely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 23:25:48 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 19:06:03 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Lee", "Honglak", ""], ["Kudlur", "Manjunath", ""], ["Dumoulin", "Vincent", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1705.06839", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Hamed Kiani Galoogahi, Chen-Hsuan Lin, and Simon Lucey", "title": "Deep-LK for Efficient Adaptive Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach for efficient regression based object\ntracking which we refer to as Deep- LK. Our approach is closely related to the\nGeneric Object Tracking Using Regression Networks (GOTURN) framework of Held et\nal. We make the following contributions. First, we demonstrate that there is a\ntheoretical relationship between siamese regression networks like GOTURN and\nthe classical Inverse-Compositional Lucas & Kanade (IC-LK) algorithm. Further,\nwe demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance\nof the currently tracked frame. We argue that this missing property in GOTURN\ncan be attributed to its poor performance on unseen objects and/or viewpoints.\nSecond, we propose a novel framework for object tracking - which we refer to as\nDeep-LK - that is inspired by the IC-LK framework. Finally, we show impressive\nresults demonstrating that Deep-LK substantially outperforms GOTURN.\nAdditionally, we demonstrate comparable tracking performance to current state\nof the art deep-trackers whilst being an order of magnitude (i.e. 100 FPS)\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 00:34:50 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 02:08:56 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wang", "Chaoyang", ""], ["Galoogahi", "Hamed Kiani", ""], ["Lin", "Chen-Hsuan", ""], ["Lucey", "Simon", ""]]}, {"id": "1705.06846", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M.W. Powers", "title": "A Predictive Account of Cafe Wall Illusions Using a Quantitative Model", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper explores the tilt illusion effect in the Cafe Wall pattern using a\nclassical Gaussian Receptive Field model. In this illusion, the mortar lines\nare misperceived as diverging or converging rather than horizontal. We examine\nthe capability of a simple bioplausible filtering model to recognize different\ndegrees of tilt effect in the Cafe Wall illusion based on different\ncharacteristics of the pattern. Our study employed a Difference of Gaussians\nmodel of retinal to cortical ON center and/or OFF center receptive fields. A\nwide range of parameters of the stimulus, for example mortar thickness,\nluminance, tiles contrast, phase of the tile displacement, have been studied.\nOur model constructs an edge map representation at multiple scales that reveals\ntilt cues and clues involved in the illusory perception of the Cafe Wall\npattern. We present here that our model can not only detect the tilt in this\npattern, but also can predict the strength of the illusion and quantify the\ndegree of tilt. For the first time quantitative predictions of a model are\nreported for this stimulus. The results of our simulations are consistent with\nprevious psychophysical findings across the full range of Cafe Wall variations\ntested. Our results also suggest that the Difference of Gaussians mechanism is\nthe heart of the effects explained by, and the mechanisms proposed for, the\nIrradiation, Brightness Induction, and Bandpass Filtering models.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 01:59:04 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 11:47:51 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 04:22:12 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 09:19:54 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""]]}, {"id": "1705.06849", "submitter": "Lianwen Jin", "authors": "Songxuan Lai, Lianwen Jin, Weixin Yang", "title": "Online Signature Verification using Recurrent Neural Network and\n  Length-normalized Path Signature", "comments": "6 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the great success of recurrent neural networks (RNNs) in\nsequential modeling, we introduce a novel RNN system to improve the performance\nof online signature verification. The training objective is to directly\nminimize intra-class variations and to push the distances between skilled\nforgeries and genuine samples above a given threshold. By back-propagating the\ntraining signals, our RNN network produced discriminative features with desired\nmetrics. Additionally, we propose a novel descriptor, called the\nlength-normalized path signature (LNPS), and apply it to online signature\nverification. LNPS has interesting properties, such as scale invariance and\nrotation invariance after linear combination, and shows promising results in\nonline signature verification. Experiments on the publicly available SVC-2004\ndataset yielded state-of-the-art performance of 2.37% equal error rate (EER).\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 02:27:58 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""], ["Yang", "Weixin", ""]]}, {"id": "1705.06861", "submitter": "Qin Zhang", "authors": "Qin Zhang, Hui Wang, Junyu Dong, Guoqiang Zhong, Xin Sun", "title": "Prediction of Sea Surface Temperature using Long Short-Term Memory", "comments": "5 page, 5 figures", "journal-ref": null, "doi": "10.1109/LGRS.2017.2733548", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter adopts long short-term memory(LSTM) to predict sea surface\ntemperature(SST), which is the first attempt, to our knowledge, to use\nrecurrent neural network to solve the problem of SST prediction, and to make\none week and one month daily prediction. We formulate the SST prediction\nproblem as a time series regression problem. LSTM is a special kind of\nrecurrent neural network, which introduces gate mechanism into vanilla RNN to\nprevent the vanished or exploding gradient problem. It has strong ability to\nmodel the temporal relationship of time series data and can handle the\nlong-term dependency problem well. The proposed network architecture is\ncomposed of two kinds of layers: LSTM layer and full-connected dense layer.\nLSTM layer is utilized to model the time series relationship. Full-connected\nlayer is utilized to map the output of LSTM layer to a final prediction. We\nexplore the optimal setting of this architecture by experiments and report the\naccuracy of coastal seas of China to confirm the effectiveness of the proposed\nmethod. In addition, we also show its online updated characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 05:14:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhang", "Qin", ""], ["Wang", "Hui", ""], ["Dong", "Junyu", ""], ["Zhong", "Guoqiang", ""], ["Sun", "Xin", ""]]}, {"id": "1705.06869", "submitter": "Yan Yang", "authors": "Yan Yang, Jian Sun, Huibin Li, Zongben Xu", "title": "ADMM-Net: A Deep Learning Approach for Compressive Sensing MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is an effective approach for fast Magnetic Resonance\nImaging (MRI). It aims at reconstructing MR images from a small number of\nunder-sampled data in k-space, and accelerating the data acquisition in MRI. To\nimprove the current MRI system in reconstruction accuracy and speed, in this\npaper, we propose two novel deep architectures, dubbed ADMM-Nets in basic and\ngeneralized versions. ADMM-Nets are defined over data flow graphs, which are\nderived from the iterative procedures in Alternating Direction Method of\nMultipliers (ADMM) algorithm for optimizing a general CS-based MRI model. They\ntake the sampled k-space data as inputs and output reconstructed MR images.\nMoreover, we extend our network to cope with complex-valued MR images. In the\ntraining phase, all parameters of the nets, e.g., transforms, shrinkage\nfunctions, etc., are discriminatively trained end-to-end. In the testing phase,\nthey have computational overhead similar to ADMM algorithm but use optimized\nparameters learned from the data for CS-based reconstruction task. We\ninvestigate different configurations in network structures and conduct\nextensive experiments on MR image reconstruction under different sampling\nrates. Due to the combination of the advantages in model-based approach and\ndeep learning approach, the ADMM-Nets achieve state-of-the-art reconstruction\naccuracies with fast computational speed.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 06:33:18 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Yang", "Yan", ""], ["Sun", "Jian", ""], ["Li", "Huibin", ""], ["Xu", "Zongben", ""]]}, {"id": "1705.06870", "submitter": "Chuyang Ye", "authors": "Chuyang Ye, Jerry L. Prince", "title": "Fiber Orientation Estimation Guided by a Deep Network", "comments": "A shorter version is accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) is currently the only tool for\nnoninvasively imaging the brain's white matter tracts. The fiber orientation\n(FO) is a key feature computed from dMRI for fiber tract reconstruction.\nBecause the number of FOs in a voxel is usually small, dictionary-based sparse\nreconstruction has been used to estimate FOs with a relatively small number of\ndiffusion gradients. However, accurate FO estimation in regions with complex FO\nconfigurations in the presence of noise can still be challenging. In this work\nwe explore the use of a deep network for FO estimation in a dictionary-based\nframework and propose an algorithm named Fiber Orientation Reconstruction\nguided by a Deep Network (FORDN). FORDN consists of two steps. First, we use a\nsmaller dictionary encoding coarse basis FOs to represent the diffusion\nsignals. To estimate the mixture fractions of the dictionary atoms (and thus\ncoarse FOs), a deep network is designed specifically for solving the sparse\nreconstruction problem. Here, the smaller dictionary is used to reduce the\ncomputational cost of training. Second, the coarse FOs inform the final FO\nestimation, where a larger dictionary encoding dense basis FOs is used and a\nweighted l1-norm regularized least squares problem is solved to encourage FOs\nthat are consistent with the network output. FORDN was evaluated and compared\nwith state-of-the-art algorithms that estimate FOs using sparse reconstruction\non simulated and real dMRI data, and the results demonstrate the benefit of\nusing a deep network for FO estimation.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 06:37:34 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ye", "Chuyang", ""], ["Prince", "Jerry L.", ""]]}, {"id": "1705.06871", "submitter": "Shirui Li", "authors": "You Hao, Shirui Li, Hanlin Mo, and Hua Li", "title": "Affine-Gradient Based Local Binary Pattern Descriptor for Texture\n  Classiffication", "comments": "11 pages,4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Affine-Gradient based Local Binary Pattern (AGLBP)\ndescriptor for texture classification. It is very hard to describe complicated\ntexture using single type information, such as Local Binary Pattern (LBP),\nwhich just utilizes the sign information of the difference between the pixel\nand its local neighbors. Our descriptor has three characteristics: 1) In order\nto make full use of the information contained in the texture, the\nAffine-Gradient, which is different from Euclidean-Gradient and invariant to\naffine transformation is incorporated into AGLBP. 2) An improved method is\nproposed for rotation invariance, which depends on the reference direction\ncalculating respect to local neighbors. 3) Feature selection method,\nconsidering both the statistical frequency and the intraclass variance of the\ntraining dataset, is also applied to reduce the dimensionality of descriptors.\nExperiments on three standard texture datasets, Outex12, Outex10 and KTH-TIPS2,\nare conducted to evaluate the performance of AGLBP. The results show that our\nproposed descriptor gets better performance comparing to some state-of-the-art\nrotation texture descriptors in texture classification.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 06:41:31 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Hao", "You", ""], ["Li", "Shirui", ""], ["Mo", "Hanlin", ""], ["Li", "Hua", ""]]}, {"id": "1705.06900", "submitter": "Dmytro Derkach", "authors": "Dmytro Derkach and Federico M. Sukno", "title": "Local Shape Spectrum Analysis for 3D Facial Expression Recognition", "comments": "12th IEEE International Conference on Face and Gesture Recognition,\n  Washington, DC, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of facial expression recognition using 3D data.\nBuilding from one of the most successful frameworks for facial analysis using\nexclusively 3D geometry, we extend the analysis from a curve-based\nrepresentation into a spectral representation, which allows a complete\ndescription of the underlying surface that can be further tuned to the desired\nlevel of detail. Spectral representations are based on the decomposition of the\ngeometry in its spatial frequency components, much like a Fourier transform,\nwhich are related to intrinsic characteristics of the surface. In this work, we\npropose the use of Graph Laplacian Features (GLF), which results from the\nprojection of local surface patches into a common basis obtained from the Graph\nLaplacian eigenspace. We test the proposed approach in the BU-3DFE database in\nterms of expressions and Action Units recognition. Our results confirm that the\nproposed GLF produces consistently higher recognition rates than the\ncurves-based approach, thanks to a more complete description of the surface,\nwhile requiring a lower computational complexity. We also show that the GLF\noutperform the most popular alternative approach for spectral representation,\nShape- DNA, which is based on the Laplace Beltrami Operator and cannot provide\na stable basis that guarantee that the extracted signatures for the different\npatches are directly comparable.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:24:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Derkach", "Dmytro", ""], ["Sukno", "Federico M.", ""]]}, {"id": "1705.06920", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad, Asad Khan, Adil Mehmood Khan, and Rasheed Hussain", "title": "Segmented and Non-Segmented Stacked Denoising Autoencoder for\n  Hyperspectral Band Reduction", "comments": "10 pages, 14 figures", "journal-ref": "Optik-2019", "doi": "10.1016/j.ijleo.2018.10.142", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image analysis often requires selecting the most informative\nbands instead of processing the whole data without losing the key information.\nExisting band reduction (BR) methods have the capability to reveal the\nnonlinear properties exhibited in the data but at the expense of loosing its\noriginal representation. To cope with the said issue, an unsupervised\nnon-linear segmented and non-segmented stacked denoising autoencoder (UDAE)\nbased BR method is proposed. Our aim is to find an optimal mapping and\nconstruct a lower-dimensional space that has a similar structure to the\noriginal data with least reconstruction error. The proposed method first\nconfronts the original hyperspectral data into smaller regions in a spatial\ndomain and then each region is processed by UDAE individually. This results in\nreduced complexity and improved efficiency of BR for both semi-supervised and\nunsupervised tasks, i.e. classification and clustering. Our experiments on\npublicly available hyperspectral datasets with various types of classifiers\ndemonstrate the effectiveness of UDAE method which equates favorably with other\nstate-of-the-art dimensionality reduction and BR methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:33:21 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 16:43:15 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 14:19:04 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 13:29:49 GMT"}, {"version": "v5", "created": "Sun, 22 Apr 2018 11:08:12 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ahmad", "Muhammad", ""], ["Khan", "Asad", ""], ["Khan", "Adil Mehmood", ""], ["Hussain", "Rasheed", ""]]}, {"id": "1705.06950", "submitter": "Joao Carreira", "authors": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier,\n  Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul\n  Natsev, Mustafa Suleyman and Andrew Zisserman", "title": "The Kinetics Human Action Video Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the DeepMind Kinetics human action video dataset. The dataset\ncontains 400 human action classes, with at least 400 video clips for each\naction. Each clip lasts around 10s and is taken from a different YouTube video.\nThe actions are human focussed and cover a broad range of classes including\nhuman-object interactions such as playing instruments, as well as human-human\ninteractions such as shaking hands. We describe the statistics of the dataset,\nhow it was collected, and give some baseline performance figures for neural\nnetwork architectures trained and tested for human action classification on\nthis dataset. We also carry out a preliminary analysis of whether imbalance in\nthe dataset leads to bias in the classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 12:07:01 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kay", "Will", ""], ["Carreira", "Joao", ""], ["Simonyan", "Karen", ""], ["Zhang", "Brian", ""], ["Hillier", "Chloe", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Viola", "Fabio", ""], ["Green", "Tim", ""], ["Back", "Trevor", ""], ["Natsev", "Paul", ""], ["Suleyman", "Mustafa", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.07015", "submitter": "Jen-Wei Kuo", "authors": "Jen-wei Kuo, Jonathan Mamou, Yao Wang, Emi Saegusa-Beecroft, Junji\n  Machi, and Ernest J. Feleppa", "title": "Segmentation of 3D High-frequency Ultrasound Images of Human Lymph Nodes\n  Using Graph Cut with Energy Functional Adapted to Local Intensity\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies by our group have shown that three-dimensional\nhigh-frequency quantitative ultrasound methods have the potential to\ndifferentiate metastatic lymph nodes from cancer-free lymph nodes dissected\nfrom human cancer patients. To successfully perform these methods inside the\nlymph node parenchyma, an automatic segmentation method is highly desired to\nexclude the surrounding thin layer of fat from quantitative ultrasound\nprocessing and accurately correct for ultrasound attenuation. In high-frequency\nultrasound images of lymph nodes, the intensity distribution of lymph node\nparenchyma and fat varies spatially because of acoustic attenuation and\nfocusing effects. Thus, the intensity contrast between two object regions\n(e.g., lymph node parenchyma and fat) is also spatially varying. In our\nprevious work, nested graph cut demonstrated its ability to simultaneously\nsegment lymph node parenchyma, fat, and the outer phosphate-buffered saline\nbath even when some boundaries are lost because of acoustic attenuation and\nfocusing effects. This paper describes a novel approach called graph cut with\nlocally adaptive energy to further deal with spatially varying distributions of\nlymph node parenchyma and fat caused by inhomogeneous acoustic attenuation. The\nproposed method achieved Dice similarity coefficients of 0.937+-0.035 when\ncompared to expert manual segmentation on a representative dataset consisting\nof 115 three-dimensional lymph node images obtained from colorectal cancer\npatients.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:25:20 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kuo", "Jen-wei", ""], ["Mamou", "Jonathan", ""], ["Wang", "Yao", ""], ["Saegusa-Beecroft", "Emi", ""], ["Machi", "Junji", ""], ["Feleppa", "Ernest J.", ""]]}, {"id": "1705.07049", "submitter": "Hung Le", "authors": "Hung Le and Ali Borji", "title": "What are the Receptive, Effective Receptive, and Projective Fields of\n  Neurons in Convolutional Neural Networks?", "comments": "Fix typo mistake on equations (2) and (3). The \"summation\" should be\n  the \"product\" instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explain in detail how receptive fields, effective receptive\nfields, and projective fields of neurons in different layers, convolution or\npooling, of a Convolutional Neural Network (CNN) are calculated. While our\nfocus here is on CNNs, the same operations, but in the reverse order, can be\nused to calculate these quantities for deconvolutional neural networks. These\nare important concepts, not only for better understanding and analyzing\nconvolutional and deconvolutional networks, but also for optimizing their\nperformance in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:25:03 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 13:18:48 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Le", "Hung", ""], ["Borji", "Ali", ""]]}, {"id": "1705.07062", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa, St\\'ephanie Bricq, Alain Lalande", "title": "MRI-PET Registration with Automated Algorithm in Pre-clinical Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET)\nautomatic 3-D registration is implemented and validated for small animal image\nvolumes so that the high-resolution anatomical MRI information can be fused\nwith the low spatial resolution of functional PET information for the\nlocalization of lesion that is currently in high demand in the study of tumor\nof cancer (oncology) and its corresponding preparation of pharmaceutical drugs.\nThough many registration algorithms are developed and applied on human brain\nvolumes, these methods may not be as efficient on small animal datasets due to\nlack of intensity information and often the high anisotropy in voxel\ndimensions. Therefore, a fully automatic registration algorithm which can\nregister not only assumably rigid small animal volumes such as brain but also\ndeformable organs such as kidney, cardiac and chest is developed using a\ncombination of global affine and local B-spline transformation models in which\nmutual information is used as a similarity criterion. The global affine\nregistration uses a multi-resolution pyramid on image volumes of 3 levels\nwhereas in local B-spline registration, a multi-resolution scheme is applied on\nthe B-spline grid of 2 levels on the finest resolution of the image volumes in\nwhich only the transform itself is affected rather than the image volumes.\nSince mutual information lacks sufficient spatial information, PCA is used to\ninject it by estimating initial translation and rotation parameters. It is\ncomputationally efficient since it is implemented using C++ and ITK library,\nand is qualitatively and quantitatively shown that this PCA-initialized global\nregistration followed by local registration is in close agreement with expert\nmanual registration and outperforms the one without PCA initialization tested\non small animal brain and kidney.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:46:30 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:42:39 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Baisa", "Nathanael L.", ""], ["Bricq", "St\u00e9phanie", ""], ["Lalande", "Alain", ""]]}, {"id": "1705.07077", "submitter": "Haifeng Li", "authors": "Haifeng Li, Jian Peng, Chao Tao, Jie Chen, Min Deng", "title": "What do We Learn by Semantic Scene Understanding for Remote Sensing\n  imagery in CNN framework?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural network (DCNN) achieved increasingly\nremarkable success and rapidly developed in the field of natural image\nrecognition. Compared with the natural image, the scale of remote sensing image\nis larger and the scene and the object it represents are more macroscopic. This\nstudy inquires whether remote sensing scene and natural scene recognitions\ndiffer and raises the following questions: What are the key factors in remote\nsensing scene recognition? Is the DCNN recognition mechanism centered on object\nrecognition still applicable to the scenarios of remote sensing scene\nunderstanding? We performed several experiments to explore the influence of the\nDCNN structure and the scale of remote sensing scene understanding from the\nperspective of scene complexity. Our experiment shows that understanding a\ncomplex scene depends on an in-depth network and multiple-scale perception.\nUsing a visualization method, we qualitatively and quantitatively analyze the\nrecognition mechanism in a complex remote sensing scene and demonstrate the\nimportance of multi-objective joint semantic support.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:23:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Li", "Haifeng", ""], ["Peng", "Jian", ""], ["Tao", "Chao", ""], ["Chen", "Jie", ""], ["Deng", "Min", ""]]}, {"id": "1705.07080", "submitter": "Karttikeya Mangalam", "authors": "Karttikeya Mangalam, K S Venkatesh", "title": "Bitwise Operations of Cellular Automaton on Gray-scale Images", "comments": "5 Pages. The code is available at :\n  https://github.com/karttikeya/Bitwise-CA-Opeartions/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Automata (CA) theory is a discrete model that represents the state\nof each of its cells from a finite set of possible values which evolve in time\naccording to a pre-defined set of transition rules. CA have been applied to a\nnumber of image processing tasks such as Convex Hull Detection, Image Denoising\netc. but mostly under the limitation of restricting the input to binary images.\nIn general, a gray-scale image may be converted to a number of different binary\nimages which are finally recombined after CA operations on each of them\nindividually. We have developed a multinomial regression based weighed\nsummation method to recombine binary images for better performance of CA based\nImage Processing algorithms. The recombination algorithm is tested for the\nspecific case of denoising Salt and Pepper Noise to test against standard\nbenchmark algorithms such as the Median Filter for various images and noise\nlevels. The results indicate several interesting invariances in the application\nof the CA, such as the particular noise realization and the choice of\nsub-sampling of pixels to determine recombination weights. Additionally, it\nappears that simpler algorithms for weight optimization which seek local minima\nwork as effectively as those that seek global minima such as Simulated\nAnnealing.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:34:29 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Mangalam", "Karttikeya", ""], ["Venkatesh", "K S", ""]]}, {"id": "1705.07091", "submitter": "Chia-Wen Lin", "authors": "Chih-Chung Hsu and Chia-Wen Lin", "title": "CNN-Based Joint Clustering and Representation Learning with Feature\n  Drift Compensation for Large-Scale Image Data", "comments": "9 pages to appear in IEEE Transactions on Multimedia (Special Issue\n  on Large-Scale Multimedia Data Retrieval, Classification, and Understanding)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large unlabeled set of images, how to efficiently and effectively\ngroup them into clusters based on extracted visual representations remains a\nchallenging problem. To address this problem, we propose a convolutional neural\nnetwork (CNN) to jointly solve clustering and representation learning in an\niterative manner. In the proposed method, given an input image set, we first\nrandomly pick k samples and extract their features as initial cluster centroids\nusing the proposed CNN with an initial model pre-trained from the ImageNet\ndataset. Mini-batch k-means is then performed to assign cluster labels to\nindividual input samples for a mini-batch of images randomly sampled from the\ninput image set until all images are processed. Subsequently, the proposed CNN\nsimultaneously updates the parameters of the proposed CNN and the centroids of\nimage clusters iteratively based on stochastic gradient descent. We also\nproposed a feature drift compensation scheme to mitigate the drift error caused\nby feature mismatch in representation learning. Experimental results\ndemonstrate the proposed method outperforms start-of-the-art clustering schemes\nin terms of accuracy and storage complexity on large-scale image sets\ncontaining millions of images.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:01:38 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 07:31:48 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "1705.07108", "submitter": "Clara Callenberg", "authors": "Clara Callenberg, Felix Heide, Gordon Wetzstein, Matthias Hullin", "title": "Snapshot Difference Imaging using Time-of-Flight Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational photography encompasses a diversity of imaging techniques, but\none of the core operations performed by many of them is to compute image\ndifferences. An intuitive approach to computing such differences is to capture\nseveral images sequentially and then process them jointly. Usually, this\napproach leads to artifacts when recording dynamic scenes. In this paper, we\nintroduce a snapshot difference imaging approach that is directly implemented\nin the sensor hardware of emerging time-of-flight cameras. With a variety of\nexamples, we demonstrate that the proposed snapshot difference imaging\ntechnique is useful for direct-global illumination separation, for direct\nimaging of spatial and temporal image gradients, for direct depth edge imaging,\nand more.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:39:43 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Callenberg", "Clara", ""], ["Heide", "Felix", ""], ["Wetzstein", "Gordon", ""], ["Hullin", "Matthias", ""]]}, {"id": "1705.07115", "submitter": "Alex Kendall", "authors": "Alex Kendall and Yarin Gal and Roberto Cipolla", "title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry\n  and Semantics", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous deep learning applications benefit from multi-task learning with\nmultiple regression and classification objectives. In this paper we make the\nobservation that the performance of such systems is strongly dependent on the\nrelative weighting between each task's loss. Tuning these weights by hand is a\ndifficult and expensive process, making multi-task learning prohibitive in\npractice. We propose a principled approach to multi-task deep learning which\nweighs multiple loss functions by considering the homoscedastic uncertainty of\neach task. This allows us to simultaneously learn various quantities with\ndifferent units or scales in both classification and regression settings. We\ndemonstrate our model learning per-pixel depth regression, semantic and\ninstance segmentation from a monocular input image. Perhaps surprisingly, we\nshow our model can learn multi-task weightings and outperform separate models\ntrained individually on each task.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:56:57 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 09:02:06 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 06:42:35 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kendall", "Alex", ""], ["Gal", "Yarin", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1705.07118", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Dirk Fortmeier, Heinz Handels", "title": "Evaluation of Direct Haptic 4D Volume Rendering of Partially Segmented\n  Data for Liver Puncture Simulation", "comments": "15 pages, 16 figures, 1 tables, 11 equations, 39 references", "journal-ref": "Nature - Scientific Reports, Nature Publishing Group (NPG),\n  7(671), 2017", "doi": "10.1038/s41598-017-00746-z", "report-no": null, "categories": "cs.GR cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an evaluation study using a force feedback evaluation\nframework for a novel direct needle force volume rendering concept in the\ncontext of liver puncture simulation. PTC/PTCD puncture interventions targeting\nthe bile ducts have been selected to illustrate this concept. The haptic\nalgorithms of the simulator system are based on (1) partially segmented patient\nimage data and (2) a non-linear spring model effective at organ borders. The\nprimary aim is to quantitatively evaluate force errors caused by our patient\nmodeling approach, in comparison to haptic force output obtained from using\ngold-standard, completely manually-segmented data. The evaluation of the force\nalgorithms compared to a force output from fully manually segmented\ngold-standard patient models, yields a low mean of 0.12 N root mean squared\nforce error and up to 1.6 N for systematic maximum absolute errors. Force\nerrors were evaluated on 31,222 preplanned test paths from 10 patients. Only\ntwelve percent of the emitted forces along these paths were affected by errors.\nThis is the first study evaluating haptic algorithms with deformable virtual\npatients in silico. We prove haptic rendering plausibility on a very high\nnumber of test paths. Important errors are below just noticeable differences\nfor the hand-arm system.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:13:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Fortmeier", "Dirk", ""], ["Handels", "Heinz", ""]]}, {"id": "1705.07137", "submitter": "Guang Yang A", "authors": "Simiao Yu, Hao Dong, Guang Yang, Greg Slabaugh, Pier Luigi Dragotti,\n  Xujiong Ye, Fangde Liu, Simon Arridge, Jennifer Keegan, David Firmin, Yike\n  Guo", "title": "Deep De-Aliasing for Fast Compressive Sensing MRI", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Magnetic Resonance Imaging (MRI) is highly in demand for many clinical\napplications in order to reduce the scanning cost and improve the patient\nexperience. This can also potentially increase the image quality by reducing\nthe motion artefacts and contrast washout. However, once an image field of view\nand the desired resolution are chosen, the minimum scanning time is normally\ndetermined by the requirement of acquiring sufficient raw data to meet the\nNyquist-Shannon sampling criteria. Compressive Sensing (CS) theory has been\nperfectly matched to the MRI scanning sequence design with much less required\nraw data for the image reconstruction. Inspired by recent advances in deep\nlearning for solving various inverse problems, we propose a conditional\nGenerative Adversarial Networks-based deep learning framework for de-aliasing\nand reconstructing MRI images from highly undersampled data with great promise\nto accelerate the data acquisition process. By coupling an innovative content\nloss with the adversarial loss our de-aliasing results are more realistic.\nFurthermore, we propose a refinement learning procedure for training the\ngenerator network, which can stabilise the training with fast convergence and\nless parameter tuning. We demonstrate that the proposed framework outperforms\nstate-of-the-art CS-MRI methods, in terms of reconstruction error and\nperceptual image quality. In addition, our method can reconstruct each image in\n0.22ms--0.37ms, which is promising for real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:17:46 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yu", "Simiao", ""], ["Dong", "Hao", ""], ["Yang", "Guang", ""], ["Slabaugh", "Greg", ""], ["Dragotti", "Pier Luigi", ""], ["Ye", "Xujiong", ""], ["Liu", "Fangde", ""], ["Arridge", "Simon", ""], ["Keegan", "Jennifer", ""], ["Firmin", "David", ""], ["Guo", "Yike", ""]]}, {"id": "1705.07142", "submitter": "Abhay Shah", "authors": "Abhay Shah, Michael Abramoff and Xiaodong Wu", "title": "Simultaneous Multiple Surface Segmentation Using Deep Learning", "comments": "8 pages", "journal-ref": null, "doi": "10.1007/978-3-319-67558-9_1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of automatically segmenting 3-D surfaces representing boundaries of\nobjects is important for quantitative analysis of volumetric images, and plays\na vital role in biomedical image analysis. Recently, graph-based methods with a\nglobal optimization property have been developed and optimized for various\nmedical imaging applications. Despite their widespread use, these require human\nexperts to design transformations, image features, surface smoothness priors,\nand re-design for a different tissue, organ or imaging modality. Here, we\npropose a Deep Learning based approach for segmentation of the surfaces in\nvolumetric medical images, by learning the essential features and\ntransformations from training data, without any human expert intervention. We\nemploy a regional approach to learn the local surface profiles. The proposed\napproach was evaluated on simultaneous intraretinal layer segmentation of\noptical coherence tomography (OCT) images of normal retinas and retinas\naffected by age related macular degeneration (AMD). The proposed approach was\nvalidated on 40 retina OCT volumes including 20 normal and 20 AMD subjects. The\nexperiments showed statistically significant improvement in accuracy for our\napproach compared to state-of-the-art graph based optimal surface segmentation\nwith convex priors (G-OSC). A single Convolution Neural Network (CNN) was used\nto learn the surfaces for both normal and diseased images. The mean unsigned\nsurface positioning errors obtained by G-OSC method 2.31 voxels (95% CI\n2.02-2.60 voxels) was improved to $1.27$ voxels (95% CI 1.14-1.40 voxels) using\nour new approach. On average, our approach takes 94.34 s, requiring 95.35 MB\nmemory, which is much faster than the 2837.46 s and 6.87 GB memory required by\nthe G-OSC method on the same computer system.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:43:07 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Shah", "Abhay", ""], ["Abramoff", "Michael", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1705.07143", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Klaus Engelke, Willi Kalender", "title": "A New 3D Segmentation Methodology for Lumbar Vertebral Bodies for the\n  Measurement of BMD and Geometry", "comments": "4 pages,2 figures, MIUA05 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new technique is presented that extracts the geometry of\nlumbar vertebral bodies from spiral CT scans. Our new multi-step segmentation\napproach yields highly accurate and precise measurement of the bone mineral\ndensity (BMD) in different volumes of interest which are defined relative to a\nlocal anatomical coordinate systems. The approach also enables the analysis of\nthe geometry of the relevant vertebrae. Intra- and inter operator precision for\nsegmentation, BMD measurement and position of the coordinate system are below\n1.5% in patient data, accuracy errors are below 1.5% for BMD and below 4% for\nvolume in phantom data. The long-term goal of the approach is to improve\nfracture prediction in osteoporosis.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:49:18 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Engelke", "Klaus", ""], ["Kalender", "Willi", ""]]}, {"id": "1705.07144", "submitter": "Sheng Lundquist", "authors": "Sheng Y. Lundquist, Melanie Mitchell, Garrett T. Kenyon", "title": "Sparse Coding on Stereo Video for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNN) require millions of labeled\ntraining examples for image classification and object detection tasks, which\nrestrict these models to domains where such datasets are available. In this\npaper, we explore the use of unsupervised sparse coding applied to stereo-video\ndata to help alleviate the need for large amounts of labeled data. We show that\nreplacing a typical supervised convolutional layer with an unsupervised\nsparse-coding layer within a DCNN allows for better performance on a car\ndetection task when only a limited number of labeled training examples is\navailable. Furthermore, the network that incorporates sparse coding allows for\nmore consistent performance over varying initializations and ordering of\ntraining examples when compared to a fully supervised DCNN. Finally, we compare\nactivations between the unsupervised sparse-coding layer and the supervised\nconvolutional layer, and show that the sparse representation exhibits an\nencoding that is depth selective, whereas encodings from the convolutional\nlayer do not exhibit such selectivity. These result indicates promise for using\nunsupervised sparse-coding approaches in real-world computer vision tasks in\ndomains with limited labeled training data.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:52:55 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:41:55 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lundquist", "Sheng Y.", ""], ["Mitchell", "Melanie", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1705.07146", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Klaus Engelke, Sebastian Meller, Willi Kalender", "title": "A New 3D Method to Segment the Lumbar Vertebral Bodies and to Determine\n  Bone Mineral Density and Geometry", "comments": "8 pages, 6 figures, 1 table, MICCAI 2005. arXiv admin note: text\n  overlap with arXiv:1705.07143", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new 3D segmentation approach for the vertebrae of\nthe lower thoracic and the lumbar spine in spiral computed tomography datasets.\nWe implemented a multi-step procedure. Its main components are deformable\nmodels, volume growing, and morphological operations. The performance analysis\nthat included an evaluation of accuracy using the European Spine Phantom, and\nof intra-operator precision using clinical CT datasets from 10 patients\nhighlight the potential for clinical use. The intra-operator precision of the\nsegmentation procedure was better than 1% for Bone Mineral Density (BMD) and\nbetter than 1.8% for volume. The long-term goal of this work is to enable\nbetter fracture prediction and improved patient monitoring in the field of\nosteoporosis. A true 3D segmentation also enables an accurate measurement of\ngeometrical parameters that can augment the classical measurement of BMD.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:59:05 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 07:52:58 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Engelke", "Klaus", ""], ["Meller", "Sebastian", ""], ["Kalender", "Willi", ""]]}, {"id": "1705.07162", "submitter": "Kihwan Kim", "authors": "Kihwan Kim, Jinwei Gu, Stephen Tyree, Pavlo Molchanov, Matthias\n  Nie{\\ss}ner, Jan Kautz", "title": "A Lightweight Approach for On-the-Fly Reflectance Estimation", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating surface reflectance (BRDF) is one key component for complete 3D\nscene capture, with wide applications in virtual reality, augmented reality,\nand human computer interaction. Prior work is either limited to controlled\nenvironments (\\eg gonioreflectometers, light stages, or multi-camera domes), or\nrequires the joint optimization of shape, illumination, and reflectance, which\nis often computationally too expensive (\\eg hours of running time) for\nreal-time applications. Moreover, most prior work requires HDR images as input\nwhich further complicates the capture process. In this paper, we propose a\nlightweight approach for surface reflectance estimation directly from $8$-bit\nRGB images in real-time, which can be easily plugged into any 3D\nscanning-and-fusion system with a commodity RGBD sensor. Our method is\nlearning-based, with an inference time of less than 90ms per scene and a model\nsize of less than 340K bytes. We propose two novel network architectures,\nHemiCNN and Grouplet, to deal with the unstructured input data from multiple\nviewpoints under unknown illumination. We further design a loss function to\nresolve the color-constancy and scale ambiguity. In addition, we have created a\nlarge synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images\nrendered with a physically-based ray tracer under a variety of natural\nillumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first\nlarge-scale benchmark dataset for reflectance estimation. Experiments on both\nsynthetic data and real data show that the proposed method effectively recovers\nsurface reflectance, and outperforms prior work for reflectance estimation in\nuncontrolled environments.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:45:57 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 05:21:39 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kim", "Kihwan", ""], ["Gu", "Jinwei", ""], ["Tyree", "Stephen", ""], ["Molchanov", "Pavlo", ""], ["Nie\u00dfner", "Matthias", ""], ["Kautz", "Jan", ""]]}, {"id": "1705.07175", "submitter": "Fabrizio Pedersoli", "authors": "Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi", "title": "Espresso: Efficient Forward Propagation for BCNNs", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications scenarios for which the computational performance\nand memory footprint of the prediction phase of Deep Neural Networks (DNNs)\nneeds to be optimized. Binary Neural Networks (BDNNs) have been shown to be an\neffective way of achieving this objective. In this paper, we show how\nConvolutional Neural Networks (CNNs) can be implemented using binary\nrepresentations. Espresso is a compact, yet powerful library written in C/CUDA\nthat features all the functionalities required for the forward propagation of\nCNNs, in a binary file less than 400KB, without any external dependencies.\nAlthough it is mainly designed to take advantage of massive GPU parallelism,\nEspresso also provides an equivalent CPU implementation for CNNs. Espresso\nprovides special convolutional and dense layers for BCNNs, leveraging\nbit-packing and bit-wise computations for efficient execution. These techniques\nprovide a speed-up of matrix-multiplication routines, and at the same time,\nreduce memory usage when storing parameters and activations. We experimentally\nshow that Espresso is significantly faster than existing implementations of\noptimized binary neural networks ($\\approx$ 2 orders of magnitude). Espresso is\nreleased under the Apache 2.0 license and is available at\nhttp://github.com/fpeder/espresso.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:29:42 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:59:16 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Pedersoli", "Fabrizio", ""], ["Tzanetakis", "George", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1705.07202", "submitter": "Lei Cai", "authors": "Lei Cai and Hongyang Gao and Shuiwang Ji", "title": "Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoder (VAE) is a powerful unsupervised learning framework\nfor image generation. One drawback of VAE is that it generates blurry images\ndue to its Gaussianity assumption and thus L2 loss. To allow the generation of\nhigh quality images by VAE, we increase the capacity of decoder network by\nemploying residual blocks and skip connections, which also enable efficient\noptimization. To overcome the limitation of L2 loss, we propose to generate\nimages in a multi-stage manner from coarse to fine. In the simplest case, the\nproposed multi-stage VAE divides the decoder into two components in which the\nsecond component generates refined images based on the course images generated\nby the first component. Since the second component is independent of the VAE\nmodel, it can employ other loss functions beyond the L2 loss and different\nmodel architectures. The proposed framework can be easily generalized to\ncontain more than two components. Experiment results on the MNIST and CelebA\ndatasets demonstrate that the proposed multi-stage VAE can generate sharper\nimages as compared to those from the original VAE.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:51:30 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Cai", "Lei", ""], ["Gao", "Hongyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.07206", "submitter": "Jian Zhao", "authors": "Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong Li, Terence\n  Sim, Shuicheng Yan, Jiashi Feng", "title": "Multiple-Human Parsing in the Wild", "comments": "The first two authors are with equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing is attracting increasing research attention. In this work, we\naim to push the frontier of human parsing by introducing the problem of\nmulti-human parsing in the wild. Existing works on human parsing mainly tackle\nsingle-person scenarios, which deviates from real-world applications where\nmultiple persons are present simultaneously with interaction and occlusion. To\naddress the multi-human parsing problem, we introduce a new multi-human parsing\n(MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP\ndataset contains multiple persons captured in real-world scenes with\npixel-level fine-grained semantic annotations in an instance-aware setting. The\nMH-Parser generates global parsing maps and person instance masks\nsimultaneously in a bottom-up fashion with the help of a new Graph-GAN model.\nWe envision that the MHP dataset will serve as a valuable data resource to\ndevelop new multi-human parsing models, and the MH-Parser offers a strong\nbaseline to drive future research for multi-human parsing in the wild.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:59:09 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 03:09:48 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Li", "Jianshu", ""], ["Zhao", "Jian", ""], ["Wei", "Yunchao", ""], ["Lang", "Congyan", ""], ["Li", "Yidong", ""], ["Sim", "Terence", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.07208", "submitter": "Ryan Dahl", "authors": "Sergio Guadarrama, Ryan Dahl, David Bieber, Mohammad Norouzi, Jonathon\n  Shlens, Kevin Murphy", "title": "PixColor: Pixel Recursive Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\".\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:10:51 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 18:38:01 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Guadarrama", "Sergio", ""], ["Dahl", "Ryan", ""], ["Bieber", "David", ""], ["Norouzi", "Mohammad", ""], ["Shlens", "Jonathon", ""], ["Murphy", "Kevin", ""]]}, {"id": "1705.07215", "submitter": "Naveen Kodali", "authors": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs", "comments": "Analysis of convergence and mode collapse by studying GAN training\n  process as regret minimization. Some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GT cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:41:56 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 15:13:01 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 00:51:40 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 21:47:51 GMT"}, {"version": "v5", "created": "Sun, 10 Dec 2017 15:24:13 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kodali", "Naveen", ""], ["Abernethy", "Jacob", ""], ["Hays", "James", ""], ["Kira", "Zsolt", ""]]}, {"id": "1705.07222", "submitter": "Jianbing Shen", "authors": "Xingping Dong and Jianbing Shen and Dongming Wu and Kan Guo and\n  Xiaogang Jin and Fatih Porikli", "title": "Quadruplet Network with One-Shot Learning for Fast Visual Object\n  Tracking", "comments": null, "journal-ref": "IEEE Transactions on Image Processing ( Volume: 28 , Issue: 7 ,\n  July 2019 )", "doi": "10.1109/TIP.2019.2898567", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the same vein of discriminative one-shot learning, Siamese networks allow\nrecognizing an object from a single exemplar with the same class label.\nHowever, they do not take advantage of the underlying structure of the data and\nthe relationship among the multitude of samples as they only rely on pairs of\ninstances for training. In this paper, we propose a new quadruplet deep network\nto examine the potential connections among the training instances, aiming to\nachieve a more powerful representation. We design four shared networks that\nreceive multi-tuple of instances as inputs and are connected by a novel loss\nfunction consisting of pair-loss and triplet-loss. According to the similarity\nmetric, we select the most similar and the most dissimilar instances as the\npositive and negative inputs of triplet loss from each multi-tuple. We show\nthat this scheme improves the training performance. Furthermore, we introduce a\nnew weight layer to automatically select suitable combination weights, which\nwill avoid the conflict between triplet and pair loss leading to worse\nperformance. We evaluate our quadruplet framework by model-free\ntracking-by-detection of objects from a single initial exemplar in several\nVisual Object Tracking benchmarks. Our extensive experimental analysis\ndemonstrates that our tracker achieves superior performance with a real-time\nprocessing speed of 78 frames-per-second (fps).\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:37:54 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 01:44:36 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 11:26:12 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Dong", "Xingping", ""], ["Shen", "Jianbing", ""], ["Wu", "Dongming", ""], ["Guo", "Kan", ""], ["Jin", "Xiaogang", ""], ["Porikli", "Fatih", ""]]}, {"id": "1705.07238", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Recurrent Scene Parsing with Perspective Understanding in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects may appear at arbitrary scales in perspective images of a scene,\nposing a challenge for recognition systems that process images at a fixed\nresolution. We propose a depth-aware gating module that adaptively selects the\npooling field size in a convolutional network architecture according to the\nobject scale (inversely proportional to the depth) so that small details are\npreserved for distant objects while larger receptive fields are used for those\nnearby. The depth gating signal is provided by stereo disparity or estimated\ndirectly from monocular input. We integrate this depth-aware gating into a\nrecurrent convolutional neural network to perform semantic segmentation. Our\nrecurrent module iteratively refines the segmentation results, leveraging the\ndepth and semantic predictions from the previous iterations.\n  Through extensive experiments on four popular large-scale RGB-D datasets, we\ndemonstrate this approach achieves competitive semantic segmentation\nperformance with a model which is substantially more compact. We carry out\nextensive analysis of this architecture including variants that operate on\nmonocular RGB but use depth as side-information during training, unsupervised\ngating as a generic attentional mechanism, and multi-resolution gating. We find\nthat gated pooling for joint semantic segmentation and depth yields\nstate-of-the-art results for quantitative monocular depth estimation.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 00:49:01 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 00:05:17 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1705.07249", "submitter": "Nicolas Stier-Moses", "authors": "Tim Danford, Onur Filiz, Jing Huang, Brian Karrer, Manohar Paluri,\n  Guan Pang, Vish Ponnampalam, Nicolas Stier-Moses, Birce Tezel", "title": "End-to-end Planning of Fixed Millimeter-Wave Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses a framework to support the design and end-to-end\nplanning of fixed millimeter-wave networks. Compared to traditional techniques,\nthe framework allows an organization to quickly plan a deployment in a\ncost-effective way. We start by using LiDAR data---basically, a 3D point cloud\ncaptured from a city---to estimate potential sites to deploy antennas and\nwhether there is line-of-sight between them. With that data on hand, we use\ncombinatorial optimization techniques to determine the optimal set of locations\nand how they should communicate with each other, to satisfy engineering (e.g.,\nlatency, polarity), design (e.g., reliability) and financial (e.g., total cost\nof operation) constraints. The primary goal is to connect as many people as\npossible to the network. Our methodology can be used for strategic planning\nwhen an organization is in the process of deciding whether to adopt a\nmillimeter-wave technology or choosing between locations, or for operational\nplanning when conducting a detailed design of the actual network to be deployed\nin a selected location.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 02:53:45 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Danford", "Tim", ""], ["Filiz", "Onur", ""], ["Huang", "Jing", ""], ["Karrer", "Brian", ""], ["Paluri", "Manohar", ""], ["Pang", "Guan", ""], ["Ponnampalam", "Vish", ""], ["Stier-Moses", "Nicolas", ""], ["Tezel", "Birce", ""]]}, {"id": "1705.07263", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, David Wagner", "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:59:23 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 04:07:05 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1705.07272", "submitter": "Hassan Foroosh", "authors": "Mais Alnasser and Hassan Foroosh", "title": "Non-Linear Phase-Shifting of Haar Wavelets for Run-Time All-Frequency\n  Lighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on real-time all-frequency image-based rendering using an\ninnovative solution for run-time computation of light transport. The approach\nis based on new results derived for non-linear phase shifting in the Haar\nwavelet domain. Although image-based methods for real-time rendering of dynamic\nglossy objects have been proposed, they do not truly scale to all possible\nfrequencies and high sampling rates without trading storage, glossiness, or\ncomputational time, while varying both lighting and viewpoint. This is due to\nthe fact that current approaches are limited to precomputed radiance transfer\n(PRT), which is prohibitively expensive in terms of memory requirements and\nreal-time rendering when both varying light and viewpoint changes are required\ntogether with high sampling rates for high frequency lighting of glossy\nmaterial. On the other hand, current methods cannot handle object rotation,\nwhich is one of the paramount issues for all PRT methods using wavelets. This\nlatter problem arises because the precomputed data are defined in a global\ncoordinate system and encoded in the wavelet domain, while the object is\nrotated in a local coordinate system. At the root of all the above problems is\nthe lack of efficient run-time solution to the nontrivial problem of rotating\nwavelets (a non-linear phase-shift), which we solve in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 07:42:39 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Alnasser", "Mais", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.07284", "submitter": "Onkar Krishna", "authors": "Onkar Krishna, Kiyoharu Aizawa, Andrea Helo, Rama Pia", "title": "Gaze Distribution Analysis and Saliency Prediction Across Age Groups", "comments": null, "journal-ref": "PLOS ONE, 2018", "doi": "10.1371/journal.pone.0193149", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the human visual system helps to develop better computational\nmodels of visual attention. State-of-the-art models have been developed to\nmimic the visual attention system of young adults that, however, largely ignore\nthe variations that occur with age. In this paper, we investigated how visual\nscene processing changes with age and we propose an age-adapted framework that\nhelps to develop a computational model that can predict saliency across\ndifferent age groups. Our analysis uncovers how the explorativeness of an\nobserver varies with age, how well saliency maps of an age group agree with\nfixation points of observers from the same or different age groups, and how age\ninfluences the center bias. We analyzed the eye movement behavior of 82\nobservers belonging to four age groups while they explored visual scenes.\nExplorativeness was quantified in terms of the entropy of a saliency map, and\narea under the curve (AUC) metrics was used to quantify the agreement analysis\nand the center bias. These results were used to develop age adapted saliency\nmodels. Our results suggest that the proposed age-adapted saliency model\noutperforms existing saliency models in predicting the regions of interest\nacross age groups.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 09:46:49 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 06:33:25 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Krishna", "Onkar", ""], ["Aizawa", "Kiyoharu", ""], ["Helo", "Andrea", ""], ["Pia", "Rama", ""]]}, {"id": "1705.07328", "submitter": "Chenyou Fan", "authors": "Chenyou Fan, Jangwon Lee, Michael S. Ryoo", "title": "Forecasting Hands and Objects in Future Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to forecast future presence and location of\nhuman hands and objects. Given an image frame, the goal is to predict what\nobjects will appear in the future frame (e.g., 5 seconds later) and where they\nwill be located at, even when they are not visible in the current frame. The\nkey idea is that (1) an intermediate representation of a convolutional object\nrecognition model abstracts scene information in its frame and that (2) we can\npredict (i.e., regress) such representations corresponding to the future frames\nbased on that of the current frame. We design a new two-stream convolutional\nneural network (CNN) architecture for videos by extending the state-of-the-art\nconvolutional object detection network, and present a new fully convolutional\nregression network for predicting future scene representations. Our experiments\nconfirm that combining the regressed future representation with our detection\nnetwork allows reliable estimation of future hands and objects in videos. We\nobtain much higher accuracy compared to the state-of-the-art future object\npresence forecast method on a public dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 16:34:55 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 23:37:51 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 04:17:07 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Fan", "Chenyou", ""], ["Lee", "Jangwon", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1705.07329", "submitter": "Benjamin Kunsberg", "authors": "Benjamin S. Kunsberg and Steven W. Zucker", "title": "Critical Contours: An Invariant Linking Image Flow with Salient Surface\n  Organization", "comments": "Version 2: A reorganized version with the same results. Technical\n  details are placed in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a key result from visual psychophysics---that individuals perceive\nshape qualitatively---to develop the use of a geometrical/topological\n\"invariant'' (the Morse--Smale complex) relating image structure with surface\nstructure. Differences across individuals are minimal near certain\nconfigurations such as ridges and boundaries, and it is these configurations\nthat are often represented in line drawings. In particular, we introduce a\nmethod for inferring a qualitative three-dimensional shape from shading\npatterns that link the shape-from-shading inference with shape-from-contour\ninference. For a given shape, certain shading patches approach \"line drawings''\nin a well-defined limit. Under this limit, and invariably with respect to\nrendering choices, these shading patterns provide a qualitative description of\nthe surface. We further show that, under this model, the contours partition the\nsurface into meaningful parts using the Morse--Smale complex. These critical\ncontours are the (perceptually) stable parts of this complex and are invariant\nover a wide class of rendering models. Intuitively, our main result shows that\ncritical contours partition smooth surfaces into bumps and valleys, in effect\nproviding a scaffold on the image from which a full surface can be\ninterpolated.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 16:54:41 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 21:17:47 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kunsberg", "Benjamin S.", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1705.07340", "submitter": "Hassan Foroosh", "authors": "Mais Alnasser and Hassan Foroosh", "title": "Phase-Shifting Separable Haar Wavelets and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for tackling the shift-invariance problem\nin the discrete Haar domain, without trading off any of its desirable\nproperties, such as compression, separability, orthogonality, and symmetry. The\npaper presents several key theoretical contributions. First, we derive closed\nform expressions for phase shifting in the Haar domain both in partially\ndecimated and fully decimated transforms. Second, it is shown that the wavelet\ncoefficients of the shifted signal can be computed solely by using the\ncoefficients of the original transformed signal. Third, we derive closed-form\nexpressions for non-integer shifts, which have not been previously reported in\nthe literature. Fourth, we establish the complexity of the proposed phase\nshifting approach using the derived analytic expressions. As an application\nexample of these results, we apply the new formulae to image rotation and\ninterpolation, and evaluate its performance against standard methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 17:53:25 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Alnasser", "Mais", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.07356", "submitter": "Reza Abbasi-Asl", "authors": "Reza Abbasi-Asl and Bin Yu", "title": "Structural Compression of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been successful in many tasks\nin machine vision, however, millions of weights in the form of thousands of\nconvolutional filters in CNNs makes them difficult for human intepretation or\nunderstanding in science. In this article, we introduce CAR, a greedy\nstructural compression scheme to obtain smaller and more interpretable CNNs,\nwhile achieving close to original accuracy. The compression is based on pruning\nfilters with the least contribution to the classification accuracy. We\ndemonstrate the interpretability of CAR-compressed CNNs by showing that our\nalgorithm prunes filters with visually redundant functionalities such as color\nfilters. These compressed networks are easier to interpret because they retain\nthe filter diversity of uncompressed networks with order of magnitude less\nfilters. Finally, a variant of CAR is introduced to quantify the importance of\neach image category to each CNN filter. Specifically, the most and the least\nimportant class labels are shown to be meaningful interpretations of each\nfilter.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 20:12:07 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 22:08:51 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 22:32:20 GMT"}, {"version": "v4", "created": "Wed, 25 Mar 2020 10:49:13 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Abbasi-Asl", "Reza", ""], ["Yu", "Bin", ""]]}, {"id": "1705.07364", "submitter": "Sohil Shah", "authors": "Abhay Yadav, Sohil Shah, Zheng Xu, David Jacobs and Tom Goldstein", "title": "Stabilizing Adversarial Nets With Prediction Methods", "comments": "Accepted at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial neural networks solve many important problems in data science,\nbut are notoriously difficult to train. These difficulties come from the fact\nthat optimal weights for adversarial nets correspond to saddle points, and not\nminimizers, of the loss function. The alternating stochastic gradient methods\ntypically used for such problems do not reliably converge to saddle points, and\nwhen convergence does happen it is often highly sensitive to learning rates. We\npropose a simple modification of stochastic gradient descent that stabilizes\nadversarial networks. We show, both in theory and practice, that the proposed\nmethod reliably converges to saddle points, and is stable with a wider range of\ntraining parameters than a non-prediction method. This makes adversarial\nnetworks less likely to \"collapse,\" and enables faster training with larger\nlearning rates.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 22:27:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 04:22:35 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 21:57:54 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yadav", "Abhay", ""], ["Shah", "Sohil", ""], ["Xu", "Zheng", ""], ["Jacobs", "David", ""], ["Goldstein", "Tom", ""]]}, {"id": "1705.07383", "submitter": "Jindong Jiang", "authors": "Jindong Jiang, Zhijun Zhang, Yongqian Huang, Lunan Zheng", "title": "Incorporating Depth into both CNN and CRF for Indoor Semantic\n  Segmentation", "comments": "Accepted by IEEE ICSESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve segmentation performance, a novel neural network architecture\n(termed DFCN-DCRF) is proposed, which combines an RGB-D fully convolutional\nneural network (DFCN) with a depth-sensitive fully-connected conditional random\nfield (DCRF). First, a DFCN architecture which fuses depth information into the\nearly layers and applies dilated convolution for later contextual reasoning is\ndesigned. Then, a depth-sensitive fully-connected conditional random field\n(DCRF) is proposed and combined with the previous DFCN to refine the\npreliminary result. Comparative experiments show that the proposed DFCN-DCRF\nhas the best performance compared with most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 03:02:01 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 04:20:28 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 03:12:52 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 11:10:02 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Jiang", "Jindong", ""], ["Zhang", "Zhijun", ""], ["Huang", "Yongqian", ""], ["Zheng", "Lunan", ""]]}, {"id": "1705.07386", "submitter": "Philip Bontrager", "authors": "Philip Bontrager, Aditi Roy, Julian Togelius, Nasir Memon, Arun Ross", "title": "DeepMasterPrints: Generating MasterPrints for Dictionary Attacks via\n  Latent Variable Evolution", "comments": "8 pages; added new verification systems and diagrams. Accepted to\n  conference Biometrics: Theory, Applications, and Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the vulnerability of fingerprint recognition\nsystems to dictionary attacks based on MasterPrints. MasterPrints are real or\nsynthetic fingerprints that can fortuitously match with a large number of\nfingerprints thereby undermining the security afforded by fingerprint systems.\nPrevious work by Roy et al. generated synthetic MasterPrints at the\nfeature-level. In this work we generate complete image-level MasterPrints known\nas DeepMasterPrints, whose attack accuracy is found to be much superior than\nthat of previous methods. The proposed method, referred to as Latent Variable\nEvolution, is based on training a Generative Adversarial Network on a set of\nreal fingerprint images. Stochastic search in the form of the Covariance Matrix\nAdaptation Evolution Strategy is then used to search for latent input variables\nto the generator network that can maximize the number of impostor matches as\nassessed by a fingerprint recognizer. Experiments convey the efficacy of the\nproposed method in generating DeepMasterPrints. The underlying method is likely\nto have broad applications in fingerprint security as well as fingerprint\nsynthesis.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 03:43:46 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 21:51:38 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 22:42:13 GMT"}, {"version": "v4", "created": "Thu, 18 Oct 2018 21:29:47 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Bontrager", "Philip", ""], ["Roy", "Aditi", ""], ["Togelius", "Julian", ""], ["Memon", "Nasir", ""], ["Ross", "Arun", ""]]}, {"id": "1705.07404", "submitter": "Joe Klobusicky", "authors": "Chirag Agarwal, Joe Klobusicky, and Dan Schonfeld", "title": "Convergence of backpropagation with momentum for network architectures\n  with skip connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of deep neural networks with networks that form a directed\nacyclic graph (DAG). For backpropagation defined by gradient descent with\nadaptive momentum, we show weights converge for a large class of nonlinear\nactivation functions. The proof generalizes the results of Wu et al. (2008) who\nshowed convergence for a feed forward network with one hidden layer. For an\nexample of the effectiveness of DAG architectures, we describe an example of\ncompression through an autoencoder, and compare against sequential feed forward\nnetworks under several metrics.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 06:50:49 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 23:57:53 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 01:42:03 GMT"}, {"version": "v4", "created": "Sun, 19 Jan 2020 04:59:25 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Agarwal", "Chirag", ""], ["Klobusicky", "Joe", ""], ["Schonfeld", "Dan", ""]]}, {"id": "1705.07420", "submitter": "Eran Goldman", "authors": "Eran Goldman and Jacob Goldberger", "title": "Large-Scale Classification of Structured Objects using a CRF with Deep\n  Class Embedding", "comments": null, "journal-ref": "Computer Vision and Image Understanding (2019) 102865", "doi": "10.1016/j.cviu.2019.102865", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning architecture to classify structured\nobjects in datasets with a large number of visually similar categories. We\nmodel sequences of images as linear-chain CRFs, and jointly learn the\nparameters from both local-visual features and neighboring classes. The visual\nfeatures are computed by convolutional layers, and the class embeddings are\nlearned by factorizing the CRF pairwise potential matrix. This forms a highly\nnonlinear objective function which is trained by optimizing a local likelihood\napproximation with batch-normalization. This model overcomes the difficulties\nof existing CRF methods to learn the contextual relationships thoroughly when\nthere is a large number of classes and the data is sparse. The performance of\nthe proposed method is illustrated on a huge dataset that contains images of\nretail-store product displays, taken in varying settings and viewpoints, and\nshows significantly improved results compared to linear CRF modeling and\nunnormalized likelihood optimization.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 09:44:17 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 13:06:25 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 18:10:15 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Goldman", "Eran", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1705.07422", "submitter": "Xuecheng Nie", "authors": "Xuecheng Nie, Jiashi Feng, Junliang Xing, Shuicheng Yan", "title": "Generative Partition Networks for Multi-Person Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Generative Partition Network (GPN) to address the\nchallenging multi-person pose estimation problem. Different from existing\nmodels that are either completely top-down or bottom-up, the proposed GPN\nintroduces a novel strategy--it generates partitions for multiple persons from\ntheir global joint candidates and infers instance-specific joint configurations\nsimultaneously. The GPN is favorably featured by low complexity and high\naccuracy of joint detection and re-organization. In particular, GPN designs a\ngenerative model that performs one feed-forward pass to efficiently generate\nrobust person detections with joint partitions, relying on dense regressions\nfrom global joint candidates in an embedding space parameterized by centroids\nof persons. In addition, GPN formulates the inference procedure for joint\nconfigurations of human poses as a graph partition problem, and conducts local\noptimization for each person detection with reliable global affinity cues,\nleading to complexity reduction and performance improvement. GPN is implemented\nwith the Hourglass architecture as the backbone network to simultaneously learn\njoint detector and dense regressor. Extensive experiments on benchmarks MPII\nHuman Pose Multi-Person, extended PASCAL-Person-Part, and WAF, show the\nefficiency of GPN with new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 09:54:48 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 12:10:36 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Nie", "Xuecheng", ""], ["Feng", "Jiashi", ""], ["Xing", "Junliang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1705.07426", "submitter": "Ankan Bansal", "authors": "Ankan Bansal, Carlos Castillo, Rajeev Ranjan, Rama Chellappa", "title": "The Do's and Don'ts for CNN-based Face Verification", "comments": "10 pages including references, added more experiments on deeper vs\n  wider dataset (section 3.2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 10:26:45 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 19:05:22 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Bansal", "Ankan", ""], ["Castillo", "Carlos", ""], ["Ranjan", "Rajeev", ""], ["Chellappa", "Rama", ""]]}, {"id": "1705.07450", "submitter": "Adriana Romero", "authors": "Adriana Romero, Michal Drozdzal, Akram Erraqabi, Simon J\\'egou, Yoshua\n  Bengio", "title": "Image Segmentation by Iterative Inference from Conditional Score\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the combination of feedforward and iterative computations in the\nvirtual cortex, and taking advantage of the ability of denoising autoencoders\nto estimate the score of a joint distribution, we propose a novel approach to\niterative inference for capturing and exploiting the complex joint distribution\nof output variables conditioned on some input variables. This approach is\napplied to image pixel-wise segmentation, with the estimated conditional score\nused to perform gradient ascent towards a mode of the estimated conditional\ndistribution. This extends previous work on score estimation by denoising\nautoencoders to the case of a conditional distribution, with a novel use of a\ncorrupted feedforward predictor replacing Gaussian corruption. An advantage of\nthis approach over more classical ways to perform iterative inference for\nstructured outputs, like conditional random fields (CRFs), is that it is not\nany more necessary to define an explicit energy function linking the output\nvariables. To keep computations tractable, such energy function\nparametrizations are typically fairly constrained, involving only a few\nneighbors of each of the output variables in each clique. We experimentally\nfind that the proposed iterative inference from conditional score estimation by\nconditional denoising autoencoders performs better than comparable models based\non CRFs or those not using any explicit modeling of the conditional joint\ndistribution of outputs.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 13:56:03 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 22:22:28 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Romero", "Adriana", ""], ["Drozdzal", "Michal", ""], ["Erraqabi", "Akram", ""], ["J\u00e9gou", "Simon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1705.07485", "submitter": "Xavier Gastaldi", "authors": "Xavier Gastaldi", "title": "Shake-Shake regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 18:51:27 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:36:46 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gastaldi", "Xavier", ""]]}, {"id": "1705.07522", "submitter": "Hamid Tizhoosh", "authors": "Morteza Babaie, Shivam Kalra, Aditya Sriram, Christopher Mitcheltree,\n  Shujin Zhu, Amin Khatami, Shahryar Rahnamayan, H.R. Tizhoosh", "title": "Classification and Retrieval of Digital Pathology Scans: A New Dataset", "comments": "Accepted for presentation at Workshop for Computer Vision for\n  Microscopy Image Analysis (CVMI 2017) @ CVPR 2017, Honolulu, Hawaii", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new dataset, \\textbf{Kimia Path24}, for image\nclassification and retrieval in digital pathology. We use the whole scan images\nof 24 different tissue textures to generate 1,325 test patches of size\n1000$\\times$1000 (0.5mm$\\times$0.5mm). Training data can be generated according\nto preferences of algorithm designer and can range from approximately 27,000 to\nover 50,000 patches if the preset parameters are adopted. We propose a compound\npatch-and-scan accuracy measurement that makes achieving high accuracies quite\nchallenging. In addition, we set the benchmarking line by applying LBP,\ndictionary approach and convolutional neural nets (CNNs) and report their\nresults. The highest accuracy was 41.80\\% for CNN.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 00:00:18 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Babaie", "Morteza", ""], ["Kalra", "Shivam", ""], ["Sriram", "Aditya", ""], ["Mitcheltree", "Christopher", ""], ["Zhu", "Shujin", ""], ["Khatami", "Amin", ""], ["Rahnamayan", "Shahryar", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1705.07543", "submitter": "Hyerin Kim", "authors": "Hye-Rin Kim, Yeong-Seok Kim, Seon Joo Kim, In-Kwon Lee", "title": "Building Emotional Machines: Recognizing Image Emotions through Deep\n  Neural Networks", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image is a very effective tool for conveying emotions. Many researchers\nhave investigated in computing the image emotions by using various features\nextracted from images. In this paper, we focus on two high level features, the\nobject and the background, and assume that the semantic information of images\nis a good cue for predicting emotion. An object is one of the most important\nelements that define an image, and we find out through experiments that there\nis a high correlation between the object and the emotion in images. Even with\nthe same object, there may be slight difference in emotion due to different\nbackgrounds, and we use the semantic information of the background to improve\nthe prediction performance. By combining the different levels of features, we\nbuild an emotion based feed forward deep neural network which produces the\nemotion values of a given image. The output emotion values in our framework are\ncontinuous values in the 2-dimensional space (Valence and Arousal), which are\nmore effective than using a few number of emotion categories in describing\nemotions. Experiments confirm the effectiveness of our network in predicting\nthe emotion of images.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:56:23 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 07:56:49 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kim", "Hye-Rin", ""], ["Kim", "Yeong-Seok", ""], ["Kim", "Seon Joo", ""], ["Lee", "In-Kwon", ""]]}, {"id": "1705.07556", "submitter": "Yancong Wei", "authors": "Yancong Wei, Qiangqiang Yuan, Huanfeng Shen, Liangpei Zhang", "title": "Boosting the accuracy of multi-spectral image pan-sharpening by learning\n  a deep residual network", "comments": "5 pages,5 figures, 1 table", "journal-ref": null, "doi": "10.1109/LGRS.2017.2736020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of fusing multi-spectral and panchromatic images\n(Pan-sharpening), the impressive effectiveness of deep neural networks has been\nrecently employed to overcome the drawbacks of traditional linear models and\nboost the fusing accuracy. However, to the best of our knowledge, existing\nresearch works are mainly based on simple and flat networks with relatively\nshallow architecture, which severely limited their performances. In this paper,\nthe concept of residual learning has been introduced to form a very deep\nconvolutional neural network to make a full use of the high non-linearity of\ndeep learning models. By both quantitative and visual assessments on a large\nnumber of high quality multi-spectral images from various sources, it has been\nsupported that our proposed model is superior to all mainstream algorithms\nincluded in the comparison, and achieved the highest spatial-spectral unified\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:13:15 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 12:16:42 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wei", "Yancong", ""], ["Yuan", "Qiangqiang", ""], ["Shen", "Huanfeng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1705.07565", "submitter": "Xin Dong", "authors": "Xin Dong, Shangyu Chen, Sinno Jialin Pan", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain\n  Surgeon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to develop slim and accurate deep neural networks has become crucial for\nreal- world applications, especially for those employed in embedded systems.\nThough previous work along this research line has shown some promising results,\nmost existing methods either fail to significantly compress a well-trained deep\nnetwork or require a heavy retraining process for the pruned deep network to\nre-boost its prediction performance. In this paper, we propose a new layer-wise\npruning method for deep neural networks. In our proposed method, parameters of\neach individual layer are pruned independently based on second order\nderivatives of a layer-wise error function with respect to the corresponding\nparameters. We prove that the final prediction performance drop after pruning\nis bounded by a linear combination of the reconstructed errors caused at each\nlayer. Therefore, there is a guarantee that one only needs to perform a light\nretraining process on the pruned network to resume its original prediction\nperformance. We conduct extensive experiments on benchmark datasets to\ndemonstrate the effectiveness of our pruning method compared with several\nstate-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:54:37 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 23:50:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dong", "Xin", ""], ["Chen", "Shangyu", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1705.07594", "submitter": "Hao Wang", "authors": "Hao Wang, Xingyu Lin, Yimeng Zhang, Tai Sing Lee", "title": "Learning Robust Object Recognition Using Composed Scenes from Generative\n  Models", "comments": "Accepted by 14th Conference on Computer and Robot Vision", "journal-ref": null, "doi": "10.1109/CRV.2017.42", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent feedback connections in the mammalian visual system have been\nhypothesized to play a role in synthesizing input in the theoretical framework\nof analysis by synthesis. The comparison of internally synthesized\nrepresentation with that of the input provides a validation mechanism during\nperceptual inference and learning. Inspired by these ideas, we proposed that\nthe synthesis machinery can compose new, unobserved images by imagination to\ntrain the network itself so as to increase the robustness of the system in\nnovel scenarios. As a proof of concept, we investigated whether images composed\nby imagination could help an object recognition system to deal with occlusion,\nwhich is challenging for the current state-of-the-art deep convolutional neural\nnetworks. We fine-tuned a network on images containing objects in various\nocclusion scenarios, that are imagined or self-generated through a deep\ngenerator network. Trained on imagined occluded scenarios under the object\npersistence constraint, our network discovered more subtle and localized image\nfeatures that were neglected by the original network for object classification,\nobtaining better separability of different object classes in the feature space.\nThis leads to significant improvement of object recognition under occlusion for\nour network relative to the original network trained only on un-occluded\nimages. In addition to providing practical benefits in object recognition under\nocclusion, this work demonstrates the use of self-generated composition of\nvisual scenes through the synthesis loop, combined with the object persistence\nconstraint, can provide opportunities for neural networks to discover new\nrelevant patterns in the data, and become more flexible in dealing with novel\nsituations.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 07:50:24 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Hao", ""], ["Lin", "Xingyu", ""], ["Zhang", "Yimeng", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1705.07609", "submitter": "Hassan Foroosh", "authors": "Yuping Shen and Hassan Foroosh", "title": "View-Invariant Recognition of Action Style Self-Dissimilarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-similarity was recently introduced as a measure of inter-class\ncongruence for classification of actions. Herein, we investigate the dual\nproblem of intra-class dissimilarity for classification of action styles. We\nintroduce self-dissimilarity matrices that discriminate between same actions\nperformed by different subjects regardless of viewing direction and camera\nparameters. We investigate two frameworks using these invariant style\ndissimilarity measures based on Principal Component Analysis (PCA) and Fisher\nDiscriminant Analysis (FDA). Extensive experiments performed on IXMAS dataset\nindicate remarkably good discriminant characteristics for the proposed\ninvariant measures for gender recognition from video data.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:38:19 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Shen", "Yuping", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.07632", "submitter": "Liang Yanchao", "authors": "Yanchao Liang, Jianhua Li", "title": "Computer vision-based food calorie estimation: dataset, method, and\n  experiment", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has been introduced to estimate calories from food images.\nBut current food image data sets don't contain volume and mass records of\nfoods, which leads to an incomplete calorie estimation. In this paper, we\npresent a novel food image data set with volume and mass records of foods, and\na deep learning method for food detection, to make a complete calorie\nestimation. Our data set includes 2978 images, and every image contains\ncorresponding each food's annotation, volume and mass records, as well as a\ncertain calibration reference. To estimate calorie of food in the proposed data\nset, a deep learning method using Faster R-CNN first is put forward to detect\nthe food. And the experiment results show our method is effective to estimate\ncalories and our data set contains adequate information for calorie estimation.\nOur data set is the first released food image data set which can be used to\nevaluate computer vision-based calorie estimation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 09:47:29 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 05:41:44 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 07:48:37 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Liang", "Yanchao", ""], ["Li", "Jianhua", ""]]}, {"id": "1705.07640", "submitter": "Leonid Keselman", "authors": "Stan Melax, Leonid Keselman, Sterling Orsten", "title": "Dynamics Based 3D Skeletal Hand Tracking", "comments": "Published in Graphics Interface 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the full skeletal pose of the hands and fingers is a challenging\nproblem that has a plethora of applications for user interaction. Existing\ntechniques either require wearable hardware, add restrictions to user pose, or\nrequire significant computation resources. This research explores a new\napproach to tracking hands, or any articulated model, by using an augmented\nrigid body simulation. This allows us to phrase 3D object tracking as a linear\ncomplementarity problem with a well-defined solution. Based on a depth sensor's\nsamples, the system generates constraints that limit motion orthogonal to the\nrigid body model's surface. These constraints, along with prior motion,\ncollision/contact constraints, and joint mechanics, are resolved with a\nprojected Gauss-Seidel solver. Due to camera noise properties and attachment\nerrors, the numerous surface constraints are impulse capped to avoid\noverpowering mechanical constraints. To improve tracking accuracy, multiple\nsimulations are spawned at each frame and fed a variety of heuristics,\nconstraints and poses. A 3D error metric selects the best-fit simulation,\nhelping the system handle challenging hand motions. Such an approach enables\nreal-time, robust, and accurate 3D skeletal tracking of a user's hand on a\nvariety of depth cameras, while only utilizing a single x86 CPU core for\nprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:01:39 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Melax", "Stan", ""], ["Keselman", "Leonid", ""], ["Orsten", "Sterling", ""]]}, {"id": "1705.07692", "submitter": "Yunlong Yu", "authors": "Zhong Ji, Yunxin Sun, Yulong Yu, Jichang Guo, and Yanwei Pang", "title": "Semantic Softmax Loss for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical pipeline for Zero-Shot Learning (ZSL) is to integrate the visual\nfeatures and the class semantic descriptors into a multimodal framework with a\nlinear or bilinear model. However, the visual features and the class semantic\ndescriptors locate in different structural spaces, a linear or bilinear model\ncan not capture the semantic interactions between different modalities well. In\nthis letter, we propose a nonlinear approach to impose ZSL as a multi-class\nclassification problem via a Semantic Softmax Loss by embedding the class\nsemantic descriptors into the softmax layer of multi-class classification\nnetwork. To narrow the structural differences between the visual features and\nsemantic descriptors, we further use an L2 normalization constraint to the\ndifferences between the visual features and visual prototypes reconstructed\nwith the semantic descriptors. The results on three benchmark datasets, i.e.,\nAwA, CUB and SUN demonstrate the proposed approach can boost the performances\nsteadily and achieve the state-of-the-art performance for both zero-shot\nclassification and zero-shot retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 12:26:04 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ji", "Zhong", ""], ["Sun", "Yunxin", ""], ["Yu", "Yulong", ""], ["Guo", "Jichang", ""], ["Pang", "Yanwei", ""]]}, {"id": "1705.07750", "submitter": "Joao Carreira", "authors": "Joao Carreira and Andrew Zisserman", "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset", "comments": "Removed references to mini-kinetics dataset that was never made\n  publicly available and repeated all experiments on the full Kinetics dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paucity of videos in current action classification datasets (UCF-101 and\nHMDB-51) has made it difficult to identify good video architectures, as most\nmethods obtain similar performance on existing small-scale benchmarks. This\npaper re-evaluates state-of-the-art architectures in light of the new Kinetics\nHuman Action Video dataset. Kinetics has two orders of magnitude more data,\nwith 400 human action classes and over 400 clips per class, and is collected\nfrom realistic, challenging YouTube videos. We provide an analysis on how\ncurrent architectures fare on the task of action classification on this dataset\nand how much performance improves on the smaller benchmark datasets after\npre-training on Kinetics.\n  We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on\n2D ConvNet inflation: filters and pooling kernels of very deep image\nclassification ConvNets are expanded into 3D, making it possible to learn\nseamless spatio-temporal feature extractors from video while leveraging\nsuccessful ImageNet architecture designs and even their parameters. We show\nthat, after pre-training on Kinetics, I3D models considerably improve upon the\nstate-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%\non UCF-101.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:57:53 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 15:24:03 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 17:10:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carreira", "Joao", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.07755", "submitter": "Antonio Jose Jimeno Yepes", "authors": "Antonio Jimeno Yepes, Jianbin Tang, Benjamin Scott Mashford", "title": "Improving classification accuracy of feedforward neural networks for\n  spiking neuromorphic chips", "comments": "IJCAI-2017. arXiv admin note: text overlap with arXiv:1605.07740", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) achieve human level performance in many image\nanalytics tasks but DNNs are mostly deployed to GPU platforms that consume a\nconsiderable amount of power. New hardware platforms using lower precision\narithmetic achieve drastic reductions in power consumption. More recently,\nbrain-inspired spiking neuromorphic chips have achieved even lower power\nconsumption, on the order of milliwatts, while still offering real-time\nprocessing.\n  However, for deploying DNNs to energy efficient neuromorphic chips the\nincompatibility between continuous neurons and synaptic weights of traditional\nDNNs, discrete spiking neurons and synapses of neuromorphic chips need to be\novercome. Previous work has achieved this by training a network to learn\ncontinuous probabilities, before it is deployed to a neuromorphic architecture,\nsuch as IBM TrueNorth Neurosynaptic System, by random sampling these\nprobabilities.\n  The main contribution of this paper is a new learning algorithm that learns a\nTrueNorth configuration ready for deployment. We achieve this by training\ndirectly a binary hardware crossbar that accommodates the TrueNorth axon\nconfiguration constrains and we propose a different neuron model.\n  Results of our approach trained on electroencephalogram (EEG) data show a\nsignificant improvement with previous work (76% vs 86% accuracy) while\nmaintaining state of the art performance on the MNIST handwritten data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 12:59:14 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yepes", "Antonio Jimeno", ""], ["Tang", "Jianbin", ""], ["Mashford", "Benjamin Scott", ""]]}, {"id": "1705.07768", "submitter": "Jeffrey Helt", "authors": "Heqing Ya, Haonan Sun, Jeffrey Helt, Tai Sing Lee", "title": "Learning to Associate Words and Images Using a Large-scale Graph", "comments": "8 pages, 7 figures, 14th Conference on Computer and Robot Vision 2017", "journal-ref": null, "doi": "10.1109/CRV.2017.52", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach for unsupervised learning of associations between\nco-occurring perceptual events using a large graph. We applied this approach to\nsuccessfully solve the image captcha of China's railroad system. The approach\nis based on the principle of suspicious coincidence. In this particular\nproblem, a user is presented with a deformed picture of a Chinese phrase and\neight low-resolution images. They must quickly select the relevant images in\norder to purchase their train tickets. This problem presents several\nchallenges: (1) the teaching labels for both the Chinese phrases and the images\nwere not available for supervised learning, (2) no pre-trained deep\nconvolutional neural networks are available for recognizing these Chinese\nphrases or the presented images, and (3) each captcha must be solved within a\nfew seconds. We collected 2.6 million captchas, with 2.6 million deformed\nChinese phrases and over 21 million images. From these data, we constructed an\nassociation graph, composed of over 6 million vertices, and linked these\nvertices based on co-occurrence information and feature similarity between\npairs of images. We then trained a deep convolutional neural network to learn a\nprojection of the Chinese phrases onto a 230-dimensional latent space. Using\nlabel propagation, we computed the likelihood of each of the eight images\nconditioned on the latent space projection of the deformed phrase for each\ncaptcha. The resulting system solved captchas with 77% accuracy in 2 seconds on\naverage. Our work, in answering this practical challenge, illustrates the power\nof this class of unsupervised association learning techniques, which may be\nrelated to the brain's general strategy for associating language stimuli with\nvisual objects on the principle of suspicious coincidence.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:24:08 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ya", "Heqing", ""], ["Sun", "Haonan", ""], ["Helt", "Jeffrey", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1705.07772", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini and Hanwen Liu", "title": "Convolutional Networks with MuxOut Layers as Multi-rate Systems for\n  Image Upscaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret convolutional networks as adaptive filters and combine them with\nso-called MuxOut layers to efficiently upscale low resolution images. We\nformalize this interpretation by deriving a linear and space-variant structure\nof a convolutional network when its activations are fixed. We introduce general\npurpose algorithms to analyze a network and show its overall filter effect for\neach given location. We use this analysis to evaluate two types of image\nupscalers: deterministic upscalers that target the recovery of details from\noriginal content; and second, a new generation of upscalers that can sample the\ndistribution of upscale aliases (images that share the same downscale version)\nthat look like real content.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:37:57 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Liu", "Hanwen", ""]]}, {"id": "1705.07777", "submitter": "Yanbo Fan", "authors": "Yanbo Fan, Jian Liang, Ran He, Bao-Gang Hu, Siwei Lyu", "title": "Robust Localized Multi-view Subspace Clustering", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-view clustering, different views may have different confidence\nlevels when learning a consensus representation. Existing methods usually\naddress this by assigning distinctive weights to different views. However, due\nto noisy nature of real-world applications, the confidence levels of samples in\nthe same view may also vary. Thus considering a unified weight for a view may\nlead to suboptimal solutions. In this paper, we propose a novel localized\nmulti-view subspace clustering model that considers the confidence levels of\nboth views and samples. By assigning weight to each sample under each view\nproperly, we can obtain a robust consensus representation via fusing the\nnoiseless structures among views and samples. We further develop a regularizer\non weight parameters based on the convex conjugacy theory, and samples weights\nare determined in an adaptive manner. An efficient iterative algorithm is\ndeveloped with a convergence guarantee. Experimental results on four benchmarks\ndemonstrate the correctness and effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:44:18 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Fan", "Yanbo", ""], ["Liang", "Jian", ""], ["He", "Ran", ""], ["Hu", "Bao-Gang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1705.07818", "submitter": "Chenliang Xu", "authors": "Li Ding and Chenliang Xu", "title": "TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for\n  Video Action Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action segmentation as a milestone towards building automatic systems to\nunderstand untrimmed videos has received considerable attention in the recent\nyears. It is typically being modeled as a sequence labeling problem but\ncontains intrinsic and sufficient differences than text parsing or speech\nprocessing. In this paper, we introduce a novel hybrid temporal convolutional\nand recurrent network (TricorNet), which has an encoder-decoder architecture:\nthe encoder consists of a hierarchy of temporal convolutional kernels that\ncapture the local motion changes of different actions; the decoder is a\nhierarchy of recurrent neural networks that are able to learn and memorize\nlong-term action dependencies after the encoding stage. Our model is simple but\nextremely effective in terms of video sequence labeling. The experimental\nresults on three public action segmentation datasets have shown that the\nproposed model achieves superior performance over the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:55:08 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ding", "Li", ""], ["Xu", "Chenliang", ""]]}, {"id": "1705.07819", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Arpit Jain, Rama Chellappa and Ser Nam Lim", "title": "Regularizing deep networks using efficient layerwise adversarial\n  training", "comments": "Published at the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18). Official link:\n  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been shown to regularize deep neural networks in\naddition to increasing their robustness to adversarial examples. However, its\nimpact on very deep state of the art networks has not been fully investigated.\nIn this paper, we present an efficient approach to perform adversarial training\nby perturbing intermediate layer activations and study the use of such\nperturbations as a regularizer during training. We use these perturbations to\ntrain very deep models such as ResNets and show improvement in performance both\non adversarial and original test data. Our experiments highlight the benefits\nof perturbing intermediate layer activations compared to perturbing only the\ninputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the\nproposed adversarial training approach. Additional results on WideResNets show\nthat our approach provides significant improvement in classification accuracy\nfor a given base model, outperforming dropout and other base models of larger\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:55:42 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 02:27:51 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Jain", "Arpit", ""], ["Chellappa", "Rama", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1705.07831", "submitter": "Ayan Chakrabarti", "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti", "title": "Stabilizing GAN Training with Multiple Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks is unstable in high-dimensions as\nthe true data distribution tends to be concentrated in a small fraction of the\nambient space. The discriminator is then quickly able to classify nearly all\ngenerated samples as fake, leaving the generator without meaningful gradients\nand causing it to deteriorate after a point in training. In this work, we\npropose training a single generator simultaneously against an array of\ndiscriminators, each of which looks at a different random low-dimensional\nprojection of the data. Individual discriminators, now provided with restricted\nviews of the input, are unable to reject generated samples perfectly and\ncontinue to provide meaningful gradients to the generator throughout training.\nMeanwhile, the generator learns to produce samples consistent with the full\ndata distribution to satisfy all discriminators simultaneously. We demonstrate\nthe practical utility of this approach experimentally, and show that it is able\nto produce image samples with higher quality than traditional training with a\nsingle discriminator.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:23:26 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 00:53:46 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Bhojanapalli", "Srinadh", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1705.07844", "submitter": "Paul Guerrero", "authors": "Paul Guerrero, Holger Winnem\\\"oller, Wilmot Li, Niloy J. Mitra", "title": "DepthCut: Improved Depth Edge Estimation Using Multiple Unreliable\n  Channels", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of scene understanding, a variety of methods exists to\nestimate different information channels from mono or stereo images, including\ndisparity, depth, and normals. Although several advances have been reported in\nthe recent years for these tasks, the estimated information is often imprecise\nparticularly near depth discontinuities or creases. Studies have however shown\nthat precisely such depth edges carry critical cues for the perception of\nshape, and play important roles in tasks like depth-based segmentation or\nforeground selection. Unfortunately, the currently extracted channels often\ncarry conflicting signals, making it difficult for subsequent applications to\neffectively use them. In this paper, we focus on the problem of obtaining\nhigh-precision depth edges (i.e., depth contours and creases) by jointly\nanalyzing such unreliable information channels. We propose DepthCut, a\ndata-driven fusion of the channels using a convolutional neural network trained\non a large dataset with known depth. The resulting depth edges can be used for\nsegmentation, decomposing a scene into depth layers with relatively flat depth,\nor improving the accuracy of the depth estimate near depth edges by\nconstraining its gradients to agree with these edges. Quantitatively, we\ncompare against 15 variants of baselines and demonstrate that our depth edges\nresult in an improved segmentation performance and an improved depth estimate\nnear depth edges compared to data-agnostic channel fusion. Qualitatively, we\ndemonstrate that the depth edges result in superior segmentation and depth\norderings.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:48:15 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 14:21:54 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Guerrero", "Paul", ""], ["Winnem\u00f6ller", "Holger", ""], ["Li", "Wilmot", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1705.07871", "submitter": "Behzad Hasani", "authors": "Behzad Hasani and Mohammad H. Mahoor", "title": "Facial Expression Recognition Using Enhanced Deep 3D Convolutional\n  Neural Networks", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "journal-ref": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "doi": "10.1109/CVPRW.2017.282", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have shown to outperform traditional methods in\nvarious visual recognition tasks including Facial Expression Recognition (FER).\nIn spite of efforts made to improve the accuracy of FER systems using DNN,\nexisting methods still are not generalizable enough in practical applications.\nThis paper proposes a 3D Convolutional Neural Network method for FER in videos.\nThis new network architecture consists of 3D Inception-ResNet layers followed\nby an LSTM unit that together extracts the spatial relations within facial\nimages as well as the temporal relations between different frames in the video.\nFacial landmark points are also used as inputs to our network which emphasize\non the importance of facial components rather than the facial regions that may\nnot contribute significantly to generating facial expressions. Our proposed\nmethod is evaluated using four publicly available databases in\nsubject-independent and cross-database tasks and outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:31:46 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hasani", "Behzad", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1705.07884", "submitter": "Behzad Hasani", "authors": "Behzad Hasani and Mohammad H. Mahoor", "title": "Facial Affect Estimation in the Wild Using Deep Residual and\n  Convolutional Networks", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "journal-ref": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "doi": "10.1109/CVPRW.2017.245", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated affective computing in the wild is a challenging task in the field\nof computer vision. This paper presents three neural network-based methods\nproposed for the task of facial affect estimation submitted to the First\nAffect-in-the-Wild challenge. These methods are based on Inception-ResNet\nmodules redesigned specifically for the task of facial affect estimation. These\nmethods are: Shallow Inception-ResNet, Deep Inception-ResNet, and\nInception-ResNet with LSTMs. These networks extract facial features in\ndifferent scales and simultaneously estimate both the valence and arousal in\neach frame. Root Mean Square Error (RMSE) rates of 0.4 and 0.3 are achieved for\nthe valence and arousal respectively with corresponding Concordance Correlation\nCoefficient (CCC) rates of 0.04 and 0.29 using Deep Inception-ResNet method.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:59:10 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hasani", "Behzad", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1705.07904", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley", "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:00:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:05 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:36:33 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["Balsubramani", "Akshay", ""], ["McAuley", "Julian", ""]]}, {"id": "1705.07962", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:32:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:27:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Beltramelli", "Tony", ""]]}, {"id": "1705.07972", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Sunpreet S. Arora, Anil K. Jain, Nicholas G.\n  Paulter Jr", "title": "Universal 3D Wearable Fingerprint Targets: Advancing Fingerprint Reader\n  Evaluations", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and manufacturing of high fidelity universal 3D\nfingerprint targets, which can be imaged on a variety of fingerprint sensing\ntechnologies, namely capacitive, contact-optical, and contactless-optical.\nUniversal 3D fingerprint targets enable, for the first time, not only a\nrepeatable and controlled evaluation of fingerprint readers, but also the\nability to conduct fingerprint reader interoperability studies. Fingerprint\nreader interoperability refers to how robust fingerprint recognition systems\nare to variations in the images acquired by different types of fingerprint\nreaders. To build universal 3D fingerprint targets, we adopt a molding and\ncasting framework consisting of (i) digital mapping of fingerprint images to a\nnegative mold, (ii) CAD modeling a scaffolding system to hold the negative\nmold, (iii) fabricating the mold and scaffolding system with a high resolution\n3D printer, (iv) producing or mixing a material with similar electrical,\noptical, and mechanical properties to that of the human finger, and (v)\nfabricating a 3D fingerprint target using controlled casting. Our experiments\nconducted with PIV and Appendix F certified optical (contact and contactless)\nand capacitive fingerprint readers demonstrate the usefulness of universal 3D\nfingerprint targets for controlled and repeatable fingerprint reader\nevaluations and also fingerprint reader interoperability studies.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:51:59 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Arora", "Sunpreet S.", ""], ["Jain", "Anil K.", ""], ["Paulter", "Nicholas G.", "Jr"]]}, {"id": "1705.07999", "submitter": "Florian Dubost", "authors": "Florian Dubost, Gerda Bortsova, Hieab Adams, Arfan Ikram, Wiro\n  Niessen, Meike Vernooij, Marleen De Bruijne", "title": "GP-Unet: Lesion Detection from Weak Labels with a 3D Regression Network", "comments": "Article published in MICCAI 2017. We corrected a few errors from the\n  first version: padding, loss, typos and update of the DOI number", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel convolutional neural network for lesion detection from\nweak labels. Only a single, global label per image - the lesion count - is\nneeded for training. We train a regression network with a fully convolutional\narchitecture combined with a global pooling layer to aggregate the 3D output\ninto a scalar indicating the lesion count. When testing on unseen images, we\nfirst run the network to estimate the number of lesions. Then we remove the\nglobal pooling layer to compute localization maps of the size of the input\nimage. We evaluate the proposed network on the detection of enlarged\nperivascular spaces in the basal ganglia in MRI. Our method achieves a\nsensitivity of 62% with on average 1.5 false positives per image. Compared with\nfour other approaches based on intensity thresholding, saliency and class maps,\nour method has a 20% higher sensitivity.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 20:55:47 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 09:46:11 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Dubost", "Florian", ""], ["Bortsova", "Gerda", ""], ["Adams", "Hieab", ""], ["Ikram", "Arfan", ""], ["Niessen", "Wiro", ""], ["Vernooij", "Meike", ""], ["De Bruijne", "Marleen", ""]]}, {"id": "1705.08016", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Otkrist Gupta, Pei Guo, Ramesh Raskar, Ryan Farrell\n  and Nikhil Naik", "title": "Pairwise Confusion for Fine-Grained Visual Classification", "comments": "Camera-Ready version for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-Grained Visual Classification (FGVC) datasets contain small sample\nsizes, along with significant intra-class variation and inter-class similarity.\nWhile prior work has addressed intra-class variation using localization and\nsegmentation techniques, inter-class similarity may also affect feature\nlearning and reduce classification performance. In this work, we address this\nproblem using a novel optimization procedure for the end-to-end neural network\ntraining on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces\noverfitting by intentionally {introducing confusion} in the activations. With\nPC regularization, we obtain state-of-the-art performance on six of the most\nwidely-used FGVC datasets and demonstrate improved localization ability. {PC}\nis easy to implement, does not need excessive hyperparameter tuning during\ntraining, and does not add significant overhead during test time.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:44:38 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 22:23:47 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 18:35:21 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Gupta", "Otkrist", ""], ["Guo", "Pei", ""], ["Raskar", "Ramesh", ""], ["Farrell", "Ryan", ""], ["Naik", "Nikhil", ""]]}, {"id": "1705.08041", "submitter": "Steven Diamond", "authors": "Steven Diamond and Vincent Sitzmann and Felix Heide and Gordon\n  Wetzstein", "title": "Unrolled Optimization with Deep Priors", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad class of problems at the core of computational imaging, sensing, and\nlow-level computer vision reduces to the inverse problem of extracting latent\nimages that follow a prior distribution, from measurements taken under a known\nphysical image formation model. Traditionally, hand-crafted priors along with\niterative optimization methods have been used to solve such problems. In this\npaper we present unrolled optimization with deep priors, a principled framework\nfor infusing knowledge of the image formation into deep networks that solve\ninverse problems in imaging, inspired by classical iterative methods. We show\nthat instances of the framework outperform the state-of-the-art by a\nsubstantial margin for a wide variety of imaging problems, such as denoising,\ndeblurring, and compressed sensing magnetic resonance imaging (MRI). Moreover,\nwe conduct experiments that explain how the framework is best used and why it\noutperforms previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:24:29 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 22:03:24 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Diamond", "Steven", ""], ["Sitzmann", "Vincent", ""], ["Heide", "Felix", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1705.08045", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi", "title": "Learning multiple visual domains with residual adapters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in learning data representations that work well\nfor many different types of problems and data. In this paper, we look in\nparticular at the task of learning a single visual representation that can be\nsuccessfully utilized in the analysis of very different types of images, from\ndog breeds to stop signs and digits. Inspired by recent work on learning\nnetworks that predict the parameters of another, we develop a tunable deep\nnetwork architecture that, by means of adapter residual modules, can be steered\non the fly to diverse visual domains. Our method achieves a high degree of\nparameter sharing while maintaining or even improving the accuracy of\ndomain-specific representations. We also introduce the Visual Decathlon\nChallenge, a benchmark that evaluates the ability of representations to capture\nsimultaneously ten very different visual domains and measures their ability to\nrecognize well uniformly.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:59:23 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 23:05:40 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 16:56:16 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 07:27:04 GMT"}, {"version": "v5", "created": "Mon, 27 Nov 2017 17:35:38 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1705.08066", "submitter": "Bo Jiang", "authors": "Bo Jiang and Chris Ding and Bin Luo", "title": "Multiple Images Recovery Using a Single Affine Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, image data often come with noises,\ncorruptions or large errors. One approach to deal with noise image data is to\nuse data recovery techniques which aim to recover the true uncorrupted signals\nfrom the observed noise images. In this paper, we first introduce a novel\ncorruption recovery transformation (CRT) model which aims to recover multiple\n(or a collection of) corrupted images using a single affine transformation.\nThen, we show that the introduced CRT can be efficiently constructed through\nlearning from training data. Once CRT is learned, we can recover the true\nsignals from the new incoming/test corrupted images explicitly. As an\napplication, we apply our CRT to image recognition task. Experimental results\non six image datasets demonstrate that the proposed CRT model is effective in\nrecovering noise image data and thus leads to better recognition results.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 03:14:50 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Jiang", "Bo", ""], ["Ding", "Chris", ""], ["Luo", "Bin", ""]]}, {"id": "1705.08078", "submitter": "Adityanarayanan Radhakrishnan", "authors": "Adityanarayanan Radhakrishnan, Charles Durham, Ali Soylemezoglu,\n  Caroline Uhler", "title": "Patchnet: Interpretable Neural Networks for Image Classification", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/77", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how a complex machine learning model makes a classification\ndecision is essential for its acceptance in sensitive areas such as health\ncare. Towards this end, we present PatchNet, a method that provides the\nfeatures indicative of each class in an image using a tradeoff between\nrestricting global image context and classification error. We mathematically\nanalyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual\nheatmap representations of the learned features, and quantitatively compare\nthese features with features selected by domain experts by applying PatchNet to\nthe classification of benign/malignant skin lesions from the ISBI-ISIC 2017\nmelanoma classification challenge.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:18:14 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 00:48:02 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 17:56:09 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 23:44:21 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Radhakrishnan", "Adityanarayanan", ""], ["Durham", "Charles", ""], ["Soylemezoglu", "Ali", ""], ["Uhler", "Caroline", ""]]}, {"id": "1705.08080", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav\n  Gupta, Roozbeh Mottaghi, Ali Farhadi", "title": "Visual Semantic Planning using Deep Successor Representations", "comments": "ICCV 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial capability of real-world intelligent agents is their ability to\nplan a sequence of actions to achieve their goals in the visual world. In this\nwork, we address the problem of visual semantic planning: the task of\npredicting a sequence of actions from visual observations that transform a\ndynamic environment from an initial state to a goal state. Doing so entails\nknowledge about objects and their affordances, as well as actions and their\npreconditions and effects. We propose learning these through interacting with a\nvisual and dynamic environment. Our proposed solution involves bootstrapping\nreinforcement learning with imitation learning. To ensure cross task\ngeneralization, we develop a deep predictive model based on successor\nrepresentations. Our experimental results show near optimal results across a\nwide range of tasks in the challenging THOR environment.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:22:47 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 21:13:49 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhu", "Yuke", ""], ["Gordon", "Daniel", ""], ["Kolve", "Eric", ""], ["Fox", "Dieter", ""], ["Fei-Fei", "Li", ""], ["Gupta", "Abhinav", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1705.08086", "submitter": "Yijun Li", "authors": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang", "title": "Universal Style Transfer via Feature Transforms", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal style transfer aims to transfer arbitrary visual styles to content\nimages. Existing feed-forward based methods, while enjoying the inference\nefficiency, are mainly limited by inability of generalizing to unseen styles or\ncompromised visual quality. In this paper, we present a simple yet effective\nmethod that tackles these limitations without training on any pre-defined\nstyles. The key ingredient of our method is a pair of feature transforms,\nwhitening and coloring, that are embedded to an image reconstruction network.\nThe whitening and coloring transforms reflect a direct matching of feature\ncovariance of the content image to a given style image, which shares similar\nspirits with the optimization of Gram matrix based cost in neural style\ntransfer. We demonstrate the effectiveness of our algorithm by generating\nhigh-quality stylized images with comparisons to a number of recent methods. We\nalso analyze our method by visualizing the whitened features and synthesizing\ntextures via simple feature coloring.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 06:10:58 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:30:43 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Li", "Yijun", ""], ["Fang", "Chen", ""], ["Yang", "Jimei", ""], ["Wang", "Zhaowen", ""], ["Lu", "Xin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1705.08101", "submitter": "Devis Tuia", "authors": "S\\'ebastien Lef\\`evre, Devis Tuia, Jan Dirk Wegner, Timoth\\'ee\n  Produit, Ahmed Samy Nassar", "title": "Towards seamless multi-view scene analysis from satellite to\n  street-level", "comments": null, "journal-ref": "Proceedings of the IEEE, 105, pp. 1884-1899, 2017", "doi": "10.1109/JPROC.2017.2684300", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss and review how combined multi-view imagery from\nsatellite to street-level can benefit scene analysis. Numerous works exist that\nmerge information from remote sensing and images acquired from the ground for\ntasks like land cover mapping, object detection, or scene understanding. What\nmakes the combination of overhead and street-level images challenging, is the\nstrongly varying viewpoint, different scale, illumination, sensor modality and\ntime of acquisition. Direct (dense) matching of images on a per-pixel basis is\nthus often impossible, and one has to resort to alternative strategies that\nwill be discussed in this paper. We review recent works that attempt to combine\nimages taken from the ground and overhead views for purposes like scene\nregistration, reconstruction, or classification. Three methods that represent\nthe wide range of potential methods and applications (change detection, image\norientation, and tree cataloging) are described in detail. We show that\ncross-fertilization between remote sensing, computer vision and machine\nlearning is very valuable to make the best of geographic data available from\nEarth Observation sensors and ground imagery. Despite its challenges, we\nbelieve that integrating these complementary data sources will lead to major\nbreakthroughs in Big GeoData.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 07:19:52 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Lef\u00e8vre", "S\u00e9bastien", ""], ["Tuia", "Devis", ""], ["Wegner", "Jan Dirk", ""], ["Produit", "Timoth\u00e9e", ""], ["Nassar", "Ahmed Samy", ""]]}, {"id": "1705.08106", "submitter": "Juanhui Tu", "authors": "Hong Liu and Juanhui Tu and Mengyuan Liu", "title": "Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action\n  Recognition", "comments": "5 pages, 6 figures, 3 tabels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It remains a challenge to efficiently extract spatialtemporal information\nfrom skeleton sequences for 3D human action recognition. Although most recent\naction recognition methods are based on Recurrent Neural Networks which present\noutstanding performance, one of the shortcomings of these methods is the\ntendency to overemphasize the temporal information. Since 3D convolutional\nneural network(3D CNN) is a powerful tool to simultaneously learn features from\nboth spatial and temporal dimensions through capturing the correlations between\nthree dimensional signals, this paper proposes a novel two-stream model using\n3D CNN. To our best knowledge, this is the first application of 3D CNN in\nskeleton-based action recognition. Our method consists of three stages. First,\nskeleton joints are mapped into a 3D coordinate space and then encoding the\nspatial and temporal information, respectively. Second, 3D CNN models are\nseperately adopted to extract deep features from two streams. Third, to enhance\nthe ability of deep features to capture global relationships, we extend every\nstream into multitemporal version. Extensive experiments on the SmartHome\ndataset and the large-scale NTU RGB-D dataset demonstrate that our method\noutperforms most of RNN-based methods, which verify the complementary property\nbetween spatial and temporal information and the robustness to noise.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 07:36:51 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 11:23:40 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Liu", "Hong", ""], ["Tu", "Juanhui", ""], ["Liu", "Mengyuan", ""]]}, {"id": "1705.08111", "submitter": "Benjamin Gutierrez Becker", "authors": "Benjam\\'in Guti\\'errez and Lo\\\"ic Peter and Tassilo Klein and\n  Christian Wachinger", "title": "A Multi-Armed Bandit to Smartly Select a Training Set from Big Medical\n  Data", "comments": "MICCAI 2017 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of big medical image data, the selection of an adequate\ntraining set is becoming more important to address the heterogeneity of\ndifferent datasets. Simply including all the data does not only incur high\nprocessing costs but can even harm the prediction. We formulate the smart and\nefficient selection of a training dataset from big medical image data as a\nmulti-armed bandit problem, solved by Thompson sampling. Our method assumes\nthat image features are not available at the time of the selection of the\nsamples, and therefore relies only on meta information associated with the\nimages. Our strategy simultaneously exploits data sources with high chances of\nyielding useful samples and explores new data regions. For our evaluation, we\nfocus on the application of estimating the age from a brain MRI. Our results on\n7,250 subjects from 10 datasets show that our approach leads to higher accuracy\nwhile only requiring a fraction of the training data.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 07:51:54 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 12:50:19 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Guti\u00e9rrez", "Benjam\u00edn", ""], ["Peter", "Lo\u00efc", ""], ["Klein", "Tassilo", ""], ["Wachinger", "Christian", ""]]}, {"id": "1705.08168", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Look, Listen and Learn", "comments": "Appears in: IEEE International Conference on Computer Vision (ICCV)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question: what can be learnt by looking at and listening to a\nlarge number of unlabelled videos? There is a valuable, but so far untapped,\nsource of information contained in the video itself -- the correspondence\nbetween the visual and the audio streams, and we introduce a novel\n\"Audio-Visual Correspondence\" learning task that makes use of this. Training\nvisual and audio networks from scratch, without any additional supervision\nother than the raw unconstrained videos themselves, is shown to successfully\nsolve this task, and, more interestingly, result in good visual and audio\nrepresentations. These features set the new state-of-the-art on two sound\nclassification benchmarks, and perform on par with the state-of-the-art\nself-supervised approaches on ImageNet classification. We also demonstrate that\nthe network is able to localize objects in both modalities, as well as perform\nfine-grained recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 10:37:54 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 12:04:50 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.08180", "submitter": "Pietro Morerio", "authors": "Pietro Morerio and Vittorio Murino", "title": "Correlation Alignment by Riemannian Metric for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation techniques address the problem of reducing the sensitivity\nof machine learning methods to the so-called domain shift, namely the\ndifference between source (training) and target (test) data distributions. In\nparticular, unsupervised domain adaptation assumes no labels are available in\nthe target domain. To this end, aligning second order statistics (covariances)\nof target and source domains have proven to be an effective approach ti fill\nthe gap between the domains. However, covariance matrices do not form a\nsubspace of the Euclidean space, but live in a Riemannian manifold with\nnon-positive curvature, making the usual Euclidean metric suboptimal to measure\ndistances. In this paper, we extend the idea of training a neural network with\na constraint on the covariances of the hidden layer features, by rigorously\naccounting for the curved structure of the manifold of symmetric positive\ndefinite matrices. The resulting loss function exploits a theoretically sound\ngeodesic distance on such manifold. Results show indeed the suboptimal nature\nof the Euclidean distance. This makes us able to perform better than previous\napproaches on the standard Office dataset, a benchmark for domain adaptation\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 11:08:48 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1705.08182", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, Marius Popescu", "title": "Unmasking the abnormal events in video", "comments": "Accepted at the 2017 International Conference on Computer Vision\n  (ICCV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for abnormal event detection in video that\nrequires no training sequences. Our framework is based on unmasking, a\ntechnique previously used for authorship verification in text documents, which\nwe adapt to our task. We iteratively train a binary classifier to distinguish\nbetween two consecutive video sequences while removing at each step the most\ndiscriminant features. Higher training accuracy rates of the intermediately\nobtained classifiers represent abnormal events. To the best of our knowledge,\nthis is the first work to apply unmasking for a computer vision task. We\ncompare our method with several state-of-the-art supervised and unsupervised\nmethods on four benchmark data sets. The empirical results indicate that our\nabnormal event detection framework can achieve state-of-the-art results, while\nrunning in real-time at 20 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 11:14:17 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 05:38:34 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 14:10:46 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Smeureanu", "Sorina", ""], ["Alexe", "Bogdan", ""], ["Popescu", "Marius", ""]]}, {"id": "1705.08207", "submitter": "Tam Nguyen", "authors": "Tam V. Nguyen, Luoqi Liu", "title": "Salient Object Detection with Semantic Priors", "comments": "accepted to IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has increasingly become a popular topic in cognitive\nand computational sciences, including computer vision and artificial\nintelligence research. In this paper, we propose integrating \\textit{semantic\npriors} into the salient object detection process. Our algorithm consists of\nthree basic steps. Firstly, the explicit saliency map is obtained based on the\nsemantic segmentation refined by the explicit saliency priors learned from the\ndata. Next, the implicit saliency map is computed based on a trained model\nwhich maps the implicit saliency priors embedded into regional features with\nthe saliency values. Finally, the explicit semantic map and the implicit map\nare adaptively fused to form a pixel-accurate saliency map which uniformly\ncovers the objects of interest. We further evaluate the proposed framework on\ntwo challenging datasets, namely, ECSSD and HKUIS. The extensive experimental\nresults demonstrate that our method outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:24:09 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Nguyen", "Tam V.", ""], ["Liu", "Luoqi", ""]]}, {"id": "1705.08214", "submitter": "Michael Gygli", "authors": "Michael Gygli", "title": "Ridiculously Fast Shot Boundary Detection with Fully Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shot boundary detection (SBD) is an important component of many video\nanalysis tasks, such as action recognition, video indexing, summarization and\nediting. Previous work typically used a combination of low-level features like\ncolor histograms, in conjunction with simple models such as SVMs. Instead, we\npropose to learn shot detection end-to-end, from pixels to final shot\nboundaries. For training such a model, we rely on our insight that all shot\nboundaries are generated. Thus, we create a dataset with one million frames and\nautomatically generated transitions such as cuts, dissolves and fades. In order\nto efficiently analyze hours of videos, we propose a Convolutional Neural\nNetwork (CNN) which is fully convolutional in time, thus allowing to use a\nlarge temporal context without the need to repeatedly processing frames. With\nthis architecture our method obtains state-of-the-art results while running at\nan unprecedented speed of more than 120x real-time.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:39:51 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gygli", "Michael", ""]]}, {"id": "1705.08244", "submitter": "A. M. Khalili", "authors": "A. M. Khalili", "title": "On the mathematics of beauty: beautiful images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will study the simplest kind of beauty which can be found\nin simple visual patterns. The proposed approach shows that aesthetically\nappealing patterns deliver higher amount of information over multiple levels in\ncomparison with less aesthetically appealing patterns when the same amount of\nenergy is used. The proposed approach is used to classify aesthetically\nappealing patterns.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 21:53:51 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 20:40:50 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 16:10:49 GMT"}, {"version": "v4", "created": "Sun, 17 Jun 2018 12:23:55 GMT"}, {"version": "v5", "created": "Sun, 3 Feb 2019 15:56:00 GMT"}, {"version": "v6", "created": "Thu, 17 Oct 2019 13:24:19 GMT"}, {"version": "v7", "created": "Fri, 8 Nov 2019 12:42:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Khalili", "A. M.", ""]]}, {"id": "1705.08252", "submitter": "Emil Eriksson", "authors": "Emil Eriksson, Gy\\\"orgy D\\'an, Viktoria Fodor", "title": "Distributed Algorithms for Feature Extraction Off-loading in\n  Multi-Camera Visual Sensor Networks", "comments": "12 pages, 7 figures, submitted to Transactions on Circuits and\n  Systems for Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time visual analysis tasks, like tracking and recognition, require swift\nexecution of computationally intensive algorithms. Visual sensor networks can\nbe enabled to perform such tasks by augmenting the sensor network with\nprocessing nodes and distributing the computational burden in a way that the\ncameras contend for the processing nodes while trying to minimize their task\ncompletion times. In this paper, we formulate the problem of minimizing the\ncompletion time of all camera sensors as an optimization problem. We propose\nalgorithms for fully distributed optimization, analyze the existence of\nequilibrium allocations, evaluate the effect of the network topology and of the\nvideo characteristics, and the benefits of central coordination. Our results\ndemonstrate that with sufficient information available, distributed\noptimization can provide low completion times, moreover predictable and stable\nperformance can be achieved with additional, sparse central coordination.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:16:43 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eriksson", "Emil", ""], ["D\u00e1n", "Gy\u00f6rgy", ""], ["Fodor", "Viktoria", ""]]}, {"id": "1705.08260", "submitter": "Menglong Ye", "authors": "Menglong Ye and Edward Johns and Ankur Handa and Lin Zhang and Philip\n  Pratt and Guang-Zhong Yang", "title": "Self-Supervised Siamese Learning on Stereo Image Pairs for Depth\n  Estimation in Robotic Surgery", "comments": "A two-page short report to be presented at the Hamlyn Symposium on\n  Medical Robotics 2017. An extension of this work is on progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic surgery has become a powerful tool for performing minimally invasive\nprocedures, providing advantages in dexterity, precision, and 3D vision, over\ntraditional surgery. One popular robotic system is the da Vinci surgical\nplatform, which allows preoperative information to be incorporated into live\nprocedures using Augmented Reality (AR). Scene depth estimation is a\nprerequisite for AR, as accurate registration requires 3D correspondences\nbetween preoperative and intraoperative organ models. In the past decade, there\nhas been much progress on depth estimation for surgical scenes, such as using\nmonocular or binocular laparoscopes [1,2]. More recently, advances in deep\nlearning have enabled depth estimation via Convolutional Neural Networks (CNNs)\n[3], but training requires a large image dataset with ground truth depths.\nInspired by [4], we propose a deep learning framework for surgical scene depth\nestimation using self-supervision for scalable data acquisition. Our framework\nconsists of an autoencoder for depth prediction, and a differentiable spatial\ntransformer for training the autoencoder on stereo image pairs without ground\ntruth depths. Validation was conducted on stereo videos collected in robotic\npartial nephrectomy.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 11:10:49 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ye", "Menglong", ""], ["Johns", "Edward", ""], ["Handa", "Ankur", ""], ["Zhang", "Lin", ""], ["Pratt", "Philip", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1705.08264", "submitter": "Hua Li", "authors": "Erbo Li and Hua Li", "title": "Isomorphism between Differential and Moment Invariants under Affine\n  Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The invariant is one of central topics in science, technology and\nengineering. The differential invariant is essential in understanding or\ndescribing some important phenomena or procedures in mathematics, physics,\nchemistry, biology or computer science etc. The derivation of differential\ninvariants is usually difficult or complicated. This paper reports a discovery\nthat under the affine transform, differential invariants have similar\nstructures with moment invariants up to a scalar function of transform\nparameters. If moment invariants are known, relative differential invariants\ncan be obtained by the substitution of moments by derivatives with the same\norder. Whereas moment invariants can be calculated by multiple integrals, this\nmethod provides a simple way to derive differential invariants without the need\nto resolve any equation system. Since the definition of moments on different\nmanifolds or in different dimension of spaces is well established, differential\ninvariants on or in them will also be well defined. Considering that moments\nhave a strong background in mathematics and physics, this technique offers a\nnew view angle to the inner structure of invariants. Projective differential\ninvariants can also be found in this way with a screening process.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 20:34:44 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 19:07:19 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Li", "Erbo", ""], ["Li", "Hua", ""]]}, {"id": "1705.08266", "submitter": "David Barina", "authors": "David Barina and Michal Kula and Michal Matysek and Pavel Zemcik", "title": "Accelerating Discrete Wavelet Transforms on GPUs", "comments": "preprint submitted to ICIP 2017. arXiv admin note: substantial text\n  overlap with arXiv:1704.08657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional discrete wavelet transform has a huge number of\napplications in image-processing techniques. Until now, several papers compared\nthe performance of such transform on graphics processing units (GPUs). However,\nall of them only dealt with lifting and convolution computation schemes. In\nthis paper, we show that corresponding horizontal and vertical lifting parts of\nthe lifting scheme can be merged into non-separable lifting units, which halves\nthe number of steps. We also discuss an optimization strategy leading to a\nreduction in the number of arithmetic operations. The schemes were assessed\nusing the OpenCL and pixel shaders. The proposed non-separable lifting scheme\noutperforms the existing schemes in many cases, irrespective of its higher\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 14:42:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Barina", "David", ""], ["Kula", "Michal", ""], ["Matysek", "Michal", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1705.08272", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Lubor Ladicky and Marc Pollefeys", "title": "Matching neural paths: transfer from recognition to correspondence\n  search", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks require finding per-part correspondences between\nobjects. In this work we focus on low-level correspondences - a highly\nambiguous matching problem. We propose to use a hierarchical semantic\nrepresentation of the objects, coming from a convolutional neural network, to\nsolve this ambiguity. Training it for low-level correspondence prediction\ndirectly might not be an option in some domains where the ground-truth\ncorrespondences are hard to obtain. We show how transfer from recognition can\nbe used to avoid such training. Our idea is to mark parts as \"matching\" if\ntheir features are close to each other at all the levels of convolutional\nfeature hierarchy (neural paths). Although the overall number of such paths is\nexponential in the number of layers, we propose a polynomial algorithm for\naggregating all of them in a single backward pass. The empirical validation is\ndone on the task of stereo correspondence and demonstrates that we achieve\ncompetitive results among the methods which do not use labeled target domain\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:40:35 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:04:30 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 22:33:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1705.08273", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Klaus Engelke, Christina Fuchs, Willi Kalender", "title": "A New 3D Segmentation Technique for QCT Scans of the Lumbar Spine to\n  Determine BMD and Vertebral Geometry", "comments": "2 pages, 2 figures, International Congress of Medical Physics 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative computed tomography (QCT) is a standard method to determine bone\nmineral density (BMD) in the spine. Traditionally single 8 - 10 mm thick slices\nhave been analyzed only. Current spiral CT scanners provide true 3D acquisition\nschemes allowing for a more differential BMD analysis and an assessment of\ngeometric parameters, which may improve fracture prediction. We developed a\nnovel 3D segmentation approach that combines deformable balloons, multi seeded\nvolume growing, and dedicated morphological operations to extract the vertebral\nbodies. An anatomy-oriented coordinate system attached automatically to each\nvertebra is used to define volumes of interest. We analyzed intra-operator\nprecision of the segmentation procedure using abdominal scans from 10 patients\n(60 mAs, 120 kV, slice thickness 1mm, B40s, Siemens Sensation 16). Our new\nsegmentation method shows excellent precision errors in the order of < 1 % for\nBMD and < 2 % for volume.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:13:38 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Engelke", "Klaus", ""], ["Fuchs", "Christina", ""], ["Kalender", "Willi", ""]]}, {"id": "1705.08280", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu,\n  Dim P. Papadopoulos, Vittorio Ferrari", "title": "How hard can it be? Estimating the difficulty of visual search in an\n  image", "comments": "Published at CVPR 2016", "journal-ref": "In Proceedings of CVPR, pp. 2157-2166, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating image difficulty defined as the human\nresponse time for solving a visual search task. We collect human annotations of\nimage difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing\nplatform. We then analyze what human interpretable image properties can have an\nimpact on visual search difficulty, and how accurate are those properties for\npredicting difficulty. Next, we build a regression model based on deep features\nlearned with state of the art convolutional neural networks and show better\nresults for predicting the ground-truth visual search difficulty scores\nproduced by human annotators. Our model is able to correctly rank about 75%\nimage pairs according to their difficulty score. We also show that our\ndifficulty predictor generalizes well to new classes not seen during training.\nFinally, we demonstrate that our predicted difficulty scores are useful for\nweakly supervised object localization (8% improvement) and semi-supervised\nobject classification (1% improvement).\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:03:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Alexe", "Bogdan", ""], ["Leordeanu", "Marius", ""], ["Popescu", "Marius", ""], ["Papadopoulos", "Dim P.", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1705.08293", "submitter": "Hassan Foroosh", "authors": "Yuping Shen and Hassan Foroosh", "title": "An Invariant Model of the Significance of Different Body Parts in\n  Recognizing Different Actions", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.04641,\n  arXiv:1705.05741, arXiv:1705.04433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that different body parts do not play equally\nimportant roles in recognizing a human action in video data. We investigate to\nwhat extent a body part plays a role in recognition of different actions and\nhence propose a generic method of assigning weights to different body points.\nThe approach is inspired by the strong evidence in the applied perception\ncommunity that humans perform recognition in a foveated manner, that is they\nrecognize events or objects by only focusing on visually significant aspects.\nAn important contribution of our method is that the computation of the weights\nassigned to body parts is invariant to viewing directions and camera parameters\nin the input data. We have performed extensive experiments to validate the\nproposed approach and demonstrate its significance. In particular, results show\nthat considerable improvement in performance is gained by taking into account\nthe relative importance of different body parts as defined by our approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 06:03:07 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Shen", "Yuping", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1705.08302", "submitter": "Ozan Oktay Mr", "authors": "Ozan Oktay, Enzo Ferrante, Konstantinos Kamnitsas, Mattias Heinrich,\n  Wenjia Bai, Jose Caballero, Stuart Cook, Antonio de Marvao, Timothy Dawes,\n  Declan O'Regan, Bernhard Kainz, Ben Glocker, Daniel Rueckert", "title": "Anatomically Constrained Neural Networks (ACNN): Application to Cardiac\n  Image Enhancement and Segmentation", "comments": "Published in IEEE Transactions on Medical Imaging (Aug 2017)", "journal-ref": null, "doi": "10.1109/TMI.2017.2743464", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Incorporation of prior knowledge about organ shape and location is key to\nimprove performance of image analysis approaches. In particular, priors can be\nuseful in cases where images are corrupted and contain artefacts due to\nlimitations in image acquisition. The highly constrained nature of anatomical\nobjects can be well captured with learning based techniques. However, in most\nrecent and promising techniques such as CNN based segmentation it is not\nobvious how to incorporate such prior knowledge. State-of-the-art methods\noperate as pixel-wise classifiers where the training objectives do not\nincorporate the structure and inter-dependencies of the output. To overcome\nthis limitation, we propose a generic training strategy that incorporates\nanatomical prior knowledge into CNNs through a new regularisation model, which\nis trained end-to-end. The new framework encourages models to follow the global\nanatomical properties of the underlying anatomy (e.g. shape, label structure)\nvia learned non-linear representations of the shape. We show that the proposed\napproach can be easily adapted to different analysis tasks (e.g. image\nenhancement, segmentation) and improve the prediction accuracy of the\nstate-of-the-art models. The applicability of our approach is shown on\nmulti-modal cardiac datasets and public benchmarks. Additionally, we\ndemonstrate how the learned deep models of 3D shapes can be interpreted and\nused as biomarkers for classification of cardiac pathologies.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 12:32:25 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 11:51:58 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 13:31:18 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 23:04:00 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Oktay", "Ozan", ""], ["Ferrante", "Enzo", ""], ["Kamnitsas", "Konstantinos", ""], ["Heinrich", "Mattias", ""], ["Bai", "Wenjia", ""], ["Caballero", "Jose", ""], ["Cook", "Stuart", ""], ["de Marvao", "Antonio", ""], ["Dawes", "Timothy", ""], ["O'Regan", "Declan", ""], ["Kainz", "Bernhard", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1705.08314", "submitter": "Roberto Henschel", "authors": "Roberto Henschel, Laura Leal-Taix\\'e, Daniel Cremers, Bodo Rosenhahn", "title": "Fusion of Head and Full-Body Detectors for Multi-Object Tracking", "comments": "10 pages, 4 figures; Winner of the MOT17 challenge; CVPRW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to track all persons in a scene, the tracking-by-detection paradigm\nhas proven to be a very effective approach. Yet, relying solely on a single\ndetector is also a major limitation, as useful image information might be\nignored. Consequently, this work demonstrates how to fuse two detectors into a\ntracking system. To obtain the trajectories, we propose to formulate tracking\nas a weighted graph labeling problem, resulting in a binary quadratic program.\nAs such problems are NP-hard, the solution can only be approximated. Based on\nthe Frank-Wolfe algorithm, we present a new solver that is crucial to handle\nsuch difficult problems. Evaluation on pedestrian tracking is provided for\nmultiple scenarios, showing superior results over single detector tracking and\nstandard QP-solvers. Finally, our tracker ranks 2nd on the MOT16 benchmark and\n1st on the new MOT17 benchmark, outperforming over 90 trackers.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:29:53 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 10:02:18 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 19:04:02 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 09:24:49 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Henschel", "Roberto", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1705.08369", "submitter": "Talha Qaiser", "authors": "Talha Qaiser, Abhik Mukherjee, Chaitanya Reddy Pb, Sai Dileep\n  Munugoti, Vamsi Tallam, Tomi Pitk\\\"aaho, Taina Lehtim\\\"aki, Thomas Naughton,\n  Matt Berseth, An\\'ibal Pedraza, Ramakrishnan Mukundan, Matthew Smith, Abhir\n  Bhalerao, Erik Rodner, Marcel Simon, Joachim Denzler, Chao-Hui Huang, Gloria\n  Bueno, David Snead, Ian Ellis, Mohammad Ilyas, Nasir Rajpoot", "title": "Her2 Challenge Contest: A Detailed Assessment of Automated Her2 Scoring\n  Algorithms in Whole Slide Images of Breast Cancer Tissues", "comments": null, "journal-ref": null, "doi": "10.1111/his.13333", "report-no": null, "categories": "cs.CV cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating expression of the Human epidermal growth factor receptor 2 (Her2)\nby visual examination of immunohistochemistry (IHC) on invasive breast cancer\n(BCa) is a key part of the diagnostic assessment of BCa due to its recognised\nimportance as a predictive and prognostic marker in clinical practice. However,\nvisual scoring of Her2 is subjective and consequently prone to inter-observer\nvariability. Given the prognostic and therapeutic implications of Her2 scoring,\na more objective method is required. In this paper, we report on a recent\nautomated Her2 scoring contest, held in conjunction with the annual PathSoc\nmeeting held in Nottingham in June 2016, aimed at systematically comparing and\nadvancing the state-of-the-art Artificial Intelligence (AI) based automated\nmethods for Her2 scoring. The contest dataset comprised of digitised whole\nslide images (WSI) of sections from 86 cases of invasive breast carcinoma\nstained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting\nalgorithms automatically predicted scores of the IHC slides for an unseen\nsubset of the dataset and the predicted scores were compared with the 'ground\ntruth' (a consensus score from at least two experts). We also report on a\nsimple Man vs Machine contest for the scoring of Her2 and show that the\nautomated methods could beat the pathology experts on this contest dataset.\nThis paper presents a benchmark for comparing the performance of automated\nalgorithms for scoring of Her2. It also demonstrates the enormous potential of\nautomated algorithms in assisting the pathologist with objective IHC scoring.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:36:04 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:47:33 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 21:39:53 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Qaiser", "Talha", ""], ["Mukherjee", "Abhik", ""], ["Pb", "Chaitanya Reddy", ""], ["Munugoti", "Sai Dileep", ""], ["Tallam", "Vamsi", ""], ["Pitk\u00e4aho", "Tomi", ""], ["Lehtim\u00e4ki", "Taina", ""], ["Naughton", "Thomas", ""], ["Berseth", "Matt", ""], ["Pedraza", "An\u00edbal", ""], ["Mukundan", "Ramakrishnan", ""], ["Smith", "Matthew", ""], ["Bhalerao", "Abhir", ""], ["Rodner", "Erik", ""], ["Simon", "Marcel", ""], ["Denzler", "Joachim", ""], ["Huang", "Chao-Hui", ""], ["Bueno", "Gloria", ""], ["Snead", "David", ""], ["Ellis", "Ian", ""], ["Ilyas", "Mohammad", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1705.08374", "submitter": "Carlos Becker", "authors": "Carlos Becker, Nicolai H\\\"ani, Elena Rosinskaya, Emmanuel d'Angelo,\n  Christoph Strecha", "title": "Classification of Aerial Photogrammetric 3D Point Clouds", "comments": "ISPRS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful method to extract per-point semantic class labels from\naerialphotogrammetry data. Labeling this kind of data is important for tasks\nsuch as environmental modelling, object classification and scene understanding.\nUnlike previous point cloud classification methods that rely exclusively on\ngeometric features, we show that incorporating color information yields a\nsignificant increase in accuracy in detecting semantic classes. We test our\nclassification method on three real-world photogrammetry datasets that were\ngenerated with Pix4Dmapper Pro, and with varying point densities. We show that\noff-the-shelf machine learning techniques coupled with our new features allow\nus to train highly accurate classifiers that generalize well to unseen data,\nprocessing point clouds containing 10 million points in less than 3 minutes on\na desktop computer.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:44:40 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Becker", "Carlos", ""], ["H\u00e4ni", "Nicolai", ""], ["Rosinskaya", "Elena", ""], ["d'Angelo", "Emmanuel", ""], ["Strecha", "Christoph", ""]]}, {"id": "1705.08386", "submitter": "Karol Kurach", "authors": "Karol Kurach, Sylvain Gelly, Michal Jastrzebski, Philip Haeusser,\n  Olivier Teytaud, Damien Vincent, Olivier Bousquet", "title": "Better Text Understanding Through Image-To-Text Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:06:32 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 08:08:20 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kurach", "Karol", ""], ["Gelly", "Sylvain", ""], ["Jastrzebski", "Michal", ""], ["Haeusser", "Philip", ""], ["Teytaud", "Olivier", ""], ["Vincent", "Damien", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1705.08421", "submitter": "Chunhui Gu", "authors": "Chunhui Gu and Chen Sun and David A. Ross and Carl Vondrick and\n  Caroline Pantofaru and Yeqing Li and Sudheendra Vijayanarasimhan and George\n  Toderici and Susanna Ricco and Rahul Sukthankar and Cordelia Schmid and\n  Jitendra Malik", "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual\n  Actions", "comments": "To appear in CVPR 2018. Check dataset page\n  https://research.google.com/ava/ for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a video dataset of spatio-temporally localized Atomic\nVisual Actions (AVA). The AVA dataset densely annotates 80 atomic visual\nactions in 430 15-minute video clips, where actions are localized in space and\ntime, resulting in 1.58M action labels with multiple labels per person\noccurring frequently. The key characteristics of our dataset are: (1) the\ndefinition of atomic visual actions, rather than composite actions; (2) precise\nspatio-temporal annotations with possibly multiple annotations for each person;\n(3) exhaustive annotation of these atomic actions over 15-minute video clips;\n(4) people temporally linked across consecutive segments; and (5) using movies\nto gather a varied set of action representations. This departs from existing\ndatasets for spatio-temporal action recognition, which typically provide sparse\nannotations for composite actions in short video clips. We will release the\ndataset publicly.\n  AVA, with its realistic scene and action complexity, exposes the intrinsic\ndifficulty of action recognition. To benchmark this, we present a novel\napproach for action localization that builds upon the current state-of-the-art\nmethods, and demonstrates better performance on JHMDB and UCF101-24 categories.\nWhile setting a new state of the art on existing datasets, the overall results\non AVA are low at 15.6% mAP, underscoring the need for developing new\napproaches for video understanding.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:11:46 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 08:09:23 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 07:58:40 GMT"}, {"version": "v4", "created": "Mon, 30 Apr 2018 17:45:11 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Gu", "Chunhui", ""], ["Sun", "Chen", ""], ["Ross", "David A.", ""], ["Vondrick", "Carl", ""], ["Pantofaru", "Caroline", ""], ["Li", "Yeqing", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Toderici", "George", ""], ["Ricco", "Susanna", ""], ["Sukthankar", "Rahul", ""], ["Schmid", "Cordelia", ""], ["Malik", "Jitendra", ""]]}, {"id": "1705.08475", "submitter": "Matthias Hein", "authors": "Matthias Hein, Maksym Andriushchenko", "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial\n  Manipulation", "comments": "final version accepted at NIPS 2017, fixed bug in implementation of\n  Cross-Lipschitz regularization and lower bound computation, now results are\n  better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that state-of-the-art classifiers are quite brittle, in\nthe sense that a small adversarial change of an originally with high confidence\ncorrectly classified input leads to a wrong classification again with high\nconfidence. This raises concerns that such classifiers are vulnerable to\nattacks and calls into question their usage in safety-critical systems. We show\nin this paper for the first time formal guarantees on the robustness of a\nclassifier by giving instance-specific lower bounds on the norm of the input\nmanipulation required to change the classifier decision. Based on this analysis\nwe propose the Cross-Lipschitz regularization functional. We show that using\nthis form of regularization in kernel methods resp. neural networks improves\nthe robustness of the classifier without any loss in prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:48:20 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 20:58:09 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hein", "Matthias", ""], ["Andriushchenko", "Maksym", ""]]}, {"id": "1705.08479", "submitter": "Ahmed Ibrahim", "authors": "Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein", "title": "Input Fast-Forwarding for Better Deep Learning", "comments": "Accepted in the 14th International Conference on Image Analysis and\n  Recognition (ICIAR) 2017, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:57:08 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Ibrahim", "Ahmed", ""], ["Abbott", "A. Lynn", ""], ["Hussein", "Mohamed E.", ""]]}, {"id": "1705.08550", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, and Xiaohui Xie", "title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification", "comments": "MICCAI 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods rely on regions of interest (ROIs) which\nrequire great efforts to annotate. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\n(MIL) for labeling a set of instances/patches, we propose end-to-end trained\ndeep multi-instance networks for mass classification based on whole mammogram\nwithout the aforementioned ROIs. We explore three different schemes to\nconstruct deep multi-instance networks for whole mammogram classification.\nExperimental results on the INbreast dataset demonstrate the robustness of\nproposed networks compared to previous work using segmentation and detection\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:16:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhu", "Wentao", ""], ["Lou", "Qi", ""], ["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1705.08562", "submitter": "Kun He", "authors": "Kun He, Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff", "title": "Hashing as Tie-Aware Learning to Rank", "comments": "15 pages, 3 figures. IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:42:46 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 02:07:11 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 04:42:56 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 20:37:00 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["He", "Kun", ""], ["Cakir", "Fatih", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1705.08583", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Suvrit Sra, Richard Hartley", "title": "Sequence Summarization Using Order-constrained Kernelized Feature\n  Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations that can compactly and effectively capture temporal evolution\nof semantic content are important to machine learning algorithms that operate\non multi-variate time-series data. We investigate such representations\nmotivated by the task of human action recognition. Here each data instance is\nencoded by a multivariate feature (such as via a deep CNN) where action\ndynamics are characterized by their variations in time. As these features are\noften non-linear, we propose a novel pooling method, kernelized rank pooling,\nthat represents a given sequence compactly as the pre-image of the parameters\nof a hyperplane in an RKHS, projections of data onto which captures their\ntemporal order. We develop this idea further and show that such a pooling\nscheme can be cast as an order-constrained kernelized PCA objective; we then\npropose to use the parameters of a kernelized low-rank feature subspace as the\nrepresentation of the sequences. We cast our formulation as an optimization\nproblem on generalized Grassmann manifolds and then solve it efficiently using\nRiemannian optimization techniques. We present experiments on several action\nrecognition datasets using diverse feature modalities and demonstrate\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 02:11:04 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Cherian", "Anoop", ""], ["Sra", "Suvrit", ""], ["Hartley", "Richard", ""]]}, {"id": "1705.08590", "submitter": "Yida Wang", "authors": "Yida Wang and Weihong Deng", "title": "Generative Model with Coordinate Metric Learning for Object Recognition\n  Based on 3D Models", "comments": "14 pages", "journal-ref": "IEEE Transactions on Image Processing 2018", "doi": "10.1109/TIP.2018.2858553", "report-no": "Volume: 27 , Issue: 12", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given large amount of real photos for training, Convolutional neural network\nshows excellent performance on object recognition tasks. However, the process\nof collecting data is so tedious and the background are also limited which\nmakes it hard to establish a perfect database. In this paper, our generative\nmodel trained with synthetic images rendered from 3D models reduces the\nworkload of data collection and limitation of conditions. Our structure is\ncomposed of two sub-networks: semantic foreground object reconstruction network\nbased on Bayesian inference and classification network based on multi-triplet\ncost function for avoiding over-fitting problem on monotone surface and fully\nutilizing pose information by establishing sphere-like distribution of\ndescriptors in each category which is helpful for recognition on regular photos\naccording to poses, lighting condition, background and category information of\nrendered images. Firstly, our conjugate structure called generative model with\nmetric learning utilizing additional foreground object channels generated from\nBayesian rendering as the joint of two sub-networks. Multi-triplet cost\nfunction based on poses for object recognition are used for metric learning\nwhich makes it possible training a category classifier purely based on\nsynthetic data. Secondly, we design a coordinate training strategy with the\nhelp of adaptive noises acting as corruption on input images to help both\nsub-networks benefit from each other and avoid inharmonious parameter tuning\ndue to different convergence speed of two sub-networks. Our structure achieves\nthe state of the art accuracy of over 50\\% on ShapeNet database with data\nmigration obstacle from synthetic images to real photos. This pipeline makes it\napplicable to do recognition on real images only based on 3D models.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 03:22:18 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 13:14:41 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Wang", "Yida", ""], ["Deng", "Weihong", ""]]}, {"id": "1705.08593", "submitter": "Davit Buniatyan", "authors": "Davit Buniatyan, Thomas Macrina, Dodam Ih, Jonathan Zung, H. Sebastian\n  Seung", "title": "Deep Learning Improves Template Matching by Normalized Cross Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template matching by normalized cross correlation (NCC) is widely used for\nfinding image correspondences. We improve the robustness of this algorithm by\npreprocessing images with \"siamese\" convolutional networks trained to maximize\nthe contrast between NCC values of true and false matches. The improvement is\nquantified using patches of brain images from serial section electron\nmicroscopy. Relative to a parameter-tuned bandpass filter, siamese\nconvolutional networks significantly reduce false matches. Furthermore, all\nfalse matches can be eliminated by removing a tiny fraction of all matches\nbased on NCC values. The improved accuracy of our method could be essential for\nconnectomics, because emerging petascale datasets may require billions of\ntemplate matches to assemble 2D images of serial sections into a 3D image\nstack. Our method is also expected to generalize to many other computer vision\napplications that use NCC template matching to find image correspondences.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 03:24:25 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Buniatyan", "Davit", ""], ["Macrina", "Thomas", ""], ["Ih", "Dodam", ""], ["Zung", "Jonathan", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1705.08620", "submitter": "Lingkun Luo", "authors": "Lingkun Luo, Xiaofang Wang, Shiqiang Hu and Liming Chen", "title": "Robust Data Geometric Structure Aligned Close yet Discriminative Domain\n  Adaptation", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is transfer learning which aims to leverage labeled\ndata in a related source domain to achieve informed knowledge transfer and help\nthe classification of unlabeled data in a target domain. In this paper, we\npropose a novel DA method, namely Robust Data Geometric Structure Aligned,\nClose yet Discriminative Domain Adaptation (RSA-CDDA), which brings closer, in\na latent joint subspace, both source and target data distributions, and aligns\ninherent hidden source and target data geometric structures while performing\ndiscriminative DA in repulsing both interclass source and target data. The\nproposed method performs domain adaptation between source and target in solving\na unified model, which incorporates data distribution constraints, in\nparticular via a nonparametric distance, i.e., Maximum Mean Discrepancy (MMD),\nas well as constraints on inherent hidden data geometric structure segmentation\nand alignment between source and target, through low rank and sparse\nrepresentation. RSA-CDDA achieves the search of a joint subspace in solving the\nproposed unified model through iterative optimization, alternating Rayleigh\nquotient algorithm and inexact augmented Lagrange multiplier algorithm.\nExtensive experiments carried out on standard DA benchmarks, i.e., 16\ncross-domain image classification tasks, verify the effectiveness of the\nproposed method, which consistently outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:01:18 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Luo", "Lingkun", ""], ["Wang", "Xiaofang", ""], ["Hu", "Shiqiang", ""], ["Chen", "Liming", ""]]}, {"id": "1705.08623", "submitter": "Junying Li", "authors": "Junying Li, Zichen Yang, Haifeng Liu, Deng Cai", "title": "Deep Rotation Equivariant Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning equivariant representations has attracted considerable\nresearch attention. Dieleman et al. introduce four operations which can be\ninserted into convolutional neural network to learn deep representations\nequivariant to rotation. However, feature maps should be copied and rotated\nfour times in each layer in their approach, which causes much running time and\nmemory overhead. In order to address this problem, we propose Deep Rotation\nEquivariant Network consisting of cycle layers, isotonic layers and decycle\nlayers. Our proposed layers apply rotation transformation on filters rather\nthan feature maps, achieving a speed up of more than 2 times with even less\nmemory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and\ndemonstrate that it can improve the performance of state-of-the-art\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:22:09 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 07:13:48 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Li", "Junying", ""], ["Yang", "Zichen", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "1705.08624", "submitter": "Yassine Maalej", "authors": "Yassine Maalej, Sameh Sorour, Ahmed Abdel-Rahim and Mohsen Guizani", "title": "VANETs Meet Autonomous Vehicles: A Multimodal 3D Environment Learning\n  Approach", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a multimodal framework for object detection,\nrecognition and mapping based on the fusion of stereo camera frames, point\ncloud Velodyne Lidar scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages\n(BSMs) exchanged using Dedicated Short Range Communication (DSRC). We merge the\nkey features of rich texture descriptions of objects from 2D images, depth and\ndistance between objects provided by 3D point cloud and awareness of hidden\nvehicles from BSMs' 3D information. We present a joint pixel to point cloud and\npixel to V2V correspondences of objects in frames from the Kitti Vision\nBenchmark Suite by using a semi-supervised manifold alignment approach to\nachieve camera-Lidar and camera-V2V mapping of their recognized objects that\nhave the same underlying manifold.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:24:21 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Maalej", "Yassine", ""], ["Sorour", "Sameh", ""], ["Abdel-Rahim", "Ahmed", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1705.08631", "submitter": "Lluis Gomez", "authors": "Lluis Gomez, Yash Patel, Mar\\c{c}al Rusi\\~nol, Dimosthenis Karatzas,\n  C.V. Jawahar", "title": "Self-supervised learning of visual features through embedding images\n  into text topic spaces", "comments": "Accepted CVPR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end training from scratch of current deep architectures for new\ncomputer vision problems would require Imagenet-scale datasets, and this is not\nalways possible. In this paper we present a method that is able to take\nadvantage of freely available multi-modal content to train computer vision\nalgorithms without human supervision. We put forward the idea of performing\nself-supervised learning of visual features by mining a large scale corpus of\nmulti-modal (text and image) documents. We show that discriminative visual\nfeatures can be learnt efficiently by training a CNN to predict the semantic\ncontext in which a particular image is more probable to appear as an\nillustration. For this we leverage the hidden semantic structures discovered in\nthe text corpus with a well-known topic modeling technique. Our experiments\ndemonstrate state of the art performance in image classification, object\ndetection, and multi-modal retrieval compared to recent self-supervised or\nnatural-supervised approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:59:30 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Gomez", "Lluis", ""], ["Patel", "Yash", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1705.08690", "submitter": "Hanul Shin", "authors": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "title": "Continual Learning with Deep Generative Replay", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to train a comprehensive artificial intelligence capable of solving\nmultiple tasks have been impeded by a chronic problem called catastrophic\nforgetting. Although simply replaying all previous data alleviates the problem,\nit requires large memory and even worse, often infeasible in real world\napplications where the access to past data is limited. Inspired by the\ngenerative nature of hippocampus as a short-term memory system in primate\nbrain, we propose the Deep Generative Replay, a novel framework with a\ncooperative dual model architecture consisting of a deep generative model\n(\"generator\") and a task solving model (\"solver\"). With only these two models,\ntraining data for previous tasks can easily be sampled and interleaved with\nthose for a new task. We test our methods in several sequential learning\nsettings involving image classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:37:38 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 15:31:38 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 02:14:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Shin", "Hanul", ""], ["Lee", "Jung Kwon", ""], ["Kim", "Jaehong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1705.08695", "submitter": "Zenglin Xu", "authors": "Hao Liu and Haoli Bai and Lirong He and Zenglin Xu", "title": "Stochastic Sequential Neural Networks with Structured Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised structure learning in high-dimensional time series data has\nattracted a lot of research interests. For example, segmenting and labelling\nhigh dimensional time series can be helpful in behavior understanding and\nmedical diagnosis. Recent advances in generative sequential modeling have\nsuggested to combine recurrent neural networks with state space models (e.g.,\nHidden Markov Models). This combination can model not only the long term\ndependency in sequential data, but also the uncertainty included in the hidden\nstates. Inheriting these advantages of stochastic neural sequential models, we\npropose a structured and stochastic sequential neural network, which models\nboth the long-term dependencies via recurrent neural networks and the\nuncertainty in the segmentation and labels via discrete random variables. For\naccurate and efficient inference, we present a bi-directional inference network\nby reparamterizing the categorical segmentation and labels with the recent\nproposed Gumbel-Softmax approximation and resort to the Stochastic Gradient\nVariational Bayes. We evaluate the proposed model in a number of tasks,\nincluding speech modeling, automatic segmentation and labeling in behavior\nunderstanding, and sequential multi-objects recognition. Experimental results\nhave demonstrated that our proposed model can achieve significant improvement\nover the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:52:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Liu", "Hao", ""], ["Bai", "Haoli", ""], ["He", "Lirong", ""], ["Xu", "Zenglin", ""]]}, {"id": "1705.08759", "submitter": "Stefan Lee", "authors": "Qing Sun, Stefan Lee, Dhruv Batra", "title": "Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence\n  Models for Fill-in-the-Blank Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the first approximate inference algorithm for 1-Best (and M-Best)\ndecoding in bidirectional neural sequence models by extending Beam Search (BS)\nto reason about both forward and backward time dependencies. Beam Search (BS)\nis a widely used approximate inference algorithm for decoding sequences from\nunidirectional neural sequence models. Interestingly, approximate inference in\nbidirectional models remains an open problem, despite their significant\nadvantage in modeling information from both the past and future. To enable the\nuse of bidirectional models, we present Bidirectional Beam Search (BiBS), an\nefficient algorithm for approximate bidirectional inference.To evaluate our\nmethod and as an interesting problem in its own right, we introduce a novel\nFill-in-the-Blank Image Captioning task which requires reasoning about both\npast and future sentence structure to reconstruct sensible image descriptions.\nWe use this task as well as the Visual Madlibs dataset to demonstrate the\neffectiveness of our approach, consistently outperforming all baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:42:47 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Sun", "Qing", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""]]}, {"id": "1705.08764", "submitter": "Minju Jung", "authors": "Minju Jung, Haanvid Lee, Jun Tani", "title": "Adaptive Detrending to Accelerate Convolutional Gated Recurrent Unit\n  Training for Contextual Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the progress of image recognition, video recognition has been\nextensively studied recently. However, most of the existing methods are focused\non short-term but not long-term video recognition, called contextual video\nrecognition. To address contextual video recognition, we use convolutional\nrecurrent neural networks (ConvRNNs) having a rich spatio-temporal information\nprocessing capability, but ConvRNNs requires extensive computation that slows\ndown training. In this paper, inspired by the normalization and detrending\nmethods, we propose adaptive detrending (AD) for temporal normalization in\norder to accelerate the training of ConvRNNs, especially for convolutional\ngated recurrent unit (ConvGRU). AD removes internal covariate shift within a\nsequence of each neuron in recurrent neural networks (RNNs) by subtracting a\ntrend. In the experiments for contextual recognition on ConvGRU, the results\nshow that (1) ConvGRU clearly outperforms the feed-forward neural networks, (2)\nAD consistently offers a significant training acceleration and generalization\nimprovement, and (3) AD is further improved by collaborating with the existing\nnormalization methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:52:23 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Jung", "Minju", ""], ["Lee", "Haanvid", ""], ["Tani", "Jun", ""]]}, {"id": "1705.08790", "submitter": "Maxim Berman", "authors": "Maxim Berman, Amal Rannen Triki and Matthew B. Blaschko", "title": "The Lov\\'asz-Softmax loss: A tractable surrogate for the optimization of\n  the intersection-over-union measure in neural networks", "comments": "Accepted as a conference paper at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jaccard index, also referred to as the intersection-over-union score, is\ncommonly employed in the evaluation of image segmentation results given its\nperceptual qualities, scale invariance - which lends appropriate relevance to\nsmall objects, and appropriate counting of false negatives, in comparison to\nper-pixel losses. We present a method for direct optimization of the mean\nintersection-over-union loss in neural networks, in the context of semantic\nimage segmentation, based on the convex Lov\\'asz extension of submodular\nlosses. The loss is shown to perform better with respect to the Jaccard index\nmeasure than the traditionally used cross-entropy loss. We show quantitative\nand qualitative differences between optimizing the Jaccard index per image\nversus optimizing the Jaccard index taken over an entire dataset. We evaluate\nthe impact of our method in a semantic segmentation pipeline and show\nsubstantially improved intersection-over-union segmentation scores on the\nPascal VOC and Cityscapes datasets using state-of-the-art deep learning\nsegmentation architectures.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:33:41 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 09:25:36 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Berman", "Maxim", ""], ["Triki", "Amal Rannen", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1705.08824", "submitter": "Paolo Russo", "authors": "Paolo Russo, Fabio Maria Carlucci, Tatiana Tommasi, Barbara Caputo", "title": "From source to target and back: symmetric bi-directional adaptive GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of generative adversarial approaches in producing images\naccording to a specific style or visual domain has recently opened new\ndirections to solve the unsupervised domain adaptation problem. It has been\nshown that source labeled images can be modified to mimic target samples making\nit possible to train directly a classifier in the target domain, despite the\noriginal lack of annotated data. Inverse mappings from the target to the source\ndomain have also been evaluated but only passing through adapted feature\nspaces, thus without new image generation. In this paper we propose to better\nexploit the potential of generative adversarial networks for adaptation by\nintroducing a novel symmetric mapping among domains. We jointly optimize\nbi-directional image transformations combining them with target self-labeling.\nMoreover we define a new class consistency loss that aligns the generators in\nthe two directions imposing to conserve the class identity of an image passing\nthrough both domain mappings. A detailed qualitative and quantitative analysis\nof the reconstructed images confirm the power of our approach. By integrating\nthe two domain specific classifiers obtained with our bi-directional network we\nexceed previous state-of-the-art unsupervised adaptation results on four\ndifferent benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:43:20 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 10:14:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Russo", "Paolo", ""], ["Carlucci", "Fabio Maria", ""], ["Tommasi", "Tatiana", ""], ["Caputo", "Barbara", ""]]}, {"id": "1705.08844", "submitter": "Rodrigo Toro Icarte", "authors": "Rodrigo Toro Icarte, Jorge A. Baier, Cristian Ruz, Alvaro Soto", "title": "How a General-Purpose Commonsense Ontology can Improve Performance of\n  Learning-Based Image Retrieval", "comments": "Accepted in IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge representation community has built general-purpose ontologies\nwhich contain large amounts of commonsense knowledge over relevant aspects of\nthe world, including useful visual information, e.g.: \"a ball is used by a\nfootball player\", \"a tennis player is located at a tennis court\". Current\nstate-of-the-art approaches for visual recognition do not exploit these\nrule-based knowledge sources. Instead, they learn recognition models directly\nfrom training examples. In this paper, we study how general-purpose\nontologies---specifically, MIT's ConceptNet ontology---can improve the\nperformance of state-of-the-art vision systems. As a testbed, we tackle the\nproblem of sentence-based image retrieval. Our retrieval approach incorporates\nknowledge from ConceptNet on top of a large pool of object detectors derived\nfrom a deep learning technique. In our experiments, we show that ConceptNet can\nimprove performance on a common benchmark dataset. Key to our performance is\nthe use of the ESPGAME dataset to select visually relevant relations from\nConceptNet. Consequently, a main conclusion of this work is that\ngeneral-purpose commonsense ontologies improve performance on visual reasoning\ntasks when properly filtered to select meaningful visual relations.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:22:53 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Icarte", "Rodrigo Toro", ""], ["Baier", "Jorge A.", ""], ["Ruz", "Cristian", ""], ["Soto", "Alvaro", ""]]}, {"id": "1705.08850", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved\n  Inference", "comments": "NIPS 2017 accepted version, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods using Generative Adversarial Networks (GANs)\nhave shown promising empirical success recently. Most of these methods use a\nshared discriminator/classifier which discriminates real examples from fake\nwhile also predicting the class label. Motivated by the ability of the GANs\ngenerator to capture the data manifold well, we propose to estimate the tangent\nspace to the data manifold using GANs and employ it to inject invariances into\nthe classifier. In the process, we propose enhancements over existing methods\nfor learning the inverse mapping (i.e., the encoder) which greatly improves in\nterms of semantic similarity of the reconstructed sample with the input sample.\nWe observe considerable empirical gains in semi-supervised learning over\nbaselines, particularly in the cases when the number of labeled examples is\nlow. We also provide insights into how fake examples influence the\nsemi-supervised learning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:35:37 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 18:34:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1705.08881", "submitter": "Jun Li", "authors": "Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji", "title": "Dense Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of current deep learning methods for dense prediction is to\napply a model on a regular patch centered on each pixel to make pixel-wise\npredictions. These methods are limited in the sense that the patches are\ndetermined by network architecture instead of learned from data. In this work,\nwe propose the dense transformer networks, which can learn the shapes and sizes\nof patches from data. The dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted into each of\nthe encoder and decoder paths. The novelty of this work is that we provide\ntechnical solutions for learning the shapes and sizes of patches from data and\nefficiently restoring the spatial correspondence required for dense prediction.\nThe proposed dense transformer modules are differentiable, thus the entire\nnetwork can be trained. We apply the proposed networks on natural and\nbiological image segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:50:32 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:10:04 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Jun", ""], ["Chen", "Yongjun", ""], ["Cai", "Lei", ""], ["Davidson", "Ian", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.08918", "submitter": "Liang Zhao", "authors": "Liang Zhao, Yang Wang, Yi Yang, Wei Xu", "title": "Unsupervised Learning Layers for Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two unsupervised learning layers (UL layers) for\nlabel-free video analysis: one for fully connected layers, and the other for\nconvolutional ones. The proposed UL layers can play two roles: they can be the\ncost function layer for providing global training signal; meanwhile they can be\nadded to any regular neural network layers for providing local training signals\nand combined with the training signals backpropagated from upper layers for\nextracting both slow and fast changing features at layers of different depths.\nTherefore, the UL layers can be used in either pure unsupervised or\nsemi-supervised settings. Both a closed-form solution and an online learning\nalgorithm for two UL layers are provided. Experiments with unlabeled synthetic\nand real-world videos demonstrated that the neural networks equipped with UL\nlayers and trained with the proposed online learning algorithm can extract\nshape and motion information from video sequences of moving objects. The\nexperiments demonstrated the potential applications of UL layers and online\nlearning algorithm to head orientation estimation and moving object\nlocalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:22:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Zhao", "Liang", ""], ["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Xu", "Wei", ""]]}, {"id": "1705.08923", "submitter": "Tao Zhou", "authors": "Tao Zhou, Muhao Chen, Jie Yu, Demetri Terzopoulos", "title": "Attention-based Natural Language Person Retrieval", "comments": "CVPR 2017 Workshop (vision meets cognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent progress in image classification and captioning using\ndeep learning, we develop a novel natural language person retrieval system\nbased on an attention mechanism. More specifically, given the description of a\nperson, the goal is to localize the person in an image. To this end, we first\nconstruct a benchmark dataset for natural language person retrieval. To do so,\nwe generate bounding boxes for persons in a public image dataset from the\nsegmentation masks, which are then annotated with descriptions and attributes\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\nFaster R-CNN as a candidate region generator. The cropped images based on the\nregion proposals as well as the whole images with attention weights are fed\ninto Convolutional Neural Networks for visual feature extraction, while the\nnatural language expression and attributes are input to Bidirectional Long\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\ntext features are integrated to score region proposals, and the one with the\nhighest score is retrieved as the output of our system. The experimental\nresults show significant improvement over the state-of-the-art method for\ngeneric object retrieval and this line of research promises to benefit search\nin surveillance video footage.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:36:58 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Zhou", "Tao", ""], ["Chen", "Muhao", ""], ["Yu", "Jie", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1705.08940", "submitter": "Quentin Bateux", "authors": "Quentin Bateux, Eric Marchand, J\\\"urgen Leitner, Francois Chaumette,\n  Peter Corke", "title": "Visual Servoing from Deep Neural Networks", "comments": "fixed authors list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network-based method to perform high-precision,\nrobust and real-time 6 DOF visual servoing. The paper describes how to create a\ndataset simulating various perturbations (occlusions and lighting conditions)\nfrom a single real-world image of the scene. A convolutional neural network is\nfine-tuned using this dataset to estimate the relative pose between two images\nof the same scene. The output of the network is then employed in a visual\nservoing control scheme. The method converges robustly even in difficult\nreal-world settings with strong lighting variations and occlusions.A\npositioning error of less than one millimeter is obtained in experiments with a\n6 DOF robot.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:39:25 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 09:26:34 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Bateux", "Quentin", ""], ["Marchand", "Eric", ""], ["Leitner", "J\u00fcrgen", ""], ["Chaumette", "Francois", ""], ["Corke", "Peter", ""]]}, {"id": "1705.08943", "submitter": "Clement Zotti", "authors": "Clement Zotti, Zhiming Luo, Alain Lalande, Olivier Humbert,\n  Pierre-Marc Jodoin", "title": "GridNet with automatic shape prior registration for automatic MRI\n  cardiac segmentation", "comments": "8 pages, 1 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully automatic MRI cardiac segmentation method\nbased on a novel deep convolutional neural network (CNN) designed for the 2017\nACDC MICCAI challenge. The novelty of our network comes with its embedded shape\nprior and its loss function tailored to the cardiac anatomy. Our model includes\na cardiac centerof-mass regression module which allows for an automatic shape\nprior registration. Also, since our method processes raw MR images without any\nmanual preprocessing and/or image cropping, our CNN learns both high-level\nfeatures (useful to distinguish the heart from other organs with a similar\nshape) and low-level features (useful to get accurate segmentation results).\nThose features are learned with a multi-resolution conv-deconv \"grid\"\narchitecture which can be seen as an extension of the U-Net. Experimental\nresults reveal that our method can segment the left and right ventricles as\nwell as the myocardium from a 3D MRI cardiac volume in 0.4 second with an\naverage Dice coefficient of 0.90 and an average Hausdorff distance of 10.4 mm.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:44:45 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 19:48:38 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zotti", "Clement", ""], ["Luo", "Zhiming", ""], ["Lalande", "Alain", ""], ["Humbert", "Olivier", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "1705.08974", "submitter": "Jungseock Joo", "authors": "Quanzeng You, Dar\\'io Garc\\'ia-Garc\\'ia, Mahohar Paluri, Jiebo Luo,\n  Jungseock Joo", "title": "Cultural Diffusion and Trends in Facebook Photographs", "comments": "10 pages, To appear in ICWSM 2017 (Full Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media is a social vehicle in which people share various moments\nof their lives with their friends, such as playing sports, cooking dinner or\njust taking a selfie for fun, via visual means, that is, photographs. Our study\ntakes a closer look at the popular visual concepts illustrating various\ncultural lifestyles from aggregated, de-identified photographs. We perform\nanalysis both at macroscopic and microscopic levels, to gain novel insights\nabout global and local visual trends as well as the dynamics of interpersonal\ncultural exchange and diffusion among Facebook friends. We processed images by\nautomatically classifying the visual content by a convolutional neural network\n(CNN). Through various statistical tests, we find that socially tied\nindividuals more likely post images showing similar cultural lifestyles. To\nfurther identify the main cause of the observed social correlation, we use the\nShuffle test and the Preference-based Matched Estimation (PME) test to\ndistinguish the effects of influence and homophily. The results indicate that\nthe visual content of each user's photographs are temporally, although not\nnecessarily causally, correlated with the photographs of their friends, which\nmay suggest the effect of influence. Our paper demonstrates that Facebook\nphotographs exhibit diverse cultural lifestyles and preferences and that the\nsocial interaction mediated through the visual channel in social media can be\nan effective mechanism for cultural diffusion.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 21:49:29 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["You", "Quanzeng", ""], ["Garc\u00eda-Garc\u00eda", "Dar\u00edo", ""], ["Paluri", "Mahohar", ""], ["Luo", "Jiebo", ""], ["Joo", "Jungseock", ""]]}, {"id": "1705.08983", "submitter": "Gregery Buzzard", "authors": "Gregery T. Buzzard, Stanley H. Chan, Suhas Sreehari, Charles A. Bouman", "title": "Plug-and-Play Unplugged: Optimization Free Reconstruction using\n  Consensus Equilibrium", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized inversion methods for image reconstruction are used widely due to\ntheir tractability and ability to combine complex physical sensor models with\nuseful regularity criteria. Such methods motivated the recently developed\nPlug-and-Play prior method, which provides a framework to use advanced\ndenoising algorithms as regularizers in inversion. However, the need to\nformulate regularized inversion as the solution to an optimization problem\nlimits the possible regularity conditions and physical sensor models.\n  In this paper, we introduce Consensus Equilibrium (CE), which generalizes\nregularized inversion to include a much wider variety of both forward\ncomponents and prior components without the need for either to be expressed\nwith a cost function. CE is based on the solution of a set of equilibrium\nequations that balance data fit and regularity. In this framework, the problem\nof MAP estimation in regularized inversion is replaced by the problem of\nsolving these equilibrium equations, which can be approached in multiple ways.\n  The key contribution of CE is to provide a novel framework for fusing\nmultiple heterogeneous models of physical sensors or models learned from data.\nWe describe the derivation of the CE equations and prove that the solution of\nthe CE equations generalizes the standard MAP estimate under appropriate\ncircumstances.\n  We also discuss algorithms for solving the CE equations, including ADMM with\na novel form of preconditioning and Newton's method. We give examples to\nillustrate consensus equilibrium and the convergence properties of these\nalgorithms and demonstrate this method on some toy problems and on a denoising\nexample in which we use an array of convolutional neural network denoisers,\nnone of which is tuned to match the noise level in a noisy image but which in\nconsensus can achieve a better result than any of them individually.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:27:00 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:31:40 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 12:36:07 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Buzzard", "Gregery T.", ""], ["Chan", "Stanley H.", ""], ["Sreehari", "Suhas", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1705.09003", "submitter": "Aiden Nibali", "authors": "Aiden Nibali, Zhen He, Stuart Morgan, Daniel Greenwood", "title": "Extraction and Classification of Diving Clips from Continuous Video\n  Footage", "comments": "To appear at CVsports 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in technology, the recording and analysis of video\ndata has become an increasingly common component of athlete training\nprogrammes. Today it is incredibly easy and affordable to set up a fixed camera\nand record athletes in a wide range of sports, such as diving, gymnastics,\ngolf, tennis, etc. However, the manual analysis of the obtained footage is a\ntime-consuming task which involves isolating actions of interest and\ncategorizing them using domain-specific knowledge. In order to automate this\nkind of task, three challenging sub-problems are often encountered: 1)\ntemporally cropping events/actions of interest from continuous video; 2)\ntracking the object of interest; and 3) classifying the events/actions of\ninterest.\n  Most previous work has focused on solving just one of the above sub-problems\nin isolation. In contrast, this paper provides a complete solution to the\noverall action monitoring task in the context of a challenging real-world\nexemplar. Specifically, we address the problem of diving classification. This\nis a challenging problem since the person (diver) of interest typically\noccupies fewer than 1% of the pixels in each frame. The model is required to\nlearn the temporal boundaries of a dive, even though other divers and\nbystanders may be in view. Finally, the model must be sensitive to subtle\nchanges in body pose over a large number of frames to determine the\nclassification code. We provide effective solutions to each of the sub-problems\nwhich combine to provide a highly functional solution to the task as a whole.\nThe techniques proposed can be easily generalized to video footage recorded\nfrom other sports.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 00:08:40 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Nibali", "Aiden", ""], ["He", "Zhen", ""], ["Morgan", "Stuart", ""], ["Greenwood", "Daniel", ""]]}, {"id": "1705.09052", "submitter": "Tong Shen", "authors": "Tong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, Ian Reid", "title": "Weakly Supervised Semantic Segmentation Based on Web Image\n  Co-segmentation", "comments": "BMVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Fully Convolutional Network (FCN) for semantic segmentation\nrequires a large number of masks with pixel level labelling, which involves a\nlarge amount of human labour and time for annotation. In contrast, web images\nand their image-level labels are much easier and cheaper to obtain. In this\nwork, we propose a novel method for weakly supervised semantic segmentation\nwith only image-level labels. The method utilizes the internet to retrieve a\nlarge number of images and uses a large scale co-segmentation framework to\ngenerate masks for the retrieved images. We first retrieve images from search\nengines, e.g. Flickr and Google, using semantic class names as queries, e.g.\nclass names in the dataset PASCAL VOC 2012. We then use high quality masks\nproduced by co-segmentation on the retrieved images as well as the target\ndataset images with image level labels to train segmentation networks. We\nobtain an IoU score of 56.9 on test set of PASCAL VOC 2012, which reaches the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:35:41 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 02:53:44 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 04:09:27 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Shen", "Tong", ""], ["Lin", "Guosheng", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1705.09107", "submitter": "Nader Mahmoud", "authors": "Nader Mahmoud, Alexandre Hostettler, Toby Collins, Luc Soler,\n  Christophe Doignon, J.M.M. Montiel", "title": "SLAM based Quasi Dense Reconstruction For Minimally Invasive Surgery\n  Scenes", "comments": "ICRA 2017 workshop C4 Surgical Robots: Compliant, Continuum,\n  Cognitive, and Collaborative", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering surgical scene structure in laparoscope surgery is crucial step\nfor surgical guidance and augmented reality applications. In this paper, a\nquasi dense reconstruction algorithm of surgical scene is proposed. This is\nbased on a state-of-the-art SLAM system, and is exploiting the initial\nexploration phase that is typically performed by the surgeon at the beginning\nof the surgery. We show how to convert the sparse SLAM map to a quasi dense\nscene reconstruction, using pairs of keyframe images and correlation-based\nfeatureless patch matching. We have validated the approach with a live porcine\nexperiment using Computed Tomography as ground truth, yielding a Root Mean\nSquared Error of 4.9mm.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 09:44:34 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Mahmoud", "Nader", ""], ["Hostettler", "Alexandre", ""], ["Collins", "Toby", ""], ["Soler", "Luc", ""], ["Doignon", "Christophe", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1705.09132", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Saeed Reza Kheradpisheh, Timoth\\'ee Masquelier, Abbas\n  Nowzari-Dalini, Mohammad Ganjtabesh", "title": "First-spike based visual categorization using reward-modulated STDP", "comments": "supplementary materials are added, Caltech face/motorbike\n  demonstration figure is updated, some parts of the main manuscript are moved\n  to the supplementary materials, additional network analysis and performance\n  comparison with deep nets are added", "journal-ref": "Mozafari, Milad, et al. \"First-Spike-Based Visual Categorization\n  Using Reward-Modulated STDP\". IEEE Transactions on Neural Networks and\n  Learning Systems (2018). DOI: https://doi.org/10.1109/TNNLS.2018.2826721", "doi": "10.1109/TNNLS.2018.2826721", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has recently regained popularity, with major\nachievements such as beating the European game of Go champion. Here, for the\nfirst time, we show that RL can be used efficiently to train a spiking neural\nnetwork (SNN) to perform object recognition in natural images without using an\nexternal classifier. We used a feedforward convolutional SNN and a temporal\ncoding scheme where the most strongly activated neurons fire first, while less\nactivated ones fire later, or not at all. In the highest layers, each neuron\nwas assigned to an object category, and it was assumed that the stimulus\ncategory was the category of the first neuron to fire. If this assumption was\ncorrect, the neuron was rewarded, i.e. spike-timing-dependent plasticity (STDP)\nwas applied, which reinforced the neuron's selectivity. Otherwise, anti-STDP\nwas applied, which encouraged the neuron to learn something else. As\ndemonstrated on various image datasets (Caltech, ETH-80, and NORB), this reward\nmodulated STDP (R-STDP) approach extracted particularly discriminative visual\nfeatures, whereas classic unsupervised STDP extracts any feature that\nconsistently repeats. As a result, R-STDP outperformed STDP on these datasets.\nFurthermore, R-STDP is suitable for online learning, and can adapt to drastic\nchanges such as label permutations. Finally, it is worth mentioning that both\nfeature extraction and classification were done with spikes, using at most one\nspike per neuron. Thus the network is hardware friendly and energy efficient.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 11:38:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 11:31:48 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 12:20:52 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Mozafari", "Milad", ""], ["Kheradpisheh", "Saeed Reza", ""], ["Masquelier", "Timoth\u00e9e", ""], ["Nowzari-Dalini", "Abbas", ""], ["Ganjtabesh", "Mohammad", ""]]}, {"id": "1705.09142", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Vishal B. Athreya and R. Venkatesh Babu", "title": "Deep image representations using caption generators", "comments": "ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning exploits large volumes of labeled data to learn powerful\nmodels. When the target dataset is small, it is a common practice to perform\ntransfer learning using pre-trained models to learn new task specific\nrepresentations. However, pre-trained CNNs for image recognition are provided\nwith limited information about the image during training, which is label alone.\nTasks such as scene retrieval suffer from features learned from this weak\nsupervision and require stronger supervision to better understand the contents\nof the image. In this paper, we exploit the features learned from caption\ngenerating models to learn novel task specific image representations. In\nparticular, we consider the state-of-the art captioning system Show and\nTell~\\cite{SnT-pami-2016} and the dense region description model\nDenseCap~\\cite{densecap-cvpr-2016}. We demonstrate that, owing to richer\nsupervision provided during the process of training, the features learned by\nthe captioning system perform better than those of CNNs. Further, we train a\nsiamese network with a modified pair-wise loss to fuse the features learned\nby~\\cite{SnT-pami-2016} and~\\cite{densecap-cvpr-2016} and learn image\nrepresentations suitable for retrieval. Experiments show that the proposed\nfusion exploits the complementary nature of the individual features and yields\nstate-of-the art retrieval results on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 12:13:27 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Athreya", "Vishal B.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1705.09193", "submitter": "Sultan Imangaliyev", "authors": "Sultan Imangaliyev, Monique H. van der Veen, Catherine M. C.\n  Volgenant, Bruno G. Loos, Bart J. F. Keijser, Wim Crielaard, Evgeni Levin", "title": "Classification of Quantitative Light-Induced Fluorescence Images Using\n  Convolutional Neural Network", "comments": "Full version of ICANN 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are an important data source for diagnosis and treatment of oral\ndiseases. The manual classification of images may lead to misdiagnosis or\nmistreatment due to subjective errors. In this paper an image classification\nmodel based on Convolutional Neural Network is applied to Quantitative\nLight-induced Fluorescence images. The deep neural network outperforms other\nstate of the art shallow classification models in predicting labels derived\nfrom three different dental plaque assessment scores. The model directly\nbenefits from multi-channel representation of the images resulting in improved\nperformance when, besides the Red colour channel, additional Green and Blue\ncolour channels are used.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:21:40 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Imangaliyev", "Sultan", ""], ["van der Veen", "Monique H.", ""], ["Volgenant", "Catherine M. C.", ""], ["Loos", "Bruno G.", ""], ["Keijser", "Bart J. F.", ""], ["Crielaard", "Wim", ""], ["Levin", "Evgeni", ""]]}, {"id": "1705.09275", "submitter": "Wenjian Hu", "authors": "Wenjian Hu, Krishna Kumar Singh, Fanyi Xiao, Jinyoung Han, Chen-Nee\n  Chuah, Yong Jae Lee", "title": "Who Will Share My Image? Predicting the Content Diffusion Path in Online\n  Social Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content popularity prediction has been extensively studied due to its\nimportance and interest for both users and hosts of social media sites like\nFacebook, Instagram, Twitter, and Pinterest. However, existing work mainly\nfocuses on modeling popularity using a single metric such as the total number\nof likes or shares. In this work, we propose Diffusion-LSTM, a memory-based\ndeep recurrent network that learns to recursively predict the entire diffusion\npath of an image through a social network. By combining user social features\nand image features, and encoding the diffusion path taken thus far with an\nexplicit memory cell, our model predicts the diffusion path of an image more\naccurately compared to alternate baselines that either encode only image or\nsocial features, or lack memory. By mapping individual users to user\nprototypes, our model can generalize to new users not seen during training.\nFinally, we demonstrate our model's capability of generating diffusion trees,\nand show that the generated trees closely resemble ground-truth trees.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:46:52 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 19:53:05 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 06:00:27 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 18:40:13 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Hu", "Wenjian", ""], ["Singh", "Krishna Kumar", ""], ["Xiao", "Fanyi", ""], ["Han", "Jinyoung", ""], ["Chuah", "Chen-Nee", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1705.09283", "submitter": "Peng Jiao", "authors": "Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu and Guoqi Li", "title": "GXNOR-Net: Training deep neural networks with ternary weights and\n  activations without full-precision memory under a unified discretization\n  framework", "comments": "11 pages, 13 figures", "journal-ref": "Neural Networks(Volume 100,April 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a pressing need to build an architecture that could subsume these\nnetworks under a unified framework that achieves both higher performance and\nless overhead. To this end, two fundamental issues are yet to be addressed. The\nfirst one is how to implement the back propagation when neuronal activations\nare discrete. The second one is how to remove the full-precision hidden weights\nin the training phase to break the bottlenecks of memory/computation\nconsumption. To address the first issue, we present a multi-step neuronal\nactivation discretization method and a derivative approximation technique that\nenable the implementing the back propagation algorithm on discrete DNNs. While\nfor the second issue, we propose a discrete state transition (DST) methodology\nto constrain the weights in a discrete space without saving the hidden weights.\nThrough this way, we build a unified framework that subsumes the binary or\nternary networks as its special cases, and under which a heuristic algorithm is\nprovided at the website https://github.com/AcrossV/Gated-XNOR. More\nparticularly, we find that when both the weights and activations become ternary\nvalues, the DNNs can be reduced to sparse binary networks, termed as gated XNOR\nnetworks (GXNOR-Nets) since only the event of non-zero weight and non-zero\nactivation enables the control gate to start the XNOR logic operations in the\noriginal binary networks. This promises the event-driven hardware design for\nefficient mobile intelligence. We achieve advanced performance compared with\nstate-of-the-art algorithms. Furthermore, the computational sparsity and the\nnumber of states in the discrete space can be flexibly modified to make it\nsuitable for various hardware platforms.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:59:41 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 03:01:18 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 07:43:44 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 05:25:13 GMT"}, {"version": "v5", "created": "Wed, 2 May 2018 17:30:40 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Deng", "Lei", ""], ["Jiao", "Peng", ""], ["Pei", "Jing", ""], ["Wu", "Zhenzhi", ""], ["Li", "Guoqi", ""]]}, {"id": "1705.09307", "submitter": "Wufeng Xue", "authors": "Wufeng Xue, Ali Islam, Mousumi Bhaduri and Shuo Li", "title": "Direct Multitype Cardiac Indices Estimation via Joint Representation and\n  Regression Learning", "comments": "accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2017.2709251", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac indices estimation is of great importance during identification and\ndiagnosis of cardiac disease in clinical routine. However, estimation of\nmultitype cardiac indices with consistently reliable and high accuracy is still\na great challenge due to the high variability of cardiac structures and\ncomplexity of temporal dynamics in cardiac MR sequences. While efforts have\nbeen devoted into cardiac volumes estimation through feature engineering\nfollowed by a independent regression model, these methods suffer from the\nvulnerable feature representation and incompatible regression model. In this\npaper, we propose a semi-automated method for multitype cardiac indices\nestimation. After manual labelling of two landmarks for ROI cropping, an\nintegrated deep neural network Indices-Net is designed to jointly learn the\nrepresentation and regression models. It comprises two tightly-coupled\nnetworks: a deep convolution autoencoder (DCAE) for cardiac image\nrepresentation, and a multiple output convolution neural network (CNN) for\nindices regression. Joint learning of the two networks effectively enhances the\nexpressiveness of image representation with respect to cardiac indices, and the\ncompatibility between image representation and indices regression, thus leading\nto accurate and reliable estimations for all the cardiac indices.\n  When applied with five-fold cross validation on MR images of 145 subjects,\nIndices-Net achieves consistently low estimation error for LV wall thicknesses\n(1.44$\\pm$0.71mm) and areas of cavity and myocardium (204$\\pm$133mm$^2$). It\noutperforms, with significant error reductions, segmentation method (55.1% and\n17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall\nthicknesses and areas, respectively. These advantages endow the proposed method\na great potential in clinical cardiac function assessment.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:01:41 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Xue", "Wufeng", ""], ["Islam", "Ali", ""], ["Bhaduri", "Mousumi", ""], ["Li", "Shuo", ""]]}, {"id": "1705.09314", "submitter": "Benjamin Hepp", "authors": "Benjamin Hepp, Matthias Nie{\\ss}ner, Otmar Hilliges", "title": "Plan3D: Viewpoint and Trajectory Optimization for Aerial Multi-View\n  Stereo Reconstruction", "comments": "31 pages, 12 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method that efficiently computes a set of viewpoints and\ntrajectories for high-quality 3D reconstructions in outdoor environments. Our\ngoal is to automatically explore an unknown area, and obtain a complete 3D scan\nof a region of interest (e.g., a large building). Images from a commodity RGB\ncamera, mounted on an autonomously navigated quadcopter, are fed into a\nmulti-view stereo reconstruction pipeline that produces high-quality results\nbut is computationally expensive. In this setting, the scanning result is\nconstrained by the restricted flight time of quadcopters. To this end, we\nintroduce a novel optimization strategy that respects these constraints by\nmaximizing the information gain from sparsely-sampled view points while\nlimiting the total travel distance of the quadcopter. At the core of our method\nlies a hierarchical volumetric representation that allows the algorithm to\ndistinguish between unknown, free, and occupied space. Furthermore, our\ninformation gain based formulation leverages this representation to handle\nocclusions in an efficient manner. In addition to the surface geometry, we\nutilize the free-space information to avoid obstacles and determine\ncollision-free flight paths. Our tool can be used to specify the region of\ninterest and to plan trajectories. We demonstrate our method by obtaining a\nnumber of compelling 3D reconstructions, and provide a thorough quantitative\nevaluation showing improvement over previous state-of-the-art and regular\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:13:43 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 17:43:04 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Hepp", "Benjamin", ""], ["Nie\u00dfner", "Matthias", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1705.09339", "submitter": "Senthil Yogamani", "authors": "B Ravi Kiran, Arindam Das and Senthil Yogamani", "title": "Rejection-Cascade of Gaussians: Real-time adaptive background\n  subtraction framework", "comments": "Accepted for National Conference on Computer Vision, Pattern\n  Recognition, Image Processing and Graphics (NCVPRIPG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background-Foreground classification is a well-studied problem in computer\nvision. Due to the pixel-wise nature of modeling and processing in the\nalgorithm, it is usually difficult to satisfy real-time constraints. There is a\ntrade-off between the speed (because of model complexity) and accuracy.\nInspired by the rejection cascade of Viola-Jones classifier, we decompose the\nGaussian Mixture Model (GMM) into an adaptive cascade of Gaussians(CoG). We\nachieve a good improvement in speed without compromising the accuracy with\nrespect to the baseline GMM model. We demonstrate a speed-up factor of 4-5x and\n17 percent average improvement in accuracy over Wallflowers surveillance\ndatasets. The CoG is then demonstrated to over the latent space representation\nof images of a convolutional variational autoencoder(VAE). We provide initial\nresults over CDW-2014 dataset, which could speed up background subtraction for\ndeep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 19:50:45 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 16:51:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kiran", "B Ravi", ""], ["Das", "Arindam", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1705.09368", "submitter": "Liqian Ma", "authors": "Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc\n  Van Gool", "title": "Pose Guided Person Image Generation", "comments": "Xu Jia and Qianru Sun contribute equally. Accepted in Proceedings of\n  31st Conference on Neural Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes the novel Pose Guided Person Generation Network (PG$^2$)\nthat allows to synthesize person images in arbitrary poses, based on an image\nof that person and a novel pose. Our generation framework PG$^2$ utilizes the\npose information explicitly and consists of two key stages: pose integration\nand image refinement. In the first stage the condition image and the target\npose are fed into a U-Net-like network to generate an initial but coarse image\nof the person with the target pose. The second stage then refines the initial\nand blurry result by training a U-Net-like generator in an adversarial way.\nExtensive experimental results on both 128$\\times$64 re-identification images\nand 256$\\times$256 fashion photos show that our model generates high-quality\nperson images with convincing details.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:29:07 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 11:56:51 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 13:41:32 GMT"}, {"version": "v4", "created": "Tue, 5 Sep 2017 07:43:06 GMT"}, {"version": "v5", "created": "Fri, 3 Nov 2017 20:43:15 GMT"}, {"version": "v6", "created": "Sun, 28 Jan 2018 09:25:08 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Ma", "Liqian", ""], ["Jia", "Xu", ""], ["Sun", "Qianru", ""], ["Schiele", "Bernt", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.09369", "submitter": "Vincent Christlein", "authors": "Vincent Christlein, Martin Gropp, Stefan Fiel, Andreas Maier", "title": "Unsupervised Feature Learning for Writer Identification and Writer\n  Retrieval", "comments": "ICDAR2017 camera ready (fixed p@2 values, missing table references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) have shown great success in\nsupervised classification tasks such as character classification or dating.\nDeep learning methods typically need a lot of annotated training data, which is\nnot available in many scenarios. In these cases, traditional methods are often\nbetter than or equivalent to deep learning methods. In this paper, we propose a\nsimple, yet effective, way to learn CNN activation features in an unsupervised\nmanner. Therefore, we train a deep residual network using surrogate classes.\nThe surrogate classes are created by clustering the training dataset, where\neach cluster index represents one surrogate class. The activations from the\npenultimate CNN layer serve as features for subsequent classification tasks. We\nevaluate the feature representations on two publicly available datasets. The\nfocus lies on the ICDAR17 competition dataset on historical document writer\nidentification (Historical-WI). We show that the activation features trained\nwithout supervision are superior to descriptors of state-of-the-art writer\nidentification methods. Additionally, we achieve comparable results in the case\nof handwriting classification using the ICFHR16 competition dataset on\nhistorical Latin script types (CLaMM16).\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:30:40 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 11:26:08 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 09:04:49 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Christlein", "Vincent", ""], ["Gropp", "Martin", ""], ["Fiel", "Stefan", ""], ["Maier", "Andreas", ""]]}, {"id": "1705.09422", "submitter": "Amirsina Torfi", "authors": "Amirsina Torfi, Jeremy Dawson, Nasser M. Nasrabadi", "title": "Text-Independent Speaker Verification Using 3D Convolutional Neural\n  Networks", "comments": "Accepted to be published in IEEE International Conference on\n  Multimedia and Expo (ICME) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)\narchitecture has been proposed for speaker verification in the text-independent\nsetting. One of the main challenges is the creation of the speaker models. Most\nof the previously-reported approaches create speaker models based on averaging\nthe extracted features from utterances of the speaker, which is known as the\nd-vector system. In our paper, we propose an adaptive feature learning by\nutilizing the 3D-CNNs for direct speaker model creation in which, for both\ndevelopment and enrollment phases, an identical number of spoken utterances per\nspeaker is fed to the network for representing the speakers' utterances and\ncreation of the speaker model. This leads to simultaneously capturing the\nspeaker-related information and building a more robust system to cope with\nwithin-speaker variation. We demonstrate that the proposed method significantly\noutperforms the traditional d-vector verification system. Moreover, the\nproposed system can also be an alternative to the traditional d-vector system\nwhich is a one-shot speaker modeling system by utilizing 3D-CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 03:19:08 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 22:36:24 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 17:01:26 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 23:11:13 GMT"}, {"version": "v5", "created": "Sun, 4 Feb 2018 05:38:16 GMT"}, {"version": "v6", "created": "Thu, 29 Mar 2018 02:19:13 GMT"}, {"version": "v7", "created": "Wed, 6 Jun 2018 21:22:54 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Torfi", "Amirsina", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1705.09425", "submitter": "Yao Qin", "authors": "Yao Qin, Mengyang Feng, Huchuan Lu, Garrison W. Cottrell", "title": "Hierarchical Cellular Automata for Visual Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection, finding the most important parts of an image, has become\nincreasingly popular in computer vision. In this paper, we introduce\nHierarchical Cellular Automata (HCA) -- a temporally evolving model to\nintelligently detect salient objects. HCA consists of two main components:\nSingle-layer Cellular Automata (SCA) and Cuboid Cellular Automata (CCA). As an\nunsupervised propagation mechanism, Single-layer Cellular Automata can exploit\nthe intrinsic relevance of similar regions through interactions with neighbors.\nLow-level image features as well as high-level semantic information extracted\nfrom deep neural networks are incorporated into the SCA to measure the\ncorrelation between different image patches. With these hierarchical deep\nfeatures, an impact factor matrix and a coherence matrix are constructed to\nbalance the influences on each cell's next state. The saliency values of all\ncells are iteratively updated according to a well-defined update rule.\nFurthermore, we propose CCA to integrate multiple saliency maps generated by\nSCA at different scales in a Bayesian framework. Therefore, single-layer\npropagation and multi-layer integration are jointly modeled in our unified HCA.\nSurprisingly, we find that the SCA can improve all existing methods that we\napplied it to, resulting in a similar precision level regardless of the\noriginal results. The CCA can act as an efficient pixel-wise aggregation\nalgorithm that can integrate state-of-the-art methods, resulting in even better\nresults. Extensive experiments on four challenging datasets demonstrate that\nthe proposed algorithm outperforms state-of-the-art conventional methods and is\ncompetitive with deep learning based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 03:43:16 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Qin", "Yao", ""], ["Feng", "Mengyang", ""], ["Lu", "Huchuan", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1705.09435", "submitter": "Kingsley Kuan", "authors": "Kingsley Kuan, Mathieu Ravaut, Gaurav Manek, Huiling Chen, Jie Lin,\n  Babar Nazir, Cen Chen, Tse Chiang Howe, Zeng Zeng, Vijay Chandrasekhar", "title": "Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data\n  Science Bowl 2017 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for computer-aided lung cancer\ndiagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans,\ndetermines if each nodule is malignant, and finally assigns a cancer\nprobability based on these results. We discuss the challenges and advantages of\nour framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st\nout of 1972 teams.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:36:29 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Kuan", "Kingsley", ""], ["Ravaut", "Mathieu", ""], ["Manek", "Gaurav", ""], ["Chen", "Huiling", ""], ["Lin", "Jie", ""], ["Nazir", "Babar", ""], ["Chen", "Cen", ""], ["Howe", "Tse Chiang", ""], ["Zeng", "Zeng", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1705.09437", "submitter": "Ruwan Tennakoon", "authors": "Ruwan Tennakoon and Alireza Sadri and Reza Hoseinnezhad and Alireza\n  Bab-Hadiashar", "title": "Effective Sampling: Fast Segmentation Using Robust Geometric Model\n  Fitting", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2834821", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the underlying models in a set of data points contaminated by\nnoise and outliers, leads to a highly complex multi-model fitting problem. This\nproblem can be posed as a clustering problem by the projection of higher order\naffinities between data points into a graph, which can then be clustered using\nspectral clustering. Calculating all possible higher order affinities is\ncomputationally expensive. Hence in most cases only a subset is used. In this\npaper, we propose an effective sampling method to obtain a highly accurate\napproximation of the full graph required to solve multi-structural model\nfitting problems in computer vision. The proposed method is based on the\nobservation that the usefulness of a graph for segmentation improves as the\ndistribution of hypotheses (used to build the graph) approaches the\ndistribution of actual parameters for the given data. In this paper, we\napproximate this actual parameter distribution using a k-th order statistics\nbased cost function and the samples are generated using a greedy algorithm\ncoupled with a data sub-sampling strategy. The experimental analysis shows that\nthe proposed method is both accurate and computationally efficient compared to\nthe state-of-the-art robust multi-model fitting techniques. The code is\npublicly available from https://github.com/RuwanT/model-fitting-cbs.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:39:07 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Tennakoon", "Ruwan", ""], ["Sadri", "Alireza", ""], ["Hoseinnezhad", "Reza", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "1705.09451", "submitter": "Biswa Sengupta", "authors": "Y Qian and P Giaccone and M Sasdelli and E Vasquez and B Sengupta", "title": "Algorithmic clothing: hybrid recommendation, from street-style-to-shop", "comments": "KDD 2017 Workshop on ML meets Fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we detail Cortexica's (https://www.cortexica.com)\nrecommendation framework -- particularly, we describe how a hybrid visual\nrecommender system can be created by combining conditional random fields for\nsegmentation and deep neural networks for object localisation and feature\nrepresentation. The recommendation system that is built after localisation,\nsegmentation and classification has two properties -- first, it is knowledge\nbased in the sense that it learns pairwise preference/occurrence matrix by\nutilising knowledge from experts (images from fashion blogs) and second, it is\ncontent-based as it utilises a deep learning based framework for learning\nfeature representation. Such a construct is especially useful when there is a\nscarcity of user preference data, that forms the foundation of many\ncollaborative recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 06:43:47 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:45:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Qian", "Y", ""], ["Giaccone", "P", ""], ["Sasdelli", "M", ""], ["Vasquez", "E", ""], ["Sengupta", "B", ""]]}, {"id": "1705.09467", "submitter": "Yichao Yan", "authors": "Yichao Yan, Bingbing Ni, Xiaokang Yang", "title": "Predicting Human Interaction via Relative Attention Model", "comments": "To appear in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human interaction is challenging as the on-going activity has to\nbe inferred based on a partially observed video. Essentially, a good algorithm\nshould effectively model the mutual influence between the two interacting\nsubjects. Also, only a small region in the scene is discriminative for\nidentifying the on-going interaction. In this work, we propose a relative\nattention model to explicitly address these difficulties. Built on a\ntri-coupled deep recurrent structure representing both interacting subjects and\nglobal interaction status, the proposed network collects spatio-temporal\ninformation from each subject, rectified with global interaction information,\nyielding effective interaction representation. Moreover, the proposed network\nalso unifies an attention module to assign higher importance to the regions\nwhich are relevant to the on-going action. Extensive experiments have been\nconducted on two public datasets, and the results demonstrate that the proposed\nrelative attention network successfully predicts informative regions between\ninteracting subjects, which in turn yields superior human interaction\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:04:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Yan", "Yichao", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1705.09474", "submitter": "Donghui Wang", "authors": "Yanan Li, Donghui Wang", "title": "Zero-Shot Learning with Generative Latent Prototype Model", "comments": "This work was completed in Oct, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning, which studies the problem of object classification for\ncategories for which we have no training examples, is gaining increasing\nattention from community. Most existing ZSL methods exploit deterministic\ntransfer learning via an in-between semantic embedding space. In this paper, we\ntry to attack this problem from a generative probabilistic modelling\nperspective. We assume for any category, the observed representation, e.g.\nimages or texts, is developed from a unique prototype in a latent space, in\nwhich the semantic relationship among prototypes is encoded via linear\nreconstruction. Taking advantage of this assumption, virtual instances of\nunseen classes can be generated from the corresponding prototype, giving rise\nto a novel ZSL model which can alleviate the domain shift problem existing in\nthe way of direct transfer learning. Extensive experiments on three benchmark\ndatasets show our proposed model can achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:22:13 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Li", "Yanan", ""], ["Wang", "Donghui", ""]]}, {"id": "1705.09476", "submitter": "Donghui Wang", "authors": "Yanan Li, Donghui Wang", "title": "Learning Robust Features with Incremental Auto-Encoders", "comments": "This work was completed in Feb, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically learning features, especially robust features, has attracted\nmuch attention in the machine learning community. In this paper, we propose a\nnew method to learn non-linear robust features by taking advantage of the data\nmanifold structure. We first follow the commonly used trick of the trade, that\nis learning robust features with artificially corrupted data, which are\ntraining samples with manually injected noise. Following the idea of the\nauto-encoder, we first assume features should contain much information to well\nreconstruct the input from its corrupted copies. However, merely reconstructing\nclean input from its noisy copies could make data manifold in the feature space\nnoisy. To address this problem, we propose a new method, called Incremental\nAuto-Encoders, to iteratively denoise the extracted features. We assume the\nnoisy manifold structure is caused by a diffusion process. Consequently, we\nreverse this specific diffusion process to further contract this noisy\nmanifold, which results in an incremental optimization of model parameters .\nFurthermore, we show these learned non-linear features can be stacked into a\nhierarchy of features. Experimental results on real-world datasets demonstrate\nthe proposed method can achieve better classification performances.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:30:41 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Li", "Yanan", ""], ["Wang", "Donghui", ""]]}, {"id": "1705.09479", "submitter": "Ruben Gomez-Ojeda", "authors": "Ruben Gomez-Ojeda, David Zu\\~niga-No\\\"el, Francisco-Angel Moreno,\n  Davide Scaramuzza, and Javier Gonzalez-Jimenez", "title": "PL-SLAM: a Stereo SLAM System through the Combination of Points and Line\n  Segments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to stereo visual SLAM rely on point features to\nestimate the camera trajectory and build a map of the environment. In\nlow-textured environments, though, it is often difficult to find a sufficient\nnumber of reliable point features and, as a consequence, the performance of\nsuch algorithms degrades. This paper proposes PL-SLAM, a stereo visual SLAM\nsystem that combines both points and line segments to work robustly in a wider\nvariety of scenarios, particularly in those where point features are scarce or\nnot well-distributed in the image. PL-SLAM leverages both points and segments\nat all the instances of the process: visual odometry, keyframe selection,\nbundle adjustment, etc. We contribute also with a loop closure procedure\nthrough a novel bag-of-words approach that exploits the combined descriptive\npower of the two kinds of features. Additionally, the resulting map is richer\nand more diverse in 3D elements, which can be exploited to infer valuable,\nhigh-level scene structures like planes, empty spaces, ground plane, etc. (not\naddressed in this work). Our proposal has been tested with several popular\ndatasets (such as KITTI and EuRoC), and is compared to state of the art methods\nlike ORB-SLAM, revealing a more robust performance in most of the experiments,\nwhile still running in real-time. An open source version of the PL-SLAM C++\ncode will be released for the benefit of the community.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:52:38 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 11:28:52 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Gomez-Ojeda", "Ruben", ""], ["Zu\u00f1iga-No\u00ebl", "David", ""], ["Moreno", "Francisco-Angel", ""], ["Scaramuzza", "Davide", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "1705.09529", "submitter": "Guang Yang A", "authors": "Guang Yang, Xiahai Zhuang, Habib Khan, Shouvik Haldar, Eva Nyktari,\n  Lei Li, Rick Wage, Xujiong Ye, Greg Slabaugh, Raad Mohiaddin, Tom Wong,\n  Jennifer Keegan, David Firmin", "title": "Fully Automatic Segmentation and Objective Assessment of Atrial Scars\n  for Longstanding Persistent Atrial Fibrillation Patients Using Late\n  Gadolinium-Enhanced MRI", "comments": "39 pages, 8 figure, 2 tables, submitted to MRM journal", "journal-ref": null, "doi": "10.1002/mp.12832", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Atrial fibrillation (AF) is the most common cardiac arrhythmia and\nis correlated with increased morbidity and mortality. It is associated with\natrial fibrosis, which may be assessed non-invasively using late\ngadolinium-enhanced (LGE) magnetic resonance imaging (MRI) where scar tissue is\nvisualised as a region of signal enhancement. In this study, we proposed a\nnovel fully automatic pipeline to achieve an accurate and objective atrial\nscarring segmentation and assessment of LGE MRI scans for the AF patients.\nMethods: Our fully automatic pipeline uniquely combined: (1) a multi-atlas\nbased whole heart segmentation (MA-WHS) to determine the cardiac anatomy from\nan MRI Roadmap acquisition which is then mapped to LGE MRI, and (2) a\nsuper-pixel and supervised learning based approach to delineate the\ndistribution and extent of atrial scarring in LGE MRI. Results: Both our MA-WHS\nand atrial scarring segmentation showed accurate delineations of cardiac\nanatomy (mean Dice = 89%) and atrial scarring (mean Dice =79%) respectively\ncompared to the established ground truth from manual segmentation. Compared\nwith previously studied methods with manual interventions, our innovative\npipeline demonstrated comparable results, but was computed fully automatically.\nConclusion: The proposed segmentation methods allow LGE MRI to be used as an\nobjective assessment tool for localisation, visualisation and quantification of\natrial scarring.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 11:04:47 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yang", "Guang", ""], ["Zhuang", "Xiahai", ""], ["Khan", "Habib", ""], ["Haldar", "Shouvik", ""], ["Nyktari", "Eva", ""], ["Li", "Lei", ""], ["Wage", "Rick", ""], ["Ye", "Xujiong", ""], ["Slabaugh", "Greg", ""], ["Mohiaddin", "Raad", ""], ["Wong", "Tom", ""], ["Keegan", "Jennifer", ""], ["Firmin", "David", ""]]}, {"id": "1705.09549", "submitter": "Daiki Ikami", "authors": "Daiki Ikami, Toshihiko Yamasaki and Kiyoharu Aizawa", "title": "Residual Expansion Algorithm: Fast and Effective Optimization for\n  Nonconvex Least Squares Problems", "comments": "Accepted to CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the residual expansion (RE) algorithm: a global (or near-global)\noptimization method for nonconvex least squares problems. Unlike most existing\nnonconvex optimization techniques, the RE algorithm is not based on either\nstochastic or multi-point searches; therefore, it can achieve fast global\noptimization. Moreover, the RE algorithm is easy to implement and successful in\nhigh-dimensional optimization. The RE algorithm exhibits excellent empirical\nperformance in terms of k-means clustering, point-set registration, optimized\nproduct quantization, and blind image deblurring.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:08:50 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ikami", "Daiki", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1705.09552", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,\n  Stefano Soatto", "title": "Classification regions of deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:38:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09554", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard, Stefano Soatto", "title": "Robustness of classifiers to universal perturbations: a geometric\n  perspective", "comments": "Published at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:42:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 20:45:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09558", "submitter": "Andrew Wilson", "authors": "Yunus Saatchi, Andrew Gordon Wilson", "title": "Bayesian GAN", "comments": "Updated to the version that appears at Advances in Neural Information\n  Processing Systems 30 (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:47:56 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:54:47 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:52:21 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Saatchi", "Yunus", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1705.09587", "submitter": "Jisoo Jeong", "authors": "Jisoo Jeong, Hyojin Park, and Nojun Kwak", "title": "Enhancement of SSD by concatenating feature maps for object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object detection method that improves the accuracy of the\nconventional SSD (Single Shot Multibox Detector), which is one of the top\nobject detection algorithms in both aspects of accuracy and speed. The\nperformance of a deep network is known to be improved as the number of feature\nmaps increases. However, it is difficult to improve the performance by simply\nraising the number of feature maps. In this paper, we propose and analyze how\nto use feature maps effectively to improve the performance of the conventional\nSSD. The enhanced performance was obtained by changing the structure close to\nthe classifier network, rather than growing layers close to the input data,\ne.g., by replacing VGGNet with ResNet. The proposed network is suitable for\nsharing the weights in the classifier networks, by which property, the training\ncan be faster with better generalization power. For the Pascal VOC 2007 test\nset trained with VOC 2007 and VOC 2012 training sets, the proposed network with\nthe input size of 300 x 300 achieved 78.5% mAP (mean average precision) at the\nspeed of 35.0 FPS (frame per second), while the network with a 512 x 512 sized\ninput achieved 80.8% mAP at 16.6 FPS using Nvidia Titan X GPU. The proposed\nnetwork shows state-of-the-art mAP, which is better than those of the\nconventional SSD, YOLO, Faster-RCNN and RFCN. Also, it is faster than\nFaster-RCNN and RFCN.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:07:41 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Jeong", "Jisoo", ""], ["Park", "Hyojin", ""], ["Kwak", "Nojun", ""]]}, {"id": "1705.09597", "submitter": "Russell Bates", "authors": "Russell Bates, Benjamin Irving, Bostjan Markelc, Jakob Kaeppler, Ruth\n  Muschel, Vicente Grau, and Julia A. Schnabel", "title": "Extracting 3D Vascular Structures from Microscopy Images using\n  Convolutional Recurrent Networks", "comments": "The article has been submitted to IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vasculature is known to be of key biological significance, especially in the\nstudy of cancer. As such, considerable effort has been focused on the automated\nmeasurement and analysis of vasculature in medical and pre-clinical images. In\ntumors in particular, the vascular networks may be extremely irregular and the\nappearance of the individual vessels may not conform to classical descriptions\nof vascular appearance. Typically, vessels are extracted by either a\nsegmentation and thinning pipeline, or by direct tracking. Neither of these\nmethods are well suited to microscopy images of tumor vasculature. In order to\naddress this we propose a method to directly extract a medial representation of\nthe vessels using Convolutional Neural Networks. We then show that these\ntwo-dimensional centerlines can be meaningfully extended into 3D in anisotropic\nand complex microscopy images using the recently popularized Convolutional Long\nShort-Term Memory units (ConvLSTM). We demonstrate the effectiveness of this\nhybrid convolutional-recurrent architecture over both 2D and 3D convolutional\ncomparators.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:30:29 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Bates", "Russell", ""], ["Irving", "Benjamin", ""], ["Markelc", "Bostjan", ""], ["Kaeppler", "Jakob", ""], ["Muschel", "Ruth", ""], ["Grau", "Vicente", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1705.09602", "submitter": "Elena Burceanu", "authors": "Elena Burceanu and Marius Leordeanu", "title": "Learning a Robust Society of Tracking Parts", "comments": "9.5 pages of main content, 2.5 of bibliography, 2 pages of appendix,\n  3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is an essential task in computer vision that has been studied\nsince the early days of the field. Being able to follow objects that undergo\ndifferent transformations in the video sequence, including changes in scale,\nillumination, shape and occlusions, makes the problem extremely difficult. One\nof the real challenges is to keep track of the changes in objects appearance\nand not drift towards the background clutter. Different from previous\napproaches, we obtain robustness against background with a tracker model that\nis composed of many different parts. They are classifiers that respond at\ndifferent scales and locations. The tracker system functions as a society of\nparts, each having its own role and level of credibility. Reliable classifiers\ndecide the tracker's next move, while newcomers are first monitored before\ngaining the necessary level of reliability to participate in the decision\nprocess. Some parts that loose their consistency are rejected, while others\nthat show consistency for a sufficiently long time are promoted to permanent\nroles. The tracker system, as a whole, could also go through different phases,\nfrom the usual, normal functioning to states of weak agreement and even crisis.\nThe tracker system has different governing rules in each state. What truly\ndistinguishes our work from others is not necessarily the strength of\nindividual tracking parts, but the way in which they work together and build a\nstrong and robust organization. We also propose an efficient way to learn\nsimultaneously many tracking parts, with a single closed-form formulation. We\nobtain a fast and robust tracker with state of the art performance on the\nchallenging OTB50 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:51:43 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Burceanu", "Elena", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1705.09606", "submitter": "Meysam Madadi", "authors": "Meysam Madadi, Sergio Escalera, Xavier Baro and Jordi Gonzalez", "title": "End-to-end Global to Local CNN Learning for Hand Pose Recovery in Depth\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in 3D pose estimation of human hands, especially\nthanks to the advent of CNNs and depth cameras, this task is still far from\nbeing solved. This is mainly due to the highly non-linear dynamics of fingers,\nwhich make hand model training a challenging task. In this paper, we exploit a\nnovel hierarchical tree-like structured CNN, in which branches are trained to\nbecome specialized in predefined subsets of hand joints, called local poses. We\nfurther fuse local pose features, extracted from hierarchical CNN branches, to\nlearn higher order dependencies among joints in the final pose by end-to-end\ntraining. Lastly, the loss function used is also defined to incorporate\nappearance and physical constraints about doable hand motion and deformation.\nFinally, we introduce a non-rigid data augmentation approach to increase the\namount of training depth data. Experimental results suggest that feeding a\ntree-shaped CNN, specialized in local poses, into a fusion network for modeling\njoints correlations and dependencies, helps to increase the precision of final\nestimations, outperforming state-of-the-art results on NYU and SyntheticHand\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:55:44 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 23:26:00 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""], ["Baro", "Xavier", ""], ["Gonzalez", "Jordi", ""]]}, {"id": "1705.09728", "submitter": "Wufeng Xue", "authors": "Wufeng Xue, Ilanit Ben Nachum, Sachin Pandey, James Warrington,\n  Stephanie Leung, and Shuo Li", "title": "Direct Estimation of Regional Wall Thicknesses via Residual Recurrent\n  Neural Network", "comments": "To appear as an oral paper in IPMI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of regional wall thicknesses (RWT) of left ventricular\n(LV) myocardium from cardiac MR sequences is of significant importance for\nidentification and diagnosis of cardiac disease. Existing RWT estimation still\nrelies on segmentation of LV myocardium, which requires strong prior\ninformation and user interaction. No work has been devoted into direct\nestimation of RWT from cardiac MR images due to the diverse shapes and\nstructures for various subjects and cardiac diseases, as well as the complex\nregional deformation of LV myocardium during the systole and diastole phases of\nthe cardiac cycle. In this paper, we present a newly proposed Residual\nRecurrent Neural Network (ResRNN) that fully leverages the spatial and temporal\ndynamics of LV myocardium to achieve accurate frame-wise RWT estimation. Our\nResRNN comprises two paths: 1) a feed forward convolution neural network (CNN)\nfor effective and robust CNN embedding learning of various cardiac images and\npreliminary estimation of RWT from each frame itself independently, and 2) a\nrecurrent neural network (RNN) for further improving the estimation by modeling\nspatial and temporal dynamics of LV myocardium. For the RNN path, we design for\ncardiac sequences a Circle-RNN to eliminate the effect of null hidden input for\nthe first time-step. Our ResRNN is capable of obtaining accurate estimation of\ncardiac RWT with Mean Absolute Error of 1.44mm (less than 1-pixel error) when\nvalidated on cardiac MR sequences of 145 subjects, evidencing its great\npotential in clinical cardiac function assessment.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 22:04:46 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Xue", "Wufeng", ""], ["Nachum", "Ilanit Ben", ""], ["Pandey", "Sachin", ""], ["Warrington", "James", ""], ["Leung", "Stephanie", ""], ["Li", "Shuo", ""]]}, {"id": "1705.09759", "submitter": "Chen Feng", "authors": "Zhiding Yu, Chen Feng, Ming-Yu Liu, Srikumar Ramalingam", "title": "CASENet: Deep Category-Aware Semantic Edge Detection", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary and edge cues are highly beneficial in improving a wide variety of\nvision tasks such as semantic segmentation, object recognition, stereo, and\nobject proposal generation. Recently, the problem of edge detection has been\nrevisited and significant progress has been made with deep learning. While\nclassical edge detection is a challenging binary problem in itself, the\ncategory-aware semantic edge detection by nature is an even more challenging\nmulti-label problem. We model the problem such that each edge pixel can be\nassociated with more than one class as they appear in contours or junctions\nbelonging to two or more semantic classes. To this end, we propose a novel\nend-to-end deep semantic edge learning architecture based on ResNet and a new\nskip-layer architecture where category-wise edge activations at the top\nconvolution layer share and are fused with the same set of bottom layer\nfeatures. We then propose a multi-label loss function to supervise the fused\nactivations. We show that our proposed architecture benefits this problem with\nbetter performance, and we outperform the current state-of-the-art semantic\nedge detection methods by a large margin on standard data sets such as SBD and\nCityscapes.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 03:35:36 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yu", "Zhiding", ""], ["Feng", "Chen", ""], ["Liu", "Ming-Yu", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1705.09764", "submitter": "Chang Song", "authors": "Chang Song, Hsin-Pai Cheng, Huanrui Yang, Sicheng Li, Chunpeng Wu,\n  Qing Wu, Hai Li, Yiran Chen", "title": "MAT: A Multi-strength Adversarial Training Method to Mitigate\n  Adversarial Attacks", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent works revealed that deep neural networks (DNNs) are vulnerable to\nso-called adversarial attacks where input examples are intentionally perturbed\nto fool DNNs. In this work, we revisit the DNN training process that includes\nadversarial examples into the training dataset so as to improve DNN's\nresilience to adversarial attacks, namely, adversarial training. Our\nexperiments show that different adversarial strengths, i.e., perturbation\nlevels of adversarial examples, have different working zones to resist the\nattack. Based on the observation, we propose a multi-strength adversarial\ntraining method (MAT) that combines the adversarial training examples with\ndifferent adversarial strengths to defend adversarial attacks. Two training\nstructures - mixed MAT and parallel MAT - are developed to facilitate the\ntradeoffs between training time and memory occupation. Our results show that\nMAT can substantially minimize the accuracy degradation of deep learning\nsystems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 04:29:04 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 18:29:36 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Song", "Chang", ""], ["Cheng", "Hsin-Pai", ""], ["Yang", "Huanrui", ""], ["Li", "Sicheng", ""], ["Wu", "Chunpeng", ""], ["Wu", "Qing", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1705.09765", "submitter": "Yue Wu Dr.", "authors": "Yue Wu, Wael AbdAlmageed, Prem Natarajan", "title": "Deep Matching and Validation Network -- An End-to-End Solution to\n  Constrained Image Splicing Localization and Detection", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image splicing is a very common image manipulation technique that is\nsometimes used for malicious purposes. A splicing detec- tion and localization\nalgorithm usually takes an input image and produces a binary decision\nindicating whether the input image has been manipulated, and also a\nsegmentation mask that corre- sponds to the spliced region. Most existing\nsplicing detection and localization pipelines suffer from two main\nshortcomings: 1) they use handcrafted features that are not robust against\nsubsequent processing (e.g., compression), and 2) each stage of the pipeline is\nusually optimized independently. In this paper we extend the formulation of the\nunderlying splicing problem to consider two input images, a query image and a\npotential donor image. Here the task is to estimate the probability that the\ndonor image has been used to splice the query image, and obtain the splicing\nmasks for both the query and donor images. We introduce a novel deep\nconvolutional neural network architecture, called Deep Matching and Validation\nNetwork (DMVN), which simultaneously localizes and detects image splicing. The\nproposed approach does not depend on handcrafted features and uses raw input\nimages to create deep learned representations. Furthermore, the DMVN is\nend-to-end op- timized to produce the probability estimates and the\nsegmentation masks. Our extensive experiments demonstrate that this approach\noutperforms state-of-the-art splicing detection methods by a large margin in\nterms of both AUC score and speed.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 05:33:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wu", "Yue", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Prem", ""]]}, {"id": "1705.09780", "submitter": "Benjamin Meyer", "authors": "Benjamin J. Meyer, Ben Harwood, Tom Drummond", "title": "Deep Metric Learning and Image Classification with Nearest Neighbour\n  Gaussian Kernels", "comments": "Accepted in the International Conference on Image Processing (ICIP)\n  2018. Formerly titled Nearest Neighbour Radial Basis Function Solvers for\n  Deep Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Gaussian kernel loss function and training algorithm for\nconvolutional neural networks that can be directly applied to both distance\nmetric learning and image classification problems. Our method treats all\ntraining features from a deep neural network as Gaussian kernel centres and\ncomputes loss by summing the influence of a feature's nearby centres in the\nfeature embedding space. Our approach is made scalable by treating it as an\napproximate nearest neighbour search problem. We show how to make end-to-end\nlearning feasible, resulting in a well formed embedding space, in which\nsemantically related instances are likely to be located near one another,\nregardless of whether or not the network was trained on those classes. Our\napproach outperforms state-of-the-art deep metric learning approaches on\nembedding learning challenges, as well as conventional softmax classification\non several datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:34:48 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 06:26:27 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 06:18:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Meyer", "Benjamin J.", ""], ["Harwood", "Ben", ""], ["Drummond", "Tom", ""]]}, {"id": "1705.09785", "submitter": "Ankit Dhall", "authors": "Ankit Dhall, Kunal Chelani, Vishnu Radhakrishnan, K.M. Krishna", "title": "LiDAR-Camera Calibration using 3D-3D Point correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of autonomous vehicles, LiDAR and cameras have become an\nindispensable combination of sensors. They both provide rich and complementary\ndata which can be used by various algorithms and machine learning to sense and\nmake vital inferences about the surroundings. We propose a novel pipeline and\nexperimental setup to find accurate rigid-body transformation for extrinsically\ncalibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences\nin LiDAR and camera frame and gives a closed form solution. We further show the\naccuracy of the estimate by fusing point clouds from two stereo cameras which\nalign perfectly with the rotation and translation estimated by our method,\nconfirming the accuracy of our method's estimates both mathematically and\nvisually. Taking our idea of extrinsic LiDAR-camera calibration forward, we\ndemonstrate how two cameras with no overlapping field-of-view can also be\ncalibrated extrinsically using 3D point correspondences. The code has been made\navailable as open-source software in the form of a ROS package, more\ninformation about which can be sought here:\nhttps://github.com/ankitdhall/lidar_camera_calibration .\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:57:50 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dhall", "Ankit", ""], ["Chelani", "Kunal", ""], ["Radhakrishnan", "Vishnu", ""], ["Krishna", "K. M.", ""]]}, {"id": "1705.09805", "submitter": "Rico Jonschkowski", "authors": "Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin\n  Riedmiller", "title": "PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured\n  State Representations", "comments": "Accepted at Robotics: Science and Systems (RSS 2017) Workshop -- New\n  Frontiers for Deep Learning in Robotics\n  http://juxi.net/workshop/deep-learning-rss-2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose position-velocity encoders (PVEs) which learn---without\nsupervision---to encode images to positions and velocities of task-relevant\nobjects. PVEs encode a single image into a low-dimensional position state and\ncompute the velocity state from finite differences in position. In contrast to\nautoencoders, position-velocity encoders are not trained by image\nreconstruction, but by making the position-velocity representation consistent\nwith priors about interacting with the physical world. We applied PVEs to\nseveral simulated control tasks from pixels and achieved promising preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 11:17:49 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 16:07:17 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 12:15:40 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Jonschkowski", "Rico", ""], ["Hafner", "Roland", ""], ["Scholz", "Jonathan", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1705.09816", "submitter": "Reza Borhani", "authors": "Reza Borhani, Jeremy Watt, Aggelos Katsaggelos", "title": "Global hard thresholding algorithms for joint sparse image\n  representation and denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding of images is traditionally done by cutting them into small\npatches and representing each patch individually over some dictionary given a\npre-determined number of nonzero coefficients to use for each patch. In lack of\na way to effectively distribute a total number (or global budget) of nonzero\ncoefficients across all patches, current sparse recovery algorithms distribute\nthe global budget equally across all patches despite the wide range of\ndifferences in structural complexity among them. In this work we propose a new\nframework for joint sparse representation and recovery of all image patches\nsimultaneously. We also present two novel global hard thresholding algorithms,\nbased on the notion of variable splitting, for solving the joint sparse model.\nExperimentation using both synthetic and real data shows effectiveness of the\nproposed framework for sparse image representation and denoising tasks.\nAdditionally, time complexity analysis of the proposed algorithms indicate high\nscalability of both algorithms, making them favorable to use on large megapixel\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 12:40:24 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Borhani", "Reza", ""], ["Watt", "Jeremy", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1705.09850", "submitter": "Mohammad Tariqul Islam", "authors": "Mohammad Tariqul Islam, Md Abdul Aowal, Ahmed Tahseen Minhaz, Khalid\n  Ashraf", "title": "Abnormality Detection and Localization in Chest X-Rays using Deep\n  Convolutional Neural Networks", "comments": "14 figures, 16 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-Rays (CXRs) are widely used for diagnosing abnormalities in the heart\nand lung area. Automatically detecting these abnormalities with high accuracy\ncould greatly enhance real world diagnosis processes. Lack of standard publicly\navailable dataset and benchmark studies, however, makes it difficult to compare\nvarious detection methods. In order to overcome these difficulties, we have\nused a publicly available Indiana CXR, JSRT and Shenzhen dataset and studied\nthe performance of known deep convolutional network (DCN) architectures on\ndifferent abnormalities. We find that the same DCN architecture doesn't perform\nwell across all abnormalities. Shallow features or earlier layers consistently\nprovide higher detection accuracy compared to deep features. We have also found\nensemble models to improve classification significantly compared to single\nmodel. Combining these insight, we report the highest accuracy on chest X-Ray\nabnormality detection on these datasets. We find that for cardiomegaly\ndetection, the deep learning method improves the accuracy by a staggering 17\npercentage point compared to rule based methods. We applied the techniques to\nthe problem of tuberculosis detection on a different dataset and achieved the\nhighest accuracy. Our localization experiments using these trained classifiers\nshow that for spatially spread out abnormalities like cardiomegaly and\npulmonary edema, the network can localize the abnormalities successfully most\nof the time. One remarkable result of the cardiomegaly localization is that the\nheart and its surrounding region is most responsible for cardiomegaly\ndetection, in contrast to the rule based models where the ratio of heart and\nlung area is used as the measure. We believe that through deep learning based\nclassification and localization, we will discover many more interesting\nfeatures in medical image diagnosis that are not considered traditionally.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 17:59:57 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 19:18:32 GMT"}, {"version": "v3", "created": "Wed, 27 Sep 2017 09:49:09 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Islam", "Mohammad Tariqul", ""], ["Aowal", "Md Abdul", ""], ["Minhaz", "Ahmed Tahseen", ""], ["Ashraf", "Khalid", ""]]}, {"id": "1705.09860", "submitter": "Edgar Sucar", "authors": "Edgar Sucar, Jean-Bernard Hayet", "title": "Probabilistic Global Scale Estimation for MonoSLAM Based on Generic\n  Object Detection", "comments": "Int. Workshop on Visual Odometry, CVPR, (July 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method to estimate the global scale of a 3D\nreconstructed model within a Kalman filtering-based monocular SLAM algorithm.\nOur Bayesian framework integrates height priors over the detected objects\nbelonging to a set of broad predefined classes, based on recent advances in\nfast generic object detection. Each observation is produced on single frames,\nso that we do not need a data association process along video frames. This is\nbecause we associate the height priors with the image region sizes at image\nplaces where map features projections fall within the object detection regions.\nWe present very promising results of this approach obtained on several\nexperiments with different object classes.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:14:31 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Sucar", "Edgar", ""], ["Hayet", "Jean-Bernard", ""]]}, {"id": "1705.09864", "submitter": "Haojin Yang", "authors": "Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel", "title": "BMXNet: An Open-Source Binary Neural Network Implementation Based on\n  MXNet", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) can drastically reduce memory size and accesses\nby applying bit-wise operations instead of standard arithmetic operations.\nTherefore it could significantly improve the efficiency and lower the energy\nconsumption at runtime, which enables the application of state-of-the-art deep\nlearning models on low power devices. BMXNet is an open-source BNN library\nbased on MXNet, which supports both XNOR-Networks and Quantized Neural\nNetworks. The developed BNN layers can be seamlessly applied with other\nstandard library components and work in both GPU and CPU mode. BMXNet is\nmaintained and developed by the multimedia research group at Hasso Plattner\nInstitute and released under Apache license. Extensive experiments validate the\nefficiency and effectiveness of our implementation. The BMXNet library, several\nsample projects, and a collection of pre-trained binary deep models are\navailable for download at https://github.com/hpi-xnor\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:52:10 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Yang", "Haojin", ""], ["Fritzsche", "Martin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1705.09882", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Zicheng Liu, Yinpeng Chen, Stefano Soatto", "title": "Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based\n  Person Re-Identification", "comments": "19 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of person re-identification from commodity depth\nsensors. One challenge for depth-based recognition is data scarcity. Our first\ncontribution addresses this problem by introducing split-rate RGB-to-Depth\ntransfer, which leverages large RGB datasets more effectively than popular\nfine-tuning approaches. Our transfer scheme is based on the observation that\nthe model parameters at the bottom layers of a deep convolutional neural\nnetwork can be directly shared between RGB and depth data while the remaining\nlayers need to be fine-tuned rapidly. Our second contribution enhances\nre-identification for video by implementing temporal attention as a\nBernoulli-Sigmoid unit acting upon frame-level features. Since this unit is\nstochastic, the temporal attention parameters are trained using reinforcement\nlearning. Extensive experiments validate the accuracy of our method in person\nre-identification from depth sequences. Finally, in a scenario where subjects\nwear unseen clothes, we show large performance gains compared to a\nstate-of-the-art model which relies on RGB data.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 01:22:38 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 01:06:54 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Liu", "Zicheng", ""], ["Chen", "Yinpeng", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09887", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, HanZe Dong, Yu-feng Ma, Zhengjun Zhang, Xiangyang Xue", "title": "Vocabulary-informed Extreme Value Learning", "comments": "we significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decided to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel unseen classes can be formulated as the extreme values of known\nclasses. This inspired the recent works on open-set recognition\n\\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no\nway of naming the novel unseen classes. To solve this problem, we propose the\nExtreme Value Learning (EVL) formulation to learn the mapping from visual\nfeature to semantic space. To model the margin and coverage distributions of\neach class, the Vocabulary-informed Learning (ViL) is adopted by using vast\nopen vocabulary in the semantic space. Essentially, by incorporating the EVL\nand ViL, we for the first time propose a novel semantic embedding paradigm --\nVocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual\nfeatures into semantic space in a probabilistic way. The learned embedding can\nbe directly used to solve supervised learning, zero-shot and open set\nrecognition simultaneously. Experiments on two benchmark datasets demonstrate\nthe effectiveness of proposed frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 02:13:06 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 23:27:27 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Dong", "HanZe", ""], ["Ma", "Yu-feng", ""], ["Zhang", "Zhengjun", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1705.09888", "submitter": "Peng Xu", "authors": "Peng Xu, Qiyue Yin, Yongye Huang, Yi-Zhe Song, Zhanyu Ma, Liang Wang,\n  Tao Xiang, W. Bastiaan Kleijn, Jun Guo", "title": "Cross-modal Subspace Learning for Fine-grained Sketch-based Image\n  Retrieval", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based image retrieval (SBIR) is challenging due to the inherent\ndomain-gap between sketch and photo. Compared with pixel-perfect depictions of\nphotos, sketches are iconic renderings of the real world with highly abstract.\nTherefore, matching sketch and photo directly using low-level visual clues are\nunsufficient, since a common low-level subspace that traverses semantically\nacross the two modalities is non-trivial to establish. Most existing SBIR\nstudies do not directly tackle this cross-modal problem. This naturally\nmotivates us to explore the effectiveness of cross-modal retrieval methods in\nSBIR, which have been applied in the image-text matching successfully. In this\npaper, we introduce and compare a series of state-of-the-art cross-modal\nsubspace learning methods and benchmark them on two recently released\nfine-grained SBIR datasets. Through thorough examination of the experimental\nresults, we have demonstrated that the subspace learning can effectively model\nthe sketch-photo domain-gap. In addition we draw a few key insights to drive\nfuture research.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 03:45:26 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Xu", "Peng", ""], ["Yin", "Qiyue", ""], ["Huang", "Yongye", ""], ["Song", "Yi-Zhe", ""], ["Ma", "Zhanyu", ""], ["Wang", "Liang", ""], ["Xiang", "Tao", ""], ["Kleijn", "W. Bastiaan", ""], ["Guo", "Jun", ""]]}, {"id": "1705.09892", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, Anton van den Hengel", "title": "Care about you: towards large-scale human-centric visual relationship\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection aims to capture interactions between pairs of\nobjects in images. Relationships between objects and humans represent a\nparticularly important subset of this problem, with implications for challenges\nsuch as understanding human behaviour, and identifying affordances, amongst\nothers. In addressing this problem we first construct a large-scale\nhuman-centric visual relationship detection dataset (HCVRD), which provides\nmany more types of relationship annotation (nearly 10K categories) than the\nprevious released datasets.\n  This large label space better reflects the reality of human-object\ninteractions, but gives rise to a long-tail distribution problem, which in turn\ndemands a zero-shot approach to labels appearing only in the test set. This is\nthe first time this issue has been addressed. We propose a webly-supervised\napproach to these problems and demonstrate that the proposed model provides a\nstrong baseline on our HCVRD dataset.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 05:53:38 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zhuang", "Bohan", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1705.09894", "submitter": "Brandon Victor", "authors": "Brandon Victor, Zhen He, Stuart Morgan, Dino Miniutti", "title": "Continuous Video to Simple Signals for Swimming Stroke Detection with\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many sports, it is useful to analyse video of an athlete in competition\nfor training purposes. In swimming, stroke rate is a common metric used by\ncoaches; requiring a laborious labelling of each individual stroke. We show\nthat using a Convolutional Neural Network (CNN) we can automatically detect\ndiscrete events in continuous video (in this case, swimming strokes). We create\na CNN that learns a mapping from a window of frames to a point on a smooth 1D\ntarget signal, with peaks denoting the location of a stroke, evaluated as a\nsliding window. To our knowledge this process of training and utilizing a CNN\nhas not been investigated before; either in sports or fundamental computer\nvision research. Most research has been focused on action recognition and using\nit to classify many clips in continuous video for action localisation.\n  In this paper we demonstrate our process works well on the task of detecting\nswimming strokes in the wild. However, without modifying the model architecture\nor training method, the process is also shown to work equally well on detecting\ntennis strokes, implying that this is a general process.\n  The outputs of our system are surprisingly smooth signals that predict an\narbitrary event at least as accurately as humans (manually evaluated from a\nsample of negative results). A number of different architectures are evaluated,\npertaining to slightly different problem formulations and signal targets.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 06:14:06 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Victor", "Brandon", ""], ["He", "Zhen", ""], ["Morgan", "Stuart", ""], ["Miniutti", "Dino", ""]]}, {"id": "1705.09912", "submitter": "Jun Xu", "authors": "Jun Xu, Lei Zhang, David Zhang, Xiangchu Feng", "title": "Multi-channel Weighted Nuclear Norm Minimization for Real Color Image\n  Denoising", "comments": "9 pages, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing denoising algorithms are developed for grayscale images,\nwhile it is not a trivial work to extend them for color image denoising because\nthe noise statistics in R, G, B channels can be very different for real noisy\nimages. In this paper, we propose a multi-channel (MC) optimization model for\nreal color image denoising under the weighted nuclear norm minimization (WNNM)\nframework. We concatenate the RGB patches to make use of the channel\nredundancy, and introduce a weight matrix to balance the data fidelity of the\nthree channels in consideration of their different noise statistics. The\nproposed MC-WNNM model does not have an analytical solution. We reformulate it\ninto a linear equality-constrained problem and solve it with the alternating\ndirection method of multipliers. Each alternative updating step has closed-form\nsolution and the convergence can be guaranteed. Extensive experiments on both\nsynthetic and real noisy image datasets demonstrate the superiority of the\nproposed MC-WNNM over state-of-the-art denoising methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 08:51:39 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 17:34:47 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Xu", "Jun", ""], ["Zhang", "Lei", ""], ["Zhang", "David", ""], ["Feng", "Xiangchu", ""]]}, {"id": "1705.09914", "submitter": "Fisher Yu", "authors": "Fisher Yu, Vladlen Koltun, Thomas Funkhouser", "title": "Dilated Residual Networks", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks for image classification progressively reduce\nresolution until the image is represented by tiny feature maps in which the\nspatial structure of the scene is no longer discernible. Such loss of spatial\nacuity can limit image classification accuracy and complicate the transfer of\nthe model to downstream applications that require detailed scene understanding.\nThese problems can be alleviated by dilation, which increases the resolution of\noutput feature maps without reducing the receptive field of individual neurons.\nWe show that dilated residual networks (DRNs) outperform their non-dilated\ncounterparts in image classification without increasing the model's depth or\ncomplexity. We then study gridding artifacts introduced by dilation, develop an\napproach to removing these artifacts (`degridding'), and show that this further\nincreases the performance of DRNs. In addition, we show that the accuracy\nadvantage of DRNs is further magnified in downstream applications such as\nobject localization and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 09:01:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yu", "Fisher", ""], ["Koltun", "Vladlen", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1705.09954", "submitter": "Bo Jiang", "authors": "Chris Ding and Bo Jiang", "title": "L1-norm Error Function Robustness and Outlier Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, data come with corruptions, large errors or\noutliers. One popular approach is to use L1-norm function. However, the\nrobustness of L1-norm function is not well understood so far. In this paper, we\npresent a new outlier regularization framework to understand and analyze the\nrobustness of L1-norm function. There are two main features for the proposed\noutlier regularization. (1) A key property of outlier regularization is that\nhow far an outlier lies away from its theoretically predicted value does not\naffect the final regularization and analysis results. (2) Another important\nfeature of outlier regularization is that it has an equivalent continuous\nrepresentation that closely relates to L1 function. This provides a new way to\nunderstand and analyze the robustness of L1 function. We apply our outlier\nregularization framework to PCA and propose an outlier regularized PCA (ORPCA)\nmodel. Comparing to the trace-normbased robust PCA, ORPCA has several benefits:\n(1) It does not suffer singular value suppression. (2) It can retain small high\nrank components which help retain fine details of data. (3) ORPCA can be\ncomputed more efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 15:53:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Ding", "Chris", ""], ["Jiang", "Bo", ""]]}, {"id": "1705.09966", "submitter": "Yongyi Lu", "authors": "Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang", "title": "Attribute-Guided Face Generation Using Conditional CycleGAN", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in attribute-guided face generation: given a low-res face\ninput image, an attribute vector that can be extracted from a high-res image\n(attribute image), our new method generates a high-res face image for the\nlow-res input that satisfies the given attributes. To address this problem, we\ncondition the CycleGAN and propose conditional CycleGAN, which is designed to\n1) handle unpaired training data because the training low/high-res and high-res\nattribute images may not necessarily align with each other, and to 2) allow\neasy control of the appearance of the generated face via the input attributes.\nWe demonstrate impressive results on the attribute-guided conditional CycleGAN,\nwhich can synthesize realistic face images with appearance easily controlled by\nuser-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using\nthe attribute image as identity to produce the corresponding conditional vector\nand by incorporating a face verification network, the attribute-guided network\nbecomes the identity-guided conditional CycleGAN which produces impressive and\ninteresting results on identity transfer. We demonstrate three applications on\nidentity-guided conditional CycleGAN: identity-preserving face superresolution,\nface swapping, and frontal face generation, which consistently show the\nadvantage of our new method.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 17:37:23 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 13:35:49 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Lu", "Yongyi", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1705.09992", "submitter": "James Herring", "authors": "James Herring, James Nagy, Lars Ruthotto", "title": "LAP: a Linearize and Project Method for Solving Inverse Problems with\n  Coupled Variables", "comments": "21 pages, 6 figures, 3 tables", "journal-ref": "STSIP 17.2 (2018) pp.127-151", "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inverse problems involve two or more sets of variables that represent\ndifferent physical quantities but are tightly coupled with each other. For\nexample, image super-resolution requires joint estimation of the image and\nmotion parameters from noisy measurements. Exploiting this structure is key for\nefficiently solving these large-scale optimization problems, which are often\nill-conditioned.\n  In this paper, we present a new method called Linearize And Project (LAP)\nthat offers a flexible framework for solving inverse problems with coupled\nvariables. LAP is most promising for cases when the subproblem corresponding to\none of the variables is considerably easier to solve than the other. LAP is\nbased on a Gauss-Newton method, and thus after linearizing the residual, it\neliminates one block of variables through projection. Due to the linearization,\nthis block can be chosen freely. Further, LAP supports direct, iterative, and\nhybrid regularization as well as constraints. Therefore LAP is attractive,\ne.g., for ill-posed imaging problems. These traits differentiate LAP from\ncommon alternatives for this type of problem such as variable projection\n(VarPro) and block coordinate descent (BCD). Our numerical experiments compare\nthe performance of LAP to BCD and VarPro using three coupled problems whose\nforward operators are linear with respect to one block and nonlinear for the\nother set of variables.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 21:11:53 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 20:02:54 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 14:35:48 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Herring", "James", ""], ["Nagy", "James", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1705.10000", "submitter": "Deyu Meng", "authors": "Hongwei Yong, Deyu Meng, Wangmeng Zuo, Lei Zhang", "title": "Robust Online Matrix Factorization for Dynamic Background Subtraction", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective online background subtraction method, which can be\nrobustly applied to practical videos that have variations in both foreground\nand background. Different from previous methods which often model the\nforeground as Gaussian or Laplacian distributions, we model the foreground for\neach frame with a specific mixture of Gaussians (MoG) distribution, which is\nupdated online frame by frame. Particularly, our MoG model in each frame is\nregularized by the learned foreground/background knowledge in previous frames.\nThis makes our online MoG model highly robust, stable and adaptive to practical\nforeground and background variations. The proposed model can be formulated as a\nconcise probabilistic MAP model, which can be readily solved by EM algorithm.\nWe further embed an affine transformation operator into the proposed model,\nwhich can be automatically adjusted to fit a wide range of video background\ntransformations and make the method more robust to camera movements. With using\nthe sub-sampling technique, the proposed method can be accelerated to execute\nmore than 250 frames per second on average, meeting the requirement of\nreal-time background subtraction for practical video processing tasks. The\nsuperiority of the proposed method is substantiated by extensive experiments\nimplemented on synthetic and real videos, as compared with state-of-the-art\nonline and offline background subtraction methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 23:09:29 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yong", "Hongwei", ""], ["Meng", "Deyu", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1705.10021", "submitter": "Prasan Shedligeri Mr.", "authors": "Prasan A Shedligeri, Sreyas Mohan, Kaushik Mitra", "title": "Data Driven Coded Aperture Design for Depth Recovery", "comments": "5 pages, 4 figures. Accepted at IEEE ICIP 2017, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inserting a patterned occluder at the aperture of a camera lens has been\nshown to improve the recovery of depth map and all-focus image compared to a\nfully open aperture. However, design of the aperture pattern plays a very\ncritical role. Previous approaches for designing aperture codes make simple\nassumptions on image distributions to obtain metrics for evaluating aperture\ncodes. However, real images may not follow those assumptions and hence the\ndesigned code may not be optimal for them. To address this drawback we propose\na data driven approach for learning the optimal aperture pattern to recover\ndepth map from a single coded image. We propose a two stage architecture where,\nin the first stage we simulate coded aperture images from a training dataset of\nall-focus images and depth maps and in the second stage we recover the depth\nmap using a deep neural network. We demonstrate that our learned aperture code\nperforms better than previously designed codes even on code design metrics\nproposed by previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 02:25:39 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 04:36:42 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Shedligeri", "Prasan A", ""], ["Mohan", "Sreyas", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1705.10034", "submitter": "Xiaopeng Zhang", "authors": "Xiaopeng Zhang, Hongkai Xiong, Weiyao Lin, Qi Tian", "title": "Ensemble of Part Detectors for Simultaneous Classification and\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based representation has been proven to be effective for a variety of\nvisual applications. However, automatic discovery of discriminative parts\nwithout object/part-level annotations is challenging. This paper proposes a\ndiscriminative mid-level representation paradigm based on the responses of a\ncollection of part detectors, which only requires the image-level labels.\nTowards this goal, we first develop a detector-based spectral clustering method\nto mine the representative and discriminative mid-level patterns for detector\ninitialization. The advantage of the proposed pattern mining technology is that\nthe distance metric based on detectors only focuses on discriminative details,\nand a set of such grouped detectors offer an effective way for consistent\npattern mining. Relying on the discovered patterns, we further formulate the\ndetector learning process as a confidence-loss sparse Multiple Instance\nLearning (cls-MIL) task, which considers the diversity of the positive samples,\nwhile avoid drifting away the well localized ones by assigning a confidence\nvalue to each positive sample. The responses of the learned detectors can form\nan effective mid-level image representation for both image classification and\nobject localization. Experiments conducted on benchmark datasets demonstrate\nthe superiority of our method over existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 04:04:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zhang", "Xiaopeng", ""], ["Xiong", "Hongkai", ""], ["Lin", "Weiyao", ""], ["Tian", "Qi", ""]]}, {"id": "1705.10041", "submitter": "Arturo Deza", "authors": "Arturo Deza, Aditya Jonnalagadda, Miguel Eckstein", "title": "Towards Metamerism via Foveated Style Transfer", "comments": "Published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $\\textit{visual metamerism}$ is defined as finding a family of\nperceptually indistinguishable, yet physically different images. In this paper,\nwe propose our NeuroFovea metamer model, a foveated generative model that is\nbased on a mixture of peripheral representations and style transfer\nforward-pass algorithms. Our gradient-descent free model is parametrized by a\nfoveated VGG19 encoder-decoder which allows us to encode images in high\ndimensional space and interpolate between the content and texture information\nwith adaptive instance normalization anywhere in the visual field. Our\ncontributions include: 1) A framework for computing metamers that resembles a\nnoisy communication system via a foveated feed-forward encoder-decoder network\n-- We observe that metamerism arises as a byproduct of noisy perturbations that\npartially lie in the perceptual null space; 2) A perceptual optimization scheme\nas a solution to the hyperparametric nature of our metamer model that requires\ntuning of the image-texture tradeoff coefficients everywhere in the visual\nfield which are a consequence of internal noise; 3) An ABX psychophysical\nevaluation of our metamers where we also find that the rate of growth of the\nreceptive fields in our model match V1 for reference metamers and V2 between\nsynthesized samples. Our model also renders metamers at roughly a second,\npresenting a $\\times1000$ speed-up compared to the previous work, which allows\nfor tractable data-driven metamer experiments.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 05:38:20 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 04:42:43 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 22:37:48 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Deza", "Arturo", ""], ["Jonnalagadda", "Aditya", ""], ["Eckstein", "Miguel", ""]]}, {"id": "1705.10060", "submitter": "Juan Jos\\'e Murillo Fuentes", "authors": "Francisco J. Simois, Juan J. Murillo-Fuentes", "title": "On the Power Spectral Density Applied to the Analysis of Old Canvases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A routine task for art historians is painting diagnostics, such as dating or\nattribution. Signal processing of the X-ray image of a canvas provides useful\ninformation about its fabric. However, previous methods may fail when very old\nand deteriorated artworks or simply canvases of small size are studied. We\npresent a new framework to analyze and further characterize the paintings from\ntheir radiographs. First, we start from a general analysis of lattices and\nprovide new unifying results about the theoretical spectra of weaves. Then, we\nuse these results to infer the main structure of the fabric, like the type of\nweave and the thread densities. We propose a practical estimation of these\ntheoretical results from paintings with the averaged power spectral density\n(PSD), which provides a more robust tool. Furthermore, we found that the PSD\nprovides a fingerprint that characterizes the whole canvas. We search and\ndiscuss some distinctive features we may find in that fingerprint. We apply\nthese results to several masterpieces of the 17th and 18th centuries from the\nMuseo Nacional del Prado to show that this approach yields accurate results in\nthread counting and is very useful for paintings comparison, even in situations\nwhere previous methods fail.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 07:49:32 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Simois", "Francisco J.", ""], ["Murillo-Fuentes", "Juan J.", ""]]}, {"id": "1705.10118", "submitter": "Di Kang", "authors": "Di Kang, Zheng Ma, Antoni B. Chan", "title": "Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks -\n  Counting, Detection, and Tracking", "comments": "accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For crowded scenes, the accuracy of object-based computer vision methods\ndeclines when the images are low-resolution and objects have severe occlusions.\nTaking counting methods for example, almost all the recent state-of-the-art\ncounting methods bypass explicit detection and adopt regression-based methods\nto directly count the objects of interest. Among regression-based methods,\ndensity map estimation, where the number of objects inside a subregion is the\nintegral of the density map over that subregion, is especially promising\nbecause it preserves spatial information, which makes it useful for both\ncounting and localization (detection and tracking). With the power of deep\nconvolutional neural networks (CNNs) the counting performance has improved\nsteadily. The goal of this paper is to evaluate density maps generated by\ndensity estimation methods on a variety of crowd analysis tasks, including\ncounting, detection, and tracking. Most existing CNN methods produce density\nmaps with resolution that is smaller than the original images, due to the\ndownsample strides in the convolution/pooling operations. To produce an\noriginal-resolution density map, we also evaluate a classical CNN that uses a\nsliding window regressor to predict the density for every pixel in the image.\nWe also consider a fully convolutional (FCNN) adaptation, with skip connections\nfrom lower convolutional layers to compensate for loss in spatial information\nduring upsampling. In our experiments, we found that the lower-resolution\ndensity maps sometimes have better counting performance. In contrast, the\noriginal-resolution density maps improved localization tasks, such as detection\nand tracking, compared to bilinear upsampling the lower-resolution density\nmaps. Finally, we also propose several metrics for measuring the quality of a\ndensity map, and relate them to experiment results on counting and\nlocalization.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 14:01:09 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Kang", "Di", ""], ["Ma", "Zheng", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1705.10120", "submitter": "Vijay Kumar", "authors": "Vijay Kumar, Anoop Namboodiri, Manohar Paluri, C V Jawahar", "title": "Pose-Aware Person Recognition", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person recognition methods that use multiple body regions have shown\nsignificant improvements over traditional face-based recognition. One of the\nprimary challenges in full-body person recognition is the extreme variation in\npose and view point. In this work, (i) we present an approach that tackles pose\nvariations utilizing multiple models that are trained on specific poses, and\ncombined using pose-aware weights during testing. (ii) For learning a person\nrepresentation, we propose a network that jointly optimizes a single loss over\nmultiple body regions. (iii) Finally, we introduce new benchmarks to evaluate\nperson recognition in diverse scenarios and show significant improvements over\npreviously proposed approaches on all the benchmarks including the photo album\nsetting of PIPA.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:47 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Kumar", "Vijay", ""], ["Namboodiri", "Anoop", ""], ["Paluri", "Manohar", ""], ["Jawahar", "C V", ""]]}, {"id": "1705.10279", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, John J. Leonard", "title": "Towards Visual Ego-motion Learning in Robots", "comments": "Conference paper; Submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures,\n  2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model-based Visual Odometry (VO) algorithms have been proposed in the\npast decade, often restricted to the type of camera optics, or the underlying\nmotion manifold observed. We envision robots to be able to learn and perform\nthese tasks, in a minimally supervised setting, as they gain more experience.\nTo this end, we propose a fully trainable solution to visual ego-motion\nestimation for varied camera optics. We propose a visual ego-motion learning\narchitecture that maps observed optical flow vectors to an ego-motion density\nestimate via a Mixture Density Network (MDN). By modeling the architecture as a\nConditional Variational Autoencoder (C-VAE), our model is able to provide\nintrospective reasoning and prediction for ego-motion induced scene-flow.\nAdditionally, our proposed model is especially amenable to bootstrapped\nego-motion learning in robots where the supervision in ego-motion estimation\nfor a particular camera sensor can be obtained from standard navigation-based\nsensor fusion strategies (GPS/INS and wheel-odometry fusion). Through\nexperiments, we show the utility of our proposed approach in enabling the\nconcept of self-supervised learning for visual ego-motion estimation in\nautonomous robots.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 16:25:50 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Pillai", "Sudeep", ""], ["Leonard", "John J.", ""]]}, {"id": "1705.10284", "submitter": "Yuhui Yuan", "authors": "Yuhui Yuan, Kuiyuan Yang, Chao Zhang", "title": "Feature Incay for Representation Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax loss is widely used in deep neural networks for multi-class\nclassification, where each class is represented by a weight vector, a sample is\nrepresented as a feature vector, and the feature vector has the largest\nprojection on the weight vector of the correct category when the model\ncorrectly classifies a sample. To ensure generalization, weight decay that\nshrinks the weight norm is often used as regularizer. Different from\ntraditional learning algorithms where features are fixed and only weights are\ntunable, features are also tunable as representation learning in deep learning.\nThus, we propose feature incay to also regularize representation learning,\nwhich favors feature vectors with large norm when the samples can be correctly\nclassified. With the feature incay, feature vectors are further pushed away\nfrom the origin along the direction of their corresponding weight vectors,\nwhich achieves better inter-class separability. In addition, the proposed\nfeature incay encourages intra-class compactness along the directions of weight\nvectors by increasing the small feature norm faster than the large ones.\nEmpirical results on MNIST, CIFAR10 and CIFAR100 demonstrate feature incay can\nimprove the generalization ability.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 16:33:54 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yuan", "Yuhui", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Chao", ""]]}, {"id": "1705.10311", "submitter": "Abhay Shah", "authors": "Junjie Bai, Abhay Shah and Xiaodong Wu", "title": "Optimal Multi-Object Segmentation with Novel Gradient Vector Flow Based\n  Shape Priors", "comments": "Paper in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape priors have been widely utilized in medical image segmentation to\nimprove segmentation accuracy and robustness. A major way to encode such a\nprior shape model is to use a mesh representation, which is prone to causing\nself-intersection or mesh folding. Those problems require complex and expensive\nalgorithms to mitigate. In this paper, we propose a novel shape prior directly\nembedded in the voxel grid space, based on gradient vector flows of a\npre-segmentation. The flexible and powerful prior shape representation is ready\nto be extended to simultaneously segmenting multiple interacting objects with\nminimum separation distance constraint. The problem is formulated as a Markov\nrandom field problem whose exact solution can be efficiently computed with a\nsingle minimum s-t cut in an appropriately constructed graph. The proposed\nalgorithm is validated on two multi-object segmentation applications: the brain\ntissue segmentation in MRI images, and the bladder/prostate segmentation in CT\nimages. Both sets of experiments show superior or competitive performance of\nthe proposed method to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:33:39 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Bai", "Junjie", ""], ["Shah", "Abhay", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1705.10369", "submitter": "Andrew Drozdov", "authors": "Katrina Evtimova, Andrew Drozdov, Douwe Kiela, Kyunghyun Cho", "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game", "comments": "Published as a conference paper at ICLR 2018. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IT cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by previous work on emergent communication in referential games, we\npropose a novel multi-modal, multi-step referential game, where the sender and\nreceiver have access to distinct modalities of an object, and their information\nexchange is bidirectional and of arbitrary duration. The multi-modal multi-step\nsetting allows agents to develop an internal communication significantly closer\nto natural language, in that they share a single set of messages, and that the\nlength of the conversation may vary according to the difficulty of the task. We\nexamine these properties empirically using a dataset consisting of images and\ntextual descriptions of mammals, where the agents are tasked with identifying\nthe correct object. Our experiments indicate that a robust and efficient\ncommunication protocol emerges, where gradual information exchange informs\nbetter predictions and higher communication bandwidth improves generalization.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:25:49 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 17:09:14 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 04:07:30 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 19:22:22 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Evtimova", "Katrina", ""], ["Drozdov", "Andrew", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1705.10413", "submitter": "Evgeny Zamyatin I", "authors": "Evgeny Zamyatin, Andrey Filchenkov", "title": "Learning to Generate Chairs with Generative Adversarial Nets", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) has gained tremendous popularity\nlately due to an ability to reinforce quality of its predictive model with\ngenerated objects and the quality of the generative model with and supervised\nfeedback. GANs allow to synthesize images with a high degree of realism.\nHowever, the learning process of such models is a very complicated optimization\nproblem and certain limitation for such models were found. It affects the\nchoice of certain layers and nonlinearities when designing architectures. In\nparticular, it does not allow to train convolutional GAN models with\nfully-connected hidden layers. In our work, we propose a modification of the\npreviously described set of rules, as well as new approaches to designing\narchitectures that will allow us to train more powerful GAN models. We show the\neffectiveness of our methods on the problem of synthesizing projections of 3D\nobjects with the possibility of interpolation by class and view point.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 23:15:50 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Zamyatin", "Evgeny", ""], ["Filchenkov", "Andrey", ""]]}, {"id": "1705.10420", "submitter": "Basura Fernando", "authors": "Basura Fernando and Stephen Gould", "title": "Discriminatively Learned Hierarchical Rank Pooling Networks", "comments": "International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present novel temporal encoding methods for action and\nactivity classification by extending the unsupervised rank pooling temporal\nencoding method in two ways. First, we present \"discriminative rank pooling\" in\nwhich the shared weights of our video representation and the parameters of the\naction classifiers are estimated jointly for a given training dataset of\nlabelled vector sequences using a bilevel optimization formulation of the\nlearning problem. When the frame level features vectors are obtained from a\nconvolutional neural network (CNN), we rank pool the network activations and\njointly estimate all parameters of the model, including CNN filters and\nfully-connected weights, in an end-to-end manner which we coined as \"end-to-end\ntrainable rank pooled CNN\". Importantly, this model can make use of any\nexisting convolutional neural network architecture (e.g., AlexNet or VGG)\nwithout modification or introduction of additional parameters. Then, we extend\nrank pooling to a high capacity video representation, called \"hierarchical rank\npooling\". Hierarchical rank pooling consists of a network of rank pooling\nfunctions, which encode temporal semantics over arbitrary long video clips\nbased on rich frame level features. By stacking non-linear feature functions\nand temporal sub-sequence encoders one on top of the other, we build a high\ncapacity encoding network of the dynamic behaviour of the video. The resulting\nvideo representation is a fixed-length feature vector describing the entire\nvideo clip that can be used as input to standard machine learning classifiers.\nWe demonstrate our approach on the task of action and activity recognition.\nObtained results are comparable to state-of-the-art methods on three important\nactivity recognition benchmarks with classification performance of 76.7% mAP on\nHollywood2, 69.4% on HMDB51, and 93.6% on UCF101.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 00:43:16 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Fernando", "Basura", ""], ["Gould", "Stephen", ""]]}, {"id": "1705.10444", "submitter": "Hehe Fan", "authors": "Hehe Fan, Liang Zheng and Yi Yang", "title": "Unsupervised Person Re-identification: Clustering and Fine-tuning", "comments": "Add more results, parameter analysis and comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superiority of deeply learned pedestrian representations has been\nreported in very recent literature of person re-identification (re-ID). In this\npaper, we consider the more pragmatic issue of learning a deep feature with no\nor only a few labels. We propose a progressive unsupervised learning (PUL)\nmethod to transfer pretrained deep representations to unseen domains. Our\nmethod is easy to implement and can be viewed as an effective baseline for\nunsupervised re-ID feature learning. Specifically, PUL iterates between 1)\npedestrian clustering and 2) fine-tuning of the convolutional neural network\n(CNN) to improve the original model trained on the irrelevant labeled dataset.\nSince the clustering results can be very noisy, we add a selection operation\nbetween the clustering and fine-tuning. At the beginning when the model is\nweak, CNN is fine-tuned on a small amount of reliable examples which locate\nnear to cluster centroids in the feature space. As the model becomes stronger\nin subsequent iterations, more images are being adaptively selected as CNN\ntraining samples. Progressively, pedestrian clustering and the CNN model are\nimproved simultaneously until algorithm convergence. This process is naturally\nformulated as self-paced learning. We then point out promising directions that\nmay lead to further improvement. Extensive experiments on three large-scale\nre-ID datasets demonstrate that PUL outputs discriminative features that\nimprove the re-ID accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:14:49 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 00:58:47 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Fan", "Hehe", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1705.10447", "submitter": "Jimmy Ren", "authors": "Jimmy Ren, Zhiyang Yu, Jianbo Liu, Rui Zhang, Wenxiu Sun, Jiahao Pang,\n  Xiaohao Chen, Qiong Yan", "title": "Robust Tracking Using Region Proposal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in visual tracking showed that deep Convolutional Neural\nNetworks (CNN) trained for image classification can be strong feature\nextractors for discriminative trackers. However, due to the drastic difference\nbetween image classification and tracking, extra treatments such as model\nensemble and feature engineering must be carried out to bridge the two domains.\nSuch procedures are either time consuming or hard to generalize well across\ndatasets. In this paper we discovered that the internal structure of Region\nProposal Network (RPN)'s top layer feature can be utilized for robust visual\ntracking. We showed that such property has to be unleashed by a novel loss\nfunction which simultaneously considers classification accuracy and bounding\nbox quality. Without ensemble and any extra treatment on feature maps, our\nproposed method achieved state-of-the-art results on several large scale\nbenchmarks including OTB50, OTB100 and VOT2016. We will make our code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:32:07 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ren", "Jimmy", ""], ["Yu", "Zhiyang", ""], ["Liu", "Jianbo", ""], ["Zhang", "Rui", ""], ["Sun", "Wenxiu", ""], ["Pang", "Jiahao", ""], ["Chen", "Xiaohao", ""], ["Yan", "Qiong", ""]]}, {"id": "1705.10450", "submitter": "Haifeng Li", "authors": "Haifeng Li, Xin Dou, Chao Tao, Zhixiang Hou, Jie Chen, Jian Peng, Min\n  Deng, Ling Zhao", "title": "RSI-CB: A Large Scale Remote Sensing Image Classification Benchmark via\n  Crowdsource Data", "comments": "41 pages, 19 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep convolutional neural network (DCNN) has seen a\nbreakthrough progress in natural image recognition because of three points:\nuniversal approximation ability via DCNN, large-scale database (such as\nImageNet), and supercomputing ability powered by GPU. The remote sensing field\nis still lacking a large-scale benchmark compared to ImageNet and Place2. In\nthis paper, we propose a remote sensing image classification benchmark (RSI-CB)\nbased on massive, scalable, and diverse crowdsource data. Using crowdsource\ndata, such as Open Street Map (OSM) data, ground objects in remote sensing\nimages can be annotated effectively by points of interest, vector data from\nOSM, or other crowdsource data. The annotated images can be used in remote\nsensing image classification tasks. Based on this method, we construct a\nworldwide large-scale benchmark for remote sensing image classification. This\nbenchmark has two sub-datasets with 256 by 256 and 128 by 128 sizes because\ndifferent DCNNs require different image sizes. The former contains 6 categories\nwith 35 subclasses of more than 24,000 images. The latter contains 6 categories\nwith 45 subclasses of more than 36,000 images. This classification system of\nground objects is defined according to the national standard of land-use\nclassification in China and is inspired by the hierarchy mechanism of ImageNet.\nFinally, we conduct many experiments to compare RSI-CB with the SAT-4, SAT-6,\nand UC-Merced datasets on handcrafted features, such as scale-invariant feature\ntransform, color histogram, local binary patterns, and GIST, and classical DCNN\nmodels, such as AlexNet, VGGNet, GoogLeNet, and ResNet.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:53:03 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 03:25:19 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 11:31:17 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Li", "Haifeng", ""], ["Dou", "Xin", ""], ["Tao", "Chao", ""], ["Hou", "Zhixiang", ""], ["Chen", "Jie", ""], ["Peng", "Jian", ""], ["Deng", "Min", ""], ["Zhao", "Ling", ""]]}, {"id": "1705.10524", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma, Jing-Hao Xue, Arne Leijon, Zheng-Hua Tan, Zhen Yang, and\n  Jun Guo", "title": "Decorrelation of Neutral Vector Variables: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel strategies for neutral vector variable\ndecorrelation. Two fundamental invertible transformations, namely serial\nnonlinear transformation and parallel nonlinear transformation, are proposed to\ncarry out the decorrelation. For a neutral vector variable, which is not\nmultivariate Gaussian distributed, the conventional principal component\nanalysis (PCA) cannot yield mutually independent scalar variables. With the two\nproposed transformations, a highly negatively correlated neutral vector can be\ntransformed to a set of mutually independent scalar variables with the same\ndegrees of freedom. We also evaluate the decorrelation performances for the\nvectors generated from a single Dirichlet distribution and a mixture of\nDirichlet distributions. The mutual independence is verified with the distance\ncorrelation measurement. The advantages of the proposed decorrelation\nstrategies are intensively studied and demonstrated with synthesized data and\npractical application evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 09:53:11 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""], ["Leijon", "Arne", ""], ["Tan", "Zheng-Hua", ""], ["Yang", "Zhen", ""], ["Guo", "Jun", ""]]}, {"id": "1705.10545", "submitter": "Hannah Spitzer", "authors": "Hannah Spitzer, Katrin Amunts, Stefan Harmeling, Timo Dickscheid", "title": "Parcellation of Visual Cortex on high-resolution histological Brain\n  Sections using Convolutional Neural Networks", "comments": "Accepted for oral presentation at International Symposium of\n  Biomedical Imaging (ISBI) 2017", "journal-ref": null, "doi": "10.1109/ISBI.2017.7950666", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopic analysis of histological sections is considered the \"gold\nstandard\" to verify structural parcellations in the human brain. Its high\nresolution allows the study of laminar and columnar patterns of cell\ndistributions, which build an important basis for the simulation of cortical\nareas and networks. However, such cytoarchitectonic mapping is a semiautomatic,\ntime consuming process that does not scale with high throughput imaging. We\npresent an automatic approach for parcellating histological sections at 2um\nresolution. It is based on a convolutional neural network that combines\ntopological information from probabilistic atlases with the texture features\nlearned from high-resolution cell-body stained images. The model is applied to\nvisual areas and trained on a sparse set of partial annotations. We show how\npredictions are transferable to new brains and spatially consistent across\nsections.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:05:00 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Spitzer", "Hannah", ""], ["Amunts", "Katrin", ""], ["Harmeling", "Stefan", ""], ["Dickscheid", "Timo", ""]]}, {"id": "1705.10546", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, Jorma Laaksonen", "title": "Saliency Revisited: Analysis of Mouse Movements versus Fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits visual saliency prediction by evaluating the recent\nadvancements in this field such as crowd-sourced mouse tracking-based databases\nand contextual annotations. We pursue a critical and quantitative approach\ntowards some of the new challenges including the quality of mouse tracking\nversus eye tracking for model training and evaluation. We extend quantitative\nevaluation of models in order to incorporate contextual information by\nproposing an evaluation methodology that allows accounting for contextual\nfactors such as text, faces, and object attributes. The proposed contextual\nevaluation scheme facilitates detailed analysis of models and helps identify\ntheir pros and cons. Through several experiments, we find that (1) mouse\ntracking data has lower inter-participant visual congruency and higher\ndispersion, compared to the eye tracking data, (2) mouse tracking data does not\ntotally agree with eye tracking in general and in terms of different contextual\nregions in specific, and (3) mouse tracking data leads to acceptable results in\ntraining current existing models, and (4) mouse tracking data is less reliable\nfor model selection and evaluation. The contextual evaluation also reveals\nthat, among the studied models, there is no single model that performs best on\nall the tested annotations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:07:43 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Ahmed", "Fawad", ""], ["Borji", "Ali", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1705.10552", "submitter": "Longquan Dai", "authors": "Longquan Dai", "title": "Interpreting and Extending The Guided Filter Via Cyclic Coordinate\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will disclose that the Guided Filter (GF) can be\ninterpreted as the Cyclic Coordinate Descent (CCD) solver of a Least Square\n(LS) objective function. This discovery implies a possible way to extend GF\nbecause we can alter the objective function of GF and define new filters as the\nfirst pass iteration of the CCD solver of modified objective functions.\nMoreover, referring to the iterative minimizing procedure of CCD, we can derive\nnew rolling filtering schemes. Hence, under the guidance of this discovery, we\nnot only propose new GF-like filters adapting to the specific requirements of\napplications but also offer thoroughly explanations for two rolling filtering\nschemes of GF as well as the way to extend them. Experiments show that our new\nfilters and extensions produce state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:29:23 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Dai", "Longquan", ""]]}, {"id": "1705.10561", "submitter": "Wenhan Luo", "authors": "Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang", "title": "End-to-end Active Object Tracking via Reinforcement Learning", "comments": "To appear in ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study active object tracking, where a tracker takes as input the visual\nobservation (i.e., frame sequence) and produces the camera control signal\n(e.g., move forward, turn left, etc.). Conventional methods tackle the tracking\nand the camera control separately, which is challenging to tune jointly. It\nalso incurs many human efforts for labeling and many expensive trial-and-errors\nin realworld. To address these issues, we propose, in this paper, an end-to-end\nsolution via deep reinforcement learning, where a ConvNet-LSTM function\napproximator is adopted for the direct frame-toaction prediction. We further\npropose an environment augmentation technique and a customized reward function,\nwhich are crucial for a successful training. The tracker trained in simulators\n(ViZDoom, Unreal Engine) shows good generalization in the case of unseen object\nmoving path, unseen object appearance, unseen background, and distracting\nobject. It can restore tracking when occasionally losing the target. With the\nexperiments over the VOT dataset, we also find that the tracking ability,\nobtained solely from simulators, can potentially transfer to real-world\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:44:50 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 15:11:45 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 16:14:24 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Luo", "Wenhan", ""], ["Sun", "Peng", ""], ["Zhong", "Fangwei", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""], ["Wang", "Yizhou", ""]]}, {"id": "1705.10574", "submitter": "Sergiy Vorobyov A.", "authors": "Farshad G. Veshki and Sergiy A. Vorobyov", "title": "Multi-Focus Image Fusion Using Sparse Representation and Coupled\n  Dictionary Learning", "comments": "25 pages, 15 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the multi-focus image fusion problem, where multiple images\ncaptured with different focal settings are to be fused into an all-in-focus\nimage of higher quality. Algorithms for this problem necessarily admit the\nsource image characteristics along with focused and blurred features. However,\nmost sparsity-based approaches use a single dictionary in focused feature space\nto describe multi-focus images, and ignore the representations in blurred\nfeature space. We propose a multi-focus image fusion approach based on sparse\nrepresentation using a coupled dictionary. It exploits the observations that\nthe patches from a given training set can be sparsely represented by a couple\nof overcomplete dictionaries related to the focused and blurred categories of\nimages and that a sparse approximation based on such coupled dictionary leads\nto a more flexible and therefore better fusion strategy than the one based on\njust selecting the sparsest representation in the original image estimate. In\naddition, to improve the fusion performance, we employ a coupled dictionary\nlearning approach that enforces pairwise correlation between atoms of\ndictionaries learned to represent the focused and blurred feature spaces. We\nalso discuss the advantages of the fusion approach based on coupled dictionary\nlearning, and present efficient algorithms for fusion based on coupled\ndictionary learning. Extensive experimental comparisons with state-of-the-art\nmulti-focus image fusion algorithms validate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 12:20:26 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 05:54:06 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 08:05:44 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Veshki", "Farshad G.", ""], ["Vorobyov", "Sergiy A.", ""]]}, {"id": "1705.10583", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "Nighttime sky/cloud image segmentation", "comments": "Accepted in Proc. IEEE International Conference on Image Processing\n  (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging the atmosphere using ground-based sky cameras is a popular approach\nto study various atmospheric phenomena. However, it usually focuses on the\ndaytime. Nighttime sky/cloud images are darker and noisier, and thus harder to\nanalyze. An accurate segmentation of sky/cloud images is already challenging\nbecause of the clouds' non-rigid structure and size, and the lower and less\nstable illumination of the night sky increases the difficulty. Nonetheless,\nnighttime cloud imaging is essential in certain applications, such as\ncontinuous weather analysis and satellite communication.\n  In this paper, we propose a superpixel-based method to segment nighttime\nsky/cloud images. We also release the first nighttime sky/cloud image\nsegmentation database to the research community. The experimental results show\nthe efficacy of our proposed algorithm for nighttime images.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 12:39:51 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1705.10589", "submitter": "James P. Sethna", "authors": "Lorien X. Hayden, Alexander A. Alemi, Paul H. Ginsparg, James P.\n  Sethna", "title": "Jeffrey's prior sampling of deep sigmoidal networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been shown to have a remarkable ability to uncover low\ndimensional structure in data: the space of possible reconstructed images form\na reduced model manifold in image space. We explore this idea directly by\nanalyzing the manifold learned by Deep Belief Networks and Stacked Denoising\nAutoencoders using Monte Carlo sampling. The model manifold forms an only\nslightly elongated hyperball with actual reconstructed data appearing\npredominantly on the boundaries of the manifold. In connection with the results\nwe present, we discuss problems of sampling high-dimensional manifolds as well\nas recent work [M. Transtrum, G. Hart, and P. Qiu, Submitted (2014)] discussing\nthe relation between high dimensional geometry and model reduction.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:58:37 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Hayden", "Lorien X.", ""], ["Alemi", "Alexander A.", ""], ["Ginsparg", "Paul H.", ""], ["Sethna", "James P.", ""]]}, {"id": "1705.10659", "submitter": "Xiatian Zhu", "authors": "Jingya Wang, Xiatian Zhu, Shaogang Gong", "title": "Discovering Visual Concept Structure with Sparse and Incomplete Tags", "comments": "Artificial Intelligence journal 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering automatically the semantic structure of tagged visual data (e.g.\nweb videos and images) is important for visual data analysis and\ninterpretation, enabling the machine intelligence for effectively processing\nthe fast-growing amount of multi-media data. However, this is non-trivial due\nto the need for jointly learning underlying correlations between heterogeneous\nvisual and tag data. The task is made more challenging by inherently sparse and\nincomplete tags. In this work, we develop a method for modelling the inherent\nvisual data concept structures based on a novel Hierarchical-Multi-Label Random\nForest model capable of correlating structured visual and tag information so as\nto more accurately interpret the visual semantics, e.g. disclosing meaningful\nvisual groups with similar high-level concepts, and recovering missing tags for\nindividual visual data samples. Specifically, our model exploits hierarchically\nstructured tags of different semantic abstractness and multiple tag statistical\ncorrelations in addition to modelling visual and tag interactions. As a result,\nour model is able to discover more accurate semantic correlation between\ntextual tags and visual features, and finally providing favourable visual\nsemantics interpretation even with highly sparse and incomplete tags. We\ndemonstrate the advantages of our proposed approach in two fundamental\napplications, visual data clustering and missing tag completion, on\nbenchmarking video (i.e. TRECVID MED 2011) and image (i.e. NUS-WIDE) datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 14:12:43 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wang", "Jingya", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1705.10694", "submitter": "Andreas Veit", "authors": "David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit", "title": "Deep Learning is Robust to Massive Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained on large supervised datasets have led to\nimpressive results in image classification and other tasks. However,\nwell-annotated datasets can be time-consuming and expensive to collect, lending\nincreased interest to larger but noisy datasets that are more easily obtained.\nIn this paper, we show that deep neural networks are capable of generalizing\nfrom training data for which true labels are massively outnumbered by incorrect\nlabels. We demonstrate remarkably high test performance after training on\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\ntest accuracy above 90 percent even after each clean training example has been\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\npatterns of label noise, even when erroneous labels are biased towards\nconfusing classes. We show that training in this regime requires a significant\nbut manageable increase in dataset size that is related to the factor by which\ncorrect labels have been diluted. Finally, we provide an analysis of our\nresults that shows how increasing noise decreases the effective batch size.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 02:02:56 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 16:51:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Rolnick", "David", ""], ["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10698", "submitter": "Mark Marsden", "authors": "Mark Marsden, Kevin McGuinness, Suzanne Little, Noel E. O'Connor", "title": "ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting,\n  Violent Behaviour Detection and Crowd Density Level Classification", "comments": "7 Pages, AVSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose ResnetCrowd, a deep residual architecture for\nsimultaneous crowd counting, violent behaviour detection and crowd density\nlevel classification. To train and evaluate the proposed multi-objective\ntechnique, a new 100 image dataset referred to as Multi Task Crowd is\nconstructed. This new dataset is the first computer vision dataset fully\nannotated for crowd counting, violent behaviour detection and density level\nclassification. Our experiments show that a multi-task approach boosts\nindividual task performance for all tasks and most notably for violent\nbehaviour detection which receives a 9\\% boost in ROC curve AUC (Area under the\ncurve). The trained ResnetCrowd model is also evaluated on several additional\nbenchmarks highlighting the superior generalisation of crowd analysis models\ntrained for multiple objectives.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:18:41 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Marsden", "Mark", ""], ["McGuinness", "Kevin", ""], ["Little", "Suzanne", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1705.10715", "submitter": "Ali Taalimi", "authors": "Ali Taalimi, Alireza Rahimpour, Liu Liu, Hairong Qi", "title": "Multi-View Task-Driven Recognition in Visual Sensor Networks", "comments": "5 pages, Accepted in International Conference of Image Processing,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, distributed smart cameras are deployed for a wide set of tasks in\nseveral application scenarios, ranging from object recognition, image\nretrieval, and forensic applications. Due to limited bandwidth in distributed\nsystems, efficient coding of local visual features has in fact been an active\ntopic of research. In this paper, we propose a novel approach to obtain a\ncompact representation of high-dimensional visual data using sensor fusion\ntechniques. We convert the problem of visual analysis in resource-limited\nscenarios to a multi-view representation learning, and we show that the key to\nfinding properly compressed representation is to exploit the position of\ncameras with respect to each other as a norm-based regularization in the\nparticular signal representation of sparse coding. Learning the representation\nof each camera is viewed as an individual task and a multi-task learning with\njoint sparsity for all nodes is employed. The proposed representation learning\nscheme is referred to as the multi-view task-driven learning for visual sensor\nnetwork (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art in\nvarious surveillance recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:10:36 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 22:03:10 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Taalimi", "Ali", ""], ["Rahimpour", "Alireza", ""], ["Liu", "Liu", ""], ["Qi", "Hairong", ""]]}, {"id": "1705.10716", "submitter": "Ali Taalimi", "authors": "Ali Taalimi, Liu Liu and Hairong Qi", "title": "Addressing Ambiguity in Multi-target Tracking by Hierarchical Strategy", "comments": "5 pages, Accepted in International Conference of Image Processing,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel hierarchical approach for the simultaneous\ntracking of multiple targets in a video. We use a network flow approach to link\ndetections in low-level and tracklets in high-level. At each step of the\nhierarchy, the confidence of candidates is measured by using a new scoring\nsystem, ConfRank, that considers the quality and the quantity of its\nneighborhood. The output of the first stage is a collection of safe tracklets\nand unlinked high-confidence detections. For each individual detection, we\ndetermine if it belongs to an existing or is a new tracklet. We show the effect\nof our framework to recover missed detections and reduce switch identity. The\nproposed tracker is referred to as TVOD for multi-target tracking using the\nvisual tracker and generic object detector. We achieve competitive results with\nlower identity switches on several datasets comparing to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:11:34 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Taalimi", "Ali", ""], ["Liu", "Liu", ""], ["Qi", "Hairong", ""]]}, {"id": "1705.10732", "submitter": "Tong Zhang", "authors": "Tong Zhang (1 and 2), Wenming Zheng (2), Zhen Cui (3), Chaolong Li (2)\n  ((1) the Department of Information Science and Engineering, Southeast\n  University, Nanjing, China (2) the Key Laboratory of Child Development and\n  Learning Science of Ministry of Education, Research Center for Learning\n  Science, Southeast University, Nanjing, China (3) School of Computer Science\n  and Engineering, Nanjing University of Science and Technology, Nanjing,\n  China)", "title": "Deep manifold-to-manifold transforming network for action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive definite (SPD) matrices (e.g., covariances, graph\nLaplacians, etc.) are widely used to model the relationship of spatial or\ntemporal domain. Nevertheless, SPD matrices are theoretically embedded on\nRiemannian manifolds. In this paper, we propose an end-to-end deep\nmanifold-to-manifold transforming network (DMT-Net) which can make SPD matrices\nflow from one Riemannian manifold to another more discriminative one. To learn\ndiscriminative SPD features characterizing both spatial and temporal\ndependencies, we specifically develop three novel layers on manifolds: (i) the\nlocal SPD convolutional layer, (ii) the non-linear SPD activation layer, and\n(iii) the Riemannian-preserved recursive layer. The SPD property is preserved\nthrough all layers without any requirement of singular value decomposition\n(SVD), which is often used in the existing methods with expensive computation\ncost. Furthermore, a diagonalizing SPD layer is designed to efficiently\ncalculate the final metric for the classification task. To evaluate our\nproposed method, we conduct extensive experiments on the task of action\nrecognition, where input signals are popularly modeled as SPD matrices. The\nexperimental results demonstrate that our DMT-Net is much more competitive over\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:38:44 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 06:08:16 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 03:09:53 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Zhang", "Tong", "", "1 and 2"], ["Zheng", "Wenming", ""], ["Cui", "Zhen", ""], ["Li", "Chaolong", ""]]}, {"id": "1705.10739", "submitter": "Titus Cieslewski", "authors": "Titus Cieslewski and Davide Scaramuzza", "title": "Efficient Decentralized Visual Place Recognition From Full-Image\n  Descriptors", "comments": "3 pages, 4 figures. This is a self-published paper that accompanies\n  our original work [1] as well as the ICRA 2017 Workshop on Multi-robot\n  Perception-Driven Control and Planning [2]", "journal-ref": "2017 International Symposium on Multi-Robot and Multi-Agent\n  Systems (MRS) 78-82", "doi": "10.1109/MRS.2017.8250934", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the adaptation of our decentralized place\nrecognition method described in [1] to full image descriptors. As we had shown,\nthe key to making a scalable decentralized visual place recognition lies in\nexploting deterministic key assignment in a distributed key-value map. Through\nthis, it is possible to reduce bandwidth by up to a factor of n, the robot\ncount, by casting visual place recognition to a key-value lookup problem. In\n[1], we exploited this for the bag-of-words method [3], [4]. Our method of\ncasting bag-of-words, however, results in a complex decentralized system, which\nhas inherently worse recall than its centralized counterpart. In this paper, we\ninstead start from the recent full-image description method NetVLAD [5]. As we\nshow, casting this to a key-value lookup problem can be achieved with k-means\nclustering, and results in a much simpler system than [1]. The resulting system\nstill has some flaws, albeit of a completely different nature: it suffers when\nthe environment seen during deployment lies in a different distribution in\nfeature space than the environment seen during training.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:43:57 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Cieslewski", "Titus", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1705.10748", "submitter": "Chih-Ting Liu", "authors": "Chih-Ting Liu, Yi-Heng Wu, Yu-Sheng Lin, Shao-Yi Chien", "title": "Computation-Performance Optimization of Convolutional Neural Networks\n  with Redundant Kernel Removal", "comments": "This paper was accepted by 2018 The International Symposium on\n  Circuits and Systems (ISCAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are widely employed in modern\ncomputer vision algorithms, where the input image is convolved iteratively by\nmany kernels to extract the knowledge behind it. However, with the depth of\nconvolutional layers getting deeper and deeper in recent years, the enormous\ncomputational complexity makes it difficult to be deployed on embedded systems\nwith limited hardware resources. In this paper, we propose two\ncomputation-performance optimization methods to reduce the redundant\nconvolution kernels of a CNN with performance and architecture constraints, and\napply it to a network for super resolution (SR). Using PSNR drop compared to\nthe original network as the performance criterion, our method can get the\noptimal PSNR under a certain computation budget constraint. On the other hand,\nour method is also capable of minimizing the computation required under a given\nPSNR drop.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:59:46 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:10:54 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 16:34:56 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Liu", "Chih-Ting", ""], ["Wu", "Yi-Heng", ""], ["Lin", "Yu-Sheng", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "1705.10762", "submitter": "Ramakrishna Vedantam", "authors": "Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy", "title": "Generative Models of Visually Grounded Imagination", "comments": "International Conference on Learning Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is easy for people to imagine what a man with pink hair looks like, even\nif they have never seen such a person before. We call the ability to create\nimages of novel semantic concepts visually grounded imagination. In this paper,\nwe show how we can modify variational auto-encoders to perform this task. Our\nmethod uses a novel training objective, and a novel product-of-experts\ninference network, which can handle partially specified (abstract) concepts in\na principled and efficient way. We also propose a set of easy-to-compute\nevaluation metrics that capture our intuitive notions of what it means to have\ngood visual imagination, namely correctness, coverage, and compositionality\n(the 3 C's). Finally, we perform a detailed comparison of our method with two\nexisting joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.\nand the BiVCCA method of Wang et.al.) by applying them to two datasets: the\nMNIST-with-attributes dataset (which we introduce here), and the CelebA\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:32:26 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 22:38:31 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 21:10:46 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 18:38:45 GMT"}, {"version": "v5", "created": "Wed, 15 Nov 2017 04:58:10 GMT"}, {"version": "v6", "created": "Mon, 12 Feb 2018 05:04:18 GMT"}, {"version": "v7", "created": "Sun, 25 Feb 2018 18:14:07 GMT"}, {"version": "v8", "created": "Fri, 9 Nov 2018 08:16:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Fischer", "Ian", ""], ["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1705.10768", "submitter": "Hua Li", "authors": "Erbo Li and Hua Li", "title": "Reflection Invariant and Symmetry Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry detection and discrimination are of fundamental meaning in science,\ntechnology, and engineering. This paper introduces reflection invariants and\ndefines the directional moment to detect symmetry for shape analysis and object\nrecognition. And it demonstrates that detection of reflection symmetry can be\ndone in a simple way by solving a trigonometric system derived from the\ndirectional moment, and discrimination of reflection symmetry can be achieved\nby application of the reflection invariants in 2D and 3D. Rotation symmetry can\nalso be determined based on that.The experiments in 2D and 3D, including the\nregular triangle, the square, and the five Platonic objects, show that all the\nreflection lines or planes can be deterministically found using directional\nmoments up to order six. This result can be used to simplify the efforts of\nsymmetry detection in research areas, such as protein structure, model\nretrieval, inverse engineering, and machine vision etc.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:45:02 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 19:03:27 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Li", "Erbo", ""], ["Li", "Hua", ""]]}, {"id": "1705.10784", "submitter": "Yue Zhang", "authors": "Weihong Guo, Guohui Song, Yue Zhang", "title": "PCM-TV-TFV: A Novel Two Stage Framework for Image Reconstruction from\n  Fourier Data", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a novel two-stage Projection Correction Modeling\n(PCM) framework for image reconstruction from (non-uniform) Fourier\nmeasurements. PCM consists of a projection stage (P-stage) motivated by the\nmulti-scale Galerkin method and a correction stage (C-stage) with an edge\nguided regularity fusing together the advantages of total variation (TV) and\ntotal fractional variation (TFV). The P-stage allows for continuous modeling of\nthe underlying image of interest. The given measurements are projected onto a\nspace in which the image is well represented. We then enhance the\nreconstruction result at the C-stage that minimizes an energy functional\nconsisting of a fidelity in the transformed domain and a novel edge guided\nregularity. We further develop efficient proximal algorithms to solve the\ncorresponding optimization problem. Various numerical results in both 1D\nsignals and 2D images have also been presented to demonstrate the superior\nperformance of the proposed two-stage method to other classical one-stage\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 22:24:34 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Guo", "Weihong", ""], ["Song", "Guohui", ""], ["Zhang", "Yue", ""]]}, {"id": "1705.10823", "submitter": "Otkrist Gupta", "authors": "Bowen Baker, Otkrist Gupta, Ramesh Raskar and Nikhil Naik", "title": "Accelerating Neural Architecture Search using Performance Prediction", "comments": "Submitted to International Conference on Learning Representations,\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for neural network hyperparameter optimization and meta-modeling are\ncomputationally expensive due to the need to train a large number of model\nconfigurations. In this paper, we show that standard frequentist regression\nmodels can predict the final performance of partially trained model\nconfigurations using features based on network architectures, hyperparameters,\nand time-series validation performance data. We empirically show that our\nperformance prediction models are much more effective than prominent Bayesian\ncounterparts, are simpler to implement, and are faster to train. Our models can\npredict final performance in both visual classification and language modeling\ndomains, are effective for predicting performance of drastically varying model\narchitectures, and can even generalize between model classes. Using these\nprediction models, we also propose an early stopping method for hyperparameter\noptimization and meta-modeling, which obtains a speedup of a factor up to 6x in\nboth hyperparameter optimization and meta-modeling. Finally, we empirically\nshow that our early stopping method can be seamlessly incorporated into both\nreinforcement learning-based architecture selection algorithms and bandit based\nsearch methods. Through extensive experimentation, we empirically show our\nperformance prediction models and early stopping algorithm are state-of-the-art\nin terms of prediction accuracy and speedup achieved while still identifying\nthe optimal model configurations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:00:53 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:09:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Baker", "Bowen", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""], ["Naik", "Nikhil", ""]]}, {"id": "1705.10861", "submitter": "Jiawei He", "authors": "Jiawei He, Mostafa S. Ibrahim, Zhiwei Deng, Greg Mori", "title": "Generic Tubelet Proposals for Action Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel framework for action localization in videos. We propose\nthe Tube Proposal Network (TPN), which can generate generic, class-independent,\nvideo-level tubelet proposals in videos. The generated tubelet proposals can be\nutilized in various video analysis tasks, including recognizing and localizing\nactions in videos. In particular, we integrate these generic tubelet proposals\ninto a unified temporal deep network for action classification. Compared with\nother methods, our generic tubelet proposal method is accurate, general, and is\nfully differentiable under a smoothL1 loss function. We demonstrate the\nperformance of our algorithm on the standard UCF-Sports, J-HMDB21, and UCF-101\ndatasets. Our class-independent TPN outperforms other tubelet generation\nmethods, and our unified temporal deep network achieves state-of-the-art\nlocalization results on all three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 20:38:31 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["He", "Jiawei", ""], ["Ibrahim", "Mostafa S.", ""], ["Deng", "Zhiwei", ""], ["Mori", "Greg", ""]]}, {"id": "1705.10872", "submitter": "Dmytro Mishkin", "authors": "Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas", "title": "Working hard to know your neighbor's margins: Local descriptor learning\n  loss", "comments": "Post-NIPS-2017 update. Better hyperparameters and better results on\n  HPatches + Brown dataset, + couple of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 21:23:19 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 11:13:55 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 14:03:14 GMT"}, {"version": "v4", "created": "Fri, 12 Jan 2018 14:40:46 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Mishchuk", "Anastasiya", ""], ["Mishkin", "Dmytro", ""], ["Radenovic", "Filip", ""], ["Matas", "Jiri", ""]]}, {"id": "1705.10882", "submitter": "David Rolnick", "authors": "David Rolnick, Yaron Meirovitch, Toufiq Parag, Hanspeter Pfister,\n  Viren Jain, Jeff W. Lichtman, Edward S. Boyden, Nir Shavit", "title": "Morphological Error Detection in 3D Segmentations", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms for connectomics rely upon localized classification,\nrather than overall morphology. This leads to a high incidence of erroneously\nmerged objects. Humans, by contrast, can easily detect such errors by acquiring\nintuition for the correct morphology of objects. Biological neurons have\ncomplicated and variable shapes, which are challenging to learn, and merge\nerrors take a multitude of different forms. We present an algorithm, MergeNet,\nthat shows 3D ConvNets can, in fact, detect merge errors from high-level\nneuronal morphology. MergeNet follows unsupervised training and operates across\ndatasets. We demonstrate the performance of MergeNet both on a variety of\nconnectomics data and on a dataset created from merged MNIST images.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 22:25:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rolnick", "David", ""], ["Meirovitch", "Yaron", ""], ["Parag", "Toufiq", ""], ["Pfister", "Hanspeter", ""], ["Jain", "Viren", ""], ["Lichtman", "Jeff W.", ""], ["Boyden", "Edward S.", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10887", "submitter": "Javier Turek", "authors": "Javier S. Turek, Alexander Huth", "title": "Efficient, sparse representation of manifold distance matrices for\n  classical scaling", "comments": "Conference CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic distance matrices can reveal shape properties that are largely\ninvariant to non-rigid deformations, and thus are often used to analyze and\nrepresent 3-D shapes. However, these matrices grow quadratically with the\nnumber of points. Thus for large point sets it is common to use a low-rank\napproximation to the distance matrix, which fits in memory and can be\nefficiently analyzed using methods such as multidimensional scaling (MDS). In\nthis paper we present a novel sparse method for efficiently representing\ngeodesic distance matrices using biharmonic interpolation. This method exploits\nknowledge of the data manifold to learn a sparse interpolation operator that\napproximates distances using a subset of points. We show that our method is 2x\nfaster and uses 20x less memory than current leading methods for solving MDS on\nlarge point sets, with similar quality. This enables analyses of large point\nsets that were previously infeasible.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:18:18 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:35:03 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Turek", "Javier S.", ""], ["Huth", "Alexander", ""]]}, {"id": "1705.10904", "submitter": "JunYoung Gwak", "authors": "JunYoung Gwak, Christopher B. Choy, Animesh Garg, Manmohan Chandraker,\n  Silvio Savarese", "title": "Weakly supervised 3D Reconstruction with Adversarial Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised 3D reconstruction has witnessed a significant progress through the\nuse of deep neural networks. However, this increase in performance requires\nlarge scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D\nsupervision as an alternative for expensive 3D CAD annotation. Specifically, we\nuse foreground masks as weak supervision through a raytrace pooling layer that\nenables perspective projection and backpropagation. Additionally, since the 3D\nreconstruction from masks is an ill posed problem, we propose to constrain the\n3D reconstruction to the manifold of unlabeled realistic 3D shapes that match\nmask observations. We demonstrate that learning a log-barrier solution to this\nconstrained optimization problem resembles the GAN objective, enabling the use\nof existing tools for training GANs. We evaluate and analyze the manifold\nconstrained reconstruction on various datasets for single and multi-view\nreconstruction of both synthetic and real images.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 01:00:34 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 05:45:38 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Gwak", "JunYoung", ""], ["Choy", "Christopher B.", ""], ["Garg", "Animesh", ""], ["Chandraker", "Manmohan", ""], ["Savarese", "Silvio", ""]]}, {"id": "1705.10915", "submitter": "Emily Denton", "authors": "Emily Denton, Vighnesh Birodkar", "title": "Unsupervised Learning of Disentangled Representations from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model DrNET that learns disentangled image representations\nfrom video. Our approach leverages the temporal coherence of video and a novel\nadversarial loss to learn a representation that factorizes each frame into a\nstationary part and a temporally varying component. The disentangled\nrepresentation can be used for a range of tasks. For example, applying a\nstandard LSTM to the time-vary components enables prediction of future frames.\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\nthe ability to coherently generate hundreds of steps into the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:12:19 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Denton", "Emily", ""], ["Birodkar", "Vighnesh", ""]]}, {"id": "1705.10928", "submitter": "You Hao", "authors": "Ming Gong and You Hao and Hanlin Mo and Hua Li", "title": "Naturally Combined Shape-Color Moment Invariants under Affine\n  Transformations", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2017.07.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a kind of naturally combined shape-color affine moment invariants\n(SCAMI), which consider both shape and color affine transformations\nsimultaneously in one single system. In the real scene, color and shape\ndeformations always exist in images simultaneously. Simple shape invariants or\ncolor invariants can not be qualified for this situation. The conventional\nmethod is just to make a simple linear combination of the two factors.\nMeanwhile, the manual selection of weights is a complex issue. Our construction\nmethod is based on the multiple integration framework. The integral kernel is\nassigned as the continued product of the shape and color invariant cores. It is\nthe first time to directly derive an invariant to dual affine transformations\nof shape and color. The manual selection of weights is no longer necessary, and\nboth the shape and color transformations are extended to affine transformation\ngroup. With the various of invariant cores, a set of lower-order invariants are\nconstructed and the completeness and independence are discussed detailedly. A\nset of SCAMIs, which called SCAMI24, are recommended, and the effectiveness and\nrobustness have been evaluated on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:04:35 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 02:44:57 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Gong", "Ming", ""], ["Hao", "You", ""], ["Mo", "Hanlin", ""], ["Li", "Hua", ""]]}, {"id": "1705.10930", "submitter": "Chao Zuo", "authors": "Chao Zuo, Tianyang Tao, Shijie Feng, Lei Huang, Anand Asundi, and Qian\n  Chen", "title": "Micro Fourier Transform Profilometry ($\\mu$FTP): 3D shape measurement at\n  10,000 frames per second", "comments": "This manuscript was originally submitted on 30th January 17", "journal-ref": null, "doi": "10.1016/j.optlaseng.2017.10.013", "report-no": null, "categories": "physics.ins-det cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in imaging sensors and digital light projection technology\nhave facilitated a rapid progress in 3D optical sensing, enabling 3D surfaces\nof complex-shaped objects to be captured with improved resolution and accuracy.\nHowever, due to the large number of projection patterns required for phase\nrecovery and disambiguation, the maximum fame rates of current 3D shape\nmeasurement techniques are still limited to the range of hundreds of frames per\nsecond (fps). Here, we demonstrate a new 3D dynamic imaging technique, Micro\nFourier Transform Profilometry ($\\mu$FTP), which can capture 3D surfaces of\ntransient events at up to 10,000 fps based on our newly developed high-speed\nfringe projection system. Compared with existing techniques, $\\mu$FTP has the\nprominent advantage of recovering an accurate, unambiguous, and dense 3D point\ncloud with only two projected patterns. Furthermore, the phase information is\nencoded within a single high-frequency fringe image, thereby allowing\nmotion-artifact-free reconstruction of transient events with temporal\nresolution of 50 microseconds. To show $\\mu$FTP's broad utility, we use it to\nreconstruct 3D videos of 4 transient scenes: vibrating cantilevers, rotating\nfan blades, bullet fired from a toy gun, and balloon's explosion triggered by a\nflying dart, which were previously difficult or even unable to be captured with\nconventional approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:09:11 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zuo", "Chao", ""], ["Tao", "Tianyang", ""], ["Feng", "Shijie", ""], ["Huang", "Lei", ""], ["Asundi", "Anand", ""], ["Chen", "Qian", ""]]}, {"id": "1705.10943", "submitter": "Stefan Sommer", "authors": "Stefan Sommer, Alexis Arnaudon, Line Kuhnel, Sarang Joshi", "title": "Bridge Simulation and Metric Estimation on Landmark Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an inference algorithm and connected Monte Carlo based estimation\nprocedures for metric estimation from landmark configurations distributed\naccording to the transition distribution of a Riemannian Brownian motion\narising from the Large Deformation Diffeomorphic Metric Mapping (LDDMM) metric.\nThe distribution possesses properties similar to the regular Euclidean normal\ndistribution but its transition density is governed by a high-dimensional PDE\nwith no closed-form solution in the nonlinear case. We show how the density can\nbe numerically approximated by Monte Carlo sampling of conditioned Brownian\nbridges, and we use this to estimate parameters of the LDDMM kernel and thus\nthe metric structure by maximum likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 05:08:08 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Sommer", "Stefan", ""], ["Arnaudon", "Alexis", ""], ["Kuhnel", "Line", ""], ["Joshi", "Sarang", ""]]}, {"id": "1705.10986", "submitter": "N Vinay Kumar", "authors": "D. S. Guru and N. Vinay Kumar", "title": "Class Specific Feature Selection for Interval Valued Data Through\n  Interval K-Means Clustering", "comments": "12 Pages, 3 figures, 7 tables", "journal-ref": "RTIP2R 2016, CCIS 709, pp. 228 TO 239, 2017", "doi": "10.1007/978-981-10-4859-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel feature selection approach for supervised interval\nvalued features is proposed. The proposed approach takes care of selecting the\nclass specific features through interval K-Means clustering. The kernel of\nK-Means clustering algorithm is modified to adapt interval valued data. During\ntraining, a set of samples corresponding to a class is fed into the interval\nK-Means clustering algorithm, which clusters features into K distinct clusters.\nHence, there are K number of features corresponding to each class.\nSubsequently, corresponding to each class, the cluster representatives are\nchosen. This procedure is repeated for all the samples of remaining classes.\nDuring testing the feature indices correspond to each class are used for\nvalidating the given dataset through classification using suitable symbolic\nclassifiers. For experimentation, four standard supervised interval datasets\nare used. The results show the superiority of the proposed model when compared\nwith the other existing state-of-the-art feature selection methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 08:43:58 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Guru", "D. S.", ""], ["Kumar", "N. Vinay", ""]]}, {"id": "1705.10999", "submitter": "Qi Li", "authors": "Qi Li, Zhenan Sun, Ran He, Tieniu Tan", "title": "Deep Supervised Discrete Hashing", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of image and video data on the web, hashing has been\nextensively studied for image or video search in recent years. Benefit from\nrecent advances in deep learning, deep hashing methods have achieved promising\nresults for image retrieval. However, there are some limitations of previous\ndeep hashing methods (e.g., the semantic information is not fully exploited).\nIn this paper, we develop a deep supervised discrete hashing algorithm based on\nthe assumption that the learned binary codes should be ideal for\nclassification. Both the pairwise label information and the classification\ninformation are used to learn the hash codes within one stream framework. We\nconstrain the outputs of the last layer to be binary codes directly, which is\nrarely investigated in deep hashing algorithm. Because of the discrete nature\nof hash codes, an alternating minimization method is used to optimize the\nobjective function. Experimental results have shown that our method outperforms\ncurrent state-of-the-art methods on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:16:38 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 14:24:08 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Li", "Qi", ""], ["Sun", "Zhenan", ""], ["He", "Ran", ""], ["Tan", "Tieniu", ""]]}, {"id": "1705.11053", "submitter": "Jianxu Chen", "authors": "Jianxu Chen, Sreya Banerjee, Abhinav Grama, Walter J. Scheirer and\n  Danny Z. Chen", "title": "Neuron Segmentation Using Deep Complete Bipartite Networks", "comments": "miccai 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the problem of automatically segmenting neuronal\ncells in dual-color confocal microscopy images. This problem is a key task in\nvarious quantitative analysis applications in neuroscience, such as tracing\ncell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using\nfully convolutional networks (FCN), has profoundly changed segmentation\nresearch in biomedical imaging. We face two major challenges in this problem.\nFirst, neuronal cells may form dense clusters, making it difficult to correctly\nidentify all individual cells (even to human experts). Consequently,\nsegmentation results of the known FCN-type models are not accurate enough.\nSecond, pixel-wise ground truth is difficult to obtain. Only a limited amount\nof approximate instance-wise annotation can be collected, which makes the\ntraining of FCN models quite cumbersome. We propose a new FCN-type deep\nlearning model, called deep complete bipartite networks (CB-Net), and a new\nscheme for leveraging approximate instance-wise annotation to train our\npixel-wise prediction model. Evaluated using seven real datasets, our proposed\nnew CB-Net model outperforms the state-of-the-art FCN models and produces\nneuron segmentation results of remarkable quality\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 12:15:59 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Chen", "Jianxu", ""], ["Banerjee", "Sreya", ""], ["Grama", "Abhinav", ""], ["Scheirer", "Walter J.", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1705.11077", "submitter": "Yong Man Ro", "authors": "Seong Tae Kim, Yong Man Ro", "title": "EvaluationNet: Can Human Skill be Evaluated by Deep Networks?", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent substantial growth of media such as YouTube, a considerable\nnumber of instructional videos covering a wide variety of tasks are available\nonline. Therefore, online instructional videos have become a rich resource for\nhumans to learn everyday skills. In order to improve the effectiveness of the\nlearning with instructional video, observation and evaluation of the activity\nare required. However, it is difficult to observe and evaluate every activity\nsteps by expert. In this study, a novel deep learning framework which targets\nhuman activity evaluation for learning from instructional video has been\nproposed. In order to deal with the inherent variability of activities, we\npropose to model activity as a structured process. First, action units are\nencoded from dense trajectories with LSTM network. The variable-length action\nunit features are then evaluated by a Siamese LSTM network. By the comparative\nexperiments on public dataset, the effectiveness of the proposed method has\nbeen demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 13:28:01 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Kim", "Seong Tae", ""], ["Ro", "Yong Man", ""]]}, {"id": "1705.11136", "submitter": "Luan Tran", "authors": "Luan Tran, Xi Yin, Xiaoming Liu", "title": "Representation Learning by Rotating Your Faces", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2868350", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large pose discrepancy between two face images is one of the fundamental\nchallenges in automatic face recognition. Conventional approaches to\npose-invariant face recognition either perform face frontalization on, or learn\na pose-invariant representation from, a non-frontal face image. We argue that\nit is more desirable to perform both tasks jointly to allow them to leverage\neach other. To this end, this paper proposes a Disentangled Representation\nlearning-Generative Adversarial Network (DR-GAN) with three distinct novelties.\nFirst, the encoder-decoder structure of the generator enables DR-GAN to learn a\nrepresentation that is both generative and discriminative, which can be used\nfor face image synthesis and pose-invariant face recognition. Second, this\nrepresentation is explicitly disentangled from other face variations such as\npose, through the pose code provided to the decoder and pose estimation in the\ndiscriminator. Third, DR-GAN can take one or multiple images as the input, and\ngenerate one unified identity representation along with an arbitrary number of\nsynthetic face images. Extensive quantitative and qualitative evaluation on a\nnumber of controlled and in-the-wild databases demonstrate the superiority of\nDR-GAN over the state of the art in both learning representations and rotating\nlarge-pose face images.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:18:37 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 22:35:23 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Tran", "Luan", ""], ["Yin", "Xi", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1705.11166", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung, Adam W. Harley, William Seto, Katerina Fragkiadaki", "title": "Adversarial Inverse Graphics Networks: Learning 2D-to-3D Lifting and\n  Image-to-Image Translation from Unpaired Supervision", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2017,\n  pp. 4354-4362", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have developed excellent feed-forward models that learn to map\nimages to desired outputs, such as to the images' latent factors, or to other\nimages, using supervised learning. Learning such mappings from unlabelled data,\nor improving upon supervised models by exploiting unlabelled data, remains\nelusive. We argue that there are two important parts to learning without\nannotations: (i) matching the predictions to the input observations, and (ii)\nmatching the predictions to known priors. We propose Adversarial Inverse\nGraphics networks (AIGNs): weakly supervised neural network models that combine\nfeedback from rendering their predictions, with distribution matching between\ntheir predictions and a collection of ground-truth factors. We apply AIGNs to\n3D human pose estimation and 3D structure and egomotion estimation, and\noutperform models supervised by only paired annotations. We further apply AIGNs\nto facial image transformation using super-resolution and inpainting renderers,\nwhile deliberately adding biases in the ground-truth datasets. Our model\nseamlessly incorporates such biases, rendering input faces towards young, old,\nfeminine, masculine or Tom Cruise-like equivalents (depending on the chosen\nbias), or adding lip and nose augmentations while inpainting concealed lips and\nnoses.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:30:07 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 23:43:36 GMT"}, {"version": "v3", "created": "Sat, 2 Sep 2017 01:10:17 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Harley", "Adam W.", ""], ["Seto", "William", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1705.11175", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa, Deepayan Bhowmik and Andrew Wallace", "title": "Long-term Correlation Tracking using Multi-layer Hybrid Features in\n  Sparse and Dense Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking a target of interest in both sparse and crowded environments is a\nchallenging problem, not yet successfully addressed in the literature. In this\npaper, we propose a new long-term visual tracking algorithm, learning\ndiscriminative correlation filters and using an online classifier, to track a\ntarget of interest in both sparse and crowded video sequences. First, we learn\na translation correlation filter using a multi-layer hybrid of convolutional\nneural networks (CNN) and traditional hand-crafted features. We combine\nadvantages of both the lower convolutional layer which retains more spatial\ndetails for precise localization and the higher convolutional layer which\nencodes semantic information for handling appearance variations, and then\nintegrate these with histogram of oriented gradients (HOG) and color-naming\ntraditional features. Second, we include a re-detection module for overcoming\ntracking failures due to long-term occlusions by training an incremental\n(online) SVM on the most confident frames using hand-engineered features. This\nre-detection module is activated only when the correlation response of the\nobject is below some pre-defined threshold. This generates high score detection\nproposals which are temporally filtered using a Gaussian mixture probability\nhypothesis density (GM-PHD) filter to find the detection proposal with the\nmaximum weight as the target state estimate by removing the other detection\nproposals as clutter. Finally, we learn a scale correlation filter for\nestimating the scale of a target by constructing a target pyramid around the\nestimated or re-detected position using the HOG features. We carry out\nextensive experiments on both sparse and dense data sets which show that our\nmethod significantly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:44:45 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 00:37:05 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 14:54:39 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 09:43:49 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 12:48:02 GMT"}, {"version": "v6", "created": "Sun, 3 Feb 2019 21:19:22 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Baisa", "Nathanael L.", ""], ["Bhowmik", "Deepayan", ""], ["Wallace", "Andrew", ""]]}, {"id": "1705.11187", "submitter": "Aparna Bharati", "authors": "Aparna Bharati, Daniel Moreira, Allan Pinto, Joel Brogan, Kevin\n  Bowyer, Patrick Flynn, Walter Scheirer, Anderson Rocha", "title": "U-Phylogeny: Undirected Provenance Graph Construction in the Wild", "comments": "5 pages, Accepted in International Conference on Image Processing,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving relationships between images and tracing back their history of\nmodifications are at the core of Multimedia Phylogeny solutions, which aim to\ncombat misinformation through doctored visual media. Nonetheless, most recent\nimage phylogeny solutions cannot properly address cases of forged composite\nimages with multiple donors, an area known as multiple parenting phylogeny\n(MPP). This paper presents a preliminary undirected graph construction solution\nfor MPP, without any strict assumptions. The algorithm is underpinned by robust\nimage representative keypoints and different geometric consistency checks among\nmatching regions in both images to provide regions of interest for direct\ncomparison. The paper introduces a novel technique to geometrically filter the\nmost promising matches as well as to aid in the shared region localization\ntask. The strength of the approach is corroborated by experiments with\nreal-world cases, with and without image distractors (unrelated cases).\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:33:31 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Bharati", "Aparna", ""], ["Moreira", "Daniel", ""], ["Pinto", "Allan", ""], ["Brogan", "Joel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Scheirer", "Walter", ""], ["Rocha", "Anderson", ""]]}, {"id": "1705.11192", "submitter": "Serhii Havrylov", "authors": "Serhii Havrylov, Ivan Titov", "title": "Emergence of Language with Multi-agent Games: Learning to Communicate\n  with Sequences of Symbols", "comments": "The paper was accepted at NIPS 2017. The extended abstract was\n  presented at ICLR 2017 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to communicate through interaction, rather than relying on explicit\nsupervision, is often considered a prerequisite for developing a general AI. We\nstudy a setting where two agents engage in playing a referential game and, from\nscratch, develop a communication protocol necessary to succeed in this game.\nUnlike previous work, we require that messages they exchange, both at train and\ntest time, are in the form of a language (i.e. sequences of discrete symbols).\nWe compare a reinforcement learning approach and one using a differentiable\nrelaxation (straight-through Gumbel-softmax estimator) and observe that the\nlatter is much faster to converge and it results in more effective protocols.\nInterestingly, we also observe that the protocol we induce by optimizing the\ncommunication success exhibits a degree of compositionality and variability\n(i.e. the same information can be phrased in different ways), both properties\ncharacteristic of natural languages. As the ultimate goal is to ensure that\ncommunication is accomplished in natural language, we also perform experiments\nwhere we inject prior information about natural language into our model and\nstudy properties of the resulting protocol.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:47:55 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 15:04:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""]]}]