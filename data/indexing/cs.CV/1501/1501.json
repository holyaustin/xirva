[{"id": "1501.00092", "submitter": "Chao Dong", "authors": "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang", "title": "Image Super-Resolution Using Deep Convolutional Networks", "comments": "14 pages, 14 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning method for single image super-resolution (SR). Our\nmethod directly learns an end-to-end mapping between the low/high-resolution\nimages. The mapping is represented as a deep convolutional neural network (CNN)\nthat takes the low-resolution image as the input and outputs the\nhigh-resolution one. We further show that traditional sparse-coding-based SR\nmethods can also be viewed as a deep convolutional network. But unlike\ntraditional methods that handle each component separately, our method jointly\noptimizes all layers. Our deep CNN has a lightweight structure, yet\ndemonstrates state-of-the-art restoration quality, and achieves fast speed for\npractical on-line usage. We explore different network structures and parameter\nsettings to achieve trade-offs between performance and speed. Moreover, we\nextend our network to cope with three color channels simultaneously, and show\nbetter overall reconstruction quality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 08:35:09 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 03:47:06 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 09:13:32 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Dong", "Chao", ""], ["Loy", "Chen Change", ""], ["He", "Kaiming", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1501.00102", "submitter": "Natalia Neverova", "authors": "Natalia Neverova and Christian Wolf and Graham W. Taylor and Florian\n  Nebout", "title": "ModDrop: adaptive multi-modal gesture recognition", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for gesture detection and localisation based on\nmulti-scale and multi-modal deep learning. Each visual modality captures\nspatial information at a particular spatial scale (such as motion of the upper\nbody or a hand), and the whole system operates at three temporal scales. Key to\nour technique is a training strategy which exploits: i) careful initialization\nof individual modalities; and ii) gradual fusion involving random dropping of\nseparate channels (dubbed ModDrop) for learning cross-modality correlations\nwhile preserving uniqueness of each modality-specific representation. We\npresent experiments on the ChaLearn 2014 Looking at People Challenge gesture\nrecognition track, in which we placed first out of 17 teams. Fusing multiple\nmodalities at several spatial and temporal scales leads to a significant\nincrease in recognition rates, allowing the model to compensate for errors of\nthe individual classifiers as well as noise in the separate channels.\nFuthermore, the proposed ModDrop training technique ensures robustness of the\nclassifier to missing signals in one or several channels to produce meaningful\npredictions from any number of available modalities. In addition, we\ndemonstrate the applicability of the proposed fusion scheme to modalities of\narbitrary nature by experiments on the same dataset augmented with audio.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 09:55:43 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 14:46:33 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Taylor", "Graham W.", ""], ["Nebout", "Florian", ""]]}, {"id": "1501.00105", "submitter": "Gholamreza Anbarjafari", "authors": "Gholamreza Anbarjafari", "title": "Face recognition using color local binary pattern from mutually\n  independent color channels", "comments": "11 pages in EURASIP Journal on Image and Video Processing, 2013", "journal-ref": null, "doi": "10.1186/1687-5281-2013-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a high performance face recognition system based on local\nbinary pattern (LBP) using the probability distribution functions (PDF) of\npixels in different mutually independent color channels which are robust to\nfrontal homogenous illumination and planer rotation is proposed. The\nillumination of faces is enhanced by using the state-of-the-art technique which\nis using discrete wavelet transform (DWT) and singular value decomposition\n(SVD). After equalization, face images are segmented by use of local Successive\nMean Quantization Transform (SMQT) followed by skin color based face detection\nsystem. Kullback-Leibler Distance (KLD) between the concatenated PDFs of a\ngiven face obtained by LBP and the concatenated PDFs of each face in the\ndatabase is used as a metric in the recognition process. Various decision\nfusion techniques have been used in order to improve the recognition rate. The\nproposed system has been tested on the FERET, HP, and Bosphorus face databases.\nThe proposed system is compared with conventional and thestate-of-the-art\ntechniques. The recognition rates obtained using FVF approach for FERET\ndatabase is 99.78% compared with 79.60% and 68.80% for conventional gray scale\nLBP and Principle Component Analysis (PCA) based face recognition techniques\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 10:14:25 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Anbarjafari", "Gholamreza", ""]]}, {"id": "1501.00108", "submitter": "Gholamreza Anbarjafari", "authors": "Gholamreza Anbarjafari", "title": "HSI based colour image equalization using iterative nth root and nth\n  power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an equalization technique for colour images is introduced. The\nmethod is based on nth root and nth power equalization approach but with\noptimization of the mean of the image in different colour channels such as RGB\nand HSI. The performance of the proposed method has been measured by the means\nof peak signal to noise ratio. The proposed algorithm has been compared with\nconventional histogram equalization and the visual and quantitative\nexperimental results are showing that the proposed method over perform the\nhistogram equalization.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 10:53:50 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Anbarjafari", "Gholamreza", ""]]}, {"id": "1501.00614", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Stephen Mussmann, Alla Petrakova, Niels da Vitoria\n  Lobo and Mubarak Shah", "title": "Understanding Trajectory Behavior: A Motion Pattern Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining the underlying patterns in gigantic and complex data is of great\nimportance to data analysts. In this paper, we propose a motion pattern\napproach to mine frequent behaviors in trajectory data. Motion patterns,\ndefined by a set of highly similar flow vector groups in a spatial locality,\nhave been shown to be very effective in extracting dominant motion behaviors in\nvideo sequences. Inspired by applications and properties of motion patterns, we\nhave designed a framework that successfully solves the general task of\ntrajectory clustering. Our proposed algorithm consists of four phases: flow\nvector computation, motion component extraction, motion component's\nreachability set creation, and motion pattern formation. For the first phase,\nwe break down trajectories into flow vectors that indicate instantaneous\nmovements. In the second phase, via a Kmeans clustering approach, we create\nmotion components by clustering the flow vectors with respect to their location\nand velocity. Next, we create motion components' reachability set in terms of\nspatial proximity and motion similarity. Finally, for the fourth phase, we\ncluster motion components using agglomerative clustering with the weighted\nJaccard distance between the motion components' signatures, a set created using\npath reachability. We have evaluated the effectiveness of our proposed method\nin an extensive set of experiments on diverse datasets. Further, we have shown\nhow our proposed method handles difficulties in the general task of trajectory\nclustering that challenge the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 00:07:00 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Mussmann", "Stephen", ""], ["Petrakova", "Alla", ""], ["Lobo", "Niels da Vitoria", ""], ["Shah", "Mubarak", ""]]}, {"id": "1501.00630", "submitter": "Yuehaw Khoo", "authors": "Yuehaw Khoo, Ankur Kapoor", "title": "Non-iterative rigid 2D/3D point-set registration using semidefinite\n  programming", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2540810", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a convex programming framework for pose estimation in 2D/3D\npoint-set registration with unknown point correspondences. We give two\nmixed-integer nonlinear program (MINP) formulations of the 2D/3D registration\nproblem when there are multiple 2D images, and propose convex relaxations for\nboth of the MINPs to semidefinite programs (SDP) that can be solved efficiently\nby interior point methods. Our approach to the 2D/3D registration problem is\nnon-iterative in nature as we jointly solve for pose and correspondence.\nFurthermore, these convex programs can readily incorporate feature descriptors\nof points to enhance registration results. We prove that the convex programs\nexactly recover the solution to the original nonconvex 2D/3D registration\nproblem under noiseless condition. We apply these formulations to the\nregistration of 3D models of coronary vessels to their 2D projections obtained\nfrom multiple intra-operative fluoroscopic images. For this application, we\nexperimentally corroborate the exact recovery property in the absence of noise\nand further demonstrate robustness of the convex programs in the presence of\nnoise.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 04:01:25 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 06:09:36 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 04:51:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Khoo", "Yuehaw", ""], ["Kapoor", "Ankur", ""]]}, {"id": "1501.00642", "submitter": "Chunhua Shen", "authors": "Chao Zhang, Chunhua Shen, Tingzhi Shen", "title": "Unsupervised Feature Learning for Dense Correspondences across Scenes", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast, accurate matching method for estimating dense pixel\ncorrespondences across scenes. It is a challenging problem to estimate dense\npixel correspondences between images depicting different scenes or instances of\nthe same object category. While most such matching methods rely on hand-crafted\nfeatures such as SIFT, we learn features from a large amount of unlabeled image\npatches using unsupervised learning. Pixel-layer features are obtained by\nencoding over the dictionary, followed by spatial pooling to obtain patch-layer\nfeatures. The learned features are then seamlessly embedded into a multi-layer\nmatch- ing framework. We experimentally demonstrate that the learned features,\ntogether with our matching model, outperforms state-of-the-art methods such as\nthe SIFT flow, coherency sensitive hashing and the recent deformable spatial\npyramid matching methods both in terms of accuracy and computation efficiency.\nFurthermore, we evaluate the performance of a few different dictionary learning\nand feature encoding methods in the proposed pixel correspondences estimation\nframework, and analyse the impact of dictionary learning and feature encoding\nwith respect to the final matching performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 06:48:24 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 09:58:37 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Zhang", "Chao", ""], ["Shen", "Chunhua", ""], ["Shen", "Tingzhi", ""]]}, {"id": "1501.00680", "submitter": "Ricardo Monge", "authors": "Osvaldo Skliar and Ricardo E. Monge and Sherry Gapper", "title": "A New Method for Signal and Image Analysis: The Square Wave Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brief review is provided of the use of the Square Wave Method (SWM) in the\nfield of signal and image analysis and it is specified how results thus\nobtained are expressed using the Square Wave Transform (SWT), in the frequency\ndomain. To illustrate the new approach introduced in this field, the results of\ntwo cases are analyzed: a) a sequence of samples (that is, measured values) of\nan electromyographic recording; and b) the classic image of Lenna.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 14:35:58 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Skliar", "Osvaldo", ""], ["Monge", "Ricardo E.", ""], ["Gapper", "Sherry", ""]]}, {"id": "1501.00752", "submitter": "Alexander Wong", "authors": "Mohammad Shafiee, Zohreh Azimifar, and Alexander Wong", "title": "A Deep-structured Conditional Random Field Model for Object Silhouette\n  Tracking", "comments": "17 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0133036", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a deep-structured conditional random field\n(DS-CRF) model for the purpose of state-based object silhouette tracking. The\nproposed DS-CRF model consists of a series of state layers, where each state\nlayer spatially characterizes the object silhouette at a particular point in\ntime. The interactions between adjacent state layers are established by\ninter-layer connectivity dynamically determined based on inter-frame optical\nflow. By incorporate both spatial and temporal context in a dynamic fashion\nwithin such a deep-structured probabilistic graphical model, the proposed\nDS-CRF model allows us to develop a framework that can accurately and\nefficiently track object silhouettes that can change greatly over time, as well\nas under different situations such as occlusion and multiple targets within the\nscene. Experiment results using video surveillance datasets containing\ndifferent scenarios such as occlusion and multiple targets showed that the\nproposed DS-CRF approach provides strong object silhouette tracking performance\nwhen compared to baseline methods such as mean-shift tracking, as well as\nstate-of-the-art methods such as context tracking and boosted particle\nfiltering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:09:34 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 18:27:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Shafiee", "Mohammad", ""], ["Azimifar", "Zohreh", ""], ["Wong", "Alexander", ""]]}, {"id": "1501.00756", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an, Ramin Raziperchikolaei", "title": "Hashing with binary autoencoders", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attractive approach for fast search in image databases is binary hashing,\nwhere each high-dimensional, real-valued image is mapped onto a\nlow-dimensional, binary vector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves binary\nconstraints, and most approaches approximate the optimization by relaxing the\nconstraints and then binarizing the result. Here, we focus on the binary\nautoencoder model, which seeks to reconstruct an image from the binary code\nproduced by the hash function. We show that the optimization can be simplified\nwith the method of auxiliary coordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder and decoder\nseparately, and one that optimizes the code for each image. Image retrieval\nexperiments, using precision/recall and a measure of code utilization, show the\nresulting hash function outperforms or is competitive with state-of-the-art\nmethods for binary hashing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:49:02 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1501.00777", "submitter": "Jun Li", "authors": "Jun Li, Heyou Chang, Jian Yang", "title": "Sparse Deep Stacking Network for Image Classification", "comments": "8 pages, 3 figures, AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sparse coding can learn good robust representation to noise and model more\nhigher-order representation for image classification. However, the inference\nalgorithm is computationally expensive even though the supervised signals are\nused to learn compact and discriminative dictionaries in sparse coding\ntechniques. Luckily, a simplified neural network module (SNNM) has been\nproposed to directly learn the discriminative dictionaries for avoiding the\nexpensive inference. But the SNNM module ignores the sparse representations.\nTherefore, we propose a sparse SNNM module by adding the mixed-norm\nregularization (l1/l2 norm). The sparse SNNM modules are further stacked to\nbuild a sparse deep stacking network (S-DSN). In the experiments, we evaluate\nS-DSN with four databases, including Extended YaleB, AR, 15 scene and\nCaltech101. Experimental results show that our model outperforms related\nclassification methods with only a linear classifier. It is worth noting that\nwe reach 98.8% recognition accuracy on 15 scene.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 08:07:31 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Li", "Jun", ""], ["Chang", "Heyou", ""], ["Yang", "Jian", ""]]}, {"id": "1501.00825", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Shuicheng Yan, Yi Yang, Mohan S Kankanhalli, Shipeng\n  Li, Jingdong Wang", "title": "Group $K$-Means", "comments": "The developed algorithm is similar with \"Christopher F. Barnes, A new\n  multiple path search technique for residual vector quantizers, 1994\", but we\n  conduct the research independently and apply it in data/feature compression\n  and image retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to learn multiple dictionaries from a dataset, and approximate\nany data point by the sum of the codewords each chosen from the corresponding\ndictionary. Although theoretically low approximation errors can be achieved by\nthe global solution, an effective solution has not been well studied in\npractice. To solve the problem, we propose a simple yet effective algorithm\n\\textit{Group $K$-Means}. Specifically, we take each dictionary, or any two\nselected dictionaries, as a group of $K$-means cluster centers, and then deal\nwith the approximation issue by minimizing the approximation errors. Besides,\nwe propose a hierarchical initialization for such a non-convex problem.\nExperimental results well validate the effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 11:43:26 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Wang", "Jianfeng", ""], ["Yan", "Shuicheng", ""], ["Yang", "Yi", ""], ["Kankanhalli", "Mohan S", ""], ["Li", "Shipeng", ""], ["Wang", "Jingdong", ""]]}, {"id": "1501.00834", "submitter": "Shun Kataoka", "authors": "Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda and Masayuki Ohzeki", "title": "Inverse Renormalization Group Transformation in Bayesian Image\n  Segmentations", "comments": "6 pages, 2 figures", "journal-ref": "Journal of the Physical Society of Japan 84 (2015) 045001", "doi": "10.7566/JPSJ.84.045001", "report-no": null, "categories": "cs.CV cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian image segmentation algorithm is proposed by combining a loopy\nbelief propagation with an inverse real space renormalization group\ntransformation to reduce the computational time. In results of our experiment,\nwe observe that the proposed method can reduce the computational time to less\nthan one-tenth of that taken by conventional Bayesian approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 12:20:09 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Tanaka", "Kazuyuki", ""], ["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Ohzeki", "Masayuki", ""]]}, {"id": "1501.00857", "submitter": "Mathieu Fauvel", "authors": "Mathieu Fauvel, Clement Dechesne, Anthony Zullo and Fr\\'ed\\'eric\n  Ferraty", "title": "Fast forward feature selection for the nonlinear classification of\n  hyperspectral images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast forward feature selection algorithm is presented in this paper. It is\nbased on a Gaussian mixture model (GMM) classifier. GMM are used for\nclassifying hyperspectral images. The algorithm selects iteratively spectral\nfeatures that maximizes an estimation of the classification rate. The\nestimation is done using the k-fold cross validation. In order to perform fast\nin terms of computing time, an efficient implementation is proposed. First, the\nGMM can be updated when the estimation of the classification rate is computed,\nrather than re-estimate the full model. Secondly, using marginalization of the\nGMM, sub models can be directly obtained from the full model learned with all\nthe spectral features. Experimental results for two real hyperspectral data\nsets show that the method performs very well in terms of classification\naccuracy and processing time. Furthermore, the extracted model contains very\nfew spectral channels.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 13:37:37 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Fauvel", "Mathieu", ""], ["Dechesne", "Clement", ""], ["Zullo", "Anthony", ""], ["Ferraty", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1501.00901", "submitter": "Yubin Deng", "authors": "Yubin Deng, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Learning to Recognize Pedestrian Attribute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to recognize pedestrian attributes at far distance is a challenging\nproblem in visual surveillance since face and body close-shots are hardly\navailable; instead, only far-view image frames of pedestrian are given. In this\nstudy, we present an alternative approach that exploits the context of\nneighboring pedestrian images for improved attribute inference compared to the\nconventional SVM-based method. In addition, we conduct extensive experiments to\nevaluate the informativeness of background and foreground features for\nattribute recognition. Experiments are based on our newly released pedestrian\nattribute dataset, which is by far the largest and most diverse of its kind.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 15:53:01 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 06:35:50 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Deng", "Yubin", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1501.00909", "submitter": "Pengpeng Liang", "authors": "Pengpeng Liang, Chunyuan Liao, Xue Mei, and Haibin Ling", "title": "Adaptive Objectness for Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2556706", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is a long standing problem in vision. While great efforts\nhave been spent to improve tracking performance, a simple yet reliable prior\nknowledge is left unexploited: the target object in tracking must be an object\nother than non-object. The recently proposed and popularized objectness measure\nprovides a natural way to model such prior in visual tracking. Thus motivated,\nin this paper we propose to adapt objectness for visual object tracking.\nInstead of directly applying an existing objectness measure that is generic and\nhandles various objects and environments, we adapt it to be compatible to the\nspecific tracking sequence and object. More specifically, we use the newly\nproposed BING objectness as the base, and then train an object-adaptive\nobjectness for each tracking task. The training is implemented by using an\nadaptive support vector machine that integrates information from the specific\ntracking target into the BING measure. We emphasize that the benefit of the\nproposed adaptive objectness, named ADOBING, is generic. To show this, we\ncombine ADOBING with seven top performed trackers in recent evaluations. We run\nthe ADOBING-enhanced trackers with their base trackers on two popular\nbenchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking\nBenchmark (100 sequences). On both benchmarks, our methods not only\nconsistently improve the base trackers, but also achieve the best known\nperformances. Noting that the way we integrate objectness in visual tracking is\ngeneric and straightforward, we expect even more improvement by using\ntracker-specific objectness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 16:24:37 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Liang", "Pengpeng", ""], ["Liao", "Chunyuan", ""], ["Mei", "Xue", ""], ["Ling", "Haibin", ""]]}, {"id": "1501.01075", "submitter": "Omar Abuzaghleh", "authors": "Omar Abuzaghleh, Miad Faezipour and Buket D. Barkana", "title": "Skincure: An Innovative Smart Phone-Based Application To Assist In\n  Melanoma Early Detection And Prevention", "comments": "appears in Signal & Image Processing : An International Journal\n  (SIPIJ) Vol.5, No.6, December 2014", "journal-ref": null, "doi": "10.5121/sipij.2014.5601", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma spreads through metastasis, and therefore it has been proven to be\nvery fatal. Statistical evidence has revealed that the majority of deaths\nresulting from skin cancer are as a result of melanoma. Further investigations\nhave shown that the survival rates in patients depend on the stage of the\ninfection; early detection and intervention of melanoma implicates higher\nchances of cure. Clinical diagnosis and prognosis of melanoma is challenging\nsince the processes are prone to misdiagnosis and inaccuracies due to doctors\nsubjectivity. This paper proposes an innovative and fully functional\nsmart-phone based application to assist in melanoma early detection and\nprevention. The application has two major components; the first component is a\nreal-time alert to help users prevent skin burn caused by sunlight; a novel\nequation to compute the time for skin to burn is thereby introduced. The second\ncomponent is an automated image analysis module which contains image\nacquisition, hair detection and exclusion, lesion segmentation, feature\nextraction, and classification. The proposed system exploits PH2 Dermoscopy\nimage database from Pedro Hispano Hospital for development and testing\npurposes. The image database contains a total of 200 dermoscopy images of\nlesions, including normal, atypical, and melanoma cases. The experimental\nresults show that the proposed system is efficient, achieving classification of\nthe normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and\n97.5%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 04:19:55 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Abuzaghleh", "Omar", ""], ["Faezipour", "Miad", ""], ["Barkana", "Buket D.", ""]]}, {"id": "1501.01083", "submitter": "Mohana S H", "authors": "S.H. Mohana, C.J. Prabhakar", "title": "Stem-Calyx Recognition of an Apple using Shape Descriptors", "comments": "15 pages, 10 figures and 2 tables in Signal & Image Processing : An\n  International Journal (SIPIJ) Vol.5, No.6, December 2014", "journal-ref": null, "doi": "10.5121/sipij.2014.5602", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a novel method to recognize stem - calyx of an apple\nusing shape descriptors. The main drawback of existing apple grading techniques\nis that stem - calyx part of an apple is treated as defects, this leads to poor\ngrading of apples. In order to overcome this drawback, we proposed an approach\nto recognize stem-calyx and differentiated from true defects based on shape\nfeatures. Our method comprises of steps such as segmentation of apple using\ngrow-cut method, candidate objects such as stem-calyx and small defects are\ndetected using multi-threshold segmentation. The shape features are extracted\nfrom detected objects using Multifractal, Fourier and Radon descriptor and\nfinally stem-calyx regions are recognized and differentiated from true defects\nusing SVM classifier. The proposed algorithm is evaluated using experiments\nconducted on apple image dataset and results exhibit considerable improvement\nin recognition of stem-calyx region compared to other techniques.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 05:51:23 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Mohana", "S. H.", ""], ["Prabhakar", "C. J.", ""]]}, {"id": "1501.01090", "submitter": "Mohana S H", "authors": "S.H. Mohana and C.J. Prabhakar", "title": "A Novel Technique for Grading of Dates using Shape and Texture Features", "comments": "15 pages, 3 figures and 6 tables in Machine Learning and\n  Applications: An International Journal (MLAIJ) Vol.1, No.2, December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a novel method to grade the date fruits based on the\ncombination of shape and texture features. The method begins with reducing the\nspecular reflection and small noise using a bilateral filter. Threshold based\nsegmentation is performed for background removal and fruit part selection from\nthe given image. Shape features is extracted using the contour of the date\nfruit and texture features are extracted using Curvelet transform and Local\nBinary Pattern (LBP) from the selected date fruit region. Finally, combinations\nof shape and texture features are fused to grade the dates into six grades.\nk-Nearest Neighbour(k-NN) classifier yields the best grading rate compared to\nother two classifiers such as Support Vector Machine (SVM) and Linear\nDiscriminant(LDA) classifiers. The experiment result shows that our technique\nachieves highest accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 06:52:09 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Mohana", "S. H.", ""], ["Prabhakar", "C. J.", ""]]}, {"id": "1501.01106", "submitter": "Hossein Bakhshi Golestani", "authors": "Hossein Bakhshi Golestani, Mohsen Joneidi, Mostafa Sadeghi", "title": "A Study on Clustering for Clustering Based Image De-Noising", "comments": "9 pages, 8 figures, Journal of Information Systems and\n  Telecommunications (JIST)", "journal-ref": "Journal of Information Systems and Telecommunications (JIST), vol.\n  2, no. 4, pp. 196-204, December 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of de-noising of an image contaminated with\nAdditive White Gaussian Noise (AWGN) is studied. This subject is an open\nproblem in signal processing for more than 50 years. Local methods suggested in\nrecent years, have obtained better results than global methods. However by more\nintelligent training in such a way that first, important data is more effective\nfor training, second, clustering in such way that training blocks lie in\nlow-rank subspaces, we can design a dictionary applicable for image de-noising\nand obtain results near the state of the art local methods. In the present\npaper, we suggest a method based on global clustering of image constructing\nblocks. As the type of clustering plays an important role in clustering-based\nde-noising methods, we address two questions about the clustering. The first,\nwhich parts of the data should be considered for clustering? and the second,\nwhat data clustering method is suitable for de-noising.? Then clustering is\nexploited to learn an over complete dictionary. By obtaining sparse\ndecomposition of the noisy image blocks in terms of the dictionary atoms, the\nde-noised version is achieved. In addition to our framework, 7 popular\ndictionary learning methods are simulated and compared. The results are\ncompared based on two major factors: (1) de-noising performance and (2)\nexecution time. Experimental results show that our dictionary learning\nframework outperforms its competitors in terms of both factors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 08:09:08 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Golestani", "Hossein Bakhshi", ""], ["Joneidi", "Mohsen", ""], ["Sadeghi", "Mostafa", ""]]}, {"id": "1501.01181", "submitter": "Alexander Vezhnevets", "authors": "Alexander Vezhnevets and Vittorio Ferrari", "title": "Object localization in ImageNet by looking out of the window", "comments": "in BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for annotating the location of objects in ImageNet.\nTraditionally, this is cast as an image window classification problem, where\neach window is considered independently and scored based on its appearance\nalone. Instead, we propose a method which scores each candidate window in the\ncontext of all other windows in the image, taking into account their similarity\nin appearance space as well as their spatial relations in the image plane. We\ndevise a fast and exact procedure to optimize our scoring function over all\ncandidate windows in an image, and we learn its parameters using structured\noutput regression. We demonstrate on 92000 images from ImageNet that this\nsignificantly improves localization over recent techniques that score windows\nin isolation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 14:03:39 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 14:55:33 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Vezhnevets", "Alexander", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1501.01186", "submitter": "Vicky Kalogeiton", "authors": "Vicky Kalogeiton, Vittorio Ferrari and Cordelia Schmid", "title": "Analysing domain shift factors between videos and images for object\n  detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the most important challenges in computer vision.\nObject detectors are usually trained on bounding-boxes from still images.\nRecently, video has been used as an alternative source of data. Yet, for a\ngiven test domain (image or video), the performance of the detector depends on\nthe domain it was trained on. In this paper, we examine the reasons behind this\nperformance gap. We define and evaluate different domain shift factors: spatial\nlocation accuracy, appearance diversity, image quality and aspect distribution.\nWe examine the impact of these factors by comparing performance before and\nafter factoring them out. The results show that all four factors affect the\nperformance of the detectors and their combined effect explains nearly the\nwhole performance gap.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 14:13:07 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 19:35:48 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 12:57:34 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Kalogeiton", "Vicky", ""], ["Ferrari", "Vittorio", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1501.01266", "submitter": "Luke Oeding", "authors": "Luke Oeding", "title": "The Quadrifocal Variety", "comments": "20 pages, updates to references, introduction, and main conjecture,\n  and added ancillary files. The article has been accepted to LAA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view Geometry is reviewed from an Algebraic Geometry perspective and\nmulti-focal tensors are constructed as equivariant projections of the\nGrassmannian. A connection to the principal minor assignment problem is made by\nconsidering several flatlander cameras. The ideal of the quadrifocal variety is\ncomputed up to degree 8 (and partially in degree 9) using the representations\nof $\\operatorname{GL}(3)^{\\times 4}$ in the polynomial ring on the space of $3\n\\times 3 \\times 3 \\times 3$ tensors. Further representation-theoretic analysis\ngives a lower bound for the number of minimal generators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 19:16:35 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 21:59:09 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 17:45:25 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Oeding", "Luke", ""]]}, {"id": "1501.01364", "submitter": "Bharath Mk", "authors": "S.M. Vaitheeswaran, Bharath M.K., and Gokul M", "title": "Leader Follower Formation Control of Ground Vehicles Using Camshift\n  Based Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous ground vehicles have been designed for the purpose of that relies\non ranging and bearing information received from forward looking camera on the\nFormation control . A visual guidance control algorithm is designed where real\ntime image processing is used to provide feedback signals. The vision subsystem\nand control subsystem work in parallel to accomplish formation control. A\nproportional navigation and line of sight guidance laws are used to estimate\nthe range and bearing information from the leader vehicle using the vision\nsubsystem. The algorithms for vision detection and localization used here are\nsimilar to approaches for many computer vision tasks such as face tracking and\ndetection that are based color-and texture based features, and non-parametric\nContinuously Adaptive Mean-shift algorithms to keep track of the leader. This\nis being proposed for the first time in the leader follower framework. The\nalgorithms are simple but effective for real time and provide an alternate\napproach to traditional based approaches like the Viola Jones algorithm.\nFurther to stabilize the follower to the leader trajectory, the sliding mode\ncontroller is used to dynamically track the leader. The performance of the\nresults is demonstrated in simulation and in practical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 04:00:26 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Vaitheeswaran", "S. M.", ""], ["K.", "Bharath M.", ""], ["M", "Gokul", ""]]}, {"id": "1501.01372", "submitter": "Yuan Xie", "authors": "Yuan Xie", "title": "Weighted Schatten $p$-Norm Minimization for Image Denoising with Local\n  and Nonlocal Regularization", "comments": "This paper has been withdrawn by the author due to a crucial\n  therotical error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a patch-wise low-rank based image denoising method with\nconstrained variational model involving local and nonlocal regularization. On\none hand, recent patch-wise methods can be represented as a low-rank matrix\napproximation problem whose convex relaxation usually depends on nuclear norm\nminimization (NNM). Here, we extend the NNM to the nonconvex schatten p-norm\nminimization with additional weights assigned to different singular values,\nwhich is referred to as the Weighted Schatten p-Norm Minimization (WSNM). An\nefficient algorithm is also proposed to solve the WSNM problem. The proposed\nWSNM not only gives better approximation to the original low-rank assumption,\nbut also considers physical meanings of different data components. On the other\nhand, due to the naive aggregation schema which integrates all the denoised\npatches into a whole image, current patch-wise denoising methods always produce\nvarious degree of artifacts in denoised results. Therefore, to further reduce\nartifacts, a data-driven regularizer called Steering Total Variation (STV)\ncombined with nonlocal TV is derived for a variational model, which imposes\nlocal and nonlocal consistency constraints on the patch-wise denoised image. A\nhighly simple but efficient algorithm is proposed to solve this variational\nmodel with convergence guarantee. Both WSNM and local \\& nonlocal consistent\nregularization are integrated into an iterative restoration framework to\nproduce final results. Extensive experimental testing shows, both qualitatively\nand quantitatively, that the proposed method can effectively remove noise, as\nwell as reduce artifacts compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 06:27:33 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 08:11:07 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2015 16:14:57 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2015 09:37:42 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Xie", "Yuan", ""]]}, {"id": "1501.01548", "submitter": "Akilan Thangarajah Mr", "authors": "Akilan Thangarajah, Buddhapala Wongkaew, Mongkol Ekpanyapong", "title": "Implementation of Auto Monitoring and Short-Message-Service System via\n  GSM Modem", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring\nsystem for any critical operational environments. It detects an undesired event\noccurring in the environment, generates an alert with detailed message and\nsends it to the user to prevent hazards. This system employs a Friendly ARM as\nmain controller while, sensors and terminals to interact with the real world. A\nGSM network is utilized to bridge the communication between monitoring system\nand user. This paper presents details of prototyping the system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 16:40:51 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 04:18:38 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Thangarajah", "Akilan", ""], ["Wongkaew", "Buddhapala", ""], ["Ekpanyapong", "Mongkol", ""]]}, {"id": "1501.01697", "submitter": "Greg Ongie", "authors": "Greg Ongie and Mathews Jacob", "title": "Super-resolution MRI Using Finite Rate of Innovation Curves", "comments": "Conference paper accepted to ISBI 2015. 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage algorithm for the super-resolution of MR images from\ntheir low-frequency k-space samples. In the first stage we estimate a\nresolution-independent mask whose zeros represent the edges of the image. This\nbuilds off recent work extending the theory of sampling signals of finite rate\nof innovation (FRI) to two-dimensional curves. We enable its application to MRI\nby proposing extensions of the signal models allowed by FRI theory, and by\ndeveloping a more robust and efficient means to determine the edge mask. In the\nsecond stage of the scheme, we recover the super-resolved MR image using the\ndiscretized edge mask as an image prior. We evaluate our scheme on simulated\nsingle-coil MR data obtained from analytical phantoms, and compare against\ntotal variation reconstructions. Our experiments show improved performance in\nboth noiseless and noisy settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 01:01:25 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 02:23:40 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Ongie", "Greg", ""], ["Jacob", "Mathews", ""]]}, {"id": "1501.01723", "submitter": "Mohammed Abdelsamea", "authors": "M. Abdelsamea, Marghny H. Mohamed, and Mohamed Bamatraf", "title": "An Effective Image Feature Classiffication using an improved SOM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature classification is a challenging problem in many computer vision\napplications, specifically, in the fields of remote sensing, image analysis and\npattern recognition. In this paper, a novel Self Organizing Map, termed\nimproved SOM (iSOM), is proposed with the aim of effectively classifying\nMammographic images based on their texture feature representation. The main\ncontribution of the iSOM is to introduce a new node structure for the map\nrepresentation and adopting a learning technique based on Kohonen SOM\naccordingly. The main idea is to control, in an unsupervised fashion, the\nweight updating procedure depending on the class reliability of the node,\nduring the weight update time. Experiments held on a real Mammographic images.\nResults showed high accuracy compared to classical SOM and other state-of-art\nclassifiers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 04:05:39 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Abdelsamea", "M.", ""], ["Mohamed", "Marghny H.", ""], ["Bamatraf", "Mohamed", ""]]}, {"id": "1501.01744", "submitter": "Eric Wengrowski", "authors": "Wenjia Yuan, Eric Wengrowski, Kristin J. Dana, Ashwin Ashok, Marco\n  Gruteser, Narayan Mandayam", "title": "Optimal Radiometric Calibration for Camera-Display Communication", "comments": "10 pages, Submitted to CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for communicating between a camera and display by\nembedding and recovering hidden and dynamic information within a displayed\nimage. A handheld camera pointed at the display can receive not only the\ndisplay image, but also the underlying message. These active scenes are\nfundamentally different from traditional passive scenes like QR codes because\nimage formation is based on display emittance, not surface reflectance.\nDetecting and decoding the message requires careful photometric modeling for\ncomputational message recovery. Unlike standard watermarking and steganography\nmethods that lie outside the domain of computer vision, our message recovery\nalgorithm uses illumination to optically communicate hidden messages in real\nworld scenes. The key innovation of our approach is an algorithm that performs\nsimultaneous radiometric calibration and message recovery in one convex\noptimization problem. By modeling the photometry of the system using a\ncamera-display transfer function (CDTF), we derive a physics-based kernel\nfunction for support vector machine classification. We demonstrate that our\nmethod of optimal online radiometric calibration (OORC) leads to an efficient\nand robust algorithm for computational messaging between nine commercial\ncameras and displays.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 06:52:47 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Yuan", "Wenjia", ""], ["Wengrowski", "Eric", ""], ["Dana", "Kristin J.", ""], ["Ashok", "Ashwin", ""], ["Gruteser", "Marco", ""], ["Mandayam", "Narayan", ""]]}, {"id": "1501.02058", "submitter": "Mouloud Kachouane", "authors": "M. Kachouane (USTHB), S. Sahki, M. Lakrouf (CDTA, USTHB), N. Ouadah\n  (CDTA)", "title": "HOG based Fast Human Detection", "comments": null, "journal-ref": "24th International Conference on Microelectronics (ICM), 2012, Dec\n  2012, Alger, Algeria. pp.1 - 4", "doi": "10.1109/ICM.2012.6471380", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects recognition in image is one of the most difficult problems in\ncomputer vision. It is also an important step for the implementation of several\nexisting applications that require high-level image interpretation. Therefore,\nthere is a growing interest in this research area during the last years. In\nthis paper, we present an algorithm for human detection and recognition in\nreal-time, from images taken by a CCD camera mounted on a car-like mobile\nrobot. The proposed technique is based on Histograms of Oriented Gradient (HOG)\nand SVM classifier. The implementation of our detector has provided good\nresults, and can be used in robotics tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 07:46:36 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Kachouane", "M.", "", "USTHB"], ["Sahki", "S.", "", "CDTA, USTHB"], ["Lakrouf", "M.", "", "CDTA, USTHB"], ["Ouadah", "N.", "", "CDTA"]]}, {"id": "1501.02113", "submitter": "Carsten Gottschlich", "authors": "Duy Hoang Thai, Stephan Huckemann, and Carsten Gottschlich", "title": "Filter Design and Performance Evaluation for Fingerprint Image\n  Segmentation", "comments": null, "journal-ref": "PLoS ONE, vol. 11, no. 5, pp. e0154160, May 2016", "doi": "10.1371/journal.pone.0154160", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition plays an important role in many commercial\napplications and is used by millions of people every day, e.g. for unlocking\nmobile phones. Fingerprint image segmentation is typically the first processing\nstep of most fingerprint algorithms and it divides an image into foreground,\nthe region of interest, and background. Two types of error can occur during\nthis step which both have a negative impact on the recognition performance:\n'true' foreground can be labeled as background and features like minutiae can\nbe lost, or conversely 'true' background can be misclassified as foreground and\nspurious features can be introduced. The contribution of this paper is\nthreefold: firstly, we propose a novel factorized directional bandpass (FDB)\nsegmentation method for texture extraction based on the directional Hilbert\ntransform of a Butterworth bandpass (DHBB) filter interwoven with\nsoft-thresholding. Secondly, we provide a manually marked ground truth\nsegmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a\nsystematic performance comparison between the FDB method and four of the most\noften cited fingerprint segmentation algorithms showing that the FDB\nsegmentation method clearly outperforms these four widely used methods. The\nbenchmark and the implementation of the FDB method are made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 12:00:08 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Huckemann", "Stephan", ""], ["Gottschlich", "Carsten", ""]]}, {"id": "1501.02246", "submitter": "Seyedmeysam Khaleghian", "authors": "Seyedmeysam Khaleghian, Anahita Emami, Mohammad Yadegari and Nasser\n  Soltani", "title": "The Effect of Wedge Tip Angles on Stress Intensity Factors in the\n  Contact Problem between Tilted Wedge and a Half Plane with an Edge Crack\n  Using Digital Image Correlation", "comments": "12 pages, 11 figures, The International Conference on Experimental\n  Solid Mechanics and Dynamics (X-MECH-2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first and second mode stress intensity factors (SIFs) of a contact\nproblem between a half-plane with an edge crack and an asymmetric tilted wedge\nwere obtained using experimental method of Digital Image Correlation (DIC). In\nthis technique, displacement and strain fields can be measured using two\ndigital images of the same sample at different stages of loading. However,\nseveral images were taken consequently in each stage of this experiment to\navoid the noise effect. A pair of images of each stage was compared to each\nother. Then, the correlation coefficients between them were studied using a\ncomputer code. The pairs with the correlation coefficient higher than 0.8 were\nselected as the acceptable match for displacement measurements near the crack\ntip. Subsequently, the SIFs of specimens were calculated using displacement\nfields obtained from DIC method. The effect of wedge tips angle on their SIFs\nwas also studied. Moreover, the results of DIC method were compared with the\nresults of photoelasticity method and a close agreement between them was\nobserved.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 05:04:57 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Khaleghian", "Seyedmeysam", ""], ["Emami", "Anahita", ""], ["Yadegari", "Mohammad", ""], ["Soltani", "Nasser", ""]]}, {"id": "1501.02372", "submitter": "Mykhail Uss Ph.D.", "authors": "M. Uss, B. Vozel, V.Lukin, K. Chehdi", "title": "Efficient Rotation-Scaling-Translation Parameters Estimation Based on\n  Fractal Image Model", "comments": "42 pages, 8 figures, 7 tables. Journal paper", "journal-ref": null, "doi": "10.1109/TGRS.2015.2453126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with area-based subpixel image registration under\nrotation-isometric scaling-translation transformation hypothesis. Our approach\nis based on a parametrical modeling of geometrically transformed textural image\nfragments and maximum likelihood estimation of transformation vector between\nthem. Due to the parametrical approach based on the fractional Brownian motion\nmodeling of the local fragments texture, the proposed estimator MLfBm (ML\nstands for \"Maximum Likelihood\" and fBm for \"Fractal Brownian motion\") has the\nability to better adapt to real image texture content compared to other methods\nrelying on universal similarity measures like mutual information or normalized\ncorrelation. The main benefits are observed when assumptions underlying the fBm\nmodel are fully satisfied, e.g. for isotropic normally distributed textures\nwith stationary increments. Experiments on both simulated and real images and\nfor high and weak correlation between registered images show that the MLfBm\nestimator offers significant improvement compared to other state-of-the-art\nmethods. It reduces translation vector, rotation angle and scaling factor\nestimation errors by a factor of about 1.75...2 and it decreases probability of\nfalse match by up to 5 times. Besides, an accurate confidence interval for\nMLfBm estimates can be obtained from the Cramer-Rao lower bound on\nrotation-scaling-translation parameters estimation error. This bound depends on\ntexture roughness, noise level in reference and template images, correlation\nbetween these images and geometrical transformation parameters.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 17:31:11 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 08:59:30 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Uss", "M.", ""], ["Vozel", "B.", ""], ["Lukin", "V.", ""], ["Chehdi", "K.", ""]]}, {"id": "1501.02376", "submitter": "Muhammad Zubair Ahmad", "authors": "Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan and Amir A.\n  Khan", "title": "Simplified vision based automatic navigation for wheat harvesting in low\n  income economies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recent developments in the domain of agricultural robotics have resulted in\ndevelopment of complex and efficient systems. Most of the land owners in the\nSouth Asian region are low income farmers. The agricultural experience for them\nis still a completely manual process. However, the extreme weather conditions,\nheat and flooding, often combine to put a lot of stress on these small land\nowners and the associated labor. In this paper, we propose a prototype for an\nautomated power reaper for the wheat crop. This automated vehicle is navigated\nusing a simple vision based approach employing the low-cost camera and assisted\nGPS. The mechanical platform is driven by three motors controlled through an\ninterface between the proposed vision algorithm and the electrical drive. The\nproposed methodology is applied on some real field scenarios to demonstrate the\nefficiency of the vision based algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 17:55:08 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Ahmad", "Muhammad Zubair", ""], ["Akhtar", "Ayyaz", ""], ["Khan", "Abdul Qadeer", ""], ["Khan", "Amir A.", ""]]}, {"id": "1501.02378", "submitter": "Muhammad Zubair Ahmad", "authors": "Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan, Amir Ali Khan,\n  Muhammad Murtaza Khan", "title": "Low Cost Semi-Autonomous Agricultural Robots In Pakistan-Vision Based\n  Navigation Scalable methodology for wheat harvesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Robots have revolutionized our way of life in recent years.One of the domains\nthat has not yet completely benefited from the robotic automation is the\nagricultural sector. Agricultural Robotics should complement humans in the\narduous tasks during different sub-domains of this sector. Extensive research\nin Agricultural Robotics has been carried out in Japan, USA, Australia and\nGermany focusing mainly on the heavy agricultural machinery. Pakistan is an\nagricultural rich country and its economy and food security are closely tied\nwith agriculture in general and wheat in particular. However, agricultural\nresearch in Pakistan is still carried out using the conventional methodologies.\nThis paper is an attempt to trigger the research in this modern domain so that\nwe can benefit from cost effective and resource efficient autonomous\nagricultural methodologies. This paper focuses on a scalable low cost\nsemi-autonomous technique for wheat harvest which primarily focuses on the\nfarmers with small land holdings. The main focus will be on the vision part of\nthe navigation system deployed by the proposed robot.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 18:14:10 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Ahmad", "Muhammad Zubair", ""], ["Akhtar", "Ayyaz", ""], ["Khan", "Abdul Qadeer", ""], ["Khan", "Amir Ali", ""], ["Khan", "Muhammad Murtaza", ""]]}, {"id": "1501.02379", "submitter": "Muhammad Zubair Ahmad", "authors": "Abdul Qadeer Khan, Ayyaz Akhtar, Muhammad Zubair Ahmad", "title": "Autonomous Farm Vehicles: Prototype of Power Reaper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Chapter 2 will begin with introduction of Agricultural Robotics. There will\nbe a literature review of the mechanical structure, vision and control\nalgorithms. In chapter 3 we will discuss the methodology in detail using block\ndiagrams and flowcharts. The results of the tested and the proposed algorithms\nwill also be displayed. In chapter 4 we will discuss the results in detail and\nhow they are of significance in our work. In chapter 5 we will conclude our\nwork and discuss some future perspectives. In appendices we will provide some\nbackground information necessary regarding this project.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 18:21:48 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Khan", "Abdul Qadeer", ""], ["Akhtar", "Ayyaz", ""], ["Ahmad", "Muhammad Zubair", ""]]}, {"id": "1501.02393", "submitter": "Raviteja Vemulapalli", "authors": "Raviteja Vemulapalli, David W. Jacobs", "title": "Riemannian Metric Learning for Symmetric Positive Definite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, symmetric positive definite (SPD) matrices have been\nreceiving considerable attention from computer vision community. Though various\ndistance measures have been proposed in the past for comparing SPD matrices,\nthe two most widely-used measures are affine-invariant distance and\nlog-Euclidean distance. This is because these two measures are true geodesic\ndistances induced by Riemannian geometry. In this work, we focus on the\nlog-Euclidean Riemannian geometry and propose a data-driven approach for\nlearning Riemannian metrics/geodesic distances for SPD matrices. We show that\nthe geodesic distance learned using the proposed approach performs better than\nvarious existing distance measures when evaluated on face matching and\nclustering tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 21:12:09 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Jacobs", "David W.", ""]]}, {"id": "1501.02530", "submitter": "Anna Senina", "authors": "Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele", "title": "A Dataset for Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:31:33 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Tandon", "Niket", ""], ["Schiele", "Bernt", ""]]}, {"id": "1501.02555", "submitter": "Qin Xiaoqian", "authors": "Xiaoqian Qin, Xiaoyang Tan, Songcan Chen", "title": "Tri-Subject Kinship Verification: Understanding the Core of A Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge in computer vision is to go beyond the modeling of\nindividual objects and to investigate the bi- (one-versus-one) or tri-\n(one-versus-two) relationship among multiple visual entities, answering such\nquestions as whether a child in a photo belongs to given parents. The\nchild-parents relationship plays a core role in a family and understanding such\nkin relationship would have fundamental impact on the behavior of an artificial\nintelligent agent working in the human world. In this work, we tackle the\nproblem of one-versus-two (tri-subject) kinship verification and our\ncontributions are three folds: 1) a novel relative symmetric bilinear model\n(RSBM) introduced to model the similarity between the child and the parents, by\nincorporating the prior knowledge that a child may resemble a particular parent\nmore than the other; 2) a spatially voted method for feature selection, which\njointly selects the most discriminative features for the child-parents pair,\nwhile taking local spatial information into account; 3) a large scale\ntri-subject kinship database characterized by over 1,000 child-parents\nfamilies. Extensive experiments on KinFaceW, Family101 and our newly released\nkinship database show that the proposed method outperforms several previous\nstate of the art methods, while could also be used to significantly boost the\nperformance of one-versus-one kinship verification when the information about\nboth parents are available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 07:32:45 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 03:48:41 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 11:50:41 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Qin", "Xiaoqian", ""], ["Tan", "Xiaoyang", ""], ["Chen", "Songcan", ""]]}, {"id": "1501.02565", "submitter": "Team Lear", "authors": "Jerome Revaud (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Philippe Weinzaepfel (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann), Cordelia Schmid (INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann)", "title": "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical\n  Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for optical flow estimation , targeted at large\ndisplacements with significant oc-clusions. It consists of two steps: i) dense\nmatching by edge-preserving interpolation from a sparse set of matches; ii)\nvariational energy minimization initialized with the dense matches. The\nsparse-to-dense interpolation relies on an appropriate choice of the distance,\nnamely an edge-aware geodesic distance. This distance is tailored to handle\nocclusions and motion boundaries -- two common and difficult issues for optical\nflow computation. We also propose an approximation scheme for the geodesic\ndistance to allow fast computation without loss of performance. Subsequent to\nthe dense interpolation step, standard one-level variational energy\nminimization is carried out on the dense matches to obtain the final flow\nestimation. The proposed approach, called Edge-Preserving Interpolation of\nCorrespondences (EpicFlow) is fast and robust to large displacements. It\nsignificantly outperforms the state of the art on MPI-Sintel and performs on\npar on Kitti and Middlebury.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 08:19:09 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 14:46:16 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Revaud", "Jerome", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Weinzaepfel", "Philippe", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"], ["Schmid", "Cordelia", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"]]}, {"id": "1501.02598", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "comments": "accepted at NAACL 2015, camera ready version, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual\ninformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)\nbuild vector-based word representations by learning to predict linguistic\ncontexts in text corpora. However, for a restricted set of words, the models\nare also exposed to visual representations of the objects they denote\n(extracted from natural images), and must predict linguistic and visual\nfeatures jointly. The MMSKIP-GRAM models achieve good performance on a variety\nof semantic benchmarks. Moreover, since they propagate visual information to\nall words, we use them to improve image labeling and retrieval in the zero-shot\nsetup, where the test concepts are never seen during model training. Finally,\nthe MMSKIP-GRAM models discover intriguing visual properties of abstract words,\npaving the way to realistic implementations of embodied theories of meaning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 10:48:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 09:37:08 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 09:47:33 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1501.02655", "submitter": "Alexander Sagel", "authors": "Alexander Sagel, Dominik Meyer, Hao Shen", "title": "Texture Retrieval via the Scattering Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of content-based image retrieval, specifically,\ntexture retrieval. It focuses on feature extraction and similarity measure for\ntexture images. Our approach employs a recently developed method, the so-called\nScattering transform, for the process of feature extraction in texture\nretrieval. It shares a distinctive property of providing a robust\nrepresentation, which is stable with respect to spatial deformations. Recent\nwork has demonstrated its capability for texture classification, and hence as a\npromising candidate for the problem of texture retrieval.\n  Moreover, we adopt a common approach of measuring the similarity of textures\nby comparing the subband histograms of a filterbank transform. To this end we\nderive a similarity measure based on the popular Bhattacharyya Kernel. Despite\nthe popularity of describing histograms using parametrized probability density\nfunctions, such as the Generalized Gaussian Distribution, it is unfortunately\nnot applicable for describing most of the Scattering transform subbands, due to\nthe complex modulus performed on each one of them. In this work, we propose to\nuse the Weibull distribution to model the Scattering subbands of descendant\nlayers.\n  Our numerical experiments demonstrated the effectiveness of the proposed\napproach, in comparison with several state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:22:28 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 12:48:04 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 08:31:05 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 10:44:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Sagel", "Alexander", ""], ["Meyer", "Dominik", ""], ["Shen", "Hao", ""]]}, {"id": "1501.02714", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Georgiana Dinu, Adam Liska, Marco Baroni", "title": "From Visual Attributes to Adjectives through Decompositional\n  Distributional Semantics", "comments": "accepted at Transactions of the Association for Computational\n  Linguistics (TACL), 3/2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automated image analysis progresses, there is increasing interest in\nricher linguistic annotation of pictures, with attributes of objects (e.g.,\nfurry, brown...) attracting most attention. By building on the recent\n\"zero-shot learning\" approach, and paying attention to the linguistic nature of\nattributes as noun modifiers, and specifically adjectives, we show that it is\npossible to tag images with attribute-denoting adjectives even when no training\ndata containing the relevant annotation are available. Our approach relies on\ntwo key observations. First, objects can be seen as bundles of attributes,\ntypically expressed as adjectival modifiers (a dog is something furry, brown,\netc.), and thus a function trained to map visual representations of objects to\nnominal labels can implicitly learn to map attributes to adjectives. Second,\nobjects and attributes come together in pictures (the same thing is a dog and\nit is brown). We can thus achieve better attribute (and object) label retrieval\nby treating images as \"visual phrases\", and decomposing their linguistic\nrepresentation into an attribute-denoting adjective and an object-denoting\nnoun. Our approach performs comparably to a method exploiting manual attribute\nannotation, it outperforms various competitive alternatives in both attribute\nand object annotation, and it automatically constructs attribute-centric\nrepresentations that significantly improve performance in supervised object\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 16:48:19 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 12:32:05 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Dinu", "Georgiana", ""], ["Liska", "Adam", ""], ["Baroni", "Marco", ""]]}, {"id": "1501.02741", "submitter": "Ming-Ming Cheng Prof.", "authors": "Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li", "title": "Salient Object Detection: A Benchmark", "comments": null, "journal-ref": "Image Processing, IEEE Transactions on (Volume:24, Issue: 12),\n  2015", "doi": "10.1109/TIP.2015.2487833", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art\nmodels (28 salient object detection, 10 fixation prediction, 1 objectness, and\n1 baseline) over 6 challenging datasets for the purpose of benchmarking salient\nobject detection and segmentation methods. From the results obtained so far,\nour evaluation shows a consistent rapid progress over the last few years in\nterms of both accuracy and running time. The top contenders in this benchmark\nsignificantly outperform the models identified as the best in the previous\nbenchmark conducted just two years ago. We find that the models designed\nspecifically for salient object detection generally work better than models in\nclosely related areas, which in turn provides a precise definition and suggests\nan appropriate treatment of this problem that distinguishes it from other\nproblems. In particular, we analyze the influences of center bias and scene\ncomplexity in model performance, which, along with the hard cases for\nstate-of-the-art models, provide useful hints towards constructing more\nchallenging large scale datasets and better saliency models. Finally, we\npropose probable solutions for tackling several open problems such as\nevaluation scores and dataset bias, which also suggest future research\ndirections in the rapidly-growing field of salient object detection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 20:24:01 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 06:24:39 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Borji", "Ali", ""], ["Cheng", "Ming-Ming", ""], ["Jiang", "Huaizu", ""], ["Li", "Jia", ""]]}, {"id": "1501.02825", "submitter": "Sven Bambach", "authors": "Sven Bambach", "title": "A Survey on Recent Advances of Computer Vision Algorithms for Egocentric\n  Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances have made lightweight, head mounted cameras\nboth practical and affordable and products like Google Glass show first\napproaches to introduce the idea of egocentric (first-person) video to the\nmainstream. Interestingly, the computer vision community has only recently\nstarted to explore this new domain of egocentric vision, where research can\nroughly be categorized into three areas: Object recognition, activity\ndetection/recognition, video summarization. In this paper, we try to give a\nbroad overview about the different problems that have been addressed and\ncollect and compare evaluation results. Moreover, along with the emergence of\nthis new domain came the introduction of numerous new and versatile benchmark\ndatasets, which we summarize and compare as well.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 21:14:56 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Bambach", "Sven", ""]]}, {"id": "1501.02876", "submitter": "Yi Shan", "authors": "Ren Wu, Shengen Yan, Yi Shan, Qingqing Dang, Gang Sun", "title": "Deep Image: Scaling up Image Recognition", "comments": "This paper has been withdrawn by the authors due to a mistake related\n  to ImageNet server submissions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a state-of-the-art image recognition system, Deep Image, developed\nusing end-to-end deep learning. The key components are a custom-built\nsupercomputer dedicated to deep learning, a highly optimized parallel algorithm\nusing new strategies for data partitioning and communication, larger deep\nneural network models, novel data augmentation approaches, and usage of\nmulti-scale high-resolution images. Our method achieves excellent results on\nmultiple challenging computer vision benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 03:42:24 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 10:12:14 GMT"}, {"version": "v3", "created": "Mon, 11 May 2015 17:36:20 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 19:44:49 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2015 03:11:28 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Wu", "Ren", ""], ["Yan", "Shengen", ""], ["Shan", "Yi", ""], ["Dang", "Qingqing", ""], ["Sun", "Gang", ""]]}, {"id": "1501.02887", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Lajish VL and Sunil Kumar Kopparapu", "title": "Online Handwritten Devanagari Stroke Recognition Using Extended\n  Directional Features", "comments": "8th International Conference on Signal Processing and Communication\n  Systems 15 - 17 December 2014, Gold Coast, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new feature set, called the extended directional\nfeatures (EDF) for use in the recognition of online handwritten strokes. We use\nEDF specifically to recognize strokes that form a basis for producing\nDevanagari script, which is the most widely used Indian language script. It\nshould be noted that stroke recognition in handwritten script is equivalent to\nphoneme recognition in speech signals and is generally very poor and of the\norder of 20% for singing voice. Experiments are conducted for the automatic\nrecognition of isolated handwritten strokes. Initially we describe the proposed\nfeature set, namely EDF and then show how this feature can be effectively\nutilized for writer independent script recognition through stroke recognition.\nExperimental results show that the extended directional feature set performs\nwell with about 65+% stroke level recognition accuracy for writer independent\ndata set.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 16:53:05 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["VL", "Lajish", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1501.02894", "submitter": "Mahdi Salarian mr", "authors": "Mehdi. Salarian, Babak. Mohamadinia, Jalil Rasekhi", "title": "A Modified No Search Algorithm for Fractal Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractal image compression has some desirable properties like high quality at\nhigh compression ratio, fast decoding, and resolution independence. Therefore\nit can be used for many applications such as texture mapping and pattern\nrecognition and image watermarking. But it suffers from long encoding time due\nto its need to find the best match between sub blocks. This time is related to\nthe approach that is used. In this paper we present a fast encoding Algorithm\nbased on no search method. Our goal is that more blocks are covered in initial\nstep of quad tree algorithm. Experimental result has been compared with other\nnew fast fractal coding methods, showing it is better in term of bit rate in\nsame condition while the other parameters are fixed.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 07:19:17 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 07:22:07 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Salarian", "Mehdi.", ""], ["Mohamadinia", "Babak.", ""], ["Rasekhi", "Jalil", ""]]}, {"id": "1501.02995", "submitter": "Renato J Cintra", "authors": "U. S. Potluri, A. Madanayake, R. J. Cintra, F. M. Bayer, S.\n  Kulasekera, A. Edirisuriya", "title": "Improved 8-point Approximate DCT for Image and Video Compression\n  Requiring Only 14 Additions", "comments": "30 pages, 7 figures, 5 tables", "journal-ref": "Circuits and Systems I: Regular Papers, IEEE Transactions on,\n  Volume 61, Issue 6, June 2014, 1727--1740", "doi": "10.1109/TCSI.2013.2295022", "report-no": null, "categories": "cs.MM cs.CV cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video processing systems such as HEVC requiring low energy consumption needed\nfor the multimedia market has lead to extensive development in fast algorithms\nfor the efficient approximation of 2-D DCT transforms. The DCT is employed in a\nmultitude of compression standards due to its remarkable energy compaction\nproperties. Multiplier-free approximate DCT transforms have been proposed that\noffer superior compression performance at very low circuit complexity. Such\napproximations can be realized in digital VLSI hardware using additions and\nsubtractions only, leading to significant reductions in chip area and power\nconsumption compared to conventional DCTs and integer transforms. In this\npaper, we introduce a novel 8-point DCT approximation that requires only 14\naddition operations and no multiplications. The proposed transform possesses\nlow computational complexity and is compared to state-of-the-art DCT\napproximations in terms of both algorithm complexity and peak signal-to-noise\nratio. The proposed DCT approximation is a candidate for reconfigurable video\nstandards such as HEVC. The proposed transform and several other DCT\napproximations are mapped to systolic-array digital architectures and\nphysically realized as digital prototype circuits using FPGA technology and\nmapped to 45 nm CMOS technology.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:26:40 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Potluri", "U. S.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Edirisuriya", "A.", ""]]}, {"id": "1501.03058", "submitter": "Arbaaz Singh Sidhu", "authors": "Arbaaz Singh Sidhu", "title": "An Adaptive Neuro-Fuzzy Inference System Modeling for Grid-Adaptive\n  Interpolation over Depth Images", "comments": "8 pages, 6 figures, International conference on Signal, Image\n  Processing and Management (SPM) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A suitable interpolation method is essential to keep the noise level minimum\nalong with the time-delay. In recent years, many different interpolation\nfilters have been developed for instance H.264-6 tap filter, and AVS- 4 tap\nfilter. The present work uses Adaptive Neuro-Fuzzy Inference System (ANFIS)\ntechnique to model and investigate the effects of a four-tap low-pass tap\nfilter (Grid-adaptive filter) on a hole-filled depth image. The work\ndemonstrates the general form of uniform interpolations for both integer and\nsub-pixel locations in terms of the sampling interval and filter length of\ndepth-images via diverse finite impulse response filtering schemes. The\ndemonstrated model combined modelling function of fuzzy inference with the\nlearning ability of artificial neural network.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 16:02:16 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Sidhu", "Arbaaz Singh", ""]]}, {"id": "1501.03069", "submitter": "Xiatian Zhu", "authors": "Xiatian Zhu, Chen Change Loy, Shaogang Gong", "title": "Learning from Multiple Sources for Video Summarisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual surveillance tasks, e.g.video summarisation, is conventionally\naccomplished through analysing imagerybased features. Relying solely on visual\ncues for public surveillance video understanding is unreliable, since visual\nobservations obtained from public space CCTV video data are often not\nsufficiently trustworthy and events of interest can be subtle. On the other\nhand, non-visual data sources such as weather reports and traffic sensory\nsignals are readily accessible but are not explored jointly to complement\nvisual data for video content analysis and summarisation. In this paper, we\npresent a novel unsupervised framework to learn jointly from both visual and\nindependently-drawn non-visual data sources for discovering meaningful latent\nstructure of surveillance video data. In particular, we investigate ways to\ncope with discrepant dimension and representation whist associating these\nheterogeneous data sources, and derive effective mechanism to tolerate with\nmissing and incomplete data from different sources. We show that the proposed\nmulti-source learning framework not only achieves better video content\nclustering than state-of-the-art methods, but also is capable of accurately\ninferring missing non-visual semantics from previously unseen videos. In\naddition, a comprehensive user study is conducted to validate the quality of\nvideo summarisation generated using the proposed multi-source model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 16:46:39 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 21:30:19 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Zhu", "Xiatian", ""], ["Loy", "Chen Change", ""], ["Gong", "Shaogang", ""]]}, {"id": "1501.03124", "submitter": "Amartansh Dubey", "authors": "Amartansh Dubey and K. M. Bhurchandi", "title": "Robust and Real Time Detection of Curvy Lanes (Curves) with Desired\n  Slopes for Driving Assistance and Autonomous Vehicles", "comments": "13 pages, 12 figures, published in International Conference on Signal\n  and Image Processing (AIRCC Publishing Corporation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest reasons for road accidents is curvy lanes and blind turns.\nEven one of the biggest hurdles for new autonomous vehicles is to detect curvy\nlanes, multiple lanes and lanes with a lot of discontinuity and noise. This\npaper presents very efficient and advanced algorithm for detecting curves\nhaving desired slopes (especially for detecting curvy lanes in real time) and\ndetection of curves (lanes) with a lot of noise, discontinuity and\ndisturbances. Overall aim is to develop robust method for this task which is\napplicable even in adverse conditions. Even in some of most famous and useful\nlibraries like OpenCV and Matlab, there is no function available for detecting\ncurves having desired slopes , shapes, discontinuities. Only few predefined\nshapes like circle, ellipse, etc, can be detected using presently available\nfunctions. Proposed algorithm can not only detect curves with discontinuity,\nnoise, desired slope but also it can perform shadow and illumination correction\nand detect/ differentiate between different curves.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 19:35:18 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Dubey", "Amartansh", ""], ["Bhurchandi", "K. M.", ""]]}, {"id": "1501.03271", "submitter": "Joseph Paul", "authors": "Joseph Suresh Paul and Uma Krishna Swamy Pillai", "title": "Higher dimensional homodyne filtering for suppression of incidental\n  phase artifacts in multichannel MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce procedural steps for extension of the\n1D homodyne phase correction for k-space truncation in all gradient encoding\ndirections. Compared to the existing method applied to 2D partial k-space,\nsignal losses introduced by the phase correction filter is observed to be\nminimal for the extended approach. In addition, the modified form of phase\ncorrection mitigates Incidental Phase Artifacts (IPA) due to truncation. For\nparallel imaging with undersampling along phase encode direction, the extended\nhomodyne filtering is shown to be effective for minimizing these artifacts when\neach of the channel k-spaces are truncated along both phase and frequency\nencode directions. This is illustrated with 2D partial k-space for flow\ncompensated multichannel Susceptibility Weighted Imaging (SWI). Extension of\nour method to 3D partial k-space shows improved reconstruction of flow\ninformation in phase contrast angiography.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 08:07:50 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Paul", "Joseph Suresh", ""], ["Pillai", "Uma Krishna Swamy", ""]]}, {"id": "1501.03302", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "comments": "Presented in AAAI-15 Workshop: Beyond the Turing Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in language and image understanding by machines has sparkled the\ninterest of the research community in more open-ended, holistic tasks, and\nrefueled an old AI dream of building intelligent machines. We discuss a few\nprominent challenges that characterize such holistic tasks and argue for\n\"question answering about images\" as a particular appealing instance of such a\nholistic task. In particular, we point out that it is a version of a Turing\nTest that is likely to be more robust to over-interpretations and contrast it\nwith tasks like grounding and generation of descriptions. Finally, we discuss\ntools to measure progress in this field.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 10:38:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 10:18:54 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1501.03320", "submitter": "Joseph Paul", "authors": "P. K. Akshara, J. S. Paul", "title": "Image enhancement in intensity projected multichannel MRI using\n  spatially adaptive directional anisotropic diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic Diffusion is widely used for noise reduction with simultaneous\npreservation of vascular structures in maximum intensity projected (MIP)\nangiograms. However, extension to minimum intensity projected (mIP) venograms\nin Susceptibility Weighted Imaging (SWI) poses difficulties due to spatially\nvarying baseline. Here, we introduce a modified version of the directional\nanisotropic diffusion which allows us to simultaneously reduce the noise and\nenhance vascular structures reconstructed using both M/mIP angiograms. This\nmethod is based on spatial adaptation of the diffusion function, separately in\nthe directions of the gradient, and along those of the minimum and maximum\ncurvatures. The existing approach of directional anisotropic diffusion uses\nbinary switched diffusion function to ensure diffusion along the direction of\nmaximum curvature stopped near the vessel borders. Here, the choice of a\nthreshold for detecting the upper limit of diffusion becomes difficult in the\npresence of spatially varying baseline. Also, the approach of using vesselness\nmeasure to steer the diffusion process results in structural discontinuities\ndue to junction suppression in mIP. The merits of the proposed method include\nelimination of the need for an apriori choice of a threshold to detect the\nvessel, and problems due to junction suppression. The proposed method is also\nextended to multi-channel phase contrast angiogram.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 11:24:02 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Akshara", "P. K.", ""], ["Paul", "J. S.", ""]]}, {"id": "1501.03383", "submitter": "Boris Schauerte", "authors": "Boris Schauerte, Rainer Stiefelhagen", "title": "On the Distribution of Salient Objects in Web Images and its Influence\n  on Salient Object Detection", "comments": null, "journal-ref": "PLoS ONE 10 (2015)", "doi": "10.1371/journal.pone.0130316", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become apparent that a Gaussian center bias can serve as an important\nprior for visual saliency detection, which has been demonstrated for predicting\nhuman eye fixations and salient object detection. Tseng et al. have shown that\nthe photographer's tendency to place interesting objects in the center is a\nlikely cause for the center bias of eye fixations. We investigate the influence\nof the photographer's center bias on salient object detection, extending our\nprevious work. We show that the centroid locations of salient objects in\nphotographs of Achanta and Liu's data set in fact correlate strongly with a\nGaussian model. This is an important insight, because it provides an empirical\nmotivation and justification for the integration of such a center bias in\nsalient object detection algorithms and helps to understand why Gaussian models\nare so effective. To assess the influence of the center bias on salient object\ndetection, we integrate an explicit Gaussian center bias model into two\nstate-of-the-art salient object detection algorithms. This way, first, we\nquantify the influence of the Gaussian center bias on pixel- and segment-based\nsalient object detection. Second, we improve the performance in terms of F1\nscore, Fb score, area under the recall-precision curve, area under the receiver\noperating characteristic curve, and hit-rate on the well-known data set by\nAchanta and Liu. Third, by debiasing Cheng et al.'s region contrast model, we\nexemplarily demonstrate that implicit center biases are partially responsible\nfor the outstanding performance of state-of-the-art algorithms. Last but not\nleast, as a result of debiasing Cheng et al.'s algorithm, we introduce a\nnon-biased salient object detection method, which is of interest for\napplications in which the image data is not likely to have a photographer's\ncenter bias (e.g., image data of surveillance cameras or autonomous robots).\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 17:36:24 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Schauerte", "Boris", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1501.03719", "submitter": "Tal Hassner", "authors": "Gil Levi and Tal Hassner", "title": "LATCH: Learned Arrangements of Three Patch Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel means of describing local image appearances using binary\nstrings. Binary descriptors have drawn increasing interest in recent years due\nto their speed and low memory footprint. A known shortcoming of these\nrepresentations is their inferior performance compared to larger, histogram\nbased descriptors such as the SIFT. Our goal is to close this performance gap\nwhile maintaining the benefits attributed to binary representations. To this\nend we propose the Learned Arrangements of Three Patch Codes descriptors, or\nLATCH. Our key observation is that existing binary descriptors are at an\nincreased risk from noise and local appearance variations. This, as they\ncompare the values of pixel pairs; changes to either of the pixels can easily\nlead to changes in descriptor values, hence damaging its performance. In order\nto provide more robustness, we instead propose a novel means of comparing pixel\npatches. This ostensibly small change, requires a substantial redesign of the\ndescriptors themselves and how they are produced. Our resulting LATCH\nrepresentation is rigorously compared to state-of-the-art binary descriptors\nand shown to provide far better performance for similar computation and space\nrequirements.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 15:38:57 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Levi", "Gil", ""], ["Hassner", "Tal", ""]]}, {"id": "1501.03755", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "Screen Content Image Segmentation Using Least Absolute Deviation Fitting", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for separating the foreground (mainly text and line\ngraphics) from the smoothly varying background in screen content images. The\nproposed method is designed based on the assumption that the background part of\nthe image is smoothly varying and can be represented by a linear combination of\na few smoothly varying basis functions, while the foreground text and graphics\ncreate sharp discontinuity and cannot be modeled by this smooth representation.\nThe algorithm separates the background and foreground using a least absolute\ndeviation method to fit the smooth model to the image pixels. This algorithm\nhas been tested on several images from HEVC standard test sequences for screen\ncontent coding, and is shown to have superior performance over other popular\nmethods, such as k-means clustering based segmentation in DjVu and shape\nprimitive extraction and coding (SPEC) algorithm. Such background/foreground\nsegmentation are important pre-processing steps for text extraction and\nseparate coding of background and foreground for compression of screen content\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 17:40:20 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 07:07:06 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1501.03771", "submitter": "Anton Osokin", "authors": "Anton Osokin, Dmitry Vetrov", "title": "Submodular relaxation for inference in Markov random fields", "comments": "This paper is accepted for publication in IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2369046", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of finding the most probable state of a\ndiscrete Markov random field (MRF), also known as the MRF energy minimization\nproblem. The task is known to be NP-hard in general and its practical\nimportance motivates numerous approximate algorithms. We propose a submodular\nrelaxation approach (SMR) based on a Lagrangian relaxation of the initial\nproblem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMR\ndoes not decompose the graph structure of the initial problem but constructs a\nsubmodular energy that is minimized within the Lagrangian relaxation. Our\napproach is applicable to both pairwise and high-order MRFs and allows to take\ninto account global potentials of certain types. We study theoretical\nproperties of the proposed approach and evaluate it experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 18:34:27 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Osokin", "Anton", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1501.03779", "submitter": "Holger Roth", "authors": "Holger R. Roth, Thomas E. Hampshire, Emma Helbren, Mingxing Hu, Roser\n  Vega, Steve Halligan, David J. Hawkes", "title": "Computer-assisted polyp matching between optical colonoscopy and CT\n  colonography: a phantom study", "comments": "This paper was presented at the SPIE Medical Imaging 2014 conference", "journal-ref": "Proc. SPIE 9036, Medical Imaging 2014: Image-Guided Procedures,\n  Robotic Interventions, and Modeling, 903609 (March 12, 2014)", "doi": "10.1117/12.2042860", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potentially precancerous polyps detected with CT colonography (CTC) need to\nbe removed subsequently, using an optical colonoscope (OC). Due to large\ncolonic deformations induced by the colonoscope, even very experienced\ncolonoscopists find it difficult to pinpoint the exact location of the\ncolonoscope tip in relation to polyps reported on CTC. This can cause unduly\nprolonged OC examinations that are stressful for the patient, colonoscopist and\nsupporting staff.\n  We developed a method, based on monocular 3D reconstruction from OC images,\nthat automatically matches polyps observed in OC with polyps reported on prior\nCTC. A matching cost is computed, using rigid point-based registration between\nsurface point clouds extracted from both modalities. A 3D printed and painted\nphantom of a 25 cm long transverse colon segment was used to validate the\nmethod on two medium sized polyps. Results indicate that the matching cost is\nsmaller at the correct corresponding polyp between OC and CTC: the value is 3.9\ntimes higher at the incorrect polyp, comparing the correct match between polyps\nto the incorrect match. Furthermore, we evaluate the matching of the\nreconstructed polyp from OC with other colonic endoluminal surface structures\nsuch as haustral folds and show that there is a minimum at the correct polyp\nfrom CTC.\n  Automated matching between polyps observed at OC and prior CTC would\nfacilitate the biopsy or removal of true-positive pathology or exclusion of\nfalse-positive CTC findings, and would reduce colonoscopy false-negative\n(missed) polyps. Ultimately, such a method might reduce healthcare costs,\npatient inconvenience and discomfort.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 19:11:26 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Roth", "Holger R.", ""], ["Hampshire", "Thomas E.", ""], ["Helbren", "Emma", ""], ["Hu", "Mingxing", ""], ["Vega", "Roser", ""], ["Halligan", "Steve", ""], ["Hawkes", "David J.", ""]]}, {"id": "1501.03879", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury and K. R. Ramakrishnan", "title": "A new ADMM algorithm for the Euclidean median and its application to\n  robust patch regression", "comments": "5 pages, 3 figures, 1 table. To appear in Proc. IEEE International\n  Conference on Acoustics, Speech, and Signal Processing, April 19-24, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean Median (EM) of a set of points $\\Omega$ in an Euclidean space\nis the point x minimizing the (weighted) sum of the Euclidean distances of x to\nthe points in $\\Omega$. While there exits no closed-form expression for the EM,\nit can nevertheless be computed using iterative methods such as the Wieszfeld\nalgorithm. The EM has classically been used as a robust estimator of centrality\nfor multivariate data. It was recently demonstrated that the EM can be used to\nperform robust patch-based denoising of images by generalizing the popular\nNon-Local Means algorithm. In this paper, we propose a novel algorithm for\ncomputing the EM (and its box-constrained counterpart) using variable splitting\nand the method of augmented Lagrangian. The attractive feature of this approach\nis that the subproblems involved in the ADMM-based optimization of the\naugmented Lagrangian can be resolved using simple closed-form projections. The\nproposed ADMM solver is used for robust patch-based image denoising and is\nshown to exhibit faster convergence compared to an existing solver.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 05:27:17 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Chaudhury", "Kunal N.", ""], ["Ramakrishnan", "K. R.", ""]]}, {"id": "1501.03915", "submitter": "Massimo Brescia Dr", "authors": "Sabina Tangaro, Nicola Amoroso, Massimo Brescia, Stefano Cavuoti,\n  Andrea Chincarini, Rosangela Errico, Paolo Inglese, Giuseppe Longo, Rosalia\n  Maglietta, Andrea Tateo, Giuseppe Riccio, Roberto Bellotti", "title": "Feature Selection based on Machine Learning in MRIs for Hippocampal\n  Segmentation", "comments": "To appear on \"Computational and Mathematical Methods in Medicine\",\n  Hindawi Publishing Corporation. 19 pages, 7 figures", "journal-ref": "Computational and Mathematical Methods in Medicine Volume 2015,\n  Article ID 814104, 10 pages, Hindawi Publishing Corporation", "doi": "10.1155/2015/814104", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurodegenerative diseases are frequently associated with structural changes\nin the brain. Magnetic Resonance Imaging (MRI) scans can show these variations\nand therefore be used as a supportive feature for a number of neurodegenerative\ndiseases. The hippocampus has been known to be a biomarker for Alzheimer\ndisease and other neurological and psychiatric diseases. However, it requires\naccurate, robust and reproducible delineation of hippocampal structures. Fully\nautomatic methods are usually the voxel based approach, for each voxel a number\nof local features were calculated. In this paper we compared four different\ntechniques for feature selection from a set of 315 features extracted for each\nvoxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrapper\nmethods, respectively, (ii) Sequential Forward Selection and (iii) Sequential\nBackward Elimination; and (iv) embedded method based on the Random Forest\nClassifier on a set of 10 T1-weighted brain MRIs and tested on an independent\nset of 25 subjects. The resulting segmentations were compared with manual\nreference labelling. By using only 23 features for each voxel (sequential\nbackward elimination) we obtained comparable state of-the-art performances with\nrespect to the standard tool FreeSurfer.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 08:45:55 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Tangaro", "Sabina", ""], ["Amoroso", "Nicola", ""], ["Brescia", "Massimo", ""], ["Cavuoti", "Stefano", ""], ["Chincarini", "Andrea", ""], ["Errico", "Rosangela", ""], ["Inglese", "Paolo", ""], ["Longo", "Giuseppe", ""], ["Maglietta", "Rosalia", ""], ["Tateo", "Andrea", ""], ["Riccio", "Giuseppe", ""], ["Bellotti", "Roberto", ""]]}, {"id": "1501.03952", "submitter": "Anant Raj", "authors": "Anant Raj, Vinay P. Namboodiri and Tinne Tuytelaars", "title": "Mind the Gap: Subspace based Hierarchical Domain Adaptation", "comments": "4 pages in Second Workshop on Transfer and Multi-Task Learning:\n  Theory meets Practice in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 11:19:45 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Raj", "Anant", ""], ["Namboodiri", "Vinay P.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1501.03997", "submitter": "Hwanchol Jang", "authors": "Hwanchol Jang, Changhyeong Yoon, Euiheon Chung, Wonshik Choi, and\n  Heung-No Lee", "title": "Holistic random encoding for imaging through multimode fibers", "comments": "under review for possible publication in Optics express", "journal-ref": null, "doi": "10.1364/OE.23.006705", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input numerical aperture (NA) of multimode fiber (MMF) can be effectively\nincreased by placing turbid media at the input end of the MMF. This provides\nthe potential for high-resolution imaging through the MMF. While the input NA\nis increased, the number of propagation modes in the MMF and hence the output\nNA remains the same. This makes the image reconstruction process\nunderdetermined and may limit the quality of the image reconstruction. In this\npaper, we aim to improve the signal to noise ratio (SNR) of the image\nreconstruction in imaging through MMF. We notice that turbid media placed in\nthe input of the MMF transforms the incoming waves into a better format for\ninformation transmission and information extraction. We call this\ntransformation as holistic random (HR) encoding of turbid media. By exploiting\nthe HR encoding, we make a considerable improvement on the SNR of the image\nreconstruction. For efficient utilization of the HR encoding, we employ sparse\nrepresentation (SR), a relatively new signal reconstruction framework when it\nis provided with a HR encoded signal. This study shows for the first time to\nour knowledge the benefit of utilizing the HR encoding of turbid media for\nrecovery in the optically underdetermined systems where the output NA of it is\nsmaller than the input NA for imaging through MMF.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 10:27:23 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Jang", "Hwanchol", ""], ["Yoon", "Changhyeong", ""], ["Chung", "Euiheon", ""], ["Choi", "Wonshik", ""], ["Lee", "Heung-No", ""]]}, {"id": "1501.04009", "submitter": "Paul Klemm", "authors": "Bernhard Preim, Paul Klemm, Helwig Hauser, Katrin Hegenscheid, Steffen\n  Oeltze, Klaus Toennies, and Henry V\\\"olzke", "title": "Visual Analytics of Image-Centric Cohort Studies in Epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiology characterizes the influence of causes to disease and health\nconditions of defined populations. Cohort studies are population-based studies\ninvolving usually large numbers of randomly selected individuals and comprising\nnumerous attributes, ranging from self-reported interview data to results from\nvarious medical examinations, e.g., blood and urine samples. Since recently,\nmedical imaging has been used as an additional instrument to assess risk\nfactors and potential prognostic information. In this chapter, we discuss such\nstudies and how the evaluation may benefit from visual analytics. Cluster\nanalysis to define groups, reliable image analysis of organs in medical imaging\ndata and shape space exploration to characterize anatomical shapes are among\nthe visual analytics tools that may enable epidemiologists to fully exploit the\npotential of their huge and complex data. To gain acceptance, visual analytics\ntools need to complement more classical epidemiologic tools, primarily\nhypothesis-driven statistical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 16:51:20 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Preim", "Bernhard", ""], ["Klemm", "Paul", ""], ["Hauser", "Helwig", ""], ["Hegenscheid", "Katrin", ""], ["Oeltze", "Steffen", ""], ["Toennies", "Klaus", ""], ["V\u00f6lzke", "Henry", ""]]}, {"id": "1501.04140", "submitter": "Mahdi Salarian mr", "authors": "H. Miar Naimi, M. Salarian", "title": "A Fast Fractal Image Compression Algorithm Using Predefined Values for\n  Contrast Scaling", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new fractal image compression algorithm is proposed in which\nthe time of encoding process is considerably reduced. The algorithm exploits a\ndomain pool reduction approach, along with using innovative predefined values\nfor contrast scaling factor, S, instead of scanning the parameter space [0,1].\nWithin this approach only domain blocks with entropies greater than a threshold\nare considered. As a novel point, it is assumed that in each step of the\nencoding process, the domain block with small enough distance shall be found\nonly for the range blocks with low activity (equivalently low entropy). This\nnovel point is used to find reasonable estimations of S, and use them in the\nencoding process as predefined values, mentioned above. The algorithm has been\nexamined for some well-known images. This result shows that our proposed\nalgorithm considerably reduces the encoding time producing images that are\napproximately the same in quality.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 01:10:32 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Naimi", "H. Miar", ""], ["Salarian", "M.", ""]]}, {"id": "1501.04158", "submitter": "Niko S\\\"underhauf", "authors": "Niko S\\\"underhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, and\n  Michael Milford", "title": "On the Performance of ConvNet Features for Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the incredible success of deep learning in the computer vision domain,\nthere has been much interest in applying Convolutional Network (ConvNet)\nfeatures in robotic fields such as visual navigation and SLAM. Unfortunately,\nthere are fundamental differences and challenges involved. Computer vision\ndatasets are very different in character to robotic camera data, real-time\nperformance is essential, and performance priorities can be different. This\npaper comprehensively evaluates and compares the utility of three\nstate-of-the-art ConvNets on the problems of particular relevance to navigation\nfor robots; viewpoint-invariance and condition-invariance, and for the first\ntime enables real-time place recognition performance using ConvNets with large\nmaps by integrating a variety of existing (locality-sensitive hashing) and\nnovel (semantic search space partitioning) optimization techniques. We present\nextensive experiments on four real world datasets cultivated to evaluate each\nof the specific challenges in place recognition. The results demonstrate that\nspeed-ups of two orders of magnitude can be achieved with minimal accuracy\ndegradation, enabling real-time performance. We confirm that networks trained\nfor semantic place categorization also perform better at (specific) place\nrecognition when faced with severe appearance changes and provide a reference\nfor which networks and layers are optimal for different aspects of the place\nrecognition problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 05:16:12 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 11:35:10 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 01:56:54 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""], ["Shirazi", "Sareh", ""], ["Upcroft", "Ben", ""], ["Milford", "Michael", ""]]}, {"id": "1501.04163", "submitter": "Gui-Song Xia", "authors": "Gui-Song Xia, Gang Liu, Wen Yang", "title": "Meaningful Objects Segmentation from SAR Images via A Multi-Scale\n  Non-Local Active Contour Model", "comments": null, "journal-ref": "IEEE Trans. Geoscience and Remote Sensing, Vol. 54, No.3, pp.1860\n  - 1873, 2016", "doi": "10.1109/TGRS.2015.2490078", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of synthetic aperture radar (SAR) images is a longstanding\nyet challenging task, not only because of the presence of speckle, but also due\nto the variations of surface backscattering properties in the images.\nTremendous investigations have been made to eliminate the speckle effects for\nthe segmentation of SAR images, while few work devotes to dealing with the\nvariations of backscattering coefficients in the images. In order to overcome\nboth the two difficulties, this paper presents a novel SAR image segmentation\nmethod by exploiting a multi-scale active contour model based on the non-local\nprocessing principle. More precisely, we first formulize the SAR segmentation\nproblem with an active contour model by integrating the non-local interactions\nbetween pairs of patches inside and outside the segmented regions. Secondly, a\nmulti-scale strategy is proposed to speed up the non-local active contour\nsegmentation procedure and to avoid falling into local minimum for achieving\nmore accurate segmentation results. Experimental results on simulated and real\nSAR images demonstrate the efficiency and feasibility of the proposed method:\nit can not only achieve precise segmentations for images with heavy speckles\nand non-local intensity variations, but also can be used for SAR images from\ndifferent types of sensors.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 06:03:28 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Xia", "Gui-Song", ""], ["Liu", "Gang", ""], ["Yang", "Wen", ""]]}, {"id": "1501.04276", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jiashi Feng, Zhouchen Lin, and Shuicheng Yan", "title": "Correlation Adaptive Subspace Segmentation by Trace Lasso", "comments": "International Conference on Computer Vision (ICCV), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the subspace segmentation problem. Given a set of data\npoints drawn from a union of subspaces, the goal is to partition them into\ntheir underlying subspaces they were drawn from. The spectral clustering method\nis used as the framework. It requires to find an affinity matrix which is close\nto block diagonal, with nonzero entries corresponding to the data point pairs\nfrom the same subspace. In this work, we argue that both sparsity and the\ngrouping effect are important for subspace segmentation. A sparse affinity\nmatrix tends to be block diagonal, with less connections between data points\nfrom different subspaces. The grouping effect ensures that the highly corrected\ndata which are usually from the same subspace can be grouped together. Sparse\nSubspace Clustering (SSC), by using $\\ell^1$-minimization, encourages sparsity\nfor data selection, but it lacks of the grouping effect. On the contrary,\nLow-Rank Representation (LRR), by rank minimization, and Least Squares\nRegression (LSR), by $\\ell^2$-regularization, exhibit strong grouping effect,\nbut they are short in subset selection. Thus the obtained affinity matrix is\nusually very sparse by SSC, yet very dense by LRR and LSR.\n  In this work, we propose the Correlation Adaptive Subspace Segmentation\n(CASS) method by using trace Lasso. CASS is a data correlation dependent method\nwhich simultaneously performs automatic data selection and groups correlated\ndata together. It can be regarded as a method which adaptively balances SSC and\nLSR. Both theoretical and experimental results show the effectiveness of CASS.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 10:02:25 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lu", "Canyi", ""], ["Feng", "Jiashi", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1501.04277", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, and Zhouchen\n  Lin", "title": "Correntropy Induced L2 Graph for Robust Subspace Clustering", "comments": "International Conference on Computer Vision (ICCV), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the robust subspace clustering problem, which aims to\ncluster the given possibly noisy data points into their underlying subspaces. A\nlarge pool of previous subspace clustering methods focus on the graph\nconstruction by different regularization of the representation coefficient. We\ninstead focus on the robustness of the model to non-Gaussian noises. We propose\na new robust clustering method by using the correntropy induced metric, which\nis robust for handling the non-Gaussian and impulsive noises. Also we further\nextend the method for handling the data with outlier rows/features. The\nmultiplicative form of half-quadratic optimization is used to optimize the\nnon-convex correntropy objective function of the proposed models. Extensive\nexperiments on face datasets well demonstrate that the proposed methods are\nmore robust to corruptions and occlusions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 10:06:55 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Lin", "Min", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1501.04284", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu and Liwei Wang", "title": "Pairwise Constraint Propagation on Multi-View Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a graph-based learning approach to pairwise constraint\npropagation on multi-view data. Although pairwise constraint propagation has\nbeen studied extensively, pairwise constraints are usually defined over pairs\nof data points from a single view, i.e., only intra-view constraint propagation\nis considered for multi-view tasks. In fact, very little attention has been\npaid to inter-view constraint propagation, which is more challenging since\npairwise constraints are now defined over pairs of data points from different\nviews. In this paper, we propose to decompose the challenging inter-view\nconstraint propagation problem into semi-supervised learning subproblems so\nthat they can be efficiently solved based on graph-based label propagation. To\nthe best of our knowledge, this is the first attempt to give an efficient\nsolution to inter-view constraint propagation from a semi-supervised learning\nviewpoint. Moreover, since graph-based label propagation has been adopted for\nbasic optimization, we develop two constrained graph construction methods for\ninterview constraint propagation, which only differ in how the intra-view\npairwise constraints are exploited. The experimental results in cross-view\nretrieval have shown the promising performance of our inter-view constraint\npropagation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 11:52:21 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lu", "Zhiwu", ""], ["Wang", "Liwei", ""]]}, {"id": "1501.04292", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu and Liwei Wang and Ji-Rong Wen", "title": "Image classification by visual bag-of-words refinement and reduction", "comments": null, "journal-ref": "Neurocomputing 173: 373-384 (2016)", "doi": "10.1016/j.neucom.2015.01.098", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for visual bag-of-words (BOW) refinement\nand reduction to overcome the drawbacks associated with the visual BOW model\nwhich has been widely used for image classification. Although very influential\nin the literature, the traditional visual BOW model has two distinct drawbacks.\nFirstly, for efficiency purposes, the visual vocabulary is commonly constructed\nby directly clustering the low-level visual feature vectors extracted from\nlocal keypoints, without considering the high-level semantics of images. That\nis, the visual BOW model still suffers from the semantic gap, and thus may lead\nto significant performance degradation in more challenging tasks (e.g. social\nimage classification). Secondly, typically thousands of visual words are\ngenerated to obtain better performance on a relatively large image dataset. Due\nto such large vocabulary size, the subsequent image classification may take\nsheer amount of time. To overcome the first drawback, we develop a graph-based\nmethod for visual BOW refinement by exploiting the tags (easy to access\nalthough noisy) of social images. More notably, for efficient image\nclassification, we further reduce the refined visual BOW model to a much\nsmaller size through semantic spectral clustering. Extensive experimental\nresults show the promising performance of the proposed framework for visual BOW\nrefinement and reduction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 12:46:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lu", "Zhiwu", ""], ["Wang", "Liwei", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1501.04318", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering based on the In-tree Graph Structure and Affinity Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A recently proposed clustering method, called the Nearest Descent (ND), can\norganize the whole dataset into a sparsely connected graph, called the In-tree.\nThis ND-based Intree structure proves able to reveal the clustering structure\nunderlying the dataset, except one imperfect place, that is, there are some\nundesired edges in this In-tree which require to be removed. Here, we propose\nan effective way to automatically remove the undesired edges in In-tree via an\neffective combination of the In-tree structure with affinity propagation (AP).\nThe key for the combination is to add edges between the reachable nodes in\nIn-tree before using AP to remove the undesired edges. The experiments on both\nsynthetic and real datasets demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 15:34:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 00:34:26 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.04367", "submitter": "Kuldeep S Kulkarni Mr.", "authors": "Kuldeep Kulkarni and Pavan Turaga", "title": "Reconstruction-free action inference from compressive imagers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent surveillance from camera networks, such as at parking lots, UAVs,\netc., often results in large amounts of video data, resulting in significant\nchallenges for inference in terms of storage, communication and computation.\nCompressive cameras have emerged as a potential solution to deal with the data\ndeluge issues in such applications. However, inference tasks such as action\nrecognition require high quality features which implies reconstructing the\noriginal video data. Much work in compressive sensing (CS) theory is geared\ntowards solving the reconstruction problem, where state-of-the-art methods are\ncomputationally intensive and provide low-quality results at high compression\nrates. Thus, reconstruction-free methods for inference are much desired. In\nthis paper, we propose reconstruction-free methods for action recognition from\ncompressive cameras at high compression ratios of 100 and above. Recognizing\nactions directly from CS measurements requires features which are mostly\nnonlinear and thus not easily applicable. This leads us to search for such\nproperties that are preserved in compressive measurements. To this end, we\npropose the use of spatio-temporal smashed filters, which are compressive\ndomain versions of pixel-domain matched filters. We conduct experiments on\npublicly available databases and show that one can obtain recognition rates\nthat are comparable to the oracle method in uncompressed setup, even for high\ncompression ratios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 23:14:15 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kulkarni", "Kuldeep", ""], ["Turaga", "Pavan", ""]]}, {"id": "1501.04378", "submitter": "Tianfei Zhou", "authors": "Jinwu Liu and Yao Lu and Tianfei Zhou", "title": "Instance Significance Guided Multiple Instance Boosting for Robust\n  Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Instance Learning (MIL) recently provides an appealing way to\nalleviate the drifting problem in visual tracking. Following the\ntracking-by-detection framework, an online MILBoost approach is developed that\nsequentially chooses weak classifiers by maximizing the bag likelihood. In this\npaper, we extend this idea towards incorporating the instance significance\nestimation into the online MILBoost framework. First, instead of treating all\ninstances equally, with each instance we associate a significance-coefficient\nthat represents its contribution to the bag likelihood. The coefficients are\nestimated by a simple Bayesian formula that jointly considers the predictions\nfrom several standard MILBoost classifiers. Next, we follow the online boosting\nframework, and propose a new criterion for the selection of weak classifiers.\nExperiments with challenging public datasets show that the proposed method\noutperforms both existing MIL based and boosting based trackers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 03:04:16 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 08:05:00 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 02:35:21 GMT"}, {"version": "v4", "created": "Sat, 25 Jul 2015 09:32:38 GMT"}, {"version": "v5", "created": "Tue, 17 Mar 2020 07:48:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Liu", "Jinwu", ""], ["Lu", "Yao", ""], ["Zhou", "Tianfei", ""]]}, {"id": "1501.04505", "submitter": "Kaihua Zhang", "authors": "Kaihua Zhang, Qingshan Liu, Yi Wu, Ming-Hsuan Yang", "title": "Robust Visual Tracking via Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep networks have been successfully applied to visual tracking by learning a\ngeneric representation offline from numerous training images. However the\noffline training is time-consuming and the learned generic representation may\nbe less discriminative for tracking specific objects. In this paper we present\nthat, even without offline training with a large amount of auxiliary data,\nsimple two-layer convolutional networks can be powerful enough to develop a\nrobust representation for visual tracking. In the first frame, we employ the\nk-means algorithm to extract a set of normalized patches from the target region\nas fixed filters, which integrate a series of adaptive contextual filters\nsurrounding the target to define a set of feature maps in the subsequent\nframes. These maps measure similarities between each filter and the useful\nlocal intensity patterns across the target, thereby encoding its local\nstructural information. Furthermore, all the maps form together a global\nrepresentation, which is built on mid-level features, thereby remaining close\nto image-level information, and hence the inner geometric layout of the target\nis also well preserved. A simple soft shrinkage method with an adaptive\nthreshold is employed to de-noise the global representation, resulting in a\nrobust sparse representation. The representation is updated via a simple and\neffective online strategy, allowing it to robustly adapt to target appearance\nvariations. Our convolution networks have surprisingly lightweight structure,\nyet perform favorably against several state-of-the-art methods on the CVPR2013\ntracking benchmark dataset with 50 challenging videos.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 14:39:51 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 06:07:22 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Zhang", "Kaihua", ""], ["Liu", "Qingshan", ""], ["Wu", "Yi", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1501.04537", "submitter": "Mohammad Haris Baig", "authors": "Mohammad Haris Baig and Lorenzo Torresani", "title": "Coupled Depth Learning", "comments": "10 pages, 3 Figures, 4 Tables with quantitative evaluations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method for estimating depth from a single image\nusing a coarse to fine approach. We argue that modeling the fine depth details\nis easier after a coarse depth map has been computed. We express a global\n(coarse) depth map of an image as a linear combination of a depth basis learned\nfrom training examples. The depth basis captures spatial and statistical\nregularities and reduces the problem of global depth estimation to the task of\npredicting the input-specific coefficients in the linear combination. This is\nformulated as a regression problem from a holistic representation of the image.\nCrucially, the depth basis and the regression function are {\\bf coupled} and\njointly optimized by our learning scheme. We demonstrate that this results in a\nsignificant improvement in accuracy compared to direct regression of depth\npixel values or approaches learning the depth basis disjointly from the\nregression function. The global depth estimate is then used as a guidance by a\nlocal refinement method that introduces depth details that were not captured at\nthe global level. Experiments on the NYUv2 and KITTI datasets show that our\nmethod outperforms the existing state-of-the-art at a considerably lower\ncomputational cost for both training and testing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 16:18:48 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 23:17:12 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 22:51:43 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 06:36:34 GMT"}, {"version": "v5", "created": "Thu, 15 Oct 2015 04:35:32 GMT"}, {"version": "v6", "created": "Tue, 9 Feb 2016 16:27:35 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Baig", "Mohammad Haris", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1501.04560", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Timothy M. Hospedales, Tao Xiang and Shaogang Gong", "title": "Transductive Multi-view Zero-Shot Learning", "comments": "accepted by IEEE TPAMI, more info and longer report will be available\n  in :http://www.eecs.qmul.ac.uk/~yf300/embedding/index.html", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2408354", "report-no": null, "categories": "cs.CV cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing zero-shot learning approaches exploit transfer learning via an\nintermediate-level semantic representation shared between an annotated\nauxiliary dataset and a target dataset with different classes and no\nannotation. A projection from a low-level feature space to the semantic\nrepresentation space is learned from the auxiliary dataset and is applied\nwithout adaptation to the target dataset. In this paper we identify two\ninherent limitations with these approaches. First, due to having disjoint and\npotentially unrelated classes, the projection functions learned from the\nauxiliary dataset/domain are biased when applied directly to the target\ndataset/domain. We call this problem the projection domain shift problem and\npropose a novel framework, transductive multi-view embedding, to solve it. The\nsecond limitation is the prototype sparsity problem which refers to the fact\nthat for each target class, only a single prototype is available for zero-shot\nlearning given a semantic representation. To overcome this problem, a novel\nheterogeneous multi-view hypergraph label propagation method is formulated for\nzero-shot learning in the transductive embedding space. It effectively exploits\nthe complementary information offered by different semantic representations and\ntakes advantage of the manifold structures of multiple representation spaces in\na coherent manner. We demonstrate through extensive experiments that the\nproposed approach (1) rectifies the projection shift between the auxiliary and\ntarget domains, (2) exploits the complementarity of multiple semantic\nrepresentations, (3) significantly outperforms existing methods for both\nzero-shot and N-shot recognition on three image and video benchmark datasets,\nand (4) enables novel cross-view annotation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 17:04:11 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:43:44 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Fu", "Yanwei", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1501.04587", "submitter": "Naiyan Wang", "authors": "Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung", "title": "Transferring Rich Feature Hierarchies for Robust Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) models have demonstrated great success in\nvarious computer vision tasks including image classification and object\ndetection. However, some equally important tasks such as visual tracking remain\nrelatively unexplored. We believe that a major hurdle that hinders the\napplication of CNN to visual tracking is the lack of properly labeled training\ndata. While existing applications that liberate the power of CNN often need an\nenormous amount of training data in the order of millions, visual tracking\napplications typically have only one labeled example in the first frame of each\nvideo. We address this research issue here by pre-training a CNN offline and\nthen transferring the rich feature hierarchies learned to online tracking. The\nCNN is also fine-tuned during online tracking to adapt to the appearance of the\ntracked target specified in the first video frame. To fit the characteristics\nof object tracking, we first pre-train the CNN to recognize what is an object,\nand then propose to generate a probability map instead of producing a simple\nclass label. Using two challenging open benchmarks for performance evaluation,\nour proposed tracker has demonstrated substantial improvement over other\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 18:54:34 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 06:18:09 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Wang", "Naiyan", ""], ["Li", "Siyi", ""], ["Gupta", "Abhinav", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1501.04656", "submitter": "Marcus A. Brubaker", "authors": "Ali Punjani and Marcus A. Brubaker", "title": "Microscopic Advances with Large-Scale Learning: Stochastic Optimization\n  for Cryo-EM", "comments": "Presented at NIPS 2014 Workshop on Machine Learning in Computational\n  Biology http://mlcb.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the 3D structures of biological molecules is a key problem for\nboth biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising\ntechnique for structure estimation which relies heavily on computational\nmethods to reconstruct 3D structures from 2D images. This paper introduces the\nchallenging Cryo-EM density estimation problem as a novel application for\nstochastic optimization techniques. Structure discovery is formulated as MAP\nestimation in a probabilistic latent-variable model, resulting in an\noptimization problem to which an array of seven stochastic optimization methods\nare applied. The methods are tested on both real and synthetic data, with some\nmethods recovering reasonable structures in less than one epoch from a random\ninitialization. Complex quasi-Newton methods are found to converge more slowly\nthan simple gradient-based methods, but all stochastic methods are found to\nconverge to similar optima. This method represents a major improvement over\nexisting methods as it is significantly faster and is able to converge from a\nrandom initialization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 22:07:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 21:13:28 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Punjani", "Ali", ""], ["Brubaker", "Marcus A.", ""]]}, {"id": "1501.04686", "submitter": "Pichao Wang", "authors": "Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang and Philip\n  Ogunbona", "title": "Deep Convolutional Neural Networks for Action Recognition Using Depth\n  Map Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approach has achieved promising results in various\nfields of computer vision. In this paper, a new framework called Hierarchical\nDepth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks\n(3ConvNets) is proposed for human action recognition using depth map sequences.\nFirstly, we rotate the original depth data in 3D pointclouds to mimic the\nrotation of cameras, so that our algorithms can handle view variant cases.\nSecondly, in order to effectively extract the body shape and motion\ninformation, we generate weighted depth motion maps (DMM) at several temporal\nscales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, three\nchannels of ConvNets are trained on the HDMMs from three projected orthogonal\nplanes separately. The proposed algorithms are evaluated on MSRAction3D,\nMSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively.\nWe also combine the last three datasets into a larger one (called Combined\nDataset) and test the proposed method on it. The results show that our approach\ncan achieve state-of-the-art results on the individual datasets and without\ndramatical performance degradation on the Combined Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 00:46:10 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Gao", "Zhimin", ""], ["Zhang", "Jing", ""], ["Tang", "Chang", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1501.04690", "submitter": "Erjin Zhou", "authors": "Erjin Zhou, Zhimin Cao, Qi Yin", "title": "Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition performance improves rapidly with the recent deep learning\ntechnique developing and underlying large training dataset accumulating. In\nthis paper, we report our observations on how big data impacts the recognition\nperformance. According to these observations, we build our Megvii Face\nRecognition System, which achieves 99.50% accuracy on the LFW benchmark,\noutperforming the previous state-of-the-art. Furthermore, we report the\nperformance in a real-world security certification scenario. There still exists\na clear gap between machine recognition and human performance. We summarize our\nexperiments and present three challenges lying ahead in recent face\nrecognition. And we indicate several possible solutions towards these\nchallenges. We hope our work will stimulate the community's discussion of the\ndifference between research benchmark and real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 01:15:02 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Zhou", "Erjin", ""], ["Cao", "Zhimin", ""], ["Yin", "Qi", ""]]}, {"id": "1501.04691", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Tracing the boundaries of materials in transparent vessels using\n  computer vision", "comments": "Code and documentation for the method described is freely available\n  at:\n  http://www.mathworks.com/matlabcentral/fileexchange/49076-find-the-boundaries-of-materials-in-transparent-vessels-using-computer-vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition of material boundaries in transparent vessels is valuable\nfor numerous applications. Such recognition is essential for estimation of\nfill-level, volume and phase-boundaries as well as for tracking of such\nchemical processes as precipitation, crystallization, condensation, evaporation\nand phase-separation. The problem of material boundary recognition in images is\nparticularly complex for materials with non-flat surfaces, i.e., solids,\npowders and viscous fluids, in which the material interfaces have unpredictable\nshapes. This work demonstrates a general method for finding the boundaries of\nmaterials inside transparent containers in images. The method uses an image of\nthe transparent vessel containing the material and the boundary of the vessel\nin this image. The recognition is based on the assumption that the material\nboundary appears in the image in the form of a curve (with various constraints)\nwhose endpoints are both positioned on the vessel contour. The probability that\na curve matches the material boundary in the image is evaluated using a cost\nfunction based on some image properties along this curve. Several image\nproperties were examined as indicators for the material boundary. The optimal\nboundary curve was found using Dijkstra's algorithm. The method was\nsuccessfully examined for recognition of various types of phase-boundaries,\nincluding liquid-air, solid-air and solid-liquid interfaces, as well as for\nvarious types of glassware containers from everyday life and the chemistry\nlaboratory (i.e., bottles, beakers, flasks, jars, columns, vials and\nseparation-funnels). In addition, the method can be easily extended to\nmaterials carried on top of carrier vessels (i.e., plates, spoons, spatulas).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 01:19:33 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 17:12:32 GMT"}, {"version": "v3", "created": "Sat, 31 Jan 2015 20:59:29 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1501.04711", "submitter": "Vijay Chandrasekhar", "authors": "Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, Hanlin\n  Goh", "title": "DeepHash: Getting Regularization, Depth and Fine-Tuning Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on representing very high-dimensional global image\ndescriptors using very compact 64-1024 bit binary hashes for instance\nretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to\nmaking DeepHash work at extremely low bitrates are three important\nconsiderations -- regularization, depth and fine-tuning -- each requiring\nsolutions specific to the hashing problem. In-depth evaluation shows that our\nscheme consistently outperforms state-of-the-art methods across all data sets\nfor both Fisher Vectors and Deep Convolutional Neural Network features, by up\nto 20 percent over other schemes. The retrieval performance with 256-bit hashes\nis close to that of the uncompressed floating point features -- a remarkable\n512 times compression.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 04:36:12 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lin", "Jie", ""], ["Morere", "Olivier", ""], ["Chandrasekhar", "Vijay", ""], ["Veillard", "Antoine", ""], ["Goh", "Hanlin", ""]]}, {"id": "1501.04717", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Kui Jia, Yueming Wang, Gang Pan, Tsung-Han Chan, Yi Ma", "title": "Robust Face Recognition by Constrained Part-based Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a reliable and practical face recognition system is a\nlong-standing goal in computer vision research. Existing literature suggests\nthat pixel-wise face alignment is the key to achieve high-accuracy face\nrecognition. By assuming a human face as piece-wise planar surfaces, where each\nsurface corresponds to a facial part, we develop in this paper a Constrained\nPart-based Alignment (CPA) algorithm for face recognition across pose and/or\nexpression. Our proposed algorithm is based on a trainable CPA model, which\nlearns appearance evidence of individual parts and a tree-structured shape\nconfiguration among different parts. Given a probe face, CPA simultaneously\naligns all its parts by fitting them to the appearance evidence with\nconsideration of the constraint from the tree-structured shape configuration.\nThis objective is formulated as a norm minimization problem regularized by\ngraph likelihoods. CPA can be easily integrated with many existing classifiers\nto perform part-based face recognition. Extensive experiments on benchmark face\ndatasets show that CPA outperforms or is on par with existing methods for\nrobust face recognition across pose, expression, and/or illumination changes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 06:05:01 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Zhang", "Yuting", ""], ["Jia", "Kui", ""], ["Wang", "Yueming", ""], ["Pan", "Gang", ""], ["Chan", "Tsung-Han", ""], ["Ma", "Yi", ""]]}, {"id": "1501.04754", "submitter": "Jiuqing Wan", "authors": "Jiuqing Wan, Yuting Nie and Li Liu", "title": "Distributed Data Association in Smart Camera Networks via Dual\n  Decomposition", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental requirements for visual surveillance using smart\ncamera networks is the correct association of each persons observations\ngenerated on different cameras. Recently, distributed data association that\ninvolves only local information processing on each camera node and mutual\ninformation exchanging between neighboring cameras has attracted many research\ninterests due to its superiority in large scale applications. In this paper, we\nformulate the problem of data association in smart camera networks as an\nInteger Programming problem by introducing a set of linking variables, and\npropose two distributed algorithms, namely L-DD and Q-DD, to solve the Integer\nProgramming problem using dual decomposition technique. In our algorithms, the\noriginal IP problem is decomposed into several sub-problems, which can be\nsolved locally and efficiently on each smart camera, and then different\nsub-problems reach consensus on their solutions in a rigorous way by adjusting\ntheir parameters based on projected sub-gradient optimization. The proposed\nmethods are simple and flexible, in that (i) we can incorporate any feature\nextraction and matching technique into our framework to measure the similarity\nbetween two observations, which is used to define the cost of each link, and\n(ii) we can decompose the original problem in any way as long as the resulting\nsub-problem can be solved independently on individual camera. We show the\ncompetitiveness of our methods in both accuracy and speed by theoretical\nanalysis and experimental comparison with state of the art algorithms on two\nreal data sets collected by camera networks in our campus garden and office\nbuilding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 10:24:31 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Wan", "Jiuqing", ""], ["Nie", "Yuting", ""], ["Liu", "Li", ""]]}, {"id": "1501.04782", "submitter": "Nenad Marku\\v{s}", "authors": "Nenad Marku\\v{s} and Igor S. Pand\\v{z}i\\'c and J\\\"orgen Ahlberg", "title": "Constructing Binary Descriptors with a Stochastic Hill Climbing Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary descriptors of image patches provide processing speed advantages and\nrequire less storage than methods that encode the patch appearance with a\nvector of real numbers. We provide evidence that, despite its simplicity, a\nstochastic hill climbing bit selection procedure for descriptor construction\ndefeats recently proposed alternatives on a standard discriminative power\nbenchmark. The method is easy to implement and understand, has no free\nparameters that need fine tuning, and runs fast.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 12:38:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 14:57:47 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Marku\u0161", "Nenad", ""], ["Pand\u017ei\u0107", "Igor S.", ""], ["Ahlberg", "J\u00f6rgen", ""]]}, {"id": "1501.04870", "submitter": "Luca Martino", "authors": "J. Read, L. Martino, P. Olmos, D. Luengo", "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to\n  Classifier Trellises", "comments": "(accepted in Pattern Recognition)", "journal-ref": "Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109", "doi": "10.1016/j.patcog.2015.01.004", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output inference tasks, such as multi-label classification, have become\nincreasingly important in recent years. A popular method for multi-label\nclassification is classifier chains, in which the predictions of individual\nclassifiers are cascaded along a chain, thus taking into account inter-label\ndependencies and improving the overall performance. Several varieties of\nclassifier chain methods have been introduced, and many of them perform very\ncompetitively across a wide range of benchmark datasets. However, scalability\nlimitations become apparent on larger datasets when modeling a fully-cascaded\nchain. In particular, the methods' strategies for discovering and modeling a\ngood chain structure constitutes a mayor computational bottleneck. In this\npaper, we present the classifier trellis (CT) method for scalable multi-label\nclassification. We compare CT with several recently proposed classifier chain\nmethods to show that it occupies an important niche: it is highly competitive\non standard multi-label problems, yet it can also scale up to thousands or even\ntens of thousands of labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:33:40 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Read", "J.", ""], ["Martino", "L.", ""], ["Olmos", "P.", ""], ["Luengo", "D.", ""]]}, {"id": "1501.04878", "submitter": "Nikhil Naik", "authors": "Nikhil Naik, Achuta Kadambi, Christoph Rhemann, Shahram Izadi, Ramesh\n  Raskar, Sing Bing Kang", "title": "A Light Transport Model for Mitigating Multipath Interference in TOF\n  Sensors", "comments": "This paper has been withdrawn by the submitter as the submission was\n  made due to a miscommunication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-wave Time-of-flight (TOF) range imaging has become a commercially\nviable technology with many applications in computer vision and graphics.\nHowever, the depth images obtained from TOF cameras contain scene dependent\nerrors due to multipath interference (MPI). Specifically, MPI occurs when\nmultiple optical reflections return to a single spatial location on the imaging\nsensor. Many prior approaches to rectifying MPI rely on sparsity in optical\nreflections, which is an extreme simplification. In this paper, we correct MPI\nby combining the standard measurements from a TOF camera with information from\ndirect and global light transport. We report results on both simulated\nexperiments and physical experiments (using the Kinect sensor). Our results,\nevaluated against ground truth, demonstrate a quantitative improvement in depth\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:56:49 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 21:11:48 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Naik", "Nikhil", ""], ["Kadambi", "Achuta", ""], ["Rhemann", "Christoph", ""], ["Izadi", "Shahram", ""], ["Raskar", "Ramesh", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1501.05152", "submitter": "Heng Yang", "authors": "Heng Yang and Ioannis Patras", "title": "Mirror, mirror on the wall, tell me, is the error small?", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do object part localization methods produce bilaterally symmetric results on\nmirror images? Surprisingly not, even though state of the art methods augment\nthe training set with mirrored images. In this paper we take a closer look into\nthis issue. We first introduce the concept of mirrorability as the ability of a\nmodel to produce symmetric results in mirrored images and introduce a\ncorresponding measure, namely the \\textit{mirror error} that is defined as the\ndifference between the detection result on an image and the mirror of the\ndetection result on its mirror image. We evaluate the mirrorability of several\nstate of the art algorithms in two of the most intensively studied problems,\nnamely human pose estimation and face alignment. Our experiments lead to\nseveral interesting findings: 1) Surprisingly, most of state of the art methods\nstruggle to preserve the mirror symmetry, despite the fact that they do have\nvery similar overall performance on the original and mirror images; 2) the low\nmirrorability is not caused by training or testing sample bias - all algorithms\nare trained on both the original images and their mirrored versions; 3) the\nmirror error is strongly correlated to the localization/alignment error (with\ncorrelation coefficients around 0.7). Since the mirror error is calculated\nwithout knowledge of the ground truth, we show two interesting applications -\nin the first it is used to guide the selection of difficult samples and in the\nsecond to give feedback in a popular Cascaded Pose Regression method for face\nalignment.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 12:22:38 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Yang", "Heng", ""], ["Patras", "Ioannis", ""]]}, {"id": "1501.05192", "submitter": "Umit Rusen Aktas", "authors": "Umit Rusen Aktas, Mete Ozay, Ales Leonardis, Jeremy L. Wyatt", "title": "A Graph Theoretic Approach for Object Shape Representation in\n  Compositional Hierarchies Using a Hybrid Generative-Descriptive Model", "comments": "Paper : 17 pages. 13th European Conference on Computer Vision (ECCV\n  2014), Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III, pp\n  566-581. Supplementary material can be downloaded from\n  http://link.springer.com/content/esm/chp:10.1007/978-3-319-10578-9_37/file/MediaObjects/978-3-319-10578-9_37_MOESM1_ESM.pdf", "journal-ref": null, "doi": "10.1007/978-3-319-10578-9_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph theoretic approach is proposed for object shape representation in a\nhierarchical compositional architecture called Compositional Hierarchy of Parts\n(CHOP). In the proposed approach, vocabulary learning is performed using a\nhybrid generative-descriptive model. First, statistical relationships between\nparts are learned using a Minimum Conditional Entropy Clustering algorithm.\nThen, selection of descriptive parts is defined as a frequent subgraph\ndiscovery problem, and solved using a Minimum Description Length (MDL)\nprinciple. Finally, part compositions are constructed by compressing the\ninternal data representation with discovered substructures. Shape\nrepresentation and computational complexity properties of the proposed approach\nand algorithms are examined using six benchmark two-dimensional shape image\ndatasets. Experiments show that CHOP can employ part shareability and indexing\nmechanisms for fast inference of part compositions using learned shape\nvocabularies. Additionally, CHOP provides better shape retrieval performance\nthan the state-of-the-art shape retrieval methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:19:09 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 16:04:57 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Aktas", "Umit Rusen", ""], ["Ozay", "Mete", ""], ["Leonardis", "Ales", ""], ["Wyatt", "Jeremy L.", ""]]}, {"id": "1501.05352", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Ramin Raziperchikolaei and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "comments": "22 pages, 12 figures; added new experiments and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised binary hashing, one wants to learn a function that maps a\nhigh-dimensional feature vector to a vector of binary codes, for application to\nfast image retrieval. This typically results in a difficult optimization\nproblem, nonconvex and nonsmooth, because of the discrete variables involved.\nMuch work has simply relaxed the problem during training, solving a continuous\noptimization, and truncating the codes a posteriori. This gives reasonable\nresults but is quite suboptimal. Recent work has tried to optimize the\nobjective directly over the binary codes and achieved better results, but the\nhash function was still learned a posteriori, which remains suboptimal. We\npropose a general framework for learning hash functions using affinity-based\nloss functions that uses auxiliary coordinates. This closes the loop and\noptimizes jointly over the hash functions and the binary codes so that they\ngradually match each other. The resulting algorithm can be seen as a corrected,\niterated version of the procedure of optimizing first over the codes and then\nlearning the hash function. Compared to this, our optimization is guaranteed to\nobtain better hash functions while being not much slower, as demonstrated\nexperimentally in various supervised datasets. In addition, our framework\nfacilitates the design of optimization algorithms for arbitrary types of loss\nand hash functions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 23:53:47 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 01:25:26 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Raziperchikolaei", "Ramin", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1501.05382", "submitter": "Wenjuan Gong", "authors": "Wenjuan Gong and Yongzhen Huang and Jordi Gonzalez and and Liang Wang", "title": "Enhanced Mixtures of Part Model for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of parts model has been successfully applied to 2D human pose\nestimation problem either as explicitly trained body part model or as latent\nvariables for the whole human body model. Mixture of parts model usually\nutilize tree structure for representing relations between body parts. Tree\nstructures facilitate training and referencing of the model but could not deal\nwith double counting problems, which hinder its applications in 3D pose\nestimation. While most of work targeted to solve these problems tend to modify\nthe tree models or the optimization target. We incorporate other cues from\ninput features. For example, in surveillance environments, human silhouettes\ncan be extracted relative easily although not flawlessly. In this condition, we\ncan combine extracted human blobs with histogram of gradient feature, which is\ncommonly used in mixture of parts model for training body part templates. The\nmethod can be easily extend to other candidate features under our generalized\nframework. We show 2D body part detection results on a public available\ndataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trained\nwith Gaussian process regression model and 2D body part detections from the\nproposed method is fed to the estimator, thus 3D poses are predictable given\nnew 2D body part detections. We also show results of 3D pose estimation on\nHumanEva dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 03:54:15 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 08:16:55 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Gong", "Wenjuan", ""], ["Huang", "Yongzhen", ""], ["Gonzalez", "Jordi", ""], ["Wang", "and Liang", ""]]}, {"id": "1501.05432", "submitter": "Xingyu Wu", "authors": "Xingyu Wu and Xia Mao and Lijiang Chen and Yuli Xue and Angelo Compare", "title": "Point Context: An Effective Shape Descriptor for RST-invariant\n  Trajectory Recognition", "comments": "11 pages, 10 figures", "journal-ref": "Journal of Mathematical Imaging and Vision, 2016", "doi": "10.1007/s10851-016-0648-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion trajectory recognition is important for characterizing the moving\nproperty of an object. The speed and accuracy of trajectory recognition rely on\na compact and discriminative feature representation, and the situations of\nvarying rotation, scaling and translation has to be specially considered. In\nthis paper we propose a novel feature extraction method for trajectories.\nFirstly a trajectory is represented by a proposed point context, which is a\nrotation-scale-translation (RST) invariant shape descriptor with a flexible\ntradeoff between computational complexity and discrimination, yet we prove that\nit is a complete shape descriptor. Secondly, the shape context is nonlinearly\nmapped to a subspace by kernel nonparametric discriminant analysis (KNDA) to\nget a compact feature representation, and thus a trajectory is projected to a\nsingle point in a low-dimensional feature space. Experimental results show\nthat, the proposed trajectory feature shows encouraging improvement than\nstate-of-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 09:10:28 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Wu", "Xingyu", ""], ["Mao", "Xia", ""], ["Chen", "Lijiang", ""], ["Xue", "Yuli", ""], ["Compare", "Angelo", ""]]}, {"id": "1501.05472", "submitter": "Subhadip Basu", "authors": "Ram Sarkar, Bibhash Sen, Nibaran Das, Subhadip Basu", "title": "Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach", "comments": "In Proceedings of IEEE Conference on AI Tools and Engineering\n  (ICAITE-08), March 6-8, 2008, Pune", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper concentrates on improvement of segmentation accuracy by addressing\nsome of the key challenges of handwritten Devanagari word image segmentation\ntechnique. In the present work, we have developed a new feature based approach\nfor identification of Matra pixels from a word image, design of a non-linear\nfuzzy membership functions for headline estimation and finally design of a\nnon-linear fuzzy functions for identifying segmentation points on the Matra.\nThe segmentation accuracy achieved by the current technique is 94.8%. This\nshows an improvement of performance by 1.8% over the previous technique [1] on\na 300-word dataset, used for the current experiment.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 12:05:25 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Sarkar", "Ram", ""], ["Sen", "Bibhash", ""], ["Das", "Nibaran", ""], ["Basu", "Subhadip", ""]]}, {"id": "1501.05494", "submitter": "Subhadip Basu", "authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram\n  Sarkar, Mahantapas Kundu", "title": "Design of a novel convex hull based feature set for recognition of\n  isolated handwritten Roman numerals", "comments": "In proceedings of UB NE ASEE 2009 conference, University of\n  Bridgeport, USA. arXiv admin note: substantial text overlap with\n  arXiv:1410.0478", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, convex hull based features are used for recognition of\nisolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier.\nExperiments of convex hull based features for handwritten character recognition\nare few in numbers. Convex hull of a pattern and the centroid of the convex\nhull both are affine invariant attributes. In this work, 25 features are\nextracted based on different bays attributes of the convex hull of the digit\npatterns. Then these patterns are divided into four sub-images with respect to\nthe centroid of the convex hull boundary. From each such sub-image 25 bays\nfeatures are also calculated. In all 125 convex hull based features are\nextracted for each numeric digit patterns under the current experiment. The\nperformance of the designed feature set is tested on the standard MNIST data\nset, consisting of 60000 training and 10000 test images of handwritten Roman\nusing an MLP based classifier a maximum success rate of 97.44% is achieved on\nthe test data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 13:43:01 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Das", "Nibaran", ""], ["Pramanik", "Sandip", ""], ["Basu", "Subhadip", ""], ["Saha", "Punam Kumar", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1501.05495", "submitter": "Subhadip Basu", "authors": "Nibaran Das, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas\n  Kundu, Mita Nasipuri", "title": "A GA Based approach for selection of local features for recognition of\n  handwritten Bangla numerals", "comments": "In proceedings of UB NE ASEE 2009 conference, University of\n  Bridgeport, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft computing approaches are mainly designed to address the real world\nill-defined, imprecisely formulated problems, combining different kind of novel\nmodels of computation, such as neural networks, genetic algorithms (GAs.\nHandwritten digit recognition is a typical example of one such problem. In the\ncurrent work we have developed a two-pass approach where the first pass\nclassifier performs a coarse classification, based on some global features of\nthe input pattern by restricting the possibility of classification decisions\nwithin a group of classes, smaller than the number of classes considered\ninitially. In the second pass, the group specific classifiers concentrate on\nthe features extracted from the selected local regions, and refine the earlier\ndecision by combining the local and the global features for selecting the true\nclass of the input pattern from the group of candidate classes selected in the\nfirst pass. To optimize the selection of local regions a GA based approach has\nbeen developed here. The maximum recognition performance on Bangla digit\nsamples as achieved on the test set, during the first pass of the two pass\napproach is 93.35%. After combining the results of the two stage classifiers,\nan overall success rate of 95.25% is achieved.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 13:46:06 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Das", "Nibaran", ""], ["Basu", "Subhadip", ""], ["Saha", "Punam Kumar", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1501.05497", "submitter": "Subhadip Basu", "authors": "Nibaran Das, Subhadip Basu, Ram Sarkar, Mahantapas Kundu, Mita\n  Nasipuri, Dipak kumar Basu", "title": "An Improved Feature Descriptor for Recognition of Handwritten Bangla\n  Alphabet", "comments": "In proceedings of ICSIP 2009, pp. 451 to 454, August 2009, Mysore,\n  India. arXiv admin note: substantial text overlap with arXiv:1203.0882,\n  arXiv:1002.4040, arXiv:1410.0478", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate feature set for representation of pattern classes is one of the\nmost important aspects of handwritten character recognition. The effectiveness\nof features depends on the discriminating power of the features chosen to\nrepresent patterns of different classes. However, discriminatory features are\nnot easily measurable. Investigative experimentation is necessary for\nidentifying discriminatory features. In the present work we have identified a\nnew variation of feature set which significantly outperforms on handwritten\nBangla alphabet from the previously used feature set. 132 number of features in\nall viz. modified shadow features, octant and centroid features, distance based\nfeatures, quad tree based longest run features are used here. Using this\nfeature set the recognition performance increases sharply from the 75.05%\nobserved in our previous work [7], to 85.40% on 50 character classes with MLP\nbased classifier on the same dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 13:50:25 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Das", "Nibaran", ""], ["Basu", "Subhadip", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak kumar", ""]]}, {"id": "1501.05499", "submitter": "Xinchao Wang", "authors": "Engin T\\\"uretken, Xinchao Wang, Carlos Becker, Carsten Haubold, Pascal\n  Fua", "title": "Globally Optimal Cell Tracking using Integer Programming", "comments": "Engin T\\\"uretken and Xinchao Wang contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to automatically tracking cell populations in\ntime-lapse images. To account for cell occlusions and overlaps, we introduce a\nrobust method that generates an over-complete set of competing detection\nhypotheses. We then perform detection and tracking simultaneously on these\nhypotheses by solving to optimality an integer program with only one type of\nflow variables. This eliminates the need for heuristics to handle missed\ndetections due to occlusions and complex morphology. We demonstrate the\neffectiveness of our approach on a range of challenging sequences consisting of\nclumped cells and show that it outperforms state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 13:51:06 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 19:44:06 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["T\u00fcretken", "Engin", ""], ["Wang", "Xinchao", ""], ["Becker", "Carlos", ""], ["Haubold", "Carsten", ""], ["Fua", "Pascal", ""]]}, {"id": "1501.05552", "submitter": "Abderrahim Halimi", "authors": "A. Halimi and P. Honeine and M. Kharouf and C. Richard and J.-Y.\n  Tourneret", "title": "Estimating the Intrinsic Dimension of Hyperspectral Images Using an\n  Eigen-Gap Approach", "comments": "21 pages, 4 figures and 4 tables", "journal-ref": null, "doi": "10.1109/TGRS.2016.2528298", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixture models are commonly used to represent hyperspectral datacube\nas a linear combinations of endmember spectra. However, determining of the\nnumber of endmembers for images embedded in noise is a crucial task. This paper\nproposes a fully automatic approach for estimating the number of endmembers in\nhyperspectral images. The estimation is based on recent results of random\nmatrix theory related to the so-called spiked population model. More precisely,\nwe study the gap between successive eigenvalues of the sample covariance matrix\nconstructed from high dimensional noisy samples. The resulting estimation\nstrategy is unsupervised and robust to correlated noise. This strategy is\nvalidated on both synthetic and real images. The experimental results are very\npromising and show the accuracy of this algorithm with respect to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 16:18:35 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Halimi", "A.", ""], ["Honeine", "P.", ""], ["Kharouf", "M.", ""], ["Richard", "C.", ""], ["Tourneret", "J. -Y.", ""]]}, {"id": "1501.05617", "submitter": "Mohamed Ali Mahjoub", "authors": "Mohamed Ali Mahjoub, Mohamed Mhiri", "title": "Unsupervised image segmentation by Global and local Criteria\n  Optimization Based on Bayesian Networks", "comments": "appears in International journal of robotics and imaging; volume 15,\n  issue 1, januray 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today Bayesian networks are more used in many areas of decision support and\nimage processing. In this way, our proposed approach uses Bayesian Network to\nmodelize the segmented image quality. This quality is calculated on a set of\nattributes that represent local evaluation measures. The idea is to have these\nlocal levels chosen in a way to be intersected into them to keep the overall\nappearance of segmentation. The approach operates in two phases: the first\nphase is to make an over-segmentation which gives superpixels card. In the\nsecond phase, we model the superpixels by a Bayesian Network. To find the\nsegmented image with the best overall quality we used two approximate inference\nmethods, the first using ICM algorithm which is widely used in Markov Models\nand a second is a recursive method called algorithm of model decomposition\nbased on max-product algorithm which is very popular in the recent works of\nimage segmentation. For our model, we have shown that the composition of these\ntwo algorithms leads to good segmentation performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 20:04:20 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Mahjoub", "Mohamed Ali", ""], ["Mhiri", "Mohamed", ""]]}, {"id": "1501.05680", "submitter": "Marc Niethammer", "authors": "Marc Niethammer, Kilian M. Pohl, Firdaus Janoos, William M. Wells III", "title": "Active Mean Fields for Probabilistic Image Segmentation: Connections\n  with Chan-Vese and Rudin-Osher-Fatemi Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a fundamental task for extracting semantically meaningful\nregions from an image. The goal of segmentation algorithms is to accurately\nassign object labels to each image location. However, image-noise, shortcomings\nof algorithms, and image ambiguities cause uncertainty in label assignment.\nEstimating the uncertainty in label assignment is important in multiple\napplication domains, such as segmenting tumors from medical images for\nradiation treatment planning. One way to estimate these uncertainties is\nthrough the computation of posteriors of Bayesian models, which is\ncomputationally prohibitive for many practical applications. On the other hand,\nmost computationally efficient methods fail to estimate label uncertainty. We\ntherefore propose in this paper the Active Mean Fields (AMF) approach, a\ntechnique based on Bayesian modeling that uses a mean-field approximation to\nefficiently compute a segmentation and its corresponding uncertainty. Based on\na variational formulation, the resulting convex model combines any\nlabel-likelihood measure with a prior on the length of the segmentation\nboundary. A specific implementation of that model is the Chan-Vese segmentation\nmodel (CV), in which the binary segmentation task is defined by a Gaussian\nlikelihood and a prior regularizing the length of the segmentation boundary.\nFurthermore, the Euler-Lagrange equations derived from the AMF model are\nequivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image\ndenoising. Solutions to the AMF model can thus be implemented by directly\nutilizing highly-efficient ROF solvers on log-likelihood ratio fields. We\nqualitatively assess the approach on synthetic data as well as on real natural\nand medical images. For a quantitative evaluation, we apply our approach to the\nicgbench dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:47:35 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 23:10:27 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Niethammer", "Marc", ""], ["Pohl", "Kilian M.", ""], ["Janoos", "Firdaus", ""], ["Wells", "William M.", "III"]]}, {"id": "1501.05684", "submitter": "Paul Honeine", "authors": "Paul Honeine, Fei Zhu", "title": "Bi-Objective Nonnegative Matrix Factorization: Linear Versus\n  Kernel-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful class of feature\nextraction techniques that has been successfully applied in many fields, namely\nin signal and image processing. Current NMF techniques have been limited to a\nsingle-objective problem in either its linear or nonlinear kernel-based\nformulation. In this paper, we propose to revisit the NMF as a multi-objective\nproblem, in particular a bi-objective one, where the objective functions\ndefined in both input and feature spaces are taken into account. By taking the\nadvantage of the sum-weighted method from the literature of multi-objective\noptimization, the proposed bi-objective NMF determines a set of nondominated,\nPareto optimal, solutions instead of a single optimal decomposition. Moreover,\nthe corresponding Pareto front is studied and approximated. Experimental\nresults on unmixing real hyperspectral images confirm the efficiency of the\nproposed bi-objective NMF compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:59:47 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Honeine", "Paul", ""], ["Zhu", "Fei", ""]]}, {"id": "1501.05703", "submitter": "Ning Zhang", "authors": "Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, Lubomir Bourdev", "title": "Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the task of recognizing peoples' identities in photo albums in an\nunconstrained setting. To facilitate this, we introduce the new People In Photo\nAlbums (PIPA) dataset, consisting of over 60000 instances of 2000 individuals\ncollected from public Flickr photo albums. With only about half of the person\nimages containing a frontal face, the recognition task is very challenging due\nto the large variations in pose, clothing, camera viewpoint, image resolution\nand illumination. We propose the Pose Invariant PErson Recognition (PIPER)\nmethod, which accumulates the cues of poselet-level person recognizers trained\nby deep convolutional networks to discount for the pose variations, combined\nwith a face recognizer and a global recognizer. Experiments on three different\nsettings confirm that in our unconstrained setup PIPER significantly improves\non the performance of DeepFace, which is one of the best face recognizers as\nmeasured on the LFW dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 02:35:01 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 18:48:27 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Zhang", "Ning", ""], ["Paluri", "Manohar", ""], ["Taigman", "Yaniv", ""], ["Fergus", "Rob", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1501.05759", "submitter": "Rodrigo Benenson", "authors": "Shanshan Zhang and Rodrigo Benenson and Bernt Schiele", "title": "Filtered Channel Features for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper starts from the observation that multiple top performing\npedestrian detectors can be modelled by using an intermediate layer filtering\nlow-level features in combination with a boosted decision forest. Based on this\nobservation we propose a unifying framework and experimentally explore\ndifferent filter families. We report extensive results enabling a systematic\nanalysis.\n  Using filtered channel features we obtain top performance on the challenging\nCaltech and KITTI datasets, while using only HOG+LUV as low-level features.\nWhen adding optical flow features we further improve detection quality and\nreport the best known results on the Caltech dataset, reaching 93% recall at 1\nFPPI.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 10:19:33 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Zhang", "Shanshan", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1501.05790", "submitter": "Rodrigo Benenson", "authors": "Jan Hosang and Mohamed Omran and Rodrigo Benenson and Bernt Schiele", "title": "Taking a Deeper Look at Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the use of convolutional neural networks (convnets)\nfor the task of pedestrian detection. Despite their recent diverse successes,\nconvnets historically underperform compared to other pedestrian detectors. We\ndeliberately omit explicitly modelling the problem into the network (e.g. parts\nor occlusion modelling) and show that we can reach competitive performance\nwithout bells and whistles. In a wide range of experiments we analyse small and\nbig convnets, their architectural choices, parameters, and the influence of\ndifferent training data, including pre-training on surrogate tasks.\n  We present the best convnet detectors on the Caltech and KITTI dataset. On\nCaltech our convnets reach top performance both for the Caltech1x and\nCaltech10x training setup. Using additional data at training time our strongest\nconvnet model is competitive even to detectors that use additional data\n(optical flow) at test time.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 13:07:56 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Hosang", "Jan", ""], ["Omran", "Mohamed", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1501.05854", "submitter": "Antonio Rueda-Toicen", "authors": "Wuilian Torres and Antonio Rueda-Toicen", "title": "Unsupervised Segmentation of Multispectral Images with Cellular Automata", "comments": "6 pages, 6 figures, conference: CIMENICS XII, 2014", "journal-ref": null, "doi": "10.13140/2.1.2250.7849", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Multispectral images acquired by satellites are used to study phenomena on\nthe Earth's surface. Unsupervised classification techniques analyze\nmultispectral image content without considering prior knowledge of the observed\nterrain; this is done using techniques which group pixels that have similar\nstatistics of digital level distribution in the various image channels. In this\npaper, we propose a methodology for unsupervised classification based on a\ndeterministic cellular automaton. The automaton is initialized in an\nunsupervised manner by setting seed cells, selected according to two criteria:\nto be representative of the spatial distribution of the dominant elements in\nthe image, and to take into account the diversity of spectral signatures in the\nimage. The automaton's evolution is based on an attack rule that is applied\nsimultaneously to all its cells. Among the noteworthy advantages of\ndeterministic cellular automata for multispectral processing of satellite\nimagery is the consideration of topological information in the image via seed\npositioning, and the ability to modify the scale of the study.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 16:11:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Torres", "Wuilian", ""], ["Rueda-Toicen", "Antonio", ""]]}, {"id": "1501.05964", "submitter": "Guangchun Cheng", "authors": "Guangchun Cheng, Yiwen Wan, Abdullah N. Saudagar, Kamesh Namuduri and\n  Bill P. Buckles", "title": "Advances in Human Action Recognition: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Human action recognition has been an important topic in computer vision due\nto its many applications such as video surveillance, human machine interaction\nand video retrieval. One core problem behind these applications is\nautomatically recognizing low-level actions and high-level activities of\ninterest. The former is usually the basis for the latter. This survey gives an\noverview of the most recent advances in human action recognition during the\npast several years, following a well-formed taxonomy proposed by a previous\nsurvey. From this state-of-the-art survey, researchers can view a panorama of\nprogress in this area for future research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 21:36:55 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Cheng", "Guangchun", ""], ["Wan", "Yiwen", ""], ["Saudagar", "Abdullah N.", ""], ["Namuduri", "Kamesh", ""], ["Buckles", "Bill P.", ""]]}, {"id": "1501.05970", "submitter": "Ju Shen Dr.", "authors": "Jianjun Yang, Yin Wang, Honggang Wang, Kun Hua, Wei Wang, Ju Shen", "title": "Automatic Objects Removal for Scene Completion", "comments": "6 pages, IEEE International Conference on Computer Communications\n  (INFOCOM 14), Workshop on Security and Privacy in Big Data, Toronto, Canada,\n  2014", "journal-ref": null, "doi": "10.1109/INFCOMW.2014.6849291", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive growth of web-based cameras and mobile devices, billions\nof photographs are uploaded to the internet. We can trivially collect a huge\nnumber of photo streams for various goals, such as 3D scene reconstruction and\nother big data applications. However, this is not an easy task due to the fact\nthe retrieved photos are neither aligned nor calibrated. Furthermore, with the\nocclusion of unexpected foreground objects like people, vehicles, it is even\nmore challenging to find feature correspondences and reconstruct realistic\nscenes. In this paper, we propose a structure based image completion algorithm\nfor object removal that produces visually plausible content with consistent\nstructure and scene texture. We use an edge matching technique to infer the\npotential structure of the unknown region. Driven by the estimated structure,\ntexture synthesis is performed automatically along the estimated curves. We\nevaluate the proposed method on different types of images: from highly\nstructured indoor environment to the natural scenes. Our experimental results\ndemonstrate satisfactory performance that can be potentially used for\nsubsequent big data processing: 3D scene reconstruction and location\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 21:46:41 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Yang", "Jianjun", ""], ["Wang", "Yin", ""], ["Wang", "Honggang", ""], ["Hua", "Kun", ""], ["Wang", "Wei", ""], ["Shen", "Ju", ""]]}, {"id": "1501.06114", "submitter": "Mahdi Salarian mr", "authors": "Mahdi Salarian", "title": "Accurate automatic segmentation of retina layers with emphasis on first\n  layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of intra-retinal boundaries in optical coherence tomography\n(OCT) is a crucial task for studying and diagnosing neurological and ocular\ndiseases. Since manual segmentation of layers is usually a time consuming task\nand relay on user, a lot of attempts done to do it automatically and without\ninterference of user. Although for extracting all layers usually same procedure\nis applied but finding the first layer is usually more difficult due to\nvanishing it in some region specially close to Fobia. To have a general\nsoftware, beside using common methods like applying shortest path algorithm on\nglobal gradient of image, some extra steps are used here to confine search area\nfor Dijstra algorithm especially for the second layer. Results demonstrates\nhigh accuracy in segmenting all present layers, especially the first one that\nis important for diagnosing issue.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 04:54:42 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 07:23:35 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Salarian", "Mahdi", ""]]}, {"id": "1501.06115", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Jun Miao, Laiyun Qing", "title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "comments": "14 pages, 6 figure, journel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) is an extremely fast learning method and has a\npowerful performance for pattern recognition tasks proven by enormous\nresearches and engineers. However, its good generalization ability is built on\nlarge numbers of hidden neurons, which is not beneficial to real time response\nin the test process. In this paper, we proposed new ways, named \"constrained\nextreme learning machines\" (CELMs), to randomly select hidden neurons based on\nsample distribution. Compared to completely random selection of hidden nodes in\nELM, the CELMs randomly select hidden nodes from the constrained vector space\ncontaining some basic combinations of original sample vectors. The experimental\nresults show that the CELMs have better generalization ability than traditional\nELM, SVM and some other related methods. Additionally, the CELMs have a similar\nfast learning speed as ELM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 05:11:34 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 11:42:01 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Zhu", "Wentao", ""], ["Miao", "Jun", ""], ["Qing", "Laiyun", ""]]}, {"id": "1501.06129", "submitter": "Swagat Kumar", "authors": "Sourav Garg, Swagat Kumar, Rajesh Ratnakaram, Prithwijit Guha", "title": "An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in\n  Dynamic Scenes", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper looks into the problem of pedestrian tracking using a monocular,\npotentially moving, uncalibrated camera. The pedestrians are located in each\nframe using a standard human detector, which are then tracked in subsequent\nframes. This is a challenging problem as one has to deal with complex\nsituations like changing background, partial or full occlusion and camera\nmotion. In order to carry out successful tracking, it is necessary to resolve\nassociations between the detected windows in the current frame with those\nobtained from the previous frame. Compared to methods that use temporal windows\nincorporating past as well as future information, we attempt to make decision\non a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolve\nthe association problem between a pair of consecutive frames by using an\naffinity matrix that defines the closeness between a pair of windows and then,\nuses a binary integer programming to obtain unique association between them. A\nsecond stage of verification based on SURF matching is used to deal with those\ncases where the above optimization scheme might yield wrong associations. The\nefficacy of the approach is demonstrated through experiments on several\nstandard pedestrian datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 08:38:48 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Garg", "Sourav", ""], ["Kumar", "Swagat", ""], ["Ratnakaram", "Rajesh", ""], ["Guha", "Prithwijit", ""]]}, {"id": "1501.06170", "submitter": "Minsu Cho", "authors": "Minsu Cho, Suha Kwak, Cordelia Schmid, Jean Ponce", "title": "Unsupervised Object Discovery and Localization in the Wild: Part-based\n  Matching with Bottom-up Region Proposals", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses unsupervised discovery and localization of dominant\nobjects from a noisy image collection with multiple object classes. The setting\nof this problem is fully unsupervised, without even image-level annotations or\nany assumption of a single dominant class. This is far more general than\ntypical colocalization, cosegmentation, or weakly-supervised localization\ntasks. We tackle the discovery and localization problem using a part-based\nregion matching approach: We use off-the-shelf region proposals to form a set\nof candidate bounding boxes for objects and object parts. These regions are\nefficiently matched across images using a probabilistic Hough transform that\nevaluates the confidence for each candidate correspondence considering both\nappearance and spatial consistency. Dominant objects are discovered and\nlocalized by comparing the scores of candidate regions and selecting those that\nstand out over other regions containing them. Extensive experimental\nevaluations on standard benchmarks demonstrate that the proposed approach\nsignificantly outperforms the current state of the art in colocalization, and\nachieves robust object discovery in challenging mixed-class datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 15:09:23 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 17:36:52 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 16:18:58 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Cho", "Minsu", ""], ["Kwak", "Suha", ""], ["Schmid", "Cordelia", ""], ["Ponce", "Jean", ""]]}, {"id": "1501.06180", "submitter": "Shanshan Zhang", "authors": "Shanshan Zhang, Christian Bauckhage, Dominik A. Klein, Armin B.\n  Cremers", "title": "Exploring Human Vision Driven Features for Pedestrian Detection", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2015.2397199", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the center-surround mechanism in the human visual attention\nsystem, we propose to use average contrast maps for the challenge of pedestrian\ndetection in street scenes due to the observation that pedestrians indeed\nexhibit discriminative contrast texture. Our main contributions are first to\ndesign a local, statistical multi-channel descriptorin order to incorporate\nboth color and gradient information. Second, we introduce a multi-direction and\nmulti-scale contrast scheme based on grid-cells in order to integrate\nexpressive local variations. Contributing to the issue of selecting most\ndiscriminative features for assessing and classification, we perform extensive\ncomparisons w.r.t. statistical descriptors, contrast measurements, and scale\nstructures. This way, we obtain reasonable results under various\nconfigurations. Empirical findings from applying our optimized detector on the\nINRIA and Caltech pedestrian datasets show that our features yield\nstate-of-the-art performance in pedestrian detection.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 16:52:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Shanshan", ""], ["Bauckhage", "Christian", ""], ["Klein", "Dominik A.", ""], ["Cremers", "Armin B.", ""]]}, {"id": "1501.06202", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang\n  Gong, Yizhou Wang, and Yuan Yao", "title": "Robust Subjective Visual Property Prediction from Crowdsourced Pairwise\n  Labels", "comments": "14 pages, accepted by IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2456887", "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating subjective visual properties from image and video\nhas attracted increasing interest. A subjective visual property is useful\neither on its own (e.g. image and video interestingness) or as an intermediate\nrepresentation for visual recognition (e.g. a relative attribute). Due to its\nambiguous nature, annotating the value of a subjective visual property for\nlearning a prediction model is challenging. To make the annotation more\nreliable, recent studies employ crowdsourcing tools to collect pairwise\ncomparison labels because human annotators are much better at ranking two\nimages/videos (e.g. which one is more interesting) than giving an absolute\nvalue to each of them separately. However, using crowdsourced data also\nintroduces outliers. Existing methods rely on majority voting to prune the\nannotation outliers/errors. They thus require large amount of pairwise labels\nto be collected. More importantly as a local outlier detection method, majority\nvoting is ineffective in identifying outliers that can cause global ranking\ninconsistencies. In this paper, we propose a more principled way to identify\nannotation outliers by formulating the subjective visual property prediction\ntask as a unified robust learning to rank problem, tackling both the outlier\ndetection and learning to rank jointly. Differing from existing methods, the\nproposed method integrates local pairwise comparison labels together to\nminimise a cost that corresponds to global inconsistency of ranking order. This\nnot only leads to better detection of annotation outliers but also enables\nlearning with extremely sparse annotations. Extensive experiments on various\nbenchmark datasets demonstrate that our new approach significantly outperforms\nstate-of-the-arts alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 20:02:45 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 05:13:45 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 18:40:56 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2015 14:42:17 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Fu", "Yanwei", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Xiong", "Jiechao", ""], ["Gong", "Shaogang", ""], ["Wang", "Yizhou", ""], ["Yao", "Yuan", ""]]}, {"id": "1501.06209", "submitter": "Martin Uecker", "authors": "Martin Uecker", "title": "Parallel Magnetic Resonance Imaging", "comments": "22 pages, 9 Figures, 76 References. Copyright: Martin Uecker. Draft\n  for a book chapter. To appear in: A Majumdar and RK Ward (eds.), MRI:\n  Physics, Image Reconstruction, and Analysis, CRC Press 2015", "journal-ref": "In: MRI: Physics, Image Reconstruction, and Analysis, CRC Press\n  2015, pp. 73-92, ISBN 9781482298871", "doi": null, "report-no": null, "categories": "cs.NA cs.CV math.NA math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scan\ntimes and, in consequence, its sensitivity to motion. Exploiting the\ncomplementary information from multiple receive coils, parallel imaging is able\nto recover images from under-sampled k-space data and to accelerate the\nmeasurement. Because parallel magnetic resonance imaging can be used to\naccelerate basically any imaging sequence it has many important applications.\nParallel imaging brought a fundamental shift in image reconstruction: Image\nreconstruction changed from a simple direct Fourier transform to the solution\nof an ill-conditioned inverse problem. This work gives an overview of image\nreconstruction from the perspective of inverse problems. After introducing\nbasic concepts such as regularization, discretization, and iterative\nreconstruction, advanced topics are discussed including algorithms for\nauto-calibration, the connection to approximation theory, and the combination\nwith compressed sensing.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 21:01:41 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 06:20:43 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Uecker", "Martin", ""]]}, {"id": "1501.06262", "submitter": "Keze Wang", "authors": "Keze Wang, Xiaolong Wang, Liang Lin, Meng Wang, Wangmeng Zuo", "title": "3D Human Activity Recognition with Reconfigurable Convolutional Neural\n  Networks", "comments": "This manuscript has 10 pages with 9 figures, and a preliminary\n  version was published in ACM MM'14 conference", "journal-ref": null, "doi": "10.1145/2647868.2654912", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity understanding with 3D/depth sensors has received increasing\nattention in multimedia processing and interactions. This work targets on\ndeveloping a novel deep model for automatic activity recognition from RGB-D\nvideos. We represent each human activity as an ensemble of cubic-like video\nsegments, and learn to discover the temporal structures for a category of\nactivities, i.e. how the activities to be decomposed in terms of\nclassification. Our model can be regarded as a structured deep architecture, as\nit extends the convolutional neural networks (CNNs) by incorporating structure\nalternatives. Specifically, we build the network consisting of 3D convolutions\nand max-pooling operators over the video segments, and introduce the latent\nvariables in each convolutional layer manipulating the activation of neurons.\nOur model thus advances existing approaches in two aspects: (i) it acts\ndirectly on the raw inputs (grayscale-depth data) to conduct recognition\ninstead of relying on hand-crafted features, and (ii) the model structure can\nbe dynamically adjusted accounting for the temporal variations of human\nactivities, i.e. the network configuration is allowed to be partially activated\nduring inference. For model training, we propose an EM-type optimization method\nthat iteratively (i) discovers the latent structure by determining the\ndecomposed actions for each training example, and (ii) learns the network\nparameters by using the back-propagation algorithm. Our approach is validated\nin challenging scenarios, and outperforms state-of-the-art methods. A large\nhuman activity database of RGB-D videos is presented in addition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 06:45:34 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 12:12:03 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 13:57:58 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Wang", "Keze", ""], ["Wang", "Xiaolong", ""], ["Lin", "Liang", ""], ["Wang", "Meng", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1501.06272", "submitter": "Fang Zhao", "authors": "Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan", "title": "Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of web images, hashing has received increasing\ninterests in large scale image retrieval. Research efforts have been devoted to\nlearning compact binary codes that preserve semantic similarity based on\nlabels. However, most of these hashing methods are designed to handle simple\nbinary similarity. The complex multilevel semantic structure of images\nassociated with multiple labels have not yet been well explored. Here we\npropose a deep semantic ranking based method for learning hash functions that\npreserve multilevel semantic similarity between multi-label images. In our\napproach, deep convolutional neural network is incorporated into hash functions\nto jointly learn feature representations and mappings from them to hash codes,\nwhich avoids the limitation of semantic representation power of hand-crafted\nfeatures. Meanwhile, a ranking list that encodes the multilevel similarity\ninformation is employed to guide the learning of such deep hash functions. An\neffective scheme based on surrogate loss is used to solve the intractable\noptimization problem of nonsmooth and multivariate ranking measures involved in\nthe learning procedure. Experimental results show the superiority of our\nproposed approach over several state-of-the-art hashing methods in term of\nranking evaluation metrics when tested on multi-label image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 07:33:40 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 04:28:58 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zhao", "Fang", ""], ["Huang", "Yongzhen", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1501.06297", "submitter": "Jonathan Masci", "authors": "Jonathan Masci and Davide Boscaini and Michael M. Bronstein and Pierre\n  Vandergheynst", "title": "Geodesic convolutional neural networks on Riemannian manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature descriptors play a crucial role in a wide range of geometry analysis\nand processing applications, including shape correspondence, retrieval, and\nsegmentation. In this paper, we introduce Geodesic Convolutional Neural\nNetworks (GCNN), a generalization of the convolutional networks (CNN) paradigm\nto non-Euclidean manifolds. Our construction is based on a local geodesic\nsystem of polar coordinates to extract \"patches\", which are then passed through\na cascade of filters and linear and non-linear operators. The coefficients of\nthe filters and linear combination weights are optimization variables that are\nlearned to minimize a task-specific cost function. We use GCNN to learn\ninvariant shape features, allowing to achieve state-of-the-art performance in\nproblems such as shape description, retrieval, and correspondence.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 09:37:58 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 11:09:13 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 13:13:11 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Masci", "Jonathan", ""], ["Boscaini", "Davide", ""], ["Bronstein", "Michael M.", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1501.06450", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-map: an Effective Nonlinear Dimensionality Reduction Method for\n  Interactive Clustering", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Scientists in many fields have the common and basic need of dimensionality\nreduction: visualizing the underlying structure of the massive multivariate\ndata in a low-dimensional space. However, many dimensionality reduction methods\nconfront the so-called \"crowding problem\" that clusters tend to overlap with\neach other in the embedding. Previously, researchers expect to avoid that\nproblem and seek to make clusters maximally separated in the embedding.\nHowever, the proposed in-tree (IT) based method, called IT-map, allows clusters\nin the embedding to be locally overlapped, while seeking to make them\ndistinguishable by some small yet key parts. IT-map provides a simple,\neffective and novel solution to cluster-preserving mapping, which makes it\npossible to cluster the original data points interactively and thus should be\nof general meaning in science and engineering.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 15:37:22 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 14:48:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.06716", "submitter": "Maria Kushnir", "authors": "Maria Kushnir and Ilan Shimshoni", "title": "A General Preprocessing Method for Improved Performance of Epipolar\n  Geometry Estimation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a deterministic preprocessing algorithm is presented, whose\noutput can be given as input to most state-of-the-art epipolar geometry\nestimation algorithms, improving their results considerably. They are now able\nto succeed on hard cases for which they failed before. The algorithm consists\nof three steps, whose scope changes from local to global. In the local step it\nextracts from a pair of images local features (e.g. SIFT). Similar features\nfrom each image are clustered and the clusters are matched yielding a large\nnumber of putative matches. In the second step pairs of spatially close\nfeatures (called 2keypoints) are matched and ranked by a classifier. The\n2keypoint matches with the highest ranks are selected. In the global step, from\neach two 2keypoint matches a fundamental matrix is computed. As quite a few of\nthe matrices are generated from correct matches they are used to rank the\nputative matches found in the first step. For each match the number of\nfundamental matrices, for which it approximately satisfies the epipolar\nconstraint, is calculated. This set of matches is combined with the putative\nmatches generated by standard methods and their probabilities to be correct are\nestimated by a classifier. These are then given as input to state-of-the-art\nepipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yielding\nmuch better results than the original algorithms. This was shown in extensive\ntesting performed on almost 900 image pairs from six publicly available\ndata-sets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 09:54:00 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Kushnir", "Maria", ""], ["Shimshoni", "Ilan", ""]]}, {"id": "1501.06722", "submitter": "Alin Popa", "authors": "Alin-Ionut Popa and Cristian Sminchisescu", "title": "Parametric Image Segmentation of Humans with Structural Shape Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The figure-ground segmentation of humans in images captured in natural\nenvironments is an outstanding open problem due to the presence of complex\nbackgrounds, articulation, varying body proportions, partial views and\nviewpoint changes. In this work we propose class-specific segmentation models\nthat leverage parametric max-flow image segmentation and a large dataset of\nhuman shapes. Our contributions are as follows: (1) formulation of a\nsub-modular energy model that combines class-specific structural constraints\nand data-driven shape priors, within a parametric max-flow optimization\nmethodology that systematically computes all breakpoints of the model in\npolynomial time; (2) design of a data-driven class-specific fusion methodology,\nbased on matching against a large training set of exemplar human shapes\n(100,000 in our experiments), that allows the shape prior to be constructed\non-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of\nstate of the art results, in two challenging datasets, H3D and MPII (where\nfigure-ground segmentation annotations have been added by us), where we\nsubstantially improve on the first ranked hypothesis estimates of mid-level\nsegmentation methods, by 20%, with hypothesis set sizes that are up to one\norder of magnitude smaller.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 10:03:45 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Popa", "Alin-Ionut", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1501.06751", "submitter": "Daphna Weinshall", "authors": "Chaim Ginzburg, Amit Raphael and Daphna Weinshall", "title": "A Cheap System for Vehicle Speed Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable detection of speed of moving vehicles is considered key to\ntraffic law enforcement in most countries, and is seen by many as an important\ntool to reduce the number of traffic accidents and fatalities. Many automatic\nsystems and different methods are employed in different countries, but as a\nrule they tend to be expensive and/or labor intensive, often employing outdated\ntechnology due to the long development time. Here we describe a speed detection\nsystem that relies on simple everyday equipment - a laptop and a consumer web\ncamera. Our method is based on tracking the license plates of cars, which gives\nthe relative movement of the cars in the image. This image displacement is\ntranslated to actual motion by using the method of projection to a reference\nplane, where the reference plane is the road itself. However, since license\nplates do not touch the road, we must compensate for the entailed distortion in\nspeed measurement. We show how to compute the compensation factor using\nknowledge of the license plate standard dimensions. Consequently our system\ncomputes the true speed of moving vehicles fast and accurately. We show\npromising results on videos obtained in a number of scenes and with different\ncar models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 11:51:58 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Ginzburg", "Chaim", ""], ["Raphael", "Amit", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1501.06993", "submitter": "Youjie Zhou", "authors": "Youjie Zhou and Hongkai Yu and Song Wang", "title": "Feature Sampling Strategies for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although dense local spatial-temporal features with bag-of-features\nrepresentation achieve state-of-the-art performance for action recognition, the\nhuge feature number and feature size prevent current methods from scaling up to\nreal size problems. In this work, we investigate different types of feature\nsampling strategies for action recognition, namely dense sampling, uniformly\nrandom sampling and selective sampling. We propose two effective selective\nsampling methods using object proposal techniques. Experiments conducted on a\nlarge video dataset show that we are able to achieve better average recognition\naccuracy using 25% less features, through one of proposed selective sampling\nmethods, and even remain comparable accuracy while discarding 70% features.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 05:41:07 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Zhou", "Youjie", ""], ["Yu", "Hongkai", ""], ["Wang", "Song", ""]]}, {"id": "1501.07180", "submitter": "Liliang Zhang", "authors": "Liliang Zhang, Liang Lin, Xian Wu, Shengyong Ding, Lei Zhang", "title": "End-to-End Photo-Sketch Generation via Fully Convolutional\n  Representation Learning", "comments": "8 pages, 6 figures. Proceeding in ACM International Conference on\n  Multimedia Retrieval (ICMR), 2015", "journal-ref": null, "doi": "10.1145/2671188.2749321", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based face recognition is an interesting task in vision and multimedia\nresearch, yet it is quite challenging due to the great difference between face\nphotos and sketches. In this paper, we propose a novel approach for\nphoto-sketch generation, aiming to automatically transform face photos into\ndetail-preserving personal sketches. Unlike the traditional models synthesizing\nsketches based on a dictionary of exemplars, we develop a fully convolutional\nnetwork to learn the end-to-end photo-sketch mapping. Our approach takes whole\nface photos as inputs and directly generates the corresponding sketch images\nwith efficient inference and learning, in which the architecture are stacked by\nonly convolutional kernels of very small sizes. To well capture the person\nidentity during the photo-sketch transformation, we define our optimization\nobjective in the form of joint generative-discriminative minimization. In\nparticular, a discriminative regularization term is incorporated into the\nphoto-sketch generation, enhancing the discriminability of the generated person\nsketches against other individuals. Extensive experiments on several standard\nbenchmarks suggest that our approach outperforms other state-of-the-art methods\nin both photo-sketch generation and face sketch verification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 16:32:53 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 14:28:21 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Zhang", "Liliang", ""], ["Lin", "Liang", ""], ["Wu", "Xian", ""], ["Ding", "Shengyong", ""], ["Zhang", "Lei", ""]]}, {"id": "1501.07304", "submitter": "Miriam Redi", "authors": "Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, Alejandro Jaimes", "title": "The Beauty of Capturing Faces: Rating the Quality of Digital Portraits", "comments": "FG 2015, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital portrait photographs are everywhere, and while the number of face\npictures keeps growing, not much work has been done to on automatic portrait\nbeauty assessment. In this paper, we design a specific framework to\nautomatically evaluate the beauty of digital portraits. To this end, we procure\na large dataset of face images annotated not only with aesthetic scores but\nalso with information about the traits of the subject portrayed. We design a\nset of visual features based on portrait photography literature, and\nextensively analyze their relation with portrait beauty, exposing interesting\nfindings about what makes a portrait beautiful. We find that the beauty of a\nportrait is linked to its artistic value, and independent from age, race and\ngender of the subject. We also show that a classifier trained with our features\nto separate beautiful portraits from non-beautiful portraits outperforms\ngeneric aesthetic classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 22:51:23 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Redi", "Miriam", ""], ["Rasiwasia", "Nikhil", ""], ["Aggarwal", "Gaurav", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "1501.07338", "submitter": "Jimmy Ren", "authors": "Jimmy SJ. Ren and Li Xu", "title": "On Vectorization of Deep Convolutional Neural Networks for Vision Tasks", "comments": "To appear in the 29th AAAI Conference on Artificial Intelligence\n  (AAAI-15). Austin, Texas, USA, January 25-30, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently have witnessed many ground-breaking results in machine learning\nand computer vision, generated by using deep convolutional neural networks\n(CNN). While the success mainly stems from the large volume of training data\nand the deep network architectures, the vector processing hardware (e.g. GPU)\nundisputedly plays a vital role in modern CNN implementations to support\nmassive computation. Though much attention was paid in the extent literature to\nunderstand the algorithmic side of deep CNN, little research was dedicated to\nthe vectorization for scaling up CNNs. In this paper, we studied the\nvectorization process of key building blocks in deep CNNs, in order to better\nunderstand and facilitate parallel implementation. Key steps in training and\ntesting deep CNNs are abstracted as matrix and vector operators, upon which\nparallelism can be easily achieved. We developed and compared six\nimplementations with various degrees of vectorization with which we illustrated\nthe impact of vectorization on the speed of model training and testing.\nBesides, a unified CNN framework for both high-level and low-level vision tasks\nis provided, along with a vectorized Matlab implementation with\nstate-of-the-art speed performance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 03:39:26 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Ren", "Jimmy SJ.", ""], ["Xu", "Li", ""]]}, {"id": "1501.07359", "submitter": "Tianfu Wu", "authors": "Tianfu Wu and Bo Li and Song-Chun Zhu", "title": "Learning And-Or Models to Represent Context and Occlusion for Car\n  Detection and Viewpoint Estimation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for learning And-Or models to represent context\nand occlusion for car detection and viewpoint estimation. The learned And-Or\nmodel represents car-to-car context and occlusion configurations at three\nlevels: (i) spatially-aligned cars, (ii) single car under different occlusion\nconfigurations, and (iii) a small number of parts. The And-Or model embeds a\ngrammar for representing large structural and appearance variations in a\nreconfigurable hierarchy. The learning process consists of two stages in a\nweakly supervised way (i.e., only bounding boxes of single cars are annotated).\nFirstly, the structure of the And-Or model is learned with three components:\n(a) mining multi-car contextual patterns based on layouts of annotated single\ncar bounding boxes, (b) mining occlusion configurations between single cars,\nand (c) learning different combinations of part visibility based on car 3D CAD\nsimulation. The And-Or model is organized in a directed and acyclic graph which\ncan be inferred by Dynamic Programming. Secondly, the model parameters (for\nappearance, deformation and bias) are jointly trained using Weak-Label\nStructural SVM. In experiments, we test our model on four car detection\ndatasets --- the KITTI dataset \\cite{Geiger12}, the PASCAL VOC2007 car\ndataset~\\cite{pascal}, and two self-collected car datasets, namely the\nStreet-Parking car dataset and the Parking-Lot car dataset, and three datasets\nfor car viewpoint estimation --- the PASCAL VOC2006 car dataset~\\cite{pascal},\nthe 3D car dataset~\\cite{savarese}, and the PASCAL3D+ car\ndataset~\\cite{xiang_wacv14}. Compared with state-of-the-art variants of\ndeformable part-based models and other methods, our model achieves significant\nimprovement consistently on the four detection datasets, and comparable\nperformance on car viewpoint estimation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 07:30:13 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 08:25:35 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Wu", "Tianfu", ""], ["Li", "Bo", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1501.07422", "submitter": "Kohta Ishikawa", "authors": "Kohta Ishikawa, Ikuro Sato, Mitsuru Ambai", "title": "Pairwise Rotation Hashing for High-dimensional Features", "comments": "16 pages, 8 figures, wrote at Mar 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Hashing is widely used for effective approximate nearest neighbors\nsearch. Even though various binary hashing methods have been proposed, very few\nmethods are feasible for extremely high-dimensional features often used in\nvisual tasks today. We propose a novel highly sparse linear hashing method\nbased on pairwise rotations. The encoding cost of the proposed algorithm is\n$\\mathrm{O}(n \\log n)$ for n-dimensional features, whereas that of the existing\nstate-of-the-art method is typically $\\mathrm{O}(n^2)$. The proposed method is\nalso remarkably faster in the learning phase. Along with the efficiency, the\nretrieval accuracy is comparable to or slightly outperforming the\nstate-of-the-art. Pairwise rotations used in our method are formulated from an\nanalytical study of the trade-off relationship between quantization error and\nentropy of binary codes. Although these hashing criteria are widely used in\nprevious researches, its analytical behavior is rarely studied. All building\nblocks of our algorithm are based on the analytical solution, and it thus\nprovides a fairly simple and efficient procedure.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 11:50:33 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Ishikawa", "Kohta", ""], ["Sato", "Ikuro", ""], ["Ambai", "Mitsuru", ""]]}, {"id": "1501.07492", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang", "title": "Weakly Supervised Learning for Salient Object Detection", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in supervised salient object detection has resulted in\nsignificant performance on benchmark datasets. Training such models, however,\nrequires expensive pixel-wise annotations of salient objects. Moreover, many\nexisting salient object detection models assume that at least one salient\nobject exists in the input image. Such an assumption often leads to less\nappealing saliency maps on the background images, which contain no salient\nobject at all. To avoid the requirement of expensive pixel-wise salient region\nannotations, in this paper, we study weakly supervised learning approaches for\nsalient object detection. Given a set of background images and salient object\nimages, we propose a solution toward jointly addressing the salient object\nexistence and detection tasks. We adopt the latent SVM framework and formulate\nthe two problems together in a single integrated objective function: saliency\nlabels of superpixels are modeled as hidden variables and involved in a\nclassification term conditioned to the salient object existence variable, which\nin turn depends on both global image and regional saliency features and\nsaliency label assignment. Experimental results on benchmark datasets validate\nthe effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 15:57:52 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 13:34:24 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Jiang", "Huaizu", ""]]}, {"id": "1501.07645", "submitter": "Sachin Talathi", "authors": "Sachin S. Talathi", "title": "Hyper-parameter optimization of Deep Convolutional Networks for object\n  recognition", "comments": "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently sequential model based optimization (SMBO) has emerged as a\npromising hyper-parameter optimization strategy in machine learning. In this\nwork, we investigate SMBO to identify architecture hyper-parameters of deep\nconvolution networks (DCNs) object recognition. We propose a simple SMBO\nstrategy that starts from a set of random initial DCN architectures to generate\nnew architectures, which on training perform well on a given dataset. Using the\nproposed SMBO strategy we are able to identify a number of DCN architectures\nthat produce results that are comparable to state-of-the-art results on object\nrecognition benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 02:08:51 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 03:32:22 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Talathi", "Sachin S.", ""]]}, {"id": "1501.07680", "submitter": "Subit  Chakrabarti", "authors": "Subit Chakrabarti and Jasmeet Judge and Anand Rangarajan and Sanjay\n  Ranka", "title": "Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous\n  Landscapes using Holistic Structure based Models", "comments": "28 pages, 14 figures, submitted to IEEE Transactions on Geoscience\n  and Remote Sensing", "journal-ref": "IEEE Trans. Geosci. Remote Sens. 54 (2008) 4629-4641", "doi": "10.1109/TGRS.2016.2547389", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel machine learning algorithm is presented for\ndisaggregation of satellite soil moisture (SM) based on self-regularized\nregressive models (SRRM) using high-resolution correlated information from\nauxiliary sources. It includes regularized clustering that assigns soft\nmemberships to each pixel at fine-scale followed by a kernel regression that\ncomputes the value of the desired variable at all pixels. Coarse-scale remotely\nsensed SM were disaggregated from 10km to 1km using land cover, precipitation,\nland surface temperature, leaf area index, and in-situ observations of SM. This\nalgorithm was evaluated using multi-scale synthetic observations in NC Florida\nfor heterogeneous agricultural land covers. It was found that the root mean\nsquare error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. The\nclusters generated represented the data well and reduced the RMSE by upto 40%\nduring periods of high heterogeneity in land-cover and meteorological\nconditions. The Kullback Leibler divergence (KLD) between the true SM and the\ndisaggregated estimates is close to 0, for both vegetated and baresoil\nlandcovers. The disaggregated estimates were compared to those generated by the\nPrinciple of Relevant Information (PRI) method. The RMSE for the PRI\ndisaggregated estimates is higher than the RMSE for the SRRM on each day of the\nseason. The KLD of the disaggregated estimates generated by the SRRM is at\nleast four orders of magnitude lower than those for the PRI disaggregated\nestimates, while the computational time needed was reduced by three times. The\nresults indicate that the SRRM can be used for disaggregating SM with complex\nnon-linear correlations on a grid with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 07:09:09 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 20:33:08 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Chakrabarti", "Subit", ""], ["Judge", "Jasmeet", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1501.07681", "submitter": "Jingbin Wang", "authors": "Lan Yang, Jingbin Wang, Yujin Tu, Prarthana Mahapatra, Nelson Cardoso", "title": "Vector Quantization by Minimizing Kullback-Leibler Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for vector quantization by minimizing the\nKullback-Leibler Divergence between the class label distributions over the\nquantization inputs, which are original vectors, and the output, which is the\nquantization subsets of the vector set. In this way, the vector quantization\noutput can keep as much information of the class label as possible. An\nobjective function is constructed and we also developed an iterative algorithm\nto minimize it. The new method is evaluated on bag-of-features based image\nclassification problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 07:16:50 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Yang", "Lan", ""], ["Wang", "Jingbin", ""], ["Tu", "Yujin", ""], ["Mahapatra", "Prarthana", ""], ["Cardoso", "Nelson", ""]]}, {"id": "1501.07683", "submitter": "Subit  Chakrabarti", "authors": "Subit Chakrabarti and Jasmeet Judge and Anand Rangarajan and Sanjay\n  Ranka", "title": "Downscaling Microwave Brightness Temperatures Using Self Regularized\n  Regressive Models", "comments": "7 pages, 4 figures, submitted to be presented at the International\n  Geoscience and Remote Sensing Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm is proposed to downscale microwave brightness temperatures\n($\\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil Moisture\nActive Passive mission to a resolution meaningful for hydrological and\nagricultural applications. This algorithm, called Self-Regularized Regressive\nModels (SRRM), uses auxiliary variables correlated to $\\mathrm{T_B}$ along-with\na limited set of \\textit{in-situ} SM observations, which are converted to high\nresolution $\\mathrm{T_B}$ observations using biophysical models. It includes an\ninformation-theoretic clustering step based on all auxiliary variables to\nidentify areas of similarity, followed by a kernel regression step that\nproduces downscaled $\\mathrm{T_B}$. This was implemented on a multi-scale\nsynthetic data-set over NC-Florida for one year. An RMSE of 5.76~K with\nstandard deviation of 2.8~k was achieved during the vegetated season and an\nRMSE of 1.2~K with a standard deviation of 0.9~K during periods of no\nvegetation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 07:24:44 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Chakrabarti", "Subit", ""], ["Judge", "Jasmeet", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1501.07692", "submitter": "Matthew Sottile", "authors": "Matthew Sottile", "title": "Blob indentation identification via curvature measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for identifying indentations on the\nboundary of solid 2D shape. It uses the signed curvature at a set of points\nalong the boundary to identify indentations and provides one parameter for\ntuning the selection mechanism for discriminating indentations from other\nboundary irregularities. An efficient implementation is described based on the\nFourier transform for calculating curvature from a sequence of points obtained\nfrom the boundary of a binary blob.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 08:12:48 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Sottile", "Matthew", ""]]}, {"id": "1501.07719", "submitter": "Simon Perkins", "authors": "Simon Perkins, Patrick Marais, Jonathan Zwart, Iniyan Natarajan, Cyril\n  Tasse and Oleg Smirnov", "title": "Montblanc: GPU accelerated Radio Interferometer Measurement Equations in\n  support of Bayesian Inference for Radio Observations", "comments": "Submitted to Astronomy and Computing\n  (http://www.journals.elsevier.com/astronomy-and-computing). The code is\n  available online at https://github.com/ska-sa/montblanc. 29 pages long, with\n  10 figures, 6 tables and 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Montblanc, a GPU implementation of the Radio interferometer\nmeasurement equation (RIME) in support of the Bayesian inference for radio\nobservations (BIRO) technique. BIRO uses Bayesian inference to select sky\nmodels that best match the visibilities observed by a radio interferometer. To\naccomplish this, BIRO evaluates the RIME multiple times, varying sky model\nparameters to produce multiple model visibilities. Chi-squared values computed\nfrom the model and observed visibilities are used as likelihood values to drive\nthe Bayesian sampling process and select the best sky model.\n  As most of the elements of the RIME and chi-squared calculation are\nindependent of one another, they are highly amenable to parallel computation.\nAdditionally, Montblanc caters for iterative RIME evaluation to produce\nmultiple chi-squared values. Modified model parameters are transferred to the\nGPU between each iteration.\n  We implemented Montblanc as a Python package based upon NVIDIA's CUDA\narchitecture. As such, it is easy to extend and implement different pipelines.\nAt present, Montblanc supports point and Gaussian morphologies, but is designed\nfor easy addition of new source profiles.\n  Montblanc's RIME implementation is performant: On an NVIDIA K40, it is\napproximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2\nCPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is\n7.7 and 12 times faster on the same K40 for single and double-precision\nfloating point respectively. However, OSKAR's RIME implementation is more\ngeneral than Montblanc's BIRO-tailored RIME.\n  Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it is\nmemory bound. In practice, profiling shows that is balanced between compute and\nmemory, as much of the data required by the problem is retained in L1 and L2\ncache.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 10:04:27 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 17:12:35 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 11:52:12 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Perkins", "Simon", ""], ["Marais", "Patrick", ""], ["Zwart", "Jonathan", ""], ["Natarajan", "Iniyan", ""], ["Tasse", "Cyril", ""], ["Smirnov", "Oleg", ""]]}, {"id": "1501.07738", "submitter": "Olivier Mor\\`ere", "authors": "Olivier Mor\\`ere, Hanlin Goh, Antoine Veillard, Vijay Chandrasekhar,\n  Jie Lin", "title": "Co-Regularized Deep Representations for Video Summarization", "comments": "Video summarization, deep convolutional neural networks,\n  co-regularized restricted Boltzmann machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact keyframe-based video summaries are a popular way of generating\nviewership on video sharing platforms. Yet, creating relevant and compelling\nsummaries for arbitrarily long videos with a small number of keyframes is a\nchallenging task. We propose a comprehensive keyframe-based summarization\nframework combining deep convolutional neural networks and restricted Boltzmann\nmachines. An original co-regularization scheme is used to discover meaningful\nsubject-scene associations. The resulting multimodal representations are then\nused to select highly-relevant keyframes. A comprehensive user study is\nconducted comparing our proposed method to a variety of schemes, including the\nsummarization currently in use by one of the most popular video sharing\nwebsites. The results show that our method consistently outperforms the\nbaseline schemes for any given amount of keyframes both in terms of\nattractiveness and informativeness. The lead is even more significant for\nsmaller summaries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 11:48:20 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Mor\u00e8re", "Olivier", ""], ["Goh", "Hanlin", ""], ["Veillard", "Antoine", ""], ["Chandrasekhar", "Vijay", ""], ["Lin", "Jie", ""]]}, {"id": "1501.07758", "submitter": "Elias Kellner", "authors": "Elias Kellner, Bibek Dhital and Marco Reisert", "title": "Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs-ringing is a well known artifact which manifests itself as spurious\noscillations in the vicinity of sharp image transients, e.g. at tissue\nboundaries. The origin can be seen in the truncation of k-space during MRI\ndata-acquisition. Consequently, correction techniques like Gegenbauer\nreconstruction or extrapolation methods aim at recovering these missing data.\nHere, we present a simple and robust method which exploits a different view on\nthe Gibbs-phenomena. The truncation in k-space can be interpreted as a\nconvolution with a sinc-function in image space. Hence, the severity of the\nartifacts depends on how the sinc-function is sampled. We propose to\nre-interpolate the image based on local, subvoxel shifts to sample the ringing\npattern at the zero-crossings of the oscillating sinc-function. With this, the\nartifact can effectively and robustly be removed with a minimal amount of\nsmoothing.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 12:52:30 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Kellner", "Elias", ""], ["Dhital", "Bibek", ""], ["Reisert", "Marco", ""]]}, {"id": "1501.07844", "submitter": "John Stuart Haberl Baxter", "authors": "John S.H. Baxter, Martin Rajchl, Jing Yuan, and Terry M. Peters", "title": "A Proximal Bregman Projection Approach to Continuous Max-Flow Problems\n  Using Entropic Distances", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One issue limiting the adaption of large-scale multi-region segmentation is\nthe sometimes prohibitive memory requirements. This is especially troubling\nconsidering advances in massively parallel computing and commercial graphics\nprocessing units because of their already limited memory compared to the\ncurrent random access memory used in more traditional computation. To address\nthis issue in the field of continuous max-flow segmentation, we have developed\na \\textit{pseudo-flow} framework using the theory of Bregman proximal\nprojections and entropic distances which implicitly represents flow variables\nbetween labels and designated source and sink nodes. This reduces the memory\nrequirements for max-flow segmentation by approximately 20\\% for Potts models\nand approximately 30\\% for hierarchical max-flow (HMF) and directed acyclic\ngraph max-flow (DAGMF) models. This represents a great improvement in the\nstate-of-the-art in max-flow segmentation, allowing for much larger problems to\nbe addressed and accelerated using commercially available graphics processing\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 17:03:22 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Baxter", "John S. H.", ""], ["Rajchl", "Martin", ""], ["Yuan", "Jing", ""], ["Peters", "Terry M.", ""]]}, {"id": "1501.07862", "submitter": "Satadal Saha", "authors": "Mahua Nandy (Pal), Satadal Saha", "title": "An Analytical Study of different Document Image Binarization Methods", "comments": "National Conference on Computing and Communication Systems\n  (COCOSYS-09), UIT, Burdwan, January 02-04, 2009, pp. 71-76", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image has been the area of research for a couple of decades because\nof its potential application in the area of text recognition, line recognition\nor any other shape recognition from the image. For most of these purposes\nbinarization of image becomes mandatory as far as recognition is concerned.\nThroughout couple decades standard algorithms have already been developed for\nthis purpose. Some of these algorithms are applicable to degraded image also.\nOur objective behind this work is to study the existing techniques, compare\nthem in view of advantages and disadvantages and modify some of these\nalgorithms to optimize time or performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 17:50:41 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Nandy", "Mahua", "", "Pal"], ["Saha", "Satadal", ""]]}, {"id": "1501.07867", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat Seyed Mousavi, Umamahesh Srinivas, Vishal Monga, Yuanming Suo,\n  Minh Dao, Trac. D. Tran", "title": "Multi-task Image Classification via Collaborative, Hierarchical\n  Spike-and-Slab Priors", "comments": "Accepted to International Conference in Image Processing (ICIP) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promising results have been achieved in image classification problems by\nexploiting the discriminative power of sparse representations for\nclassification (SRC). Recently, it has been shown that the use of\n\\emph{class-specific} spike-and-slab priors in conjunction with the\nclass-specific dictionaries from SRC is particularly effective in low training\nscenarios. As a logical extension, we build on this framework for multitask\nscenarios, wherein multiple representations of the same physical phenomena are\navailable. We experimentally demonstrate the benefits of mining joint\ninformation from different camera views for multi-view face recognition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 18:05:01 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Mousavi", "Hojjat Seyed", ""], ["Srinivas", "Umamahesh", ""], ["Monga", "Vishal", ""], ["Suo", "Yuanming", ""], ["Dao", "Minh", ""], ["Tran", "Trac. D.", ""]]}, {"id": "1501.07873", "submitter": "Yongxin Yang", "authors": "Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang and Timothy Hospedales", "title": "Sketch-a-Net that Beats Humans", "comments": "Accepted to BMVC 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-scale multi-channel deep neural network framework that,\nfor the first time, yields sketch recognition performance surpassing that of\nhumans. Our superior performance is a result of explicitly embedding the unique\ncharacteristics of sketches in our model: (i) a network architecture designed\nfor sketch rather than natural photo statistics, (ii) a multi-channel\ngeneralisation that encodes sequential ordering in the sketching process, and\n(iii) a multi-scale network ensemble with joint Bayesian fusion that accounts\nfor the different levels of abstraction exhibited in free-hand sketches. We\nshow that state-of-the-art deep networks specifically engineered for photos of\nnatural objects fail to perform well on sketch recognition, regardless whether\nthey are trained using photo or sketch. Our network on the other hand not only\ndelivers the best performance on the largest human sketch dataset to date, but\nalso is small in size making efficient training possible using just CPUs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 18:35:59 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 18:59:06 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 15:59:05 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Yu", "Qian", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy", ""]]}]