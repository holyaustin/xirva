[{"id": "1811.00053", "submitter": "Sheikh Muhammad Saiful Islam", "authors": "Sheikh Muhammad Saiful Islam and Md Mahedi Hasan", "title": "DEEPGONET: Multi-label Prediction of GO Annotation for Protein from\n  Sequence Using Cascaded Convolutional and Recurrent Network", "comments": "Accepted in ICCIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present gap between the amount of available protein sequence due to the\ndevelopment of next generation sequencing technology (NGS) and slow and\nexpensive experimental extraction of useful information like annotation of\nprotein sequence in different functional aspects, is ever widening, which can\nbe reduced by employing automatic function prediction (AFP) approaches. Gene\nOntology (GO), comprising of more than 40, 000 classes, defines three aspects\nof protein function names Biological Process (BP), Cellular Component (CC),\nMolecular Function (MF). Multiple functions of a single protein, has made\nautomatic function prediction a large-scale, multi-class, multi-label task. In\nthis paper, we present DEEPGONET, a novel cascaded convolutional and recurrent\nneural network, to predict the top-level hierarchy of GO ontology. The network\ntakes the primary sequence of protein as input which makes it more useful than\nother prevailing state-of-the-art deep learning based methods with multi-modal\ninput, making them less applicable for proteins where only primary sequence is\navailable. All predictions of different protein functions of our network are\nperformed by the same architecture, a proof of better generalization as\ndemonstrated by promising performance on a variety of organisms while trained\non Homo sapiens only, which is made possible by efficient exploration of vast\noutput space by leveraging hierarchical relationship among GO classes. The\npromising performance of our model makes it a potential avenue for directing\nexperimental protein functions exploration efficiently by vastly eliminating\npossible routes which is done by the exploring only the suggested routes from\nour model. Our proposed model is also very simple and efficient in terms of\ncomputational time and space compared to other architectures in literature.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:26:41 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Islam", "Sheikh Muhammad Saiful", ""], ["Hasan", "Md Mahedi", ""]]}, {"id": "1811.00056", "submitter": "Boyu Zhang", "authors": "Boyu Zhang, Azadeh Davoodi, and Yu-Hen Hu", "title": "A Mixture of Expert Approach for Low-Cost Customization of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to customize a trained Deep Neural Network (DNN) locally using\nuser-specific data may greatly enhance user experiences, reduce development\ncosts, and protect user's privacy. In this work, we propose to incorporate a\nnovel Mixture of Experts (MOE) approach to accomplish this goal. This\narchitecture comprises of a Global Expert (GE), a Local Expert (LE) and a\nGating Network (GN). The GE is a trained DNN developed on a large training\ndataset representative of many potential users. After deployment on an embedded\nedge device, GE will be subject to customized, user-specific data (e.g., accent\nin speech) and its performance may suffer. This problem may be alleviated by\ntraining a local DNN (the local expert, LE) on a small size customized training\ndata to correct the errors made by GE. A gating network then will be trained to\ndetermine whether an incoming data should be handled by GE or LE. Since the\ncustomized dataset is in general very small, the cost of training LE and GN\nwould be much lower than that of re-training of GE. The training of LE and GN\nthus can be performed at local device, properly protecting the privacy of\ncustomized training data. In this work, we developed a prototype MOE\narchitecture for handwritten alphanumeric character recognition task. We use\nEMNIST as the generic dataset, LeNet5 as GE, and handwritings of 10 users as\nthe customized dataset. We show that with the LE and GN, the classification\naccuracy is significantly enhanced over the customized dataset with almost no\ndegradation of accuracy over the generic dataset. In terms of energy and\nnetwork size, the overhead of LE and GN is around 2.5% compared to those of GE.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:37:28 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zhang", "Boyu", ""], ["Davoodi", "Azadeh", ""], ["Hu", "Yu-Hen", ""]]}, {"id": "1811.00112", "submitter": "Daniel S\\'aez Trigueros", "authors": "Daniel S\\'aez Trigueros, Li Meng, Margaret Hartnett", "title": "Generating Photo-Realistic Training Data to Improve Face Recognition\n  Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the feasibility of using synthetic data to\naugment face datasets. In particular, we propose a novel generative adversarial\nnetwork (GAN) that can disentangle identity-related attributes from\nnon-identity-related attributes. This is done by training an embedding network\nthat maps discrete identity labels to an identity latent space that follows a\nsimple prior distribution, and training a GAN conditioned on samples from that\ndistribution. Our proposed GAN allows us to augment face datasets by generating\nboth synthetic images of subjects in the training set and synthetic images of\nnew subjects not in the training set. By using recent advances in GAN training,\nwe show that the synthetic images generated by our model are photo-realistic,\nand that training with augmented datasets can indeed increase the accuracy of\nface recognition models as compared with models trained with real images alone.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:53:25 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Trigueros", "Daniel S\u00e1ez", ""], ["Meng", "Li", ""], ["Hartnett", "Margaret", ""]]}, {"id": "1811.00116", "submitter": "Daniel S\\'aez Trigueros", "authors": "Daniel S\\'aez Trigueros, Li Meng, Margaret Hartnett", "title": "Face Recognition: From Traditional to Deep Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting in the seventies, face recognition has become one of the most\nresearched topics in computer vision and biometrics. Traditional methods based\non hand-crafted features and traditional machine learning techniques have\nrecently been superseded by deep neural networks trained with very large\ndatasets. In this paper we provide a comprehensive and up-to-date literature\nreview of popular face recognition methods including both traditional\n(geometry-based, holistic, feature-based and hybrid methods) and deep learning\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:58:39 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Trigueros", "Daniel S\u00e1ez", ""], ["Meng", "Li", ""], ["Hartnett", "Margaret", ""]]}, {"id": "1811.00120", "submitter": "Yu Sun", "authors": "Yu Sun, Shiqi Xu, Yunzhe Li, Lei Tian, Brendt Wohlberg, and Ulugbek S.\n  Kamilov", "title": "Regularized Fourier Ptychography using an Online Plug-and-Play Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683057", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plug-and-play priors (PnP) framework has been recently shown to achieve\nstate-of-the-art results in regularized image reconstruction by leveraging a\nsophisticated denoiser within an iterative algorithm. In this paper, we propose\na new online PnP algorithm for Fourier ptychographic microscopy (FPM) based on\nthe fast iterative shrinkage/threshold algorithm (FISTA). Specifically, the\nproposed algorithm uses only a subset of measurements, which makes it scalable\nto a large set of measurements. We validate the algorithm by showing that it\ncan lead to significant performance gains on both simulated and experimental\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 21:04:42 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 14:58:31 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sun", "Yu", ""], ["Xu", "Shiqi", ""], ["Li", "Yunzhe", ""], ["Tian", "Lei", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1811.00143", "submitter": "Minghuang Ma", "authors": "Minghuang Ma, Hadi Pouransari, Daniel Chao, Saurabh Adya, Santiago\n  Akle Serrano, Yi Qin, Dan Gimnicher, Dominic Walsh", "title": "Democratizing Production-Scale Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest and demand for training deep neural networks have been\nexperiencing rapid growth, spanning a wide range of applications in both\nacademia and industry. However, training them distributed and at scale remains\ndifficult due to the complex ecosystem of tools and hardware involved. One\nconsequence is that the responsibility of orchestrating these complex\ncomponents is often left to one-off scripts and glue code customized for\nspecific problems. To address these restrictions, we introduce \\emph{Alchemist}\n- an internal service built at Apple from the ground up for \\emph{easy},\n\\emph{fast}, and \\emph{scalable} distributed training. We discuss its design,\nimplementation, and examples of running different flavors of distributed\ntraining. We also present case studies of its internal adoption in the\ndevelopment of autonomous systems, where training times have been reduced by\n10x to keep up with the ever-growing data collection.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:39:59 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 05:47:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ma", "Minghuang", ""], ["Pouransari", "Hadi", ""], ["Chao", "Daniel", ""], ["Adya", "Saurabh", ""], ["Serrano", "Santiago Akle", ""], ["Qin", "Yi", ""], ["Gimnicher", "Dan", ""], ["Walsh", "Dominic", ""]]}, {"id": "1811.00161", "submitter": "Zahra Sadeghi", "authors": "Zahra Sadeghi", "title": "Conceptual Content in Deep Convolutional Neural Networks: An analysis\n  into multi-faceted properties of neurons", "comments": "12 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, convolutional layers of pre-trained VGG16 model are analyzed.\nThe analysis is based on the responses of neurons to the images of classes in\nImageNet database. First, a visualization method is proposed in order to\nillustrate the learned content of each neuron. Next, single and multi-faceted\nneurons are investigated based on the diversity of neurons responses to\ndifferent category of objects. Finally, neuronal similarities at each layer are\ncomputed and compared. The results demonstrate that the neurons in lower layers\nexhibit a multi-faceted behavior, whereas the majority of neurons in higher\nlayers com-prise single-faceted property and tend to respond to a smaller\nnumber of concepts.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:56:26 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 19:36:46 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 10:02:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Sadeghi", "Zahra", ""]]}, {"id": "1811.00174", "submitter": "Shuangting Liu", "authors": "Shuangting Liu, Jiaqi Zhang, Yuxin Chen, Yifan Liu, Zengchang Qin, Tao\n  Wan", "title": "Pixel Level Data Augmentation for Semantic Image Segmentation using\n  Generative Adversarial Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is one of the basic topics in computer vision, it aims\nto assign semantic labels to every pixel of an image. Unbalanced semantic label\ndistribution could have a negative influence on segmentation accuracy. In this\npaper, we investigate using data augmentation approach to balance the semantic\nlabel distribution in order to improve segmentation performance. We propose\nusing generative adversarial networks (GANs) to generate realistic images for\nimproving the performance of semantic segmentation networks. Experimental\nresults show that the proposed method can not only improve segmentation\nperformance on those classes with low accuracy, but also obtain 1.3% to 2.1%\nincrease in average segmentation accuracy. It shows that this augmentation\nmethod can boost accuracy and be easily applicable to any other segmentation\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:07:16 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 13:50:34 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 08:46:53 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 05:49:32 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Liu", "Shuangting", ""], ["Zhang", "Jiaqi", ""], ["Chen", "Yuxin", ""], ["Liu", "Yifan", ""], ["Qin", "Zengchang", ""], ["Wan", "Tao", ""]]}, {"id": "1811.00189", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Dongdong Hou, Weiming Zhang and Nenghai Yu", "title": "Reversible Adversarial Examples", "comments": "arXiv admin note: text overlap with arXiv:1806.09186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have recently led to significant improvement in many\nfields such as image classification and speech recognition. However, these\nmachine learning models are vulnerable to adversarial examples which can\nmislead machine learning classifiers to give incorrect classifications. In this\npaper, we take advantage of reversible data hiding to construct reversible\nadversarial examples which are still misclassified by Deep Neural Networks.\nFurthermore, the proposed method can recover original images from reversible\nadversarial examples with no distortion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:28:31 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 14:30:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Liu", "Jiayang", ""], ["Hou", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.00201", "submitter": "Ayan Kumar Bhunia", "authors": "Pranay Mukherjee, Abhirup Das, Ayan Kumar Bhunia, Partha Pratim Roy", "title": "Cogni-Net: Cognitive Feature Learning through Deep Visual Perception", "comments": "IEEE International Conference on Image Processing (ICIP), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we ask computers to recognize what we see from brain signals alone? Our\npaper seeks to utilize the knowledge learnt in the visual domain by popular\npre-trained vision models and use it to teach a recurrent model being trained\non brain signals to learn a discriminative manifold of the human brain's\ncognition of different visual object categories in response to perceived visual\ncues. For this we make use of brain EEG signals triggered from visual stimuli\nlike images and leverage the natural synchronization between images and their\ncorresponding brain signals to learn a novel representation of the cognitive\nfeature space. The concept of knowledge distillation has been used here for\ntraining the deep cognition model, CogniNet\\footnote{The source code of the\nproposed system is publicly available at\n{https://www.github.com/53X/CogniNET}}, by employing a student-teacher learning\ntechnique in order to bridge the process of inter-modal knowledge transfer. The\nproposed novel architecture obtains state-of-the-art results, significantly\nsurpassing other existing models. The experiments performed by us also suggest\nthat if visual stimuli information like brain EEG signals can be gathered on a\nlarge scale, then that would help to obtain a better understanding of the\nlargely unexplored domain of human brain cognition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:14:18 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 10:30:43 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Mukherjee", "Pranay", ""], ["Das", "Abhirup", ""], ["Bhunia", "Ayan Kumar", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1811.00202", "submitter": "Yz Gu", "authors": "Yinzheng Gu and Chuanpeng Li and Jinbin Xie", "title": "Attention-Aware Generalized Mean Pooling for Image Retrieval", "comments": "Shortened version for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that image descriptors extracted by convolutional neural\nnetworks (CNNs) achieve remarkable results for retrieval problems. In this\npaper, we apply attention mechanism to CNN, which aims at enhancing more\nrelevant features that correspond to important keypoints in the input image.\nThe generated attention-aware features are then aggregated by the previous\nstate-of-the-art generalized mean (GeM) pooling followed by normalization to\nproduce a compact global descriptor, which can be efficiently compared to other\nimage descriptors by the dot product. An extensive comparison of our proposed\napproach with state-of-the-art methods is performed on the new challenging\nROxford5k and RParis6k retrieval benchmarks. Results indicate significant\nimprovement over previous work. In particular, our attention-aware GeM (AGeM)\ndescriptor outperforms state-of-the-art method on ROxford5k under the `Hard'\nevaluation protocal.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:16:02 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 06:11:13 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gu", "Yinzheng", ""], ["Li", "Chuanpeng", ""], ["Xie", "Jinbin", ""]]}, {"id": "1811.00206", "submitter": "Zhuliang Yao", "authors": "Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, Lanshun Nie", "title": "Balanced Sparsity for Efficient DNN Inference on GPU", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v33i01.33015676", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In trained deep neural networks, unstructured pruning can reduce redundant\nweights to lower storage cost. However, it requires the customization of\nhardwares to speed up practical inference. Another trend accelerates sparse\nmodel inference on general-purpose hardwares by adopting coarse-grained\nsparsity to prune or regularize consecutive weights for efficient computation.\nBut this method often sacrifices model accuracy. In this paper, we propose a\nnovel fine-grained sparsity approach, balanced sparsity, to achieve high model\naccuracy with commercial hardwares efficiently. Our approach adapts to high\nparallelism property of GPU, showing incredible potential for sparsity in the\nwidely deployment of deep learning services. Experiment results show that\nbalanced sparsity achieves up to 3.1x practical speedup for model inference on\nGPU, while retains the same high model accuracy as fine-grained sparsity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:30:51 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 14:03:29 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 01:48:44 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 06:26:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Yao", "Zhuliang", ""], ["Cao", "Shijie", ""], ["Xiao", "Wencong", ""], ["Zhang", "Chen", ""], ["Nie", "Lanshun", ""]]}, {"id": "1811.00218", "submitter": "Hu Han", "authors": "Hu Han, Jie Li, Anil K. Jain, Shiguang Shan, and Xilin Chen", "title": "Tattoo Image Search at Scale: Joint Detection and Compact Representation\n  Learning", "comments": "Technical Report (15 pages, 14 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of digital images in video surveillance and social media\nhas led to the significant need for efficient search of persons of interest in\nlaw enforcement and forensic applications. Despite tremendous progress in\nprimary biometric traits (e.g., face and fingerprint) based person\nidentification, a single biometric trait alone cannot meet the desired\nrecognition accuracy in forensic scenarios. Tattoos, as one of the important\nsoft biometric traits, have been found to be valuable for assisting in person\nidentification. However, tattoo search in a large collection of unconstrained\nimages remains a difficult problem, and existing tattoo search methods mainly\nfocus on matching cropped tattoos, which is different from real application\nscenarios. To close the gap, we propose an efficient tattoo search approach\nthat is able to learn tattoo detection and compact representation jointly in a\nsingle convolutional neural network (CNN) via multi-task learning. While the\nfeatures in the backbone network are shared by both tattoo detection and\ncompact representation learning, individual latent layers of each sub-network\noptimize the shared features toward the detection and feature learning tasks,\nrespectively. We resolve the small batch size issue inside the joint tattoo\ndetection and compact representation learning network via random image stitch\nand preceding feature buffering. We evaluate the proposed tattoo search system\nusing multiple public-domain tattoo benchmarks, and a gallery set with about\n300K distracter tattoo images compiled from these datasets and images from the\nInternet. In addition, we also introduce a tattoo sketch dataset containing 300\ntattoos for sketch-based tattoo search. Experimental results show that the\nproposed approach has superior performance in tattoo detection and tattoo\nsearch at scale compared to several state-of-the-art tattoo retrieval\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:20:31 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Han", "Hu", ""], ["Li", "Jie", ""], ["Jain", "Anil K.", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1811.00220", "submitter": "Ashif Iquebal", "authors": "Ashif Sikandar Iquebal and Satish Bukkapatnam", "title": "Consistent estimation of the max-flow problem: Towards unsupervised\n  image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in the image-based diagnostics of complex biological and\nmanufacturing processes have brought unsupervised image segmentation to the\nforefront of enabling automated, on the fly decision making. However, most\nexisting unsupervised segmentation approaches are either computationally\ncomplex or require manual parameter selection (e.g., flow capacities in\nmax-flow/min-cut segmentation). In this work, we present a fully unsupervised\nsegmentation approach using a continuous max-flow formulation over the image\ndomain while optimally estimating the flow parameters from the image\ncharacteristics. More specifically, we show that the maximum a posteriori\nestimate of the image labels can be formulated as a continuous max-flow problem\ngiven the flow capacities are known. The flow capacities are then iteratively\nobtained by employing a novel Markov random field prior over the image domain.\nWe present theoretical results to establish the posterior consistency of the\nflow capacities. We compare the performance of our approach on two real-world\ncase studies including brain tumor image segmentation and defect identification\nin additively manufactured components using electron microscopic images.\nComparative results with several state-of-the-art supervised as well as\nunsupervised methods suggest that the present method performs statistically\nsimilar to the supervised methods, but results in more than 90% improvement in\nthe Dice score when compared to the state-of-the-art unsupervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:35:01 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 20:50:32 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Iquebal", "Ashif Sikandar", ""], ["Bukkapatnam", "Satish", ""]]}, {"id": "1811.00222", "submitter": "Kaidi Cao", "authors": "Kaidi Cao, Jing Liao, Lu Yuan", "title": "CariGANs: Unpaired Photo-to-Caricature Translation", "comments": "To appear at SIGGRAPH Asia 2018", "journal-ref": "ACM Transactions on Graphics, Vol. 37, No. 6, Article 244.\n  Publication date: November 2018", "doi": "10.1145/3272127.3275046", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial caricature is an art form of drawing faces in an exaggerated way to\nconvey humor or sarcasm. In this paper, we propose the first Generative\nAdversarial Network (GAN) for unpaired photo-to-caricature translation, which\nwe call \"CariGANs\". It explicitly models geometric exaggeration and appearance\nstylization using two components: CariGeoGAN, which only models the\ngeometry-to-geometry transformation from face photos to caricatures, and\nCariStyGAN, which transfers the style appearance from caricatures to face\nphotos without any geometry deformation. In this way, a difficult cross-domain\ntranslation problem is decoupled into two easier tasks. The perceptual study\nshows that caricatures generated by our CariGANs are closer to the hand-drawn\nones, and at the same time better persevere the identity, compared to\nstate-of-the-art methods. Moreover, our CariGANs allow users to control the\nshape exaggeration degree and change the color/texture style by tuning the\nparameters or giving an example caricature.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:39:20 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 03:47:13 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cao", "Kaidi", ""], ["Liao", "Jing", ""], ["Yuan", "Lu", ""]]}, {"id": "1811.00228", "submitter": "Daouda Sow", "authors": "Daouda Sow and Zengchang Qin and Mouhamed Niasse and Tao Wan", "title": "A sequential guiding network with attention for image captioning", "comments": "5 pages, 2 figures, 1 table, IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances of deep learning in both computer vision (CV) and natural\nlanguage processing (NLP) provide us a new way of understanding semantics, by\nwhich we can deal with more challenging tasks such as automatic description\ngeneration from natural images. In this challenge, the encoder-decoder\nframework has achieved promising performance when a convolutional neural\nnetwork (CNN) is used as image encoder and a recurrent neural network (RNN) as\ndecoder. In this paper, we introduce a sequential guiding network that guides\nthe decoder during word generation. The new model is an extension of the\nencoder-decoder framework with attention that has an additional guiding long\nshort-term memory (LSTM) and can be trained in an end-to-end manner by using\nimage/descriptions pairs. We validate our approach by conducting extensive\nexperiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model\nachieves significant improvement comparing to the other state-of-the-art deep\nlearning models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:03:26 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 07:06:03 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 22:35:58 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Sow", "Daouda", ""], ["Qin", "Zengchang", ""], ["Niasse", "Mouhamed", ""], ["Wan", "Tao", ""]]}, {"id": "1811.00233", "submitter": "Toru Tamaki", "authors": "Tsubasa Hirakawa, Takayoshi Yamashita, Toru Tamaki and Hironobu\n  Fujiyoshi", "title": "Survey on Vision-based Path Prediction", "comments": "DAPI 2018", "journal-ref": null, "doi": "10.1007/978-3-319-91131-1_4", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path prediction is a fundamental task for estimating how pedestrians or\nvehicles are going to move in a scene. Because path prediction as a task of\ncomputer vision uses video as input, various information used for prediction,\nsuch as the environment surrounding the target and the internal state of the\ntarget, need to be estimated from the video in addition to predicting paths.\nMany prediction approaches that include understanding the environment and the\ninternal state have been proposed. In this survey, we systematically summarize\nmethods of path prediction that take video as input and and extract features\nfrom the video. Moreover, we introduce datasets used to evaluate path\nprediction methods quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:12:01 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Tamaki", "Toru", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1811.00249", "submitter": "Dichao Hu", "authors": "Dichao Hu", "title": "Examining Performance of Sketch-to-Image Translation Models with\n  Multiclass Automatically Generated Paired Training Data", "comments": "6 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation is a computer vision task that involves translating one\nrepresentation of the scene into another. Various approaches have been proposed\nand achieved highly desirable results. Nevertheless, its accomplishment\nrequires abundant paired training data which are expensive to acquire.\nTherefore, models for translation are usually trained on a set of paired\ntraining data which are carefully and laboriously designed. Our work is focused\non learning through automatically generated paired data. We propose a method to\ngenerate fake sketches from images using an adversarial network and then pair\nthe images with corresponding fake sketches to form large-scale multi-class\npaired training data for training a sketch-to-image translation model. Our\nmodel is an encoder-decoder architecture where the encoder generates fake\nsketches from images and the decoder performs sketch-to-image translation.\nQualitative results show that the encoder can be used for generating\nlarge-scale multi-class paired data under low supervision. Our current dataset\nnow contains 61255 image and (fake) sketch pairs from 256 different categories.\nThese figures can be greatly increased in the future thanks to our weak\nreliance on manually labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:59:53 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Hu", "Dichao", ""]]}, {"id": "1811.00250", "submitter": "Yang He", "authors": "Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang", "title": "Filter Pruning via Geometric Median for Deep Convolutional Neural\n  Networks Acceleration", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works utilized ''smaller-norm-less-important'' criterion to prune\nfilters with smaller norm values in a convolutional neural network. In this\npaper, we analyze this norm-based criterion and point out that its\neffectiveness depends on two requirements that are not always met: (1) the norm\ndeviation of the filters should be large; (2) the minimum norm of the filters\nshould be small. To solve this problem, we propose a novel filter pruning\nmethod, namely Filter Pruning via Geometric Median (FPGM), to compress the\nmodel regardless of those two requirements. Unlike previous methods, FPGM\ncompresses CNN models by pruning filters with redundancy, rather than those\nwith ''relatively less'' importance. When applied to two image classification\nbenchmarks, our method validates its usefulness and strengths. Notably, on\nCIFAR-10, FPGM reduces more than 52% FLOPs on ResNet-110 with even 2.69%\nrelative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than\n42% FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the\nstate-of-the-art. Code is publicly available on GitHub:\nhttps://github.com/he-y/filter-pruning-geometric-median\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 06:03:05 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 03:59:11 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 10:31:03 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["He", "Yang", ""], ["Liu", "Ping", ""], ["Wang", "Ziwei", ""], ["Hu", "Zhilan", ""], ["Yang", "Yi", ""]]}, {"id": "1811.00256", "submitter": "Yaqiang Yao", "authors": "Yaqiang Yao, Yan Liu, Huanhuan Chen", "title": "Skeleton-based Activity Recognition with Local Order Preserving Match of\n  Linear Patches", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition has drawn considerable attention recently in the\nfield of computer vision due to the development of commodity depth cameras, by\nwhich the human activity is represented as a sequence of 3D skeleton postures.\nAssuming human body 3D joint locations of an activity lie on a manifold, the\nproblem of recognizing human activity is formulated as the computation of\nactivity manifold-manifold distance (AMMD). In this paper, we first design an\nefficient division method to decompose a manifold into ordered continuous\nmaximal linear patches (CMLPs) that denote meaningful action snippets of the\naction sequence. Then the CMLP is represented by its position (average value of\npoints) and the first principal component, which specify the major posture and\nmain evolving direction of an action snippet, respectively. Finally, we compute\nthe distance between CMLPs by taking both the posture and direction into\nconsideration. Based on these preparations, an intuitive distance measure that\npreserves the local order of action snippets is proposed to compute AMMD. The\nperformance on two benchmark datasets demonstrates the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 06:46:33 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yao", "Yaqiang", ""], ["Liu", "Yan", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1811.00270", "submitter": "Xiangbo Shu", "authors": "Xiangbo Shu, Jinhui Tang, Guo-Jun Qi, Wei Liu, Jian Yang", "title": "Hierarchical Long Short-Term Concurrent Memory for Human Interaction\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to address the problem of human interaction recognition\nin videos by exploring the long-term inter-related dynamics among multiple\npersons. Recently, Long Short-Term Memory (LSTM) has become a popular choice to\nmodel individual dynamic for single-person action recognition due to its\nability of capturing the temporal motion information in a range. However,\nexisting RNN models focus only on capturing the dynamics of human interaction\nby simply combining all dynamics of individuals or modeling them as a whole.\nSuch models neglect the inter-related dynamics of how human interactions change\nover time. To this end, we propose a novel Hierarchical Long Short-Term\nConcurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among\na group of persons for recognizing the human interactions. Specifically, we\nfirst feed each person's static features into a Single-Person LSTM to learn the\nsingle-person dynamic. Subsequently, the outputs of all Single-Person LSTM\nunits are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly\nconsists of multiple sub-memory units, a new cell gate and a new co-memory\ncell. In a Co-LSTM unit, each sub-memory unit stores individual motion\ninformation, while this Co-LSTM unit selectively integrates and stores\ninter-related motion information between multiple interacting persons from\nmultiple sub-memory units via the cell gate and co-memory cell, respectively.\nExtensive experiments on four public datasets validate the effectiveness of the\nproposed H-LSTCM by comparing against baseline and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:36:28 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Shu", "Xiangbo", ""], ["Tang", "Jinhui", ""], ["Qi", "Guo-Jun", ""], ["Liu", "Wei", ""], ["Yang", "Jian", ""]]}, {"id": "1811.00274", "submitter": "Cho Ying Wu", "authors": "Cho Ying Wu, Ulrich Neumann", "title": "Efficient Multi-Domain Dictionary Learning with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the multi-domain dictionary learning (MDDL) to make\ndictionary learning-based classification more robust to data representing in\ndifferent domains. We use adversarial neural networks to generate data in\ndifferent styles, and collect all the generated data into a miscellaneous\ndictionary. To tackle the dictionary learning with many samples, we compute the\nweighting matrix that compress the miscellaneous dictionary from multi-sample\nper class to single sample per class. We show that the time complexity solving\nthe proposed MDDL with weighting matrix is the same as solving the dictionary\nwith single sample per class. Moreover, since the weighting matrix could help\nthe solver rely more on the training data, which possibly lie in the same\ndomain with the testing data, the classification could be more accurate.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:52:19 GMT"}], "update_date": "2018-11-04", "authors_parsed": [["Wu", "Cho Ying", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1811.00312", "submitter": "Ev Zisselman", "authors": "Ev Zisselman, Jeremias Sulam, Michael Elad", "title": "A Local Block Coordinate Descent Algorithm for the Convolutional Sparse\n  Coding Model", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Sparse Coding (CSC) model has recently gained considerable\ntraction in the signal and image processing communities. By providing a global,\nyet tractable, model that operates on the whole image, the CSC was shown to\novercome several limitations of the patch-based sparse model while achieving\nsuperior performance in various applications. Contemporary methods for pursuit\nand learning the CSC dictionary often rely on the Alternating Direction Method\nof Multipliers (ADMM) in the Fourier domain for the computational convenience\nof convolutions, while ignoring the local characterizations of the image. A\nrecent work by Papyan et al. suggested the SBDL algorithm for the CSC, while\noperating locally on image patches. SBDL demonstrates better performance\ncompared to the Fourier-based methods, albeit still relying on the ADMM. In\nthis work we maintain the localized strategy of the SBDL, while proposing a new\nand much simpler approach based on the Block Coordinate Descent algorithm -\nthis method is termed Local Block Coordinate Descent (LoBCoD). Furthermore, we\nintroduce a novel stochastic gradient descent version of LoBCoD for training\nthe convolutional filters. The Stochastic-LoBCoD leverages the benefits of\nonline learning, while being applicable to a single training image. We\ndemonstrate the advantages of the proposed algorithms for image inpainting and\nmulti-focus image fusion, achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:02:38 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zisselman", "Ev", ""], ["Sulam", "Jeremias", ""], ["Elad", "Michael", ""]]}, {"id": "1811.00313", "submitter": "Mehryar Emambakhsh", "authors": "Mehryar Emambakhsh, Alessandro Bay, Eduard Vazquez", "title": "Convolutional Recurrent Predictor: Implicit Representation for\n  Multi-target Filtering and Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2931170", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining a multi-target motion model, which is an important step of tracking\nalgorithms, can be very challenging. Using fixed models (as in several\ngenerative Bayesian algorithms, such as Kalman filters) can fail to accurately\npredict sophisticated target motions. On the other hand, sequential learning of\nthe motion model (for example, using recurrent neural networks) can be\ncomputationally complex and difficult due to the variable unknown number of\ntargets. In this paper, we propose a multi-target filtering and tracking\nalgorithm which learns the motion model, simultaneously for all targets, from\nan implicitly represented state map and performs spatio-temporal data\nprediction. To this end, the multi-target state is modelled over a continuous\nhypothetical target space, using random finite sets and Gaussian mixture\nprobability hypothesis density formulations. The prediction step is recursively\nperformed using a deep convolutional recurrent neural network with a long\nshort-term memory architecture, which is trained as a regression block, on the\nfly, over \"probability density difference\" maps. Our approach is evaluated over\nwidely used pedestrian tracking benchmarks, remarkably outperforming\nstate-of-the-art multi-target filtering algorithms, while giving competitive\nresults when compared with other tracking approaches: The proposed approach\ngenerates an average 40.40 and 62.29 optimal sub-pattern assignment (OSPA)\nerrors on MOT15 and MOT16/17 datasets, respectively, while producing 62.0%,\n70.0% and 66.9% multi-object tracking accuracy (MOTA) on MOT16/17, PNNL Parking\nLot and PETS09 pedestrian tracking datasets, respectively, when publicly\navailable detectors are used.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:06:57 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 17:24:20 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2019 14:29:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Emambakhsh", "Mehryar", ""], ["Bay", "Alessandro", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1811.00327", "submitter": "Vasileios Argyriou", "authors": "Vasileios Argyriou", "title": "Asymmetric Bilateral Phase Correlation for Optical Flow Estimation in\n  the Frequency Domain", "comments": "SITIS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of motion estimation in images operating in the\nfrequency domain. A method is presented which extends phase correlation to\nhandle multiple motions present in an area. Our scheme is based on a novel\nBilateral-Phase Correlation (BLPC) technique that incorporates the concept and\nprinciples of Bilateral Filters retaining the motion boundaries by taking into\naccount the difference both in value and distance in a manner very similar to\nGaussian convolution. The optical flow is obtained by applying the proposed\nmethod at certain locations selected based on the present motion differences\nand then performing non-uniform interpolation in a multi-scale iterative\nframework. Experiments with several well-known datasets with and without\nground-truth show that our scheme outperforms recently proposed\nstate-of-the-art phase correlation based optical flow methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:51:29 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Argyriou", "Vasileios", ""]]}, {"id": "1811.00342", "submitter": "Ying Tai", "authors": "Ying Tai, Yicong Liang, Xiaoming Liu, Lei Duan, Jilin Li, Chengjie\n  Wang, Feiyue Huang, Yu Chen", "title": "Towards Highly Accurate and Stable Face Alignment for High-Resolution\n  Videos", "comments": "Accepted to AAAI 2019. 8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, heatmap regression based models have shown their\neffectiveness in face alignment and pose estimation. However, Conventional\nHeatmap Regression (CHR) is not accurate nor stable when dealing with\nhigh-resolution facial videos, since it finds the maximum activated location in\nheatmaps which are generated from rounding coordinates, and thus leads to\nquantization errors when scaling back to the original high-resolution space. In\nthis paper, we propose a Fractional Heatmap Regression (FHR) for\nhigh-resolution video-based face alignment. The proposed FHR can accurately\nestimate the fractional part according to the 2D Gaussian function by sampling\nthree points in heatmaps. To further stabilize the landmarks among continuous\nvideo frames while maintaining the precise at the same time, we propose a novel\nstabilization loss that contains two terms to address time delay and non-smooth\nissues, respectively. Experiments on 300W, 300-VW and Talking Face datasets\nclearly demonstrate that the proposed method is more accurate and stable than\nthe state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 12:35:09 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 14:40:25 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Tai", "Ying", ""], ["Liang", "Yicong", ""], ["Liu", "Xiaoming", ""], ["Duan", "Lei", ""], ["Li", "Jilin", ""], ["Wang", "Chengjie", ""], ["Huang", "Feiyue", ""], ["Chen", "Yu", ""]]}, {"id": "1811.00344", "submitter": "Subeesh Vasu", "authors": "Subeesh Vasu, Nimisha Thekke Madam, Rajagopalan A.N", "title": "Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual\n  Super-resolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) based methods have recently achieved great\nsuccess for image super-resolution (SR). However, most deep CNN based SR models\nattempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while\nresulting in poor quantified perceptual quality (e.g. human opinion score,\nno-reference quality measures such as NIQE). Few works have attempted to\nimprove the perceptual quality at the cost of performance reduction in\ndistortion measures. A very recent study has revealed that distortion and\nperceptual quality are at odds with each other and there is always a trade-off\nbetween the two. Often the restoration algorithms that are superior in terms of\nperceptual quality, are inferior in terms of distortion measures. Our work\nattempts to analyze the trade-off between distortion and perceptual quality for\nthe problem of single image SR. To this end, we use the well-known SR\narchitecture-enhanced deep super-resolution (EDSR) network and show that it can\nbe adapted to achieve better perceptual quality for a specific range of the\ndistortion measure. While the original network of EDSR was trained to minimize\nthe error defined based on per-pixel accuracy alone, we train our network using\na generative adversarial network framework with EDSR as the generator module.\nOur proposed network, called enhanced perceptual super-resolution network\n(EPSR), is trained with a combination of mean squared error loss, perceptual\nloss, and adversarial loss. Our experiments reveal that EPSR achieves the\nstate-of-the-art trade-off between distortion and perceptual quality while the\nexisting methods perform well in either of these measures alone.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 12:45:25 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 06:12:53 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Vasu", "Subeesh", ""], ["Madam", "Nimisha Thekke", ""], ["N", "Rajagopalan A.", ""]]}, {"id": "1811.00367", "submitter": "Yanyun Qu", "authors": "Xiaotong Luo, Rong Chen, Yuan Xie, Yanyun Qu, and Cuihua Li", "title": "Bi-GANs-ST for Perceptual Image Super-resolution", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality measurement is a critical problem for image super-resolution\n(SR) algorithms. Usually, they are evaluated by some well-known objective\nmetrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results\nin accordance with the perception of human being. Recently, a more reasonable\nperception measurement has been proposed in [1], which is also adopted by the\nPIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a\nhigh-quality SR result which balances between the two indices, i.e., the\nperception index and root-mean-square error (RMSE). To do so, we design a new\ndeep SR framework, dubbed Bi-GANs-ST, by integrating two complementary\ngenerative adversarial networks (GAN) branches. One is memory residual SRGAN\n(MR-SRGAN), which emphasizes on improving the objective performance, such as\nreducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which\nobtains the result that favors better subjective perception via a two-stage\nadversarial training mechanism. Then, to produce final result with excellent\nperception scores and RMSE, we use soft-thresholding method to merge the\nresults generated by the two GANs. Our method performs well on the perceptual\nimage super-resolution task of the PIRM 2018 challenge. Experimental results on\nfive benchmarks show that our proposal achieves highly competent performance\ncompared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 13:27:56 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Luo", "Xiaotong", ""], ["Chen", "Rong", ""], ["Xie", "Yuan", ""], ["Qu", "Yanyun", ""], ["Li", "Cuihua", ""]]}, {"id": "1811.00386", "submitter": "Cedric Scheerlinck", "authors": "Cedric Scheerlinck, Nick Barnes, Robert Mahony", "title": "Continuous-time Intensity Estimation Using Event Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras provide asynchronous, data-driven measurements of local\ntemporal contrast over a large dynamic range with extremely high temporal\nresolution. Conventional cameras capture low-frequency reference intensity\ninformation. These two sensor modalities provide complementary information. We\npropose a computationally efficient, asynchronous filter that continuously\nfuses image frames and events into a single high-temporal-resolution,\nhigh-dynamic-range image state. In absence of conventional image frames, the\nfilter can be run on events only. We present experimental results on\nhigh-speed, high-dynamic-range sequences, as well as on new ground truth\ndatasets we generate to demonstrate the proposed algorithm outperforms existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 13:56:26 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Scheerlinck", "Cedric", ""], ["Barnes", "Nick", ""], ["Mahony", "Robert", ""]]}, {"id": "1811.00401", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "J\\\"orn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge", "title": "Excessive Invariance Causes Adversarial Vulnerability", "comments": null, "journal-ref": "Proceedings of the 7th International Conference on Learning\n  Representations (ICLR), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their impressive performance, deep neural networks exhibit striking\nfailures on out-of-distribution inputs. One core idea of adversarial example\nresearch is to reveal neural network errors under such distribution shifts. We\ndecompose these errors into two complementary sources: sensitivity and\ninvariance. We show deep networks are not only too sensitive to task-irrelevant\nchanges of their input, as is well-known from epsilon-adversarial examples, but\nare also too invariant to a wide range of task-relevant changes, thus making\nvast regions in input space vulnerable to adversarial attacks. We show such\nexcessive invariance occurs across various tasks and architecture types. On\nMNIST and ImageNet one can manipulate the class-specific content of almost any\nimage without changing the hidden activations. We identify an insufficiency of\nthe standard cross-entropy loss as a reason for these failures. Further, we\nextend this objective based on an information-theoretic analysis so it\nencourages the model to consider all task-dependent features in its decision.\nThis provides the first approach tailored explicitly to overcome excessive\ninvariance and resulting vulnerabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:14:03 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 03:26:21 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 04:12:20 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2020 07:26:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["Behrmann", "Jens", ""], ["Zemel", "Richard", ""], ["Bethge", "Matthias", ""]]}, {"id": "1811.00438", "submitter": "Rahul Mitra", "authors": "Nehal Doiphode, Rahul Mitra, Shuaib Ahmed, Arjun Jain", "title": "An Improved Learning Framework for Covariant Local Feature Detection", "comments": "15 pages", "journal-ref": "ACCV 2018 Camera Ready", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning feature detection has been largely an unexplored area when compared\nto handcrafted feature detection. Recent learning formulations use the\ncovariant constraint in their loss function to learn covariant detectors.\nHowever, just learning from covariant constraint can lead to detection of\nunstable features. To impart further, stability detectors are trained to\nextract pre-determined features obtained by hand-crafted detectors. However, in\nthe process they lose the ability to detect novel features. In an attempt to\novercome the above limitations, we propose an improved scheme by incorporating\ncovariant constraints in form of triplets with addition to an affine covariant\nconstraint. We show that using these additional constraints one can learn to\ndetect novel and stable features without using pre-determined features for\ntraining. Extensive experiments show our model achieves state-of-the-art\nperformance in repeatability score on the well known datasets such as\nVgg-Affine, EF, and Webcam.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:34:00 GMT"}], "update_date": "2018-11-04", "authors_parsed": [["Doiphode", "Nehal", ""], ["Mitra", "Rahul", ""], ["Ahmed", "Shuaib", ""], ["Jain", "Arjun", ""]]}, {"id": "1811.00445", "submitter": "Wenbin Li", "authors": "Wenbin Li, Wei Xiong, Haofu Liao, Jing Huo, Yang Gao and Jiebo Luo", "title": "CariGAN: Caricature Generation through Weakly Paired Adversarial\n  Learning", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature generation is an interesting yet challenging task. The primary\ngoal is to generate plausible caricatures with reasonable exaggerations given\nface images. Conventional caricature generation approaches mainly use low-level\ngeometric transformations such as image warping to generate exaggerated images,\nwhich lack richness and diversity in terms of content and style. The recent\nprogress in generative adversarial networks (GANs) makes it possible to learn\nan image-to-image transformation from data, so that richer contents and styles\ncan be generated. However, directly applying the GAN-based models to this task\nleads to unsatisfactory results because there is a large variance in the\ncaricature distribution. Moreover, some models require strictly paired training\ndata which largely limits their usage scenarios. In this paper, we propose\nCariGAN overcome these problems. Instead of training on paired data, CariGAN\nlearns transformations only from weakly paired images. Specifically, to enforce\nreasonable exaggeration and facial deformation, facial landmarks are adopted as\nan additional condition to constrain the generated image. Furthermore, an\nattention mechanism is introduced to encourage our model to focus on the key\nfacial parts so that more vivid details in these regions can be generated.\nFinally, a Diversity Loss is proposed to encourage the model to produce diverse\nresults to help alleviate the `mode collapse' problem of the conventional\nGAN-based models. Extensive experiments on a new large-scale `WebCaricature'\ndataset show that the proposed CariGAN can generate more plausible caricatures\nwith larger diversity compared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:40:23 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 21:22:31 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Li", "Wenbin", ""], ["Xiong", "Wei", ""], ["Liao", "Haofu", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1811.00472", "submitter": "Weidi Xie", "authors": "Erika Lu, Weidi Xie and Andrew Zisserman", "title": "Class-Agnostic Counting", "comments": "Asian Conference on Computer Vision (ACCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly all existing counting methods are designed for a specific object\nclass. Our work, however, aims to create a counting model able to count any\nclass of object. To achieve this goal, we formulate counting as a matching\nproblem, enabling us to exploit the image self-similarity property that\nnaturally exists in object counting problems. We make the following three\ncontributions: first, a Generic Matching Network (GMN) architecture that can\npotentially count any object in a class-agnostic manner; second, by\nreformulating the counting problem as one of matching objects, we can take\nadvantage of the abundance of video data labeled for tracking, which contains\nnatural repetitions suitable for training a counting model. Such data enables\nus to train the GMN. Third, to customize the GMN to different user\nrequirements, an adapter module is used to specialize the model with minimal\neffort, i.e. using a few labeled examples, and adapting only a small fraction\nof the trained parameters. This is a form of few-shot learning, which is\npractical for domains where labels are limited due to requiring expert\nknowledge (e.g. microbiology). We demonstrate the flexibility of our method on\na diverse set of existing counting benchmarks: specifically cells, cars, and\nhuman crowds. The model achieves competitive performance on cell and crowd\ncounting datasets, and surpasses the state-of-the-art on the car dataset using\nonly three training images. When training on the entire dataset, the proposed\nmethod outperforms all previous methods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:11:42 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Lu", "Erika", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1811.00473", "submitter": "Moacir Antonelli Ponti", "authors": "Gabriel B. Cavallari, Leonardo Sampaio Ferraz Ribeiro, Moacir\n  Antonelli Ponti", "title": "Unsupervised representation learning using convolutional and stacked\n  auto-encoders: a domain and cross-domain feature space analysis", "comments": "SIBGRAPI 2018 - Conference on Graphics, Patterns and Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A feature learning task involves training models that are capable of\ninferring good representations (transformations of the original space) from\ninput data alone. When working with limited or unlabelled data, and also when\nmultiple visual domains are considered, methods that rely on large annotated\ndatasets, such as Convolutional Neural Networks (CNNs), cannot be employed. In\nthis paper we investigate different auto-encoder (AE) architectures, which\nrequire no labels, and explore training strategies to learn representations\nfrom images. The models are evaluated considering both the reconstruction error\nof the images and the feature spaces in terms of their discriminative power. We\nstudy the role of dense and convolutional layers on the results, as well as the\ndepth and capacity of the networks, since those are shown to affect both the\ndimensionality reduction and the capability of generalising for different\nvisual domains. Classification results with AE features were as discriminative\nas pre-trained CNN features. Our findings can be used as guidelines for the\ndesign of unsupervised representation learning methods within and across\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:11:45 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Cavallari", "Gabriel B.", ""], ["Ribeiro", "Leonardo Sampaio Ferraz", ""], ["Ponti", "Moacir Antonelli", ""]]}, {"id": "1811.00482", "submitter": "Xiaofan Xu", "authors": "Xiaofan Xu, Mi Sun Park, Cormac Brick", "title": "Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce hybrid pruning which combines both coarse-grained channel and\nfine-grained weight pruning to reduce model size, computation and power demands\nwith no to little loss in accuracy for enabling modern networks deployment on\nresource-constrained devices, such as always-on security cameras and drones.\nAdditionally, to effectively perform channel pruning, we propose a fast\nsensitivity test that helps us quickly identify the sensitivity of within and\nacross layers of a network to the output accuracy for target multiplier\naccumulators (MACs) or accuracy tolerance. Our experiment shows significantly\nbetter results on ResNet50 on ImageNet compared to existing work, even with an\nadditional constraint of channels be hardware-friendly number.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:24:50 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Xu", "Xiaofan", ""], ["Park", "Mi Sun", ""], ["Brick", "Cormac", ""]]}, {"id": "1811.00491", "submitter": "Alane Suhr", "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav\n  Artzi", "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs", "comments": "ACL 2019 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:47:44 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 16:13:16 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 05:26:36 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Suhr", "Alane", ""], ["Zhou", "Stephanie", ""], ["Zhang", "Ally", ""], ["Zhang", "Iris", ""], ["Bai", "Huajun", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.00501", "submitter": "Roey Mechrez", "authors": "Avi Ben-Cohen, Roey Mechrez, Noa Yedidia, Hayit Greenspan", "title": "Improving CNN Training using Disentanglement for Liver Lesion\n  Classification in CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data is the key component in designing algorithms for medical image\nanalysis and in many cases it is the main bottleneck in achieving good results.\nRecent progress in image generation has enabled the training of neural network\nbased solutions using synthetic data. A key factor in the generation of new\nsamples is controlling the important appearance features and potentially being\nable to generate a new sample of a specific class with different variants. In\nthis work we suggest the synthesis of new data by mixing the class specified\nand unspecified representation of different factors in the training data. Our\nexperiments on liver lesion classification in CT show an average improvement of\n7.4% in accuracy over the baseline training scheme.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:12:14 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Mechrez", "Roey", ""], ["Yedidia", "Noa", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1811.00506", "submitter": "Jing Bi", "authors": "Jing Bi, Tianyou Xiao, Qiuyue Sun and Chenliang Xu", "title": "Navigation by Imitation in a Pedestrian-Rich Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained on demonstrations of human actions give robot\nthe ability to perform self-driving on the road. However, navigation in a\npedestrian-rich environment, such as a campus setup, is still challenging---one\nneeds to take frequent interventions to the robot and take control over the\nrobot from early steps leading to a mistake. An arduous burden is, hence,\nplaced on the learning framework design and data acquisition. In this paper, we\npropose a new learning-from-intervention Dataset Aggregation (DAgger) algorithm\nto overcome the limitations brought by applying imitation learning to\nnavigation in the pedestrian-rich environment. Our new learning algorithm\nimplements an error backtrack function that is able to effectively learn from\nexpert interventions. Combining our new learning algorithm with deep\nconvolutional neural networks and a hierarchically-nested policy-selection\nmechanism, we show that our robot is able to map pixels direct to control\ncommands and navigate successfully in real world without explicitly modeling\nthe pedestrian behaviors or the world model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:20:03 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Bi", "Jing", ""], ["Xiao", "Tianyou", ""], ["Sun", "Qiuyue", ""], ["Xu", "Chenliang", ""]]}, {"id": "1811.00538", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Svetlana Lazebnik, Alexander G. Schwing", "title": "Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual\n  Question Answering", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately answering a question about a given image requires combining\nobservations with general knowledge. While this is effortless for humans,\nreasoning with general knowledge remains an algorithmic challenge. To advance\nresearch in this direction a novel `fact-based' visual question answering\n(FVQA) task has been introduced recently along with a large set of curated\nfacts which link two entities, i.e., two possible answers, via a relation.\nGiven a question-image pair, deep network techniques have been employed to\nsuccessively reduce the large set of facts until one of the two entities of the\nfinal remaining fact is predicted as the answer. We observe that a successive\nprocess which considers one fact at a time to form a local decision is\nsub-optimal. Instead, we develop an entity graph and use a graph convolutional\nnetwork to `reason' about the correct answer by jointly considering all\nentities. We show on the challenging FVQA dataset that this leads to an\nimprovement in accuracy of around 7% compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:59:56 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Lazebnik", "Svetlana", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1811.00597", "submitter": "Parnian Afshar", "authors": "Parnian Afshar, Konstantinos N. Plataniotis, Arash Mohammadi", "title": "Capsule Networks for Brain Tumor Classification based on MRI Images and\n  Course Tumor Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to official statistics, cancer is considered as the second leading\ncause of human fatalities. Among different types of cancer, brain tumor is seen\nas one of the deadliest forms due to its aggressive nature, heterogeneous\ncharacteristics, and low relative survival rate. Determining the type of brain\ntumor has significant impact on the treatment choice and patient's survival.\nHuman-centered diagnosis is typically error-prone and unreliable resulting in a\nrecent surge of interest to automatize this process using convolutional neural\nnetworks (CNNs). CNNs, however, fail to fully utilize spatial relations, which\nis particularly harmful for tumor classification, as the relation between the\ntumor and its surrounding tissue is a critical indicator of the tumor's type.\nIn our recent work, we have incorporated newly developed CapsNets to overcome\nthis shortcoming. CapsNets are, however, highly sensitive to the miscellaneous\nimage background. The paper addresses this gap. The main contribution is to\nequip CapsNet with access to the tumor surrounding tissues, without distracting\nit from the main target. A modified CapsNet architecture is, therefore,\nproposed for brain tumor classification, which takes the tumor coarse\nboundaries as extra inputs within its pipeline to increase the CapsNet's focus.\nThe proposed approach noticeably outperforms its counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:23:38 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Afshar", "Parnian", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""]]}, {"id": "1811.00648", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Pascal Colling, Thomas-Paul Hack, Robin Chan,\n  Fabian H\\\"uger, Peter Schlicht, Hanno Gottschalk", "title": "Prediction Error Meta Classification in Semantic Segmentation: Detection\n  via Aggregated Dispersion Measures of Softmax Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that \"meta\" classifies whether seg-ments predicted by a\nsemantic segmentation neural networkintersect with the ground truth. For this\npurpose, we employ measures of dispersion for predicted pixel-wise class\nprobability distributions, like classification entropy, that yield heat maps of\nthe input scene's size. We aggregate these dispersion measures segment-wise and\nderive metrics that are well-correlated with the segment-wise IoU of prediction\nand ground truth. This procedure yields an almost plug and play post-processing\ntool to rate the prediction quality of semantic segmentation networks on\nsegment level. This is especially relevant for monitoring neural networks in\nonline applications like automated driving or medical imaging where reliability\nis of utmost importance. In our tests, we use publicly available\nstate-of-the-art networks trained on the Cityscapes dataset and the BraTS2017\ndataset and analyze the predictive power of different metrics as well as\ndifferent sets of metrics. To this end, we compute logistic LASSO regression\nfits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC\nvalues of up to 91.55%. We complement these tests with linear regression fits\nto predict the segment-wise IoU and obtain prediction standard deviations of\ndown to 0.130 as well as $R^2$ values of up to 84.15%. We show that these\nresults clearly outperform standard approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:00:00 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 14:38:24 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Rottmann", "Matthias", ""], ["Colling", "Pascal", ""], ["Hack", "Thomas-Paul", ""], ["Chan", "Robin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1811.00656", "submitter": "Yuezun Li", "authors": "Yuezun Li and Siwei Lyu", "title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts", "comments": "CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe a new deep learning based method that can\neffectively distinguish AI-generated fake videos (referred to as {\\em DeepFake}\nvideos hereafter) from real videos. Our method is based on the observations\nthat current DeepFake algorithm can only generate images of limited\nresolutions, which need to be further warped to match the original faces in the\nsource video. Such transforms leave distinctive artifacts in the resulting\nDeepFake videos, and we show that they can be effectively captured by\nconvolutional neural networks (CNNs). Compared to previous methods which use a\nlarge amount of real and DeepFake generated images to train CNN classifier, our\nmethod does not need DeepFake generated images as negative training examples\nsince we target the artifacts in affine face warping as the distinctive feature\nto distinguish real and fake images. The advantages of our method are two-fold:\n(1) Such artifacts can be simulated directly using simple image processing\noperations on a image to make it as negative example. Since training a DeepFake\nmodel to generate negative examples is time-consuming and resource-demanding,\nour method saves a plenty of time and resources in training data collection;\n(2) Since such artifacts are general existed in DeepFake videos from different\nsources, our method is more robust compared to others. Our method is evaluated\non two sets of DeepFake video datasets for its effectiveness in practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:13:55 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 01:20:52 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 15:06:39 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Li", "Yuezun", ""], ["Lyu", "Siwei", ""]]}, {"id": "1811.00661", "submitter": "Yuezun Li", "authors": "Xin Yang, Yuezun Li and Siwei Lyu", "title": "Exposing Deep Fakes Using Inconsistent Head Poses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to expose AI-generated fake face\nimages or videos (commonly known as the Deep Fakes). Our method is based on the\nobservations that Deep Fakes are created by splicing synthesized face region\ninto the original image, and in doing so, introducing errors that can be\nrevealed when 3D head poses are estimated from the face images. We perform\nexperiments to demonstrate this phenomenon and further develop a classification\nmethod based on this cue. Using features based on this cue, an SVM classifier\nis evaluated using a set of real face images and Deep Fakes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:27:20 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:47:51 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Yang", "Xin", ""], ["Li", "Yuezun", ""], ["Lyu", "Siwei", ""]]}, {"id": "1811.00662", "submitter": "Ji Zhang", "authors": "Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal", "title": "Introduction to the 1st Place Winning Model of OpenImages Relationship\n  Detection Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the model we built that achieved 1st place in the\nOpenImage Visual Relationship Detection Challenge on Kaggle. Three key factors\ncontribute the most to our success: 1) language bias is a powerful baseline for\nthis task. We build the empirical distribution $P(predicate|subject,object)$ in\nthe training set and directly use that in testing. This baseline achieved the\n2nd place when submitted; 2) spatial features are as important as visual\nfeatures, especially for spatial relationships such as \"under\" and \"inside of\";\n3) It is a very effective way to fuse different features by first building\nseparate modules for each of them, then adding their output logits before the\nfinal softmax layer. We show in ablation study that each factor can improve the\nperformance to a non-trivial extent, and the model reaches optimal when all of\nthem are combined.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:44:01 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 19:42:31 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhang", "Ji", ""], ["Shih", "Kevin", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1811.00684", "submitter": "Fitsum Reda", "authors": "Fitsum A. Reda, Guilin Liu, Kevin J. Shih, Robert Kirby, Jon Barker,\n  David Tarjan, Andrew Tao, Bryan Catanzaro", "title": "SDCNet: Video Prediction Using Spatially-Displaced Convolution", "comments": "Published in ECCV 2018. Codes available at\n  https://github.com/NVIDIA/semantic-segmentation/tree/sdcnet/sdcnet. Project\n  page available at https://nv-adlr.github.io/publication/2018-SDCNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for high-resolution video frame prediction by\nconditioning on both past frames and past optical flows. Previous approaches\nrely on resampling past frames, guided by a learned future optical flow, or on\ndirect generation of pixels. Resampling based on flow is insufficient because\nit cannot deal with disocclusions. Generative models currently lead to blurry\nresults. Recent approaches synthesis a pixel by convolving input patches with a\npredicted kernel. However, their memory requirement increases with kernel size.\nHere, we spatially-displaced convolution (SDC) module for video frame\nprediction. We learn a motion vector and a kernel for each pixel and synthesize\na pixel by applying the kernel at a displaced location in the source image,\ndefined by the predicted motion vector. Our approach inherits the merits of\nboth vector-based and kernel-based approaches, while ameliorating their\nrespective disadvantages. We train our model on 428K unlabelled 1080p video\ngame frames. Our approach produces state-of-the-art results, achieving an SSIM\nscore of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech\nPedestrian videos. Our model handles large motion effectively and synthesizes\ncrisp frames with consistent motion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 00:14:05 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 00:13:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Reda", "Fitsum A.", ""], ["Liu", "Guilin", ""], ["Shih", "Kevin J.", ""], ["Kirby", "Robert", ""], ["Barker", "Jon", ""], ["Tarjan", "David", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1811.00700", "submitter": "Jia Li", "authors": "Jia Li, Yafei Song, Jianfeng Zhu, Lele Cheng, Ying Su, Lin Ye,\n  Pengcheng Yuan, Shumin Han", "title": "Learning from Large-scale Noisy Web Data with Ubiquitous Reweighting for\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many advances of deep learning techniques originate from the efforts of\naddressing the image classification task on large-scale datasets. However, the\nconstruction of such clean datasets is costly and time-consuming since the\nInternet is overwhelmed by noisy images with inadequate and inaccurate tags. In\nthis paper, we propose a Ubiquitous Reweighting Network (URNet) that learns an\nimage classification model from large-scale noisy data. By observing the web\ndata, we find that there are five key challenges, i.e., imbalanced class sizes,\nhigh intra-classes diversity and inter-class similarity, imprecise instances,\ninsufficient representative instances, and ambiguous class labels. To alleviate\nthese challenges, we assume that every training instance has the potential to\ncontribute positively by alleviating the data bias and noise via reweighting\nthe influence of each instance according to different class sizes, large\ninstance clusters, its confidence, small instance bags and the labels. In this\nmanner, the influence of bias and noise in the web data can be gradually\nalleviated, leading to the steadily improving performance of URNet.\nExperimental results in the WebVision 2018 challenge with 16 million noisy\ntraining images from 5000 classes show that our approach outperforms\nstate-of-the-art models and ranks the first place in the image classification\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 01:26:28 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 10:30:02 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Li", "Jia", ""], ["Song", "Yafei", ""], ["Zhu", "Jianfeng", ""], ["Cheng", "Lele", ""], ["Su", "Ying", ""], ["Ye", "Lin", ""], ["Yuan", "Pengcheng", ""], ["Han", "Shumin", ""]]}, {"id": "1811.00743", "submitter": "Gullal Singh Cheema", "authors": "Ankita Shukla and Gullal Singh Cheema and Saket Anand and Qamar\n  Qureshi and Yadvendradev Jhala", "title": "Unique Identification of Macaques for Population Monitoring and Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite loss of natural habitat due to development and urbanization, certain\nspecies like the Rhesus macaque have adapted well to the urban environment.\nWith abundant food and no predators, macaque populations have increased\nsubstantially in urban areas, leading to frequent conflicts with humans.\nOverpopulated areas often witness macaques raiding crops, feeding on bird and\nsnake eggs as well as destruction of nests, thus adversely affecting other\nspecies in the ecosystem. In order to mitigate these adverse effects,\nsterilization has emerged as a humane and effective way of population control\nof macaques. As sterilization requires physical capture of individuals or\ngroups, their unique identification is integral to such control measures. In\nthis work, we propose the Macaque Face Identification (MFID), an image based,\nnon-invasive tool that relies on macaque facial recognition to identify\nindividuals, and can be used to verify if they are sterilized. Our primary\ncontribution is a robust facial recognition and verification module designed\nfor Rhesus macaques, but extensible to other non-human primate species. We\nevaluate the performance of MFID on a dataset of 93 monkeys under closed set,\nopen set and verification evaluation protocols. Finally, we also report state\nof the art results when evaluating our proposed model on endangered primate\nspecies.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:32:36 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 20:55:31 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Shukla", "Ankita", ""], ["Cheema", "Gullal Singh", ""], ["Anand", "Saket", ""], ["Qureshi", "Qamar", ""], ["Jhala", "Yadvendradev", ""]]}, {"id": "1811.00751", "submitter": "Chunhua Shen", "authors": "Hui Li, Peng Wang, Chunhua Shen, Guyu Zhang", "title": "Show, Attend and Read: A Simple and Strong Baseline for Irregular Text\n  Recognition", "comments": "Accepted to Proc. AAAI Conference on Artificial Intelligence 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing irregular text in natural scene images is challenging due to the\nlarge variance in text appearance, such as curvature, orientation and\ndistortion. Most existing approaches rely heavily on sophisticated model\ndesigns and/or extra fine-grained annotations, which, to some extent, increase\nthe difficulty in algorithm implementation and data collection. In this work,\nwe propose an easy-to-implement strong baseline for irregular scene text\nrecognition, using off-the-shelf neural network components and only word-level\nannotations. It is composed of a $31$-layer ResNet, an LSTM-based\nencoder-decoder framework and a 2-dimensional attention module. Despite its\nsimplicity, the proposed method is robust and achieves state-of-the-art\nperformance on both regular and irregular scene text recognition benchmarks.\nCode is available at: https://tinyurl.com/ShowAttendRead\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 06:13:16 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 05:58:16 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Hui", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Zhang", "Guyu", ""]]}, {"id": "1811.00793", "submitter": "Iro Laina", "authors": "Ghazal Ghazaei, Iro Laina, Christian Rupprecht, Federico Tombari,\n  Nassir Navab, Kianoush Nazarpour", "title": "Dealing with Ambiguity in Robotic Grasping via Multiple Predictions", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans excel in grasping and manipulating objects because of their life-long\nexperience and knowledge about the 3D shape and weight distribution of objects.\nHowever, the lack of such intuition in robots makes robotic grasping an\nexceptionally challenging task. There are often several equally viable options\nof grasping an object. However, this ambiguity is not modeled in conventional\nsystems that estimate a single, optimal grasp position. We propose to tackle\nthis problem by simultaneously estimating multiple grasp poses from a single\nRGB image of the target object. Further, we reformulate the problem of robotic\ngrasping by replacing conventional grasp rectangles with grasp belief maps,\nwhich hold more precise location information than a rectangle and account for\nthe uncertainty inherent to the task. We augment a fully convolutional neural\nnetwork with a multiple hypothesis prediction model that predicts a set of\ngrasp hypotheses in under 60ms, which is critical for real-time robotic\napplications. The grasp detection accuracy reaches over 90% for unseen objects,\noutperforming the current state of the art on this task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:42:15 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Ghazaei", "Ghazal", ""], ["Laina", "Iro", ""], ["Rupprecht", "Christian", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""], ["Nazarpour", "Kianoush", ""]]}, {"id": "1811.00846", "submitter": "Yashasvi Baweja", "authors": "Rishabh Garg, Yashasvi Baweja, Soumyadeep Ghosh, Mayank Vatsa, Richa\n  Singh, Nalini Ratha", "title": "Heterogeneity Aware Deep Embedding for Mobile Periocular Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile biometric approaches provide the convenience of secure authentication\nwith an omnipresent technology. However, this brings an additional challenge of\nrecognizing biometric patterns in unconstrained environment including\nvariations in mobile camera sensors, illumination conditions, and capture\ndistance. To address the heterogeneous challenge, this research presents a\nnovel heterogeneity aware loss function within a deep learning framework. The\neffectiveness of the proposed loss function is evaluated for periocular\nbiometrics using the CSIP, IMP and VISOB mobile periocular databases. The\nresults show that the proposed algorithm yields state-of-the-art results in a\nheterogeneous environment and improves generalizability for cross-database\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 13:25:38 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Garg", "Rishabh", ""], ["Baweja", "Yashasvi", ""], ["Ghosh", "Soumyadeep", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Ratha", "Nalini", ""]]}, {"id": "1811.00871", "submitter": "Jaemin Son", "authors": "Jaemin Son, Woong Bae, Sangkeun Kim, Sang Jun Park, and Kyu-Hwan Jung", "title": "Classification of Findings with Localized Lesions in Fundoscopic Images\n  using a Regionally Guided CNN", "comments": "8 pages, Computational Pathology and Ophthalmic Medical Image\n  Analysis, pp.176-184", "journal-ref": null, "doi": "10.1007/978-3-030-00949-6_21", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fundoscopic images are often investigated by ophthalmologists to spot\nabnormal lesions to make diagnoses. Recent successes of convolutional neural\nnetworks are confined to diagnoses of few diseases without proper localization\nof lesion. In this paper, we propose an efficient annotation method for\nlocalizing lesions and a CNN architecture that can classify an individual\nfinding and localize the lesions at the same time. Also, we introduce a new\nloss function to guide the network to learn meaningful patterns with the\nguidance of the regional annotations. In experiments, we demonstrate that our\nnetwork performed better than the widely used network and the guidance loss\nhelps achieve higher AUROC up to 4.1% and superior localization capability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:15:14 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Son", "Jaemin", ""], ["Bae", "Woong", ""], ["Kim", "Sangkeun", ""], ["Park", "Sang Jun", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "1811.00982", "submitter": "Jordi Pont-Tuset", "authors": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\n  Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci,\n  Alexander Kolesnikov, Tom Duerig, Vittorio Ferrari", "title": "The Open Images Dataset V4: Unified image classification, object\n  detection, and visual relationship detection at scale", "comments": "Accepted to International Journal of Computer Vision, 2020", "journal-ref": null, "doi": "10.1007/s11263-020-01316-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Open Images V4, a dataset of 9.2M images with unified annotations\nfor image classification, object detection and visual relationship detection.\nThe images have a Creative Commons Attribution license that allows to share and\nadapt the material, and they have been collected from Flickr without a\npredefined list of class names or tags, leading to natural class statistics and\navoiding an initial design bias. Open Images V4 offers large scale across\nseveral dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding\nboxes for 600 object classes, and 375k visual relationship annotations\ninvolving 57 classes. For object detection in particular, we provide 15x more\nbounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The\nimages often show complex scenes with several objects (8 annotated objects per\nimage on average). We annotated visual relationships between them, which\nsupport visual relationship detection, an emerging task that requires\nstructured reasoning. We provide in-depth comprehensive statistics about the\ndataset, we validate the quality of the annotations, we study how the\nperformance of several modern models evolves with increasing amounts of\ntraining data, and we demonstrate two applications made possible by having\nunified annotations of multiple types coexisting in the same images. We hope\nthat the scale, quality, and variety of Open Images V4 will foster further\nresearch and innovation even beyond the areas of image classification, object\ndetection, and visual relationship detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:58:28 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:15:33 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Kuznetsova", "Alina", ""], ["Rom", "Hassan", ""], ["Alldrin", "Neil", ""], ["Uijlings", "Jasper", ""], ["Krasin", "Ivan", ""], ["Pont-Tuset", "Jordi", ""], ["Kamali", "Shahab", ""], ["Popov", "Stefan", ""], ["Malloci", "Matteo", ""], ["Kolesnikov", "Alexander", ""], ["Duerig", "Tom", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1811.00995", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud,\n  J\\\"orn-Henrik Jacobsen", "title": "Invertible Residual Networks", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that standard ResNet architectures can be made invertible, allowing\nthe same model to be used for classification, density estimation, and\ngeneration. Typically, enforcing invertibility requires partitioning dimensions\nor restricting network architectures. In contrast, our approach only requires\nadding a simple normalization step during training, already available in\nstandard frameworks. Invertible ResNets define a generative model which can be\ntrained by maximum likelihood on unlabeled data. To compute likelihoods, we\nintroduce a tractable approximation to the Jacobian log-determinant of a\nresidual block. Our empirical evaluation shows that invertible ResNets perform\ncompetitively with both state-of-the-art image classifiers and flow-based\ngenerative models, something that has not been previously achieved with a\nsingle architecture.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:17:55 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 17:18:26 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 18:19:33 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Behrmann", "Jens", ""], ["Grathwohl", "Will", ""], ["Chen", "Ricky T. Q.", ""], ["Duvenaud", "David", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""]]}, {"id": "1811.01045", "submitter": "Pan Ji", "authors": "Tong Zhang, Pan Ji, Mehrtash Harandi, Richard Hartley, Ian Reid", "title": "Scalable Deep $k$-Subspace Clustering", "comments": "To appear in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering algorithms are notorious for their scalability issues\nbecause building and processing large affinity matrices are demanding. In this\npaper, we introduce a method that simultaneously learns an embedding space\nalong subspaces within it to minimize a notion of reconstruction error, thus\naddressing the problem of subspace clustering in an end-to-end learning\nparadigm. To achieve our goal, we propose a scheme to update subspaces within a\ndeep neural network. This in turn frees us from the need of having an affinity\nmatrix to perform clustering. Unlike previous attempts, our method can easily\nscale up to large datasets, making it unique in the context of unsupervised\nlearning with deep architectures. Our experiments show that our method\nsignificantly improves the clustering accuracy while enjoying cheaper memory\nfootprints.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 18:54:10 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Tong", ""], ["Ji", "Pan", ""], ["Harandi", "Mehrtash", ""], ["Hartley", "Richard", ""], ["Reid", "Ian", ""]]}, {"id": "1811.01051", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Junyan Wu, Eric Z. Chen and Hongda Jiang", "title": "What evidence does deep learning model use to classify Skin Lesions?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is a type of skin cancer with the most rapidly increasing incidence.\nEarly detection of melanoma using dermoscopy images significantly increases\npatients' survival rate. However, accurately classifying skin lesions by eye,\nespecially in the early stage of melanoma, is extremely challenging for the\ndermatologists. Hence, the discovery of reliable biomarkers will be meaningful\nfor melanoma diagnosis. Recent years, the value of deep learning empowered\ncomputer-assisted diagnose has been shown in biomedical imaging based decision\nmaking. However, much research focuses on improving disease detection accuracy\nbut not exploring the evidence of pathology. In this paper, we propose a method\nto interpret the deep learning classification findings. Firstly, we propose an\naccurate neural network architecture to classify skin lesions. Secondly, we\nutilize a prediction difference analysis method that examines each patch on the\nimage through patch-wised corrupting to detect the biomarkers. Lastly, we\nvalidate that our biomarker findings are corresponding to the patterns in the\nliterature. The findings can be significant and useful to guide clinical\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 18:57:50 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 15:07:29 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 13:54:47 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Wu", "Junyan", ""], ["Chen", "Eric Z.", ""], ["Jiang", "Hongda", ""]]}, {"id": "1811.01068", "submitter": "Adrian Penate Sanchez", "authors": "Adrian Penate-Sanchez, Lourdes Agapito", "title": "3D Pick & Mix: Object Part Blending in Joint Shape and Image Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present 3D Pick & Mix, a new 3D shape retrieval system that provides users\nwith a new level of freedom to explore 3D shape and Internet image collections\nby introducing the ability to reason about objects at the level of their\nconstituent parts. While classic retrieval systems can only formulate simple\nsearches such as \"find the 3D model that is most similar to the input image\"\nour new approach can formulate advanced and semantically meaningful search\nqueries such as: \"find me the 3D model that best combines the design of the\nlegs of the chair in image 1 but with no armrests, like the chair in image 2\".\nMany applications could benefit from such rich queries, users could browse\nthrough catalogues of furniture and pick and mix parts, combining for example\nthe legs of a chair from one shop and the armrests from another shop.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:56:55 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Penate-Sanchez", "Adrian", ""], ["Agapito", "Lourdes", ""]]}, {"id": "1811.01085", "submitter": "Jonathan Rubin", "authors": "S. Mazdak Abulnaga and Jonathan Rubin", "title": "Ischemic Stroke Lesion Segmentation in CT Perfusion Scans using Pyramid\n  Pooling and Focal Loss", "comments": "BrainLes 2018 MICCAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully convolutional neural network for segmenting ischemic\nstroke lesions in CT perfusion images for the ISLES 2018 challenge. Treatment\nof stroke is time sensitive and current standards for lesion identification\nrequire manual segmentation, a time consuming and challenging process.\nAutomatic segmentation methods present the possibility of accurately\nidentifying lesions and improving treatment planning. Our model is based on the\nPSPNet, a network architecture that makes use of pyramid pooling to provide\nglobal and local contextual information. To learn the varying shapes of the\nlesions, we train our network using focal loss, a loss function designed for\nthe network to focus on learning the more difficult samples. We compare our\nmodel to networks trained using the U-Net and V-Net architectures. Our approach\ndemonstrates effective performance in lesion segmentation and ranked among the\ntop performers at the challenge conclusion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 20:55:20 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Abulnaga", "S. Mazdak", ""], ["Rubin", "Jonathan", ""]]}, {"id": "1811.01194", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis, Muhammad Haris Khan, Georgios Tzimiropoulos", "title": "Pushing the boundaries of audiovisual word recognition using Residual\n  Networks and LSTMs", "comments": "Accepted to Computer Vision and Image Understanding (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual and audiovisual speech recognition are witnessing a renaissance which\nis largely due to the advent of deep learning methods. In this paper, we\npresent a deep learning architecture for lipreading and audiovisual word\nrecognition, which combines Residual Networks equipped with spatiotemporal\ninput layers and Bidirectional LSTMs. The lipreading architecture attains\n11.92% misclassification rate on the challenging Lipreading-In-The-Wild\ndatabase, which is composed of excerpts from BBC-TV, each containing one of the\n500 target words. Audiovisual experiments are performed using both intermediate\nand late integration, as well as several types and levels of environmental\nnoise, and notable improvements over the audio-only network are reported, even\nin the case of clean speech. A further analysis on the utility of target word\nboundaries is provided, as well as on the capacity of the network in modeling\nthe linguistic context of the target word. Finally, we examine difficult word\npairs and discuss how visual information helps towards attaining higher\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 11:30:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Stafylakis", "Themos", ""], ["Khan", "Muhammad Haris", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1811.01206", "submitter": "Qiangguo Jin", "authors": "Qiangguo Jin, Zhaopeng Meng, Tuan D. Pham, Qi Chen, Leyi Wei, Ran Su", "title": "DUNet: A deformable network for retinal vessel segmentation", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2019.04.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of retinal vessels in fundus images plays an important\nrole in the diagnosis of some diseases such as diabetes and hypertension. In\nthis paper, we propose Deformable U-Net (DUNet), which exploits the retinal\nvessels' local features with a U-shape architecture, in an end to end manner\nfor retinal vessel segmentation. Inspired by the recently introduced deformable\nconvolutional networks, we integrate the deformable convolution into the\nproposed network. The DUNet, with upsampling operators to increase the output\nresolution, is designed to extract context information and enable precise\nlocalization by combining low-level feature maps with high-level ones.\nFurthermore, DUNet captures the retinal vessels at various shapes and scales by\nadaptively adjusting the receptive fields according to vessels' scales and\nshapes. Three public datasets DRIVE, STARE and CHASE_DB1 are used to train and\ntest our model. Detailed comparisons between the proposed network and the\ndeformable neural network, U-Net are provided in our study. Results show that\nmore detailed vessels are extracted by DUNet and it exhibits state-of-the-art\nperformance for retinal vessel segmentation with a global accuracy of\n0.9697/0.9722/0.9724 and AUC of 0.9856/0.9868/0.9863 on DRIVE, STARE and\nCHASE_DB1 respectively. Moreover, to show the generalization ability of the\nDUNet, we used another two retinal vessel data sets, one is named WIDE and the\nother is a synthetic data set with diverse styles, named SYNTHE, to\nqualitatively and quantitatively analyzed and compared with other methods.\nResults indicates that DUNet outperforms other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 13:05:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Jin", "Qiangguo", ""], ["Meng", "Zhaopeng", ""], ["Pham", "Tuan D.", ""], ["Chen", "Qi", ""], ["Wei", "Leyi", ""], ["Su", "Ran", ""]]}, {"id": "1811.01238", "submitter": "Norah Asiri N", "authors": "Norah Asiri, Muhammad Hussain, Fadwa Al Adel, Nazih Alzaidi", "title": "Deep Learning based Computer-Aided Diagnosis Systems for Diabetic\n  Retinopathy: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) results in vision loss if not treated early. A\ncomputer-aided diagnosis (CAD) system based on retinal fundus images is an\nefficient and effective method for early DR diagnosis and assisting experts. A\ncomputer-aided diagnosis (CAD) system involves various stages like detection,\nsegmentation and classification of lesions in fundus images. Many traditional\nmachine-learning (ML) techniques based on hand-engineered features have been\nintroduced. The recent emergence of deep learning (DL) and its decisive victory\nover traditional ML methods for various applications motivated the researchers\nto employ it for DR diagnosis, and many deep-learning-based methods have been\nintroduced. In this paper, we review these methods, highlighting their pros and\ncons. In addition, we point out the challenges to be addressed in designing and\nlearning about efficient, effective and robust deep-learning algorithms for\nvarious problems in DR diagnosis and draw attention to directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 15:58:57 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 21:45:48 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Asiri", "Norah", ""], ["Hussain", "Muhammad", ""], ["Adel", "Fadwa Al", ""], ["Alzaidi", "Nazih", ""]]}, {"id": "1811.01268", "submitter": "Samvit Jain", "authors": "Samvit Jain and Xun Zhang and Yuhao Zhou and Ganesh Ananthanarayanan\n  and Junchen Jiang and Yuanchao Shu and Joseph Gonzalez", "title": "ReXCam: Resource-Efficient, Cross-Camera Video Analytics at Scale", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enterprises are increasingly deploying large camera networks for video\nanalytics. Many target applications entail a common problem template: searching\nfor and tracking an object or activity of interest (e.g. a speeding vehicle, a\nbreak-in) through a large camera network in live video. Such cross-camera\nanalytics is compute and data intensive, with cost growing with the number of\ncameras and time. To address this cost challenge, we present ReXCam, a new\nsystem for efficient cross-camera video analytics. ReXCam exploits spatial and\ntemporal locality in the dynamics of real camera networks to guide its\ninference-time search for a query identity. In an offline profiling phase,\nReXCam builds a cross-camera correlation model that encodes the locality\nobserved in historical traffic patterns. At inference time, ReXCam applies this\nmodel to filter frames that are not spatially and temporally correlated with\nthe query identity's current position. In the cases of occasional missed\ndetections, ReXCam performs a fast-replay search on recently filtered video\nframes, enabling gracefully recovery. Together, these techniques allow ReXCam\nto reduce compute workload by 8.3x on an 8-camera dataset, and by 23x - 38x on\na simulated 130-camera dataset. ReXCam has been implemented and deployed on a\ntestbed of 5 AWS DeepLens cameras.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 19:15:15 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 22:50:29 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 05:22:54 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 03:17:48 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jain", "Samvit", ""], ["Zhang", "Xun", ""], ["Zhou", "Yuhao", ""], ["Ananthanarayanan", "Ganesh", ""], ["Jiang", "Junchen", ""], ["Shu", "Yuanchao", ""], ["Gonzalez", "Joseph", ""]]}, {"id": "1811.01290", "submitter": "Shahar Mahpod", "authors": "Shahar Mahpod and Yosi Keller", "title": "Auto-ML Deep Learning for Rashi Scripts OCR", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an OCR scheme for manuscripts printed in Rashi font\nthat is an ancient Hebrew font and corresponding dialect used in religious\nJewish literature, for more than 600 years. The proposed scheme utilizes a\nconvolution neural network (CNN) for visual inference and Long-Short Term\nMemory (LSTM) to learn the Rashi scripts dialect. In particular, we derive an\nAutoML scheme to optimize the CNN architecture, and a book-specific CNN\ntraining to improve the OCR accuracy. The proposed scheme achieved an accuracy\nof more than 99.8% using a dataset of more than 3M annotated letters from the\nResponsa Project dataset.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 21:53:47 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 21:11:36 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Mahpod", "Shahar", ""], ["Keller", "Yosi", ""]]}, {"id": "1811.01292", "submitter": "Ziyan Wang", "authors": "Ricson Cheng, Ziyan Wang and Katerina Fragkiadaki", "title": "Geometry-Aware Recurrent Neural Networks for Active Visual Recognition", "comments": "To appear in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present recurrent geometry-aware neural networks that integrate visual\ninformation across multiple views of a scene into 3D latent feature tensors,\nwhile maintaining an one-to-one mapping between 3D physical locations in the\nworld scene and latent feature locations. Object detection, object\nsegmentation, and 3D reconstruction is then carried out directly using the\nconstructed 3D feature memory, as opposed to any of the input 2D images. The\nproposed models are equipped with differentiable egomotion-aware feature\nwarping and (learned) depth-aware unprojection operations to achieve\ngeometrically consistent mapping between the features in the input frame and\nthe constructed latent model of the scene. We empirically show the proposed\nmodel generalizes much better than geometryunaware LSTM/GRU networks,\nespecially under the presence of multiple objects and cross-object occlusions.\nCombined with active view selection policies, our model learns to select\ninformative viewpoints to integrate information from by \"undoing\" cross-object\nocclusions, seamlessly combining geometry with learning from experience.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 22:24:00 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 04:07:09 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Cheng", "Ricson", ""], ["Wang", "Ziyan", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1811.01306", "submitter": "Domenico G. Sorrenti", "authors": "Augusto L. Ballardini, Daniele Cattaneo, and Domenico G. Sorrenti", "title": "A dataset for benchmarking vision-based localization at intersections", "comments": "7 pages, 26 figures, report describing the work done to prepare a\n  dataset of sequences of a vehicle approaching an intersection, using the\n  sequences recorded in the KITTI dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report we present the work performed in order to build a dataset for\nbenchmarking vision-based localization at intersections, i.e., a set of stereo\nvideo sequences taken from a road vehicle that is approaching an intersection,\naltogether with a reliable measure of the observer position. This report is\nmeant to complement our paper \"Vision-Based Localization at Intersections using\nDigital Maps\" submitted to ICRA2019. It complements the paper because the paper\nuses the dataset, but it had no space for describing the work done to obtain\nit. Moreover, the dataset is of interest for all those tackling the task of\nonline localization at intersections for road vehicles, e.g., for a\nquantitative comparison with the proposal in our submitted paper, and it is\ntherefore appropriate to put the dataset description in a separate report. We\nconsidered all datasets from road vehicles that we could find as for the end of\nAugust 2018. After our evaluation, we kept only sub-sequences from the KITTI\ndataset. In the future we will increase the collection of sequences with data\nfrom our vehicle.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 01:10:09 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ballardini", "Augusto L.", ""], ["Cattaneo", "Daniele", ""], ["Sorrenti", "Domenico G.", ""]]}, {"id": "1811.01328", "submitter": "Qiangguo Jin", "authors": "Qiangguo Jin, Zhaopeng Meng, Changming Sun, Leyi Wei, Ran Su", "title": "RA-UNet: A hybrid deep attention-aware network to extract liver and\n  tumor in CT scans", "comments": null, "journal-ref": null, "doi": "10.3389/fbioe.2020.605132", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of liver and tumor from CT volumes is a challenging task\ndue to their heterogeneous and diffusive shapes. Recently, 2D and 3D deep\nconvolutional neural networks have become popular in medical image segmentation\ntasks because of the utilization of large labeled datasets to learn\nhierarchical features. However, 3D networks have some drawbacks due to their\nhigh cost on computational resources. In this paper, we propose a 3D hybrid\nresidual attention-aware segmentation method, named RA-UNet, to precisely\nextract the liver volume of interests (VOI) and segment tumors from the liver\nVOI. The proposed network has a basic architecture as a 3D U-Net which extracts\ncontextual information combining low-level feature maps with high-level ones.\nAttention modules are stacked so that the attention-aware features change\nadaptively as the network goes \"very deep\" and this is made possible by\nresidual learning. This is the first work that an attention residual mechanism\nis used to process medical volumetric images. We evaluated our framework on the\npublic MICCAI 2017 Liver Tumor Segmentation dataset and the 3DIRCADb dataset.\nThe results show that our architecture outperforms other state-of-the-art\nmethods. We also extend our RA-UNet to brain tumor segmentation on the\nBraTS2018 and BraTS2017 datasets, and the results indicate that RA-UNet\nachieves good performance on a brain tumor segmentation task as well.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 06:18:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Jin", "Qiangguo", ""], ["Meng", "Zhaopeng", ""], ["Sun", "Changming", ""], ["Wei", "Leyi", ""], ["Su", "Ran", ""]]}, {"id": "1811.01333", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Tuan-Anh Bui, Ngai-Man Cheung", "title": "Improving GAN with neighbors embedding and gradient matching", "comments": "Published as a conference paper at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new techniques for training Generative Adversarial Networks\n(GANs). Our objectives are to alleviate mode collapse in GAN and improve the\nquality of the generated samples. First, we propose neighbor embedding, a\nmanifold learning-based regularization to explicitly retain local structures of\nlatent samples in the generated samples. This prevents generator from producing\nnearly identical data samples from different latent samples, and reduces mode\ncollapse. We propose an inverse t-SNE regularizer to achieve this. Second, we\npropose a new technique, gradient matching, to align the distributions of the\ngenerated samples and the real samples. As it is challenging to work with\nhigh-dimensional sample distributions, we propose to align these distributions\nthrough the scalar discriminator scores. We constrain the difference between\nthe discriminator scores of the real samples and generated ones. We further\nconstrain the difference between the gradients of these discriminator scores.\nWe derive these constraints from Taylor approximations of the discriminator\nfunction. We perform experiments to demonstrate that our proposed techniques\nare computationally simple and easy to be incorporated in existing systems.\nWhen Gradient matching and Neighbour embedding are applied together, our GN-GAN\nachieves outstanding results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets,\ne.g. FID score of $30.80$ for the STL-10 dataset. Our code is available at:\nhttps://github.com/tntrung/gan\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 08:06:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Bui", "Tuan-Anh", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1811.01335", "submitter": "Zechun Liu", "authors": "Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, Kwang-Ting\n  Cheng", "title": "Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance", "comments": "To appear in IJCV. It is the journal extension of our ECCV paper:\n  arXiv:1808.00278. Codes are available on:\n  https://github.com/liuzechun/Bi-Real-net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study 1-bit convolutional neural networks (CNNs), of which\nboth the weights and activations are binary. While efficient, the lacking of\nrepresentational capability and the training difficulty impede 1-bit CNNs from\nperforming as well as real-valued networks. We propose Bi-Real net with a novel\ntraining algorithm to tackle these two challenges. To enhance the\nrepresentational capability, we propagate the real-valued activations generated\nby each 1-bit convolution via a parameter-free shortcut. To address the\ntraining difficulty, we propose a training algorithm using a tighter\napproximation to the derivative of the sign function, a magnitude-aware\ngradient for weight updating, a better initialization method, and a two-step\nscheme for training a deep network. Experiments on ImageNet show that an\n18-layer Bi-Real net with the proposed training algorithm achieves 56.4% top-1\nclassification accuracy, which is 10% higher than the state-of-the-arts (e.g.,\nXNOR-Net) with greater memory saving and lower computational cost. Bi-Real net\nis also the first to scale up 1-bit CNNs to an ultra-deep network with 152\nlayers, and achieves 64.5% top-1 accuracy on ImageNet. A 50-layer Bi-Real net\nshows comparable performance to a real-valued network on the depth estimation\ntask with only a 0.3% accuracy gap.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 08:15:26 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 03:23:31 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Liu", "Zechun", ""], ["Luo", "Wenhan", ""], ["Wu", "Baoyuan", ""], ["Yang", "Xin", ""], ["Liu", "Wei", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "1811.01343", "submitter": "Dana Berman", "authors": "Dana Berman, Deborah Levy, Shai Avidan, Tali Treibitz", "title": "Underwater Single Image Color Restoration Using Haze-Lines and a New\n  Quantitative Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images suffer from color distortion and low contrast, because\nlight is attenuated while it propagates through water. Attenuation under water\nvaries with wavelength, unlike terrestrial images where attenuation is assumed\nto be spectrally uniform. The attenuation depends both on the water body and\nthe 3D structure of the scene, making color restoration difficult.\n  Unlike existing single underwater image enhancement techniques, our method\ntakes into account multiple spectral profiles of different water types. By\nestimating just two additional global parameters: the attenuation ratios of the\nblue-red and blue-green color channels, the problem is reduced to single image\ndehazing, where all color channels have the same attenuation coefficients.\nSince the water type is unknown, we evaluate different parameters out of an\nexisting library of water types. Each type leads to a different restored image\nand the best result is automatically chosen based on color distribution.\n  We collected a dataset of images taken in different locations with varying\nwater properties, showing color charts in the scenes. Moreover, to obtain\nground truth, the 3D structure of the scene was calculated based on stereo\nimaging. This dataset enables a quantitative evaluation of restoration\nalgorithms on natural images and shows the advantage of our method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 09:16:38 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 17:19:03 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 20:38:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Berman", "Dana", ""], ["Levy", "Deborah", ""], ["Avidan", "Shai", ""], ["Treibitz", "Tali", ""]]}, {"id": "1811.01395", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Ankan Kumar Bhunia, Shuvozit Ghose, Abhirup Das,\n  Partha Pratim Roy, Umapada Pal", "title": "A Deep One-Shot Network for Query-based Logo Retrieval", "comments": "Accepted in Pattern Recognition, Elsevier(2019)", "journal-ref": null, "doi": "10.1016/j.patcog.2019.106965", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo detection in real-world scene images is an important problem with\napplications in advertisement and marketing. Existing general-purpose object\ndetection methods require large training data with annotations for every logo\nclass. These methods do not satisfy the incremental demand of logo classes\nnecessary for practical deployment since it is practically impossible to have\nsuch annotated data for new unseen logo. In this work, we develop an\neasy-to-implement query-based logo detection and localization system by\nemploying a one-shot learning technique. Given an image of a query logo, our\nmodel searches for it within a given target image and predicts the possible\nlocation of the logo by estimating a binary segmentation mask. The proposed\nmodel consists of a conditional branch and a segmentation branch. The former\ngives a conditional latent representation of the given query logo which is\ncombined with feature maps of the segmentation branch at multiple scales in\norder to find the matching position of the query logo in a target image, should\nit be present. Feature matching between the latent query representation and\nmulti-scale feature maps of segmentation branch using simple concatenation\noperation followed by 1x1 convolution layer makes our model scale-invariant.\nDespite its simplicity, our query-based logo retrieval framework achieved\nsuperior performance in FlickrLogos-32 and TopLogos-10 dataset over different\nexisting baselines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 16:16:45 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 10:54:23 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2019 22:01:26 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 18:43:33 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 18:55:59 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Bhunia", "Ankan Kumar", ""], ["Ghose", "Shuvozit", ""], ["Das", "Abhirup", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1811.01396", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Abhirup Das, Ankan Kumar Bhunia, Perla Sai Raj\n  Kishore, Partha Pratim Roy", "title": "Handwriting Recognition in Low-resource Scripts using Adversarial\n  Learning", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten Word Recognition and Spotting is a challenging field dealing with\nhandwritten text possessing irregular and complex shapes. The design of deep\nneural network models makes it necessary to extend training datasets in order\nto introduce variations and increase the number of samples; word-retrieval is\ntherefore very difficult in low-resource scripts. Much of the existing\nliterature comprises preprocessing strategies which are seldom sufficient to\ncover all possible variations. We propose the Adversarial Feature Deformation\nModule (AFDM) that learns ways to elastically warp extracted features in a\nscalable manner. The AFDM is inserted between intermediate layers and trained\nalternatively with the original framework, boosting its capability to better\nlearn highly informative features rather than trivial ones. We test our\nmeta-framework, which is built on top of popular word-spotting and\nword-recognition frameworks and enhanced by the AFDM, not only on extensive\nLatin word datasets but also sparser Indic scripts. We record results for\nvarying training data sizes, and observe that our enhanced network generalizes\nmuch better in the low-data regime; the overall word-error rates and mAP scores\nare observed to improve as well.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 16:24:09 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 09:14:28 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 03:26:39 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 22:58:14 GMT"}, {"version": "v5", "created": "Mon, 25 Feb 2019 10:40:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Das", "Abhirup", ""], ["Bhunia", "Ankan Kumar", ""], ["Kishore", "Perla Sai Raj", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1811.01401", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Perla Sai Raj Kishore, Pranay Mukherjee, Abhirup\n  Das, Partha Pratim Roy", "title": "Texture Synthesis Guided Deep Hashing for Texture Image Retrieval", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2019 Video Presentation: https://www.youtube.com/watch?v=tXaXTGhzaJo", "journal-ref": null, "doi": "10.1109/WACV.2019.00070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large-scale explosion of images and videos over the internet,\nefficient hashing methods have been developed to facilitate memory and time\nefficient retrieval of similar images. However, none of the existing works uses\nhashing to address texture image retrieval mostly because of the lack of\nsufficiently large texture image databases. Our work addresses this problem by\ndeveloping a novel deep learning architecture that generates binary hash codes\nfor input texture images. For this, we first pre-train a Texture Synthesis\nNetwork (TSN) which takes a texture patch as input and outputs an enlarged view\nof the texture by injecting newer texture content. Thus it signifies that the\nTSN encodes the learnt texture specific information in its intermediate layers.\nIn the next stage, a second network gathers the multi-scale feature\nrepresentations from the TSN's intermediate layers using channel-wise\nattention, combines them in a progressive manner to a dense continuous\nrepresentation which is finally converted into a binary hash code with the help\nof individual and pairwise label information. The new enlarged texture patches\nalso help in data augmentation to alleviate the problem of insufficient texture\ndata and are used to train the second stage of the network. Experiments on\nthree public texture image retrieval datasets indicate the superiority of our\ntexture synthesis guided hashing approach over current state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 16:41:47 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 05:31:18 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 01:20:38 GMT"}, {"version": "v4", "created": "Sat, 20 Apr 2019 17:43:57 GMT"}, {"version": "v5", "created": "Wed, 5 Jun 2019 13:45:14 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Kishore", "Perla Sai Raj", ""], ["Mukherjee", "Pranay", ""], ["Das", "Abhirup", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1811.01405", "submitter": "Rory Smith", "authors": "Rory Smith, Tilo Burghardt", "title": "DeepKey: Towards End-to-End Physical Key Replication From a Single\n  Photograph", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes DeepKey, an end-to-end deep neural architecture capable\nof taking a digital RGB image of an 'everyday' scene containing a pin tumbler\nkey (e.g. lying on a table or carpet) and fully automatically inferring a\nprintable 3D key model. We report on the key detection performance and describe\nhow candidates can be transformed into physical prints. We show an example\nopening a real-world lock. Our system is described in detail, providing a\nbreakdown of all components including key detection, pose normalisation,\nbitting segmentation and 3D model inference. We provide an in-depth evaluation\nand conclude by reflecting on limitations, applications, potential security\nrisks and societal impact. We contribute the DeepKey Datasets of 5, 300+ images\ncovering a few test keys with bounding boxes, pose and unaligned mask data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 17:25:23 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Smith", "Rory", ""], ["Burghardt", "Tilo", ""]]}, {"id": "1811.01424", "submitter": "Gorkem Polat", "authors": "Gorkem Polat, Ugur Halici, Yesim Serinagaoglu Dogrusoz", "title": "False Positive Reduction in Lung Computed Tomography Images using\n  Convolutional Neural Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. In this study, we propose a novel and simple framework\nthat analyzes CT lung screenings using convolutional neural networks (CNNs) and\nreduces false positives. Our framework shows that even non-complex\narchitectures are very powerful to classify 3D nodule data when compared to\ntraditional methods. We also use different fusions in order to show their power\nand effect on the overall score. 3D CNNs are preferred over 2D CNNs because\ndata are in 3D, and 2D convolutional operations may result in information loss.\nMini-batch is used in order to overcome class-imbalance. Proposed framework has\nbeen validated according to the LUNA16 challenge evaluation and got score of\n0.786, which is the average sensitivity values at seven predefined false\npositive (FP) points.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 19:46:48 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Polat", "Gorkem", ""], ["Halici", "Ugur", ""], ["Dogrusoz", "Yesim Serinagaoglu", ""]]}, {"id": "1811.01474", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, Walter J. Scheirer, Kevin W. Bowyer, Patrick J.\n  Flynn", "title": "Fast Face Image Synthesis with Minimal Training", "comments": "To appear in IEEE WACV 2019. Get our data (2M face images of 12K\n  synthetic subjects, 8K 3D head models) by accessing the \"Notre Dame Synthetic\n  Face Dataset\" here: https://cvrl.nd.edu/projects/data/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to generate realistic face images of both real and\nsynthetic identities (people who do not exist) with different facial yaw, shape\nand resolution.The synthesized images can be used to augment datasets to train\nCNNs or as massive distractor sets for biometric verification experiments\nwithout any privacy concerns. Additionally, law enforcement can make use of\nthis technique to train forensic experts to recognize faces. Our method samples\nface components from a pool of multiple face images of real identities to\ngenerate the synthetic texture. Then, a real 3D head model compatible to the\ngenerated texture is used to render it under different facial yaw\ntransformations. We perform multiple quantitative experiments to assess the\neffectiveness of our synthesis procedure in CNN training and its potential use\nto generate distractor face images. Additionally, we compare our method with\npopular GAN models in terms of visual quality and execution time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:28:53 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 13:53:11 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 20:29:36 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Scheirer", "Walter J.", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1811.01476", "submitter": "Alexander Wong", "authors": "Mohammad Saeed Shafiee, Mohammad Javad Shafiee, and Alexander Wong", "title": "Dynamic Representations Toward Efficient Inference on Deep Neural\n  Networks by Decision Gates", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks extract rich features from the input data, the\ncurrent trade-off between depth and computational cost makes it difficult to\nadopt deep neural networks for many industrial applications, especially when\ncomputing power is limited. Here, we are inspired by the idea that, while\ndeeper embeddings are needed to discriminate difficult samples (i.e.,\nfine-grained discrimination), a large number of samples can be well\ndiscriminated via much shallower embeddings (i.e., coarse-grained\ndiscrimination). In this study, we introduce the simple yet effective concept\nof decision gates (d-gate), modules trained to decide whether a sample needs to\nbe projected into a deeper embedding or if an early prediction can be made at\nthe d-gate, thus enabling the computation of dynamic representations at\ndifferent depths. The proposed d-gate modules can be integrated with any deep\nneural network and reduces the average computational cost of the deep neural\nnetworks while maintaining modeling accuracy. The proposed d-gate framework is\nexamined via different network architectures and datasets, with experimental\nresults showing that leveraging the proposed d-gate modules led to a ~43%\nspeed-up and 44% FLOPs reduction on ResNet-101 and 55% speed-up and 39% FLOPs\nreduction on DenseNet-201 trained on the CIFAR10 dataset with only ~2% drop in\naccuracy. Furthermore, experiments where d-gate modules are integrated into\nResNet-101 trained on the ImageNet dataset demonstrate that it is possible to\nreduce the computational cost of the network by 1.5 GFLOPs without any drop in\nthe modeling accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:37:39 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 02:55:19 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 02:47:56 GMT"}, {"version": "v4", "created": "Sat, 11 May 2019 16:35:49 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Shafiee", "Mohammad Saeed", ""], ["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1811.01494", "submitter": "Wenwei Xu", "authors": "Wenwei Xu, Shari Matzner", "title": "Underwater Fish Detection using Deep Learning for Water Power\n  Applications", "comments": "Accepted at CSCI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clean energy from oceans and rivers is becoming a reality with the\ndevelopment of new technologies like tidal and instream turbines that generate\nelectricity from naturally flowing water. These new technologies are being\nmonitored for effects on fish and other wildlife using underwater video.\nMethods for automated analysis of underwater video are needed to lower the\ncosts of analysis and improve accuracy. A deep learning model, YOLO, was\ntrained to recognize fish in underwater video using three very different\ndatasets recorded at real-world water power sites. Training and testing with\nexamples from all three datasets resulted in a mean average precision (mAP)\nscore of 0.5392. To test how well a model could generalize to new datasets, the\nmodel was trained using examples from only two of the datasets and then tested\non examples from all three datasets. The resulting model could not recognize\nfish in the dataset that was not part of the training set. The mAP scores on\nthe other two datasets that were included in the training set were higher than\nthe scores achieved by the model trained on all three datasets. These results\nindicate that different methods are needed in order to produce a trained model\nthat can generalize to new data sets such as those encountered in real world\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 02:58:09 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Xu", "Wenwei", ""], ["Matzner", "Shari", ""]]}, {"id": "1811.01504", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Deep Multiple Description Coding by Learning Scalar Quantization", "comments": "8 pages, 4 figures. (DCC 2019: Data Compression Conference). Testing\n  datasets for \"Deep Optimized Multiple Description Image Coding via Scalar\n  Quantization Learning\" can be found in the website of\n  https://github.com/mdcnn/Deep-Multiple-Description-Coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep multiple description coding framework, whose\nquantizers are adaptively learned via the minimization of multiple description\ncompressive loss. Firstly, our framework is built upon auto-encoder networks,\nwhich have multiple description multi-scale dilated encoder network and\nmultiple description decoder networks. Secondly, two entropy estimation\nnetworks are learned to estimate the informative amounts of the quantized\ntensors, which can further supervise the learning of multiple description\nencoder network to represent the input image delicately. Thirdly, a pair of\nscalar quantizers accompanied by two importance-indicator maps is automatically\nlearned in an end-to-end self-supervised way. Finally, multiple description\nstructural dissimilarity distance loss is imposed on multiple description\ndecoded images in pixel domain for diversified multiple description generations\nrather than on feature tensors in feature domain, in addition to multiple\ndescription reconstruction loss. Through testing on two commonly used datasets,\nit is verified that our method is beyond several state-of-the-art multiple\ndescription coding approaches in terms of coding efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 03:49:23 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 09:20:33 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 09:25:48 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1811.01526", "submitter": "Maryam Sultana", "authors": "Maryam Sultana, Arif Mahmood, Sajid Javed and Soon Ki Jung", "title": "Unsupervised RGBD Video Object Segmentation Using GANs", "comments": "15 pages, 3 figures, ACCV workshop on RGB-D-sensing and understanding\n  via combined colour and depth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object segmentation is a fundamental step in many advanced vision\napplications. Most existing algorithms are based on handcrafted features such\nas HOG, super-pixel segmentation or texture-based techniques, while recently\ndeep features have been found to be more efficient. Existing algorithms observe\nperformance degradation in the presence of challenges such as illumination\nvariations, shadows, and color camouflage. To handle these challenges we\npropose a fusion based moving object segmentation algorithm which exploits\ncolor as well as depth information using GAN to achieve more accuracy. Our goal\nis to segment moving objects in the presence of challenging background scenes,\nin real environments. To address this problem, GAN is trained in an\nunsupervised manner on color and depth information independently with\nchallenging video sequences. During testing, the trained GAN generates\nbackgrounds similar to that in the test sample. The generated background\nsamples are then compared with the test sample to segment moving objects. The\nfinal result is computed by fusion of object boundaries in both modalities, RGB\nand the depth. The comparison of our proposed algorithm with five\nstate-of-the-art methods on publicly available dataset has shown the strength\nof our algorithm for moving object segmentation in videos in the presence of\nchallenging real scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 06:11:24 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sultana", "Maryam", ""], ["Mahmood", "Arif", ""], ["Javed", "Sajid", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1811.01534", "submitter": "R\\\"udiger G\\\"obl", "authors": "R\\\"udiger G\\\"obl, Diana Mateus, Christoph Hennersperger, Maximilian\n  Baust, Nassir Navab", "title": "Redefining Ultrasound Compounding: Computational Sonography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand three-dimensional ultrasound (3D-US) has gained considerable\ninterest in research, but even today suffers from its high inter-operator\nvariability in clinical practice. The high variability mainly arises from\ntracking inaccuracies as well as the directionality of the ultrasound data,\nbeing neglected in most of today's reconstruction methods. By providing a novel\nparadigm for the acquisition and reconstruction of tracked freehand 3D\nultrasound, this work presents the concept of Computational Sonography (CS) to\nmodel the directionality of ultrasound information. CS preserves the\ndirectionality of the acquired data, and allows for its exploitation by\ncomputational algorithms. In this regard, we propose a set of mathematical\nmodels to represent 3D-US data, inspired by the physics of ultrasound imaging.\nWe compare different models of Computational Sonography to classical scalar\ncompounding for freehand acquisitions, providing both an improved preservation\nof US directionality as well as improved image quality in 3D. The novel concept\nis evaluated for a set of phantom datasets, as well as for in-vivo acquisitions\nof muscoloskeletal and vascular applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:07:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["G\u00f6bl", "R\u00fcdiger", ""], ["Mateus", "Diana", ""], ["Hennersperger", "Christoph", ""], ["Baust", "Maximilian", ""], ["Navab", "Nassir", ""]]}, {"id": "1811.01549", "submitter": "Dongliang He", "authors": "Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li,\n  Limin Wang, Shilei Wen", "title": "StNet: Local and Global Spatial-Temporal Modeling for Action Recognition", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning for static image understanding, it\nremains unclear what are the most effective network architectures for the\nspatial-temporal modeling in videos. In this paper, in contrast to the existing\nCNN+RNN or pure 3D convolution based approaches, we explore a novel spatial\ntemporal network (StNet) architecture for both local and global\nspatial-temporal modeling in videos. Particularly, StNet stacks N successive\nvideo frames into a \\emph{super-image} which has 3N channels and applies 2D\nconvolution on super-images to capture local spatial-temporal relationship. To\nmodel global spatial-temporal relationship, we apply temporal convolution on\nthe local spatial-temporal feature maps. Specifically, a novel temporal\nXception block is proposed in StNet. It employs a separate channel-wise and\ntemporal-wise convolution over the feature sequence of video. Extensive\nexperiments on the Kinetics dataset demonstrate that our framework outperforms\nseveral state-of-the-art approaches in action recognition and can strike a\nsatisfying trade-off between recognition accuracy and model complexity. We\nfurther demonstrate the generalization performance of the leaned video\nrepresentations on the UCF101 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 08:30:49 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 06:36:30 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 05:27:39 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["He", "Dongliang", ""], ["Zhou", "Zhichao", ""], ["Gan", "Chuang", ""], ["Li", "Fu", ""], ["Liu", "Xiao", ""], ["Li", "Yandong", ""], ["Wang", "Limin", ""], ["Wen", "Shilei", ""]]}, {"id": "1811.01567", "submitter": "Naiyan Wang", "authors": "Xinbang Zhang, Zehao Huang, Naiyan Wang", "title": "You Only Search Once: Single Shot Neural Architecture Search via Direct\n  Sparse Optimization", "comments": "ICLR2019 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Neural Architecture Search (NAS) has aroused great interest in both\nacademia and industry, however it remains challenging because of its huge and\nnon-continuous search space. Instead of applying evolutionary algorithm or\nreinforcement learning as previous works, this paper proposes a Direct Sparse\nOptimization NAS (DSO-NAS) method. In DSO-NAS, we provide a novel model pruning\nview to NAS problem. In specific, we start from a completely connected block,\nand then introduce scaling factors to scale the information flow between\noperations. Next, we impose sparse regularizations to prune useless connections\nin the architecture. Lastly, we derive an efficient and theoretically sound\noptimization method to solve it. Our method enjoys both advantages of\ndifferentiability and efficiency, therefore can be directly applied to large\ndatasets like ImageNet. Particularly, On CIFAR-10 dataset, DSO-NAS achieves an\naverage test error 2.84\\%, while on the ImageNet dataset DSO-NAS achieves\n25.4\\% test error under 600M FLOPs with 8 GPUs in 18 hours.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:28:13 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Xinbang", ""], ["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1811.01571", "submitter": "Mohsen Yavartanoo", "authors": "Mohsen Yavartanoo, Eu Young Kim, Kyoung Mu Lee", "title": "SPNet: Deep 3D Object Classification and Retrieval using Stereographic\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient Stereographic Projection Neural Network (SPNet) for\nlearning representations of 3D objects. We first transform a 3D input volume\ninto a 2D planar image using stereographic projection. We then present a\nshallow 2D convolutional neural network (CNN) to estimate the object category\nfollowed by view ensemble, which combines the responses from multiple views of\nthe object to further enhance the predictions. Specifically, the proposed\napproach consists of four stages: (1) Stereographic projection of a 3D object,\n(2) view-specific feature learning, (3) view selection and (4) view ensemble.\nThe proposed approach performs comparably to the state-of-the-art methods while\nhaving substantially lower GPU memory as well as network parameters. Despite\nits lightness, the experiments on 3D object classification and shape retrievals\ndemonstrate the high performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:43:15 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 04:03:19 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Yavartanoo", "Mohsen", ""], ["Kim", "Eu Young", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1811.01592", "submitter": "Ting Sun", "authors": "Ting Sun, Dezhen Song, Dit-Yan Yeung, Ming Liu", "title": "Semi-Semantic Line-Cluster Assisted Monocular SLAM for Indoor\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to reduce the scale drift for indoor\nmonocular simultaneous localization and mapping (SLAM). We leverage the prior\nknowledge that in the indoor environment, the line segments form tight\nclusters, e.g. many door frames in a straight corridor are of the same shape,\nsize and orientation, so the same edges of these door frames form a tight line\nsegment cluster. We implement our method in the popular ORB-SLAM2, which also\nserves as our baseline. In the front end we detect the line segments in each\nframe and incrementally cluster them in the 3D space. In the back end, we\noptimize the map imposing the constraint that the line segments of the same\ncluster should be the same. Experimental results show that our proposed method\nsuccessfully reduces the scale drift for indoor monocular SLAM.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:31:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sun", "Ting", ""], ["Song", "Dezhen", ""], ["Yeung", "Dit-Yan", ""], ["Liu", "Ming", ""]]}, {"id": "1811.01602", "submitter": "Michal Neoral", "authors": "Michal Neoral, Jan \\v{S}ochman and Ji\\v{r}\\'i Matas", "title": "Continual Occlusions and Optical Flow Estimation", "comments": "ACCV2018, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two optical flow estimation problems are addressed: i) occlusion estimation\nand handling, and ii) estimation from image sequences longer than two frames.\nThe proposed ContinualFlow method estimates occlusions before flow, avoiding\nthe use of flow corrupted by occlusions for their estimation. We show that\nproviding occlusion masks as an additional input to flow estimation improves\nthe standard performance metric by more than 25\\% on both KITTI and Sintel. As\na second contribution, a novel method for incorporating information from past\nframes into flow estimation is introduced. The previous frame flow serves as an\ninput to occlusion estimation and as a prior in occluded regions, i.e. those\nwithout visual correspondences. By continually using the previous frame flow,\nContinualFlow performance improves further by 18\\% on KITTI and 7\\% on Sintel,\nachieving top performance on KITTI and Sintel.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:46:27 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Neoral", "Michal", ""], ["\u0160ochman", "Jan", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1811.01627", "submitter": "Rateb Jabbar Mr.", "authors": "Rateb Jabbar, Khalifa Al-Khalifa, Mohamed Kharbeche, Wael Alhajyaseen,\n  Mohsen Jafari, Shan Jiang", "title": "Real-time Driver Drowsiness Detection for Android Application Using Deep\n  Neural Networks Techniques", "comments": "Conference, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road crashes and related forms of accidents are a common cause of injury and\ndeath among the human population. According to 2015 data from the World Health\nOrganization, road traffic injuries resulted in approximately 1.25 million\ndeaths worldwide, i.e. approximately every 25 seconds an individual will\nexperience a fatal crash. While the cost of traffic accidents in Europe is\nestimated at around 160 billion Euros, driver drowsiness accounts for\napproximately 100,000 accidents per year in the United States alone as reported\nby The American National Highway Traffic Safety Administration (NHTSA). In this\npaper, a novel approach towards real-time drowsiness detection is proposed.\nThis approach is based on a deep learning method that can be implemented on\nAndroid applications with high accuracy. The main contribution of this work is\nthe compression of heavy baseline model to a lightweight model. Moreover,\nminimal network structure is designed based on facial landmark key point\ndetection to recognize whether the driver is drowsy. The proposed model is able\nto achieve an accuracy of more than 80%. Keywords: Driver Monitoring System;\nDrowsiness Detection; Deep Learning; Real-time Deep Neural Network; Android.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 11:49:26 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jabbar", "Rateb", ""], ["Al-Khalifa", "Khalifa", ""], ["Kharbeche", "Mohamed", ""], ["Alhajyaseen", "Wael", ""], ["Jafari", "Mohsen", ""], ["Jiang", "Shan", ""]]}, {"id": "1811.01749", "submitter": "Bertrand Girard", "authors": "David Vigouroux, Sylvain Picard", "title": "FUNN: Flexible Unsupervised Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated high accuracy in image classification\ntasks. However, they were shown to be weak against adversarial examples: a\nsmall perturbation in the image which changes the classification output\ndramatically. In recent years, several defenses have been proposed to solve\nthis issue in supervised classification tasks. We propose a method to obtain\nrobust features in unsupervised learning tasks against adversarial attacks. Our\nmethod differs from existing solutions by directly learning the robust features\nwithout the need to project the adversarial examples in the original examples\ndistribution space. A first auto-encoder A1 is in charge of perturbing the\ninput image to fool another auto-encoder A2 which is in charge of regenerating\nthe original image. A1 tries to find the less perturbed image under the\nconstraint that the error in the output of A2 should be at least equal to a\nthreshold. Thanks to this training, the encoder of A2 will be robust against\nadversarial attacks and could be used in different tasks like classification.\nUsing state-of-art network architectures, we demonstrate the robustness of the\nfeatures obtained thanks to this method in classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:42:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Vigouroux", "David", ""], ["Picard", "Sylvain", ""]]}, {"id": "1811.01791", "submitter": "Abdelrahman Eldesokey", "authors": "Abdelrahman Eldesokey, Michael Felsberg and Fahad Shahbaz Khan", "title": "Confidence Propagation through CNNs for Guided Sparse Depth Regression", "comments": "14 pages, 14 Figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2019", "doi": "10.1109/TPAMI.2019.2929170", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, convolutional neural networks (CNNs) process data on a regular\ngrid, e.g. data generated by ordinary cameras. Designing CNNs for sparse and\nirregularly spaced input data is still an open research problem with numerous\napplications in autonomous driving, robotics, and surveillance. In this paper,\nwe propose an algebraically-constrained normalized convolution layer for CNNs\nwith highly sparse input that has a smaller number of network parameters\ncompared to related work. We propose novel strategies for determining the\nconfidence from the convolution operation and propagating it to consecutive\nlayers. We also propose an objective function that simultaneously minimizes the\ndata error while maximizing the output confidence. To integrate structural\ninformation, we also investigate fusion strategies to combine depth and RGB\ninformation in our normalized convolution network framework. In addition, we\nintroduce the use of output confidence as an auxiliary information to improve\nthe results. The capabilities of our normalized convolution network framework\nare demonstrated for the problem of scene depth completion. Comprehensive\nexperiments are performed on the KITTI-Depth and the NYU-Depth-v2 datasets. The\nresults clearly demonstrate that the proposed approach achieves superior\nperformance while requiring only about 1-5% of the number of parameters\ncompared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:28:02 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 08:11:00 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Eldesokey", "Abdelrahman", ""], ["Felsberg", "Michael", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "1811.01907", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Jiaming Xie, Yun\n  Liang, Sijia Liu, Xue Lin and Yanzhi Wang", "title": "A Unified Framework of DNN Weight Pruning and Weight\n  Clustering/Quantization Using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model compression techniques of Deep Neural Networks (DNNs) have been\ninvestigated, including weight pruning, weight clustering and quantization,\netc. Weight pruning leverages the redundancy in the number of weights in DNNs,\nwhile weight clustering/quantization leverages the redundancy in the number of\nbit representations of weights. They can be effectively combined in order to\nexploit the maximum degree of redundancy. However, there lacks a systematic\ninvestigation in literature towards this direction.\n  In this paper, we fill this void and develop a unified, systematic framework\nof DNN weight pruning and clustering/quantization using Alternating Direction\nMethod of Multipliers (ADMM), a powerful technique in optimization theory to\ndeal with non-convex optimization problems. Both DNN weight pruning and\nclustering/quantization, as well as their combinations, can be solved in a\nunified manner. For further performance improvement in this framework, we adopt\nmultiple techniques including iterative weight quantization and retraining,\njoint weight clustering training and centroid updating, weight clustering\nretraining, etc. The proposed framework achieves significant improvements both\nin individual weight pruning and clustering/quantization problems, as well as\ntheir combinations. For weight pruning alone, we achieve 167x weight reduction\nin LeNet-5, 24.7x in AlexNet, and 23.4x in VGGNet, without any accuracy loss.\nFor the combination of DNN weight pruning and clustering/quantization, we\nachieve 1,910x and 210x storage reduction of weight data on LeNet-5 and\nAlexNet, respectively, without accuracy loss. Our codes and models are released\nat the link http://bit.ly/2D3F0np\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:34:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ye", "Shaokai", ""], ["Zhang", "Tianyun", ""], ["Zhang", "Kaiqi", ""], ["Li", "Jiayu", ""], ["Xie", "Jiaming", ""], ["Liang", "Yun", ""], ["Liu", "Sijia", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1811.01984", "submitter": "Fotios Logothetis Mr", "authors": "Fotios Logothetis and Roberto Mecca and Roberto Cipolla", "title": "A Differential Volumetric Approach to Multi-View Photometric Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly accurate 3D volumetric reconstruction is still an open research topic\nwhere the main difficulty is usually related to merging some rough estimations\nwith high frequency details. One of the most promising methods is the fusion\nbetween multi-view stereo and photometric stereo images. Beside the intrinsic\ndifficulties that multi-view stereo and photometric stereo in order to work\nreliably, supplementary problems arise when considered together.\n  In this work, we present a volumetric approach to the multi-view photometric\nstereo problem. The key point of our method is the signed distance field\nparameterisation and its relation to the surface normal. This is exploited in\norder to obtain a linear partial differential equation which is solved in a\nvariational framework, that combines multiple images from multiple points of\nview in a single system. In addition, the volumetric approach is naturally\nimplemented on an octree, which allows for fast ray-tracing that reliably\nalleviates occlusions and cast shadows.\n  Our approach is evaluated on synthetic and real data-sets and achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:11:17 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 09:47:10 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Logothetis", "Fotios", ""], ["Mecca", "Roberto", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1811.02017", "submitter": "Taco Cohen", "authors": "Taco Cohen, Mario Geiger, Maurice Weiler", "title": "A General Theory of Equivariant CNNs on Homogeneous Spaces", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019) 9142-9153", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory of Group equivariant Convolutional Neural\nNetworks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere.\nFeature maps in these networks represent fields on a homogeneous base space,\nand layers are equivariant maps between spaces of fields. The theory enables a\nsystematic classification of all existing G-CNNs in terms of their symmetry\ngroup, base space, and field type. We also consider a fundamental question:\nwhat is the most general kind of equivariant linear map between feature spaces\n(fields) of given types? Following Mackey, we show that such maps correspond\none-to-one with convolutions using equivariant kernels, and characterize the\nspace of such kernels.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:22:10 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 14:59:52 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Cohen", "Taco", ""], ["Geiger", "Mario", ""], ["Weiler", "Maurice", ""]]}, {"id": "1811.02046", "submitter": "Yilei Shi", "authors": "Yilei Shi, Xiao Xiang Zhu, Richard Bamler", "title": "Non-Local Compressive Sensing Based SAR Tomography", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TGRS.2018.2879382", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tomographic SAR (TomoSAR) inversion of urban areas is an inherently sparse\nreconstruction problem and, hence, can be solved using compressive sensing (CS)\nalgorithms. This paper proposes solutions for two notorious problems in this\nfield: 1) TomoSAR requires a high number of data sets, which makes the\ntechnique expensive. However, it can be shown that the number of acquisitions\nand the signal-to-noise ratio (SNR) can be traded off against each other,\nbecause it is asymptotically only the product of the number of acquisitions and\nSNR that determines the reconstruction quality. We propose to increase SNR by\nintegrating non-local estimation into the inversion and show that a reasonable\nreconstruction of buildings from only seven interferograms is feasible. 2)\nCS-based inversion is computationally expensive and therefore barely suitable\nfor large-scale applications. We introduce a new fast and accurate algorithm\nfor solving the non-local L1-L2-minimization problem, central to CS-based\nreconstruction algorithms. The applicability of the algorithm is demonstrated\nusing simulated data and TerraSAR-X high-resolution spotlight images over an\narea in Munich, Germany.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:46:56 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Shi", "Yilei", ""], ["Zhu", "Xiao Xiang", ""], ["Bamler", "Richard", ""]]}, {"id": "1811.02074", "submitter": "Zhun Zhong", "authors": "Fengxiang Yang, Zhun Zhong, Zhiming Luo, Sheng Lian, Shaozi Li", "title": "Leveraging Virtual and Real Person for Unsupervised Person\n  Re-identification", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is a challenging problem especially when no\nlabels are available for training. Although recent deep re-ID methods have\nachieved great improvement, it is still difficult to optimize deep re-ID model\nwithout annotations in training data. To address this problem, this study\nintroduces a novel approach for unsupervised person re-ID by leveraging virtual\nand real data. Our approach includes two components: virtual person generation\nand training of deep re-ID model. For virtual person generation, we learn a\nperson generation model and a camera style transfer model using unlabeled real\ndata to generate virtual persons with different poses and camera styles. The\nvirtual data is formed as labeled training data, enabling subsequently training\ndeep re-ID model in supervision. For training of deep re-ID model, we divide it\ninto three steps: 1) pre-training a coarse re-ID model by using virtual data;\n2) collaborative filtering based positive pair mining from the real data; and\n3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs\nand virtual data. The final re-ID model is achieved by iterating between step 2\nand step 3 until convergence. Experimental results on two large-scale datasets,\nMarket-1501 and DukeMTMC-reID, demonstrate the effectiveness of our approach\nand shows that the state of the art is achieved in unsupervised person re-ID.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:00:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yang", "Fengxiang", ""], ["Zhong", "Zhun", ""], ["Luo", "Zhiming", ""], ["Lian", "Sheng", ""], ["Li", "Shaozi", ""]]}, {"id": "1811.02090", "submitter": "Ahmed Mostayed", "authors": "Ahmed Mostayed, Junye Luo, Xingliang Shu, and William Wee", "title": "Classification of 12-Lead ECG Signals with Bi-directional LSTM Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a recurrent neural network classifier to detect pathologies in\n12-lead ECG signals and train and validate the classifier with the Chinese\nphysiological signal challenge dataset (http://www.icbeb.org/Challenge.html).\nThe recurrent neural network consists of two bi-directional LSTM layers and can\ntrain on arbitrary-length ECG signals. Our best trained model achieved an\naverage F1 score of 74.15% on the validation set.\n  Keywords: ECG classification, Deep learning, RNN, Bi-directional LSTM, QRS\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:53:20 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Mostayed", "Ahmed", ""], ["Luo", "Junye", ""], ["Shu", "Xingliang", ""], ["Wee", "William", ""]]}, {"id": "1811.02146", "submitter": "Yuexin Ma", "authors": "Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, Dinesh\n  Manocha", "title": "TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents", "comments": "Accepted by AAAI(Oral) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To safely and efficiently navigate in complex urban traffic, autonomous\nvehicles must make responsible predictions in relation to surrounding\ntraffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and\ncritical task is to explore the movement patterns of different traffic-agents\nand predict their future trajectories accurately to help the autonomous vehicle\nmake reasonable navigation decision. To solve this problem, we propose a long\nshort-term memory-based (LSTM-based) realtime traffic prediction algorithm,\nTrafficPredict. Our approach uses an instance layer to learn instances'\nmovements and interactions and has a category layer to learn the similarities\nof instances belonging to the same type to refine the prediction. In order to\nevaluate its performance, we collected trajectory datasets in a large city\nconsisting of varying conditions and traffic densities. The dataset includes\nmany challenging scenarios where vehicles, bicycles, and pedestrians move among\none another. We evaluate the performance of TrafficPredict on our new dataset\nand highlight its higher accuracy for trajectory prediction by comparing with\nprior prediction methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:34:20 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 03:26:14 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 05:22:40 GMT"}, {"version": "v4", "created": "Mon, 17 Dec 2018 06:44:19 GMT"}, {"version": "v5", "created": "Tue, 9 Apr 2019 07:08:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ma", "Yuexin", ""], ["Zhu", "Xinge", ""], ["Zhang", "Sibo", ""], ["Yang", "Ruigang", ""], ["Wang", "Wenping", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1811.02189", "submitter": "Weijie Kong", "authors": "Weijie Kong, Nannan Li, Shan Liu, Thomas Li, Ge Li", "title": "BLP -- Boundary Likelihood Pinpointing Networks for Accurate Temporal\n  Action Localization", "comments": "Accepted to International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous progress achieved in temporal action detection,\nstate-of-the-art methods still suffer from the sharp performance deterioration\nwhen localizing the starting and ending temporal action boundaries. Although\nmost methods apply boundary regression paradigm to tackle this problem, we\nargue that the direct regression lacks detailed enough information to yield\naccurate temporal boundaries. In this paper, we propose a novel Boundary\nLikelihood Pinpointing (BLP) network to alleviate this deficiency of boundary\nregression and improve the localization accuracy. Given a loosely localized\nsearch interval that contains an action instance, BLP casts the problem of\nlocalizing temporal boundaries as that of assigning probabilities on each\nequally divided unit of this interval. These generated probabilities provide\nuseful information regarding the boundary location of the action inside this\nsearch interval. Based on these probabilities, we introduce a boundary\npinpointing paradigm to pinpoint the accurate boundaries under a simple\nprobabilistic framework. Compared with other C3D feature based detectors,\nextensive experiments demonstrate that BLP significantly improves the\nlocalization performance of recent state-of-the-art detectors, and achieves\ncompetitive detection mAP on both THUMOS' 14 and ActivityNet datasets,\nparticularly when the evaluation tIoU is high.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:54:58 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 08:25:27 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 03:33:40 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 14:28:03 GMT"}, {"version": "v5", "created": "Tue, 19 Feb 2019 03:46:11 GMT"}, {"version": "v6", "created": "Mon, 16 Dec 2019 03:09:43 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kong", "Weijie", ""], ["Li", "Nannan", ""], ["Liu", "Shan", ""], ["Li", "Thomas", ""], ["Li", "Ge", ""]]}, {"id": "1811.02191", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Lars Petersson", "title": "3DCapsule: Extending the Capsule Architecture to Classify 3D Point\n  Clouds", "comments": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the 3DCapsule, which is a 3D extension of the recently\nintroduced Capsule concept that makes it applicable to unordered point sets.\nThe original Capsule relies on the existence of a spatial relationship between\nthe elements in the feature map it is presented with, whereas in point\npermutation invariant formulations of 3D point set classification methods, such\nrelationships are typically lost. Here, a new layer called ComposeCaps is\nintroduced that, in lieu of a spatially relevant feature mapping, learns a new\nmapping that can be exploited by the 3DCapsule. Previous works in the 3D point\nset classification domain have focused on other parts of the architecture,\nwhereas instead, the 3DCapsule is a drop-in replacement of the commonly used\nfully connected classifier. It is demonstrated via an ablation study, that when\nthe 3DCapsule is applied to recent 3D point set classification architectures,\nit consistently shows an improvement, in particular when subjected to noisy\ndata. Similarly, the ComposeCaps layer is evaluated and demonstrates an\nimprovement over the baseline. In an apples-to-apples comparison against\nstate-of-the-art methods, again, better performance is demonstrated by the\n3DCapsule.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:57:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Cheraghian", "Ali", ""], ["Petersson", "Lars", ""]]}, {"id": "1811.02194", "submitter": "Fei Yang", "authors": "Fei Yang, Qian Zhang, Chi Zheng, and Guoping Qiu", "title": "In-the-wild Facial Expression Recognition in Extreme Poses", "comments": "Published on ICGIP2017", "journal-ref": null, "doi": "10.1117/12.2302626", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the computer research area, facial expression recognition is a hot\nresearch problem. Recent years, the research has moved from the lab environment\nto in-the-wild circumstances. It is challenging, especially under extreme\nposes. But current expression detection systems are trying to avoid the pose\neffects and gain the general applicable ability. In this work, we solve the\nproblem in the opposite approach. We consider the head poses and detect the\nexpressions within special head poses. Our work includes two parts: detect the\nhead pose and group it into one pre-defined head pose class; do facial\nexpression recognize within each pose class. Our experiments show that the\nrecognition results with pose class grouping are much better than that of\ndirect recognition without considering poses. We combine the hand-crafted\nfeatures, SIFT, LBP and geometric feature, with deep learning feature as the\nrepresentation of the expressions. The handcrafted features are added into the\ndeep learning framework along with the high level deep learning features. As a\ncomparison, we implement SVM and random forest to as the prediction models. To\ntrain and test our methodology, we labeled the face dataset with 6 basic\nexpressions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:02:14 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yang", "Fei", ""], ["Zhang", "Qian", ""], ["Zheng", "Chi", ""], ["Qiu", "Guoping", ""]]}, {"id": "1811.02208", "submitter": "Qiangqiang Wu", "authors": "Qiangqiang Wu, Yan Yan, Yanjie Liang, Yi Liu, and Hanzi Wang", "title": "DSNet: Deep and Shallow Feature Learning for Efficient Visual Tracking", "comments": "To appear at ACCV 2018. 14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years, Discriminative Correlation Filter (DCF) based tracking\nmethods have achieved great success in visual tracking. However, the\nmulti-resolution convolutional feature maps trained from other tasks like image\nclassification, cannot be naturally used in the conventional DCF formulation.\nFurthermore, these high-dimensional feature maps significantly increase the\ntracking complexity and thus limit the tracking speed. In this paper, we\npresent a deep and shallow feature learning network, namely DSNet, to learn the\nmulti-level same-resolution compressed (MSC) features for efficient online\ntracking, in an end-to-end offline manner. Specifically, the proposed DSNet\ncompresses multi-level convolutional features to uniform spatial resolution\nfeatures. The learned MSC features effectively encode both appearance and\nsemantic information of objects in the same-resolution feature maps, thus\nenabling an elegant combination of the MSC features with any DCF-based methods.\nAdditionally, a channel reliability measurement (CRM) method is presented to\nfurther refine the learned MSC features. We demonstrate the effectiveness of\nthe MSC features learned from the proposed DSNet on two DCF tracking\nframeworks: the basic DCF framework and the continuous convolution operator\nframework. Extensive experiments show that the learned MSC features have the\nappealing advantage of allowing the equipped DCF-based tracking methods to\nperform favorably against the state-of-the-art methods while running at high\nframe rates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:55:42 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wu", "Qiangqiang", ""], ["Yan", "Yan", ""], ["Liang", "Yanjie", ""], ["Liu", "Yi", ""], ["Wang", "Hanzi", ""]]}, {"id": "1811.02233", "submitter": "Rui Qian", "authors": "Rui Qian, Yunchao Wei, Honghui Shi, Jiachen Li, Jiaying Liu and Thomas\n  Huang", "title": "Weakly Supervised Scene Parsing with Point-based Distance Metric\n  Learning", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene parsing is suffering from the fact that pixel-level\nannotations are hard to be collected. To tackle this issue, we propose a\nPoint-based Distance Metric Learning (PDML) in this paper. PDML does not\nrequire dense annotated masks and only leverages several labeled points that\nare much easier to obtain to guide the training process. Concretely, we\nleverage semantic relationship among the annotated points by encouraging the\nfeature representations of the intra- and inter-category points to keep\nconsistent, i.e. points within the same category should have more similar\nfeature representations compared to those from different categories. We\nformulate such a characteristic into a simple distance metric loss, which\ncollaborates with the point-wise cross-entropy loss to optimize the deep neural\nnetworks. Furthermore, to fully exploit the limited annotations, distance\nmetric learning is conducted across different training images instead of simply\nadopting an image-dependent manner. We conduct extensive experiments on two\nchallenging scene parsing benchmarks of PASCAL-Context and ADE 20K to validate\nthe effectiveness of our PDML, and competitive mIoU scores are achieved.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:00:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Qian", "Rui", ""], ["Wei", "Yunchao", ""], ["Shi", "Honghui", ""], ["Li", "Jiachen", ""], ["Liu", "Jiaying", ""], ["Huang", "Thomas", ""]]}, {"id": "1811.02234", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Semantic bottleneck for computer vision tasks", "comments": null, "journal-ref": "Asian Conference on Computer Vision (ACCV), Dec 2018, Perth,\n  Australia", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for the representation of images that is\nsemantic by nature, addressing the question of computation intelligibility in\ncomputer vision tasks. More specifically, our proposition is to introduce what\nwe call a semantic bottleneck in the processing pipeline, which is a crossing\npoint in which the representation of the image is entirely expressed with\nnatural language , while retaining the efficiency of numerical representations.\nWe show that our approach is able to generate semantic representations that\ngive state-of-the-art results on semantic content-based image retrieval and\nalso perform very well on image classification tasks. Intelligibility is\nevaluated through user centered experiments for failure detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:01:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1811.02248", "submitter": "Apostolos Modas", "authors": "Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "SparseFool: a few pixels make a big difference", "comments": "In Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have achieved extraordinary results on image\nclassification tasks, but have been shown to be vulnerable to attacks with\ncarefully crafted perturbations of the input data. Although most attacks\nusually change values of many image's pixels, it has been shown that deep\nnetworks are also vulnerable to sparse alterations of the input. However, no\ncomputationally efficient method has been proposed to compute sparse\nperturbations. In this paper, we exploit the low mean curvature of the decision\nboundary, and propose SparseFool, a geometry inspired sparse attack that\ncontrols the sparsity of the perturbations. Extensive evaluations show that our\napproach computes sparse perturbations very fast, and scales efficiently to\nhigh dimensional data. We further analyze the transferability and the visual\neffects of the perturbations, and show the existence of shared semantic\ninformation across the images and the networks. Finally, we show that\nadversarial training can only slightly improve the robustness against sparse\nadditive perturbations computed with SparseFool.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:30:34 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 11:56:58 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 19:41:22 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 16:33:41 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1811.02291", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Josef Kittler", "title": "MDLatLRR: A novel decomposition method for infrared and visible image\n  fusion", "comments": "IEEE Trans. Image Processing 2020, 14 pages, 17 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TIP.2020.2975984", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image decomposition is crucial for many image processing tasks, as it allows\nto extract salient features from source images. A good image decomposition\nmethod could lead to a better performance, especially in image fusion tasks. We\npropose a multi-level image decomposition method based on latent low-rank\nrepresentation(LatLRR), which is called MDLatLRR. This decomposition method is\napplicable to many image processing fields. In this paper, we focus on the\nimage fusion task. We develop a novel image fusion framework based on MDLatLRR,\nwhich is used to decompose source images into detail parts(salient features)\nand base parts. A nuclear-norm based fusion strategy is used to fuse the detail\nparts, and the base parts are fused by an averaging strategy. Compared with\nother state-of-the-art fusion methods, the proposed algorithm exhibits better\nfusion performance in both subjective and objective evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:21:53 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 09:01:50 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 02:36:06 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2020 04:30:59 GMT"}, {"version": "v5", "created": "Mon, 23 Mar 2020 07:49:48 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1811.02307", "submitter": "Vasili Ramanishka", "authors": "Vasili Ramanishka, Yi-Ting Chen, Teruhisa Misu, Kate Saenko", "title": "Toward Driving Scene Understanding: A Dataset for Learning Driver\n  Behavior and Causal Reasoning", "comments": "The dataset is available at https://usa.honda-ri.com/hdd", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2018, pp. 7699-7707", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving Scene understanding is a key ingredient for intelligent\ntransportation systems. To achieve systems that can operate in a complex\nphysical and social environment, they need to understand and learn how humans\ndrive and interact with traffic scenes. We present the Honda Research Institute\nDriving Dataset (HDD), a challenging dataset to enable research on learning\ndriver behavior in real-life environments. The dataset includes 104 hours of\nreal human driving in the San Francisco Bay Area collected using an\ninstrumented vehicle equipped with different sensors. We provide a detailed\nanalysis of HDD with a comparison to other driving datasets. A novel annotation\nmethodology is introduced to enable research on driver behavior understanding\nfrom untrimmed data sequences. As the first step, baseline algorithms for\ndriver behavior detection are trained and tested to demonstrate the feasibility\nof the proposed task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:02:55 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ramanishka", "Vasili", ""], ["Chen", "Yi-Ting", ""], ["Misu", "Teruhisa", ""], ["Saenko", "Kate", ""]]}, {"id": "1811.02308", "submitter": "Ruturaj Gavaskar", "authors": "Ruturaj G. Gavaskar and Kunal N. Chaudhury", "title": "Fast Adaptive Bilateral Filtering", "comments": null, "journal-ref": "R. G. Gavaskar and K. N. Chaudhury, \"Fast Adaptive Bilateral\n  Filtering,\" IEEE Transactions on Image Processing, vol. 28, no. 2, pp.\n  779-790, Feb. 2019", "doi": "10.1109/TIP.2018.2871597", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical bilateral filter, a fixed Gaussian range kernel is used\nalong with a spatial kernel for edge-preserving smoothing. We consider a\ngeneralization of this filter, the so-called adaptive bilateral filter, where\nthe center and width of the Gaussian range kernel is allowed to change from\npixel to pixel. Though this variant was originally proposed for sharpening and\nnoise removal, it can also be used for other applications such as artifact\nremoval and texture filtering. Similar to the bilateral filter, the brute-force\nimplementation of its adaptive counterpart requires intense computations. While\nseveral fast algorithms have been proposed in the literature for bilateral\nfiltering, most of them work only with a fixed range kernel. In this paper, we\npropose a fast algorithm for adaptive bilateral filtering, whose complexity\ndoes not scale with the spatial filter width. This is based on the observation\nthat the concerned filtering can be performed purely in range space using an\nappropriately defined local histogram. We show that by replacing the histogram\nwith a polynomial and the finite range-space sum with an integral, we can\napproximate the filter using analytic functions. In particular, an efficient\nalgorithm is derived using the following innovations: the polynomial is fitted\nby matching its moments to those of the target histogram (this is done using\nfast convolutions), and the analytic functions are recursively computed using\nintegration-by-parts. Our algorithm can accelerate the brute-force\nimplementation by at least $20 \\times$, without perceptible distortions in the\nvisual quality. We demonstrate the effectiveness of our algorithm for\nsharpening, JPEG deblocking, and texture filtering.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:04:44 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Gavaskar", "Ruturaj G.", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1811.02328", "submitter": "Kaipeng Zhang", "authors": "Kaipeng Zhang, Zhanpeng Zhang, Chia-Wen Cheng, Winston H. Hsu, Yu\n  Qiao, Wei Liu, Tong Zhang", "title": "Super-Identity Convolutional Neural Network for Face Hallucination", "comments": "Published in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination is a generative task to super-resolve the facial image\nwith low resolution while human perception of face heavily relies on identity\ninformation. However, previous face hallucination approaches largely ignore\nfacial identity recovery. This paper proposes Super-Identity Convolutional\nNeural Network (SICNN) to recover identity information for generating faces\nclosed to the real identity. Specifically, we define a super-identity loss to\nmeasure the identity difference between a hallucinated face and its\ncorresponding high-resolution face within the hypersphere identity metric\nspace. However, directly using this loss will lead to a Dynamic Domain\nDivergence problem, which is caused by the large margin between the\nhigh-resolution domain and the hallucination domain. To overcome this\nchallenge, we present a domain-integrated training approach by constructing a\nrobust identity metric for faces from these two domains. Extensive experimental\nevaluations demonstrate that the proposed SICNN achieves superior visual\nquality over the state-of-the-art methods on a challenging task to\nsuper-resolve 12$\\times$14 faces with an 8$\\times$ upscaling factor. In\naddition, SICNN significantly improves the recognizability of\nultra-low-resolution faces.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:50:08 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Zhang", "Kaipeng", ""], ["Zhang", "Zhanpeng", ""], ["Cheng", "Chia-Wen", ""], ["Hsu", "Winston H.", ""], ["Qiao", "Yu", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1811.02357", "submitter": "Anish Khadka Mr", "authors": "Anish R. Khadka, Paolo Remagnino, Vasileios Argyriou", "title": "Object 3D Reconstruction based on Photometric Stereo and Inverted\n  Rendering", "comments": "8 pages, 11 figure, SITIS conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for 3D reconstruction such as Photometric stereo recover the shape\nand reflectance properties using multiple images of an object taken with\nvariable lighting conditions from a fixed viewpoint. Photometric stereo assumes\nthat a scene is illuminated only directly by the illumination source. As\nresult, indirect illumination effects due to inter-reflections introduce strong\nbiases in the recovered shape. Our suggested approach is to recover scene\nproperties in the presence of indirect illumination. To this end, we proposed\nan iterative PS method combined with a reverted Monte-Carlo ray tracing\nalgorithm to overcome the inter-reflection effects aiming to separate the\ndirect and indirect lighting. This approach iteratively reconstructs a surface\nconsidering both the environment around the object and its concavities. We\ndemonstrate and evaluate our approach using three datasets and the overall\nresults illustrate improvement over the classic PS approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:07:35 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Khadka", "Anish R.", ""], ["Remagnino", "Paolo", ""], ["Argyriou", "Vasileios", ""]]}, {"id": "1811.02360", "submitter": "Chongyang Wang", "authors": "Chongyang Wang, Min Peng, Tao Bi, Tong Chen", "title": "Micro-Attention for Micro-Expression recognition", "comments": "17 pages, 5 figures, 7 tables, Code is available at GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expression, for its high objectivity in emotion detection, has emerged\nto be a promising modality in affective computing. Recently, deep learning\nmethods have been successfully introduced into the micro-expression recognition\narea. Whilst the higher recognition accuracy achieved, substantial challenges\nin micro-expression recognition remain. The existence of micro expression in\nsmall-local areas on face and limited size of available databases still\nconstrain the recognition accuracy on such emotional facial behavior. In this\nwork, to tackle such challenges, we propose a novel attention mechanism called\nmicro-attention cooperating with residual network. Micro-attention enables the\nnetwork to learn to focus on facial areas of interest covering different action\nunits. Moreover, coping with small datasets, the micro-attention is designed\nwithout adding noticeable parameters while a simple yet efficient transfer\nlearning approach is together utilized to alleviate the overfitting risk. With\nextensive experimental evaluations on three benchmarks (CASMEII, SAMM and SMIC)\nand post-hoc feature visualizations, we demonstrate the effectiveness of the\nproposed micro-attention and push the boundary of automatic recognition of\nmicro-expression.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:13:01 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 11:52:52 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 13:07:04 GMT"}, {"version": "v4", "created": "Sun, 7 Apr 2019 14:30:49 GMT"}, {"version": "v5", "created": "Tue, 27 Aug 2019 08:23:10 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Chongyang", ""], ["Peng", "Min", ""], ["Bi", "Tao", ""], ["Chen", "Tong", ""]]}, {"id": "1811.02363", "submitter": "Pravin Nair", "authors": "Pravin Nair and Kunal. N. Chaudhury", "title": "Fast High-Dimensional Bilateral and Nonlocal Means Filtering", "comments": "This work is accepted in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2878955", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fast algorithms for bilateral and nonlocal means filtering mostly\nwork with grayscale images. They cannot easily be extended to high-dimensional\ndata such as color and hyperspectral images, patch-based data, flow-fields,\netc. In this paper, we propose a fast algorithm for high-dimensional bilateral\nand nonlocal means filtering. Unlike existing approaches, where the focus is on\napproximating the data (using quantization) or the filter kernel (via analytic\nexpansions), we locally approximate the kernel using weighted and shifted\ncopies of a Gaussian, where the weights and shifts are inferred from the data.\nThe algorithm emerging from the proposed approximation essentially involves\nclustering and fast convolutions, and is easy to implement. Moreover, a variant\nof our algorithm comes with a guarantee (bound) on the approximation error,\nwhich is not enjoyed by existing algorithms. We present some results for\nhigh-dimensional bilateral and nonlocal means filtering to demonstrate the\nspeed and accuracy of our proposal. Moreover, we also show that our algorithm\ncan outperform state-of-the-art fast approximations in terms of accuracy and\ntiming.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:20:35 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Nair", "Pravin", ""], ["Chaudhury", "Kunal. N.", ""]]}, {"id": "1811.02372", "submitter": "Eric K. Tokuda Mr.", "authors": "Eric K. Tokuda, Claudio T. Silva, Roberto M. Cesar-Jr", "title": "Identifica\\c{c}\\~ao autom\\'atica de picha\\c{c}\\~ao a partir de imagens\n  urbanas", "comments": "Presented at IEEE-SIBGRAPI WiP'18, in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graffiti tagging is a common issue in great cities an local authorities are\non the move to combat it. The tagging map of a city can be a useful tool as it\nmay help to clean-up highly saturated regions and discourage future acts in the\nneighbourhood and currently there is no way of getting a tagging map of a\nregion in an automatic fashion and manual inspection or crowd participation are\nrequired. In this work, we describe a work in progress in creating an automatic\nway to get a tagging map of a city or region. It is based on the use of street\nview images and on the detection of graffiti tags in the images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:41:57 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Tokuda", "Eric K.", ""], ["Silva", "Claudio T.", ""], ["Cesar-Jr", "Roberto M.", ""]]}, {"id": "1811.02373", "submitter": "Vasily Morzhakov", "authors": "Vasily Morzhakov", "title": "Sets of autoencoders with shared latent spaces", "comments": "13 pages,16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders receive latent models of input data. It was shown in recent\nworks that they also estimate probability density functions of the input. This\nfact makes using the Bayesian decision theory possible. If we obtain latent\nmodels of input data for each class or for some points in the space of\nparameters in a parameter estimation task, we are able to estimate likelihood\nfunctions for those classes or points in parameter space. We show how the set\nof autoencoders solves the recognition problem. Each autoencoder describes its\nown model or context, a latent vector that presents input data in the latent\nspace may be called treatment in its context. Sharing latent spaces of\nautoencoders gives a very important property that is the ability to separate\ntreatment and context where the input information is treated through the set of\nautoencoders. There are two remarkable and most valuable results of this work:\na mechanism that shows a possible way of forming abstract concepts and a way of\nreducing dataset's size during training. These results are confirmed by tests\npresented in the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:42:36 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Morzhakov", "Vasily", ""]]}, {"id": "1811.02385", "submitter": "Aniket Bhatnagar", "authors": "Aniket Bhatnagar, Sanchit Aggarwal", "title": "Fine-grained Apparel Classification and Retrieval without rich\n  annotations", "comments": "14 pages, 6 figures, 3 tables, Submitted to Springer Journal of\n  Applied Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to correctly classify and retrieve apparel images has a variety\nof applications important to e-commerce, online advertising and internet\nsearch. In this work, we propose a robust framework for fine-grained apparel\nclassification, in-shop and cross-domain retrieval which eliminates the\nrequirement of rich annotations like bounding boxes and human-joints or\nclothing landmarks, and training of bounding box/ key-landmark detector for the\nsame. Factors such as subtle appearance differences, variations in human poses,\ndifferent shooting angles, apparel deformations, and self-occlusion add to the\nchallenges in classification and retrieval of apparel items. Cross-domain\nretrieval is even harder due to the presence of large variation between online\nshopping images, usually taken in ideal lighting, pose, positive angle and\nclean background as compared with street photos captured by users in\ncomplicated conditions with poor lighting and cluttered scenes. Our framework\nuses compact bilinear CNN with tensor sketch algorithm to generate embeddings\nthat capture local pairwise feature interactions in a translationally invariant\nmanner. For apparel classification, we pass the feature embeddings through a\nsoftmax classifier, while, the in-shop and cross-domain retrieval pipelines use\na triplet-loss based optimization approach, such that squared Euclidean\ndistance between embeddings measures the dissimilarity between the images.\nUnlike previous works that relied on bounding box, key clothing landmarks or\nhuman joint detectors to assist the final deep classifier, proposed framework\ncan be trained directly on the provided category labels or generated triplets\nfor triplet loss optimization. Lastly, Experimental results on the DeepFashion\nfine-grained categorization, and in-shop and consumer-to-shop retrieval\ndatasets provide a comparative analysis with previous work performed in the\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:55:33 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Bhatnagar", "Aniket", ""], ["Aggarwal", "Sanchit", ""]]}, {"id": "1811.02396", "submitter": "Paramanand Chandramouli", "authors": "Paramanand Chandramouli, Samuel Burri, Claudio Bruschini, Edoardo\n  Charbon and Andreas Kolb", "title": "A Bit Too Much? High Speed Imaging from Sparse Photon Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in photographic sensing technologies have made it possible to\nachieve light detection in terms of a single photon. Photon counting sensors\nare being increasingly used in many diverse applications. We address the\nproblem of jointly recovering spatial and temporal scene radiance from very few\nphoton counts. Our ConvNet-based scheme effectively combines spatial and\ntemporal information present in measurements to reduce noise. We demonstrate\nthat using our method one can acquire videos at a high frame rate and still\nachieve good quality signal-to-noise ratio. Experiments show that the proposed\nscheme performs quite well in different challenging scenarios while the\nexisting approaches are unable to handle them.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:07:08 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 16:38:09 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 14:30:00 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Chandramouli", "Paramanand", ""], ["Burri", "Samuel", ""], ["Bruschini", "Claudio", ""], ["Charbon", "Edoardo", ""], ["Kolb", "Andreas", ""]]}, {"id": "1811.02413", "submitter": "Tales Cesar De Oliveira Imbiriba", "authors": "Tales Imbiriba, Ricardo Augusto Borsoi, Jos\\'e Carlos Moreira Bermudez", "title": "Low-Rank Tensor Modeling for Hyperspectral Unmixing Accounting for\n  Spectral Variability", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.06355", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional hyperspectral unmixing methods neglect the underlying variability\nof spectral signatures often observed in typical hyperspectral images (HI),\npropagating these missmodeling errors throughout the whole unmixing process.\nAttempts to model material spectra as members of sets or as random variables\ntend to lead to severely ill-posed unmixing problems. Although parametric\nmodels have been proposed to overcome this drawback by handling endmember\nvariability through generalizations of the mixing model, the success of these\ntechniques depend on employing appropriate regularization strategies. Moreover,\nthe existing approaches fail to adequately explore the natural multidimensinal\nrepresentation of HIs. Recently, tensor-based strategies considered low-rank\ndecompositions of hyperspectral images as an alternative to impose\nlow-dimensional structures on the solutions of standard and multitemporal\nunmixing problems. These strategies, however, present two main drawbacks: 1)\nthey confine the solutions to low-rank tensors, which often cannot represent\nthe complexity of real-world scenarios; and 2) they lack guarantees that\nendmembers and abundances will be correctly factorized in their respective\ntensors. In this work, we propose a more flexible approach, called ULTRA-V,\nthat imposes low-rank structures through regularizations whose strictness is\ncontrolled by scalar parameters. Simulations attest the superior accuracy of\nthe method when compared with state-of-the-art unmixing algorithms that account\nfor spectral variability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:09:58 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 16:41:11 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 18:21:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Imbiriba", "Tales", ""], ["Borsoi", "Ricardo Augusto", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1811.02447", "submitter": "Frederic Jurie", "authors": "Valentin Vielzeuf, Alexis Lechervy, St\\'ephane Pateux, Fr\\'ed\\'eric\n  Jurie", "title": "Multi-Level Sensor Fusion with Deep Learning", "comments": "arXiv admin note: text overlap with arXiv:1808.07275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of deep learning, this article presents an original deep\nnetwork, namely CentralNet, for the fusion of information coming from different\nsensors. This approach is designed to efficiently and automatically balance the\ntrade-off between early and late fusion (i.e. between the fusion of low-level\nvs high-level information). More specifically, at each level of abstraction-the\ndifferent levels of deep networks-uni-modal representations of the data are fed\nto a central neural network which combines them into a common embedding. In\naddition, a multi-objective regularization is also introduced, helping to both\noptimize the central network and the unimodal networks. Experiments on four\nmultimodal datasets not only show state-of-the-art performance, but also\ndemonstrate that CentralNet can actually choose the best possible fusion\nstrategy for a given problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:26:45 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Lechervy", "Alexis", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1811.02454", "submitter": "Zhao Zhong", "authors": "Chen Lin, Zhao Zhong, Wei Wu, Junjie Yan", "title": "Synaptic Strength For Convolutional Neural Network", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNNs) are both computation and memory intensive\nwhich hindered their deployment in mobile devices. Inspired by the relevant\nconcept in neural science literature, we propose Synaptic Pruning: a\ndata-driven method to prune connections between input and output feature maps\nwith a newly proposed class of parameters called Synaptic Strength. Synaptic\nStrength is designed to capture the importance of a connection based on the\namount of information it transports. Experiment results show the effectiveness\nof our approach. On CIFAR-10, we prune connections for various CNN models with\nup to 96% , which results in significant size reduction and computation saving.\nFurther evaluation on ImageNet demonstrates that synaptic pruning is able to\ndiscover efficient models which is competitive to state-of-the-art compact CNNs\nsuch as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as\nfollowing: (1) We introduce Synaptic Strength, a new class of parameters for\nCNNs to indicate the importance of each connections. (2) Our approach can prune\nvarious CNNs with high compression without compromising accuracy. (3) Further\ninvestigation shows, the proposed Synaptic Strength is a better indicator for\nkernel pruning compared with the previous approach in both empirical result and\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:06:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lin", "Chen", ""], ["Zhong", "Zhao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "1811.02471", "submitter": "Marc Ru{\\ss}wurm", "authors": "Marc Ru{\\ss}wurm and Marco K\\\"orner", "title": "Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing\n  Imagery", "comments": "Cameraready version to NeurIPS 2018 Spatiotemporal Workshop.\n  Openreview: https://openreview.net/forum?id=Sye7df9CK7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clouds frequently cover the Earth's surface and pose an omnipresent challenge\nto optical Earth observation methods. The vast majority of remote sensing\napproaches either selectively choose single cloud-free observations or employ a\npre-classification strategy to identify and mask cloudy pixels. We follow a\ndifferent strategy and treat cloud coverage as noise that is inherent to the\nobserved satellite data. In prior work, we directly employed a straightforward\n\\emph{convolutional long short-term memory} network for vegetation\nclassification without explicit cloud filtering and achieved state-of-the-art\nclassification accuracies. In this work, we investigate this cloud-robustness\nfurther by visualizing internal cell activations and performing an ablation\nexperiment on datasets of different cloud coverage. In the visualizations of\nnetwork states, we identified some cells in which modulation and input gates\nclosed on cloudy pixels. This indicates that the network has internalized a\ncloud-filtering mechanism without being specifically trained on cloud labels.\nOverall, our results question the necessity of sophisticated pre-processing\npipelines for multi-temporal deep learning approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 17:58:22 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 11:30:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ru\u00dfwurm", "Marc", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1811.02476", "submitter": "Wenbo Li", "authors": "Wenbo Li, Longyin Wen, Xiao Bian, Siwei Lyu", "title": "Evolvement Constrained Adversarial Learning for Video Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video style transfer is a useful component for applications such as augmented\nreality, non-photorealistic rendering, and interactive games. Many existing\nmethods use optical flow to preserve the temporal smoothness of the synthesized\nvideo. However, the estimation of optical flow is sensitive to occlusions and\nrapid motions. Thus, in this work, we introduce a novel evolve-sync loss\ncomputed by evolvements to replace optical flow. Using this evolve-sync loss,\nwe build an adversarial learning framework, termed as Video Style Transfer\nGenerative Adversarial Network (VST-GAN), which improves upon the MGAN method\nfor image style transfer for more efficient video style transfer. We perform\nextensive experimental evaluations of our method and show quantitative and\nqualitative improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:31:19 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Wenbo", ""], ["Wen", "Longyin", ""], ["Bian", "Xiao", ""], ["Lyu", "Siwei", ""]]}, {"id": "1811.02496", "submitter": "Konstantinos Kamnitsas", "authors": "Chaitanya Baweja, Ben Glocker, Konstantinos Kamnitsas", "title": "Towards continual learning in medical imaging", "comments": "Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates continual learning of two segmentation tasks in brain\nMRI with neural networks. To explore in this context the capabilities of\ncurrent methods for countering catastrophic forgetting of the first task when a\nnew one is learned, we investigate elastic weight consolidation, a recently\nproposed method based on Fisher information, originally evaluated on\nreinforcement learning of Atari games. We use it to sequentially learn\nsegmentation of normal brain structures and then segmentation of white matter\nlesions. Our findings show this recent method reduces catastrophic forgetting,\nwhile large room for improvement exists in these challenging settings for\ncontinual learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:09:17 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Baweja", "Chaitanya", ""], ["Glocker", "Ben", ""], ["Kamnitsas", "Konstantinos", ""]]}, {"id": "1811.02539", "submitter": "Szu-Yeu Hu", "authors": "Szu-Yeu Hu, Andrew Beers, Ken Chang, Kathi H\\\"obel, J. Peter Campbell,\n  Deniz Erdogumus, Stratis Ioannidis, Jennifer Dy, Michael F. Chiang, Jayashree\n  Kalpathy-Cramer and James M. Brown", "title": "Deep feature transfer between localization and segmentation tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new pre-training scheme for U-net based image\nsegmentation. We first train the encoding arm as a localization network to\npredict the center of the target, before extending it into a U-net architecture\nfor segmentation. We apply our proposed method to the problem of segmenting the\noptic disc from fundus photographs. Our work shows that the features learned by\nencoding arm can be transferred to the segmentation network to reduce the\nannotation burden. We propose that an approach could have broad utility for\nmedical image segmentation, and alleviate the burden of delineating complex\nstructures by pre-training on annotations that are much easier to acquire.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:28:56 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 17:23:13 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hu", "Szu-Yeu", ""], ["Beers", "Andrew", ""], ["Chang", "Ken", ""], ["H\u00f6bel", "Kathi", ""], ["Campbell", "J. Peter", ""], ["Erdogumus", "Deniz", ""], ["Ioannidis", "Stratis", ""], ["Dy", "Jennifer", ""], ["Chiang", "Michael F.", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Brown", "James M.", ""]]}, {"id": "1811.02545", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Hao Yu, Aron Sarmasi, Gautam Pradeep, Yong Jae\n  Lee", "title": "Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised\n  Localization and Beyond", "comments": "TPAMI submission. This is a journal extension of our ICCV 2017 paper\n  arXiv:1704.04232", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose 'Hide-and-Seek' a general purpose data augmentation technique,\nwhich is complementary to existing data augmentation techniques and is\nbeneficial for various visual recognition tasks. The key idea is to hide\npatches in a training image randomly, in order to force the network to seek\nother relevant content when the most discriminative content is hidden. Our\napproach only needs to modify the input image and can work with any network to\nimprove its performance. During testing, it does not need to hide any patches.\nThe main advantage of Hide-and-Seek over existing data augmentation techniques\nis its ability to improve object localization accuracy in the weakly-supervised\nsetting, and we therefore use this task to motivate the approach. However,\nHide-and-Seek is not tied only to the image localization task, and can\ngeneralize to other forms of visual input like videos, as well as other\nrecognition tasks like image classification, temporal action localization,\nsemantic segmentation, emotion recognition, age/gender estimation, and person\nre-identification. We perform extensive experiments to showcase the advantage\nof Hide-and-Seek on these various visual recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:35:16 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Yu", "Hao", ""], ["Sarmasi", "Aron", ""], ["Pradeep", "Gautam", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1811.02565", "submitter": "Xinhai Liu", "authors": "Xinhai Liu, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker", "title": "Point2Sequence: Learning the Shape Representation of 3D Point Clouds\n  with an Attention-based Sequence to Sequence Network", "comments": "To be published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring contextual information in the local region is important for shape\nunderstanding and analysis. Existing studies often employ hand-crafted or\nexplicit ways to encode contextual information of local regions. However, it is\nhard to capture fine-grained contextual information in hand-crafted or explicit\nmanners, such as the correlation between different areas in a local region,\nwhich limits the discriminative ability of learned features. To resolve this\nissue, we propose a novel deep learning model for 3D point clouds, named\nPoint2Sequence, to learn 3D shape features by capturing fine-grained contextual\ninformation in a novel implicit way. Point2Sequence employs a novel sequence\nlearning model for point clouds to capture the correlations by aggregating\nmulti-scale areas of each local region with attention. Specifically,\nPoint2Sequence first learns the feature of each area scale in a local region.\nThen, it captures the correlation between area scales in the process of\naggregating all area scales using a recurrent neural network (RNN) based\nencoder-decoder structure, where an attention mechanism is proposed to\nhighlight the importance of different area scales. Experimental results show\nthat Point2Sequence achieves state-of-the-art performance in shape\nclassification and segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:07:03 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 07:40:03 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Liu", "Xinhai", ""], ["Han", "Zhizhong", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1811.02608", "submitter": "Gilles Baechler", "authors": "Laurent Valentin Jospin, Gilles Baechler and Adam Scholefield", "title": "Embedded polarizing filters to separate diffuse and specular reflection", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarizing filters provide a powerful way to separate diffuse and specular\nreflection; however, traditional methods rely on several captures and require\nproper alignment of the filters. Recently, camera manufacturers have proposed\nto embed polarizing micro-filters in front of the sensor, creating a mosaic of\npixels with different polarizations. In this paper, we investigate the\nadvantages of such camera designs. In particular, we consider different design\npatterns for the filter arrays and propose an algorithm to demosaic an image\ngenerated by such cameras. This essentially allows us to separate the diffuse\nand specular components using a single image. The performance of our algorithm\nis compared with a color-based method using synthetic and real data. Finally,\nwe demonstrate how we can recover the normals of a scene using the diffuse\nimages estimated by our method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:43:45 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Jospin", "Laurent Valentin", ""], ["Baechler", "Gilles", ""], ["Scholefield", "Adam", ""]]}, {"id": "1811.02627", "submitter": "Bin Song", "authors": "Yue Zhang, Bin Song, Xiaojiang Du, and Mohsen Guizani", "title": "Vehicle Tracking Using Surveillance with Multimodal Data Fusion", "comments": "8 pages,6 figures,33 conferences", "journal-ref": null, "doi": "10.1109/TITS.2017.2787101", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle location prediction or vehicle tracking is a significant topic within\nconnected vehicles. This task, however, is difficult if only a single modal\ndata is available, probably causing bias and impeding the accuracy. With the\ndevelopment of sensor networks in connected vehicles, multimodal data are\nbecoming accessible. Therefore, we propose a framework for vehicle tracking\nwith multimodal data fusion. Specifically, we fuse the results of two\nmodalities, images and velocity, in our vehicle-tracking task. Images, being\nprocessed in the module of vehicle detection, provide direct information about\nthe features of vehicles, whereas velocity estimation can further evaluate the\npossible location of the target vehicles, which reduces the number of features\nbeing compared, and decreases the time consumption and computational cost.\nVehicle detection is designed with a color-faster R-CNN, which takes both the\nshape and color of the vehicles into consideration. Meanwhile, velocity\nestimation is through the Kalman filter, which is a classical method for\ntracking. Finally, a multimodal data fusion method is applied to integrate\nthese outcomes so that vehicle-tracking tasks can be achieved. Experimental\nresults suggest the efficiency of our methods, which can track vehicles using a\nseries of surveillance cameras in urban areas.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 04:14:05 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zhang", "Yue", ""], ["Song", "Bin", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1811.02628", "submitter": "Dong Yul Oh", "authors": "Dong Yul Oh and Il Dong Yun", "title": "Learning Bone Suppression from Dual Energy Chest X-rays using\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suppressing bones on chest X-rays such as ribs and clavicle is often expected\nto improve pathologies classification. These bones can interfere with a broad\nrange of diagnostic tasks on pulmonary disease except for musculoskeletal\nsystem. Current conventional method for acquisition of bone suppressed X-rays\nis dual energy imaging, which captures two radiographs at a very short interval\nwith different energy levels; however, the patient is exposed to radiation\ntwice and the artifacts arise due to heartbeats between two shots. In this\npaper, we introduce a deep generative model trained to predict bone suppressed\nimages on single energy chest X-rays, analyzing a finite set of previously\nacquired dual energy chest X-rays. Since the relatively small amount of data is\navailable, such approach relies on the methodology maximizing the data\nutilization. Here we integrate the following two approaches. First, we use a\nconditional generative adversarial network that complements the traditional\nregression method minimizing the pairwise image difference. Second, we use Haar\n2D wavelet decomposition to offer a perceptual guideline in frequency details\nto allow the model to converge quickly and efficiently. As a result, we achieve\nstate-of-the-art performance on bone suppression as compared to the existing\napproaches with dual energy chest X-rays.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:01:31 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Oh", "Dong Yul", ""], ["Yun", "Il Dong", ""]]}, {"id": "1811.02629", "submitter": "Spyridon Bakas", "authors": "Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus\n  Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung\n  Min Ha, Martin Rozycki, Marcel Prastawa, Esther Alberts, Jana Lipkova, John\n  Freymann, Justin Kirby, Michel Bilello, Hassan Fathallah-Shaykh, Roland\n  Wiest, Jan Kirschke, Benedikt Wiestler, Rivka Colen, Aikaterini Kotrotsou,\n  Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-Andre\n  Weber, Abhishek Mahajan, Ujjwal Baid, Elizabeth Gerstner, Dongjin Kwon, Gagan\n  Acharya, Manu Agarwal, Mahbubul Alam, Alberto Albiol, Antonio Albiol,\n  Francisco J. Albiol, Varghese Alex, Nigel Allinson, Pedro H. A. Amorim,\n  Abhijit Amrutkar, Ganesh Anand, Simon Andermatt, Tal Arbel, Pablo Arbelaez,\n  Aaron Avery, Muneeza Azmat, Pranjal B., W Bai, Subhashis Banerjee, Bill\n  Barth, Thomas Batchelder, Kayhan Batmanghelich, Enzo Battistella, Andrew\n  Beers, Mikhail Belyaev, Martin Bendszus, Eze Benson, Jose Bernal, Halandur\n  Nagaraja Bharath, George Biros, Sotirios Bisdas, James Brown, Mariano\n  Cabezas, Shilei Cao, Jorge M. Cardoso, Eric N Carver, Adri\\`a Casamitjana,\n  Laura Silvana Castillo, Marcel Cat\\`a, Philippe Cattin, Albert Cerigues,\n  Vinicius S. Chagas, Siddhartha Chandra, Yi-Ju Chang, Shiyu Chang, Ken Chang,\n  Joseph Chazalon, Shengcong Chen, Wei Chen, Jefferson W Chen, Zhaolin Chen,\n  Kun Cheng, Ahana Roy Choudhury, Roger Chylla, Albert Cl\\'erigues, Steven\n  Colleman, Ramiro German Rodriguez Colmeiro, Marc Combalia, Anthony Costa,\n  Xiaomeng Cui, Zhenzhen Dai, Lutao Dai, Laura Alexandra Daza, Eric Deutsch,\n  Changxing Ding, Chao Dong, Shidu Dong, Wojciech Dudzik, Zach Eaton-Rosen,\n  Gary Egan, Guilherme Escudero, Th\\'eo Estienne, Richard Everson, Jonathan\n  Fabrizio, Yong Fan, Longwei Fang, Xue Feng, Enzo Ferrante, Lucas Fidon,\n  Martin Fischer, Andrew P. French, Naomi Fridman, Huan Fu, David Fuentes,\n  Yaozong Gao, Evan Gates, David Gering, Amir Gholami, Willi Gierke, Ben\n  Glocker, Mingming Gong, Sandra Gonz\\'alez-Vill\\'a, T. Grosges, Yuanfang Guan,\n  Sheng Guo, Sudeep Gupta, Woo-Sup Han, Il Song Han, Konstantin Harmuth,\n  Huiguang He, Aura Hern\\'andez-Sabat\\'e, Evelyn Herrmann, Naveen Himthani,\n  Winston Hsu, Cheyu Hsu, Xiaojun Hu, Xiaobin Hu, Yan Hu, Yifan Hu, Rui Hua,\n  Teng-Yi Huang, Weilin Huang, Sabine Van Huffel, Quan Huo, Vivek HV, Khan M.\n  Iftekharuddin, Fabian Isensee, Mobarakol Islam, Aaron S. Jackson, Sachin R.\n  Jambawalikar, Andrew Jesson, Weijian Jian, Peter Jin, V Jeya Maria Jose,\n  Alain Jungo, B Kainz, Konstantinos Kamnitsas, Po-Yu Kao, Ayush Karnawat,\n  Thomas Kellermeier, Adel Kermi, Kurt Keutzer, Mohamed Tarek Khadir, Mahendra\n  Khened, Philipp Kickingereder, Geena Kim, Nik King, Haley Knapp, Urspeter\n  Knecht, Lisa Kohli, Deren Kong, Xiangmao Kong, Simon Koppers, Avinash Kori,\n  Ganapathy Krishnamurthi, Egor Krivov, Piyush Kumar, Kaisar Kushibar, Dmitrii\n  Lachinov, Tryphon Lambrou, Joon Lee, Chengen Lee, Yuehchou Lee, M Lee,\n  Szidonia Lefkovits, Laszlo Lefkovits, James Levitt, Tengfei Li, Hongwei Li,\n  Wenqi Li, Hongyang Li, Xiaochuan Li, Yuexiang Li, Heng Li, Zhenye Li, Xiaoyu\n  Li, Zeju Li, XiaoGang Li, Wenqi Li, Zheng-Shen Lin, Fengming Lin, Pietro Lio,\n  Chang Liu, Boqiang Liu, Xiang Liu, Mingyuan Liu, Ju Liu, Luyan Liu, Xavier\n  Llado, Marc Moreno Lopez, Pablo Ribalta Lorenzo, Zhentai Lu, Lin Luo, Zhigang\n  Luo, Jun Ma, Kai Ma, Thomas Mackie, Anant Madabushi, Issam Mahmoudi, Klaus H.\n  Maier-Hein, Pradipta Maji, CP Mammen, Andreas Mang, B. S. Manjunath, Michal\n  Marcinkiewicz, S McDonagh, Stephen McKenna, Richard McKinley, Miriam Mehl,\n  Sachin Mehta, Raghav Mehta, Raphael Meier, Christoph Meinel, Dorit Merhof,\n  Craig Meyer, Robert Miller, Sushmita Mitra, Aliasgar Moiyadi, David\n  Molina-Garcia, Miguel A.B. Monteiro, Grzegorz Mrukwa, Andriy Myronenko, Jakub\n  Nalepa, Thuyen Ngo, Dong Nie, Holly Ning, Chen Niu, Nicholas K Nuechterlein,\n  Eric Oermann, Arlindo Oliveira, Diego D. C. Oliveira, Arnau Oliver, Alexander\n  F. I. Osman, Yu-Nian Ou, Sebastien Ourselin, Nikos Paragios, Moo Sung Park,\n  Brad Paschke, J. Gregory Pauloski, Kamlesh Pawar, Nick Pawlowski, Linmin Pei,\n  Suting Peng, Silvio M. Pereira, Julian Perez-Beteta, Victor M. Perez-Garcia,\n  Simon Pezold, Bao Pham, Ashish Phophalia, Gemma Piella, G.N. Pillai, Marie\n  Piraud, Maxim Pisov, Anmol Popli, Michael P. Pound, Reza Pourreza, Prateek\n  Prasanna, Vesna Prkovska, Tony P. Pridmore, Santi Puch, \\'Elodie Puybareau,\n  Buyue Qian, Xu Qiao, Martin Rajchl, Swapnil Rane, Michael Rebsamen, Hongliang\n  Ren, Xuhua Ren, Karthik Revanuru, Mina Rezaei, Oliver Rippel, Luis Carlos\n  Rivera, Charlotte Robert, Bruce Rosen, Daniel Rueckert, Mohammed Safwan,\n  Mostafa Salem, Joaquim Salvi, Irina Sanchez, Irina S\\'anchez, Heitor M.\n  Santos, Emmett Sartor, Dawid Schellingerhout, Klaudius Scheufele, Matthew R.\n  Scott, Artur A. Scussel, Sara Sedlar, Juan Pablo Serrano-Rubio, N. Jon Shah,\n  Nameetha Shah, Mazhar Shaikh, B. Uma Shankar, Zeina Shboul, Haipeng Shen,\n  Dinggang Shen, Linlin Shen, Haocheng Shen, Varun Shenoy, Feng Shi, Hyung Eun\n  Shin, Hai Shu, Diana Sima, M Sinclair, Orjan Smedby, James M. Snyder,\n  Mohammadreza Soltaninejad, Guidong Song, Mehul Soni, Jean Stawiaski, Shashank\n  Subramanian, Li Sun, Roger Sun, Jiawei Sun, Kay Sun, Yu Sun, Guoxia Sun,\n  Shuang Sun, Yannick R Suter, Laszlo Szilagyi, Sanjay Talbar, Dacheng Tao,\n  Dacheng Tao, Zhongzhao Teng, Siddhesh Thakur, Meenakshi H Thakur, Sameer\n  Tharakan, Pallavi Tiwari, Guillaume Tochon, Tuan Tran, Yuhsiang M. Tsai,\n  Kuan-Lun Tseng, Tran Anh Tuan, Vadim Turlapov, Nicholas Tustison, Maria\n  Vakalopoulou, Sergi Valverde, Rami Vanguri, Evgeny Vasiliev, Jonathan\n  Ventura, Luis Vera, Tom Vercauteren, C. A. Verrastro, Lasitha Vidyaratne,\n  Veronica Vilaplana, Ajeet Vivekanandan, Guotai Wang, Qian Wang, Chiatse J.\n  Wang, Weichung Wang, Duo Wang, Ruixuan Wang, Yuanyuan Wang, Chunliang Wang,\n  Guotai Wang, Ning Wen, Xin Wen, Leon Weninger, Wolfgang Wick, Shaocheng Wu,\n  Qiang Wu, Yihong Wu, Yong Xia, Yanwu Xu, Xiaowen Xu, Peiyuan Xu, Tsai-Ling\n  Yang, Xiaoping Yang, Hao-Yu Yang, Junlin Yang, Haojin Yang, Guang Yang,\n  Hongdou Yao, Xujiong Ye, Changchang Yin, Brett Young-Moxon, Jinhua Yu,\n  Xiangyu Yue, Songtao Zhang, Angela Zhang, Kun Zhang, Xuejie Zhang, Lichi\n  Zhang, Xiaoyue Zhang, Yazhuo Zhang, Lei Zhang, Jianguo Zhang, Xiang Zhang,\n  Tianhao Zhang, Sicheng Zhao, Yu Zhao, Xiaomei Zhao, Liang Zhao, Yefeng Zheng,\n  Liming Zhong, Chenhong Zhou, Xiaobing Zhou, Fan Zhou, Hongtu Zhu, Jin Zhu,\n  Ying Zhuge, Weiwei Zong, Jayashree Kalpathy-Cramer, Keyvan Farahani, Christos\n  Davatzikos, Koen van Leemput, Bjoern Menze", "title": "Identifying the Best Machine Learning Algorithms for Brain Tumor\n  Segmentation, Progression Assessment, and Overall Survival Prediction in the\n  BRATS Challenge", "comments": "The International Multimodal Brain Tumor Segmentation (BraTS)\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are the most common primary brain malignancies, with different\ndegrees of aggressiveness, variable prognosis and various heterogeneous\nhistologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic\ncore, active and non-enhancing core. This intrinsic heterogeneity is also\nportrayed in their radio-phenotype, as their sub-regions are depicted by\nvarying intensity profiles disseminated across multi-parametric magnetic\nresonance imaging (mpMRI) scans, reflecting varying biological properties.\nTheir heterogeneous shape, extent, and location are some of the factors that\nmake these tumors difficult to resect, and in some cases inoperable. The amount\nof resected tumor is a factor also considered in longitudinal scans, when\nevaluating the apparent tumor for potential diagnosis of progression.\nFurthermore, there is mounting evidence that accurate segmentation of the\nvarious tumor sub-regions can offer the basis for quantitative image analysis\ntowards prediction of patient overall survival. This study assesses the\nstate-of-the-art machine learning (ML) methods used for brain tumor image\nanalysis in mpMRI scans, during the last seven instances of the International\nBrain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we\nfocus on i) evaluating segmentations of the various glioma sub-regions in\npre-operative mpMRI scans, ii) assessing potential tumor progression by virtue\nof longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO\ncriteria, and iii) predicting the overall survival from pre-operative mpMRI\nscans of patients that underwent gross total resection. Finally, we investigate\nthe challenge of identifying the best ML algorithms for each of these tasks,\nconsidering that apart from being diverse on each instance of the challenge,\nthe multi-institutional mpMRI BraTS dataset has also been a continuously\nevolving/growing dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 05:10:18 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 23:18:19 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 13:35:04 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Bakas", "Spyridon", ""], ["Reyes", "Mauricio", ""], ["Jakab", "Andras", ""], ["Bauer", "Stefan", ""], ["Rempfler", "Markus", ""], ["Crimi", "Alessandro", ""], ["Shinohara", "Russell Takeshi", ""], ["Berger", "Christoph", ""], ["Ha", "Sung Min", ""], ["Rozycki", "Martin", ""], ["Prastawa", "Marcel", ""], ["Alberts", "Esther", ""], ["Lipkova", "Jana", ""], ["Freymann", "John", ""], ["Kirby", "Justin", ""], ["Bilello", "Michel", ""], ["Fathallah-Shaykh", "Hassan", ""], ["Wiest", "Roland", ""], ["Kirschke", "Jan", ""], ["Wiestler", "Benedikt", ""], ["Colen", "Rivka", ""], ["Kotrotsou", "Aikaterini", ""], ["Lamontagne", "Pamela", ""], ["Marcus", "Daniel", ""], ["Milchenko", "Mikhail", ""], ["Nazeri", "Arash", ""], ["Weber", "Marc-Andre", ""], ["Mahajan", "Abhishek", ""], ["Baid", "Ujjwal", ""], ["Gerstner", "Elizabeth", ""], ["Kwon", "Dongjin", ""], ["Acharya", "Gagan", ""], ["Agarwal", "Manu", ""], ["Alam", "Mahbubul", ""], ["Albiol", "Alberto", ""], ["Albiol", "Antonio", ""], ["Albiol", "Francisco J.", ""], ["Alex", "Varghese", ""], ["Allinson", "Nigel", ""], ["Amorim", "Pedro H. A.", ""], ["Amrutkar", "Abhijit", ""], ["Anand", "Ganesh", ""], ["Andermatt", "Simon", ""], ["Arbel", "Tal", ""], ["Arbelaez", "Pablo", ""], ["Avery", "Aaron", ""], ["Azmat", "Muneeza", ""], ["B.", "Pranjal", ""], ["Bai", "W", ""], ["Banerjee", "Subhashis", ""], ["Barth", "Bill", ""], ["Batchelder", "Thomas", ""], ["Batmanghelich", "Kayhan", ""], ["Battistella", "Enzo", ""], ["Beers", "Andrew", ""], ["Belyaev", "Mikhail", ""], ["Bendszus", "Martin", ""], ["Benson", "Eze", ""], ["Bernal", "Jose", ""], ["Bharath", "Halandur Nagaraja", ""], ["Biros", "George", ""], ["Bisdas", "Sotirios", ""], ["Brown", "James", ""], ["Cabezas", "Mariano", ""], ["Cao", "Shilei", ""], ["Cardoso", "Jorge M.", ""], ["Carver", "Eric N", ""], ["Casamitjana", "Adri\u00e0", ""], ["Castillo", "Laura Silvana", ""], ["Cat\u00e0", "Marcel", ""], ["Cattin", "Philippe", ""], ["Cerigues", "Albert", ""], ["Chagas", "Vinicius S.", ""], ["Chandra", "Siddhartha", ""], ["Chang", "Yi-Ju", ""], ["Chang", "Shiyu", ""], ["Chang", "Ken", ""], ["Chazalon", "Joseph", ""], ["Chen", "Shengcong", ""], ["Chen", "Wei", ""], ["Chen", "Jefferson W", ""], ["Chen", "Zhaolin", ""], ["Cheng", "Kun", ""], ["Choudhury", "Ahana Roy", ""], ["Chylla", "Roger", ""], ["Cl\u00e9rigues", "Albert", ""], ["Colleman", "Steven", ""], ["Colmeiro", "Ramiro German Rodriguez", ""], ["Combalia", "Marc", ""], ["Costa", "Anthony", ""], ["Cui", "Xiaomeng", ""], ["Dai", "Zhenzhen", ""], ["Dai", "Lutao", ""], ["Daza", "Laura Alexandra", ""], ["Deutsch", "Eric", ""], ["Ding", "Changxing", ""], ["Dong", "Chao", ""], ["Dong", "Shidu", ""], ["Dudzik", "Wojciech", ""], ["Eaton-Rosen", "Zach", ""], ["Egan", "Gary", ""], ["Escudero", "Guilherme", ""], ["Estienne", "Th\u00e9o", ""], ["Everson", "Richard", ""], ["Fabrizio", "Jonathan", ""], ["Fan", "Yong", ""], ["Fang", "Longwei", ""], ["Feng", "Xue", ""], ["Ferrante", "Enzo", ""], ["Fidon", "Lucas", ""], ["Fischer", "Martin", ""], ["French", "Andrew P.", ""], ["Fridman", "Naomi", ""], ["Fu", "Huan", ""], ["Fuentes", "David", ""], ["Gao", "Yaozong", ""], ["Gates", "Evan", ""], ["Gering", "David", ""], ["Gholami", "Amir", ""], ["Gierke", "Willi", ""], ["Glocker", "Ben", ""], ["Gong", "Mingming", ""], ["Gonz\u00e1lez-Vill\u00e1", "Sandra", ""], ["Grosges", "T.", ""], ["Guan", "Yuanfang", ""], ["Guo", "Sheng", ""], ["Gupta", "Sudeep", ""], ["Han", "Woo-Sup", ""], ["Han", "Il Song", ""], ["Harmuth", "Konstantin", ""], ["He", "Huiguang", ""], ["Hern\u00e1ndez-Sabat\u00e9", "Aura", ""], ["Herrmann", "Evelyn", ""], ["Himthani", "Naveen", ""], ["Hsu", "Winston", ""], ["Hsu", "Cheyu", ""], ["Hu", "Xiaojun", ""], ["Hu", "Xiaobin", ""], ["Hu", "Yan", ""], ["Hu", "Yifan", ""], ["Hua", "Rui", ""], ["Huang", "Teng-Yi", ""], ["Huang", "Weilin", ""], ["Van Huffel", "Sabine", ""], ["Huo", "Quan", ""], ["HV", "Vivek", ""], ["Iftekharuddin", "Khan M.", ""], ["Isensee", "Fabian", ""], ["Islam", "Mobarakol", ""], ["Jackson", "Aaron S.", ""], ["Jambawalikar", "Sachin R.", ""], ["Jesson", "Andrew", ""], ["Jian", "Weijian", ""], ["Jin", "Peter", ""], ["Jose", "V Jeya Maria", ""], ["Jungo", "Alain", ""], ["Kainz", "B", ""], ["Kamnitsas", "Konstantinos", ""], ["Kao", "Po-Yu", ""], ["Karnawat", "Ayush", ""], ["Kellermeier", "Thomas", ""], ["Kermi", "Adel", ""], ["Keutzer", "Kurt", ""], ["Khadir", "Mohamed Tarek", ""], ["Khened", "Mahendra", ""], ["Kickingereder", "Philipp", ""], ["Kim", "Geena", ""], ["King", "Nik", ""], ["Knapp", "Haley", ""], ["Knecht", "Urspeter", ""], ["Kohli", "Lisa", ""], ["Kong", "Deren", ""], ["Kong", "Xiangmao", ""], ["Koppers", "Simon", ""], ["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathy", ""], ["Krivov", "Egor", ""], ["Kumar", "Piyush", ""], ["Kushibar", "Kaisar", ""], ["Lachinov", "Dmitrii", ""], ["Lambrou", "Tryphon", ""], ["Lee", "Joon", ""], ["Lee", "Chengen", ""], ["Lee", "Yuehchou", ""], ["Lee", "M", ""], ["Lefkovits", "Szidonia", ""], ["Lefkovits", "Laszlo", ""], ["Levitt", "James", ""], ["Li", "Tengfei", ""], ["Li", "Hongwei", ""], ["Li", "Wenqi", ""], ["Li", "Hongyang", ""], ["Li", "Xiaochuan", ""], ["Li", "Yuexiang", ""], ["Li", "Heng", ""], ["Li", "Zhenye", ""], ["Li", "Xiaoyu", ""], ["Li", "Zeju", ""], ["Li", "XiaoGang", ""], ["Li", "Wenqi", ""], ["Lin", "Zheng-Shen", ""], ["Lin", "Fengming", ""], ["Lio", "Pietro", ""], ["Liu", "Chang", ""], ["Liu", "Boqiang", ""], ["Liu", "Xiang", ""], ["Liu", "Mingyuan", ""], ["Liu", "Ju", ""], ["Liu", "Luyan", ""], ["Llado", "Xavier", ""], ["Lopez", "Marc Moreno", ""], ["Lorenzo", "Pablo Ribalta", ""], ["Lu", "Zhentai", ""], ["Luo", "Lin", ""], ["Luo", "Zhigang", ""], ["Ma", "Jun", ""], ["Ma", "Kai", ""], ["Mackie", "Thomas", ""], ["Madabushi", "Anant", ""], ["Mahmoudi", "Issam", ""], ["Maier-Hein", "Klaus H.", ""], ["Maji", "Pradipta", ""], ["Mammen", "CP", ""], ["Mang", "Andreas", ""], ["Manjunath", "B. S.", ""], ["Marcinkiewicz", "Michal", ""], ["McDonagh", "S", ""], ["McKenna", "Stephen", ""], ["McKinley", "Richard", ""], ["Mehl", "Miriam", ""], ["Mehta", "Sachin", ""], ["Mehta", "Raghav", ""], ["Meier", "Raphael", ""], ["Meinel", "Christoph", ""], ["Merhof", "Dorit", ""], ["Meyer", "Craig", ""], ["Miller", "Robert", ""], ["Mitra", "Sushmita", ""], ["Moiyadi", "Aliasgar", ""], ["Molina-Garcia", "David", ""], ["Monteiro", "Miguel A. B.", ""], ["Mrukwa", "Grzegorz", ""], ["Myronenko", "Andriy", ""], ["Nalepa", "Jakub", ""], ["Ngo", "Thuyen", ""], ["Nie", "Dong", ""], ["Ning", "Holly", ""], ["Niu", "Chen", ""], ["Nuechterlein", "Nicholas K", ""], ["Oermann", "Eric", ""], ["Oliveira", "Arlindo", ""], ["Oliveira", "Diego D. C.", ""], ["Oliver", "Arnau", ""], ["Osman", "Alexander F. I.", ""], ["Ou", "Yu-Nian", ""], ["Ourselin", "Sebastien", ""], ["Paragios", "Nikos", ""], ["Park", "Moo Sung", ""], ["Paschke", "Brad", ""], ["Pauloski", "J. Gregory", ""], ["Pawar", "Kamlesh", ""], ["Pawlowski", "Nick", ""], ["Pei", "Linmin", ""], ["Peng", "Suting", ""], ["Pereira", "Silvio M.", ""], ["Perez-Beteta", "Julian", ""], ["Perez-Garcia", "Victor M.", ""], ["Pezold", "Simon", ""], ["Pham", "Bao", ""], ["Phophalia", "Ashish", ""], ["Piella", "Gemma", ""], ["Pillai", "G. N.", ""], ["Piraud", "Marie", ""], ["Pisov", "Maxim", ""], ["Popli", "Anmol", ""], ["Pound", "Michael P.", ""], ["Pourreza", "Reza", ""], ["Prasanna", "Prateek", ""], ["Prkovska", "Vesna", ""], ["Pridmore", "Tony P.", ""], ["Puch", "Santi", ""], ["Puybareau", "\u00c9lodie", ""], ["Qian", "Buyue", ""], ["Qiao", "Xu", ""], ["Rajchl", "Martin", ""], ["Rane", "Swapnil", ""], ["Rebsamen", "Michael", ""], ["Ren", "Hongliang", ""], ["Ren", "Xuhua", ""], ["Revanuru", "Karthik", ""], ["Rezaei", "Mina", ""], ["Rippel", "Oliver", ""], ["Rivera", "Luis Carlos", ""], ["Robert", "Charlotte", ""], ["Rosen", "Bruce", ""], ["Rueckert", "Daniel", ""], ["Safwan", "Mohammed", ""], ["Salem", "Mostafa", ""], ["Salvi", "Joaquim", ""], ["Sanchez", "Irina", ""], ["S\u00e1nchez", "Irina", ""], ["Santos", "Heitor M.", ""], ["Sartor", "Emmett", ""], ["Schellingerhout", "Dawid", ""], ["Scheufele", "Klaudius", ""], ["Scott", "Matthew R.", ""], ["Scussel", "Artur A.", ""], ["Sedlar", "Sara", ""], ["Serrano-Rubio", "Juan Pablo", ""], ["Shah", "N. Jon", ""], ["Shah", "Nameetha", ""], ["Shaikh", "Mazhar", ""], ["Shankar", "B. Uma", ""], ["Shboul", "Zeina", ""], ["Shen", "Haipeng", ""], ["Shen", "Dinggang", ""], ["Shen", "Linlin", ""], ["Shen", "Haocheng", ""], ["Shenoy", "Varun", ""], ["Shi", "Feng", ""], ["Shin", "Hyung Eun", ""], ["Shu", "Hai", ""], ["Sima", "Diana", ""], ["Sinclair", "M", ""], ["Smedby", "Orjan", ""], ["Snyder", "James M.", ""], ["Soltaninejad", "Mohammadreza", ""], ["Song", "Guidong", ""], ["Soni", "Mehul", ""], ["Stawiaski", "Jean", ""], ["Subramanian", "Shashank", ""], ["Sun", "Li", ""], ["Sun", "Roger", ""], ["Sun", "Jiawei", ""], ["Sun", "Kay", ""], ["Sun", "Yu", ""], ["Sun", "Guoxia", ""], ["Sun", "Shuang", ""], ["Suter", "Yannick R", ""], ["Szilagyi", "Laszlo", ""], ["Talbar", "Sanjay", ""], ["Tao", "Dacheng", ""], ["Tao", "Dacheng", ""], ["Teng", "Zhongzhao", ""], ["Thakur", "Siddhesh", ""], ["Thakur", "Meenakshi H", ""], ["Tharakan", "Sameer", ""], ["Tiwari", "Pallavi", ""], ["Tochon", "Guillaume", ""], ["Tran", "Tuan", ""], ["Tsai", "Yuhsiang M.", ""], ["Tseng", "Kuan-Lun", ""], ["Tuan", "Tran Anh", ""], ["Turlapov", "Vadim", ""], ["Tustison", "Nicholas", ""], ["Vakalopoulou", "Maria", ""], ["Valverde", "Sergi", ""], ["Vanguri", "Rami", ""], ["Vasiliev", "Evgeny", ""], ["Ventura", "Jonathan", ""], ["Vera", "Luis", ""], ["Vercauteren", "Tom", ""], ["Verrastro", "C. A.", ""], ["Vidyaratne", "Lasitha", ""], ["Vilaplana", "Veronica", ""], ["Vivekanandan", "Ajeet", ""], ["Wang", "Guotai", ""], ["Wang", "Qian", ""], ["Wang", "Chiatse J.", ""], ["Wang", "Weichung", ""], ["Wang", "Duo", ""], ["Wang", "Ruixuan", ""], ["Wang", "Yuanyuan", ""], ["Wang", "Chunliang", ""], ["Wang", "Guotai", ""], ["Wen", "Ning", ""], ["Wen", "Xin", ""], ["Weninger", "Leon", ""], ["Wick", "Wolfgang", ""], ["Wu", "Shaocheng", ""], ["Wu", "Qiang", ""], ["Wu", "Yihong", ""], ["Xia", "Yong", ""], ["Xu", "Yanwu", ""], ["Xu", "Xiaowen", ""], ["Xu", "Peiyuan", ""], ["Yang", "Tsai-Ling", ""], ["Yang", "Xiaoping", ""], ["Yang", "Hao-Yu", ""], ["Yang", "Junlin", ""], ["Yang", "Haojin", ""], ["Yang", "Guang", ""], ["Yao", "Hongdou", ""], ["Ye", "Xujiong", ""], ["Yin", "Changchang", ""], ["Young-Moxon", "Brett", ""], ["Yu", "Jinhua", ""], ["Yue", "Xiangyu", ""], ["Zhang", "Songtao", ""], ["Zhang", "Angela", ""], ["Zhang", "Kun", ""], ["Zhang", "Xuejie", ""], ["Zhang", "Lichi", ""], ["Zhang", "Xiaoyue", ""], ["Zhang", "Yazhuo", ""], ["Zhang", "Lei", ""], ["Zhang", "Jianguo", ""], ["Zhang", "Xiang", ""], ["Zhang", "Tianhao", ""], ["Zhao", "Sicheng", ""], ["Zhao", "Yu", ""], ["Zhao", "Xiaomei", ""], ["Zhao", "Liang", ""], ["Zheng", "Yefeng", ""], ["Zhong", "Liming", ""], ["Zhou", "Chenhong", ""], ["Zhou", "Xiaobing", ""], ["Zhou", "Fan", ""], ["Zhu", "Hongtu", ""], ["Zhu", "Jin", ""], ["Zhuge", "Ying", ""], ["Zong", "Weiwei", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Farahani", "Keyvan", ""], ["Davatzikos", "Christos", ""], ["van Leemput", "Koen", ""], ["Menze", "Bjoern", ""]]}, {"id": "1811.02636", "submitter": "Qiuwen Lou", "authors": "Qiuwen Lou, Chenyun Pan, John McGuiness, Andras Horvath, Azad Naeemi,\n  Michael Niemier, X. Sharon Hu", "title": "A mixed signal architecture for convolutional neural networks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) accelerators with improved energy and delay are\ndesirable for meeting the requirements of hardware targeted for IoT and edge\ncomputing systems. Convolutional neural networks (CoNNs) belong to one of the\nmost popular types of DNN architectures. This paper presents the design and\nevaluation of an accelerator for CoNNs. The system-level architecture is based\non mixed-signal, cellular neural networks (CeNNs). Specifically, we present (i)\nthe implementation of different layers, including convolution, ReLU, and\npooling, in a CoNN using CeNN, (ii) modified CoNN structures with CeNN-friendly\nlayers to reduce computational overheads typically associated with a CoNN,\n(iii) a mixed-signal CeNN architecture that performs CoNN computations in the\nanalog and mixed signal domain, and (iv) design space exploration that\nidentifies what CeNN-based algorithm and architectural features fare best\ncompared to existing algorithms and architectures when evaluated over common\ndatasets -- MNIST and CIFAR-10. Notably, the proposed approach can lead to\n8.7$\\times$ improvements in energy-delay product (EDP) per digit classification\nfor the MNIST dataset at iso-accuracy when compared with the state-of-the-art\nDNN engine, while our approach could offer 4.3$\\times$ improvements in EDP when\ncompared to other network implementations for the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 18:51:57 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 16:48:37 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 19:25:57 GMT"}, {"version": "v4", "created": "Thu, 2 May 2019 20:51:40 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Lou", "Qiuwen", ""], ["Pan", "Chenyun", ""], ["McGuiness", "John", ""], ["Horvath", "Andras", ""], ["Naeemi", "Azad", ""], ["Niemier", "Michael", ""], ["Hu", "X. Sharon", ""]]}, {"id": "1811.02639", "submitter": "Zhuwei Qin", "authors": "Zhuwei Qin, Fuxun Yu, ChenChen Liu, Xiang Chen", "title": "Demystifying Neural Network Filter Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on filter magnitude ranking (e.g. L1 norm), conventional filter pruning\nmethods for Convolutional Neural Networks (CNNs) have been proved with great\neffectiveness in computation load reduction. Although effective, these methods\nare rarely analyzed in a perspective of filter functionality. In this work, we\nexplore the filter pruning and the retraining through qualitative filter\nfunctionality interpretation. We find that the filter magnitude based method\nfails to eliminate the filters with repetitive functionality. And the\nretraining phase is actually used to reconstruct the remained filters for\nfunctionality compensation for the wrongly-pruned critical filters. With a\nproposed functionality-oriented pruning method, we further testify that, by\nprecisely addressing the filter functionality redundancy, a CNN can be pruned\nwithout considerable accuracy drop, and the retraining phase is unnecessary.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 23:49:18 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Qin", "Zhuwei", ""], ["Yu", "Fuxun", ""], ["Liu", "ChenChen", ""], ["Chen", "Xiang", ""]]}, {"id": "1811.02642", "submitter": "Aman Rana", "authors": "Aman Rana, Gregory Yauney, Alarice Lowe, Pratik Shah", "title": "Computational Histological Staining and Destaining of Prostate Core\n  Biopsy RGB Images with Generative Adversarial Neural Networks", "comments": "Accepted for publication at 2018 IEEE International Conference on\n  Machine Learning and Applications (ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00133", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Histopathology tissue samples are widely available in two states:\nparaffin-embedded unstained and non-paraffin-embedded stained whole slide RGB\nimages (WSRI). Hematoxylin and eosin stain (H&E) is one of the principal stains\nin histology but suffers from several shortcomings related to tissue\npreparation, staining protocols, slowness and human error. We report two novel\napproaches for training machine learning models for the computational H&E\nstaining and destaining of prostate core biopsy RGB images. The staining model\nuses a conditional generative adversarial network that learns hierarchical\nnon-linear mappings between whole slide RGB image (WSRI) pairs of prostate core\nbiopsy before and after H&E staining. The trained staining model can then\ngenerate computationally H&E-stained prostate core WSRIs using previously\nunseen non-stained biopsy images as input. The destaining model, by learning\nmappings between an H&E stained WSRI and a non-stained WSRI of the same biopsy,\ncan computationally destain previously unseen H&E-stained images. Structural\nand anatomical details of prostate tissue and colors, shapes, geometries,\nlocations of nuclei, stroma, vessels, glands and other cellular components were\ngenerated by both models with structural similarity indices of 0.68 (staining)\nand 0.84 (destaining). The proposed staining and destaining models can engender\ncomputational H&E staining and destaining of WSRI biopsies without additional\nequipment and devices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 17:36:00 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 16:37:49 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rana", "Aman", ""], ["Yauney", "Gregory", ""], ["Lowe", "Alarice", ""], ["Shah", "Pratik", ""]]}, {"id": "1811.02643", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Zhuwei Qin, Xiang Chen", "title": "Distilling Critical Paths in Convolutional Neural Networks", "comments": "Accepted in NIPS18 CDNNRIA workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network compression and acceleration are widely demanded currently due\nto the resource constraints on most deployment targets. In this paper, through\nanalyzing the filter activation, gradients, and visualizing the filters'\nfunctionality in convolutional neural networks, we show that the filters in\nhigher layers learn extremely task-specific features, which are exclusive for\nonly a small subset of the overall tasks, or even a single class. Based on such\nfindings, we reveal the critical paths of information flow for different\nclasses. And by their intrinsic property of exclusiveness, we propose a\ncritical path distillation method, which can effectively customize the\nconvolutional neural networks to small ones with much smaller model size and\nless computation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 00:56:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 06:41:04 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Yu", "Fuxun", ""], ["Qin", "Zhuwei", ""], ["Chen", "Xiang", ""]]}, {"id": "1811.02644", "submitter": "Zefang Zong", "authors": "Zefang Zong, Jie Feng, Kechun Liu, Hongzhi Shi, Yong Li", "title": "DeepDPM: Dynamic Population Mapping via Deep Neural Network", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic high resolution data on human population distribution is of great\nimportance for a wide spectrum of activities and real-life applications, but is\ntoo difficult and expensive to obtain directly. Therefore, generating\nfine-scaled population distributions from coarse population data is of great\nsignificance. However, there are three major challenges: 1) the complexity in\nspatial relations between high and low resolution population; 2) the dependence\nof population distributions on other external information; 3) the difficulty in\nretrieving temporal distribution patterns. In this paper, we first propose the\nidea to generate dynamic population distributions in full-time series, then we\ndesign dynamic population mapping via deep neural network(DeepDPM), a model\nthat describes both spatial and temporal patterns using coarse data and point\nof interest information. In DeepDPM, we utilize super-resolution convolutional\nneural network(SRCNN) based model to directly map coarse data into higher\nresolution data, and a time-embedded long short-term memory model to\neffectively capture the periodicity nature to smooth the finer-scaled results\nfrom the previous static SRCNN model. We perform extensive experiments on a\nreal-life mobile dataset collected from Shanghai. Our results demonstrate that\nDeepDPM outperforms previous state-of-the-art methods and a suite of frequent\ndata-mining approaches. Moreover, DeepDPM breaks through the limitation from\nprevious works in time dimension so that dynamic predictions in all-day time\nslots can be obtained.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:46:38 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 03:24:35 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zong", "Zefang", ""], ["Feng", "Jie", ""], ["Liu", "Kechun", ""], ["Shi", "Hongzhi", ""], ["Li", "Yong", ""]]}, {"id": "1811.02650", "submitter": "Jian Li DR", "authors": "Jian Li", "title": "Visual Attention is Beyond One Single Saliency Map", "comments": "arXiv admin note: text overlap with arXiv:1605.01999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of later years, numerous bottom-up attention models have been proposed on\ndifferent assumptions. However, the produced saliency maps may be different\nfrom each other even from the same input image. We also observe that human\nfixation map varies across time greatly. When people freely view an image, they\ntend to allocate attention at salient regions of large scale at first, and then\nsearch more and more detailed regions. In this paper, we argue that, for one\ninput image visual attention cannot be described by only one single saliency\nmap, and this mechanism should be modeled as a dynamic process. Under the\nfrequency domain paradigm, we proposed a global inhibition model to mimic this\nprocess by suppressing the {\\it non-saliency} in the input image; we also show\nthat the dynamic process is influenced by one parameter in the frequency\ndomain. Experiments illustrate that the proposed model is capable of predicting\nhuman dynamic fixation distribution.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 04:10:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Li", "Jian", ""]]}, {"id": "1811.02651", "submitter": "Emre E\\u{g}riboz", "authors": "Emre E\\u{g}riboz, Furkan Kaynar, Song\\\"ul Varl{\\i} Albayrak, Benan\n  M\\\"usellim, Tuba Sel\\c{c}uk", "title": "Finding and Following of Honeycombing Regions in Computed Tomography\n  Lung Images by Deep Learning", "comments": "4 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, besides the medical treatment methods in medical field,\nComputer Aided Diagnosis (CAD) systems which can facilitate the decision making\nphase of the physician and can detect the disease at an early stage have\nstarted to be used frequently. The diagnosis of Idiopathic Pulmonary Fibrosis\n(IPF) disease by using CAD systems is very important in that it can be followed\nby doctors and radiologists. It has become possible to diagnose and follow up\nthe disease with the help of CAD systems by the development of high resolution\ncomputed imaging scanners and increasing size of computation power. The purpose\nof this project is to design a tool that will help specialists diagnose and\nfollow up the IPF disease by identifying areas of honeycombing and ground glass\npatterns in High Resolution Computed Tomography (HRCT) lung images. Creating a\nprogram module that segments the lung pair and creating a self-learner deep\nlearning model from given Computed Tomography (CT) images for the specific\ndiseased regions thanks to doctors are the main purposes of this work. Through\nthe created model, program module will be able to find special regions in given\nnew CT images. In this study, the performance of lung segmentation was tested\nby the S{\\o}rensen-Dice coefficient method and the mean performance was\nmeasured as 90.7%, testing of the created model was performed with data not\nused in the training stage of the CNN network, and the average performance was\nmeasured as 87.8% for healthy regions, 73.3% for ground-glass areas and 69.1%\nfor honeycombing zones.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:29:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 21:25:09 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 10:24:10 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["E\u011friboz", "Emre", ""], ["Kaynar", "Furkan", ""], ["Albayrak", "Song\u00fcl Varl\u0131", ""], ["M\u00fcsellim", "Benan", ""], ["Sel\u00e7uk", "Tuba", ""]]}, {"id": "1811.02654", "submitter": "Ryan Sherman", "authors": "Ryan Sherman", "title": "A Volumetric Convolutional Neural Network for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain cancer can be very fatal, but chances of survival increase through\nearly detection and treatment. Doctors use Magnetic Resonance Imaging (MRI) to\ndetect and locate tumors in the brain, and very carefully analyze scans to\nsegment brain tumors. Manual segmentation is time consuming and tiring for\ndoctors, and it can be difficult for them to notice extremely small\nabnormalities. Automated segmentations performed by computers offer quicker\ndiagnoses, the ability to notice small details, and more accurate\nsegmentations. Advances in deep learning and computer hardware have allowed for\nhigh-performing automated segmentation approaches. However, several problems\npersist in practice: increased training time, class imbalance, and low\nperformance. In this paper, I propose applying V-Net, a volumetric, fully\nconvolutional neural network, to segment brain tumors in MRI scans from the\nBraTS Challenges. With this approach, I achieve a whole tumor dice score of\n0.89 and train the network in a short time while addressing class imbalance\nwith the use of a dice loss layer. Then, I propose applying an existing\ntechnique to improve automated segmentation performance in practice.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 02:28:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Sherman", "Ryan", ""]]}, {"id": "1811.02656", "submitter": "Titouan Parcollet", "authors": "Titouan Parcollet, Mohamed Morchid, Georges Linar\\`es", "title": "Quaternion Convolutional Neural Networks for Heterogeneous Image\n  Processing", "comments": "Submitted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have recently achieved state-of-the-art\nresults in various applications. In the case of image recognition, an ideal\nmodel has to learn independently of the training data, both local dependencies\nbetween the three components (R,G,B) of a pixel, and the global relations\ndescribing edges or shapes, making it efficient with small or heterogeneous\ndatasets. Quaternion-valued convolutional neural networks (QCNN) solved this\nproblematic by introducing multidimensional algebra to CNN. This paper proposes\nto explore the fundamental reason of the success of QCNN over CNN, by\ninvestigating the impact of the Hamilton product on a color image\nreconstruction task performed from a gray-scale only training. By learning\nindependently both internal and external relations and with less parameters\nthan real valued convolutional encoder-decoder (CAE), quaternion convolutional\nencoder-decoders (QCAE) perfectly reconstructed unseen color images while CAE\nproduced worst and gray-scale versions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 11:22:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Parcollet", "Titouan", ""], ["Morchid", "Mohamed", ""], ["Linar\u00e8s", "Georges", ""]]}, {"id": "1811.02657", "submitter": "Tan Nguyen", "authors": "Tan Nguyen, Nhat Ho, Ankit Patel, Anima Anandkumar, Michael I. Jordan,\n  Richard G. Baraniuk", "title": "A Bayesian Perspective of Convolutional Neural Networks through a\n  Deconvolutional Generative Model", "comments": "Keywords: neural nets, generative models, semi-supervised learning,\n  cross-entropy, statistical guarantees 80 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of Convolutional Neural Networks (CNNs) for\nsupervised prediction in images, we design the Deconvolutional Generative Model\n(DGM), a new probabilistic generative model whose inference calculations\ncorrespond to those in a given CNN architecture. The DGM uses a CNN to design\nthe prior distribution in the probabilistic model. Furthermore, the DGM\ngenerates images from coarse to finer scales. It introduces a small set of\nlatent variables at each scale, and enforces dependencies among all the latent\nvariables via a conjugate prior distribution. This conjugate prior yields a new\nregularizer based on paths rendered in the generative model for training\nCNNs-the Rendering Path Normalization (RPN). We demonstrate that this\nregularizer improves generalization, both in theory and in practice. In\naddition, likelihood estimation in the DGM yields training losses for CNNs, and\ninspired by this, we design a new loss termed as the Max-Min cross entropy\nwhich outperforms the traditional cross-entropy loss for object classification.\nThe Max-Min cross entropy suggests a new deep network architecture, namely the\nMax-Min network, which can learn from less labeled data while maintaining good\nprediction performance. Our experiments demonstrate that the DGM with the RPN\nand the Max-Min architecture exceeds or matches the-state-of-art on benchmarks\nincluding SVHN, CIFAR10, and CIFAR100 for semi-supervised and supervised\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:27:37 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:21:21 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nguyen", "Tan", ""], ["Ho", "Nhat", ""], ["Patel", "Ankit", ""], ["Anandkumar", "Anima", ""], ["Jordan", "Michael I.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1811.02658", "submitter": "George Kesidis", "authors": "Yujia Wang, David J. Miller, George Kesidis", "title": "When Not to Classify: Detection of Reverse Engineering Attacks on DNN\n  Image Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses detection of a reverse engineering (RE) attack targeting\na deep neural network (DNN) image classifier; by querying, RE's aim is to\ndiscover the classifier's decision rule. RE can enable test-time evasion\nattacks, which require knowledge of the classifier. Recently, we proposed a\nquite effective approach (ADA) to detect test-time evasion attacks. In this\npaper, we extend ADA to detect RE attacks (ADA-RE). We demonstrate our method\nis successful in detecting \"stealthy\" RE attacks before they learn enough to\nlaunch effective test-time evasion attacks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:59:49 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wang", "Yujia", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "1811.02659", "submitter": "Aman Rana", "authors": "Perikumar Javia, Aman Rana, Nathan Shapiro, Pratik Shah", "title": "Machine Learning Algorithms for Classification of Microcirculation\n  Images from Septic and Non-Septic Patients", "comments": "Accepted for publication at 2018 IEEE International Conference on\n  Machine Learning and Applications (IEEE ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00097", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sepsis is a life-threatening disease and one of the major causes of death in\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\nautomated diagnosis of sepsis. We report a machine learning classifier capable\nof distinguishing non-septic and septic images from dark field microcirculation\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\nunder the receiver operating characteristics of the classifier was 0.92, the\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\nfeature space of trained classifier were visualized using t-SNE embedding and\nwere separable and distinguished between images from critically ill and\nnon-septic patients. Using an unsupervised convolutional autoencoder,\nindependent of the clinical diagnosis, we also report clustering of learned\nfeatures from a compressed representation associated with healthy images and\nthose with microcirculatory dysfunction. The feature space used by our trained\nclassifier to distinguish between images from septic and non-septic patients\nhas potential diagnostic application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:34:18 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 16:50:47 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Javia", "Perikumar", ""], ["Rana", "Aman", ""], ["Shapiro", "Nathan", ""], ["Shah", "Pratik", ""]]}, {"id": "1811.02661", "submitter": "Trent Kyono", "authors": "Trent Kyono, Fiona J. Gilbert, Mihaela van der Schaar", "title": "MAMMO: A Deep Learning Solution for Facilitating Radiologist-Machine\n  Collaboration in Breast Cancer Diagnosis", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an aging and growing population, the number of women requiring either\nscreening or symptomatic mammograms is increasing. To reduce the number of\nmammograms that need to be read by a radiologist while keeping the diagnostic\naccuracy the same or better than current clinical practice, we develop Man and\nMachine Mammography Oracle (MAMMO) - a clinical decision support system capable\nof triaging mammograms into those that can be confidently classified by a\nmachine and those that cannot be, thus requiring the reading of a radiologist.\nThe first component of MAMMO is a novel multi-view convolutional neural network\n(CNN) with multi-task learning (MTL). MTL enables the CNN to learn the\nradiological assessments known to be associated with cancer, such as breast\ndensity, conspicuity, suspicion, etc., in addition to learning the primary task\nof cancer diagnosis. We show that MTL has two advantages: 1) learning refined\nfeature representations associated with cancer improves the classification\nperformance of the diagnosis task and 2) issuing radiological assessments\nprovides an additional layer of model interpretability that a radiologist can\nuse to debug and scrutinize the diagnoses provided by the CNN. The second\ncomponent of MAMMO is a triage network, which takes as input the radiological\nassessment and diagnostic predictions of the first network's MTL outputs and\ndetermines which mammograms can be correctly and confidently diagnosed by the\nCNN and which mammograms cannot, thus needing to be read by a radiologist.\nResults obtained on a private dataset of 8,162 patients show that MAMMO reduced\nthe number of radiologist readings by 42.8% while improving the overall\ndiagnostic accuracy in comparison to readings done by radiologists alone. We\nanalyze the triage of patients decided by MAMMO to gain a better understanding\nof what unique mammogram characteristics require radiologists' expertise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:45:53 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Kyono", "Trent", ""], ["Gilbert", "Fiona J.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.02662", "submitter": "Guixiang Ma", "authors": "Guixiang Ma, Nesreen K. Ahmed, Ted Willke, Dipanjan Sengupta, Michael\n  W. Cole, Nicholas B. Turk-Browne, Philip S. Yu", "title": "Similarity Learning with Higher-Order Graph Convolutions for Brain\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a similarity metric has gained much attention recently, where the\ngoal is to learn a function that maps input patterns to a target space while\npreserving the semantic distance in the input space. While most related work\nfocused on images, we focus instead on learning a similarity metric for\nneuroimages, such as fMRI and DTI images. We propose an end-to-end similarity\nlearning framework called Higher-order Siamese GCN for multi-subject fMRI data\nanalysis. The proposed framework learns the brain network representations via a\nsupervised metric-based approach with siamese neural networks using two graph\nconvolutional networks as the twin networks. Our proposed framework performs\nhigher-order convolutions by incorporating higher-order proximity in graph\nconvolutional networks to characterize and learn the community structure in\nbrain connectivity networks. To the best of our knowledge, this is the first\ncommunity-preserving similarity learning framework for multi-subject brain\nnetwork analysis. Experimental results on four real fMRI datasets demonstrate\nthe potential use cases of the proposed framework for multi-subject brain\nanalysis in health and neuropsychiatric disorders. Our proposed approach\nachieves an average AUC gain of 75% compared to PCA, an average AUC gain of\n65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared\nto S-GCN across the four datasets, indicating promising application in clinical\ninvestigation and brain disease diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:51:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 03:49:36 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 16:01:11 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 06:16:46 GMT"}, {"version": "v5", "created": "Wed, 1 May 2019 21:10:27 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ma", "Guixiang", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Ted", ""], ["Sengupta", "Dipanjan", ""], ["Cole", "Michael W.", ""], ["Turk-Browne", "Nicholas B.", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.02667", "submitter": "Jakub Nalepa", "authors": "Pablo Ribalta Lorenzo, Lukasz Tulczyjew, Michal Marcinkiewicz, Jakub\n  Nalepa", "title": "Band Selection from Hyperspectral Images Using Attention-based\n  Convolutional Neural Networks", "comments": "This is an initial draft of the paper submitted to IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new attention-based convolutional neural networks for\nselecting bands from hyperspectral images. The proposed approach re-uses\nconvolutional activations at different depths, identifying the most informative\nregions of the spectrum with the help of gating mechanisms. Our attention\ntechniques are modular and easy to implement, and they can be seamlessly\ntrained end-to-end using gradient descent. Our rigorous experiments showed that\ndeep models equipped with the attention mechanism deliver high-quality\nclassification, and repeatedly identify significant bands in the training data,\npermitting the creation of refined and extremely compact sets that retain the\nmost meaningful features.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 19:32:48 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 18:11:11 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 08:50:48 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lorenzo", "Pablo Ribalta", ""], ["Tulczyjew", "Lukasz", ""], ["Marcinkiewicz", "Michal", ""], ["Nalepa", "Jakub", ""]]}, {"id": "1811.02668", "submitter": "Nghia (Andy) Nguyen", "authors": "Hanadi El Achi, Tatiana Belousova, Lei Chen, Amer Wahed, Iris Wang,\n  Zhihong Hu, Zeyad Kanaan, Adan Rios, Andy N.D. Nguyen", "title": "Automated Diagnosis of Lymphoma with Digital Pathology Images Using Deep\n  Learning", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown promising results in using Deep Learning to detect\nmalignancy in whole slide imaging. However, they were limited to just\npredicting positive or negative finding for a specific neoplasm. We attempted\nto use Deep Learning with a convolutional neural network algorithm to build a\nlymphoma diagnostic model for four diagnostic categories: benign lymph node,\ndiffuse large B cell lymphoma, Burkitt lymphoma, and small lymphocytic\nlymphoma. Our software was written in Python language. We obtained digital\nwhole slide images of Hematoxylin and Eosin stained slides of 128 cases\nincluding 32 cases for each diagnostic category. Four sets of 5 representative\nimages, 40x40 pixels in dimension, were taken for each case. A total of 2,560\nimages were obtained from which 1,856 were used for training, 464 for\nvalidation and 240 for testing. For each test set of 5 images, the predicted\ndiagnosis was combined from prediction of 5 images. The test results showed\nexcellent diagnostic accuracy at 95% for image-by-image prediction and at 10%\nfor set-by-set prediction. This preliminary study provided a proof of concept\nfor incorporating automated lymphoma diagnostic screen into future pathology\nworkflow to augment the pathologists' productivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:40:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Achi", "Hanadi El", ""], ["Belousova", "Tatiana", ""], ["Chen", "Lei", ""], ["Wahed", "Amer", ""], ["Wang", "Iris", ""], ["Hu", "Zhihong", ""], ["Kanaan", "Zeyad", ""], ["Rios", "Adan", ""], ["Nguyen", "Andy N. D.", ""]]}, {"id": "1811.02682", "submitter": "Bin Song", "authors": "Xu Kang, Bin Song, Jie Guo, Xiaojiang Du and Mohsen Guizani", "title": "Attention-Mechanism-based Tracking Method for Intelligent Internet of\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle tracking task plays an important role on the internet of vehicles and\nintelligent transportation system. Beyond the traditional GPS sensor, the image\nsensor can capture different kinds of vehicles, analyze their driving situation\nand can interact with them. Aiming at the problem that the traditional\nconvolutional neural network is vulnerable to background interference, this\npaper proposes vehicle tracking method based on human attention mechanism for\nself-selection of deep features with an inter-channel fully connected layer. It\nmainly includes the following contents: 1) A fully convolutional neural network\nfused attention mechanism with the selection of the deep features for\nconvolution. 2) A separation method for template and semantic background region\nto separate target vehicles from the background in the initial frame\nadaptively. 3) A two-stage method for model training using our traffic dataset.\nThe experimental results show that the proposed method improves the tracking\naccuracy without an increase in tracking time. Meanwhile, it strengthens the\nrobustness of algorithm under the condition of the complex background region.\nThe success rate of the proposed method in overall traffic datasets is higher\nthan Siamese network by about 10 percent, and the overall precision is higher\nthan Siamese network by 8 percent.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 04:13:25 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Kang", "Xu", ""], ["Song", "Bin", ""], ["Guo", "Jie", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1811.02688", "submitter": "Le Zhang", "authors": "Le Zhang, Ali Gooya, Marco Pereanez, Bo Dong, Stefan K. Piechnik,\n  Stefan Neubauer, Steffen E. Petersen, Alejandro F. Frangi", "title": "Automatic Assessment of Full Left Ventricular Coverage in Cardiac Cine\n  Magnetic Resonance Imaging with Fisher-Discriminative 3D CNN", "comments": "12 pages, 5 figures, accepted by IEEE Transactions on Biomedical\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac magnetic resonance (CMR) images play a growing role in the diagnostic\nimaging of cardiovascular diseases. Full coverage of the left ventricle (LV),\nfrom base to apex, is a basic criterion for CMR image quality and necessary for\naccurate measurement of cardiac volume and functional assessment. Incomplete\ncoverage of the LV is identified through visual inspection, which is\ntime-consuming and usually done retrospectively in the assessment of large\nimaging cohorts. This paper proposes a novel automatic method for determining\nLV coverage from CMR images by using Fisher-discriminative three-dimensional\n(FD3D) convolutional neural networks (CNNs). In contrast to our previous method\nemploying 2D CNNs, this approach utilizes spatial contextual information in CMR\nvolumes, extracts more representative high-level features and enhances the\ndiscriminative capacity of the baseline 2D CNN learning framework, thus\nachieving superior detection accuracy. A two-stage framework is proposed to\nidentify missing basal and apical slices in measurements of CMR volume. First,\nthe FD3D CNN extracts high-level features from the CMR stacks. These image\nrepresentations are then used to detect the missing basal and apical slices.\nCompared to the traditional 3D CNN strategy, the proposed FD3D CNN minimizes\nwithin-class scatter and maximizes between-class scatter. We performed\nextensive experiments to validate the proposed method on more than 5,000\nindependent volumetric CMR scans from the UK Biobank study, achieving low error\nrates for missing basal/apical slice detection (4.9\\%/4.6\\%). The proposed\nmethod can also be adopted for assessing LV coverage for other types of CMR\nimage data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:07:09 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 12:00:14 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhang", "Le", ""], ["Gooya", "Ali", ""], ["Pereanez", "Marco", ""], ["Dong", "Bo", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Petersen", "Steffen E.", ""], ["Frangi", "Alejandro F.", ""]]}, {"id": "1811.02689", "submitter": "Kentaro Yoshioka", "authors": "Kentaro Yoshioka, Edward Lee, Mark Horowitz", "title": "Training Domain Specific Models for Energy-Efficient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end framework for training domain specific models (DSMs)\nto obtain both high accuracy and computational efficiency for object detection\ntasks. DSMs are trained with distillation \\cite{hinton2015distilling} and focus\non achieving high accuracy at a limited domain (e.g. fixed view of an\nintersection). We argue that DSMs can capture essential features well even with\na small model size, enabling higher accuracy and efficiency than traditional\ntechniques. In addition, we improve the training efficiency by reducing the\ndataset size by culling easy to classify images from the training set. For the\nlimited domain, we observed that compact DSMs significantly surpass the\naccuracy of COCO trained models of the same size. By training on a compact\ndataset, we show that with an accuracy drop of only 3.6\\%, the training time\ncan be reduced by 93\\%. The codes are uploaded in\nhttps://github.com/kentaroy47/training-domain-specific-models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:07:54 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 06:13:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yoshioka", "Kentaro", ""], ["Lee", "Edward", ""], ["Horowitz", "Mark", ""]]}, {"id": "1811.02740", "submitter": "Rui Zhang", "authors": "Rui Zhang, Sheng Tang, Yu Li, Junbo Guo, Yongdong Zhang, Jintao Li,\n  Shuicheng Yan", "title": "Style Separation and Synthesis via Generative Adversarial Networks", "comments": null, "journal-ref": "The 26th ACM international conference on Multimedia (ACM MM),\n  2018, pp. 183-191", "doi": "10.1145/3240508.3240524", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style synthesis attracts great interests recently, while few works focus on\nits dual problem \"style separation\". In this paper, we propose the Style\nSeparation and Synthesis Generative Adversarial Network (S3-GAN) to\nsimultaneously implement style separation and style synthesis on object\nphotographs of specific categories. Based on the assumption that the object\nphotographs lie on a manifold, and the contents and styles are independent, we\nemploy S3-GAN to build mappings between the manifold and a latent vector space\nfor separating and synthesizing the contents and styles. The S3-GAN consists of\nan encoder network, a generator network, and an adversarial network. The\nencoder network performs style separation by mapping an object photograph to a\nlatent vector. Two halves of the latent vector represent the content and style,\nrespectively. The generator network performs style synthesis by taking a\nconcatenated vector as input. The concatenated vector contains the style half\nvector of the style target image and the content half vector of the content\ntarget image. Once obtaining the images from the generator network, an\nadversarial network is imposed to generate more photo-realistic images.\nExperiments on CelebA and UT Zappos 50K datasets demonstrate that the S3-GAN\nhas the capacity of style separation and synthesis simultaneously, and could\ncapture various styles in a single model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 02:58:46 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zhang", "Rui", ""], ["Tang", "Sheng", ""], ["Li", "Yu", ""], ["Guo", "Junbo", ""], ["Zhang", "Yongdong", ""], ["Li", "Jintao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1811.02744", "submitter": "Zhizhong Han", "authors": "Zhizhong Han, Mingyang Shang, Yu-Shen Liu, Matthias Zwicker", "title": "View Inter-Prediction GAN: Unsupervised Representation Learning for 3D\n  Shapes by Learning Global Shape Memories to Support Local View Predictions", "comments": "To be published at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel unsupervised representation learning\napproach for 3D shapes, which is an important research challenge as it avoids\nthe manual effort required for collecting supervised data. Our method trains an\nRNN-based neural network architecture to solve multiple view inter-prediction\ntasks for each shape. Given several nearby views of a shape, we define view\ninter-prediction as the task of predicting the center view between the input\nviews, and reconstructing the input views in a low-level feature space. The key\nidea of our approach is to implement the shape representation as a\nshape-specific global memory that is shared between all local view\ninter-predictions for each shape. Intuitively, this memory enables the system\nto aggregate information that is useful to better solve the view\ninter-prediction tasks for each shape, and to leverage the memory as a\nview-independent shape representation. Our approach obtains the best results\nusing a combination of L_2 and adversarial losses for the view inter-prediction\ntask. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised\n3D feature learning on three large scale 3D shape benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 03:25:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Han", "Zhizhong", ""], ["Shang", "Mingyang", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1811.02745", "submitter": "Zhizhong Han", "authors": "Zhizhong Han, Mingyang Shang, Xiyang Wang, Yu-Shen Liu, Matthias\n  Zwicker", "title": "Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by\n  Joint Reconstruction and Prediction of View and Word Sequences", "comments": "To be pubilished at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent method employs 3D voxels to represent 3D shapes, but this limits the\napproach to low resolutions due to the computational cost caused by the cubic\ncomplexity of 3D voxels. Hence the method suffers from a lack of detailed\ngeometry. To resolve this issue, we propose Y^2Seq2Seq, a view-based model, to\nlearn cross-modal representations by joint reconstruction and prediction of\nview and word sequences. Specifically, the network architecture of Y^2Seq2Seq\nbridges the semantic meaning embedded in the two modalities by two coupled `Y'\nlike sequence-to-sequence (Seq2Seq) structures. In addition, our novel\nhierarchical constraints further increase the discriminability of the\ncross-modal representations by employing more detailed discriminative\ninformation. Experimental results on cross-modal retrieval and 3D shape\ncaptioning show that Y^2Seq2Seq outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 03:33:03 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Han", "Zhizhong", ""], ["Shang", "Mingyang", ""], ["Wang", "Xiyang", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1811.02746", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Sabesan Sivapalan, Sridha Sridharan,\n  Clinton Fookes and Sandra Mau", "title": "Component-based Attention for Large-scale Trademark Retrieval", "comments": "Fix typos related to authors' information", "journal-ref": null, "doi": "10.1109/TIFS.2019.2959921", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for large-scale trademark retrieval (TR) systems has significantly\nincreased to combat the rise in international trademark infringement.\nUnfortunately, the ranking accuracy of current approaches using either\nhand-crafted or pre-trained deep convolution neural network (DCNN) features is\ninadequate for large-scale deployments. We show in this paper that the ranking\naccuracy of TR systems can be significantly improved by incorporating hard and\nsoft attention mechanisms, which direct attention to critical information such\nas figurative elements and reduce attention given to distracting and\nuninformative elements such as text and background. Our proposed approach\nachieves state-of-the-art results on a challenging large-scale trademark\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 03:33:28 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 06:25:55 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Sivapalan", "Sabesan", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""], ["Mau", "Sandra", ""]]}, {"id": "1811.02759", "submitter": "Yuenan Hou", "authors": "Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy", "title": "Learning to Steer by Mimicking Features from Heterogeneous Auxiliary\n  Networks", "comments": "8 pages, 6 figures; Accepted by AAAI 2019; Our project page is\n  available at https://cardwing.github.io/projects/FM-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of many existing end-to-end steering angle prediction models\nheavily relies on steering angles as the supervisory signal. Without learning\nfrom much richer contexts, these methods are susceptible to the presence of\nsharp road curves, challenging traffic conditions, strong shadows, and severe\nlighting changes. In this paper, we considerably improve the accuracy and\nrobustness of predictions through heterogeneous auxiliary networks feature\nmimicking, a new and effective training method that provides us with much\nricher contextual signals apart from steering direction. Specifically, we train\nour steering angle predictive model by distilling multi-layer knowledge from\nmultiple heterogeneous auxiliary networks that perform related but different\ntasks, e.g., image segmentation or optical flow estimation. As opposed to\nmulti-task learning, our method does not require expensive annotations of\nrelated tasks on the target set. This is made possible by applying contemporary\noff-the-shelf networks on the target set and mimicking their features in\ndifferent layers after transformation. The auxiliary networks are discarded\nafter training without affecting the runtime efficiency of our model. Our\napproach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming\nthe previous best by a large margin of 12.8% and 52.1%, respectively.\nEncouraging results are also shown on Berkeley Deep Drive (BDD) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:47:25 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Hou", "Yuenan", ""], ["Ma", "Zheng", ""], ["Liu", "Chunxiao", ""], ["Loy", "Chen Change", ""]]}, {"id": "1811.02765", "submitter": "Xin Wang", "authors": "Xin Wang, Jiawei Wu, Da Zhang, Yu Su, William Yang Wang", "title": "Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video\n  Captioning", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although promising results have been achieved in video captioning, existing\nmodels are limited to the fixed inventory of activities in the training corpus,\nand do not generalize to open vocabulary scenarios. Here we introduce a novel\ntask, zero-shot video captioning, that aims at describing out-of-domain videos\nof unseen activities. Videos of different activities usually require different\ncaptioning strategies in many aspects, i.e. word selection, semantic\nconstruction, and style expression etc, which poses a great challenge to depict\nnovel activities without paired training data. But meanwhile, similar\nactivities share some of those aspects in common. Therefore, We propose a\nprincipled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video\ncaptioning, which learns to compose different experts based on different topic\nembeddings, implicitly transferring the knowledge learned from seen activities\nto unseen ones. Besides, we leverage external topic-related text corpus to\nconstruct the topic embedding for each activity, which embodies the most\nrelevant semantic vectors within the topic. Empirical results not only validate\nthe effectiveness of our method in utilizing semantic knowledge for video\ncaptioning, but also show its strong generalization ability when describing\nnovel activities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:33:07 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 23:22:19 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Jiawei", ""], ["Zhang", "Da", ""], ["Su", "Yu", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.02793", "submitter": "Jin Huang", "authors": "Gui-Song Xia, Jin Huang, Nan Xue, Qikai Lu, Xiaoxiang Zhu", "title": "GeoSay: A Geometric Saliency for Extracting Buildings in Remote Sensing\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of buildings in remote sensing images is an important\nbut challenging task and finds many applications in different fields such as\nurban planning, navigation and so on. This paper addresses the problem of\nbuildings extraction in very high-spatial-resolution (VHSR) remote sensing (RS)\nimages, whose spatial resolution is often up to half meters and provides rich\ninformation about buildings. Based on the observation that buildings in VHSR-RS\nimages are always more distinguishable in geometry than in texture or spectral\ndomain, this paper proposes a geometric building index (GBI) for accurate\nbuilding extraction, by computing the geometric saliency from VHSR-RS images.\nMore precisely, given an image, the geometric saliency is derived from a\nmid-level geometric representations based on meaningful junctions that can\nlocally describe geometrical structures of images. The resulting GBI is finally\nmeasured by integrating the derived geometric saliency of buildings.\nExperiments on three public and commonly used datasets demonstrate that the\nproposed GBI achieves the state-of-the-art performance and shows impressive\ngeneralization capability. Additionally, GBI preserves both the exact position\nand accurate shape of single buildings compared to existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:12:24 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Xia", "Gui-Song", ""], ["Huang", "Jin", ""], ["Xue", "Nan", ""], ["Lu", "Qikai", ""], ["Zhu", "Xiaoxiang", ""]]}, {"id": "1811.02796", "submitter": "Chengchao Shen", "authors": "Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, Mingli Song", "title": "Amalgamating Knowledge towards Comprehensive Classification", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep learning, there have been an\nunprecedentedly large number of trained deep network models available online.\nReusing such trained models can significantly reduce the cost of training the\nnew models from scratch, if not infeasible at all as the annotations used for\nthe training original networks are often unavailable to public. We propose in\nthis paper to study a new model-reusing task, which we term as \\emph{knowledge\namalgamation}. Given multiple trained teacher networks, each of which\nspecializes in a different classification problem, the goal of knowledge\namalgamation is to learn a lightweight student model capable of handling the\ncomprehensive classification. We assume no other annotations except the outputs\nfrom the teacher models are available, and thus focus on extracting and\namalgamating knowledge from the multiple teachers. To this end, we propose a\npilot two-step strategy to tackle the knowledge amalgamation task, by learning\nfirst the compact feature representations from teachers and then the network\nparameters in a layer-wise manner so as to build the student model. We apply\nthis approach to four public datasets and obtain very encouraging results: even\nwithout any human annotation, the obtained student model is competent to handle\nthe comprehensive classification task and in most cases outperforms the\nteachers in individual sub-tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:21:39 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 06:10:29 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Shen", "Chengchao", ""], ["Wang", "Xinchao", ""], ["Song", "Jie", ""], ["Sun", "Li", ""], ["Song", "Mingli", ""]]}, {"id": "1811.02797", "submitter": "Lucian Itu", "authors": "Costin Ciusdel, Alexandru Turcea, Andrei Puiu, Lucian Itu, Lucian\n  Calmac, Emma Weiss, Cornelia Margineanu, Elisabeta Badila, Martin Berger,\n  Thomas Redel, Tiziano Passerini, Mehmet Gulsun, Puneet Sharma", "title": "Deep Neural Networks for ECG-free Cardiac Phase and End-Diastolic Frame\n  Detection on Coronary Angiographies", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive coronary angiography (ICA) is the gold standard in Coronary Artery\nDisease (CAD) imaging. Detection of the end-diastolic frame (EDF) and, in\ngeneral, cardiac phase detection on each temporal frame of a coronary\nangiography acquisition is of significant importance for the anatomical and\nnon-invasive functional assessment of CAD. This task is generally performed via\nmanual frame selection or semi-automated selection based on simultaneously\nacquired ECG signals - thus introducing the requirement of simultaneous ECG\nrecordings. We evaluate the performance of a purely image based workflow based\non deep neural networks for fully automated cardiac phase and EDF detection on\ncoronary angiographies. A first deep neural network (DNN), trained to detect\ncoronary arteries, is employed to preselect a subset of frames in which\ncoronary arteries are well visible. A second DNN predicts cardiac phase labels\nfor each frame. Only in the training and evaluation phases for the second DNN,\nECG signals are used to provide ground truth labels for each angiographic\nframe. The networks were trained on 17800 coronary angiographies from 3900\npatients and evaluated on 27900 coronary angiographies from 6250 patients. No\nexclusion criteria related to patient state, previous interventions, or\npathology were formulated. Cardiac phase detection had an accuracy of 97.6%, a\nsensitivity of 97.6% and a specificity of 97.5% on the evaluation set. EDF\nprediction had a precision of 97.4% and a recall of 96.9%. Several sub-group\nanalyses were performed, indicating that the cardiac phase detection\nperformance is largely independent from acquisition angles and the heart rate\nof the patient. The average execution time of cardiac phase detection for one\nangiographic series was on average less than five seconds on a standard\nworkstation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:23:59 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Ciusdel", "Costin", ""], ["Turcea", "Alexandru", ""], ["Puiu", "Andrei", ""], ["Itu", "Lucian", ""], ["Calmac", "Lucian", ""], ["Weiss", "Emma", ""], ["Margineanu", "Cornelia", ""], ["Badila", "Elisabeta", ""], ["Berger", "Martin", ""], ["Redel", "Thomas", ""], ["Passerini", "Tiziano", ""], ["Gulsun", "Mehmet", ""], ["Sharma", "Puneet", ""]]}, {"id": "1811.02804", "submitter": "Qingnan Fan", "authors": "Qingnan Fan, Jiaolong Yang, David Wipf, Baoquan Chen, Xin Tong", "title": "Image Smoothing via Unsupervised Learning", "comments": "Accepted in SIGGRAPH Asia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing represents a fundamental component of many disparate computer\nvision and graphics applications. In this paper, we present a unified\nunsupervised (label-free) learning framework that facilitates generating\nflexible and high-quality smoothing effects by directly learning from data\nusing deep convolutional neural networks (CNNs). The heart of the design is the\ntraining signal as a novel energy function that includes an edge-preserving\nregularizer which helps maintain important yet potentially vulnerable image\nstructures, and a spatially-adaptive Lp flattening criterion which imposes\ndifferent forms of regularization onto different image regions for better\nsmoothing quality. We implement a diverse set of image smoothing solutions\nemploying the unified framework targeting various applications such as, image\nabstraction, pencil sketching, detail enhancement, texture removal and\ncontent-aware image manipulation, and obtain results comparable with or better\nthan previous methods. Moreover, our method is extremely fast with a modern GPU\n(e.g, 200 fps for 1280x720 images). Our codes and model are released in\nhttps://github.com/fqnchina/ImageSmoothing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:08:05 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Fan", "Qingnan", ""], ["Yang", "Jiaolong", ""], ["Wipf", "David", ""], ["Chen", "Baoquan", ""], ["Tong", "Xin", ""]]}, {"id": "1811.02805", "submitter": "Yukun Tian", "authors": "Yukun Tian and Yiming Lei and Junping Zhang and James Z. Wang", "title": "PaDNet: Pan-Density Crowd Counting", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2952083", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting crowds in varying density scenes or in different\ndensity regions of the same scene, named as pan-density crowd counting, is\nhighly challenging. Previous methods are designed for single density scenes or\ndo not fully utilize pan-density information. We propose a novel framework, the\nPan-Density Network (PaDNet), for pan-density crowd counting. In order to\neffectively capture pan-density information, PaDNet has a novel module, the\nDensity-Aware Network (DAN), that contains multiple sub-networks pretrained on\nscenarios with different densities. Further, a module named the Feature\nEnhancement Layer (FEL) is proposed to aggregate the feature maps learned by\nDAN. It learns an enhancement rate or a weight for each feature map to boost\nthese feature maps. Further, we propose two refined metrics, Patch MAE (PMAE)\nand Patch RMSE (PRMSE), for better evaluating the model performance on\npan-density scenarios. Extensive experiments on four crowd counting benchmark\ndatasets indicate that PaDNet achieves state-of-the-art recognition performance\nand high robustness in pan-density crowd counting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:10:47 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 11:27:35 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 04:57:29 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Tian", "Yukun", ""], ["Lei", "Yiming", ""], ["Zhang", "Junping", ""], ["Wang", "James Z.", ""]]}, {"id": "1811.02840", "submitter": "David Tellez", "authors": "David Tellez, Geert Litjens, Jeroen van der Laak, Francesco Ciompi", "title": "Neural Image Compression for Gigapixel Histopathology Image Analysis", "comments": "Accepted in the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence journal", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2936841", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Image Compression (NIC), a two-step method to build\nconvolutional neural networks for gigapixel image analysis solely using weak\nimage-level labels. First, gigapixel images are compressed using a neural\nnetwork trained in an unsupervised fashion, retaining high-level information\nwhile suppressing pixel-level noise. Second, a convolutional neural network\n(CNN) is trained on these compressed image representations to predict\nimage-level labels, avoiding the need for fine-grained manual annotations. We\ncompared several encoding strategies, namely reconstruction error minimization,\ncontrastive training and adversarial feature learning, and evaluated NIC on a\nsynthetic task and two public histopathology datasets. We found that NIC can\nexploit visual cues associated with image-level labels successfully,\nintegrating both global and local visual information. Furthermore, we\nvisualized the regions of the input gigapixel images where the CNN attended to,\nand confirmed that they overlapped with annotations from human experts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:29:34 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:25:07 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Tellez", "David", ""], ["Litjens", "Geert", ""], ["van der Laak", "Jeroen", ""], ["Ciompi", "Francesco", ""]]}, {"id": "1811.02910", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Sungmin Eum, Heesung Kwon", "title": "DOD-CNN: Doubly-injecting Object Information for Event Recognition", "comments": "ICASSP 2019, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing an event in an image can be enhanced by detecting relevant\nobjects in two ways: 1) indirectly utilizing object detection information\nwithin the unified architecture or 2) directly making use of the object\ndetection output results. We introduce a novel approach, referred to as\nDoubly-injected Object Detection CNN (DOD-CNN), exploiting the object\ninformation in both ways for the task of event recognition. The structure of\nthis network is inspired by the Integrated Object Detection CNN (IOD-CNN) where\nobject information is indirectly exploited by the event recognition module\nthrough the shared portion of the network. In the DOD-CNN architecture, the\nintermediate object detection outputs are directly injected into the event\nrecognition network while keeping the indirect sharing structure inherited from\nthe IOD-CNN, thus being `doubly-injected'. We also introduce a batch pooling\nlayer which constructs one representative feature map from multiple object\nhypotheses. We have demonstrated the effectiveness of injecting the object\ndetection information in two different ways in the task of malicious event\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 14:44:17 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 18:42:41 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1811.02928", "submitter": "Dongdong Chen", "authors": "Dongdong Hou and Weiming Zhang and Jiayang Liu and Siyan Zhou and\n  Dongdong Chen and Nenghai Yu", "title": "Emerging Applications of Reversible Data Hiding", "comments": "ICIGP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible data hiding (RDH) is one special type of information hiding, by\nwhich the host sequence as well as the embedded data can be both restored from\nthe marked sequence without loss. Beside media annotation and integrity\nauthentication, recently some scholars begin to apply RDH in many other fields\ninnovatively. In this paper, we summarize these emerging applications,\nincluding steganography, adversarial example, visual transformation, image\nprocessing, and give out the general frameworks to make these operations\nreversible. As far as we are concerned, this is the first paper to summarize\nthe extended applications of RDH.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:18:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Hou", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Liu", "Jiayang", ""], ["Zhou", "Siyan", ""], ["Chen", "Dongdong", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.02942", "submitter": "Shahab Aslani", "authors": "Shahab Aslani, Michael Dayan, Loredana Storelli, Massimo Filippi,\n  Vittorio Murino, Maria A Rocca, Diego Sona", "title": "Multi-branch Convolutional Neural Network for Multiple Sclerosis Lesion\n  Segmentation", "comments": "This paper has been accepted for publication in NeuroImage", "journal-ref": null, "doi": "10.1016/j.neuroimage.2019.03.068", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an automated approach for segmenting multiple\nsclerosis (MS) lesions from multi-modal brain magnetic resonance images. Our\nmethod is based on a deep end-to-end 2D convolutional neural network (CNN) for\nslice-based segmentation of 3D volumetric data. The proposed CNN includes a\nmulti-branch downsampling path, which enables the network to encode information\nfrom multiple modalities separately. Multi-scale feature fusion blocks are\nproposed to combine feature maps from different modalities at different stages\nof the network. Then, multi-scale feature upsampling blocks are introduced to\nupsize combined feature maps to leverage information from lesion shape and\nlocation. We trained and tested the proposed model using orthogonal plane\norientations of each 3D modality to exploit the contextual information in all\ndirections. The proposed pipeline is evaluated on two different datasets: a\nprivate dataset including 37 MS patients and a publicly available dataset known\nas the ISBI 2015 longitudinal MS lesion segmentation challenge dataset,\nconsisting of 14 MS patients. Considering the ISBI challenge, at the time of\nsubmission, our method was amongst the top performing solutions. On the private\ndataset, using the same array of performance metrics as in the ISBI challenge,\nthe proposed approach shows high improvements in MS lesion segmentation\ncompared with other publicly available tools.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:42:57 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 13:10:39 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 21:30:40 GMT"}, {"version": "v4", "created": "Mon, 8 Apr 2019 17:12:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Aslani", "Shahab", ""], ["Dayan", "Michael", ""], ["Storelli", "Loredana", ""], ["Filippi", "Massimo", ""], ["Murino", "Vittorio", ""], ["Rocca", "Maria A", ""], ["Sona", "Diego", ""]]}, {"id": "1811.02946", "submitter": "Imanol Luengo", "authors": "Imanol Luengo, Evangello Flouty, Petros Giataganas, Piyamate\n  Wisanuvej, Jean Nehme, Danail Stoyanov", "title": "SurReal: enhancing Surgical simulation Realism using style transfer", "comments": null, "journal-ref": "BMVC 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical simulation is an increasingly important element of surgical\neducation. Using simulation can be a means to address some of the significant\nchallenges in developing surgical skills with limited time and resources. The\nphoto-realistic fidelity of simulations is a key feature that can improve the\nexperience and transfer ratio of trainees. In this paper, we demonstrate how we\ncan enhance the visual fidelity of existing surgical simulation by performing\nstyle transfer of multi-class labels from real surgical video onto synthetic\ncontent. We demonstrate our approach on simulations of cataract surgery using\nreal data labels from an existing public dataset. Our results highlight the\nfeasibility of the approach and also the powerful possibility to extend this\ntechnique to incorporate additional temporal constraints and to different\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:49:09 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Luengo", "Imanol", ""], ["Flouty", "Evangello", ""], ["Giataganas", "Petros", ""], ["Wisanuvej", "Piyamate", ""], ["Nehme", "Jean", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1811.02949", "submitter": "Roshanak Zakizadeh", "authors": "Roshanak Zakizadeh, Yu Qian, Michele Sasdelli and Eduard Vazquez", "title": "Instance Retrieval at Fine-grained Level Using Multi-Attribute\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for instance ranking and retrieval at\nfine-grained level based on the global features extracted from a\nmulti-attribute recognition model which is not dependent on landmarks\ninformation or part-based annotations. Further, we make this architecture\nsuitable for mobile-device application by adopting the bilinear CNN to make the\nmulti-attribute recognition model smaller (in terms of the number of\nparameters). The experiments run on the Dress category of DeepFashion In-Shop\nClothes Retrieval and CUB200 datasets show that the results of instance\nretrieval at fine-grained level are promising for these datasets, specially in\nterms of texture and color.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:54:23 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zakizadeh", "Roshanak", ""], ["Qian", "Yu", ""], ["Sasdelli", "Michele", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1811.03014", "submitter": "Adrienne Mendrik", "authors": "Adrienne M. Mendrik, Stephen R. Aylward", "title": "Beyond the Leaderboard: Insight and Deployment Challenges to Address\n  Research Problems", "comments": "This two-page abstract was accepted for the NIPS 2018 Challenges in\n  Machine Learning (CiML) workshop \"Machine Learning competitions \"in the\n  wild\": Playing in the real world or in real time\" on Saturday December 8,\n  2018 in Palais des congres de Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical image analysis field, organizing challenges with associated\nworkshops at international conferences began in 2007 and has grown to include\nover 150 challenges. Several of these challenges have had a major impact in the\nfield. However, whereas well-designed challenges have the potential to unite\nand focus the field on creating solutions to important problems, poorly\ndesigned and documented challenges can equally impede a field and lead to\npursuing incremental improvements in metric scores with no theoretic or\nclinical significance. This is supported by a critical assessment of challenges\nat the international MICCAI conference. In this assessment the main observation\nwas that small changes to the underlying challenge data can drastically change\nthe ranking order on the leaderboard. Related to this is the practice of\nleaderboard climbing, which is characterized by participants focusing on\nincrementally improving metric results rather than advancing science or solving\nthe driving problem of a challenge. In this abstract we look beyond the\nleaderboard of a challenge and instead look at the conclusions that can be\ndrawn from a challenge with respect to the research problem that it is\naddressing. Research study design is well described in other research areas and\ncan be translated to challenge design when viewing challenges as research\nstudies on algorithm performance that address a research problem. Based on the\ntwo main types of scientific research study design, we propose two main\nchallenge types, which we think would benefit other research areas as well: 1)\nan insight challenge that is based on a qualitative study design and 2) a\ndeployment challenge that is based on a quantitative study design. In addition\nwe briefly touch upon related considerations with respect to statistical\nsignificance versus practical significance, generalizability and data\nsaturation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 10:18:07 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mendrik", "Adrienne M.", ""], ["Aylward", "Stephen R.", ""]]}, {"id": "1811.03032", "submitter": "Ahmad Khaliq", "authors": "Ahmad Khaliq, Shoaib Ehsan, Zetao Chen, Michael Milford, Klaus\n  McDonald-Maier", "title": "A Holistic Visual Place Recognition Approach using Lightweight CNNs for\n  Significant ViewPoint and Appearance Changes", "comments": "Conditionally Accepted as short paper at IEEE Transactions on\n  Robotics (T-RO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a lightweight visual place recognition approach, capable\nof achieving high performance with low computational cost, and feasible for\nmobile robotics under significant viewpoint and appearance changes. Results on\nseveral benchmark datasets confirm an average boost of 13% in accuracy, and 12x\naverage speedup relative to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:33:52 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 20:05:14 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 00:27:38 GMT"}, {"version": "v4", "created": "Sun, 27 Oct 2019 14:59:19 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Khaliq", "Ahmad", ""], ["Ehsan", "Shoaib", ""], ["Chen", "Zetao", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1811.03055", "submitter": "Gautam Bhattacharya", "authors": "Gautam Bhattacharya, Jahangir Alam, Patrick Kenny", "title": "Adapting End-to-End Neural Speaker Verification to New Languages and\n  Recording Conditions with Adversarial Training", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a novel approach for adapting speaker embeddings\nto new domains based on adversarial training of neural networks. We apply our\nembeddings to the task of text-independent speaker verification, a challenging,\nreal-world problem in biometric security. We further the development of\nend-to-end speaker embedding models by combing a novel 1-dimensional,\nself-attentive residual network, an angular margin loss function and\nadversarial training strategy. Our model is able to learn extremely compact,\n64-dimensional speaker embeddings that deliver competitive performance on a\nnumber of popular datasets using simple cosine distance scoring. One the\nNIST-SRE 2016 task we are able to beat a strong i-vector baseline, while on the\nSpeakers in the Wild task our model was able to outperform both i-vector and\nx-vector baselines, showing an absolute improvement of 2.19% over the latter.\nAdditionally, we show that the integration of adversarial training consistently\nleads to a significant improvement over an unadapted model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:15:34 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Bhattacharya", "Gautam", ""], ["Alam", "Jahangir", ""], ["Kenny", "Patrick", ""]]}, {"id": "1811.03060", "submitter": "Raphael Tang", "authors": "Raphael Tang, Ashutosh Adhikari, Jimmy Lin", "title": "FLOPs as a Direct Optimization Objective for Learning Sparse Neural\n  Networks", "comments": "4 pages, accepted to the NIPS 2018 Workshop on Compact Deep Neural\n  Networks with Industrial Applications (CDNNRIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a plethora of techniques for inducing structured sparsity in\nparametric models during the optimization process, with the final goal of\nresource-efficient inference. However, few methods target a specific number of\nfloating-point operations (FLOPs) as part of the optimization objective,\ndespite many reporting FLOPs as part of the results. Furthermore, a\none-size-fits-all approach ignores realistic system constraints, which differ\nsignificantly between, say, a GPU and a mobile phone -- FLOPs on the former\nincur less latency than on the latter; thus, it is important for practitioners\nto be able to specify a target number of FLOPs during model compression. In\nthis work, we extend a state-of-the-art technique to directly incorporate FLOPs\nas part of the optimization objective and show that, given a desired FLOPs\nrequirement, different neural networks can be successfully trained for image\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:21:25 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 18:20:10 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Tang", "Raphael", ""], ["Adhikari", "Ashutosh", ""], ["Lin", "Jimmy", ""]]}, {"id": "1811.03063", "submitter": "Gautam Bhattacharya", "authors": "Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, Patrick Kenny", "title": "Generative Adversarial Speaker Embedding Networks for Domain Robust\n  End-to-End Speaker Verification", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a novel approach for learning domain-invariant speaker\nembeddings using Generative Adversarial Networks. The main idea is to confuse a\ndomain discriminator so that is can't tell if embeddings are from the source or\ntarget domains. We train several GAN variants using our proposed framework and\napply them to the speaker verification task. On the challenging NIST-SRE 2016\ndataset, we are able to match the performance of a strong baseline x-vector\nsystem. In contrast to the the baseline systems which are dependent on\ndimensionality reduction (LDA) and an external classifier (PLDA), our proposed\nspeaker embeddings can be scored using simple cosine distance. This is achieved\nby optimizing our models end-to-end, using an angular margin loss function.\nFurthermore, we are able to significantly boost verification performance by\naveraging our different GAN models at the score level, achieving a relative\nimprovement of 7.2% over the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:23:01 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Bhattacharya", "Gautam", ""], ["Monteiro", "Joao", ""], ["Alam", "Jahangir", ""], ["Kenny", "Patrick", ""]]}, {"id": "1811.03066", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chablani, David\n  Sontag, Xavier Amatriain", "title": "Prototypical Clustering Networks for Dermatological Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of image classification for the purpose of aiding\ndoctors in dermatological diagnosis. Dermatological diagnosis poses two major\nchallenges for standard off-the-shelf techniques: First, the data distribution\nis typically extremely long tailed. Second, intra-class variability is often\nlarge. To address the first issue, we formulate the problem as low-shot\nlearning, where once deployed, a base classifier must rapidly generalize to\ndiagnose novel conditions given very few labeled examples. To model diverse\nclasses effectively, we propose Prototypical Clustering Networks (PCN), an\nextension to Prototypical Networks that learns a mixture of prototypes for each\nclass. Prototypes are initialized for each class via clustering and refined via\nan online update scheme. Classification is performed by measuring similarity to\na weighted combination of prototypes within a class, where the weights are the\ninferred cluster responsibilities. We demonstrate the strengths of our approach\nin effective diagnosis on a realistic dataset of dermatological conditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:27:41 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Prabhu", "Viraj", ""], ["Kannan", "Anitha", ""], ["Ravuri", "Murali", ""], ["Chablani", "Manish", ""], ["Sontag", "David", ""], ["Amatriain", "Xavier", ""]]}, {"id": "1811.03120", "submitter": "Matthieu De Rochemonteix", "authors": "Vincent Billaut, Matthieu de Rochemonteix, Marc Thibault", "title": "ColorUNet: A convolutional classification approach to colorization", "comments": "9 pages, 10 figures Stanford University CS231n project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the challenge of colorizing grayscale images. We take a\ndeep convolutional neural network approach, and choose to take the angle of\nclassification, working on a finite set of possible colors. Similarly to a\nrecent paper, we implement a loss and a prediction function that favor\nrealistic, colorful images rather than \"true\" ones.\n  We show that a rather lightweight architecture inspired by the U-Net, and\ntrained on a reasonable amount of pictures of landscapes, achieves satisfactory\nresults on this specific subset of pictures. We show that data augmentation\nsignificantly improves the performance and robustness of the model, and provide\nvisual analysis of the prediction confidence.\n  We show an application of our model, extending the task to video\ncolorization. We suggest a way to smooth color predictions across frames,\nwithout the need to train a recurrent network designed for sequential inputs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 19:20:59 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Billaut", "Vincent", ""], ["de Rochemonteix", "Matthieu", ""], ["Thibault", "Marc", ""]]}, {"id": "1811.03151", "submitter": "Kathleen Greene", "authors": "K. Gretchen Greene", "title": "DragonPaint: Rule based bootstrapping for small data with an application\n  to cartoon coloring", "comments": null, "journal-ref": "In Proceedings of the Fourth International Conference on\n  Predictive Applications and APIs, 82, 1-9, Boston, MA, USA, 2018. PMLR", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we confront the problem of deep learning's big labeled data\nrequirements, offer a rule based strategy for extreme augmentation of small\ndata sets and apply that strategy with the image to image translation model by\nIsola et al. (2016) to automate cel style cartoon coloring with very limited\ntraining data. While our experimental results using geometric rules and\ntransformations demonstrate the performance of our methods on an image\ntranslation task with industry applications in art, design and animation, we\nalso propose the use of rules on partial data sets as a generalizable small\ndata strategy, potentially applicable across data types and domains.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:23:31 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Greene", "K. Gretchen", ""]]}, {"id": "1811.03157", "submitter": "Ali Taimori", "authors": "Ali Taimori, Farokh Marvasti", "title": "Forensic Discrimination between Traditional and Compressive Imaging\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing is a new technology for modern computational imaging\nsystems. In comparison to widespread conventional image sensing, the\ncompressive imaging paradigm requires specific forensic analysis techniques and\ntools. In this regards, one of basic scenarios in image forensics is to\ndistinguish traditionally sensed images from sophisticated compressively sensed\nones. To do this, we first mathematically and systematically model the imaging\nsystem based on compressive sensing technology. Afterwards, a simplified\nversion of the whole model is presented, which is appropriate for forensic\ninvestigation applications. We estimate the nonlinear system of compressive\nsensing with a linear model. Then, we model the imaging pipeline as an inverse\nproblem and demonstrate that different imagers have discriminative degradation\nkernels. Hence, blur kernels of various imaging systems have utilized as\nfootprints for discriminating image acquisition sources. In order to accomplish\nthe identification cycle, we have utilized the state-of-the-art Convolutional\nNeural Network (CNN) and Support Vector Machine (SVM) approaches to learn a\nclassification system from estimated blur kernels. Numerical experiments show\npromising identification results. Simulation codes are available for research\nand development purposes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:42:13 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Taimori", "Ali", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1811.03173", "submitter": "Matthew Kirchner", "authors": "Matthew R. Kirchner", "title": "Automatic Thresholding of SIFT Descriptors", "comments": "In the proceedings of the 2016 IEEE International Conference on Image\n  Processing (ICIP), pp. 291-295", "journal-ref": "IEEE International Conference on Image Processing, pp. 291-295,\n  2016", "doi": "10.1109/ICIP.2016.7532365", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to perform automatic thresholding of SIFT descriptors\nthat improves matching performance by at least 15.9% on the Oxford image\nmatching benchmark. The method uses a contrario methodology to determine a\nunique bin magnitude threshold. This is done by building a generative uniform\nbackground model for descriptors and determining when bin magnitudes have\nreached a sufficient level. The presented method, called meaningful clamping,\ncontrasts from the current SIFT implementation by efficiently computing a\nclamping threshold that is unique for every descriptor.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:43:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kirchner", "Matthew R.", ""]]}, {"id": "1811.03188", "submitter": "Vahan Huroyan", "authors": "Vahan Huroyan, Gilad Lerman, and Hau-Tieng Wu", "title": "Solving Jigsaw Puzzles By the Graph Connection Laplacian", "comments": null, "journal-ref": "SIAM J. Imaging Sci. 13(4) (2020) 1717-1753", "doi": "10.1137/19M1290760", "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel mathematical framework to address the problem of\nautomatically solving large jigsaw puzzles. This problem assumes a large image,\nwhich is cut into equal square pieces that are arbitrarily rotated and\nshuffled, and asks to recover the original image given the transformed pieces.\nThe main contribution of this work is a method for recovering the rotations of\nthe pieces when both shuffles and rotations are unknown. A major challenge of\nthis procedure is estimating the graph connection Laplacian without the\nknowledge of shuffles. A careful combination of our proposed method for\nestimating rotations with any existing method for estimating shuffles results\nin a practical solution for the jigsaw puzzle problem. Our theory guarantees,\nin a clean setting, that our basic idea of recovering rotations is robust to\nsome corruption of the connection graph. Numerical experiments demonstrate the\ncompetitive accuracy of this solution, its robustness to corruption and, its\ncomputational advantage for large puzzles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:45:03 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:29:46 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 04:50:51 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 01:50:50 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2020 12:04:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huroyan", "Vahan", ""], ["Lerman", "Gilad", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1811.03196", "submitter": "Yanchun Xie", "authors": "Yanchun Xie, Jimin Xiao, Kaizhu Huang, Jeyarajan Thiyagalingam, Yao\n  Zhao", "title": "Correlation Filter Selection for Visual Tracking Using Reinforcement\n  Learning", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter has been proven to be an effective tool for a number of\napproaches in visual tracking, particularly for seeking a good balance between\ntracking accuracy and speed. However, correlation filter based models are\nsusceptible to wrong updates stemming from inaccurate tracking results. To\ndate, little effort has been devoted towards handling the correlation filter\nupdate problem. In this paper, we propose a novel approach to address the\ncorrelation filter update problem. In our approach, we update and maintain\nmultiple correlation filter models in parallel, and we use deep reinforcement\nlearning for the selection of an optimal correlation filter model among them.\nTo facilitate the decision process in an efficient manner, we propose a\ndecision-net to deal target appearance modeling, which is trained through\nhundreds of challenging videos using proximal policy optimization and a\nlightweight learning network. An exhaustive evaluation of the proposed approach\non the OTB100 and OTB2013 benchmarks show that the approach is effective enough\nto achieve the average success rate of 62.3% and the average precision score of\n81.2%, both exceeding the performance of traditional correlation filter based\ntrackers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:24:42 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Xie", "Yanchun", ""], ["Xiao", "Jimin", ""], ["Huang", "Kaizhu", ""], ["Thiyagalingam", "Jeyarajan", ""], ["Zhao", "Yao", ""]]}, {"id": "1811.03208", "submitter": "Kerry Halupka", "authors": "Kerry Halupka, Rahil Garnavi and Stephen Moore", "title": "Deep Semantic Instance Segmentation of Tree-like Structures Using\n  Synthetic Data", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-like structures, such as blood vessels, often express complexity at very\nfine scales, requiring high-resolution grids to adequately describe their\nshape. Such sparse morphology can alternately be represented by locations of\ncentreline points, but learning from this type of data with deep learning is\nchallenging due to it being unordered, and permutation invariant. In this work,\nwe propose a deep neural network that directly consumes unordered points along\nthe centreline of a branching structure, to identify the topology of the\nrepresented structure in a single-shot. Key to our approach is the use of a\nnovel multi-task loss function, enabling instance segmentation of arbitrarily\ncomplex branching structures. We train the network solely using synthetically\ngenerated data, utilizing domain randomization to facilitate the transfer to\nreal 2D and 3D data. Results show that our network can reliably extract\nmeaningful information about branch locations, bifurcations and endpoints, and\nsets a new benchmark for semantic instance segmentation in branching\nstructures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:23:45 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Halupka", "Kerry", ""], ["Garnavi", "Rahil", ""], ["Moore", "Stephen", ""]]}, {"id": "1811.03214", "submitter": "Olivier Augereau", "authors": "Marco Stricker, Olivier Augereau, Koichi Kise, Motoi Iwata", "title": "Facial Landmark Detection for Manga Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of facial landmark detection has been widely covered for pictures\nof human faces, but it is still a challenge for drawings. Indeed, the\nproportions and symmetry of standard human faces are not always used for comics\nor mangas. The personal style of the author, the limitation of colors, etc.\nmakes the landmark detection on faces in drawings a difficult task. Detecting\nthe landmarks on manga images will be useful to provide new services for easily\nediting the character faces, estimating the character emotions, or generating\nautomatically some animations such as lip or eye movements.\n  This paper contains two main contributions: 1) a new landmark annotation\nmodel for manga faces, and 2) a deep learning approach to detect these\nlandmarks. We use the \"Deep Alignment Network\", a multi stage architecture\nwhere the first stage makes an initial estimation which gets refined in further\nstages. The first results show that the proposed method succeed to accurately\nfind the landmarks in more than 80% of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:36:51 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Stricker", "Marco", ""], ["Augereau", "Olivier", ""], ["Kise", "Koichi", ""], ["Iwata", "Motoi", ""]]}, {"id": "1811.03217", "submitter": "Weichen Dai", "authors": "Weichen Dai, Yu Zhang, Ping Li, Zheng Fang, and Sebastian Scherer", "title": "RGB-D SLAM in Dynamic Environments Using Point Correlations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simultaneous localization and mapping (SLAM) method that\neliminates the influence of moving objects in dynamic environments is proposed.\nThis method utilizes the correlation between map points to separate points that\nare part of the static scene and points that are part of different moving\nobjects into different groups. A sparse graph is first created using Delaunay\ntriangulation from all map points. In this graph, the vertices represent map\npoints, and each edge represents the correlation between adjacent points. If\nthe relative position between two points remains consistent over time, there is\ncorrelation between them, and they are considered to be moving together\nrigidly. If not, they are considered to have no correlation and to be in\nseparate groups. After the edges between the uncorrelated points are removed\nduring point-correlation optimization, the remaining graph separates the map\npoints of the moving objects from the map points of the static scene. The\nlargest group is assumed to be the group of reliable static map points.\nFinally, motion estimation is performed using only these points. The proposed\nmethod was implemented for RGB-D sensors, evaluated with a public RGB-D\nbenchmark, and tested in several additional challenging environments. The\nexperimental results demonstrate that robust and accurate performance can be\nachieved by the proposed SLAM method in both slightly and highly dynamic\nenvironments. Compared with other state-of-the-art methods, the proposed method\ncan provide competitive accuracy with good real-time performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:52:00 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 08:55:15 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dai", "Weichen", ""], ["Zhang", "Yu", ""], ["Li", "Ping", ""], ["Fang", "Zheng", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1811.03218", "submitter": "Michael Jacobs", "authors": "Michael A. Jacobs, Christopher Umbricht, Vishwa Parekh, Riham El\n  Khouli, Leslie Cope, Katarzyna J. Macura, Susan Harvey, Antonio C. Wolff", "title": "Advanced machine learning informatics modeling using clinical and\n  radiological imaging metrics for characterizing breast tumor characteristics\n  with the OncotypeDX gene array", "comments": "32 pages, 6 figures, Abstract number SSQ01-04:Radiological Society of\n  North America 2015 Scientific Assembly and Annual Meeting,Chicago IL", "journal-ref": null, "doi": null, "report-no": "SSQ01-04", "categories": "physics.med-ph cs.AI cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose-Optimal use of established and imaging methods, such as\nmultiparametric magnetic resonance imaging(mpMRI) can simultaneously identify\nkey functional parameters and provide unique imaging phenotypes of breast\ncancer. Therefore, we have developed and implemented a new machine-learning\ninformatic system that integrates clinical variables, derived from imaging and\nclinical health records, to compare with the 21-gene array assay, OncotypeDX.\nMaterials and methods-We tested our informatics modeling in a subset of\npatients (n=81) who had ER+ disease and underwent OncotypeDX gene expression\nand breast mpMRI testing. The machine-learning informatic method is termed\nIntegrated Radiomic Informatic System-IRIS was applied to the mpMRI, clinical\nand pathologic descriptors, as well as a gene array analysis. The IRIS method\nusing an advanced graph theoretic model and quantitative metrics. Summary\nstatistics (mean and standard deviations) for the quantitative imaging\nparameters were obtained. Sensitivity and specificity and Area Under the Curve\nwere calculated for the classification of the patients. Results-The OncotypeDX\nclassification by IRIS model had sensitivity of 95% and specificity of 89% with\nAUC of 0.92. The breast lesion size was larger for the high-risk groups and\nlower for both low risk and intermediate risk groups. There were significant\ndifferences in PK-DCE and ADC map values in each group. The ADC map values for\nhigh- and intermediate-risk groups were significantly lower than the low-risk\ngroup. Conclusion-These initial studies provide deeper understandings of\nimaging features and molecular gene array OncotypeDX score. This insight\nprovides the foundation to relate these imaging features to the assessment of\ntreatment response for improved personalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:53:22 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Jacobs", "Michael A.", ""], ["Umbricht", "Christopher", ""], ["Parekh", "Vishwa", ""], ["Khouli", "Riham El", ""], ["Cope", "Leslie", ""], ["Macura", "Katarzyna J.", ""], ["Harvey", "Susan", ""], ["Wolff", "Antonio C.", ""]]}, {"id": "1811.03233", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi", "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by\n  Hidden Neurons", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An activation boundary for a neuron refers to a separating hyperplane that\ndetermines whether the neuron is activated or deactivated. It has been long\nconsidered in neural networks that the activations of neurons, rather than\ntheir exact output values, play the most important role in forming\nclassification friendly partitions of the hidden feature space. However, as far\nas we know, this aspect of neural networks has not been considered in the\nliterature of knowledge transfer. In this paper, we propose a knowledge\ntransfer method via distillation of activation boundaries formed by hidden\nneurons. For the distillation, we propose an activation transfer loss that has\nthe minimum value when the boundaries generated by the student coincide with\nthose by the teacher. Since the activation transfer loss is not differentiable,\nwe design a piecewise differentiable loss approximating the activation transfer\nloss. By the proposed method, the student learns a separating boundary between\nactivation region and deactivation region formed by each neuron in the teacher.\nThrough the experiments in various aspects of knowledge transfer, it is\nverified that the proposed method outperforms the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 02:47:56 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 15:29:53 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Heo", "Byeongho", ""], ["Lee", "Minsik", ""], ["Yun", "Sangdoo", ""], ["Choi", "Jin Young", ""]]}, {"id": "1811.03236", "submitter": "Mingyang Guan", "authors": "Mingyang Guan, Zhengguo Li, Renjie He, and Changyun Wen", "title": "High Speed Tracking With A Fourier Domain Kernelized Correlation Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to design a high speed tracking approach using l1-norm due\nto its non-differentiability. In this paper, a new kernelized correlation\nfilter is introduced by leveraging the sparsity attribute of l1-norm based\nregularization to design a high speed tracker. We combine the l1-norm and\nl2-norm based regularizations in one Huber-type loss function, and then\nformulate an optimization problem in the Fourier Domain for fast computation,\nwhich enables the tracker to adaptively ignore the noisy features produced from\nocclusion and illumination variation, while keep the advantages of l2-norm\nbased regression. This is achieved due to the attribute of Convolution Theorem\nthat the correlation in spatial domain corresponds to an element-wise product\nin the Fourier domain, resulting in that the l1-norm optimization problem could\nbe decomposed into multiple sub-optimization spaces in the Fourier domain. But\nthe optimized variables in the Fourier domain are complex, which makes using\nthe l1-norm impossible if the real and imaginary parts of the variables cannot\nbe separated. However, our proposed optimization problem is formulated in such\na way that their real part and imaginary parts are indeed well separated. As\nsuch, the proposed optimization problem can be solved efficiently to obtain\ntheir optimal values independently with closed-form solutions. Extensive\nexperiments on two large benchmark datasets demonstrate that the proposed\ntracking algorithm significantly improves the tracking accuracy of the original\nkernelized correlation filter (KCF) while with little sacrifice on tracking\nspeed. Moreover, it outperforms the state-of-the-art approaches in terms of\naccuracy, efficiency, and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 02:59:31 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 10:03:52 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 06:03:32 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Guan", "Mingyang", ""], ["Li", "Zhengguo", ""], ["He", "Renjie", ""], ["Wen", "Changyun", ""]]}, {"id": "1811.03252", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang and Piotr Koniusz", "title": "Model Selection for Generalized Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of generalized zero-shot learning, the datapoints from unknown\nclasses are not available during training. The main challenge for generalized\nzero-shot learning is the unbalanced data distribution which makes it hard for\nthe classifier to distinguish if a given testing sample comes from a seen or\nunseen class. However, using Generative Adversarial Network (GAN) to generate\nauxiliary datapoints by the semantic embeddings of unseen classes alleviates\nthe above problem. Current approaches combine the auxiliary datapoints and\noriginal training data to train the generalized zero-shot learning model and\nobtain state-of-the-art results. Inspired by such models, we propose to feed\nthe generated data via a model selection mechanism. Specifically, we leverage\ntwo sources of datapoints (observed and auxiliary) to train some classifier to\nrecognize which test datapoints come from seen and which from unseen classes.\nThis way, generalized zero-shot learning can be divided into two disjoint\nclassification tasks, thus reducing the negative influence of the unbalanced\ndata distribution. Our evaluations on four publicly available datasets for\ngeneralized zero-shot learning show that our model obtains state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:47:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhang", "Hongguang", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1811.03264", "submitter": "Songyou Peng", "authors": "Songyou Peng, Peter Sturm", "title": "Calibration Wizard: A Guidance System for Camera Calibration Based on\n  Modelling Geometric and Corner Uncertainty", "comments": "Oral presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the accuracy of a calibration depends strongly on the\nchoice of camera poses from which images of a calibration object are acquired.\nWe present a system -- Calibration Wizard -- that interactively guides a user\ntowards taking optimal calibration images. For each new image to be taken, the\nsystem computes, from all previously acquired images, the pose that leads to\nthe globally maximum reduction of expected uncertainty on intrinsic parameters\nand then guides the user towards that pose. We also show how to incorporate\nuncertainty in corner point position in a novel principled manner, for both,\ncalibration and computation of the next best pose. Synthetic and real-world\nexperiments are performed to demonstrate the effectiveness of Calibration\nWizard.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 04:40:09 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 12:51:32 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Peng", "Songyou", ""], ["Sturm", "Peter", ""]]}, {"id": "1811.03268", "submitter": "Luisa Polania", "authors": "Luisa Polania, Dongning Wang, Glenn Fung", "title": "Ordinal Regression using Noisy Pairwise Comparisons for Body Mass Index\n  Range Estimation", "comments": "Paper accepted for publication at the 2019 IEEE Winter Conference on\n  Applications of Computer Vision (WACV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal regression aims to classify instances into ordinal categories. In\nthis paper, body mass index (BMI) category estimation from facial images is\ncast as an ordinal regression problem. In particular, noisy binary search\nalgorithms based on pairwise comparisons are employed to exploit the ordinal\nrelationship among BMI categories. Comparisons are performed with Siamese\narchitectures, one of which uses the Bradley-Terry model probabilities as\ntarget. The Bradley-Terry model is an approach to describe probabilities of the\npossible outcomes when elements of a set are repeatedly compared with one\nanother in pairs. Experimental results show that our approach outperforms\nclassification and regression-based methods at estimating BMI categories.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 04:55:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Polania", "Luisa", ""], ["Wang", "Dongning", ""], ["Fung", "Glenn", ""]]}, {"id": "1811.03280", "submitter": "Yuma Kinoshita", "authors": "Chien Cheng Chien, Yuma Kinoshita, Sayaka Shiota and Hitoshi Kiya", "title": "A Retinex-based Image Enhancement Scheme with Noise Aware Shadow-up\n  Function", "comments": "To appear in IWAIT-IFMIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel image contrast enhancement method based on both a\nnoise aware shadow-up function and Retinex (retina and cortex) decomposition.\nUnder low light conditions, images taken by digital cameras have low contrast\nin dark or bright regions. This is due to a limited dynamic range that imaging\nsensors have. For this reason, various contrast enhancement methods have been\nproposed. Our proposed method can enhance the contrast of images without not\nonly over-enhancement but also noise amplification. In the proposed method, an\nimage is decomposed into illumination layer and reflectance layer based on the\nretinex theory, and lightness information of the illumination layer is\nadjusted. A shadow-up function is used for preventing over-enhancement. The\nproposed mapping function, designed by using a noise aware histogram, allows\nnot only to enhance contrast of dark region, but also to avoid amplifying\nnoise, even under strong noise environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:35:02 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chien", "Chien Cheng", ""], ["Kinoshita", "Yuma", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1811.03291", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta", "title": "Doc2Im: document to image conversion through self-attentive embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is a fundamental task in NLP applications. Latest\nresearch in this field has largely been divided into two major sub-fields.\nLearning representations is one sub-field and learning deeper models, both\nsequential and convolutional, which again connects back to the representation\nis the other side. We posit the idea that the stronger the representation is,\nthe simpler classifier models are needed to achieve higher performance. In this\npaper we propose a completely novel direction to text classification research,\nwherein we convert text to a representation very similar to images, such that\nany deep network able to handle images is equally able to handle text. We take\na deeper look at the representation of documents as an image and subsequently\nutilize very simple convolution based models taken as is from computer vision\ndomain. This image can be cropped, re-scaled, re-sampled and augmented just\nlike any other image to work with most of the state-of-the-art large\nconvolution based models which have been designed to handle large image\ndatasets. We show impressive results with some of the latest benchmarks in the\nrelated fields. We perform transfer learning experiments, both from text to\ntext domain and also from image to text domain. We believe this is a paradigm\nshift from the way document understanding and text classification has been\ntraditionally done, and will drive numerous novel research ideas in the\ncommunity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 06:51:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Gupta", "Mithun Das", ""]]}, {"id": "1811.03305", "submitter": "Mahesh Subedar", "authors": "Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo", "title": "BAR: Bayesian Activity Recognition using variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 08:04:09 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 08:08:34 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Krishnan", "Ranganath", ""], ["Subedar", "Mahesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1811.03331", "submitter": "Naoki Kato", "authors": "Naoki Kato, Tianqi Li, Kohei Nishino, Yusuke Uchida", "title": "Improving Multi-Person Pose Estimation using Label Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant attention is being paid to multi-person pose estimation methods\nrecently, as there has been rapid progress in the field owing to convolutional\nneural networks. Especially, recent method which exploits part confidence maps\nand Part Affinity Fields (PAFs) has achieved accurate real-time prediction of\nmulti-person keypoints. However, human annotated labels are sometimes\ninappropriate for learning models. For example, if there is a limb that extends\noutside an image, a keypoint for the limb may not have annotations because it\nexists outside of the image, and thus the labels for the limb can not be\ngenerated. If a model is trained with data including such missing labels, the\noutput of the model for the location, even though it is correct, is penalized\nas a false positive, which is likely to cause negative effects on the\nperformance of the model. In this paper, we point out the existence of some\npatterns of inappropriate labels, and propose a novel method for correcting\nsuch labels with a teacher model trained on such incomplete data. Experiments\non the COCO dataset show that training with the corrected labels improves the\nperformance of the model and also speeds up training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 09:38:38 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Kato", "Naoki", ""], ["Li", "Tianqi", ""], ["Nishino", "Kohei", ""], ["Uchida", "Yusuke", ""]]}, {"id": "1811.03343", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Vivek Singh, Yifan Wu, Klaus Kirchberg, James Duncan,\n  Ankur Kapoor", "title": "Repetitive Motion Estimation Network: Recover cardiac and respiratory\n  signal from thoracic imaging", "comments": "Accepted by NIPS workshop MED-NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking organ motion is important in image-guided interventions, but motion\nannotations are not always easily available. Thus, we propose Repetitive Motion\nEstimation Network (RMEN) to recover cardiac and respiratory signals. It learns\nthe spatio-temporal repetition patterns, embedding high dimensional motion\nmanifolds to 1D vectors with partial motion phase boundary annotations.\nCompared with the best alternative models, our proposed RMEN significantly\ndecreased the QRS peaks detection offsets by 59.3%. Results showed that RMEN\ncould handle the irregular cardiac and respiratory motion cases. Repetitive\nmotion patterns learned by RMEN were visualized and indicated in the feature\nmaps.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 10:30:10 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Singh", "Vivek", ""], ["Wu", "Yifan", ""], ["Kirchberg", "Klaus", ""], ["Duncan", "James", ""], ["Kapoor", "Ankur", ""]]}, {"id": "1811.03378", "submitter": "Chigozie Nwankpa", "authors": "Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, Stephen Marshall", "title": "Activation Functions: Comparison of trends in Practice and Research for\n  Deep Learning", "comments": "20 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully used in diverse emerging domains\nto solve real world complex problems with may more deep learning(DL)\narchitectures, being developed to date. To achieve these state-of-the-art\nperformances, the DL architectures use activation functions (AFs), to perform\ndiverse computations between the hidden layers and the output layers of any\ngiven DL architecture. This paper presents a survey on the existing AFs used in\ndeep learning applications and highlights the recent trends in the use of the\nactivation functions for deep learning applications. The novelty of this paper\nis that it compiles majority of the AFs used in DL and outlines the current\ntrends in the applications and usage of these functions in practical deep\nlearning deployments against the state-of-the-art research results. This\ncompilation will aid in making effective decisions in the choice of the most\nsuitable and appropriate activation function for any given application, ready\nfor deployment. This paper is timely because most research papers on AF\nhighlights similar works and results while this paper will be the first, to\ncompile the trends in AF applications in practice against the research results\nfrom literature, found in deep learning research to date.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:28:43 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Nwankpa", "Chigozie", ""], ["Ijomah", "Winifred", ""], ["Gachagan", "Anthony", ""], ["Marshall", "Stephen", ""]]}, {"id": "1811.03382", "submitter": "Sebastian Bodenstedt", "authors": "Sebastian Bodenstedt and Dominik Rivoir and Alexander Jenke and Martin\n  Wagner and Michael Breucha and Beat M\\\"uller-Stich and S\\\"oren Torge Mees and\n  J\\\"urgen Weitz and Stefanie Speidel", "title": "Active Learning using Deep Bayesian Networks for Surgical Workflow\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications in the field of computer assisted surgery, such as\nproviding the position of a tumor, specifying the most probable tool required\nnext by the surgeon or determining the remaining duration of surgery, methods\nfor surgical workflow analysis are a prerequisite. Often machine learning based\napproaches serve as basis for surgical workflow analysis. In general machine\nlearning algorithms, such as convolutional neural networks (CNN), require large\namounts of labeled data. While data is often available in abundance, many tasks\nin surgical workflow analysis need data annotated by domain experts, making it\ndifficult to obtain a sufficient amount of annotations.\n  The aim of using active learning to train a machine learning model is to\nreduce the annotation effort. Active learning methods determine which unlabeled\ndata points would provide the most information according to some metric, such\nas prediction uncertainty. Experts will then be asked to only annotate these\ndata points. The model is then retrained with the new data and used to select\nfurther data for annotation. Recently, active learning has been applied to CNN\nby means of Deep Bayesian Networks (DBN). These networks make it possible to\nassign uncertainties to predictions.\n  In this paper, we present a DBN-based active learning approach adapted for\nimage-based surgical workflow analysis task. Furthermore, by using a recurrent\narchitecture, we extend this network to video-based surgical workflow analysis.\nWe evaluate these approaches on the Cholec80 dataset by performing instrument\npresence detection and surgical phase segmentation. Here we are able to show\nthat using a DBN-based active learning approach for selecting what data points\nto annotate next outperforms a baseline based on randomly selecting data\npoints.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:42:05 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 15:25:35 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Bodenstedt", "Sebastian", ""], ["Rivoir", "Dominik", ""], ["Jenke", "Alexander", ""], ["Wagner", "Martin", ""], ["Breucha", "Michael", ""], ["M\u00fcller-Stich", "Beat", ""], ["Mees", "S\u00f6ren Torge", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1811.03384", "submitter": "Sebastian Bodenstedt", "authors": "Sebastian Bodenstedt and Martin Wagner and Lars M\\\"undermann and\n  Hannes Kenngott and Beat M\\\"uller-Stich and Michael Breucha and S\\\"oren Torge\n  Mees and J\\\"urgen Weitz and Stefanie Speidel", "title": "Prediction of laparoscopic procedure duration using unlabeled,\n  multimodal sensor data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose The course of surgical procedures is often unpredictable, making it\ndifficult to estimate the duration of procedures beforehand. A context-aware\nmethod that analyses the workflow of an intervention online and automatically\npredicts the remaining duration would alleviate these problems. As basis for\nsuch an estimate, information regarding the current state of the intervention\nis required. Methods Today, the operating room contains a diverse range of\nsensors. During laparoscopic interventions, the endoscopic video stream is an\nideal source of such information. Extracting quantitative information from the\nvideo is challenging though, due to its high dimensionality. Other surgical\ndevices (e.g. insufflator, lights, etc.) provide data streams which are, in\ncontrast to the video stream, more compact and easier to quantify. Though\nwhether such streams offer sufficient information for estimating the duration\nof surgery is uncertain. Here, we propose and compare methods, based on\nconvolutional neural networks, for continuously predicting the duration of\nlaparoscopic interventions based on unlabeled data, such as from endoscopic\nimages and surgical device streams. Results The methods are evaluated on 80\nlaparoscopic interventions of various types, for which surgical device data and\nthe endoscopic video are available. Here the combined method performs best with\nan overall average error of 37% and an average halftime error of 28%.\nConclusion In this paper, we present, to our knowledge, the first approach for\nonline procedure duration prediction using unlabeled endoscopic video data and\nsurgical device data in a laparoscopic setting. We also show that a method\nincorporating both vision and device data performs better than methods based\nonly on vision, while methods only based on tool usage and surgical device data\nperform poorly, showing the importance of the visual channel.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:47:03 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 15:00:42 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bodenstedt", "Sebastian", ""], ["Wagner", "Martin", ""], ["M\u00fcndermann", "Lars", ""], ["Kenngott", "Hannes", ""], ["M\u00fcller-Stich", "Beat", ""], ["Breucha", "Michael", ""], ["Mees", "S\u00f6ren Torge", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1811.03403", "submitter": "Jarryd Son", "authors": "Jarryd Son, Amit Mishra", "title": "ExGate: Externally Controlled Gating for Feature-based Attention in\n  Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual capabilities of artificial systems have come a long way since the\nadvent of deep learning. These methods have proven to be effective, however\nthey are not as efficient as their biological counterparts. Visual attention is\na set of mechanisms that are employed in biological visual systems to ease\ncomputational load by only processing pertinent parts of the stimuli. This\npaper addresses the implementation of top-down, feature-based attention in an\nartificial neural network by use of externally controlled neuron gating. Our\nresults showed a 5% increase in classification accuracy on the CIFAR-10 dataset\nversus a non-gated version, while adding very few parameters. Our gated model\nalso produces more reasonable errors in predictions by drastically reducing\nprediction of classes that belong to a different category to the true class.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:39:49 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Son", "Jarryd", ""], ["Mishra", "Amit", ""]]}, {"id": "1811.03433", "submitter": "Qiao Zheng", "authors": "Qiao Zheng, Herv\\'e Delingette, Nicholas Ayache", "title": "Explainable cardiac pathology classification on cine MRI with motion\n  characterization by semi-supervised learning of apparent flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to classify cardiac pathology based on a novel approach\nto extract image derived features to characterize the shape and motion of the\nheart. An original semi-supervised learning procedure, which makes efficient\nuse of a large amount of non-segmented images and a small amount of images\nsegmented manually by experts, is developed to generate pixel-wise apparent\nflow between two time points of a 2D+t cine MRI image sequence. Combining the\napparent flow maps and cardiac segmentation masks, we obtain a local apparent\nflow corresponding to the 2D motion of myocardium and ventricular cavities.\nThis leads to the generation of time series of the radius and thickness of\nmyocardial segments to represent cardiac motion. These time series of motion\nfeatures are reliable and explainable characteristics of pathological cardiac\nmotion. Furthermore, they are combined with shape-related features to classify\ncardiac pathologies. Using only nine feature values as input, we propose an\nexplainable, simple and flexible model for pathology classification. On ACDC\ntraining set and testing set, the model achieves 95% and 94% respectively as\nclassification accuracy. Its performance is hence comparable to that of the\nstate-of-the-art. Comparison with various other models is performed to outline\nsome advantages of our model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 14:22:05 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:52:47 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Zheng", "Qiao", ""], ["Delingette", "Herv\u00e9", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1811.03447", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Chris Yakopcic, Tarek M. Taha, and Vijayan K. Asari", "title": "Microscopic Nuclei Classification, Segmentation and Detection with\n  improved Deep Convolutional Neural Network (DCNN) Approaches", "comments": "18 pages, 16 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to cellular heterogeneity, cell nuclei classification, segmentation, and\ndetection from pathological images are challenging tasks. In the last few\nyears, Deep Convolutional Neural Networks (DCNN) approaches have been shown\nstate-of-the-art (SOTA) performance on histopathological imaging in different\nstudies. In this work, we have proposed different advanced DCNN models and\nevaluated for nuclei classification, segmentation, and detection. First, the\nDensely Connected Recurrent Convolutional Network (DCRN) model is used for\nnuclei classification. Second, Recurrent Residual U-Net (R2U-Net) is applied\nfor nuclei segmentation. Third, the R2U-Net regression model which is named\nUD-Net is used for nuclei detection from pathological images. The experiments\nare conducted with different datasets including Routine Colon Cancer(RCC)\nclassification and detection dataset, and Nuclei Segmentation Challenge 2018\ndataset. The experimental results show that the proposed DCNN models provide\nsuperior performance compared to the existing approaches for nuclei\nclassification, segmentation, and detection tasks. The results are evaluated\nwith different performance metrics including precision, recall, Dice\nCoefficient (DC), Means Squared Errors (MSE), F1-score, and overall accuracy.\nWe have achieved around 3.4% and 4.5% better F-1 score for nuclei\nclassification and detection tasks compared to recently published DCNN based\nmethod. In addition, R2U-Net shows around 92.15% testing accuracy in term of\nDC. These improved methods will help for pathological practices for better\nquantitative analysis of nuclei in Whole Slide Images(WSI) which ultimately\nwill help for better understanding of different types of cancer in clinical\nworkflow.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 14:34:43 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1811.03456", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Weiming Zhang, Nenghai Yu", "title": "CAAD 2018: Iterative Ensemble Adversarial Attack", "comments": "arXiv admin note: text overlap with arXiv:1811.00189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Adversarial attacks can be used to evaluate the robustness of deep\nlearning models before they are deployed. Unfortunately, most of existing\nadversarial attacks can only fool a black-box model with a low success rate. To\nimprove the success rates for black-box adversarial attacks, we proposed an\niterated adversarial attack against an ensemble of image classifiers. With this\nmethod, we won the 5th place in CAAD 2018 Targeted Adversarial Attack\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:36:55 GMT"}], "update_date": "2018-11-11", "authors_parsed": [["Liu", "Jiayang", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.03478", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Shuai Guo, Hong Qiao, Yang Wang, Bin Wang, Wenbo Luo,\n  Mingming Zhang, Keye Zhang, and Bixuan Du", "title": "Multi-view Laplacian Eigenmaps Based on Bag-of-Neighbors For RGBD Human\n  Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotion recognition is an important direction in the field of biometric\nand information forensics. However, most existing human emotion research are\nbased on the single RGB view. In this paper, we introduce a RGBD video-emotion\ndataset and a RGBD face-emotion dataset for research. To our best knowledge,\nthis may be the first RGBD video-emotion dataset. We propose a new supervised\nnonlinear multi-view laplacian eigenmaps (MvLE) approach and a\nmultihidden-layer out-of-sample network (MHON) for RGB-D humanemotion\nrecognition. To get better representations of RGB view and depth view, MvLE is\nused to map the training set of both views from original space into the common\nsubspace. As RGB view and depth view lie in different spaces, a new distance\nmetric bag of neighbors (BON) used in MvLE can get the similar distributions of\nthe two views. Finally, MHON is used to get the low-dimensional representations\nof test data and predict their labels. MvLE can deal with the cases that RGB\nview and depth view have different size of features, even different number of\nsamples and classes. And our methods can be easily extended to more than two\nviews. The experiment results indicate the effectiveness of our methods over\nsome state-of-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:03:58 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Liu", "Shenglan", ""], ["Guo", "Shuai", ""], ["Qiao", "Hong", ""], ["Wang", "Yang", ""], ["Wang", "Bin", ""], ["Luo", "Wenbo", ""], ["Zhang", "Mingming", ""], ["Zhang", "Keye", ""], ["Du", "Bixuan", ""]]}, {"id": "1811.03492", "submitter": "Enrique Sanchez-Lozano", "authors": "Enrique Sanchez and Michel Valstar", "title": "Triple consistency loss for pairing distributions in GAN-based face\n  synthesis", "comments": "Project site https://github.com/ESanchezLozano/GANnotation ,\n  https://youtu.be/-8r7zexg4yg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have shown impressive results for the task of\nobject translation, including face-to-face translation. A key component behind\nthe success of recent approaches is the self-consistency loss, which encourages\na network to recover the original input image when the output generated for a\ndesired attribute is itself passed through the same network, but with the\ntarget attribute inverted. While the self-consistency loss yields\nphoto-realistic results, it can be shown that the input and target domains,\nsupposed to be close, differ substantially. This is empirically found by\nobserving that a network recovers the input image even if attributes other than\nthe inversion of the original goal are set as target. This stops one combining\nnetworks for different tasks, or using a network to do progressive forward\npasses. In this paper, we show empirical evidence of this effect, and propose a\nnew loss to bridge the gap between the distributions of the input and target\ndomains. This \"triple consistency loss\", aims to minimise the distance between\nthe outputs generated by the network for different routes to the target,\nindependent of any intermediate steps. To show this is effective, we\nincorporate the triple consistency loss into the training of a new\nlandmark-guided face to face synthesis, where, contrary to previous works, the\ngenerated images can simultaneously undergo a large transformation in both\nexpression and pose. To the best of our knowledge, we are the first to tackle\nthe problem of mismatching distributions in self-domain synthesis, and to\npropose \"in-the-wild\" landmark-guided synthesis. Code will be available at\nhttps://github.com/ESanchezLozano/GANnotation\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:32:18 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Sanchez", "Enrique", ""], ["Valstar", "Michel", ""]]}, {"id": "1811.03529", "submitter": "Mubariz Zaffar", "authors": "Mubariz Zaffar, Shoaib Ehsan, Michael Milford and Klaus Mcdonald Maier", "title": "Memorable Maps: A Framework for Re-defining Places in Visual Place\n  Recognition", "comments": "13 pages, 25 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cognition-inspired agnostic framework for building a\nmap for Visual Place Recognition. This framework draws inspiration from\nhuman-memorability, utilizes the traditional image entropy concept and computes\nthe static content in an image; thereby presenting a tri-folded criterion to\nassess the 'memorability' of an image for visual place recognition. A dataset\nnamely 'ESSEX3IN1' is created, composed of highly confusing images from indoor,\noutdoor and natural scenes for analysis. When used in conjunction with\nstate-of-the-art visual place recognition methods, the proposed framework\nprovides significant performance boost to these techniques, as evidenced by\nresults on ESSEX3IN1 and other public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:18:50 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 16:21:49 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Zaffar", "Mubariz", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["Maier", "Klaus Mcdonald", ""]]}, {"id": "1811.03535", "submitter": "Wayne Treible", "authors": "Wayne Treible, Scott Sorensen, Andrew D. Gilliam, Chandra Kambhamettu,\n  Joseph L. Mundy", "title": "Learning Dense Stereo Matching for Digital Surface Models from Satellite\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital Surface Model generation from satellite imagery is a difficult task\nthat has been largely overlooked by the deep learning community. Stereo\nreconstruction techniques developed for terrestrial systems including self\ndriving cars do not translate well to satellite imagery where image pairs vary\nconsiderably. In this work we present neural network tailored for Digital\nSurface Model generation, a ground truthing and training scheme which maximizes\navailable hardware, and we present a comparison to existing methods. The\nresulting models are smooth, preserve boundaries, and enable further\nprocessing. This represents one of the first attempts at leveraging deep\nlearning in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:31:23 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 19:41:57 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Treible", "Wayne", ""], ["Sorensen", "Scott", ""], ["Gilliam", "Andrew D.", ""], ["Kambhamettu", "Chandra", ""], ["Mundy", "Joseph L.", ""]]}, {"id": "1811.03542", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jianwei Feng, Martial Hebert", "title": "Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks for semantic segmentation requires annotation of large\namounts of data, which can be time-consuming and expensive. Unfortunately,\nthese trained networks still generalize poorly when tested in domains not\nconsistent with the training data. In this paper, we show that by carefully\npresenting a mixture of labeled source domain and proxy-labeled target domain\ndata to a network, we can achieve state-of-the-art unsupervised domain\nadaptation results. With our design, the network progressively learns features\nspecific to the target domain using annotation from only the source domain. We\ngenerate proxy labels for the target domain using the network's own\npredictions. Our architecture then allows selective mining of easy samples from\nthis set of proxy labels, and hard samples from the annotated source domain. We\nconduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets\non synthetic-to-real domain adaptation and geographic domain adaptation,\nshowing the advantages of our method over baselines and existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:44:59 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chitta", "Kashyap", ""], ["Feng", "Jianwei", ""], ["Hebert", "Martial", ""]]}, {"id": "1811.03549", "submitter": "Shuai Chen", "authors": "Shuai Chen, Marleen de Bruijne", "title": "An End-to-end Approach to Semantic Segmentation with 3D CNN and\n  Posterior-CRF in Medical Images", "comments": "Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-connected Conditional Random Field (CRF) is often used as\npost-processing to refine voxel classification results by encouraging spatial\ncoherence. In this paper, we propose a new end-to-end training method called\nPosterior-CRF. In contrast with previous approaches which use the original\nimage intensity in the CRF, our approach applies 3D, fully connected CRF to the\nposterior probabilities from a CNN and optimizes both CNN and CRF together. The\nexperiments on white matter hyperintensities segmentation demonstrate that our\nmethod outperforms CNN, post-processing CRF and different end-to-end training\nCRF approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:00:05 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chen", "Shuai", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1811.03567", "submitter": "Honglin Chen", "authors": "Will Xiao, Honglin Chen, Qianli Liao and Tomaso Poggio", "title": "Biologically-plausible learning algorithms can scale to large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation (BP) algorithm is often thought to be biologically\nimplausible in the brain. One of the main reasons is that BP requires symmetric\nweight matrices in the feedforward and feedback pathways. To address this\n\"weight transport problem\" (Grossberg, 1987), two more biologically plausible\nalgorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax\nBP's weight symmetry requirements and demonstrate comparable learning\ncapabilities to that of BP on small datasets. However, a recent study by\nBartunov et al. (2018) evaluate variants of target-propagation (TP) and\nfeedback alignment (FA) on MINIST, CIFAR, and ImageNet datasets, and find that\nalthough many of the proposed algorithms perform well on MNIST and CIFAR, they\nperform significantly worse than BP on ImageNet. Here, we additionally evaluate\nthe sign-symmetry algorithm (Liao et al., 2016), which differs from both BP and\nFA in that the feedback and feedforward weights share signs but not magnitudes.\nWe examine the performance of sign-symmetry and feedback alignment on ImageNet\nand MS COCO datasets using different network architectures (ResNet-18 and\nAlexNet for ImageNet, RetinaNet for MS COCO). Surprisingly, networks trained\nwith sign-symmetry can attain classification performance approaching that of\nBP-trained networks. These results complement the study by Bartunov et al.\n(2018), and establish a new benchmark for future biologically plausible\nlearning algorithms on more difficult datasets and more complex architectures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:43:59 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 21:23:57 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 02:03:52 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Xiao", "Will", ""], ["Chen", "Honglin", ""], ["Liao", "Qianli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1811.03575", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski", "title": "Large-Scale Visual Active Learning with Deep Probabilistic Ensembles", "comments": "arXiv admin note: text overlap with arXiv:1811.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating the right data for training deep neural networks is an important\nchallenge. Active learning using uncertainty estimates from Bayesian Neural\nNetworks (BNNs) could provide an effective solution to this. Despite being\ntheoretically principled, BNNs require approximations to be applied to\nlarge-scale problems, where both performance and uncertainty estimation are\ncrucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a\nscalable technique that uses a regularized ensemble to approximate a deep BNN.\nWe conduct a series of large-scale visual active learning experiments to\nevaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet\ndatasets, and semantic segmentation with the BDD100k dataset. Our models\nrequire significantly less training data to achieve competitive performances,\nand steadily improve upon strong active learning baselines as the annotation\nbudget is increased.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:56:43 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:45:26 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 02:02:13 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Chitta", "Kashyap", ""], ["Alvarez", "Jose M.", ""], ["Lesnikowski", "Adam", ""]]}, {"id": "1811.03601", "submitter": "Jack Langerman", "authors": "Ziming Qiu, Jack Langerman, Nitin Nair, Orlando Aristizabal, Jonathan\n  Mamou, Daniel H. Turnbull, Jeffrey Ketterling, Yao Wang", "title": "Deep BV: A Fully Automated System for Brain Ventricle Localization and\n  Segmentation in 3D Ultrasound Images of Embryonic Mice", "comments": "IEEE Signal Processing in Medicine and Biology Symposium - 2018, 6\n  pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric analysis of brain ventricle (BV) structure is a key tool in the\nstudy of central nervous system development in embryonic mice. High-frequency\nultrasound (HFU) is the only non-invasive, real-time modality available for\nrapid volumetric imaging of embryos in utero. However, manual segmentation of\nthe BV from HFU volumes is tedious, time-consuming, and requires specialized\nexpertise. In this paper, we propose a novel deep learning based BV\nsegmentation system for whole-body HFU images of mouse embryos. Our fully\nautomated system consists of two modules: localization and segmentation. It\nfirst applies a volumetric convolutional neural network on a 3D sliding window\nover the entire volume to identify a 3D bounding box containing the entire BV.\nIt then employs a fully convolutional network to segment the detected bounding\nbox into BV and background. The system achieves a Dice Similarity Coefficient\n(DSC) of 0.8956 for BV segmentation on an unseen 111 HFU volume test set\nsurpassing the previous state-of-the-art method (DSC of 0.7119) by a margin of\n25%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:07:53 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Qiu", "Ziming", ""], ["Langerman", "Jack", ""], ["Nair", "Nitin", ""], ["Aristizabal", "Orlando", ""], ["Mamou", "Jonathan", ""], ["Turnbull", "Daniel H.", ""], ["Ketterling", "Jeffrey", ""], ["Wang", "Yao", ""]]}, {"id": "1811.03621", "submitter": "Hang Qiu", "authors": "Hang Qiu, Krishna Chintalapudi, Ramesh Govindan", "title": "Satyam: Democratizing Groundtruth for Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The democratization of machine learning (ML) has led to ML-based machine\nvision systems for autonomous driving, traffic monitoring, and video\nsurveillance. However, true democratization cannot be achieved without greatly\nsimplifying the process of collecting groundtruth for training and testing\nthese systems. This groundtruth collection is necessary to ensure good\nperformance under varying conditions. In this paper, we present the design and\nevaluation of Satyam, a first-of-its-kind system that enables a layperson to\nlaunch groundtruth collection tasks for machine vision with minimal effort.\nSatyam leverages a crowdtasking platform, Amazon Mechanical Turk, and automates\nseveral challenging aspects of groundtruth collection: creating and launching\nof custom web-UI tasks for obtaining the desired groundtruth, controlling\nresult quality in the face of spammers and untrained workers, adapting prices\nto match task complexity, filtering spammers and workers with poor performance,\nand processing worker payments. We validate Satyam using several popular\nbenchmark vision datasets, and demonstrate that groundtruth obtained by Satyam\nis comparable to that obtained from trained experts and provides matching ML\nperformance when used for training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:35:47 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Qiu", "Hang", ""], ["Chintalapudi", "Krishna", ""], ["Govindan", "Ramesh", ""]]}, {"id": "1811.03680", "submitter": "Cuixian Chen", "authors": "Caroline Werther, Morgan Ferguson, Kevin Park, Troy Kling, Cuixian\n  Chen, Yishi Wang", "title": "Gender Effect on Face Recognition for a Large Longitudinal Database", "comments": "This paper has been accepted by IEEE International Workshop on\n  Information Forensics and Security (2018 WIFS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aging or gender variation can affect the face recognition performance\ndramatically. While most of the face recognition studies are focused on the\nvariation of pose, illumination and expression, it is important to consider the\ninfluence of gender effect and how to design an effective matching framework.\nIn this paper, we address these problems on a very large longitudinal database\nMORPH-II which contains 55,134 face images of 13,617 individuals. First, we\nconsider four comprehensive experiments with different combination of gender\ndistribution and subset size, including: 1) equal gender distribution; 2) a\nlarge highly unbalanced gender distribution; 3) consider different gender\ncombinations, such as male only, female only, or mixed gender; and 4) the\neffect of subset size in terms of number of individuals. Second, we consider\neight nearest neighbor distance metrics and also Support Vector Machine (SVM)\nfor classifiers and test the effect of different classifiers. Last, we consider\ndifferent fusion techniques for an effective matching framework to improve the\nrecognition performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:12:58 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Werther", "Caroline", ""], ["Ferguson", "Morgan", ""], ["Park", "Kevin", ""], ["Kling", "Troy", ""], ["Chen", "Cuixian", ""], ["Wang", "Yishi", ""]]}, {"id": "1811.03691", "submitter": "Hongming Shan", "authors": "Hongming Shan, Atul Padole, Fatemeh Homayounieh, Uwe Kruger, Ruhani\n  Doda Khera, Chayanin Nitiwarangkul, Mannudeep K. Kalra, Ge Wang", "title": "Can Deep Learning Outperform Modern Commercial CT Image Reconstruction\n  Methods?", "comments": "17 pages, 7 figures", "journal-ref": "Nature Machine Intelligence, 1(6) (2019) 269-276", "doi": "10.1038/s42256-019-0057-9", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Commercial iterative reconstruction techniques on modern CT scanners target\nradiation dose reduction but there are lingering concerns over their impact on\nimage appearance and low contrast detectability. Recently, machine learning,\nespecially deep learning, has been actively investigated for CT. Here we design\na novel neural network architecture for low-dose CT (LDCT) and compare it with\ncommercial iterative reconstruction methods used for standard of care CT. While\npopular neural networks are trained for end-to-end mapping, driven by big data,\nour novel neural network is intended for end-to-process mapping so that\nintermediate image targets are obtained with the associated search gradients\nalong which the final image targets are gradually reached. This learned dynamic\nprocess allows to include radiologists in the training loop to optimize the\nLDCT denoising workflow in a task-specific fashion with the denoising depth as\na key parameter. Our progressive denoising network was trained with the Mayo\nLDCT Challenge Dataset, and tested on images of the chest and abdominal regions\nscanned on the CT scanners made by three leading CT vendors. The best deep\nlearning based reconstructions are systematically compared to the best\niterative reconstructions in a double-blinded reader study. It is found that\nour deep learning approach performs either comparably or favorably in terms of\nnoise suppression and structural fidelity, and runs orders of magnitude faster\nthan the commercial iterative CT reconstruction algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:04:22 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Shan", "Hongming", ""], ["Padole", "Atul", ""], ["Homayounieh", "Fatemeh", ""], ["Kruger", "Uwe", ""], ["Khera", "Ruhani Doda", ""], ["Nitiwarangkul", "Chayanin", ""], ["Kalra", "Mannudeep K.", ""], ["Wang", "Ge", ""]]}, {"id": "1811.03692", "submitter": "Deepak Mishra", "authors": "Deepak Mishra, Prathosh A. P., Aravind Jayendran, Varun Srivastava,\n  Santanu Chaudhury", "title": "Mode matching in GANs through latent space learning and inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown remarkable success in\ngeneration of unstructured data, such as, natural images. However, discovery\nand separation of modes in the generated space, essential for several tasks\nbeyond naive data generation, is still a challenge. In this paper, we address\nthe problem of imposing desired modal properties on the generated space using a\nlatent distribution, engineered in accordance with the modal properties of the\ntrue data distribution. This is achieved by training a latent space inversion\nnetwork in tandem with the generative network using a divergence loss. The\nlatent space is made to follow a continuous multimodal distribution generated\nby reparameterization of a pair of continuous and discrete random variables. In\naddition, the modal priors of the latent distribution are learned to match with\nthe true data distribution using minimal-supervision with negligible increment\nin number of learnable parameters. We validate our method on multiple tasks\nsuch as mode separation, conditional generation, and attribute discovery on\nmultiple real world image datasets and demonstrate its efficacy over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:08:12 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 06:44:46 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 07:02:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mishra", "Deepak", ""], ["P.", "Prathosh A.", ""], ["Jayendran", "Aravind", ""], ["Srivastava", "Varun", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1811.03695", "submitter": "Marcus Badgeley", "authors": "Marcus A. Badgeley, John R. Zech, Luke Oakden-Rayner, Benjamin S.\n  Glicksberg, Manway Liu, William Gale, Michael V. McConnell, Beth Percha,\n  Thomas M. Snyder, Joel T. Dudley", "title": "Deep Learning Predicts Hip Fracture using Confounding Patient and\n  Healthcare Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hip fractures are a leading cause of death and disability among older adults.\nHip fractures are also the most commonly missed diagnosis on pelvic\nradiographs. Computer-Aided Diagnosis (CAD) algorithms have shown promise for\nhelping radiologists detect fractures, but the image features underpinning\ntheir predictions are notoriously difficult to understand. In this study, we\ntrained deep learning models on 17,587 radiographs to classify fracture, five\npatient traits, and 14 hospital process variables. All 20 variables could be\npredicted from a radiograph (p < 0.05), with the best performances on scanner\nmodel (AUC=1.00), scanner brand (AUC=0.98), and whether the order was marked\n\"priority\" (AUC=0.79). Fracture was predicted moderately well from the image\n(AUC=0.78) and better when combining image features with patient data\n(AUC=0.86, p=2e-9) or patient data plus hospital process features (AUC=0.91,\np=1e-21). The model performance on a test set with matched patient variables\nwas significantly lower than a random test set (AUC=0.67, p=0.003); and when\nthe test set was matched on patient and image acquisition variables, the model\nperformed randomly (AUC=0.52, 95% CI 0.46-0.58), indicating that these\nvariables were the main source of the model's predictive ability overall. We\nalso used Naive Bayes to combine evidence from image models with patient and\nhospital data and found their inclusion improved performance, but that this\napproach was nevertheless inferior to directly modeling all variables. If CAD\nalgorithms are inexplicably leveraging patient and process variables in their\npredictions, it is unclear how radiologists should interpret their predictions\nin the context of other known patient data. Further research is needed to\nilluminate deep learning decision processes so that computers and clinicians\ncan effectively cooperate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:23:07 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Badgeley", "Marcus A.", ""], ["Zech", "John R.", ""], ["Oakden-Rayner", "Luke", ""], ["Glicksberg", "Benjamin S.", ""], ["Liu", "Manway", ""], ["Gale", "William", ""], ["McConnell", "Michael V.", ""], ["Percha", "Beth", ""], ["Snyder", "Thomas M.", ""], ["Dudley", "Joel T.", ""]]}, {"id": "1811.03707", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Michal Myller, Michal Kawulok", "title": "Validating Hyperspectral Image Segmentation", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2895697", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral satellite imaging attracts enormous research attention in the\nremote sensing community, hence automated approaches for precise segmentation\nof such imagery are being rapidly developed. In this letter, we share our\nobservations on the strategy for validating hyperspectral image segmentation\nalgorithms currently followed in the literature, and show that it can lead to\nover-optimistic experimental insights. We introduce a new routine for\ngenerating segmentation benchmarks, and use it to elaborate ready-to-use\nhyperspectral training-test data partitions. They can be utilized for fair\nvalidation of new and existing algorithms without any training-test data\nleakage.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:59:40 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Nalepa", "Jakub", ""], ["Myller", "Michal", ""], ["Kawulok", "Michal", ""]]}, {"id": "1811.03721", "submitter": "Christoph Vogel", "authors": "Christoph Vogel, Patrick Kn\\\"obelreiter, Thomas Pock", "title": "Learning Energy Based Inpainting for Optical Flow", "comments": null, "journal-ref": "Proc. Asian Conf. on Computer Vision (ACCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern optical flow methods are often composed of a cascade of many\nindependent steps or formulated as a black box neural network that is hard to\ninterpret and analyze. In this work we seek for a plain, interpretable, but\nlearnable solution. We propose a novel inpainting based algorithm that\napproaches the problem in three steps: feature selection and matching,\nselection of supporting points and energy based inpainting. To facilitate the\ninference we propose an optimization layer that allows to backpropagate through\n10K iterations of a first-order method without any numerical or memory\nproblems. Compared to recent state-of-the-art networks, our modular CNN is very\nlightweight and competitive with other, more involved, inpainting based\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 00:14:38 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Vogel", "Christoph", ""], ["Kn\u00f6belreiter", "Patrick", ""], ["Pock", "Thomas", ""]]}, {"id": "1811.03736", "submitter": "Xiaoshuai Sun", "authors": "Xiaoshuai Sun", "title": "Semantic and Contrast-Aware Saliency", "comments": "arXiv admin note: text overlap with arXiv:1710.04071 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed an integrated model of semantic-aware and\ncontrast-aware saliency combining both bottom-up and top-down cues for\neffective saliency estimation and eye fixation prediction. The proposed model\nprocesses visual information using two pathways. The first pathway aims to\ncapture the attractive semantic information in images, especially for the\npresence of meaningful objects and object parts such as human faces. The second\npathway is based on multi-scale on-line feature learning and information\nmaximization, which learns an adaptive sparse representation for the input and\ndiscovers the high contrast salient patterns within the image context. The two\npathways characterize both long-term and short-term attention cues and are\nintegrated dynamically using maxima normalization. We investigate two different\nimplementations of the semantic pathway including an End-to-End deep neural\nnetwork solution and a dynamic feature integration solution, resulting in the\nSCA and SCAFI model respectively. Experimental results on artificial images and\n5 popular benchmark datasets demonstrate the superior performance and better\nplausibility of the proposed model over both classic approaches and recent deep\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:03:01 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Sun", "Xiaoshuai", ""]]}, {"id": "1811.03762", "submitter": "Yonggyu Park", "authors": "Yonggyu Park, Junhyun Lee, Yookyung Koh, Inyeop Lee, Jinhyuk Lee,\n  Jaewoo Kang", "title": "Typeface Completion with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mood of a text and the intention of the writer can be reflected in the\ntypeface. However, in designing a typeface, it is difficult to keep the style\nof various characters consistent, especially for languages with lots of\nmorphological variations such as Chinese. In this paper, we propose a Typeface\nCompletion Network (TCN) which takes one character as an input, and\nautomatically completes the entire set of characters in the same style as the\ninput characters. Unlike existing models proposed for image-to-image\ntranslation, TCN embeds a character image into two separate vectors\nrepresenting typeface and content. Combined with a reconstruction loss from the\nlatent space, and with other various losses, TCN overcomes the inherent\ndifficulty in designing a typeface. Also, compared to previous image-to-image\ntranslation models, TCN generates high quality character images of the same\ntypeface with a much smaller number of model parameters. We validate our\nproposed model on the Chinese and English character datasets, which is paired\ndata, and the CelebA dataset, which is unpaired data. In these datasets, TCN\noutperforms recently proposed state-of-the-art models for image-to-image\ntranslation. The source code of our model is available at\nhttps://github.com/yongqyu/TCN.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:49:12 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 13:01:12 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Park", "Yonggyu", ""], ["Lee", "Junhyun", ""], ["Koh", "Yookyung", ""], ["Lee", "Inyeop", ""], ["Lee", "Jinhyuk", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1811.03768", "submitter": "Guangcong Wang", "authors": "Wenqi Liang, Guangcong Wang, Jianhuang Lai, Junyong Zhu", "title": "M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain transfer learning (CDTL) is an extremely challenging task for\nthe person re-identification (ReID). Given a source domain with annotations and\na target domain without annotations, CDTL seeks an effective method to transfer\nthe knowledge from the source domain to the target domain. However, such a\nsimple two-domain transfer learning method is unavailable for the person ReID\nin that the source/target domain consists of several sub-domains, e.g.,\ncamera-based sub-domains. To address this intractable problem, we propose a\nnovel Many-to-Many Generative Adversarial Transfer Learning method (M2M-GAN)\nthat takes multiple source sub-domains and multiple target sub-domains into\nconsideration and performs each sub-domain transferring mapping from the source\ndomain to the target domain in a unified optimization process. The proposed\nmethod first translates the image styles of source sub-domains into that of\ntarget sub-domains, and then performs the supervised learning by using the\ntransferred images and the corresponding annotations in source domain. As the\ngap is reduced, M2M-GAN achieves a promising result for the cross-domain person\nReID. Experimental results on three benchmark datasets Market-1501,\nDukeMTMC-reID and MSMT17 show the effectiveness of our M2M-GAN.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 04:14:52 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Liang", "Wenqi", ""], ["Wang", "Guangcong", ""], ["Lai", "Jianhuang", ""], ["Zhu", "Junyong", ""]]}, {"id": "1811.03773", "submitter": "Benjamin Johnston", "authors": "Benjamin Johnston, Philip de Chazal", "title": "A Fully Automated System for Sizing Nasal PAP Masks Using Facial\n  Photographs", "comments": "IEEE EMBS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a fully automated system for sizing nasal Positive Airway Pressure\n(PAP) masks. The system is comprised of a mix of HOG object detectors as well\nas multiple convolutional neural network stages for facial landmark detection.\nThe models were trained using samples from the publicly available PUT and MUCT\ndatasets while transfer learning was also employed to improve the performance\nof the models on facial photographs of actual PAP mask users. The fully\nautomated system demonstrated an overall accuracy of 64.71% in correctly\nselecting the appropriate mask size and 86.1% accuracy sizing within 1 mask\nsize.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 04:50:53 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Johnston", "Benjamin", ""], ["de Chazal", "Philip", ""]]}, {"id": "1811.03782", "submitter": "Risheng Liu", "authors": "Risheng Liu, Yuxi Zhang, Shichao Cheng, Xin Fan, Zhongxuan Luo", "title": "A Theoretically Guaranteed Deep Optimization Framework for Robust\n  Compressive Sensing MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is one of the most dynamic and safe imaging\ntechniques available for clinical applications. However, the rather slow speed\nof MRI acquisitions limits the patient throughput and potential indi cations.\nCompressive Sensing (CS) has proven to be an efficient technique for\naccelerating MRI acquisition. The most widely used CS-MRI model, founded on the\npremise of reconstructing an image from an incompletely filled k-space, leads\nto an ill-posed inverse problem. In the past years, lots of efforts have been\nmade to efficiently optimize the CS-MRI model. Inspired by deep learning\ntechniques, some preliminary works have tried to incorporate deep architectures\ninto CS-MRI process. Unfortunately, the convergence issues (due to the\nexperience-based networks) and the robustness (i.e., lack real-world noise\nmodeling) of these deeply trained optimization methods are still missing. In\nthis work, we develop a new paradigm to integrate designed numerical solvers\nand the data-driven architectures for CS-MRI. By introducing an optimal\ncondition checking mechanism, we can successfully prove the convergence of our\nestablished deep CS-MRI optimization scheme. Furthermore, we explicitly\nformulate the Rician noise distributions within our framework and obtain an\nextended CS-MRI network to handle the real-world nosies in the MRI process.\nExtensive experimental results verify that the proposed paradigm outperforms\nthe existing state-of-the-art techniques both in reconstruction accuracy and\nefficiency as well as robustness to noises in real scene.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 05:35:50 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 06:39:00 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 13:46:13 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Liu", "Risheng", ""], ["Zhang", "Yuxi", ""], ["Cheng", "Shichao", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1811.03804", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai", "title": "Gradient Descent Finds Global Minima of Deep Neural Networks", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent finds a global minimum in training deep neural networks\ndespite the objective function being non-convex. The current paper proves\ngradient descent achieves zero training loss in polynomial time for a deep\nover-parameterized neural network with residual connections (ResNet). Our\nanalysis relies on the particular structure of the Gram matrix induced by the\nneural network architecture. This structure allows us to show the Gram matrix\nis stable throughout the training process and this stability implies the global\noptimality of the gradient descent algorithm. We further extend our analysis to\ndeep residual convolutional neural networks and obtain a similar convergence\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 07:39:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 05:31:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 03:35:26 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 19:01:22 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Li", "Haochuan", ""], ["Wang", "Liwei", ""], ["Zhai", "Xiyu", ""]]}, {"id": "1811.03815", "submitter": "Edwin Yuan", "authors": "Edwin Yuan, Junkyo Suh", "title": "Neural Stain Normalization and Unsupervised Classification of Cell\n  Nuclei in Histopathological Breast Cancer Images", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a complete pipeline for stain normalization,\nsegmentation, and classification of nuclei in hematoxylin and eosin (H&E)\nstained breast cancer histopathology images. In the first step, we use a\nCNN-based stain transfer technique to normalize the staining characteristics of\n(H&E) images. We then train a neural network to segment images of nuclei from\nthe H&E images. Finally, we train an Information Maximizing Generative\nAdversarial Network (InfoGAN) to learn visual representations of different\ntypes of nuclei and classify them in an entirely unsupervised manner. The\nresults show that our proposed CNN stain normalization yields improved visual\nsimilarity and cell segmentation performance compared to the conventional\nSVD-based stain normalization method. In the final step of our pipeline, we\ndemonstrate the ability to perform fully unsupervised clustering of various\nbreast histopathology cell types based on morphological and color attributes.\nIn addition, we quantitatively evaluate our neural network - based techniques\nagainst various quantitative metrics to validate the effectiveness of our\npipeline.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 08:34:36 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Yuan", "Edwin", ""], ["Suh", "Junkyo", ""]]}, {"id": "1811.03818", "submitter": "Youngwook Kwon", "authors": "Kiwoo Shin, Youngwook Paul Kwon, Masayoshi Tomizuka", "title": "RoarNet: A Robust 3D Object Detection based on RegiOn Approximation\n  Refinement", "comments": "7 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RoarNet, a new approach for 3D object detection from a 2D image\nand 3D Lidar point clouds. Based on two-stage object detection framework with\nPointNet as our backbone network, we suggest several novel ideas to improve 3D\nobject detection performance. The first part of our method, RoarNet_2D,\nestimates the 3D poses of objects from a monocular image, which approximates\nwhere to examine further, and derives multiple candidates that are\ngeometrically feasible. This step significantly narrows down feasible 3D\nregions, which otherwise requires demanding processing of 3D point clouds in a\nhuge search space. Then the second part, RoarNet_3D, takes the candidate\nregions and conducts in-depth inferences to conclude final poses in a recursive\nmanner. Inspired by PointNet, RoarNet_3D processes 3D point clouds directly\nwithout any loss of data, leading to precise detection. We evaluate our method\nin KITTI, a 3D object detection benchmark. Our result shows that RoarNet has\nsuperior performance to state-of-the-art methods that are publicly available.\nRemarkably, RoarNet also outperforms state-of-the-art methods even in settings\nwhere Lidar and camera are not time synchronized, which is practically\nimportant for actual driving environments. RoarNet is implemented in Tensorflow\nand publicly available with pre-trained models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 08:43:45 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Shin", "Kiwoo", ""], ["Kwon", "Youngwook Paul", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1811.03825", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov", "title": "Changing the Image Memorability: From Basic Photo Editing to GANs", "comments": "Accepted to CVPR 2019 Workshop (MBCCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Memorability is considered to be an important characteristic of visual\ncontent, whereas for advertisement and educational purposes it is often\ncrucial. Despite numerous studies on understanding and predicting image\nmemorability, there are almost no achievements in memorability modification. In\nthis work, we study two approaches to image editing - GAN and classical image\nprocessing - and show their impact on memorability. The visual features which\ninfluence memorability directly stay unknown till now, hence it is impossible\nto control it manually. As a solution, we let GAN learn it deeply using labeled\ndata, and then use it for conditional generation of new images. By analogy with\nalgorithms which edit facial attributes, we consider memorability as yet\nanother attribute and operate with it in the same way. Obtained data is also\ninteresting for analysis, simply because there are no real-world examples of\nsuccessful change of image memorability while preserving its other attributes.\nWe believe this may give many new answers to the question \"what makes an image\nmemorable?\" Apart from that we also study the influence of conventional\nphoto-editing tools (Photoshop, Instagram, etc.) used daily by a wide audience\non memorability. In this case, we start from real practical methods and study\nit using statistics and recent advances in memorability prediction.\nPhotographers, designers, and advertisers will benefit from the results of this\nstudy directly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 09:18:42 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 22:04:28 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 12:49:43 GMT"}, {"version": "v4", "created": "Sat, 20 Apr 2019 10:20:38 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sidorov", "Oleksii", ""]]}, {"id": "1811.03830", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume, Behzad Bozorgtabar, Hazim Kemal Ekenel, Jean-Philippe\n  Thiran, Maria Gabrani", "title": "Image-Level Attentional Context Modeling Using Nested-Graph Neural\n  Networks", "comments": "NIPS 2018, Relational Representation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new scene graph generation method called image-level\nattentional context modeling (ILAC). Our model includes an attentional graph\nnetwork that effectively propagates contextual information across the graph\nusing image-level features. Whereas previous works use an object-centric\ncontext, we build an image-level context agent to encode the scene properties.\nThe proposed method comprises a single-stream network that iteratively refines\nthe scene graph with a nested graph neural network. We demonstrate that our\napproach achieves competitive performance with the state-of-the-art for scene\ngraph generation on the Visual Genome dataset, while requiring fewer parameters\nthan other methods. We also show that ILAC can improve regular object detectors\nby incorporating relational image-level information.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 09:33:31 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 07:46:33 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Jaume", "Guillaume", ""], ["Bozorgtabar", "Behzad", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""], ["Gabrani", "Maria", ""]]}, {"id": "1811.03848", "submitter": "Sune Darkner", "authors": "Sune Darkner, Stefan Sommer, Andreas Schuhmacher, Henrik Ingerslev\n  Anders O. Baandrup, Carsten Thomsen, S{\\o}ren J{\\o}nsson", "title": "An Average of the Human Ear Canal: Recovering Acoustical Properties via\n  Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are highly dependent on the ability to process audio in order to\ninteract through conversation and navigate from sound. For this, the shape of\nthe ear acts as a mechanical audio filter. The anatomy of the outer human ear\ncanal to approximately 15-20 mm beyond the Tragus is well described because of\nits importance for customized hearing aid production. This is however not the\ncase for the part of the ear canal that is embedded in the skull, until the\ntypanic membrane. Due to the sensitivity of the outer ear, this part, referred\nto as the bony part, has only been described in a few population studies and\nonly ex-vivo. We present a study of the entire ear canal including the bony\npart and the tympanic membrane. We form an average ear canal from a number of\nMRI scans using standard image registration methods. We show that the obtained\nrepresentation is realistic in the sense that it has acoustical properties\nalmost identical to a real ear.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 10:19:25 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Darkner", "Sune", ""], ["Sommer", "Stefan", ""], ["Schuhmacher", "Andreas", ""], ["Baandrup", "Henrik Ingerslev Anders O.", ""], ["Thomsen", "Carsten", ""], ["J\u00f8nsson", "S\u00f8ren", ""]]}, {"id": "1811.03875", "submitter": "Herman Kamper", "authors": "Ryan Eloff, Herman A. Engelbrecht, Herman Kamper", "title": "Multimodal One-Shot Learning of Speech and Images", "comments": "5 pages, 1 figure, 3 tables; accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a robot is shown new concepts visually together with spoken tags,\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\nclass, it is shown a new set of unseen instances of these objects, and asked to\npick the \"milk\". Without receiving any hard labels, could it learn to match the\nnew continuous speech input to the correct visual instance? Although unimodal\none-shot learning has been studied, where one labelled example in a single\nmodality is given per class, this example motivates multimodal one-shot\nlearning. Our main contribution is to formally define this task, and to propose\nseveral baseline and advanced models. We use a dataset of paired spoken and\nvisual digits to specifically investigate recent advances in Siamese\nconvolutional neural networks. Our best Siamese model achieves twice the\naccuracy of a nearest neighbour model using pixel-distance over images and\ndynamic time warping over speech in 11-way cross-modal matching.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:14:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:08:03 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Eloff", "Ryan", ""], ["Engelbrecht", "Herman A.", ""], ["Kamper", "Herman", ""]]}, {"id": "1811.03879", "submitter": "Nawid Sayed", "authors": "Nawid Sayed, Biagio Brattoli, Bj\\\"orn Ommer", "title": "Cross and Learn: Cross-Modal Self-Supervision", "comments": "GCPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a self-supervised method for representation learning\nutilizing two different modalities. Based on the observation that cross-modal\ninformation has a high semantic meaning we propose a method to effectively\nexploit this signal. For our approach we utilize video data since it is\navailable on a large scale and provides easily accessible modalities given by\nRGB and optical flow. We demonstrate state-of-the-art performance on highly\ncontested action recognition datasets in the context of self-supervised\nlearning. We show that our feature representation also transfers to other tasks\nand conduct extensive ablation studies to validate our core contributions. Code\nand model can be found at https://github.com/nawidsayed/Cross-and-Learn.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:29:39 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 10:57:57 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 17:00:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sayed", "Nawid", ""], ["Brattoli", "Biagio", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1811.03921", "submitter": "Shijie Lin", "authors": "Shijie Lin, Jinwang Wang, Wen Yang, Guisong Xia", "title": "Toward Autonomous Rotation-Aware Unmanned Aerial Grasping", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous Unmanned Aerial Manipulators (UAMs) have shown promising\npotentials to transform passive sensing missions into active 3-dimension\ninteractive missions, but they still suffer from some difficulties impeding\ntheir wide applications, such as target detection and stabilization. This\nletter presents a vision-based autonomous UAM with a 3DoF robotic arm for\nrotational grasping, with a compensation on displacement for center of gravity.\nFirst, the hardware, software architecture and state estimation methods are\ndetailed. All the mechanical designs are fully provided as open-source hardware\nfor the reuse by the community. Then, we analyze the flow distribution\ngenerated by rotors and plan the robotic arm's motion based on this analysis.\nNext, a novel detection approach called Rotation-SqueezeDet is proposed to\nenable rotation-aware grasping, which can give the target position and rotation\nangle in near real-time on Jetson TX2. Finally, the effectiveness of the\nproposed scheme is validated in multiple experimental trials, highlighting it's\napplicability of autonomous aerial grasping in GPS-denied environments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:28:31 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Lin", "Shijie", ""], ["Wang", "Jinwang", ""], ["Yang", "Wen", ""], ["Xia", "Guisong", ""]]}, {"id": "1811.03945", "submitter": "Jianlong Wu", "authors": "Xingyu Xie, Jianlong Wu, Guangcan Liu, Jun Wang", "title": "Matrix Recovery with Implicitly Low-Rank Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of matrix recovery, which aims to restore\na target matrix of authentic samples from grossly corrupted observations. Most\nof the existing methods, such as the well-known Robust Principal Component\nAnalysis (RPCA), assume that the target matrix we wish to recover is low-rank.\nHowever, the underlying data structure is often non-linear in practice,\ntherefore the low-rankness assumption could be violated. To tackle this issue,\nwe propose a novel method for matrix recovery in this paper, which could well\nhandle the case where the target matrix is low-rank in an implicit feature\nspace but high-rank or even full-rank in its original form. Namely, our method\npursues the low-rank structure of the target matrix in an implicit feature\nspace. By making use of the specifics of an accelerated proximal gradient based\noptimization algorithm, the proposed method could recover the target matrix\nwith non-linear structures from its corrupted version. Comprehensive\nexperiments on both synthetic and real datasets demonstrate the superiority of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:04:24 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Xie", "Xingyu", ""], ["Wu", "Jianlong", ""], ["Liu", "Guangcan", ""], ["Wang", "Jun", ""]]}, {"id": "1811.04000", "submitter": "Alexey Ozerov", "authors": "Sanjeel Parekh, Alexey Ozerov, Slim Essid (LTCI), Ngoc Duong, Patrick\n  P\\'erez, Ga\\\"el Richard (LTCI)", "title": "Identify, locate and separate: Audio-visual object extraction in large\n  video collections using weak supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of audiovisual scene analysis for weakly-labeled data.\nTo this end, we build upon our previous audiovisual representation learning\nframework to perform object classification in noisy acoustic environments and\nintegrate audio source enhancement capability. This is made possible by a novel\nuse of non-negative matrix factorization for the audio modality. Our approach\nis founded on the multiple instance learning paradigm. Its effectiveness is\nestablished through experiments over a challenging dataset of music instrument\nperformance videos. We also show encouraging visual object localization\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 16:19:41 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Parekh", "Sanjeel", "", "LTCI"], ["Ozerov", "Alexey", "", "LTCI"], ["Essid", "Slim", "", "LTCI"], ["Duong", "Ngoc", "", "LTCI"], ["P\u00e9rez", "Patrick", "", "LTCI"], ["Richard", "Ga\u00ebl", "", "LTCI"]]}, {"id": "1811.04045", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Shunxing Bao, Camilo Bermudez, Hyeonsoo\n  Moon, Prasanna Parvathaneni, Tamara K. Moyo, Michael R. Savona, Albert Assad,\n  Richard G. Abramson, Bennett A. Landman", "title": "Splenomegaly Segmentation on Multi-modal MRI using Deep Convolutional\n  Networks", "comments": "Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The findings of splenomegaly, abnormal enlargement of the spleen, is a\nnon-invasive clinical biomarker for liver and spleen disease. Automated\nsegmentation methods are essential to efficiently quantify splenomegaly from\nclinically acquired abdominal magnetic resonance imaging (MRI) scans. However,\nthe task is challenging due to (1) large anatomical and spatial variations of\nsplenomegaly, (2) large inter- and intra-scan intensity variations on\nmulti-modal MRI, and (3) limited numbers of labeled splenomegaly scans. In this\npaper, we propose the Splenomegaly Segmentation Network (SS-Net) to introduce\nthe deep convolutional neural network (DCNN) approaches in multi-modal MRI\nsplenomegaly segmentation. Large convolutional kernel layers were used to\naddress the spatial and anatomical variations, while the conditional generative\nadversarial networks (GAN) were employed to leverage the segmentation\nperformance of SS-Net in an end-to-end manner. A clinically acquired cohort\ncontaining both T1-weighted (T1w) and T2-weighted (T2w) MRI splenomegaly scans\nwas used to train and evaluate the performance of multi-atlas segmentation\n(MAS), 2D DCNN networks, and a 3D DCNN network. From the experimental results,\nthe DCNN methods achieved superior performance to the state-of-the-art MAS\nmethod. The proposed SS-Net method achieved the highest median and mean Dice\nscores among investigated baseline DCNN methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:59:57 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Bao", "Shunxing", ""], ["Bermudez", "Camilo", ""], ["Moon", "Hyeonsoo", ""], ["Parvathaneni", "Prasanna", ""], ["Moyo", "Tamara K.", ""], ["Savona", "Michael R.", ""], ["Assad", "Albert", ""], ["Abramson", "Richard G.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1811.04047", "submitter": "Hongyang Jia", "authors": "Hongyang Jia, Yinqi Tang, Hossein Valavi, Jintao Zhang and Naveen\n  Verma", "title": "A Microprocessor implemented in 65nm CMOS with Configurable and\n  Bit-scalable Accelerator for Programmable In-memory Computing", "comments": null, "journal-ref": "IEEE Journal of Solid-State Circuits, vol. 55, no. 9, pp.\n  2609-2621, Sept. 2020", "doi": "10.1109/JSSC.2020.2987714", "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a programmable in-memory-computing processor,\ndemonstrated in a 65nm CMOS technology. For data-centric workloads, such as\ndeep neural networks, data movement often dominates when implemented with\ntoday's computing architectures. This has motivated spatial architectures,\nwhere the arrangement of data-storage and compute hardware is distributed and\nexplicitly aligned to the computation dataflow, most notably for matrix-vector\nmultiplication. In-memory computing is a spatial architecture where processing\nelements correspond to dense bit cells, providing local storage and compute,\ntypically employing analog operation. Though this raises the potential for high\nenergy efficiency and throughput, analog operation has significantly limited\nrobustness, scale, and programmability. This paper describes a 590kb\nin-memory-computing accelerator integrated in a programmable processor\narchitecture, by exploiting recent approaches to charge-domain in-memory\ncomputing. The architecture takes the approach of tight coupling with an\nembedded CPU, through accelerator interfaces enabling integration in the\nstandard processor memory space. Additionally, a near-memory-computing datapath\nboth enables diverse computations locally, to address operations required\nacross applications, and enables bit-precision scalability for\nmatrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme.\nChip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput\nof 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element\nprecisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b\nweights and activations for CIFAR-10 classification consume 5.3/105.2\n$\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software\nimplementation (89.3/92.4 $\\%$ accuracy).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:03:14 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Jia", "Hongyang", ""], ["Tang", "Yinqi", ""], ["Valavi", "Hossein", ""], ["Zhang", "Jintao", ""], ["Verma", "Naveen", ""]]}, {"id": "1811.04091", "submitter": "Ali Athar", "authors": "Maryam Babaee, Ali Athar, Gerhard Rigoll", "title": "Multiple People Tracking Using Hierarchical Deep Tracklet\n  Re-identification", "comments": "13 pages (8 main + 2 bibliography + 5 appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multiple people tracking in monocular videos is challenging\nbecause of the numerous difficulties involved: occlusions, varying\nenvironments, crowded scenes, camera parameters and motion. In the\ntracking-by-detection paradigm, most approaches adopt person re-identification\ntechniques based on computing the pairwise similarity between detections.\nHowever, these techniques are less effective in handling long-term occlusions.\nBy contrast, tracklet (a sequence of detections) re-identification can improve\nassociation accuracy since tracklets offer a richer set of visual appearance\nand spatio-temporal cues. In this paper, we propose a tracking framework that\nemploys a hierarchical clustering mechanism for merging tracklets. To this end,\ntracklet re-identification is performed by utilizing a novel multi-stage deep\nnetwork that can jointly reason about the visual appearance and spatio-temporal\nproperties of a pair of tracklets, thereby providing a robust measure of\naffinity. Experimental results on the challenging MOT16 and MOT17 benchmarks\nshow that our method significantly outperforms state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 19:03:10 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 17:27:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Babaee", "Maryam", ""], ["Athar", "Ali", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1811.04110", "submitter": "Akshay Raj Dhamija", "authors": "Akshay Raj Dhamija, Manuel G\\\"unther, Terrance E. Boult", "title": "Reducing Network Agnostophobia", "comments": "Neural Information Processing Systems (NeurIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agnostophobia, the fear of the unknown, can be experienced by deep learning\nengineers while applying their networks to real-world applications.\nUnfortunately, network behavior is not well defined for inputs far from a\nnetworks training set. In an uncontrolled environment, networks face many\ninstances that are not of interest to them and have to be rejected in order to\navoid a false positive. This problem has previously been tackled by researchers\nby either a) thresholding softmax, which by construction cannot return \"none of\nthe known classes\", or b) using an additional background or garbage class. In\nthis paper, we show that both of these approaches help, but are generally\ninsufficient when previously unseen classes are encountered. We also introduce\na new evaluation metric that focuses on comparing the performance of multiple\napproaches in scenarios where such unseen classes or unknowns are encountered.\nOur major contributions are simple yet effective Entropic Open-Set and\nObjectosphere losses that train networks using negative samples from some\nclasses. These novel losses are designed to maximize entropy for unknown inputs\nwhile increasing separation in deep feature space by modifying magnitudes of\nknown and unknown samples. Experiments on networks trained to classify classes\nfrom MNIST and CIFAR-10 show that our novel loss functions are significantly\nbetter at dealing with unknown inputs from datasets such as Devanagari,\nNotMNIST, CIFAR-100, and SVHN.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 19:29:58 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 02:58:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Dhamija", "Akshay Raj", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1811.04129", "submitter": "Xiaoyang Wang", "authors": "Yang Fu, Xiaoyang Wang, Yunchao Wei, Thomas Huang", "title": "STA: Spatial-Temporal Attention for Large-Scale Video-based Person\n  Re-Identification", "comments": "Accepted as a conference paper at AAAI 2019", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  vol. 33, pp. 8287-8294. 2019", "doi": "10.1609/aaai.v33i01.33018287", "report-no": "ITD-18-58439W", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel Spatial-Temporal Attention (STA) approach to\ntackle the large-scale person re-identification task in videos. Different from\nthe most existing methods, which simply compute representations of video clips\nusing frame-level aggregation (e.g. average pooling), the proposed STA adopts a\nmore effective way for producing robust clip-level feature representation.\nConcretely, our STA fully exploits those discriminative parts of one target\nperson in both spatial and temporal dimensions, which results in a 2-D\nattention score matrix via inter-frame regularization to measure the\nimportances of spatial parts across different frames. Thus, a more robust\nclip-level feature representation can be generated according to a weighted sum\noperation guided by the mined 2-D attention score matrix. In this way, the\nchallenging cases for video-based person re-identification such as pose\nvariation and partial occlusion can be well tackled by the STA. We conduct\nextensive experiments on two large-scale benchmarks, i.e. MARS and\nDukeMTMC-VideoReID. In particular, the mAP reaches 87.7% on MARS, which\nsignificantly outperforms the state-of-the-arts with a large margin of more\nthan 11.6%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 20:43:31 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Fu", "Yang", ""], ["Wang", "Xiaoyang", ""], ["Wei", "Yunchao", ""], ["Huang", "Thomas", ""]]}, {"id": "1811.04167", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang and Piotr Koniusz", "title": "Power Normalizing Second-order Similarity Network for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second- and higher-order statistics of data points have played an important\nrole in advancing the state of the art on several computer vision problems such\nas the fine-grained image and scene recognition. However, these statistics need\nto be passed via an appropriate pooling scheme to obtain the best performance.\nPower Normalizations are non-linear activation units which enjoy\nprobability-inspired derivations and can be applied in CNNs. In this paper, we\npropose a similarity learning network leveraging second-order information and\nPower Normalizations. To this end, we propose several formulations capturing\nsecond-order statistics and derive a sigmoid-like Power Normalizing function to\ndemonstrate its interpretability. Our model is trained end-to-end to learn the\nsimilarity between the support set and query images for the problem of one- and\nfew-shot learning. The evaluations on Omniglot, miniImagenet and Open MIC\ndatasets demonstrate that this network obtains state-of-the-art results on\nseveral few-shot learning protocols.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 00:50:06 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Hongguang", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1811.04172", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Graham Healy, Alan F. Smeaton, Tomas E. Ward", "title": "Use of Neural Signals to Evaluate the Quality of Generative Adversarial\n  Network Performance in Facial Image Generation", "comments": null, "journal-ref": "Cognitive Computation, August 2019", "doi": "10.1007/s12559-019-09670-y", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in using generative adversarial networks (GANs)\nto produce image content that is indistinguishable from real images as judged\nby a typical person. A number of GAN variants for this purpose have been\nproposed, however, evaluating GANs performance is inherently difficult because\ncurrent methods for measuring the quality of their output are not always\nconsistent with what a human perceives. We propose a novel approach that\ncombines a brain-computer interface (BCI) with GANs to generate a measure we\ncall Neuroscore, which closely mirrors the behavioral ground truth measured\nfrom participants tasked with discerning real from synthetic images. This\ntechnique we call a neuro-AI interface, as it provides an interface between a\nhuman's neural systems and an AI process. In this paper, we first compare the\nthree most widely used metrics in the literature for evaluating GANs in terms\nof visual quality and compare their outputs with human judgments. Secondly we\npropose and demonstrate a novel approach using neural signals and rapid serial\nvisual presentation (RSVP) that directly measures a human perceptual response\nto facial production quality, independent of a behavioral response measurement.\nThe correlation between our proposed Neuroscore and human perceptual judgments\nhas Pearson correlation statistics: $\\mathrm{r}(48) = -0.767, \\mathrm{p} =\n2.089e-10$. We also present the bootstrap result for the correlation i.e.,\n$\\mathrm{p}\\leq 0.0001$. Results show that our Neuroscore is more consistent\nwith human judgment compared to the conventional metrics we evaluated. We\nconclude that neural signals have potential applications for high quality,\nrapid evaluation of GANs in the context of visual image synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 01:37:56 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 16:22:03 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 15:28:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Wang", "Zhengwei", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""]]}, {"id": "1811.04179", "submitter": "Valts Blukis", "authors": "Valts Blukis, Dipendra Misra, Ross A. Knepper, Yoav Artzi", "title": "Mapping Navigation Instructions to Continuous Control Actions with\n  Position-Visitation Prediction", "comments": "Appeared in Conference on Robot Learning 2018", "journal-ref": "In Conference on Robot Learning (pp. 505-518) (2018)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for mapping natural language instructions and raw\nobservations to continuous control of a quadcopter drone. Our model predicts\ninterpretable position-visitation distributions indicating where the agent\nshould go during execution and where it should stop, and uses the predicted\ndistributions to select the actions to execute. This two-step model\ndecomposition allows for simple and efficient training using a combination of\nsupervised learning and imitation learning. We evaluate our approach with a\nrealistic drone simulator, and demonstrate absolute task-completion accuracy\nimprovements of 16.85% over two state-of-the-art instruction-following methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 02:57:38 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 18:37:30 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Blukis", "Valts", ""], ["Misra", "Dipendra", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.04184", "submitter": "Farshid Farhat", "authors": "Farshid Farhat, Mohammad Mahdi Kamani, James Z. Wang", "title": "CAPTAIN: Comprehensive Composition Assistance for Photo Taking", "comments": "30 pages, 21 figures, 4 tables, submitted to IJCV (International\n  Journal of Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people are interested in taking astonishing photos and sharing with\nothers. Emerging hightech hardware and software facilitate ubiquitousness and\nfunctionality of digital photography. Because composition matters in\nphotography, researchers have leveraged some common composition techniques to\nassess the aesthetic quality of photos computationally. However, composition\ntechniques developed by professionals are far more diverse than well-documented\ntechniques can cover. We leverage the vast underexplored innovations in\nphotography for computational composition assistance. We propose a\ncomprehensive framework, named CAPTAIN (Composition Assistance for Photo\nTaking), containing integrated deep-learned semantic detectors, sub-genre\ncategorization, artistic pose clustering, personalized aesthetics-based image\nretrieval, and style set matching. The framework is backed by a large dataset\ncrawled from a photo-sharing Website with mostly photography enthusiasts and\nprofessionals. The work proposes a sequence of steps that have not been\nexplored in the past by researchers. The work addresses personal preferences\nfor composition through presenting a ranked-list of photographs to the user\nbased on user-specified weights in the similarity measure. The matching\nalgorithm recognizes the best shot among a sequence of shots with respect to\nthe user's preferred style set. We have conducted a number of experiments on\nthe newly proposed components and reported findings. A user study demonstrates\nthat the work is useful to those taking photos.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 03:43:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Farhat", "Farshid", ""], ["Kamani", "Mohammad Mahdi", ""], ["Wang", "James Z.", ""]]}, {"id": "1811.04199", "submitter": "Amir Ashouri", "authors": "Amir H. Ashouri, Tarek S. Abdelrahman, Alwyn Dos Remedios", "title": "Fast On-the-fly Retraining-free Sparsification of Convolutional Neural\n  Networks", "comments": "Extended Version of Our Accepted Paper in NIPS 2018, CDNNRIA\n  Workshop: (https://nips.cc/Conferences/2018/Schedule?showEvent=10941)-\n  Reviews are available at OpenReview\n  (https://openreview.net/forum?id=rkz1YD0vjm)", "journal-ref": "Elsevier Neurocomputing, 2019", "doi": "10.1016/j.neucom.2019.08.063", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Convolutional Neural Networks (CNNs) are complex, encompassing\nmillions of parameters. Their deployment exerts computational, storage and\nenergy demands, particularly on embedded platforms. Existing approaches to\nprune or sparsify CNNs require retraining to maintain inference accuracy. Such\nretraining is not feasible in some contexts. In this paper, we explore the\nsparsification of CNNs by proposing three model-independent methods. Our\nmethods are applied on-the-fly and require no retraining. We show that the\nstate-of-the-art models' weights can be reduced by up to 73% (compression\nfactor of 3.7x) without incurring more than 5% loss in Top-5 accuracy.\nAdditional fine-tuning gains only 8% in sparsity, which indicates that our fast\non-the-fly methods are effective.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 05:43:36 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 23:54:25 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 17:03:08 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Ashouri", "Amir H.", ""], ["Abdelrahman", "Tarek S.", ""], ["Remedios", "Alwyn Dos", ""]]}, {"id": "1811.04208", "submitter": "Ruotao Xu Dr", "authors": "Ruotao Xu, Yuhui Quan, Yong Xu", "title": "Image Cartoon-Texture Decomposition Using Isotropic Patch Recurrence", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at separating the cartoon and texture layers from an image,\ncartoon-texture decomposition approaches resort to image priors to model\ncartoon and texture respectively. In recent years, patch recurrence has emerged\nas a powerful prior for image recovery. However, the existing strategies of\nusing patch recurrence are ineffective to cartoon-texture decomposition, as\nboth cartoon contours and texture patterns exhibit strong patch recurrence in\nimages. To address this issue, we introduce the isotropy prior of patch\nrecurrence, that the spatial configuration of similar patches in texture\nexhibits the isotropic structure which is different from that in cartoon, to\nmodel the texture component. Based on the isotropic patch recurrence, we\nconstruct a nonlocal sparsification system which can effectively distinguish\nwell-patterned features from contour edges. Incorporating the constructed\nnonlocal system into morphology component analysis, we develop an effective\nmethod to both noiseless and noisy cartoon-texture decomposition. The\nexperimental results have demonstrated the superior performance of the proposed\nmethod to the existing ones, as well as the effectiveness of the isotropic\npatch recurrence prior.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 07:39:55 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Xu", "Ruotao", ""], ["Quan", "Yuhui", ""], ["Xu", "Yong", ""]]}, {"id": "1811.04217", "submitter": "Abdul Sadka Hamid", "authors": "Bodor Almatrouk, Mohammad Rafiq Swash, Abdul Hamid Sadka", "title": "Innovative 3D Depth Map Generation From A Holoscopic 3D Image Based on\n  Graph Cut Technique", "comments": "6 pages, 3 figures, international journal on recent and innovation\n  trends in computing and communication, (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holoscopic 3D imaging is a promising technique for capturing full colour\nspatial 3D images using a single aperture holoscopic 3D camera. It mimics fly's\neye technique with a microlens array, which views the scene at a slightly\ndifferent angle to its adjacent lens that records three dimensional information\nonto a two dimensional surface. This paper proposes a method of depth map\ngeneration from a holoscopic 3D image based on graph cut technique. The\nprincipal objective of this study is to estimate the depth information\npresented in a holoscopic 3D image with high precision. As such, depth map\nextraction is measured from a single still holoscopic 3D image which consists\nof multiple viewpoint images. The viewpoints are extracted and utilised for\ndisparity calculation via disparity space image technique and pixels\ndisplacement is measured with sub pixel accuracy to overcome the issue of the\nnarrow baseline between the viewpoint images for stereo matching. In addition,\ncost aggregation is used to correlate the matching costs within a particular\nneighbouring region using sum of absolute difference SAD combined with\ngradient-based metric and winner takes all algorithm is employed to select the\nminimum elements in the array as optimal disparity value. Finally, the optimal\ndepth map is obtained using graph cut technique. The proposed method extends\nthe utilisation of holoscopic 3D imaging system and enables the expansion of\nthe technology for various applications of autonomous robotics, medical,\ninspection, AR VR, security and entertainment where 3D depth sensing and\nmeasurement are a concern.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 08:59:19 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Almatrouk", "Bodor", ""], ["Swash", "Mohammad Rafiq", ""], ["Sadka", "Abdul Hamid", ""]]}, {"id": "1811.04230", "submitter": "Subhankar Chattoraj", "authors": "Sawon Pratiher, Subhankar Chattoraj and Rajdeep Mukherjee", "title": "StationPlot: A New Non-stationarity Quantification Tool for Detection of\n  Epileptic Seizures", "comments": "This paper is accepted for presentation at IEEE Global Conference on\n  Signal and Information Processing (IEEE GlobalSIP), California, USA, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel non-stationarity visualization tool known as StationPlot is developed\nfor deciphering the chaotic behavior of a dynamical time series. A family of\nanalytic measures enumerating geometrical aspects of the non-stationarity &\ndegree of variability is formulated by convex hull geometry (CHG) on\nStationPlot. In the Euclidean space, both trend-stationary (TS) &\ndifference-stationary (DS) perturbations are comprehended by the asymmetric\nstructure of StationPlot's region of interest (ROI). The proposed method is\nexperimentally validated using EEG signals, where it comprehend the relative\ntemporal evolution of neural dynamics & its non-stationary morphology, thereby\nexemplifying its diagnostic competence for seizure activity (SA) detection.\nExperimental results & analysis-of-Variance (ANOVA) on the extracted CHG\nfeatures demonstrates better classification performances as compared to the\nexisting shallow feature based state-of-the-art & validates its efficacy as\ngeometry-rich discriminative descriptors for signal processing applications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 10:34:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Pratiher", "Sawon", ""], ["Chattoraj", "Subhankar", ""], ["Mukherjee", "Rajdeep", ""]]}, {"id": "1811.04237", "submitter": "Guyue Hu", "authors": "Guyue Hu, Bo Cui, Shan Yu", "title": "Skeleton-Based Action Recognition with Synchronous Local and Non-local\n  Spatio-temporal Learning and Frequency Attention", "comments": "6 pages,4 figures; accepted to ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from its succinctness and robustness, skeleton-based action\nrecognition has recently attracted much attention. Most existing methods\nutilize local networks (e.g., recurrent, convolutional, and graph convolutional\nnetworks) to extract spatio-temporal dynamics hierarchically. As a consequence,\nthe local and non-local dependencies, which contain more details and semantics\nrespectively, are asynchronously captured in different level of layers.\nMoreover, existing methods are limited to the spatio-temporal domain and ignore\ninformation in the frequency domain. To better extract synchronous detailed and\nsemantic information from multi-domains, we propose a residual frequency\nattention (rFA) block to focus on discriminative patterns in the frequency\ndomain, and a synchronous local and non-local (SLnL) block to simultaneously\ncapture the details and semantics in the spatio-temporal domain. Besides, a\nsoft-margin focal loss (SMFL) is proposed to optimize the learning whole\nprocess, which automatically conducts data selection and encourages intrinsic\nmargins in classifiers. Our approach significantly outperforms other\nstate-of-the-art methods on several large-scale datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 11:33:08 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 12:12:31 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 12:50:41 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Hu", "Guyue", ""], ["Cui", "Bo", ""], ["Yu", "Shan", ""]]}, {"id": "1811.04239", "submitter": "Geesara Prathap Kulathunga", "authors": "Geesara Prathap, Titus Nanda Kumara, Roshan Ragel", "title": "Near Real-Time Data Labeling Using a Depth Sensor for EMG Based\n  Prosthetic Arms", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01057-7_25", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing sEMG (Surface Electromyography) signals belonging to a particular\naction (e.g., lateral arm raise) automatically is a challenging task as EMG\nsignals themselves have a lot of variation even for the same action due to\nseveral factors. To overcome this issue, there should be a proper separation\nwhich indicates similar patterns repetitively for a particular action in raw\nsignals. A repetitive pattern is not always matched because the same action can\nbe carried out with different time duration. Thus, a depth sensor (Kinect) was\nused for pattern identification where three joint angles were recording\ncontinuously which is clearly separable for a particular action while recording\nsEMG signals. To Segment out a repetitive pattern in angle data, MDTW (Moving\nDynamic Time Warping) approach is introduced. This technique is allowed to\nretrieve suspected motion of interest from raw signals. MDTW based on DTW\nalgorithm, but it will be moving through the whole dataset in a pre-defined\nmanner which is capable of picking up almost all the suspected segments inside\na given dataset an optimal way. Elevated bicep curl and lateral arm raise\nmovements are taken as motions of interest to show how the proposed technique\ncan be employed to achieve auto identification and labelling. The full\nimplementation is available at https://github.com/GPrathap/OpenBCIPython\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 11:59:56 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Prathap", "Geesara", ""], ["Kumara", "Titus Nanda", ""], ["Ragel", "Roshan", ""]]}, {"id": "1811.04241", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Chris Yakopcic, Tarek M. Taha, and Vijayan K. Asari", "title": "Breast Cancer Classification from Histopathological Images with\n  Inception Recurrent Residual Convolutional Neural Network", "comments": "15 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Deep Convolutional Neural Network (DCNN) is one of the most powerful and\nsuccessful deep learning approaches. DCNNs have already provided superior\nperformance in different modalities of medical imaging including breast cancer\nclassification, segmentation, and detection. Breast cancer is one of the most\ncommon and dangerous cancers impacting women worldwide. In this paper, we have\nproposed a method for breast cancer classification with the Inception Recurrent\nResidual Convolutional Neural Network (IRRCNN) model. The IRRCNN is a powerful\nDCNN model that combines the strength of the Inception Network (Inception-v4),\nthe Residual Network (ResNet), and the Recurrent Convolutional Neural Network\n(RCNN). The IRRCNN shows superior performance against equivalent Inception\nNetworks, Residual Networks, and RCNNs for object recognition tasks. In this\npaper, the IRRCNN approach is applied for breast cancer classification on two\npublicly available datasets including BreakHis and Breast Cancer Classification\nChallenge 2015. The experimental results are compared against the existing\nmachine learning and deep learning-based approaches with respect to\nimage-based, patch-based, image-level, and patient-level classification. The\nIRRCNN model provides superior classification performance in terms of\nsensitivity, Area Under the Curve (AUC), the ROC curve, and global accuracy\ncompared to existing approaches for both datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 12:10:14 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1811.04247", "submitter": "Geesara Prathap Kulathunga", "authors": "Geesara Prathap, Ilya Afanasyev", "title": "Deep Learning Approach for Building Detection in Satellite Multispectral\n  Imagery", "comments": null, "journal-ref": null, "doi": "10.1109/IS.2018.8710471", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building detection from satellite multispectral imagery data is being a\nfundamental but a challenging problem mainly because it requires correct\nrecovery of building footprints from high-resolution images. In this work, we\npropose a deep learning approach for building detection by applying numerous\nenhancements throughout the process. Initial dataset is preprocessed by 2-sigma\npercentile normalization. Then data preparation includes ensemble modelling\nwhere 3 models were created while incorporating OpenStreetMap data. Binary\nDistance Transformation (BDT) is used for improving data labeling process and\nthe U-Net (Convolutional Networks for Biomedical Image Segmentation) is\nmodified by adding batch normalization wrappers. Afterwards, it is explained\nhow each component of our approach is correlated with the final detection\naccuracy. Finally, we compare our results with winning solutions of SpaceNet 2\ncompetition for real satellite multispectral images of Vegas, Paris, Shanghai\nand Khartoum, demonstrating the importance of our solution for achieving higher\nbuilding detection accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 12:53:37 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Prathap", "Geesara", ""], ["Afanasyev", "Ilya", ""]]}, {"id": "1811.04250", "submitter": "Franklin Abodo", "authors": "Franklin Abodo, Robert Rittmuller, Brian Sumner and Andrew Berthaume", "title": "Detecting Work Zones in SHRP 2 NDS Videos Using Deep Learning Based\n  Computer Vision", "comments": "IEEE 17th International Conference on Machine Learning and\n  Applications (ICMLA 2018), 3 figures, 1 table, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturalistic driving studies seek to perform the observations of human driver\nbehavior in the variety of environmental conditions necessary to analyze,\nunderstand and predict that behavior using statistical and physical models. The\nsecond Strategic Highway Research Program (SHRP 2) funds a number of\ntransportation safety-related projects including its primary effort, the\nNaturalistic Driving Study (NDS), and an effort supplementary to the NDS, the\nRoadway Information Database (RID). This work seeks to expand the range of\nanswerable research questions that researchers might pose to the NDS and RID\ndatabases. Specifically, we present the SHRP 2 NDS Video Analytics (SNVA)\nsoftware application, which extracts information from NDS-instrumented\nvehicles' forward-facing camera footage and efficiently integrates that\ninformation into the RID, tying the video content to geolocations and other\ntrip attributes. Of particular interest to researchers and other stakeholders\nis the integration of work zone, traffic signal state and weather information.\nThe version of SNVA introduced in this paper focuses on work zone detection,\nthe highest priority. The ability to automate the discovery and cataloging of\nthis information, and to do so quickly, is especially important given the two\npetabyte (2PB) size of the NDS video data set.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 13:07:06 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Abodo", "Franklin", ""], ["Rittmuller", "Robert", ""], ["Sumner", "Brian", ""], ["Berthaume", "Andrew", ""]]}, {"id": "1811.04256", "submitter": "Shangbang Long", "authors": "Shangbang Long, Xin He, Cong Yao", "title": "Scene Text Detection and Recognition: The Deep Learning Era", "comments": "This is a pre-print of an article published in International Journal\n  of Computer Vision. The final authenticated version will be available online\n  at: https://doi.org/10.1007/s11263-020-01369-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise and development of deep learning, computer vision has been\ntremendously transformed and reshaped. As an important research area in\ncomputer vision, scene text detection and recognition has been inescapably\ninfluenced by this wave of revolution, consequentially entering the era of deep\nlearning. In recent years, the community has witnessed substantial advancements\nin mindset, approach and performance. This survey is aimed at summarizing and\nanalyzing the major changes and significant progresses of scene text detection\nand recognition in the deep learning era. Through this article, we devote to:\n(1) introduce new insights and ideas; (2) highlight recent techniques and\nbenchmarks; (3) look ahead into future trends. Specifically, we will emphasize\nthe dramatic differences brought by deep learning and the grand challenges\nstill remained. We expect that this review paper would serve as a reference\nbook for researchers in this field. Related resources are also collected and\ncompiled in our Github repository: https://github.com/Jyouhou/SceneTextPapers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 13:56:31 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 03:36:09 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 08:20:04 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 21:48:20 GMT"}, {"version": "v5", "created": "Sun, 9 Aug 2020 15:57:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Long", "Shangbang", ""], ["He", "Xin", ""], ["Yao", "Cong", ""]]}, {"id": "1811.04281", "submitter": "Yongpei Zhu", "authors": "Yongpei Zhu, Zicong Zhou, Guojun Liao, Qianxi Yang, Kehong Yuan", "title": "The Method of Multimodal MRI Brain Image Segmentation Based on\n  Differential Geometric Features", "comments": "18 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of brain tissue in magnetic resonance images (MRI) is a\ndiffcult task due to different types of brain abnormalities. Using information\nand features from multimodal MRI including T1, T1-weighted inversion recovery\n(T1-IR) and T2-FLAIR and differential geometric features including the Jacobian\ndeterminant(JD) and the curl vector(CV) derived from T1 modality can result in\na more accurate analysis of brain images. In this paper, we use the\ndifferential geometric information including JD and CV as image characteristics\nto measure the differences between different MRI images, which represent local\nsize changes and local rotations of the brain image, and we can use them as one\nCNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) to\nget more accurate results of brain segmentation. We test this method on two\ndatasets including IBSR dataset and MRBrainS datasets based on the deep\nvoxelwise residual network, namely VoxResNet, and obtain excellent improvement\nover single modality or three modalities and increases average\nDSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about\n1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset.\nMoreover, we discuss that one modality combined with its JD or CV information\ncan replace the segmentation effect of three modalities, which can provide\nmedical conveniences for doctor to diagnose because only to extract T1-modality\nMRI image of patients. Finally, we also compare the segmentation performance of\nour method in two networks, VoxResNet and U-Net network. The results show\nVoxResNet has a better performance than U-Net network with our method in brain\nMRI segmentation. We believe the proposed method can advance the performance in\nbrain segmentation and clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 16:46:28 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 13:52:22 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 03:20:56 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 15:35:04 GMT"}, {"version": "v5", "created": "Wed, 6 Mar 2019 15:39:27 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhu", "Yongpei", ""], ["Zhou", "Zicong", ""], ["Liao", "Guojun", ""], ["Yang", "Qianxi", ""], ["Yuan", "Kehong", ""]]}, {"id": "1811.04289", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, James G. Terry, Jiachen Wang, Vishwesh Nath, Camilo\n  Bermudez, Shunxing Bao, Prasanna Parvathaneni, J. Jeffery Carr, Bennett A.\n  Landman", "title": "Coronary Calcium Detection using 3D Attention Identical Dual Deep\n  Network Based on Weakly Supervised Learning", "comments": "Accepted by SPIE medical imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery calcium (CAC) is biomarker of advanced subclinical coronary\nartery disease and predicts myocardial infarction and death prior to age 60\nyears. The slice-wise manual delineation has been regarded as the gold standard\nof coronary calcium detection. However, manual efforts are time and resource\nconsuming and even impracticable to be applied on large-scale cohorts. In this\npaper, we propose the attention identical dual network (AID-Net) to perform CAC\ndetection using scan-rescan longitudinal non-contrast CT scans with weakly\nsupervised attention by only using per scan level labels. To leverage the\nperformance, 3D attention mechanisms were integrated into the AID-Net to\nprovide complementary information for classification tasks. Moreover, the 3D\nGradient-weighted Class Activation Mapping (Grad-CAM) was also proposed at the\ntesting stage to interpret the behaviors of the deep neural network. 5075\nnon-contrast chest CT scans were used as training, validation and testing\ndatasets. Baseline performance was assessed on the same cohort. From the\nresults, the proposed AID-Net achieved the superior performance on\nclassification accuracy (0.9272) and AUC (0.9627).\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 18:22:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Huo", "Yuankai", ""], ["Terry", "James G.", ""], ["Wang", "Jiachen", ""], ["Nath", "Vishwesh", ""], ["Bermudez", "Camilo", ""], ["Bao", "Shunxing", ""], ["Parvathaneni", "Prasanna", ""], ["Carr", "J. Jeffery", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1811.04303", "submitter": "Alexander Wong", "authors": "Andrew Hryniowski and Alexander Wong", "title": "PolyNeuron: Automatic Neuron Discovery via Learned Polyharmonic Spline\n  Activations", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated deep neural network architecture design has received a significant\namount of recent attention. However, this attention has not been equally shared\nby one of the fundamental building blocks of a deep neural network, the\nneurons. In this study, we propose PolyNeuron, a novel automatic neuron\ndiscovery approach based on learned polyharmonic spline activations. More\nspecifically, PolyNeuron revolves around learning polyharmonic splines,\ncharacterized by a set of control points, that represent the activation\nfunctions of the neurons in a deep neural network. A relaxed variant of\nPolyNeuron, which we term PolyNeuron-R, loosens the constraints imposed by\nPolyNeuron to reduce the computational complexity for discovering the neuron\nactivation functions in an automated manner. Experiments show both PolyNeuron\nand PolyNeuron-R lead to networks that have improved or comparable performance\non multiple network architectures (LeNet-5 and ResNet-20) using different\ndatasets (MNIST and CIFAR10). As such, automatic neuron discovery approaches\nsuch as PolyNeuron is a worthy direction to explore.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 20:14:26 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hryniowski", "Andrew", ""], ["Wong", "Alexander", ""]]}, {"id": "1811.04309", "submitter": "Soubarna Banik", "authors": "Soubarna Banik, Mikko Lauri, Simone Frintrop", "title": "Multi-label Object Attribute Classification using a Convolutional Neural\n  Network", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects of different classes can be described using a limited number of\nattributes such as color, shape, pattern, and texture. Learning to detect\nobject attributes instead of only detecting objects can be helpful in dealing\nwith a priori unknown objects. With this inspiration, a deep convolutional\nneural network for low-level object attribute classification, called the Deep\nAttribute Network (DAN), is proposed. Since object features are implicitly\nlearned by object recognition networks, one such existing network is modified\nand fine-tuned for developing DAN. The performance of DAN is evaluated on the\nImageNet Attribute and a-Pascal datasets. Experiments show that in comparison\nwith state-of-the-art methods, the proposed model achieves better results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 20:27:59 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Banik", "Soubarna", ""], ["Lauri", "Mikko", ""], ["Frintrop", "Simone", ""]]}, {"id": "1811.04312", "submitter": "Hongwei Li", "authors": "Hongwei Li, Andrii Zhygallo and Bjoern Menze", "title": "Automatic Brain Structures Segmentation Using Deep Residual Dilated\n  U-Net", "comments": "to be appeared in MICCAI post-proceedings (BrainLes workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain image segmentation is used for visualizing and quantifying anatomical\nstructures of the brain. We present an automated ap-proach using 2D deep\nresidual dilated networks which captures rich context information of different\ntissues for the segmentation of eight brain structures. The proposed system was\nevaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22\nteams. We further compared the method with traditional U-Net using\nleave-one-subject-out cross-validation setting on the public dataset.\nExperimental results shows that the proposed method outperforms traditional\nU-Net (i.e. 80.9% vs 78.3% in averaged Dice score, 4.35mm vs 11.59mm in\naveraged robust Hausdorff distance) and is computationally efficient.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 20:47:46 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Li", "Hongwei", ""], ["Zhygallo", "Andrii", ""], ["Menze", "Bjoern", ""]]}, {"id": "1811.04323", "submitter": "Ryosuke Furuta", "authors": "Ryosuke Furuta, Naoto Inoue, Toshihiko Yamasaki", "title": "Fully Convolutional Network with Multi-Step Reinforcement Learning for\n  Image Processing", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles a new problem setting: reinforcement learning with\npixel-wise rewards (pixelRL) for image processing. After the introduction of\nthe deep Q-network, deep RL has been achieving great success. However, the\napplications of deep RL for image processing are still limited. Therefore, we\nextend deep RL to pixelRL for various image processing applications. In\npixelRL, each pixel has an agent, and the agent changes the pixel value by\ntaking an action. We also propose an effective learning method for pixelRL that\nsignificantly improves the performance by considering not only the future\nstates of the own pixel but also those of the neighbor pixels. The proposed\nmethod can be applied to some image processing tasks that require pixel-wise\nmanipulations, where deep RL has never been applied. We apply the proposed\nmethod to three image processing tasks: image denoising, image restoration, and\nlocal color enhancement. Our experimental results demonstrate that the proposed\nmethod achieves comparable or better performance, compared with the\nstate-of-the-art methods based on supervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 22:59:44 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:58:01 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Furuta", "Ryosuke", ""], ["Inoue", "Naoto", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "1811.04346", "submitter": "Vishal Agarwal", "authors": "Vishal Agarwal", "title": "Deep Face Quality Assessment", "comments": "Course project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face image quality is an important factor in facial recognition systems as\nits verification and recognition accuracy is highly dependent on the quality of\nimage presented. Rejecting low quality images can significantly increase the\naccuracy of any facial recognition system. In this project, a simple approach\nis presented to train a deep convolutional neural network to perform end-to-end\nface image quality assessment. The work is done in 2 stages : First, generation\nof quality score label and secondly, training a deep convolutional neural\nnetwork in a supervised manner to predict quality score between 0 and 1. The\ngeneration of quality labels is done by comparing the face image with a\ntemplate of best quality images and then evaluating the normalized score based\non the similarity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 04:17:10 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Agarwal", "Vishal", ""]]}, {"id": "1811.04358", "submitter": "Ahmed ElSayed", "authors": "Ahmed ElSayed, Elif Kongar, Ausif Mahmood, Tarek Sobh, Terrance Boult", "title": "Neural Generative Models for 3D Faces with Application in 3D Texture\n  Free Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using heterogeneous depth cameras and 3D scanners in 3D face verification\ncauses variations in the resolution of the 3D point clouds. To solve this\nissue, previous studies use 3D registration techniques. Out of these proposed\ntechniques, detecting points of correspondence is proven to be an efficient\nmethod given that the data belongs to the same individual. However, if the data\nbelongs to different persons, the registration algorithms can convert the 3D\npoint cloud of one person to another, destroying the distinguishing features\nbetween the two point clouds. Another issue regarding the storage size of the\npoint clouds. That is, if the captured depth image contains around 50 thousand\npoints in the cloud for a single pose for one individual, then the storage size\nof the entire dataset will be in order of giga if not tera bytes. With these\nmotivations, this work introduces a new technique for 3D point clouds\ngeneration using a neural modeling system to handle the differences caused by\nheterogeneous depth cameras, and to generate a new face canonical compact\nrepresentation. The proposed system reduces the stored 3D dataset size, and if\nrequired, provides an accurate dataset regeneration. Furthermore, the system\ngenerates neural models for all gallery point clouds and stores these models to\nrepresent the faces in the recognition or verification processes. For the probe\ncloud to be verified, a new model is generated specifically for that particular\ncloud and is matched against pre-stored gallery model presentations to identify\nthe query cloud. This work also introduces the utilization of Siamese deep\nneural network in 3D face verification using generated model representations as\nraw data for the deep network, and shows that the accuracy of the trained\nnetwork is comparable all published results on Bosphorus dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 05:56:09 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["ElSayed", "Ahmed", ""], ["Kongar", "Elif", ""], ["Mahmood", "Ausif", ""], ["Sobh", "Tarek", ""], ["Boult", "Terrance", ""]]}, {"id": "1811.04370", "submitter": "Soham Saha", "authors": "Soham Saha, Girish Varma, C.V.Jawahar", "title": "Improved Visual Relocalization by Discovering Anchor Points", "comments": "10 Pages, 6 figures, BMVC 2018, Newcastle, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We address the visual relocalization problem of predicting the location and\ncamera orientation or pose (6DOF) of the given input scene. We propose a method\nbased on how humans determine their location using the visible landmarks. We\ndefine anchor points uniformly across the route map and propose a deep learning\narchitecture which predicts the most relevant anchor point present in the scene\nas well as the relative offsets with respect to it. The relevant anchor point\nneed not be the nearest anchor point to the ground truth location, as it might\nnot be visible due to the pose. Hence we propose a multi task loss function,\nwhich discovers the relevant anchor point, without needing the ground truth for\nit. We validate the effectiveness of our approach by experimenting on\nCambridgeLandmarks (large scale outdoor scenes) as well as 7 Scenes (indoor\nscenes) using variousCNN feature extractors. Our method improves the median\nerror in indoor as well as outdoor localization datasets compared to the\nprevious best deep learning model known as PoseNet (with geometric\nre-projection loss) using the same feature extractor. We improve the median\nerror in localization in the specific case of Street scene, by over 8m.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 08:39:57 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Saha", "Soham", ""], ["Varma", "Girish", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1811.04374", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Thomas Lidy, Stephan Karner, Matthias Hecker", "title": "Fashion and Apparel Classification using Convolutional Neural Networks", "comments": "Proceedings of the 10th Forum Media Technology and 3rd All Around\n  Audio Symposium, St. Poelten, Austria, November 29-30, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical study of applying deep Convolutional Neural Networks\n(CNN) to the task of fashion and apparel image classification to improve\nmeta-data enrichment of e-commerce applications. Five different CNN\narchitectures were analyzed using clean and pre-trained models. The models were\nevaluated in three different tasks person detection, product and gender\nclassification, on two small and large scale datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 09:18:32 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Schindler", "Alexander", ""], ["Lidy", "Thomas", ""], ["Karner", "Stephan", ""], ["Hecker", "Matthias", ""]]}, {"id": "1811.04387", "submitter": "Yunho Jeon", "authors": "Yunho Jeon, Junmo Kim", "title": "Integrating Multiple Receptive Fields through Grouped Active Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have achieved great success in various vision tasks.\nThis is mainly due to a considerable amount of research on network structure.\nIn this study, instead of focusing on architectures, we focused on the\nconvolution unit itself. The existing convolution unit has a fixed shape and is\nlimited to observing restricted receptive fields. In earlier work, we proposed\nthe active convolution unit (ACU), which can freely define its shape and learn\nby itself. In this paper, we provide a detailed analysis of the previously\nproposed unit and show that it is an efficient representation of a sparse\nweight convolution. Furthermore, we extend an ACU to a grouped ACU, which can\nobserve multiple receptive fields in one layer. We found that the performance\nof a naive grouped convolution is degraded by increasing the number of groups;\nhowever, the proposed unit retains the accuracy even though the number of\nparameters decreases. Based on this result, we suggest a depthwise ACU, and\nvarious experiments have shown that our unit is efficient and can replace the\nexisting convolutions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 10:32:43 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 03:59:18 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jeon", "Yunho", ""], ["Kim", "Junmo", ""]]}, {"id": "1811.04406", "submitter": "Kasanagottu Sai Ram", "authors": "K. Sai Ram, Jayanta Mukherjee, Amit Patra and Partha Pratim Das", "title": "HSD-CNN: Hierarchically self decomposing CNN architecture using class\n  specific filter sensitivity analysis", "comments": "Accepted in ICVGIP,2018", "journal-ref": null, "doi": "10.1145/3293353.3293383", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Convolutional neural networks (CNN) are trained on large domain\ndatasets and are hence typically over-represented and inefficient in limited\nclass applications. An efficient way to convert such large many-class\npre-trained networks into small few-class networks is through a hierarchical\ndecomposition of its feature maps. To alleviate this issue, we propose an\nautomated framework for such decomposition in Hierarchically Self Decomposing\nCNN (HSD-CNN), in four steps. HSD-CNN is derived automatically using a\nclass-specific filter sensitivity analysis that quantifies the impact of\nspecific features on a class prediction. The decomposed hierarchical network\ncan be utilized and deployed directly to obtain sub-networks for a subset of\nclasses, and it is shown to perform better without the requirement of\nretraining these sub-networks. Experimental results show that HSD-CNN generally\ndoes not degrade accuracy if the full set of classes are used. Interestingly,\nwhen operating on known subsets of classes, HSD-CNN has an improvement in\naccuracy with a much smaller model size, requiring much fewer operations.\nHSD-CNN flow is verified on the CIFAR10, CIFAR100 and CALTECH101 data sets. We\nreport accuracies up to $85.6\\%$ ( $94.75\\%$ ) on scenarios with 13 ( 4 )\nclasses of CIFAR100, using a pre-trained VGG-16 network on the full data set.\nIn this case, the proposed HSD-CNN requires $3.97 \\times$ fewer parameters and\nhas $71.22\\%$ savings in operations, in comparison to baseline VGG-16\ncontaining features for all 100 classes.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 12:20:18 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 21:34:36 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Ram", "K. Sai", ""], ["Mukherjee", "Jayanta", ""], ["Patra", "Amit", ""], ["Das", "Partha Pratim", ""]]}, {"id": "1811.04407", "submitter": "Liu Yuezhang", "authors": "Liu Yuezhang, Ruohan Zhang, Dana H. Ballard", "title": "An initial attempt of combining visual selective attention with deep\n  reinforcement learning", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention serves as a means of feature selection mechanism in the\nperceptual system. Motivated by Broadbent's leaky filter model of selective\nattention, we evaluate how such mechanism could be implemented and affect the\nlearning process of deep reinforcement learning. We visualize and analyze the\nfeature maps of DQN on a toy problem Catch, and propose an approach to combine\nvisual selective attention with deep reinforcement learning. We experiment with\noptical flow-based attention and A2C on Atari games. Experiment results show\nthat visual selective attention could lead to improvements in terms of sample\nefficiency on tested games. An intriguing relation between attention and batch\nnormalization is also discovered.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 12:22:44 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 07:14:00 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 17:48:44 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yuezhang", "Liu", ""], ["Zhang", "Ruohan", ""], ["Ballard", "Dana H.", ""]]}, {"id": "1811.04437", "submitter": "Bo Zhou", "authors": "Bo Zhou, Randolph Crawford, Belma Dogdas, Gregory Goldmacher, Antong\n  Chen", "title": "A Progressively-trained Scale-invariant and Boundary-aware Deep Neural\n  Network for the Automatic 3D Segmentation of Lung Lesions", "comments": "10 pages, 7 figures, 3 tables, accepted by 2019 IEEE Winter\n  Conference on Applications of Computer Vision (2019 WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Volumetric segmentation of lesions on CT scans is important for many types of\nanalysis, including lesion growth kinetic modeling in clinical trials and\nmachine learning of radiomic features. Manual segmentation is laborious, and\nimpractical for large-scale use. For routine clinical use, and in clinical\ntrials that apply the Response Evaluation Criteria In Solid Tumors (RECIST),\nclinicians typically outline the boundaries of a lesion on a single slice to\nextract diameter measurements. In this work, we have collected a large-scale\ndatabase, named LesionVis, with pixel-wise manual 2D lesion delineations on the\nRECIST-slices. To extend the 2D segmentations to 3D, we propose a volumetric\nprogressive lesion segmentation (PLS) algorithm to automatically segment the 3D\nlesion volume from 2D delineations using a scale-invariant and boundary-aware\ndeep convolutional network (SIBA-Net). The SIBA-Net copes with the size\ntransition of a lesion when the PLS progresses from the RECIST-slice to the\nedge-slices, as well as when performing longitudinal assessment of lesions\nwhose size change over multiple time points. The proposed PLS-SiBA-Net (P-SiBA)\napproach is assessed on the lung lesion cases from LesionVis. Our experimental\nresults demonstrate that the P-SiBA approach achieves mean Dice similarity\ncoefficients (DSC) of 0.81, which significantly improves 3D segmentation\naccuracy compared with the approaches proposed previously (highest mean DSC at\n0.78 on LesionVis). In summary, by leveraging the limited 2D delineations on\nthe RECIST-slices, P-SiBA is an effective semi-supervised approach to produce\naccurate lesion segmentations in 3D.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 17:32:38 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhou", "Bo", ""], ["Crawford", "Randolph", ""], ["Dogdas", "Belma", ""], ["Goldmacher", "Gregory", ""], ["Chen", "Antong", ""]]}, {"id": "1811.04453", "submitter": "Peetak Mitra", "authors": "Peetak Mitra", "title": "Pedestrian Collision Avoidance System (PeCAS): a Deep Learning Framework", "comments": "arXiv admin note: text overlap with arXiv:1412.0069 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning based framework to identify pedestrians, and\ncaution distracted drivers, in an effort to prevent the loss of life and\nproperty. This framework uses two Convolutional Neural Networks (CNN), one\nwhich detects pedestrians and the second which predicts the onset of drowsiness\nin a driver, is implemented on a Raspberry Pi 3 Model B+, shows great promise.\nThe algorithm for implementing such a low-cost, low-compute model is presented\nand the results discussed.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:00:12 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:09:24 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Mitra", "Peetak", ""]]}, {"id": "1811.04457", "submitter": "Lina Felsner", "authors": "Shiyang Hu, Lina Felsner, Andreas Maier, Veronika Ludwig, Gisela\n  Anton, and Christian Riess", "title": "A 3-D Projection Model for X-ray Dark-field Imaging", "comments": "Shiyang Hu and Lina Felsner contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Talbot-Lau X-ray phase-contrast imaging is a novel imaging modality, which\nprovides not only an X-ray absorption image, but also additionally a\ndifferential phase image and a dark-field image. The dark-field image is\nrelated to small angle scattering and has an interesting property when canning\noriented structures: the recorded signal depends on the relative orientation of\nthe structure in the imaging system. Exactly this property allows to draw\nconclusions about the orientation and to reconstruct the structure. However,\nthe reconstruction is a complex, non-trivial challenge. A lot of research was\nconducted towards this goal in the last years and several reconstruction\nalgorithms were proposed. A key step of the reconstruction algorithm is the\ninversion of a forward projection model. Up until now, only 2-D projection\nmodels are available, with effectively limit the scanning trajectory to a 2-D\nplane. To obtain true 3-D information, this limitation requires to combine\nseveral 2-D scans, which leads to quite complex, impractical acquisitions\nschemes. Furthermore, it is not possible with these models to use 3-D\ntrajectories that might allow simpler protocols, like for example a helical\ntrajectory. To address these limitations, we propose in this work a very\ngeneral 3-D projection model. Our projection model defines the dark-field\nsignal dependent on an arbitrarily chosen ray and sensitivity direction. We\nderive the projection model under the assumption that the observed scatter\ndistribution has a Gaussian shape. We theoretically show the consistency of our\nmodel with more constrained existing 2-D models. Furthermore, we experimentally\nshow the compatibility of our model with dark-field measurements of two\nmatchsticks. We believe that this 3-D projection model is an important step\ntowards more flexible trajectories and imaging protocols that are much better\napplicable in practice.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:09:48 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 16:11:08 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Hu", "Shiyang", ""], ["Felsner", "Lina", ""], ["Maier", "Andreas", ""], ["Ludwig", "Veronika", ""], ["Anton", "Gisela", ""], ["Riess", "Christian", ""]]}, {"id": "1811.04463", "submitter": "Fayyaz Minhas", "authors": "Kanza Hamid, Amina Asif, Wajid Abbasi, Durre Sabih and Fayyaz Minhas", "title": "Machine Learning with Abstention for Automated Liver Disease Diagnosis", "comments": "Preprint version before submission for publication. complete version\n  published in proc. 15th International Conference on Frontiers of Information\n  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan.\n  http://ieeexplore.ieee.org/document/8261064/", "journal-ref": "15th IEEE International Conference on Frontiers of Information\n  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan", "doi": "10.1109/FIT.2017.00070", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for detection of liver abnormalities in\nan automated manner using ultrasound images. For this purpose, we have\nimplemented a machine learning model that can not only generate labels (normal\nand abnormal) for a given ultrasound image but it can also detect when its\nprediction is likely to be incorrect. The proposed model abstains from\ngenerating the label of a test example if it is not confident about its\nprediction. Such behavior is commonly practiced by medical doctors who, when\ngiven insufficient information or a difficult case, can chose to carry out\nfurther clinical or diagnostic tests before generating a diagnosis. However,\nexisting machine learning models are designed in a way to always generate a\nlabel for a given example even when the confidence of their prediction is low.\nWe have proposed a novel stochastic gradient based solver for the learning with\nabstention paradigm and use it to make a practical, state of the art method for\nliver disease classification. The proposed method has been benchmarked on a\ndata set of approximately 100 patients from MINAR, Multan, Pakistan and our\nresults show that the proposed scheme offers state of the art classification\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:37:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hamid", "Kanza", ""], ["Asif", "Amina", ""], ["Abbasi", "Wajid", ""], ["Sabih", "Durre", ""], ["Minhas", "Fayyaz", ""]]}, {"id": "1811.04491", "submitter": "Kowshik Thopalli", "authors": "Kowshik Thopalli, Rushil Anirudh, Jayaraman J. Thiagarajan, Pavan\n  Turaga", "title": "Multiple Subspace Alignment Improves Domain Adaptation", "comments": "under review in ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised domain adaptation (DA) method for\ncross-domain visual recognition. Though subspace methods have found success in\nDA, their performance is often limited due to the assumption of approximating\nan entire dataset using a single low-dimensional subspace. Instead, we develop\na method to effectively represent the source and target datasets via a\ncollection of low-dimensional subspaces, and subsequently align them by\nexploiting the natural geometry of the space of subspaces, on the Grassmann\nmanifold. We demonstrate the effectiveness of this approach, using empirical\nstudies on two widely used benchmarks, with state of the art domain adaptation\nperformance\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 22:02:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Thopalli", "Kowshik", ""], ["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Turaga", "Pavan", ""]]}, {"id": "1811.04507", "submitter": "Yao Zhu", "authors": "Yao Zhu, Saksham Suri, Pranav Kulkarni, Yueru Chen, Jiali Duan, C.-C.\n  Jay Kuo", "title": "An Interpretable Generative Model for Handwritten Digit Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interpretable generative model for handwritten digits synthesis is\nproposed in this work. Modern image generative models, such as Generative\nAdversarial Networks (GANs) and Variational Autoencoders (VAEs), are trained by\nbackpropagation (BP). The training process is complex and the underlying\nmechanism is difficult to explain. We propose an interpretable multi-stage PCA\nmethod to achieve the same goal and use handwritten digit images synthesis as\nan illustrative example. First, we derive principal-component-analysis-based\n(PCA-based) transform kernels at each stage based on the covariance of its\ninputs. This results in a sequence of transforms that convert input images of\ncorrelated pixels to spectral vectors of uncorrelated components. In other\nwords, it is a whitening process. Then, we can synthesize an image based on\nrandom vectors and multi-stage transform kernels through a coloring process.\nThe generative model is a feedforward (FF) design since no BP is used in model\nparameter determination. Its design complexity is significantly lower, and the\nwhole design process is explainable. Finally, we design an FF generative model\nusing the MNIST dataset, compare synthesis results with those obtained by\nstate-of-the-art GAN and VAE methods, and show that the proposed generative\nmodel achieves comparable performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 23:37:07 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhu", "Yao", ""], ["Suri", "Saksham", ""], ["Kulkarni", "Pranav", ""], ["Chen", "Yueru", ""], ["Duan", "Jiali", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1811.04533", "submitter": "Qijie Zhao", "authors": "Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying Chen, Ling Cai,\n  Haibin Ling", "title": "M2Det: A Single-Shot Object Detector based on Multi-Level Feature\n  Pyramid Network", "comments": "AAAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramids are widely exploited by both the state-of-the-art one-stage\nobject detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object\ndetectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising from\nscale variation across object instances. Although these object detectors with\nfeature pyramids achieve encouraging results, they have some limitations due to\nthat they only simply construct the feature pyramid according to the inherent\nmulti-scale, pyramidal architecture of the backbones which are actually\ndesigned for object classification task. Newly, in this work, we present a\nmethod called Multi-Level Feature Pyramid Network (MLFPN) to construct more\neffective feature pyramids for detecting objects of different scales. First, we\nfuse multi-level features (i.e. multiple layers) extracted by backbone as the\nbase feature. Second, we feed the base feature into a block of alternating\njoint Thinned U-shape Modules and Feature Fusion Modules and exploit the\ndecoder layers of each u-shape module as the features for detecting objects.\nFinally, we gather up the decoder layers with equivalent scales (sizes) to\ndevelop a feature pyramid for object detection, in which every feature map\nconsists of the layers (features) from multiple levels. To evaluate the\neffectiveness of the proposed MLFPN, we design and train a powerful end-to-end\none-stage object detector we call M2Det by integrating it into the architecture\nof SSD, which gets better detection performance than state-of-the-art one-stage\ndetectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 at\nspeed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with\nmulti-scale inference strategy, which is the new state-of-the-art results among\none-stage detectors. The code will be made available on\n\\url{https://github.com/qijiezhao/M2Det.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:01:50 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:24:58 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 14:32:16 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Zhao", "Qijie", ""], ["Sheng", "Tao", ""], ["Wang", "Yongtao", ""], ["Tang", "Zhi", ""], ["Chen", "Ying", ""], ["Cai", "Ling", ""], ["Ling", "Haibin", ""]]}, {"id": "1811.04535", "submitter": "Shashank Shekhar", "authors": "Janpreet Singh, Shashank Shekhar", "title": "Road Damage Detection And Classification In Smartphone Captured Images\n  Using Mask R-CNN", "comments": "Both authors contributed equally to the paper. Under submission to\n  the IEEE International Conference On Big Data Cup 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the design, experiments and results of our solution to\nthe Road Damage Detection and Classification Challenge held as part of the 2018\nIEEE International Conference On Big Data Cup. Automatic detection and\nclassification of damage in roads is an essential problem for multiple\napplications like maintenance and autonomous driving. We demonstrate that\nconvolutional neural net based instance detection and classfication approaches\ncan be used to solve this problem. In particular we show that Mask-RCNN, one of\nthe state-of-the-art algorithms for object detection, localization and instance\nsegmentation of natural images, can be used to perform this task in a fast\nmanner with effective results. We achieve a mean F1 score of 0.528 at an IoU of\n50% on the task of detection and classification of different types of damages\nin real-world road images acquired using a smartphone camera and our average\ninference time for each image is 0.105 seconds on an NVIDIA GeForce 1080Ti\ngraphic card. The code and saved models for our approach can be found here :\nhttps://github.com/sshkhr/BigDataCup18 Submission\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:16:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Singh", "Janpreet", ""], ["Shekhar", "Shashank", ""]]}, {"id": "1811.04537", "submitter": "Pallav Bera", "authors": "Pallav Kumar Bera and Rajesh Kumar and Can Isik", "title": "Identification of Internal Faults in Indirect Symmetrical Phase Shift\n  Transformers Using Ensemble Learning", "comments": "18th IEEE International Symposium on Signal Processing and\n  Information Technology (ISSPIT), 2018", "journal-ref": "IEEE International Symposium on Signal Processing and Information\n  Technology (ISSPIT), 2018, pp. 1-6,", "doi": "10.1109/ISSPIT.2018.8705100", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes methods to identify 40 different types of internal faults\nin an Indirect Symmetrical Phase Shift Transformer (ISPST). The ISPST was\nmodeled using Power System Computer Aided Design (PSCAD)/ Electromagnetic\nTransients including DC (EMTDC). The internal faults were simulated by varying\nthe transformer tapping, backward and forward phase shifts, loading, and\npercentage of winding faulted. Data for 960 cases of each type of fault was\nrecorded. A series of features were extracted for a, b, and c phases from time,\nfrequency, time-frequency, and information theory domains. The importance of\nthe extracted features was evaluated through univariate tests which helped to\nreduce the number of features. The selected features were then used for\ntraining five state-of-the-art machine learning classifiers. Extremely Random\nTrees and Random Forest, the ensemble-based learners, achieved the accuracy of\n98.76% and 97.54% respectively outperforming Multilayer Perceptron (96.13%),\nLogistic Regression (93.54%), and Support Vector Machines (92.60%)\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:22:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bera", "Pallav Kumar", ""], ["Kumar", "Rajesh", ""], ["Isik", "Can", ""]]}, {"id": "1811.04544", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin, Jie Wu", "title": "Visual Saliency Maps Can Apply to Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human eyes concentrate different facial regions during distinct cognitive\nactivities. We study utilising facial visual saliency maps to classify\ndifferent facial expressions into different emotions. Our results show that our\nnovel method of merely using facial saliency maps can achieve a descent\naccuracy of 65\\%, much higher than the chance level of $1/7$. Furthermore, our\napproach is of semi-supervision, i.e., our facial saliency maps are generated\nfrom a general saliency prediction algorithm that is not explicitly designed\nfor face images. We also discovered that the classification accuracies of each\nemotional class using saliency maps demonstrate a strong positive correlation\nwith the accuracies produced by face images. Our work implies that humans may\nlook at different facial areas in order to perceive different emotions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:51:33 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Qin", "Zhenyue", ""], ["Wu", "Jie", ""]]}, {"id": "1811.04595", "submitter": "Anran Wang", "authors": "Anran Wang, Anh Tuan Luu, Chuan-Sheng Foo, Hongyuan Zhu, Yi Tay, Vijay\n  Chandrasekhar", "title": "Holistic Multi-modal Memory Network for Movie Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering questions according to multi-modal context is a challenging problem\nas it requires a deep integration of different data sources. Existing\napproaches only employ partial interactions among data sources in one attention\nhop. In this paper, we present the Holistic Multi-modal Memory Network (HMMN)\nframework which fully considers the interactions between different input\nsources (multi-modal context, question) in each hop. In addition, it takes\nanswer choices into consideration during the context retrieval stage.\nTherefore, the proposed framework effectively integrates multi-modal context,\nquestion, and answer information, which leads to more informative context\nretrieved for question answering. Our HMMN framework achieves state-of-the-art\naccuracy on MovieQA dataset. Extensive ablation studies show the importance of\nholistic reasoning and contributions of different attention strategies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:10:21 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wang", "Anran", ""], ["Luu", "Anh Tuan", ""], ["Foo", "Chuan-Sheng", ""], ["Zhu", "Hongyuan", ""], ["Tay", "Yi", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1811.04602", "submitter": "Maximilian M\\\"arz Mr.", "authors": "T. A. Bubba, G. Kutyniok, M. Lassas, M. M\\\"arz, W. Samek, S. Siltanen,\n  V. Srinivasan", "title": "Learning The Invisible: A Hybrid Deep Learning-Shearlet Framework for\n  Limited Angle Computed Tomography", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab10ca", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high complexity of various inverse problems poses a significant challenge\nto model-based reconstruction schemes, which in such situations often reach\ntheir limits. At the same time, we witness an exceptional success of data-based\nmethodologies such as deep learning. However, in the context of inverse\nproblems, deep neural networks mostly act as black box routines, used for\ninstance for a somewhat unspecified removal of artifacts in classical image\nreconstructions. In this paper, we will focus on the severely ill-posed inverse\nproblem of limited angle computed tomography, in which entire boundary sections\nare not captured in the measurements. We will develop a hybrid reconstruction\nframework that fuses model-based sparse regularization with data-driven deep\nlearning. Our method is reliable in the sense that we only learn the part that\ncan provably not be handled by model-based methods, while applying the\ntheoretically controllable sparse regularization technique to the remaining\nparts. Such a decomposition into visible and invisible segments is achieved by\nmeans of the shearlet transform that allows to resolve wavefront sets in the\nphase space. Furthermore, this split enables us to assign the clear task of\ninferring unknown shearlet coefficients to the neural network and thereby\noffering an interpretation of its performance in the context of limited angle\ncomputed tomography. Our numerical experiments show that our algorithm\nsignificantly surpasses both pure model- and more data-based reconstruction\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:36:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bubba", "T. A.", ""], ["Kutyniok", "G.", ""], ["Lassas", "M.", ""], ["M\u00e4rz", "M.", ""], ["Samek", "W.", ""], ["Siltanen", "S.", ""], ["Srinivasan", "V.", ""]]}, {"id": "1811.04608", "submitter": "Cong Chen", "authors": "Cong Chen, Kim Batselier, Ching-Yun Ko, Ngai Wong", "title": "Matrix Product Operator Restricted Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A restricted Boltzmann machine (RBM) learns a probability distribution over\nits input samples and has numerous uses like dimensionality reduction,\nclassification and generative modeling. Conventional RBMs accept vectorized\ndata that dismisses potentially important structural information in the\noriginal tensor (multi-way) input. Matrix-variate and tensor-variate RBMs,\nnamed MvRBM and TvRBM, have been proposed but are all restrictive by model\nconstruction, which leads to a weak model expression power. This work presents\nthe matrix product operator RBM (MPORBM) that utilizes a tensor network\ngeneralization of Mv/TvRBM, preserves input formats in both the visible and\nhidden layers, and results in higher expressive power. A novel training\nalgorithm integrating contrastive divergence and an alternating optimization\nprocedure is also developed. Numerical experiments compare the MPORBM with the\ntraditional RBM and MvRBM for data classification and image completion and\ndenoising tasks. The expressive power of the MPORBM as a function of the\nMPO-rank is also investigated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:58:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chen", "Cong", ""], ["Batselier", "Kim", ""], ["Ko", "Ching-Yun", ""], ["Wong", "Ngai", ""]]}, {"id": "1811.04620", "submitter": "Hang Yang", "authors": "Hang Yang and Zhongbo Zhang", "title": "Depth Image Upsampling based on Guided Filter with Low Gradient\n  Minimization", "comments": "28 pages, 7figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel upsampling framework to enhance the spatial\nresolution of the depth image. In our framework, the upscaling of a\nlow-resolution depth image is guided by a corresponding intensity images, we\nformulate it as a cost aggregation problem with the guided filter. However, the\nguided filter does not make full use of the properties of the depth image.\nSince depth images have quite sparse gradients, it inspires us to regularize\nthe gradients for improving depth upscaling results. Statistics show a special\nproperty of depth images, that is, there is a non-ignorable part of pixels\nwhose horizontal or vertical derivatives are equal to $\\pm 1$. Considering this\nspecial property, we propose a low gradient regularization method which reduces\nthe penalty for horizontal or vertical derivative $\\pm1$. The proposed low\ngradient regularization is integrated with the guided filter into the depth\nimage upsampling method. Experimental results demonstrate the effectiveness of\nour proposed approach both qualitatively and quantitatively compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:36:12 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yang", "Hang", ""], ["Zhang", "Zhongbo", ""]]}, {"id": "1811.04627", "submitter": "Zhen Chen", "authors": "Zhen Chen, Anthimos Georgiadis", "title": "Parameterized Synthetic Image Data Set for Fisheye Lens", "comments": "2018 5th International Conference on Information Science and Control\n  Engineering", "journal-ref": null, "doi": "10.1109/ICISCE.2018.00084", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on different projection geometry, a fisheye image can be presented as a\nparameterized non-rectilinear image. Deep neural networks(DNN) is one of the\nsolutions to extract parameters for fisheye image feature description. However,\na large number of images are required for training a reasonable prediction\nmodel for DNN. In this paper, we propose to extend the scale of the training\ndataset using parameterized synthetic images. It effectively boosts the\ndiversity of images and avoids the data scale limitation. To simulate different\nviewing angles and distances, we adopt controllable parameterized projection\nprocesses on transformation. The reliability of the proposed method is proved\nby testing images captured by our fisheye camera. The synthetic dataset is the\nfirst dataset that is able to extend to a big scale labeled fisheye image\ndataset. It is accessible via: http://www2.leuphana.de/misl/fisheye-data-set/.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:48:43 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Chen", "Zhen", ""], ["Georgiadis", "Anthimos", ""]]}, {"id": "1811.04645", "submitter": "Bin Shao", "authors": "Lianping Yang, Bin Shao, Ting Sun, Song Ding, Xiangde Zhang", "title": "Hallucinating very low-resolution and obscured face images", "comments": "20 pages, Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the face hallucination methods are designed for complete inputs. They\nwill not work well if the inputs are very tiny or contaminated by large\nocclusion. Inspired by this fact, we propose an obscured face hallucination\nnetwork(OFHNet). The OFHNet consists of four parts: an inpainting network, an\nupsampling network, a discriminative network, and a fixed facial landmark\ndetection network. The inpainting network restores the low-resolution(LR)\nobscured face images. The following upsampling network is to upsample the\noutput of inpainting network. In order to ensure the generated\nhigh-resolution(HR) face images more photo-realistic, we utilize the\ndiscriminative network and the facial landmark detection network to better the\nresult of upsampling network. In addition, we present a semantic structure\nloss, which makes the generated HR face images more pleasing. Extensive\nexperiments show that our framework can restore the appealing HR face images\nfrom 1/4 missing area LR face images with a challenging scaling factor of 8x.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:40:09 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 13:27:10 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 10:37:10 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 02:24:34 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Yang", "Lianping", ""], ["Shao", "Bin", ""], ["Sun", "Ting", ""], ["Ding", "Song", ""], ["Zhang", "Xiangde", ""]]}, {"id": "1811.04661", "submitter": "Namita Jain Mrs", "authors": "Namita Jain, Susmita Ghosh, C. A. Murthy", "title": "RelDenClu: A Relative Density based Biclustering Method for identifying\n  non-linear feature relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing biclustering algorithms for finding feature relation based\nbiclusters often depend on assumptions like monotonicity or linearity. Though a\nfew algorithms overcome this problem by using density-based methods, they tend\nto miss out many biclusters because they use global criteria for identifying\ndense regions. The proposed method, RelDenClu uses the local variations in\nmarginal and joint densities for each pair of features to find the subset of\nobservations, which forms the bases of the relation between them. It then finds\nthe set of features connected by a common set of observations, resulting in a\nbicluster.\n  To show the effectiveness of the proposed methodology, experimentation has\nbeen carried out on fifteen types of simulated datasets. Further, it has been\napplied to six real-life datasets. For three of these real-life datasets, the\nproposed method is used for unsupervised learning, while for other three\nreal-life datasets it is used as an aid to supervised learning. For all the\ndatasets the performance of the proposed method is compared with that of seven\ndifferent state-of-the-art algorithms and the proposed algorithm is seen to\nproduce better results. The efficacy of proposed algorithm is also seen by its\nuse on COVID-19 dataset for identifying some features (genetic, demographics\nand others) that are likely to affect the spread of COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 11:11:26 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 10:26:25 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 17:39:50 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 09:54:59 GMT"}, {"version": "v5", "created": "Tue, 11 May 2021 11:32:37 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Jain", "Namita", ""], ["Ghosh", "Susmita", ""], ["Murthy", "C. A.", ""]]}, {"id": "1811.04678", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif, Karim Armanious, Fady Aziz, Urs Schneider, Bin Yang", "title": "Towards Adversarial Denoising of Radar Micro-Doppler Signatures", "comments": "Accepted in IEEE International Radar Conference 2019", "journal-ref": null, "doi": "10.1109/RADAR41533.2019.171396", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are considered the state-of-the-art in\nthe field of image generation. They learn the joint distribution of the\ntraining data and attempt to generate new data samples in high dimensional\nspace following the same distribution as the input. Recent improvements in GANs\nopened the field to many other computer vision applications based on improving\nand changing the characteristics of the input image to follow some given\ntraining requirements. In this paper, we propose a novel technique for the\ndenoising and reconstruction of the micro-Doppler ($\\boldsymbol{\\mu}$-D)\nspectra of walking humans based on GANs. Two sets of experiments were collected\non 22 subjects walking on a treadmill at an intermediate velocity using a\n\\unit[25]{GHz} CW radar. In one set, a clean $\\boldsymbol{\\mu}$-D spectrum is\ncollected for each subject by placing the radar at a close distance to the\nsubject. In the other set, variations are introduced in the experiment setup to\nintroduce different noise and clutter effects on the spectrum by changing the\ndistance and placing reflective objects between the radar and the target.\nSynthetic paired noisy and noise-free spectra were used for training, while\nvalidation was carried out on the real noisy measured data. Finally,\nqualitative and quantitative comparison with other classical radar denoising\napproaches in the literature demonstrated the proposed GANs framework is better\nand more robust to different noise levels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 11:52:02 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 13:55:59 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 14:23:22 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Armanious", "Karim", ""], ["Aziz", "Fady", ""], ["Schneider", "Urs", ""], ["Yang", "Bin", ""]]}, {"id": "1811.04682", "submitter": "Songmin Dai", "authors": "Songmin Dai, Xiaoqiang Li, Lu Wang, Pin Wu, Weiqin Tong, Yimin Chen", "title": "Learning Segmentation Masks with the Independence Prior", "comments": "7+5 pages, 13 figures, Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance with a bad mask might make a composite image that uses it look\nfake. This encourages us to learn segmentation by generating realistic\ncomposite images. To achieve this, we propose a novel framework that exploits a\nnew proposed prior called the independence prior based on Generative\nAdversarial Networks (GANs). The generator produces an image with multiple\ncategory-specific instance providers, a layout module and a composition module.\nFirstly, each provider independently outputs a category-specific instance image\nwith a soft mask. Then the provided instances' poses are corrected by the\nlayout module. Lastly, the composition module combines these instances into a\nfinal image. Training with adversarial loss and penalty for mask area, each\nprovider learns a mask that is as small as possible but enough to cover a\ncomplete category-specific instance. Weakly supervised semantic segmentation\nmethods widely use grouping cues modeling the association between image parts,\nwhich are either artificially designed or learned with costly segmentation\nlabels or only modeled on local pairs. Unlike them, our method automatically\nmodels the dependence between any parts and learns instance segmentation. We\napply our framework in two cases: (1) Foreground segmentation on\ncategory-specific images with box-level annotation. (2) Unsupervised learning\nof instance appearances and masks with only one image of homogeneous object\ncluster (HOC). We get appealing results in both tasks, which shows the\nindependence prior is useful for instance segmentation and it is possible to\nunsupervisedly learn instance masks with only one image.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 12:06:30 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 11:40:15 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Dai", "Songmin", ""], ["Li", "Xiaoqiang", ""], ["Wang", "Lu", ""], ["Wu", "Pin", ""], ["Tong", "Weiqin", ""], ["Chen", "Yimin", ""]]}, {"id": "1811.04756", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla and Sebastian Maisch and Tobias Ritschel and Timo\n  Ropinski", "title": "Deep-learning the Latent Space of Light Transport", "comments": "Eurographics Symposium on Rendering 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a method to directly deep-learn light transport, i. e., the\nmapping from a 3D geometry-illumination-material configuration to a shaded 2D\nimage. While many previous learning methods have employed 2D convolutional\nneural networks applied to images, we show for the first time that light\ntransport can be learned directly in 3D. The benefit of 3D over 2D is, that the\nformer can also correctly capture illumination effects related to occluded\nand/or semi-transparent geometry. To learn 3D light transport, we represent the\n3D scene as an unstructured 3D point cloud, which is later, during rendering,\nprojected to the 2D output image. Thus, we suggest a two-stage operator\ncomprising of a 3D network that first transforms the point cloud into a latent\nrepresentation, which is later on projected to the 2D output image using a\ndedicated 3D-2D network in a second step. We will show that our approach\nresults in improved quality in terms of temporal coherence while retaining most\nof the computational efficiency of common 2D methods. As a consequence, the\nproposed two stage-operator serves as a valuable extension to modern deferred\nshading approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:55:58 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 10:22:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Maisch", "Sebastian", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "1811.04768", "submitter": "Mingyang Geng", "authors": "Mingyang Geng, Kele Xu, Bo Ding, Huaimin Wang, Lei Zhang", "title": "Learning data augmentation policies using augmented random search", "comments": "Submitted to ICASSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous attempts for data augmentation are designed manually, and the\naugmentation policies are dataset-specific. Recently, an automatic data\naugmentation approach, named AutoAugment, is proposed using reinforcement\nlearning. AutoAugment searches for the augmentation polices in the discrete\nsearch space, which may lead to a sub-optimal solution. In this paper, we\nemploy the Augmented Random Search method (ARS) to improve the performance of\nAutoAugment. Our key contribution is to change the discrete search space to\ncontinuous space, which will improve the searching performance and maintain the\ndiversities between sub-policies. With the proposed method, state-of-the-art\naccuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without\nadditional data). Our code is available at https://github.com/gmy2013/ARS-Aug.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:14:18 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Geng", "Mingyang", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Zhang", "Lei", ""]]}, {"id": "1811.04772", "submitter": "Ankit Manerikar", "authors": "Ankit Manerikar, Tanmay Prakash, Avinash C. Kak", "title": "Adaptive Target Recognition: A Case Study Involving Airport Baggage\n  Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the question whether it is possible to design a\ncomputer-vision based automatic threat recognition (ATR) system so that it can\nadapt to changing specifications of a threat without having to create a new ATR\neach time. The changes in threat specifications, which may be warranted by\nintelligence reports and world events, are typically regarding the physical\ncharacteristics of what constitutes a threat: its material composition, its\nshape, its method of concealment, etc. Here we present our design of an AATR\nsystem (Adaptive ATR) that can adapt to changing specifications in materials\ncharacterization (meaning density, as measured by its x-ray attenuation\ncoefficient), its mass, and its thickness. Our design uses a two-stage cascaded\napproach, in which the first stage is characterized by a high recall rate over\nthe entire range of possibilities for the threat parameters that are allowed to\nchange. The purpose of the second stage is to then fine-tune the performance of\nthe overall system for the current threat specifications. The computational\neffort for this fine-tuning for achieving a desired PD/PFA rate is far less\nthan what it would take to create a new classifier with the same overall\nperformance for the new set of threat specifications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:15:39 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 19:20:45 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Manerikar", "Ankit", ""], ["Prakash", "Tanmay", ""], ["Kak", "Avinash C.", ""]]}, {"id": "1811.04778", "submitter": "Heng Fan", "authors": "Heng Fan, Peng Chu, Longin Jan Latecki, Haibin Ling", "title": "Scene Parsing via Dense Recurrent Neural Networks with Attentional\n  Selection", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1801.06831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown the ability to improve scene\nparsing through capturing long-range dependencies among image units. In this\npaper, we propose dense RNNs for scene labeling by exploring various long-range\nsemantic dependencies among image units. Different from existing RNN based\napproaches, our dense RNNs are able to capture richer contextual dependencies\nfor each image unit by enabling immediate connections between each pair of\nimage units, which significantly enhances their discriminative power. Besides,\nto select relevant dependencies and meanwhile to restrain irrelevant ones for\neach unit from dense connections, we introduce an attention model into dense\nRNNs. The attention model allows automatically assigning more importance to\nhelpful dependencies while less weight to unconcerned dependencies. Integrating\nwith convolutional neural networks (CNNs), we develop an end-to-end scene\nlabeling system. Extensive experiments on three large-scale benchmarks\ndemonstrate that the proposed approach can improve the baselines by large\nmargins and outperform other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:46:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Fan", "Heng", ""], ["Chu", "Peng", ""], ["Latecki", "Longin Jan", ""], ["Ling", "Haibin", ""]]}, {"id": "1811.04815", "submitter": "Shi Yin", "authors": "Shi Yin, Qinmu Peng, Hongming Li, Zhengqiang Zhang, Xinge You, Susan\n  L. Furth, Gregory E. Tasian, Yong Fan", "title": "Automatic kidney segmentation in ultrasound images using subsequent\n  boundary distance regression and pixelwise classification networks", "comments": "The paper has been submitted to the Medical Image Analysis for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains challenging to automatically segment kidneys in clinical\nultrasound (US) images due to the kidneys' varied shapes and image intensity\ndistributions, although semi-automatic methods have achieved promising\nperformance. In this study, we propose subsequent boundary distance regression\nand pixel classification networks to segment the kidneys, informed by the fact\nthat the kidney boundaries have relatively homogenous texture patterns across\nimages. Particularly, we first use deep neural networks pre-trained for\nclassification of natural images to extract high-level image features from US\nimages, then these features are used as input to learn kidney boundary distance\nmaps using a boundary distance regression network, and finally the predicted\nboundary distance maps are classified as kidney pixels or non-kidney pixels\nusing a pixel classification network in an end-to-end learning fashion. We also\nadopted a data-augmentation method based on kidney shape registration to\ngenerate enriched training data from a small number of US images with manually\nsegmented kidney labels. Experimental results have demonstrated that our method\ncould effectively improve the performance of automatic kidney segmentation,\nsignificantly better than deep learning-based pixel classification networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:54:59 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 20:11:27 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2019 16:11:52 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yin", "Shi", ""], ["Peng", "Qinmu", ""], ["Li", "Hongming", ""], ["Zhang", "Zhengqiang", ""], ["You", "Xinge", ""], ["Furth", "Susan L.", ""], ["Tasian", "Gregory E.", ""], ["Fan", "Yong", ""]]}, {"id": "1811.04817", "submitter": "Pedram Hassanzadeh", "authors": "Ashesh Chattopadhyay and Pedram Hassanzadeh and Saba Pasha", "title": "A test case for application of convolutional neural networks to\n  spatio-temporal climate data: Re-identifying clustered weather patterns", "comments": null, "journal-ref": "Scientific Reports, 2020", "doi": "10.1038/s41598-020-57897-9", "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) can potentially provide powerful tools\nfor classifying and identifying patterns in climate and environmental data.\nHowever, because of the inherent complexities of such data, which are often\nspatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\ndesigned/evaluated for each specific dataset and application. Yet to start,\nCNN, a supervised technique, requires a large labeled dataset. Labeling demands\n(human) expert time, which combined with the limited number of relevant\nexamples in this area, can discourage using CNNs for new problems. To address\nthese challenges, here we (1) Propose an effective auto-labeling strategy based\non using an unsupervised clustering algorithm and evaluating the performance of\nCNNs in re-identifying these clusters; (2) Use this approach to label thousands\nof daily large-scale weather patterns over North America in the outputs of a\nfully-coupled climate model and show the capabilities of CNNs in re-identifying\nthe 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\ncluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\nnonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\ntraining samples per cluster. Effects of architecture and hyperparameters on\nthe performance of CNNs are examined and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:56:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chattopadhyay", "Ashesh", ""], ["Hassanzadeh", "Pedram", ""], ["Pasha", "Saba", ""]]}, {"id": "1811.04857", "submitter": "He Huang", "authors": "He Huang, Changhu Wang, Philip S. Yu, Chang-Dong Wang", "title": "Generative Dual Adversarial Network for Generalized Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of generalized zero-shot learning which\nrequires the model to train on image-label pairs from some seen classes and\ntest on the task of classifying new images from both seen and unseen classes.\nMost previous models try to learn a fixed one-directional mapping between\nvisual and semantic space, while some recently proposed generative methods try\nto generate image features for unseen classes so that the zero-shot learning\nproblem becomes a traditional fully-supervised classification problem. In this\npaper, we propose a novel model that provides a unified framework for three\ndifferent approaches: visual-> semantic mapping, semantic->visual mapping, and\nmetric learning. Specifically, our proposed model consists of a feature\ngenerator that can generate various visual features given class embeddings as\ninput, a regressor that maps each visual feature back to its corresponding\nclass embedding, and a discriminator that learns to evaluate the closeness of\nan image feature and a class embedding. All three components are trained under\nthe combination of cyclic consistency loss and dual adversarial loss.\nExperimental results show that our model not only preserves higher accuracy in\nclassifying images from seen classes, but also performs better than existing\nstate-of-the-art models in in classifying images from unseen classes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:04:51 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 15:17:40 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 21:05:32 GMT"}, {"version": "v4", "created": "Sat, 25 May 2019 08:40:29 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Huang", "He", ""], ["Wang", "Changhu", ""], ["Yu", "Philip S.", ""], ["Wang", "Chang-Dong", ""]]}, {"id": "1811.04863", "submitter": "Panagiotis Mousouliotis", "authors": "Ioannis Athanasiadis, Panagiotis Mousouliotis, Loukas Petrou", "title": "A Framework of Transfer Learning in Object Detection for Embedded\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is one of the subjects undergoing intense study in the area\nof machine learning. In object recognition and object detection there are known\nexperiments for the transferability of parameters, but not for neural networks\nwhich are suitable for object detection in real time embedded applications,\nsuch as the SqueezeDet neural network. We use transfer learning to accelerate\nthe training of SqueezeDet to a new group of classes. Also, experiments are\nconducted to study the transferability and co-adaptation phenomena introduced\nby the transfer learning process. To accelerate training, we propose a new\nimplementation of the SqueezeDet training which provides a faster pipeline for\ndata processing and achieves 1.8 times speedup compared to the initial\nimplementation. Finally, we created a mechanism for automatic hyperparameter\noptimization using an empirical method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:12:16 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 22:08:42 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Athanasiadis", "Ioannis", ""], ["Mousouliotis", "Panagiotis", ""], ["Petrou", "Loukas", ""]]}, {"id": "1811.04869", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Sudeep Sarkar", "title": "A Perceptual Prediction Framework for Self Supervised Event Segmentation", "comments": "CVPR 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal segmentation of long videos is an important problem, that has\nlargely been tackled through supervised learning, often requiring large amounts\nof annotated training data. In this paper, we tackle the problem of\nself-supervised temporal segmentation of long videos that alleviate the need\nfor any supervision. We introduce a self-supervised, predictive learning\nframework that draws inspiration from cognitive psychology to segment long,\nvisually complex videos into individual, stable segments that share the same\nsemantics. We also introduce a new adaptive learning paradigm that helps reduce\nthe effect of catastrophic forgetting in recurrent neural networks. Extensive\nexperiments on three publicly available datasets - Breakfast Actions, 50\nSalads, and INRIA Instructional Videos datasets show the efficacy of the\nproposed approach. We show that the proposed approach is able to outperform\nweakly-supervised and other unsupervised learning approaches by up to 24% and\nhave competitive performance compared to fully supervised approaches. We also\nshow that the proposed approach is able to learn highly discriminative features\nthat help improve action recognition when used in a representation learning\nparadigm.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:26:28 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 16:00:21 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 22:40:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1811.04893", "submitter": "Ritwik Gupta", "authors": "Ritwik Gupta, Carson D. Sestili, Javier A. Vazquez-Trejo, Matthew E.\n  Gaston", "title": "Focusing on the Big Picture: Insights into a Systems Approach to Deep\n  Learning for Satellite Imagery", "comments": "Accepted to IEEE Big Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning tasks are often complicated and require a variety of components\nworking together efficiently to perform well. Due to the often large scale of\nthese tasks, there is a necessity to iterate quickly in order to attempt a\nvariety of methods and to find and fix bugs. While participating in IARPA's\nFunctional Map of the World challenge, we identified challenges along the\nentire deep learning pipeline and found various solutions to these challenges.\nIn this paper, we present the performance, engineering, and deep learning\nconsiderations with processing and modeling data, as well as underlying\ninfrastructure considerations that support large-scale deep learning tasks. We\nalso discuss insights and observations with regard to satellite imagery and\ndeep learning for image classification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:25:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gupta", "Ritwik", ""], ["Sestili", "Carson D.", ""], ["Vazquez-Trejo", "Javier A.", ""], ["Gaston", "Matthew E.", ""]]}, {"id": "1811.04907", "submitter": "Yannick Suter", "authors": "Yannick Suter, Alain Jungo, Michael Rebsamen, Urspeter Knecht, Evelyn\n  Herrmann, Roland Wiest, Mauricio Reyes", "title": "Deep Learning versus Classical Regression for Brain Tumor Patient\n  Survival Prediction", "comments": "Contribution to The International Multimodal Brain Tumor Segmentation\n  (BraTS) Challenge 2018, survival prediction task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for regression tasks on medical imaging data has shown\npromising results. However, compared to other approaches, their power is\nstrongly linked to the dataset size. In this study, we evaluate\n3D-convolutional neural networks (CNNs) and classical regression methods with\nhand-crafted features for survival time regression of patients with high grade\nbrain tumors. The tested CNNs for regression showed promising but unstable\nresults. The best performing deep learning approach reached an accuracy of\n51.5% on held-out samples of the training set. All tested deep learning\nexperiments were outperformed by a Support Vector Classifier (SVC) using 30\nradiomic features. The investigated features included intensity, shape,\nlocation and deep features. The submitted method to the BraTS 2018 survival\nprediction challenge is an ensemble of SVCs, which reached a cross-validated\naccuracy of 72.2% on the BraTS 2018 training set, 57.1% on the validation set,\nand 42.9% on the testing set. The results suggest that more training data is\nnecessary for a stable performance of a CNN model for direct regression from\nmagnetic resonance images, and that non-imaging clinical patient information is\ncrucial along with imaging information.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:45:08 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Suter", "Yannick", ""], ["Jungo", "Alain", ""], ["Rebsamen", "Michael", ""], ["Knecht", "Urspeter", ""], ["Herrmann", "Evelyn", ""], ["Wiest", "Roland", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1811.04989", "submitter": "Chenxu Luo", "authors": "Chenxu Luo, Xiao Chu, Alan Yuille", "title": "OriNet: A Fully Convolutional Network for 3D Human Pose Estimation", "comments": "BMVC 2018. Code available at https://github.com/chenxuluo/OriNet-demo", "journal-ref": "BMVC 2018 - Proceedings of the British Machine Vision Conference\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully convolutional network for 3D human pose\nestimation from monocular images. We use limb orientations as a new way to\nrepresent 3D poses and bind the orientation together with the bounding box of\neach limb region to better associate images and predictions. The 3D\norientations are modeled jointly with 2D keypoint detections. Without\nadditional constraints, this simple method can achieve good results on several\nlarge-scale benchmarks. Further experiments show that our method can generalize\nwell to novel scenes and is robust to inaccurate bounding boxes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:18:30 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Luo", "Chenxu", ""], ["Chu", "Xiao", ""], ["Yuille", "Alan", ""]]}, {"id": "1811.05006", "submitter": "Eric K. Tokuda", "authors": "Eric K. Tokuda, Yitzchak Lockerman, Gabriel B. A. Ferreira, Ethan\n  Sorrelgreen, David Boyle, Roberto M. Cesar-Jr., Claudio T. Silva", "title": "A new approach for pedestrian density estimation using moving sensors\n  and computer vision", "comments": "Submitted to ACM-TSAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An understanding of pedestrian dynamics is indispensable for numerous urban\napplications including the design of transportation networks and planing for\nbusiness development. Pedestrian counting often requires utilizing manual or\ntechnical means to count individuals in each location of interest. However,\nsuch methods do not scale to the size of a city and a new approach to fill this\ngap is here proposed. In this project, we used a large dense dataset of images\nof New York City along with computer vision techniques to construct a\nspatio-temporal map of relative person density. Due to the limitations of state\nof the art computer vision methods, such automatic detection of person is\ninherently subject to errors. We model these errors as a probabilistic process,\nfor which we provide theoretical analysis and thorough numerical simulations.\nWe demonstrate that, within our assumptions, our methodology can supply a\nreasonable estimate of person densities and provide theoretical bounds for the\nresulting error.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:19:38 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 16:12:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Tokuda", "Eric K.", ""], ["Lockerman", "Yitzchak", ""], ["Ferreira", "Gabriel B. A.", ""], ["Sorrelgreen", "Ethan", ""], ["Boyle", "David", ""], ["Cesar-Jr.", "Roberto M.", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1811.05013", "submitter": "Ankesh Anand", "authors": "Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron\n  Courville", "title": "Blindfold Baselines for Embodied QA", "comments": "NIPS 2018 Visually-Grounded Interaction and Language (ViGilL)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:45:41 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Anand", "Ankesh", ""], ["Belilovsky", "Eugene", ""], ["Kastner", "Kyle", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.05014", "submitter": "Rongcheng Lin", "authors": "Rongcheng Lin, Jing Xiao, Jianping Fan", "title": "NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features\n  for Large-scale Video Classification", "comments": "ECCV 2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast and efficient network architecture, NeXtVLAD, to\naggregate frame-level features into a compact feature vector for large-scale\nvideo classification. Briefly speaking, the basic idea is to decompose a\nhigh-dimensional feature into a group of relatively low-dimensional vectors\nwith attention before applying NetVLAD aggregation over time. This NeXtVLAD\napproach turns out to be both effective and parameter efficient in aggregating\ntemporal information. In the 2nd Youtube-8M video understanding challenge, a\nsingle NeXtVLAD model with less than 80M parameters achieves a GAP score of\n0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in\n0.88722, which is ranked 3rd over 394 teams. The code is publicly available at\nhttps://github.com/linrongc/youtube-8m.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:53:28 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Lin", "Rongcheng", ""], ["Xiao", "Jing", ""], ["Fan", "Jianping", ""]]}, {"id": "1811.05027", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Shiyang Cheng and Evangelos Ververas and Irene\n  Kotsia and Stefanos Zafeiriou", "title": "Deep Neural Network Augmentation: Generating Faces for Affect Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for synthesizing facial affect; either\nin terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness\nand surprise), or in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the emotion activation). The proposed\napproach accepts the following inputs: i) a neutral 2D image of a person; ii) a\nbasic facial expression or a pair of valence-arousal (VA) emotional state\ndescriptors to be generated, or a path of affect in the 2D VA Space to be\ngenerated as an image sequence. In order to synthesize affect in terms of VA,\nfor this person, $600,000$ frames from the 4DFAB database were annotated. The\naffect synthesis is implemented by fitting a 3D Morphable Model on the neutral\nimage, then deforming the reconstructed face and adding the inputted affect,\nand blending the new face with the given affect into the original image.\nQualitative experiments illustrate the generation of realistic images, when the\nneutral image is sampled from thirteen well known lab-controlled or in-the-wild\ndatabases, including Aff-Wild, AffectNet, RAF-DB; comparisons with Generative\nAdversarial Networks (GANs) show the higher quality achieved by the proposed\napproach. Then, quantitative experiments are conducted, in which the\nsynthesized images are used for data augmentation in training Deep Neural\nNetworks to perform affect recognition over all databases; greatly improved\nperformances are achieved when compared with state-of-the-art methods, as well\nas with GAN-based data augmentation, in all cases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:42:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 21:33:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Cheng", "Shiyang", ""], ["Ververas", "Evangelos", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.05029", "submitter": "Ricardo Martin Brualla", "authors": "Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel\n  Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip\n  Davidson, Anastasia Tkach, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann,\n  Dan B Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello", "title": "LookinGood: Enhancing Performance Capture with Real-time Neural\n  Re-Rendering", "comments": "The supplementary video is available at: http://youtu.be/Md3tdAKoLGU\n  To be presented at SIGGRAPH Asia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by augmented and virtual reality applications such as telepresence,\nthere has been a recent focus in real-time performance capture of humans under\nmotion. However, given the real-time constraint, these systems often suffer\nfrom artifacts in geometry and texture such as holes and noise in the final\nrendering, poor lighting, and low-resolution textures. We take the novel\napproach to augment such real-time performance capture systems with a deep\narchitecture that takes a rendering from an arbitrary viewpoint, and jointly\nperforms completion, super resolution, and denoising of the imagery in\nreal-time. We call this approach neural (re-)rendering, and our live system\n\"LookinGood\". Our deep architecture is trained to produce high resolution and\nhigh quality images from a coarse rendering in real-time. First, we propose a\nself-supervised training method that does not require manual ground-truth\nannotation. We contribute a specialized reconstruction error that uses semantic\ninformation to focus on relevant parts of the subject, e.g. the face. We also\nintroduce a salient reweighing scheme of the loss function that is able to\ndiscard outliers. We specifically design the system for virtual and augmented\nreality headsets where the consistency between the left and right eye plays a\ncrucial role in the final user experience. Finally, we generate temporally\nstable results by explicitly minimizing the difference between two consecutive\nframes. We tested the proposed system in two different scenarios: one involving\na single RGB-D sensor, and upper body reconstruction of an actor, the second\nconsisting of full body 360 degree capture. Through extensive experimentation,\nwe demonstrate how our system generalizes across unseen sequences and subjects.\nThe supplementary video is available at http://youtu.be/Md3tdAKoLGU.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:51:19 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Pandey", "Rohit", ""], ["Yang", "Shuoran", ""], ["Pidlypenskyi", "Pavel", ""], ["Taylor", "Jonathan", ""], ["Valentin", "Julien", ""], ["Khamis", "Sameh", ""], ["Davidson", "Philip", ""], ["Tkach", "Anastasia", ""], ["Lincoln", "Peter", ""], ["Kowdle", "Adarsh", ""], ["Rhemann", "Christoph", ""], ["Goldman", "Dan B", ""], ["Keskin", "Cem", ""], ["Seitz", "Steve", ""], ["Izadi", "Shahram", ""], ["Fanello", "Sean", ""]]}, {"id": "1811.05118", "submitter": "Zezheng Wang", "authors": "Zezheng Wang, Chenxu Zhao, Yunxiao Qin, Qiusheng Zhou, Guojun Qi, Jun\n  Wan, Zhen Lei", "title": "Exploiting temporal and depth information for multi-frame face\n  anti-spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is significant to the security of face recognition\nsystems. Previous works on depth supervised learning have proved the\neffectiveness for face anti-spoofing. Nevertheless, they only considered the\ndepth as an auxiliary supervision in the single frame. Different from these\nmethods, we develop a new method to estimate depth information from multiple\nRGB frames and propose a depth-supervised architecture which can efficiently\nencodes spatiotemporal information for presentation attack detection. It\nincludes two novel modules: optical flow guided feature block (OFFB) and\nconvolution gated recurrent units (ConvGRU) module, which are designed to\nextract short-term and long-term motion to discriminate living and spoofing\nfaces. Extensive experiments demonstrate that the proposed approach achieves\nstate-of-the-art results on four benchmark datasets, namely OULU-NPU, SiW,\nCASIA-MFSD, and Replay-Attack.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 05:59:34 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 13:32:21 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 11:49:19 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Wang", "Zezheng", ""], ["Zhao", "Chenxu", ""], ["Qin", "Yunxiao", ""], ["Zhou", "Qiusheng", ""], ["Qi", "Guojun", ""], ["Wan", "Jun", ""], ["Lei", "Zhen", ""]]}, {"id": "1811.05147", "submitter": "Kairan Yang", "authors": "Kairan Yang, Feng Geng", "title": "Application of Faster R-CNN model on Human Running Pattern Recognition", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advance algorithms like Faster Regional Convolutional Neural Network\n(Faster R-CNN) models are suitable to identify classified moving objects, due\nto the efficiency in learning the training dataset superior than ordinary CNN\nalgorithms and the higher accuracy of labeling correct classes in the\nvalidation and testing dataset. This research examined and compared the three\nR-CNN type algorithms in object recognition to show the superior efficiency and\naccuracy of Faster R-CNN model on classifying human running patterns. Then it\ndescribed the effect of Faster R-CNN in detecting different types of running\npatterns exhibited by a single individual or multiple individuals by conducting\na dataset fitting experiment. In this study, the Faster R-CNN algorithm is\nimplemented directly from the version released by Ross Girshick.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 07:51:51 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Yang", "Kairan", ""], ["Geng", "Feng", ""]]}, {"id": "1811.05163", "submitter": "Jingqing Zhu", "authors": "Jianqing Zhu, Huanqiang Zeng, Jingchang Huang, Shengcai Liao, Zhen\n  Lei, Canhui Cai and LiXin Zheng", "title": "Vehicle Re-identification Using Quadruple Directional Deep Learning\n  Features", "comments": "this paper has been submitted to IEEE Transactions on Intelligent\n  Transportation Systems, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to resist the adverse effect of viewpoint variations for improving\nvehicle re-identification performance, we design quadruple directional deep\nlearning networks to extract quadruple directional deep learning features\n(QD-DLF) of vehicle images. The quadruple directional deep learning networks\nare with similar overall architecture, including the same basic deep learning\narchitecture but different directional feature pooling layers. Specifically,\nthe same basic deep learning architecture is a shortly and densely connected\nconvolutional neural network to extract basic feature maps of an input square\nvehicle image in the first stage. Then, the quadruple directional deep learning\nnetworks utilize different directional pooling layers, i.e., horizontal average\npooling (HAP) layer, vertical average pooling (VAP) layer, diagonal average\npooling (DAP) layer and anti-diagonal average pooling (AAP) layer, to compress\nthe basic feature maps into horizontal, vertical, diagonal and anti-diagonal\ndirectional feature maps, respectively.\n  Finally, these directional feature maps are spatially normalized and\nconcatenated together as a quadruple directional deep learning feature for\nvehicle re-identification. Extensive experiments on both VeRi and VehicleID\ndatabases show that the proposed QD-DLF approach outperforms multiple\nstate-of-the-art vehicle re-identification methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:44:19 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Zhu", "Jianqing", ""], ["Zeng", "Huanqiang", ""], ["Huang", "Jingchang", ""], ["Liao", "Shengcai", ""], ["Lei", "Zhen", ""], ["Cai", "Canhui", ""], ["Zheng", "LiXin", ""]]}, {"id": "1811.05180", "submitter": "Mumtaz Ali", "authors": "Mumtaz A. Kaloi, Kun He", "title": "Child Gender Determination with Convolutional Neural Networks on Hand\n  Radio-Graphs", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: In forensic or medico-legal investigation as well as in\nanthropology the gender determination of the subject (hit by a disastrous or\nany kind of traumatic situation) is mostly the first step. In state-of-the-art\ntechniques the gender is determined by examining dimensions of the bones of\nskull and the pelvis area. In worse situations when there is only a small\nportion of the human remains to be investigated and the subject is a child, we\nneed alternative techniques to determine the gender of the subject. In this\nwork we propose a technique called GDCNN (Gender Determination with\nConvolutional Neural Networks), where the left hand radio-graphs of the\nchildren between a wide range of ages in 1 month to 18 years are examined to\ndetermine the gender. To our knowledge this technique is first of its kind.\nFurther to identify the area of the attention we used Class Activation Mapping\n(CAM). Results: The results suggest the accuracy of the model is as high as\n98%, which is very convincing by taking into account the incompletely grown\nskeleton of the children. The attention observed with CAM discovers that the\nlower part of the hand around carpals (wrist) is more important for child\ngender determination.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:43:55 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kaloi", "Mumtaz A.", ""], ["He", "Kun", ""]]}, {"id": "1811.05181", "submitter": "Yu Liu", "authors": "Buyu Li, Yu Liu, Xiaogang Wang", "title": "Gradient Harmonized Single-stage Detector", "comments": "To appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of two-stage detectors, single-stage detector is\nstill a more elegant and efficient way, yet suffers from the two well-known\ndisharmonies during training, i.e. the huge difference in quantity between\npositive and negative examples as well as between easy and hard examples. In\nthis work, we first point out that the essential effect of the two disharmonies\ncan be summarized in term of the gradient. Further, we propose a novel gradient\nharmonizing mechanism (GHM) to be a hedging for the disharmonies. The\nphilosophy behind GHM can be easily embedded into both classification loss\nfunction like cross-entropy (CE) and regression loss function like smooth-$L_1$\n($SL_1$) loss. To this end, two novel loss functions called GHM-C and GHM-R are\ndesigned to balancing the gradient flow for anchor classification and bounding\nbox refinement, respectively. Ablation study on MS COCO demonstrates that\nwithout laborious hyper-parameter tuning, both GHM-C and GHM-R can bring\nsubstantial improvement for single-stage detector. Without any whistles and\nbells, our model achieves 41.6 mAP on COCO test-dev set which surpasses the\nstate-of-the-art method, Focal Loss (FL) + $SL_1$, by 0.8.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:47:52 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Li", "Buyu", ""], ["Liu", "Yu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1811.05233", "submitter": "Yuichi Kageyama", "authors": "Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki\n  Tanaka and Yuichi Kageyama", "title": "Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling the distributed deep learning to a massive GPU cluster level is\nchallenging due to the instability of the large mini-batch training and the\noverhead of the gradient synchronization. We address the instability of the\nlarge mini-batch training with batch-size control and label smoothing. We\naddress the overhead of the gradient synchronization with 2D-Torus all-reduce.\nSpecifically, 2D-Torus all-reduce arranges GPUs in a logical 2D grid and\nperforms a series of collective operation in different orientations. These two\ntechniques are implemented with Neural Network Libraries (NNL). We have\nsuccessfully trained ImageNet/ResNet-50 in 122 seconds without significant\naccuracy loss on ABCI cluster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:52:04 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 09:18:09 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Mikami", "Hiroaki", ""], ["Suganuma", "Hisahiro", ""], ["U-chupala", "Pongsakorn", ""], ["Tanaka", "Yoshiki", ""], ["Kageyama", "Yuichi", ""]]}, {"id": "1811.05243", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim, Taewook Kim, Bong-Nam Kang, Jieun Kim and Daijin Kim", "title": "BAN: Focusing on Boundary Context for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual context is one of the important clue for object detection and the\ncontext information for boundaries of an object is especially valuable. We\npropose a boundary aware network (BAN) designed to exploit the visual contexts\nincluding boundary information and surroundings, named boundary context, and\ndefine three types of the boundary contexts: side, vertex and in/out-boundary\ncontext. Our BAN consists of 10 sub-networks for the area belonging to the\nboundary contexts. The detection head of BAN is defined as an ensemble of these\nsub-networks with different contributions depending on the sub-problem of\ndetection. To verify our method, we visualize the activation of the\nsub-networks according to the boundary contexts and empirically show that the\nsub-networks contribute more to the related sub-problem in detection. We\nevaluate our method on PASCAL VOC detection benchmark and MS COCO dataset. The\nproposed method achieves the mean Average Precision (mAP) of 83.4% on PASCAL\nVOC and 36.9% on MS COCO. BAN allows the convolution network to provide an\nadditional source of contexts for detection and selectively focus on the more\nimportant contexts, and it can be generally applied to many other detection\nmethods as well to enhance the accuracy in detection.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:10:38 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kim", "Yonghyun", ""], ["Kim", "Taewook", ""], ["Kang", "Bong-Nam", ""], ["Kim", "Jieun", ""], ["Kim", "Daijin", ""]]}, {"id": "1811.05250", "submitter": "Pan Zhou", "authors": "Pan Zhou, Wenwen Yang, Wei Chen, Yanfeng Wang, Jia Jia", "title": "Modality Attention for End-to-End Audio-visual Speech Recognition", "comments": "accepted by ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual speech recognition (AVSR) system is thought to be one of the\nmost promising solutions for robust speech recognition, especially in noisy\nenvironment. In this paper, we propose a novel multimodal attention based\nmethod for audio-visual speech recognition which could automatically learn the\nfused representation from both modalities based on their importance. Our method\nis realized using state-of-the-art sequence-to-sequence (Seq2seq)\narchitectures. Experimental results show that relative improvements from 2% up\nto 36% over the auditory modality alone are obtained depending on the different\nsignal-to-noise-ratio (SNR). Compared to the traditional feature concatenation\nmethods, our proposed approach can achieve better recognition performance under\nboth clean and noisy conditions. We believe modality attention based end-to-end\nmethod can be easily generalized to other multimodal tasks with correlated\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:28:03 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 04:21:06 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhou", "Pan", ""], ["Yang", "Wenwen", ""], ["Chen", "Wei", ""], ["Wang", "Yanfeng", ""], ["Jia", "Jia", ""]]}, {"id": "1811.05253", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Yuan Xie, Fangyu Wu, Jeremy S. Smith, Wenjin Lu and\n  Bailing Zhang", "title": "Image Captioning Based on a Hierarchical Attention Mechanism and Policy\n  Gradient Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating the descriptions of an image, i.e., image\ncaptioning, is an important and fundamental topic in artificial intelligence,\nwhich bridges the gap between computer vision and natural language processing.\nBased on the successful deep learning models, especially the CNN model and Long\nShort-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical\nattention model by utilizing both of the global CNN features and the local\nobject features for more effective feature representation and reasoning in\nimage captioning. The generative adversarial network (GAN), together with a\nreinforcement learning (RL) algorithm, is applied to solve the exposure bias\nproblem in RNN-based supervised training for language problems. In addition,\nthrough the automatic measurement of the consistency between the generated\ncaption and the image content by the discriminator in the GAN framework and RL\noptimization, we make the finally generated sentences more accurate and\nnatural. Comprehensive experiments show the improved performance of the\nhierarchical attention mechanism and the effectiveness of our RL-based\noptimization method. Our model achieves state-of-the-art results on several\nimportant metrics in the MSCOCO dataset, using only greedy inference.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:31:26 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 03:31:31 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Yan", "Shiyang", ""], ["Xie", "Yuan", ""], ["Wu", "Fangyu", ""], ["Smith", "Jeremy S.", ""], ["Lu", "Wenjin", ""], ["Zhang", "Bailing", ""]]}, {"id": "1811.05255", "submitter": "Thierry Bouwmans", "authors": "Thierry Bouwmans, Sajid Javed, Maryam Sultana, Soon Ki Jung", "title": "Deep Neural Network Concepts for Background Subtraction: A Systematic\n  Review and Comparative Evaluation", "comments": "46 pages, 4 figures, submitted to neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional neural networks show a powerful framework for background\nsubtraction in video acquired by static cameras. Indeed, the well-known SOBS\nmethod and its variants based on neural networks were the leader methods on the\nlargescale CDnet 2012 dataset during a long time. Recently, convolutional\nneural networks which belong to deep learning methods were employed with\nsuccess for background initialization, foreground detection and deep learned\nfeatures. Currently, the top current background subtraction methods in CDnet\n2014 are based on deep neural networks with a large gap of performance in\ncomparison on the conventional unsupervised approaches based on multi-features\nor multi-cues strategies. Furthermore, a huge amount of papers was published\nsince 2016 when Braham and Van Droogenbroeck published their first work on CNN\napplied to background subtraction providing a regular gain of performance. In\nthis context, we provide the first review of deep neural network concepts in\nbackground subtraction for novices and experts in order to analyze this success\nand to provide further directions. For this, we first surveyed the methods used\nbackground initialization, background subtraction and deep learned features.\nThen, we discuss the adequacy of deep neural networks for background\nsubtraction. Finally, experimental results are presented on the CDnet 2014\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:35:19 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Bouwmans", "Thierry", ""], ["Javed", "Sajid", ""], ["Sultana", "Maryam", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1811.05295", "submitter": "Lei Jiang", "authors": "Lei Jiang, XiaoJun Wu, Josef Kittler", "title": "Pose Invariant 3D Face Reconstruction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction is an important task in the field of computer vision.\nAlthough 3D face reconstruction has being developing rapidly in recent years,\nit is still a challenge for face reconstruction under large pose. That is\nbecause much of the information about a face in a large pose will be\nunknowable. In order to address this issue, this paper proposes a novel 3D face\nreconstruction algorithm (PIFR) based on 3D Morphable Model (3DMM). After input\na single face image, it generates a frontal image by normalizing the image.\nThen we set weighted sum of the 3D parameters of the two images. Our method\nsolves the problem of face reconstruction of a single image of a traditional\nmethod in a large pose, works on arbitrary Pose and Expressions, greatly\nimproves the accuracy of reconstruction. Experiments on the challenging AFW,\nLFPW and AFLW database show that our algorithm significantly improves the\naccuracy of 3D face reconstruction even under extreme poses .\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:00:48 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Jiang", "Lei", ""], ["Wu", "XiaoJun", ""], ["Kittler", "Josef", ""]]}, {"id": "1811.05299", "submitter": "Kaixuan Chen", "authors": "Kaixuan Chen, Lina Yao, Dalin Zhang, Xiaojun Chang, Guodong Long, Sen\n  Wang", "title": "Distributionally Robust Semi-Supervised Learning for People-Centric\n  Sensing", "comments": "8 pages, accepted by AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is crucial for alleviating labelling burdens in\npeople-centric sensing. However, human-generated data inherently suffer from\ndistribution shift in semi-supervised learning due to the diverse biological\nconditions and behavior patterns of humans. To address this problem, we propose\na generic distributionally robust model for semi-supervised learning on\ndistributionally shifted data. Considering both the discrepancy and the\nconsistency between the labeled data and the unlabeled data, we learn the\nlatent features that reduce person-specific discrepancy and preserve\ntask-specific consistency. We evaluate our model in a variety of people-centric\nrecognition tasks on real-world datasets, including intention recognition,\nactivity recognition, muscular movement recognition and gesture recognition.\nThe experiment results demonstrate that the proposed model outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:53:33 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Zhang", "Dalin", ""], ["Chang", "Xiaojun", ""], ["Long", "Guodong", ""], ["Wang", "Sen", ""]]}, {"id": "1811.05304", "submitter": "Fu-En Wang", "authors": "Fu-En Wang, Hou-Ning Hu, Hsien-Tzu Cheng, Juan-Ting Lin, Shang-Ta\n  Yang, Meng-Li Shih, Hung-Kuo Chu, Min Sun", "title": "Self-Supervised Learning of Depth and Camera Motion from 360{\\deg}\n  Videos", "comments": "ACCV 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 360{\\deg} cameras become prevalent in many autonomous systems (e.g.,\nself-driving cars and drones), efficient 360{\\deg} perception becomes more and\nmore important. We propose a novel self-supervised learning approach for\npredicting the omnidirectional depth and camera motion from a 360{\\deg} video.\nIn particular, starting from the SfMLearner, which is designed for cameras with\nnormal field-of-view, we introduce three key features to process 360{\\deg}\nimages efficiently. Firstly, we convert each image from equirectangular\nprojection to cubic projection in order to avoid image distortion. In each\nnetwork layer, we use Cube Padding (CP), which pads intermediate features from\nadjacent faces, to avoid image boundaries. Secondly, we propose a novel\n\"spherical\" photometric consistency constraint on the whole viewing sphere. In\nthis way, no pixel will be projected outside the image boundary which typically\nhappens in images with normal field-of-view. Finally, rather than naively\nestimating six independent camera motions (i.e., naively applying SfM-Learner\nto each face on a cube), we propose a novel camera pose consistency loss to\nensure the estimated camera motions reaching consensus. To train and evaluate\nour approach, we collect a new PanoSUNCG dataset containing a large amount of\n360{\\deg} videos with groundtruth depth and camera motion. Our approach\nachieves state-of-the-art depth prediction and camera motion estimation on\nPanoSUNCG with faster inference speed comparing to equirectangular. In\nreal-world indoor videos, our approach can also achieve qualitatively\nreasonable depth prediction by acquiring model pre-trained on PanoSUNCG.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:07:27 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Wang", "Fu-En", ""], ["Hu", "Hou-Ning", ""], ["Cheng", "Hsien-Tzu", ""], ["Lin", "Juan-Ting", ""], ["Yang", "Shang-Ta", ""], ["Shih", "Meng-Li", ""], ["Chu", "Hung-Kuo", ""], ["Sun", "Min", ""]]}, {"id": "1811.05306", "submitter": "Qingwen Xu", "authors": "Qingwen Xu, Arturo Gomez Chavez, Heiko B\\\"ulow, Andreas Birk, S\\\"oren\n  Schwertfeger", "title": "Improved Fourier Mellin Invariant for Robust Rotation Estimation with\n  Omni-cameras", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": "10.1109/ICIP.2019.8802933", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods such as the improved Fourier Mellin Invariant (iFMI)\ntransform have proved faster, more robust and accurate than feature based\nmethods on image registration. However, iFMI is restricted to work only when\nthe camera moves in 2D space and has not been applied on omni-cameras images so\nfar. In this work, we extend the iFMI method and apply a motion model to\nestimate an omni-camera's pose when it moves in 3D space. This is particularly\nuseful in field robotics applications to get a rapid and comprehensive view of\nunstructured environments, and to estimate robustly the robot pose. In the\nexperiment section, we compared the extended iFMI method against ORB and AKAZE\nfeature based approaches on three datasets showing different type of\nenvironments: office, lawn and urban scenery (MPI-omni dataset). The results\nshow that our method boosts the accuracy of the robot pose estimation two to\nfour times with respect to the feature registration techniques, while offering\nlower processing times. Furthermore, the iFMI approach presents the best\nperformance against motion blur typically present in mobile robotics.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:09:32 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 13:12:50 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 15:59:59 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 02:16:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xu", "Qingwen", ""], ["Chavez", "Arturo Gomez", ""], ["B\u00fclow", "Heiko", ""], ["Birk", "Andreas", ""], ["Schwertfeger", "S\u00f6ren", ""]]}, {"id": "1811.05324", "submitter": "Lia Morra", "authors": "Daniela Sacchetto, Lia Morra, Silvano Agliozzo, Daniela Bernardi,\n  Tomas Bjorklund, Beniamino Brancato, Patrizia Bravetti, Luca A. Carbonaro,\n  Loredana Correale, Carmen Fant\\`o, Elisabetta Favettini, Laura Martincich,\n  Luisella Milanesio, Sara Mombelloni, Francesco Monetti, Doralba Morrone,\n  Marco Pellegrini, Barbara Pesce, Antonella Petrillo, Gianni Saguatti, Carmen\n  Stevanin, Rubina M. Trimboli, Paola Tuttobene, Marvi Valentini, Vincenzo\n  Marra, Alfonso Frigerio, Alberto Bert, Francesco Sardanelli", "title": "Mammographic density: Comparison of visual assessment with fully\n  automatic calculation on a multivendor dataset", "comments": null, "journal-ref": "Sacchetto, Daniela et al. \"Mammographic density: comparison of\n  visual assessment with fully automatic calculation on a multivendor dataset.\"\n  European radiology 26, no. 1 (2016): 175-183", "doi": "10.1007/s00330-015-3784-2", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objectives: To compare breast density (BD) assessment provided by an\nautomated BD evaluator (ABDE) with that provided by a panel of experienced\nbreast radiologists, on a multivendor dataset.\n  Methods: Twenty-one radiologists assessed 613 screening/diagnostic digital\nmammograms from 9 centers and 6 different vendors, using the BI-RADS a, b, c,\nand d density classification. The same mammograms were also evaluated by an\nABDE providing the ratio between fibroglandular and total breast area on a\ncontinuous scale and, automatically, the BI-RADS score. Panel majority report\n(PMR) was used as reference standard. Agreement (k) and accuracy (proportion of\ncases correctly classified) were calculated for binary (BI-RADS a-b versus c-d)\nand 4-class classification.\n  Results: While the agreement of individual radiologists with PMR ranged from\nk=0.483 to k=0.885, the ABDE correctly classified 563/613 mammograms (92%). A\nsubstantial agreement for binary classification was found for individual reader\npairs (k=0.620, standard deviation [SD]=0.140), individual versus PMR (k=0.736,\nSD=0.117), and individual versus ABDE (k=0.674, SD=0.095). Agreement between\nABDE and PMR was almost perfect (k=0.831).\n  Conclusions: The ABDE showed an almost perfect agreement with a\n21-radiologist panel in binary BD classification on a multivendor dataset,\nearning a chance as a reproducible alternative to visual evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:31:36 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sacchetto", "Daniela", ""], ["Morra", "Lia", ""], ["Agliozzo", "Silvano", ""], ["Bernardi", "Daniela", ""], ["Bjorklund", "Tomas", ""], ["Brancato", "Beniamino", ""], ["Bravetti", "Patrizia", ""], ["Carbonaro", "Luca A.", ""], ["Correale", "Loredana", ""], ["Fant\u00f2", "Carmen", ""], ["Favettini", "Elisabetta", ""], ["Martincich", "Laura", ""], ["Milanesio", "Luisella", ""], ["Mombelloni", "Sara", ""], ["Monetti", "Francesco", ""], ["Morrone", "Doralba", ""], ["Pellegrini", "Marco", ""], ["Pesce", "Barbara", ""], ["Petrillo", "Antonella", ""], ["Saguatti", "Gianni", ""], ["Stevanin", "Carmen", ""], ["Trimboli", "Rubina M.", ""], ["Tuttobene", "Paola", ""], ["Valentini", "Marvi", ""], ["Marra", "Vincenzo", ""], ["Frigerio", "Alfonso", ""], ["Bert", "Alberto", ""], ["Sardanelli", "Francesco", ""]]}, {"id": "1811.05340", "submitter": "Wenxuan Xie", "authors": "Hao Luo, Wenxuan Xie, Xinggang Wang, Wenjun Zeng", "title": "Detect or Track: Towards Cost-Effective Video Object Detection/Tracking", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detectors and trackers are developing fast. Trackers\nare in general more efficient than detectors but bear the risk of drifting. A\nquestion is hence raised -- how to improve the accuracy of video object\ndetection/tracking by utilizing the existing detectors and trackers within a\ngiven time budget? A baseline is frame skipping -- detecting every N-th frames\nand tracking for the frames in between. This baseline, however, is suboptimal\nsince the detection frequency should depend on the tracking quality. To this\nend, we propose a scheduler network, which determines to detect or track at a\ncertain frame, as a generalization of Siamese trackers. Although being\nlight-weight and simple in structure, the scheduler network is more effective\nthan the frame skipping baselines and flow-based approaches, as validated on\nImageNet VID dataset in video object detection/tracking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:44:10 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Luo", "Hao", ""], ["Xie", "Wenxuan", ""], ["Wang", "Xinggang", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1811.05389", "submitter": "Songwei Ge", "authors": "Chun-Liang Li, Eunsu Kang, Songwei Ge, Lingyao Zhang, Austin Dill,\n  Manzil Zaheer, Barnabas Poczos", "title": "Hallucinating Point Cloud into 3D Sculptural Object", "comments": "Accepted by Second Workshop on Machine Learning for Creativity and\n  Design, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our team of artists and machine learning researchers designed a creative\nalgorithm that can generate authentic sculptural artworks. These artworks do\nnot mimic any given forms and cannot be easily categorized into the dataset\ncategories. Our approach extends DeepDream from images to 3D point clouds. The\nproposed algorithm, Amalgamated DeepDream (ADD), leverages the properties of\npoint clouds to create objects with better quality than the naive extension.\nADD presents promise for the creativity of machines, the kind of creativity\nthat pushes artists to explore novel methods or materials and to create new\ngenres instead of creating variations of existing forms or styles within one\ngenre. For example, from Realism to Abstract Expressionism, or to Minimalism.\nLastly, we present the sculptures that are 3D printed based on the point clouds\ncreated by ADD.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:28:32 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 23:08:27 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 04:41:54 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Chun-Liang", ""], ["Kang", "Eunsu", ""], ["Ge", "Songwei", ""], ["Zhang", "Lingyao", ""], ["Dill", "Austin", ""], ["Zaheer", "Manzil", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1811.05416", "submitter": "Lili Tao", "authors": "Lili Tao and Timothy Volonakis and Bo Tan and Yanguo Jing and Kevin\n  Chetty and Melvyn Smith", "title": "Home Activity Monitoring using Low Resolution Infrared Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action monitoring in a home environment provides important information for\nhealth monitoring and may serve as input into a smart home environment. Visual\nanalysis using cameras can recognise actions in a complex scene, such as\nsomeones living room. However, although there the huge potential benefits and\nimportance, specifically for health, cameras are not widely accepted because of\nprivacy concerns. This paper recognises human activities using a sensor that\nretains privacy. The sensor is not only different by being thermal, but it is\nalso of low resolution: 8x8 pixels. The combination of the thermal imaging, and\nthe low spatial resolution ensures the privacy of individuals. We present an\napproach to recognise daily activities using this sensor based on a discrete\ncosine transform. We evaluate the proposed method on a state-of-the-art dataset\nand experimentally confirm that our approach outperforms the baseline method.\nWe also introduce a new dataset, and evaluate the method on it. Here we show\nthat the sensor is considered better at detecting the occurrence of falls and\nActivities of Daily Living. Our method achieves an overall accuracy of 87.50%\nacross 7 activities with a fall detection sensitivity of 100% and specificity\nof 99.21%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:15:11 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Tao", "Lili", ""], ["Volonakis", "Timothy", ""], ["Tan", "Bo", ""], ["Jing", "Yanguo", ""], ["Chetty", "Kevin", ""], ["Smith", "Melvyn", ""]]}, {"id": "1811.05419", "submitter": "Feng Zhang", "authors": "Feng Zhang, Xiatian Zhu, Mao Ye", "title": "Fast Human Pose Estimation", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing human pose estimation approaches often only consider how to improve\nthe model generalisation performance, but putting aside the significant\nefficiency problem. This leads to the development of heavy models with poor\nscalability and cost-effectiveness in practical use. In this work, we\ninvestigate the under-studied but practically critical pose model efficiency\nproblem. To this end, we present a new Fast Pose Distillation (FPD) model\nlearning strategy. Specifically, the FPD trains a lightweight pose neural\nnetwork architecture capable of executing rapidly with low computational cost.\nIt is achieved by effectively transferring the pose structure knowledge of a\nstrong teacher network. Extensive evaluations demonstrate the advantages of our\nFPD method over a broad range of state-of-the-art pose estimation approaches in\nterms of model cost-effectiveness on two standard benchmark datasets, MPII\nHuman Pose and Leeds Sports Pose.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:18:14 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 14:05:56 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhang", "Feng", ""], ["Zhu", "Xiatian", ""], ["Ye", "Mao", ""]]}, {"id": "1811.05432", "submitter": "Coline Devin", "authors": "Dequan Wang, Coline Devin, Qi-Zhi Cai, Fisher Yu, Trevor Darrell", "title": "Deep Object-Centric Policies for Autonomous Driving", "comments": "Accepted at ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learning visuomotor skills in an end-to-end manner is appealing, deep\nneural networks are often uninterpretable and fail in surprising ways. For\nrobotics tasks, such as autonomous driving, models that explicitly represent\nobjects may be more robust to new scenes and provide intuitive visualizations.\nWe describe a taxonomy of \"object-centric\" models which leverage both object\ninstances and end-to-end learning. In the Grand Theft Auto V simulator, we show\nthat object-centric models outperform object-agnostic methods in scenes with\nother vehicles and pedestrians, even with an imperfect detector. We also\ndemonstrate that our architectures perform well on real-world environments by\nevaluating on the Berkeley DeepDrive Video dataset, where an object-centric\nmodel outperforms object-agnostic models in the low-data regimes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:44:24 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 17:39:36 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Wang", "Dequan", ""], ["Devin", "Coline", ""], ["Cai", "Qi-Zhi", ""], ["Yu", "Fisher", ""], ["Darrell", "Trevor", ""]]}, {"id": "1811.05531", "submitter": "Dimitris Spathis", "authors": "Dimitris Spathis, Nikolaos Passalis, Anastasios Tefas", "title": "Interactive dimensionality reduction using similarity projections", "comments": "Accepted at Knowledge-Based Systems", "journal-ref": null, "doi": "10.1016/j.knosys.2018.11.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning allow us to analyze and describe the\ncontent of high-dimensional data like text, audio, images or other signals. In\norder to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR)\ntechniques are employed. Most of these techniques, e.g., PCA or t-SNE, produce\nstatic projections without taking into account corrections from humans or other\ndata exploration scenarios. In this work, we propose the interactive Similarity\nProjection (iSP), a novel interactive DR framework based on similarity\nembeddings, where we form a differentiable objective based on the user\ninteractions and perform learning using gradient descent, with an end-to-end\ntrainable architecture. Two interaction scenarios are evaluated. First, a\ncommon methodology in multidimensional projection is to project a subset of\ndata, arrange them in classes or clusters, and project the rest unseen dataset\nbased on that manipulation, in a kind of semi-supervised interpolation. We\nreport results that outperform competitive baselines in a wide range of metrics\nand datasets. Second, we explore the scenario of manipulating some classes,\nwhile enriching the optimization with high-dimensional neighbor information.\nApart from improving classification precision and clustering on images and text\ndocuments, the new emerging structure of the projection unveils semantic\nmanifolds. For example, on the Head Pose dataset, by just dragging the faces\nlooking far left to the left and those looking far right to the right, all\nfaces are re-arranged on a continuum even on the vertical axis (face up and\ndown). This end-to-end framework can be used for fast, visual semi-supervised\nlearning, manifold exploration, interactive domain adaptation of neural\nembeddings and transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 21:21:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Spathis", "Dimitris", ""], ["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1811.05588", "submitter": "Jonathan Pedoeem", "authors": "Jonathan Pedoeem, Rachel Huang", "title": "YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU\n  Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on YOLO-LITE, a real-time object detection model developed\nto run on portable devices such as a laptop or cellphone lacking a Graphics\nProcessing Unit (GPU). The model was first trained on the PASCAL VOC dataset\nthen on the COCO dataset, achieving a mAP of 33.81% and 12.26% respectively.\nYOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after\nimplemented onto a website with only 7 layers and 482 million FLOPS. This speed\nis 3.8x faster than the fastest state of art model, SSD MobilenetvI. Based on\nthe original object detection algorithm YOLOV2, YOLO- LITE was designed to\ncreate a smaller, faster, and more efficient model increasing the accessibility\nof real-time object detection to a variety of devices.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 01:20:08 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Pedoeem", "Jonathan", ""], ["Huang", "Rachel", ""]]}, {"id": "1811.05621", "submitter": "Hadi Kazemi", "authors": "Hadi Kazemi, Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi", "title": "Style and Content Disentanglement in Generative Adversarial Networks", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangling factors of variation within data has become a very challenging\nproblem for image generation tasks. Current frameworks for training a\nGenerative Adversarial Network (GAN), learn to disentangle the representations\nof the data in an unsupervised fashion and capture the most significant factors\nof the data variations. However, these approaches ignore the principle of\ncontent and style disentanglement in image generation, which means their\nlearned latent code may alter the content and style of the generated images at\nthe same time. This paper describes the Style and Content Disentangled GAN\n(SC-GAN), a new unsupervised algorithm for training GANs that learns\ndisentangled style and content representations of the data. We assume that the\nrepresentation of an image can be decomposed into a content code that\nrepresents the geometrical information of the data, and a style code that\ncaptures textural properties. Consequently, by fixing the style portion of the\nlatent representation, we can generate diverse images in a particular style.\nReversely, we can set the content code and generate a specific scene in a\nvariety of styles. The proposed SC-GAN has two components: a content code which\nis the input to the generator, and a style code which modifies the scene style\nthrough modification of the Adaptive Instance Normalization (AdaIN) layers'\nparameters. We evaluate the proposed SC-GAN framework on a set of baseline\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 03:25:07 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kazemi", "Hadi", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1811.05625", "submitter": "Jia Li", "authors": "Kui Fu, Jia Li, Yu Zhang, Hongze Shen, Yonghong Tian", "title": "Model-guided Multi-path Knowledge Aggregation for Aerial Saliency\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging vision platform, a drone can look from many abnormal\nviewpoints which brings many new challenges into the classic vision task of\nvideo saliency prediction. To investigate these challenges, this paper proposes\na large-scale video dataset for aerial saliency prediction, which consists of\nground-truth salient object regions of 1,000 aerial videos, annotated by 24\nsubjects. To the best of our knowledge, it is the first large-scale video\ndataset that focuses on visual saliency prediction on drones. Based on this\ndataset, we propose a Model-guided Multi-path Network (MM-Net) that serves as a\nbaseline model for aerial video saliency prediction. Inspired by the annotation\nprocess in eye-tracking experiments, MM-Net adopts multiple information paths,\neach of which is initialized under the guidance of a classic saliency model.\nAfter that, the visual saliency knowledge encoded in the most representative\npaths is selected and aggregated to improve the capability of MM-Net in\npredicting spatial saliency in aerial scenarios. Finally, these spatial\npredictions are adaptively combined with the temporal saliency predictions via\na spatiotemporal optimization algorithm. Experimental results show that MM-Net\noutperforms ten state-of-the-art models in predicting aerial video saliency.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 03:56:01 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 06:53:48 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fu", "Kui", ""], ["Li", "Jia", ""], ["Zhang", "Yu", ""], ["Shen", "Hongze", ""], ["Tian", "Yonghong", ""]]}, {"id": "1811.05699", "submitter": "Ezequiel De La Rosa", "authors": "Ezequiel de la Rosa, Diana M. Sima, Thijs Vande Vyvere, Jan S.\n  Kirschke, Bjoern Menze", "title": "A Radiomics Approach to Traumatic Brain Injury Prediction in CT Scans", "comments": "Submitted to ISBI 2019", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019) (pp. 732-735). IEEE", "doi": "10.1109/ISBI.2019.8759229", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Tomography (CT) is the gold standard technique for brain damage\nevaluation after acute Traumatic Brain Injury (TBI). It allows identification\nof most lesion types and determines the need of surgical or alternative\ntherapeutic procedures. However, the traditional approach for lesion\nclassification is restricted to visual image inspection. In this work, we\ncharacterize and predict TBI lesions by using CT-derived radiomics descriptors.\nRelevant shape, intensity and texture biomarkers characterizing the different\nlesions are isolated and a lesion predictive model is built by using Partial\nLeast Squares. On a dataset containing 155 scans (105 train, 50 test) the\nmethodology achieved 89.7 % accuracy over the unseen data. When a model was\nbuild using only texture features, a 88.2 % accuracy was obtained. Our results\nsuggest that selected radiomics descriptors could play a key role in brain\ninjury prediction. Besides, the proposed methodology is close to reproduce\nradiologists decision making. These results open new possibilities for\nradiomics-inspired brain lesion detection, segmentation and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:29:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["de la Rosa", "Ezequiel", ""], ["Sima", "Diana M.", ""], ["Vyvere", "Thijs Vande", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern", ""]]}, {"id": "1811.05760", "submitter": "Aniruddha Bhattacharya", "authors": "Aniruddha Bhattacharya and K.V. Kadambari", "title": "A Multimodal Approach towards Emotion Recognition of Music using Audio\n  and Lyrical Content", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MoodNet - A Deep Convolutional Neural Network based architecture\nto effectively predict the emotion associated with a piece of music given its\naudio and lyrical content.We evaluate different architectures consisting of\nvarying number of two-dimensional convolutional and subsampling layers,followed\nby dense layers.We use Mel-Spectrograms to represent the audio content and word\nembeddings-specifically 100 dimensional word vectors, to represent the textual\ncontent represented by the lyrics.We feed input data from both modalities to\nour MoodNet architecture.The output from both the modalities are then fused as\na fully connected layer and softmax classfier is used to predict the category\nof emotion.Using F1-score as our metric,our results show excellent performance\nof MoodNet over the two datasets we experimented on-The MIREX Multimodal\ndataset and the Million Song Dataset.Our experiments reflect the hypothesis\nthat more complex models perform better with more training data.We also observe\nthat lyrics outperform audio as a better expressed modality and conclude that\ncombining and using features from multiple modalities for prediction tasks\nresult in superior performance in comparison to using a single modality as\ninput.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 20:51:03 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Bhattacharya", "Aniruddha", ""], ["Kadambari", "K. V.", ""]]}, {"id": "1811.05773", "submitter": "Christian Bartz", "authors": "Christian Bartz, Haojin Yang, Joseph Bethge and Christoph Meinel", "title": "LoANs: Weakly Supervised Object Detection with Localizer Assessor\n  Networks", "comments": "To appear in AMV18. Code, datasets and models available at\n  https://github.com/Bartzi/loans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have achieved remarkable performance on the\ntask of object detection and recognition. The reason for this success is mainly\ngrounded in the availability of large scale, fully annotated datasets, but the\ncreation of such a dataset is a complicated and costly task. In this paper, we\npropose a novel method for weakly supervised object detection that simplifies\nthe process of gathering data for training an object detector. We train an\nensemble of two models that work together in a student-teacher fashion. Our\nstudent (localizer) is a model that learns to localize an object, the teacher\n(assessor) assesses the quality of the localization and provides feedback to\nthe student. The student uses this feedback to learn how to localize objects\nand is thus entirely supervised by the teacher, as we are using no labels for\ntraining the localizer. In our experiments, we show that our model is very\nrobust to noise and reaches competitive performance compared to a\nstate-of-the-art fully supervised approach. We also show the simplicity of\ncreating a new dataset, based on a few videos (e.g. downloaded from YouTube)\nand artificially generated data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 13:45:54 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 15:55:55 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Bartz", "Christian", ""], ["Yang", "Haojin", ""], ["Bethge", "Joseph", ""], ["Meinel", "Christoph", ""]]}, {"id": "1811.05804", "submitter": "Benjamin Biggs", "authors": "Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon and Roberto Cipolla", "title": "Creatures great and SMAL: Recovering the shape and motion of animals\n  from video", "comments": "17 pages, ACCV 2018 oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system to recover the 3D shape and motion of a wide variety of\nquadrupeds from video. The system comprises a machine learning front-end which\npredicts candidate 2D joint positions, a discrete optimization which finds\nkinematically plausible joint correspondences, and an energy minimization stage\nwhich fits a detailed 3D model to the image. In order to overcome the limited\navailability of motion capture training data from animals, and the difficulty\nof generating realistic synthetic training images, the system is designed to\nwork on silhouette data. The joint candidate predictor is trained on\nsynthetically generated silhouette images, and at test time, deep learning\nmethods or standard video segmentation tools are used to extract silhouettes\nfrom real data. The system is tested on animal videos from several species, and\nshows accurate reconstructions of 3D shape and pose.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:24:07 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Biggs", "Benjamin", ""], ["Roddick", "Thomas", ""], ["Fitzgibbon", "Andrew", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1811.05817", "submitter": "Alexander Wong", "authors": "Xiaodan Hu, Audrey G. Chung, Paul Fieguth, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "ProstateGAN: Mitigating Data Bias via Prostate Diffusion Imaging\n  Synthesis with Generative Adversarial Networks", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown considerable promise for\nmitigating the challenge of data scarcity when building machine learning-driven\nanalysis algorithms. Specifically, a number of studies have shown that\nGAN-based image synthesis for data augmentation can aid in improving\nclassification accuracy in a number of medical image analysis tasks, such as\nbrain and liver image analysis. However, the efficacy of leveraging GANs for\ntackling prostate cancer analysis has not been previously explored. Motivated\nby this, in this study we introduce ProstateGAN, a GAN-based model for\nsynthesizing realistic prostate diffusion imaging data. More specifically, in\norder to generate new diffusion imaging data corresponding to a particular\ncancer grade (Gleason score), we propose a conditional deep convolutional GAN\narchitecture that takes Gleason scores into consideration during the training\nprocess. Experimental results show that high-quality synthetic prostate\ndiffusion imaging data can be generated using the proposed ProstateGAN for\nspecified Gleason scores.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:44:42 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 01:35:54 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Hu", "Xiaodan", ""], ["Chung", "Audrey G.", ""], ["Fieguth", "Paul", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1811.05819", "submitter": "Md Tahmid Hossain", "authors": "Md Tahmid Hossain, Shyh Wei Teng, Dengsheng Zhang, Suryani Lim, Guojun\n  Lu", "title": "Distortion Robust Image Classification using Deep Convolutional Neural\n  Network with Discrete Cosine Transform", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP.2019.8803787", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network is good at image classification. However, it is\nfound to be vulnerable to image quality degradation. Even a small amount of\ndistortion such as noise or blur can severely hamper the performance of these\nCNN architectures. Most of the work in the literature strives to mitigate this\nproblem simply by fine-tuning a pre-trained CNN on mutually exclusive or a\nunion set of distorted training data. This iterative fine-tuning process with\nall known types of distortion is exhaustive and the network struggles to handle\nunseen distortions. In this work, we propose distortion robust DCT-Net, a\nDiscrete Cosine Transform based module integrated into a deep network which is\nbuilt on top of VGG16. Unlike other works in the literature, DCT-Net is \"blind\"\nto the distortion type and level in an image both during training and testing.\nAs a part of the training process, the proposed DCT module discards input\ninformation which mostly represents the contribution of high frequencies. The\nDCT-Net is trained \"blindly\" only once and applied in generic situation without\nfurther retraining. We also extend the idea of traditional dropout and present\na training adaptive version of the same. We evaluate our proposed method\nagainst Gaussian blur, motion blur, salt and pepper noise, Gaussian noise and\nspeckle noise added to CIFAR-10/100 and ImageNet test sets. Experimental\nresults demonstrate that once trained, DCT-Net not only generalizes well to a\nvariety of unseen image distortions but also outperforms other methods in the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:52:06 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 11:48:11 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 03:07:57 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 09:32:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hossain", "Md Tahmid", ""], ["Teng", "Shyh Wei", ""], ["Zhang", "Dengsheng", ""], ["Lim", "Suryani", ""], ["Lu", "Guojun", ""]]}, {"id": "1811.05863", "submitter": "Xu Han", "authors": "Xu Han, Laurent Albera, Amar Kachenoura, Huazhong Shu, Lotfi Senhadji", "title": "Robust low-rank multilinear tensor approximation for a joint estimation\n  of the multilinear rank and the loading matrices", "comments": "it needs to be improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to compute the best low-rank tensor approximation using the\nMultilinear Tensor Decomposition (MTD) model, it is essential to estimate the\nrank of the underlying multilinear tensor from the noisy observation tensor. In\nthis paper, we propose a Robust MTD (R-MTD) method, which jointly estimates the\nmultilinear rank and the loading matrices. Based on the low-rank property and\nan over-estimation of the core tensor, this joint estimation problem is solved\nby promoting (group) sparsity of the over-estimated core tensor. Group sparsity\nis promoted using mixed-norms. Then we establish a link between the mixed-norms\nand the nuclear norm, showing that mixed-norms are better candidates for a\nconvex envelope of the rank. After several iterations of the Alternating\nDirection Method of Multipliers (ADMM), the Minimum Description Length (MDL)\ncriterion computed from the eigenvalues of the unfolding matrices of the\nestimated core tensor is minimized in order to estimate the multilinear rank.\nThe latter is then used to estimate more accurately the loading matrices. We\nfurther develop another R-MTD method, called R-OMTD, by imposing an\northonormality constraint on each loading matrix in order to decrease the\ncomputation complexity. A series of simulated noisy tensor and real-world data\nare used to show the effectiveness of the proposed methods compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:46:34 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:32:29 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Han", "Xu", ""], ["Albera", "Laurent", ""], ["Kachenoura", "Amar", ""], ["Shu", "Huazhong", ""], ["Senhadji", "Lotfi", ""]]}, {"id": "1811.05894", "submitter": "Daniil Osokin", "authors": "Alexander Kozlov, Daniil Osokin", "title": "Development of Real-time ADAS Object Detector for Deployment on CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we outline the set of problems, which any Object Detection CNN\nfaces when its development comes to the deployment stage and propose methods to\ndeal with such difficulties. We show that these practices allow one to get\nObject Detection network, which can recognize two classes: vehicles and\npedestrians and achieves more than 60 frames per second inference speed on\nCore$^{TM}$ i5-6500 CPU. The proposed model is built on top of the popular\nSingle Shot MultiBox Object Detection framework but with substantial\nimprovements, which were inspired by the discovered problems. The network has\njust 1.96 GMAC complexity and less than 7 MB model size. It is publicly\navailable as a part of Intel$\\circledR$ OpenVINO$^{TM}$ Toolkit.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 16:37:09 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kozlov", "Alexander", ""], ["Osokin", "Daniil", ""]]}, {"id": "1811.05911", "submitter": "Benjamin Naujoks", "authors": "Benjamin Naujoks, Patrick Burger and Hans-Joachim Wuensche", "title": "The Greedy Dirichlet Process Filter - An Online Clustering Multi-Target\n  Tracker", "comments": null, "journal-ref": null, "doi": "10.1109/GlobalSIP.2018.8646554", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable collision avoidance is one of the main requirements for autonomous\ndriving. Hence, it is important to correctly estimate the states of an unknown\nnumber of static and dynamic objects in real-time. Here, data association is a\nmajor challenge for every multi-target tracker. We propose a novel multi-target\ntracker called Greedy Dirichlet Process Filter (GDPF) based on the\nnon-parametric Bayesian model called Dirichlet Processes and the fast posterior\ncomputation algorithm Sequential Updating and Greedy Search (SUGS). By adding a\ntemporal dependence we get a real-time capable tracking framework without the\nneed of a previous clustering or data association step. Real-world tests show\nthat GDPF outperforms other multi-target tracker in terms of accuracy and\nstability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:09:22 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 17:20:41 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Naujoks", "Benjamin", ""], ["Burger", "Patrick", ""], ["Wuensche", "Hans-Joachim", ""]]}, {"id": "1811.05939", "submitter": "Rawal Khirodkar", "authors": "Rawal Khirodkar, Donghyun Yoo, Kris M. Kitani", "title": "Domain Randomization for Scene-Specific Car Detection and Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of domain gap when making use of synthetic data to train\na scene-specific object detector and pose estimator. While previous works have\nshown that the constraints of learning a scene-specific model can be leveraged\nto create geometrically and photometrically consistent synthetic data, care\nmust be taken to design synthetic content which is as close as possible to the\nreal-world data distribution. In this work, we propose to solve domain gap\nthrough the use of appearance randomization to generate a wide range of\nsynthetic objects to span the space of realistic images for training. An\nablation study of our results is presented to delineate the individual\ncontribution of different components in the randomization process. We evaluate\nour method on VIRAT, UA-DETRAC, EPFL-Car datasets, where we demonstrate that\nusing scene specific domain randomized synthetic data is better than\nfine-tuning off-the-shelf models on limited real data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:13:34 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Khirodkar", "Rawal", ""], ["Yoo", "Donghyun", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1811.05967", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Alexander Schwing and Derek Hoiem", "title": "No-Frills Human-Object Interaction Detection: Factorization, Layout\n  Encodings, and Training Techniques", "comments": "Accepted to ICCV 2019. Project Page:\n  http://tanmaygupta.info/no_frills/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for human-object interaction detection a relatively simple\nfactorized model with appearance and layout encodings constructed from\npre-trained object detectors outperforms more sophisticated approaches. Our\nmodel includes factors for detection scores, human and object appearance, and\ncoarse (box-pair configuration) and optionally fine-grained layout (human\npose). We also develop training techniques that improve learning efficiency by:\n(1) eliminating a train-inference mismatch; (2) rejecting easy negatives during\nmini-batch training; and (3) using a ratio of negatives to positives that is\ntwo orders of magnitude larger than existing approaches. We conduct a thorough\nablation study to understand the importance of different factors and training\ntechniques using the challenging HICO-Det dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:58:14 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 17:58:17 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Gupta", "Tanmay", ""], ["Schwing", "Alexander", ""], ["Hoiem", "Derek", ""]]}, {"id": "1811.06017", "submitter": "Cunxi Yu", "authors": "Cunxi Yu and Wang Zhou", "title": "Performance Estimation of Synthesis Flows cross Technologies using LSTMs\n  and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing complexity of Integrated Circuits (ICs) and\nSystem-on-Chip (SoC), developing high-quality synthesis flows within a short\nmarket time becomes more challenging. We propose a general approach that\nprecisely estimates the Quality-of-Result (QoR), such as delay and area, of\nunseen synthesis flows for specific designs. The main idea is training a\nRecurrent Neural Network (RNN) regressor, where the flows are inputs and QoRs\nare ground truth. The RNN regressor is constructed with Long Short-Term Memory\n(LSTM) and fully-connected layers. This approach is demonstrated with 1.2\nmillion data points collected using 14nm, 7nm regular-voltage (RVT), and 7nm\nlow-voltage (LVT) FinFET technologies with twelve IC designs. The accuracy of\npredicting the QoRs (delay and area) within one technology is\n$\\boldsymbol{\\geq}$\\textbf{98.0}\\% over $\\sim$240,000 test points. To enable\naccurate predictions cross different technologies and different IC designs, we\npropose a transfer-learning approach that utilizes the model pre-trained with\n14nm datasets. Our transfer learning approach obtains estimation accuracy\n$\\geq$96.3\\% over $\\sim$960,000 test points, using only 100 data points for\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:17:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Yu", "Cunxi", ""], ["Zhou", "Wang", ""]]}, {"id": "1811.06038", "submitter": "Mahdi S. Hosseini Dr.", "authors": "Mahdi S. Hosseini and Yueyang Zhang and Lyndon Chan and Konstantinos\n  N. Plataniotis and Jasper A.Z. Brawley-Hayes and Savvas Damaskinos", "title": "Focus Quality Assessment of High-Throughput Whole Slide Imaging in\n  Digital Pathology", "comments": "10 pages, This work has been submitted to the IEEE for possible\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges facing the adoption of digital pathology workflows for\nclinical use is the need for automated quality control. As the scanners\nsometimes determine focus inaccurately, the resultant image blur deteriorates\nthe scanned slide to the point of being unusable. Also, the scanned slide\nimages tend to be extremely large when scanned at greater or equal 20X image\nresolution. Hence, for digital pathology to be clinically useful, it is\nnecessary to use computational tools to quickly and accurately quantify the\nimage focus quality and determine whether an image needs to be re-scanned. We\npropose a no-reference focus quality assessment metric specifically for digital\npathology images, that operates by using a sum of even-derivative filter bases\nto synthesize a human visual system-like kernel, which is modeled as the\ninverse of the lens' point spread function. This kernel is then applied to a\ndigital pathology image to modify high-frequency image information deteriorated\nby the scanner's optics and quantify the focus quality at the patch level. We\nshow in several experiments that our method correlates better with ground-truth\n$z$-level data than other methods, and is more computationally efficient. We\nalso extend our method to generate a local slide-level focus quality heatmap,\nwhich can be used for automated slide quality control, and demonstrate the\nutility of our method for clinical scan quality control by comparison with\nsubjective slide quality scores.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:07:26 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Hosseini", "Mahdi S.", ""], ["Zhang", "Yueyang", ""], ["Chan", "Lyndon", ""], ["Plataniotis", "Konstantinos N.", ""], ["Brawley-Hayes", "Jasper A. Z.", ""], ["Damaskinos", "Savvas", ""]]}, {"id": "1811.06042", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone, Pedro Ballester, Rodrigo C. Barros, Julien\n  Cohen-Adad", "title": "Unsupervised domain adaptation for medical imaging segmentation with\n  self-ensembling", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in deep learning methods have come to define the\nstate-of-the-art for many medical imaging applications, surpassing even human\njudgment in several tasks. Those models, however, when trained to reduce the\nempirical risk on a single domain, fail to generalize when applied to other\ndomains, a very common scenario in medical imaging due to the variability of\nimages and anatomical structures, even across the same imaging modality. In\nthis work, we extend the method of unsupervised domain adaptation using\nself-ensembling for the semantic segmentation task and explore multiple facets\nof the method on a small and realistic publicly-available magnetic resonance\n(MRI) dataset. Through an extensive evaluation, we show that self-ensembling\ncan indeed improve the generalization of the models even when using a small\namount of unlabelled data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:18:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 22:38:53 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Perone", "Christian S.", ""], ["Ballester", "Pedro", ""], ["Barros", "Rodrigo C.", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1811.06047", "submitter": "Nachiket Deo", "authors": "Nachiket Deo and Mohan M. Trivedi", "title": "Looking at the Driver/Rider in Autonomous Vehicles to Predict Take-Over\n  Readiness", "comments": "Submitted to IEEE transactions on Intelligent Vehicles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous estimation the driver's take-over readiness is critical for safe\nand timely transfer of control during the failure modes of autonomous vehicles.\nIn this paper, we propose a data-driven approach for estimating the driver's\ntake-over readiness based purely on observable cues from in-vehicle vision\nsensors. We present an extensive naturalistic drive dataset of drivers in a\nconditionally autonomous vehicle running on Californian freeways. We collect\nsubjective ratings for the driver's take-over readiness from multiple human\nobservers viewing the sensor feed. Analysis of the ratings in terms of\nintra-class correlation coefficients (ICCs) shows a high degree of consistency\nin the ratings across raters. We define a metric for the driver's take-over\nreadiness termed the 'Observable Readiness Index (ORI)' based on the ratings.\nFinally, we propose an LSTM model for continuous estimation of the driver's ORI\nbased on a holistic representation of the driver's state, capturing gaze, hand,\npose and foot activity. Our model estimates the ORI with a mean absolute error\nof 0.449 on a 5 point scale.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:29:37 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1811.06051", "submitter": "Antong Chen", "authors": "Dongqing Zhang, Ilknur Icke, Belma Dogdas, Sarayu Parimal, Smita\n  Sampath, Joseph Forbes, Ansuman Bagchi, Chih-Liang Chin, Antong Chen", "title": "A multi-level convolutional LSTM model for the segmentation of left\n  ventricle myocardium in infarcted porcine cine MR images", "comments": "4 pages, 3 figures, 1 table. IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of left ventricle (LV) myocardium in cardiac\nshort-axis cine MR images acquired on subjects with myocardial infarction is a\nchallenging task, mainly because of the various types of image inhomogeneity\ncaused by the infarctions. Among the approaches proposed to automate the LV\nmyocardium segmentation task, methods based upon deep convolutional neural\nnetworks (CNN) have demonstrated their exceptional accuracy and robustness in\nrecent years. However, most of the CNN-based approaches treat the frames in a\ncardiac cycle independently, which fails to capture the valuable dynamics of\nheart motion. Herein, an approach based on recurrent neural network (RNN),\nspecifically a multi-level convolutional long short-term memory (ConvLSTM)\nmodel, is proposed to take the motion of the heart into consideration. Based on\na ResNet-56 CNN, LV-related image features in consecutive frames of a cardiac\ncycle are extracted at both the low- and high-resolution levels, which are\nprocessed by the corresponding multi-level ConvLSTM models to generate the\nmyocardium segmentations. A leave-one-out experiment was carried out on a set\nof 3,600 cardiac cine MR slices collected in-house for 8 porcine subjects with\nsurgically induced myocardial infarction. Compared with a solely CNN-based\napproach, the proposed approach demonstrated its superior robustness against\nimage inhomogeneity by incorporating information from adjacent frames. It also\noutperformed a one-level ConvLSTM approach thanks to its capabilities to take\nadvantage of image features at multiple resolution levels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:37:53 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhang", "Dongqing", ""], ["Icke", "Ilknur", ""], ["Dogdas", "Belma", ""], ["Parimal", "Sarayu", ""], ["Sampath", "Smita", ""], ["Forbes", "Joseph", ""], ["Bagchi", "Ansuman", ""], ["Chin", "Chih-Liang", ""], ["Chen", "Antong", ""]]}, {"id": "1811.06065", "submitter": "Vincenzo Ciancia", "authors": "Fabrizio Banci Buonamici, Gina Belmonte, Vincenzo Ciancia, Diego\n  Latella, Mieke Massink", "title": "Spatial Logics and Model Checking for Medical Imaging (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on spatial and spatio-temporal model checking provides novel\nimage analysis methodologies, rooted in logical methods for topological spaces.\nMedical Imaging (MI) is a field where such methods show potential for\nground-breaking innovation. Our starting point is SLCS, the Spatial Logic for\nClosure Spaces -- Closure Spaces being a generalisation of topological spaces,\ncovering also discrete space structures -- and topochecker, a model-checker for\nSLCS (and extensions thereof). We introduce the logical language ImgQL (\"Image\nQuery Language\"). ImgQL extends SLCS with logical operators describing distance\nand region similarity. The spatio-temporal model checker topochecker is\ncorrespondingly enhanced with state-of-the-art algorithms, borrowed from\ncomputational image processing, for efficient implementation of distancebased\noperators, namely distance transforms. Similarity between regions is defined by\nmeans of a statistical similarity operator, based on notions from statistical\ntexture analysis. We illustrate our approach by means of two examples of\nanalysis of Magnetic Resonance images: segmentation of glioblastoma and its\noedema, and segmentation of rectal carcinoma.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:02:31 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Buonamici", "Fabrizio Banci", ""], ["Belmonte", "Gina", ""], ["Ciancia", "Vincenzo", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "1811.06067", "submitter": "Sambuddha Ghosal", "authors": "Balaji Sesha Sarath Pokuri, Sambuddha Ghosal, Apurva Kokate, Baskar\n  Ganapathysubramanian and Soumik Sarkar", "title": "Interpretable deep learning for guided structure-property explorations\n  in photovoltaics", "comments": "Workshop on Machine Learning for Molecules and Materials (MLMM),\n  Neural Information Processing Systems (NeurIPS) 2018, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of an organic photovoltaic device is intricately connected to\nits active layer morphology. This connection between the active layer and\ndevice performance is very expensive to evaluate, either experimentally or\ncomputationally. Hence, designing morphologies to achieve higher performances\nis non-trivial and often intractable. To solve this, we first introduce a deep\nconvolutional neural network (CNN) architecture that can serve as a fast and\nrobust surrogate for the complex structure-property map. Several tests were\nperformed to gain trust in this trained model. Then, we utilize this fast\nframework to perform robust microstructural design to enhance device\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:08:08 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 19:59:56 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 02:14:14 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Pokuri", "Balaji Sesha Sarath", ""], ["Ghosal", "Sambuddha", ""], ["Kokate", "Apurva", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1811.06090", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "ReSIFT: Reliability-Weighted SIFT-based Image Quality Assessment", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": "D. Temel and G. AlRegib, \"ReSIFT: Reliability-weighted sift-based\n  image quality assessment,\" 2016 IEEE International Conference on Image\n  Processing (ICIP), Phoenix, AZ, 2016, pp. 2047-2051", "doi": "10.1109/ICIP.2016.7532718", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a full-reference image quality estimator based on SIFT\ndescriptor matching over reliability-weighted feature maps. Reliability\nassignment includes a smoothing operation, a transformation to perceptual color\ndomain, a local normalization stage, and a spectral residual computation with\nglobal normalization. The proposed method ReSIFT is tested on the LIVE and the\nLIVE Multiply Distorted databases and compared with 11 state-of-the-art\nfull-reference quality estimators. In terms of the Pearson and the Spearman\ncorrelation, ReSIFT is the best performing quality estimator in the overall\ndatabases. Moreover, ReSIFT is the best performing quality estimator in at\nleast one distortion group in compression, noise, and blur category.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:08:26 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.06106", "submitter": "Jonathan Rubin", "authors": "Jwala Dhamala, Emmanuel Azuh, Abdullah Al-Dujaili, Jonathan Rubin and\n  Una-May O'Reilly", "title": "Multivariate Time-series Similarity Assessment via Unsupervised\n  Representation Learning and Stratified Locality Sensitive Hashing:\n  Application to Early Acute Hypotensive Episode Detection", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/66", "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely prediction of clinically critical events in Intensive Care Unit (ICU)\nis important for improving care and survival rate. Most of the existing\napproaches are based on the application of various classification methods on\nexplicitly extracted statistical features from vital signals. In this work, we\npropose to eliminate the high cost of engineering hand-crafted features from\nmultivariate time-series of physiologic signals by learning their\nrepresentation with a sequence-to-sequence auto-encoder. We then propose to\nhash the learned representations to enable signal similarity assessment for the\nprediction of critical events. We apply this methodological framework to\npredict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of\nvital signal recordings. Experiments demonstrate the ability of the presented\nframework in accurately predicting an upcoming AHE.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:53:40 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 22:11:23 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 15:25:50 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Dhamala", "Jwala", ""], ["Azuh", "Emmanuel", ""], ["Al-Dujaili", "Abdullah", ""], ["Rubin", "Jonathan", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1811.06115", "submitter": "Fergal Cotter", "authors": "Fergal Cotter, Nick Kingsbury", "title": "Deep Learning in the Wavelet Domain", "comments": "4 pages + 1 reference. 2 figures 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the possibility of, and the possible advantages to\nlearning the filters of convolutional neural networks (CNNs) for image analysis\nin the wavelet domain. We are stimulated by both Mallat's scattering transform\nand the idea of filtering in the Fourier domain. It is important to explore new\nspaces in which to learn, as these may provide inherent advantages that are not\navailable in the pixel space. However, the scattering transform is limited by\nits inability to learn in between scattering orders, and any Fourier domain\nfiltering is limited by the large number of filter parameters needed to get\nlocalized filters. Instead we consider filtering in the wavelet domain with\nlearnable filters. The wavelet space allows us to have local, smooth filters\nwith far fewer parameters, and learnability can give us flexibility. We present\na novel layer which takes CNN activations into the wavelet space, learns\nparameters and returns to the pixel space. This allows it to be easily dropped\nin to any neural network without affecting the structure. As part of this work,\nwe show how to pass gradients through a multirate system and give preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 23:33:09 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Cotter", "Fergal", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1811.06152", "submitter": "Vincent Casser", "authors": "Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova", "title": "Depth Prediction Without the Sensors: Leveraging Structure for\n  Unsupervised Learning from Monocular Videos", "comments": "Thirty-Third AAAI Conference on Artificial Intelligence (AAAI'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict scene depth from RGB inputs is a challenging task both\nfor indoor and outdoor robot navigation. In this work we address unsupervised\nlearning of scene depth and robot ego-motion where supervision is provided by\nmonocular videos, as cameras are the cheapest, least restrictive and most\nubiquitous sensor for robotics.\n  Previous work in unsupervised image-to-depth learning has established strong\nbaselines in the domain. We propose a novel approach which produces higher\nquality results, is able to model moving objects and is shown to transfer\nacross data domains, e.g. from outdoors to indoor scenes. The main idea is to\nintroduce geometric structure in the learning process, by modeling the scene\nand the individual objects; camera ego-motion and object motions are learned\nfrom monocular videos as input. Furthermore an online refinement method is\nintroduced to adapt learning on the fly to unknown domains.\n  The proposed approach outperforms all state-of-the-art approaches, including\nthose that handle motion e.g. through learned flow. Our results are comparable\nin quality to the ones which used stereo as supervision and significantly\nimprove depth prediction on scenes and datasets which contain a lot of object\nmotion. The approach is of practical relevance, as it allows transfer across\nenvironments, by transferring models trained on data collected for robot\nnavigation in urban scenes to indoor navigation settings. The code associated\nwith this paper can be found at https://sites.google.com/view/struct2depth.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:59:38 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Casser", "Vincent", ""], ["Pirk", "Soeren", ""], ["Mahjourian", "Reza", ""], ["Angelova", "Anelia", ""]]}, {"id": "1811.06165", "submitter": "Mohamed Akrout", "authors": "Mohamed Akrout, Amir-massoud Farahmand, Tory Jarmain", "title": "Improving Skin Condition Classification with a Question Answering Model", "comments": null, "journal-ref": "Medical Imaging meets NeurIPS Workshop (2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a skin condition classification methodology based on a sequential\npipeline of a pre-trained Convolutional Neural Network (CNN) and a Question\nAnswering (QA) model. This method enables us to not only increase the\nclassification confidence and accuracy of the deployed CNN system, but also\nenables the emulation of the conventional approach of doctors asking the\nrelevant questions in refining the ultimate diagnosis and differential. By\ncombining the CNN output in the form of classification probabilities as a prior\nto the QA model and the image textual description, we greedily ask the best\nsymptom that maximizes the information gain over symptoms. We demonstrate that\ncombining the QA model with the CNN increases the accuracy up to 10% as\ncompared to the CNN alone, and more than 30% as compared to the QA model alone.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 04:27:21 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Akrout", "Mohamed", ""], ["Farahmand", "Amir-massoud", ""], ["Jarmain", "Tory", ""]]}, {"id": "1811.06186", "submitter": "Hanqing Chao", "authors": "Hanqing Chao, Yiwei He, Junping Zhang, Jianfeng Feng", "title": "GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition", "comments": "AAAI 2019, code is available at https://github.com/AbnerHqC/GaitSet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a unique biometric feature that can be recognized at a distance, gait has\nbroad applications in crime prevention, forensic identification and social\nsecurity. To portray a gait, existing gait recognition methods utilize either a\ngait template, where temporal information is hard to preserve, or a gait\nsequence, which must keep unnecessary sequential constraints and thus loses the\nflexibility of gait recognition. In this paper we present a novel perspective,\nwhere a gait is regarded as a set consisting of independent frames. We propose\na new network named GaitSet to learn identity information from the set. Based\non the set perspective, our method is immune to permutation of frames, and can\nnaturally integrate frames from different videos which have been filmed under\ndifferent scenarios, such as diverse viewing angles, different clothes/carrying\nconditions. Experiments show that under normal walking conditions, our\nsingle-model method achieves an average rank-1 accuracy of 95.0% on the CASIA-B\ngait dataset and an 87.1% accuracy on the OU-MVLP gait dataset. These results\nrepresent new state-of-the-art recognition accuracy. On various complex\nscenarios, our model exhibits a significant level of robustness. It achieves\naccuracies of 87.2% and 70.4% on CASIA-B under bag-carrying and coat-wearing\nwalking conditions, respectively. These outperform the existing best methods by\na large margin. The method presented can also achieve a satisfactory accuracy\nwith a small number of frames in a test sample, e.g., 82.5% on CASIA-B with\nonly 7 frames. The source code has been released at\nhttps://github.com/AbnerHqC/GaitSet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 05:23:14 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 07:23:17 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 13:21:47 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 06:07:17 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Chao", "Hanqing", ""], ["He", "Yiwei", ""], ["Zhang", "Junping", ""], ["Feng", "Jianfeng", ""]]}, {"id": "1811.06193", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa, James Reed, Kamran Kowsari, April Celeste\n  R.Leviton, Janet I. Warren, and Donald E. Brown", "title": "From Videos to URLs: A Multi-Browser Guide To Extract User's Behavior\n  with Optical Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking users' activities on the World Wide Web (WWW) allows researchers to\nanalyze each user's internet behavior as time passes and for the amount of time\nspent on a particular domain. This analysis can be used in research design, as\nresearchers may access to their participant's behaviors while browsing the web.\nWeb search behavior has been a subject of interest because of its real-world\napplications in marketing, digital advertisement, and identifying potential\nthreats online. In this paper, we present an image-processing based method to\nextract domains which are visited by a participant over multiple browsers\nduring a lab session. This method could provide another way to collect users'\nactivities during an online session given that the session recorder collected\nthe data. The method can also be used to collect the textual content of\nweb-pages that an individual visits for later analysis\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 05:59:05 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 00:24:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Reed", "James", ""], ["Kowsari", "Kamran", ""], ["Leviton", "April Celeste R.", ""], ["Warren", "Janet I.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1811.06194", "submitter": "Kaushal Bhogale", "authors": "Kaushal Bhogale, Nishant Shankar, Adheesh Juvekar, Asutosh Padhi", "title": "Face Verification and Forgery Detection for Ophthalmic Surgery Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although modern face verification systems are accessible and accurate, they\nare not always robust to pose variance and occlusions. Moreover, accurate\nmodels require a large amount of data to train. We structure our experiments to\noperate on small amounts of data obtained from an NGO that funds ophthalmic\nsurgeries. We set up our face verification task as that of verifying\npre-operation and post-operation images of a patient that undergoes ophthalmic\nsurgery, and as such the post-operation images have occlusions like an eye\npatch. In this paper, we present a system that performs the face verification\ntask using one-shot learning. To this end, our paper uses deep convolutional\nnetworks and compares different model architectures and loss functions. Our\nbest model achieves 85% test accuracy. During inference time, we also attempt\nto detect image forgeries in addition to performing face verification. To\nachieve this, we use Error Level Analysis. Finally, we propose an inference\npipeline that demonstrates how these techniques can be used to implement an\nautomated face verification and forgery detection system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 05:59:42 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Bhogale", "Kaushal", ""], ["Shankar", "Nishant", ""], ["Juvekar", "Adheesh", ""], ["Padhi", "Asutosh", ""]]}, {"id": "1811.06277", "submitter": "Michael Werman", "authors": "Shachar Honig and Michael Werman", "title": "Image declipping with deep networks", "comments": "5 pages", "journal-ref": "2018 25th IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep network to recover pixel values lost to clipping. The\nclipped area of the image is typically a uniform area of minimum or maximum\nbrightness, losing image detail and color fidelity. The degree to which the\nclipping is visually noticeable depends on the amount by which values were\nclipped, and the extent of the clipped area. Clipping may occur in any (or all)\nof the pixel's color channels. Although clipped pixels are common and occur to\nsome degree in almost every image we tested, current automatic solutions have\nonly partial success in repairing clipped pixels and work only in limited cases\nsuch as only with overexposure (not under-exposure) and when some of the color\nchannels are not clipped. Using neural networks and their ability to model\nnatural images allows our neural network, DeclipNet, to reconstruct data in\nclipped regions producing state of the art results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:26:09 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honig", "Shachar", ""], ["Werman", "Michael", ""]]}, {"id": "1811.06284", "submitter": "Guansong Lu", "authors": "Guansong Lu, Zhiming Zhou, Yuxuan Song, Kan Ren, Yong Yu", "title": "Guiding the One-to-one Mapping in CycleGAN via Optimal Transport", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CycleGAN is capable of learning a one-to-one mapping between two data\ndistributions without paired examples, achieving the task of unsupervised data\ntranslation. However, there is no theoretical guarantee on the property of the\nlearned one-to-one mapping in CycleGAN. In this paper, we experimentally find\nthat, under some circumstances, the one-to-one mapping learned by CycleGAN is\njust a random one within the large feasible solution space. Based on this\nobservation, we explore to add extra constraints such that the one-to-one\nmapping is controllable and satisfies more properties related to specific\ntasks. We propose to solve an optimal transport mapping restrained by a\ntask-specific cost function that reflects the desired properties, and use the\nbarycenters of optimal transport mapping to serve as references for CycleGAN.\nOur experiments indicate that the proposed algorithm is capable of learning a\none-to-one mapping with the desired properties.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:34:33 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Lu", "Guansong", ""], ["Zhou", "Zhiming", ""], ["Song", "Yuxuan", ""], ["Ren", "Kan", ""], ["Yu", "Yong", ""]]}, {"id": "1811.06287", "submitter": "Michael Werman", "authors": "Levi Offen and Michael Werman", "title": "Sketch based Reduced Memory Hough Transform", "comments": "5 pages", "journal-ref": "2018 25th IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes using sketch algorithms to represent the votes in Hough\ntransforms. Replacing the accumulator array with a sketch (Sketch Hough\nTransform - SHT) significantly reduces the memory needed to compute a Hough\ntransform. We also present a new sketch, Count Median Update, which works\nbetter than known sketch methods for replacing the accumulator array in the\nHough Transform.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:44:35 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Offen", "Levi", ""], ["Werman", "Michael", ""]]}, {"id": "1811.06295", "submitter": "Chen Du", "authors": "Chen Du, Chunheng Wang, Yanna Wang, Cunzhao Shi, Baihua Xiao", "title": "Selective Feature Connection Mechanism: Concatenating Multi-layer CNN\n  Features with a Feature Selector", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different layers of deep convolutional neural networks(CNNs) can encode\ndifferent-level information. High-layer features always contain more semantic\ninformation, and low-layer features contain more detail information. However,\nlow-layer features suffer from the background clutter and semantic ambiguity.\nDuring visual recognition, the feature combination of the low-layer and\nhigh-level features plays an important role in context modulation. If directly\ncombining the high-layer and low-layer features, the background clutter and\nsemantic ambiguity may be caused due to the introduction of detailed\ninformation. In this paper, we propose a general network architecture to\nconcatenate CNN features of different layers in a simple and effective way,\ncalled Selective Feature Connection Mechanism (SFCM). Low-level features are\nselectively linked to high-level features with a feature selector which is\ngenerated by high-level features. The proposed connection mechanism can\neffectively overcome the above-mentioned drawbacks. We demonstrate the\neffectiveness, superiority, and universal applicability of this method on\nmultiple challenging computer vision tasks, including image classification,\nscene text detection, and image-to-image translation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:58:21 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 05:53:51 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 07:44:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Du", "Chen", ""], ["Wang", "Chunheng", ""], ["Wang", "Yanna", ""], ["Shi", "Cunzhao", ""], ["Xiao", "Baihua", ""]]}, {"id": "1811.06308", "submitter": "David Berga", "authors": "David Berga, Xavier Otazu", "title": "A Neurodynamic model of Saliency prediction in V1", "comments": "17 pages, 17 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lateral connections in the primary visual cortex (V1) have long been\nhypothesized to be responsible of several visual processing mechanisms such as\nbrightness induction, chromatic induction, visual discomfort and bottom-up\nvisual attention (also named saliency). Many computational models have been\ndeveloped to independently predict these and other visual processes, but no\ncomputational model has been able to reproduce all of them simultaneously. In\nthis work we show that a biologically plausible computational model of lateral\ninteractions of V1 is able to simultaneously predict saliency and all the\naforementioned visual processes. Our model's (NSWAM) architecture is based on\nPennachio's neurodynamic model of lateral connections of V1. It is defined as a\nnetwork of firing rate neurons, sensitive to visual features such as\nbrightness, color, orientation and scale. We tested NSWAM saliency predictions\nusing images from several eye tracking datasets. We show that accuracy of\npredictions, using shuffled metrics, obtained by our architecture is similar to\nother state-of-the-art computational methods, particularly with synthetic\nimages (CAT2000-Pattern & SID4VAM) which mainly contain low level features.\nMoreover, we outperform other biologically-inspired saliency models that are\nspecifically designed to exclusively reproduce saliency. Hence, we show that\nour biologically plausible model of lateral connections can simultaneously\nexplain different visual proceses present in V1 (without applying any type of\ntraining or optimization and keeping the same parametrization for all the\nvisual processes). This can be useful for the definition of a unified\narchitecture of the primary visual cortex.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 12:11:24 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 11:34:37 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 17:57:33 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 16:21:19 GMT"}, {"version": "v5", "created": "Fri, 14 Dec 2018 18:04:06 GMT"}, {"version": "v6", "created": "Fri, 1 Feb 2019 17:45:10 GMT"}, {"version": "v7", "created": "Sun, 29 Sep 2019 22:19:01 GMT"}, {"version": "v8", "created": "Fri, 18 Sep 2020 20:36:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Berga", "David", ""], ["Otazu", "Xavier", ""]]}, {"id": "1811.06318", "submitter": "SeyedMajid Azimi", "authors": "Seyed Majid Azimi", "title": "ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV\n  Imagery", "comments": "Accepted in ECCV 2018, UAVision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  On-board real-time vehicle detection is of great significance for UAVs and\nother embedded mobile platforms. We propose a computationally inexpensive\ndetection network for vehicle detection in UAV imagery which we call\nShuffleDet. In order to enhance the speed-wise performance, we construct our\nmethod primarily using channel shuffling and grouped convolutions. We apply\ninception modules and deformable modules to consider the size and geometric\nshape of the vehicles. ShuffleDet is evaluated on CARPK and PUCPR+ datasets and\ncompared against the state-of-the-art real-time object detection networks.\nShuffleDet achieves 3.8 GFLOPs while it provides competitive performance on\ntest sets of both datasets. We show that our algorithm achieves real-time\nperformance by running at the speed of 14 frames per second on NVIDIA Jetson\nTX2 showing high potential for this method for real-time processing in UAVs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 12:42:03 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Azimi", "Seyed Majid", ""]]}, {"id": "1811.06347", "submitter": "ZhiYuan Li", "authors": "Zhiyuan Li, Min Jin, Qi Wu, Huaxiang Lu", "title": "Deep Template Matching for Offline Handwritten Chinese Character\n  Recognition", "comments": "5 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like its remarkable achievements in many computer vision tasks, the\nconvolutional neural networks (CNN) provide an end-to-end solution in\nhandwritten Chinese character recognition (HCCR) with great success. However,\nthe process of learning discriminative features for image recognition is\ndifficult in cases where little data is available. In this paper, we propose a\nnovel method for learning siamese neural network which employ a special\nstructure to predict the similarity between handwritten Chinese characters and\ntemplate images. The optimization of siamese neural network can be treated as a\nsimple binary classification problem. When the training process has been\nfinished, the powerful discriminative features help us to generalize the\npredictive power not just to new data, but to entirely new classes that never\nappear in the training set. Experiments performed on the ICDAR-2013 offline\nHCCR datasets have shown that the proposed method has a very promising\ngeneralization ability to the new classes that never appear in the training\nset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:02:22 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Li", "Zhiyuan", ""], ["Jin", "Min", ""], ["Wu", "Qi", ""], ["Lu", "Huaxiang", ""]]}, {"id": "1811.06378", "submitter": "Mikhail Aliev", "authors": "M. Aliev, E.I. Ershov, D.P. Nikolaev", "title": "On the use of FHT, its modification for practical applications and the\n  structure of Hough image", "comments": "8 pages, 8 figures. Submitted and presented at ICMV 2018", "journal-ref": null, "doi": "10.1117/12.2522803", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the Fast Hough Transform (FHT) algorithm proposed by\nM.L. Brady. We propose how to modify the standard FHT to calculate sums along\nlines within any given range of their inclination angles. We also describe a\nnew way to visualise Hough-image based on regrouping of accumulator space\naround its center. Finally, we prove that using Brady parameterization\ntransforms any line into a figure of type \"angle\".\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:49:35 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Aliev", "M.", ""], ["Ershov", "E. I.", ""], ["Nikolaev", "D. P.", ""]]}, {"id": "1811.06405", "submitter": "Bong-Nam Kang", "authors": "Bong-Nam Kang, Yonghyun Kim, Daijin Kim", "title": "Pairwise Relational Networks using Local Appearance Features for Face\n  Recognition", "comments": "To be appear in R2L workshop at NIPS 2018. arXiv admin note:\n  substantial text overlap with arXiv:1808.04976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new face recognition method, called a pairwise relational\nnetwork (PRN), which takes local appearance features around landmark points on\nthe feature map, and captures unique pairwise relations with the same identity\nand discriminative pairwise relations between different identities. The PRN\naims to determine facial part-relational structure from local appearance\nfeature pairs. Because meaningful pairwise relations should be identity\ndependent, we add a face identity state feature, which obtains from the long\nshort-term memory (LSTM) units network with the sequential local appearance\nfeatures. To further improve accuracy, we combined the global appearance\nfeatures with the pairwise relational feature. Experimental results on the LFW\nshow that the PRN achieved 99.76% accuracy. On the YTF, PRN achieved the\nstate-of-the-art accuracy (96.3%). The PRN also achieved comparable results to\nthe state-of-the-art for both face verification and face identification tasks\non the IJB-A and IJB-B. This work is already published on ECCV 2018.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:43:04 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kang", "Bong-Nam", ""], ["Kim", "Yonghyun", ""], ["Kim", "Daijin", ""]]}, {"id": "1811.06410", "submitter": "Sanghyun Woo", "authors": "Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon", "title": "LinkNet: Relational Embedding for Scene Graph", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects and their relationships are critical contents for image\nunderstanding. A scene graph provides a structured description that captures\nthese properties of an image. However, reasoning about the relationships\nbetween objects is very challenging and only a few recent works have attempted\nto solve the problem of generating a scene graph from an image. In this paper,\nwe present a method that improves scene graph generation by explicitly modeling\ninter-dependency among the entire object instances. We design a simple and\neffective relational embedding module that enables our model to jointly\nrepresent connections among all related objects, rather than focus on an object\nin isolation. Our method significantly benefits the main part of the scene\ngraph generation task: relationship classification. Using it on top of a basic\nFaster R-CNN, our model achieves state-of-the-art results on the Visual Genome\nbenchmark. We further push the performance by introducing global context\nencoding module and geometrical layout encoding module. We validate our final\nmodel, LinkNet, through extensive ablation studies, demonstrating its efficacy\nin scene graph generation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:54:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Kweon", "In So", ""]]}, {"id": "1811.06446", "submitter": "Cuixian Chen", "authors": "Benjamin Yip, Garrett Bingham, Katherine Kempfert, Jonathan Fabish,\n  Troy Kling, Cuixian Chen, Yishi Wang", "title": "Preliminary Studies on a Large Face Database", "comments": "It has been accepted in the 5th National Symposium for NSF REU\n  Research in Data Science, Systems, and Security. G. Bingham and K. Kempfert\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We perform preliminary studies on a large longitudinal face database\nMORPH-II, which is a benchmark dataset in the field of computer vision and\npattern recognition. First, we summarize the inconsistencies in the dataset and\nintroduce the steps and strategy taken for cleaning. The potential implications\nof these inconsistencies on prior research are introduced. Next, we propose a\nnew automatic subsetting scheme for evaluation protocol. It is intended to\novercome the unbalanced racial and gender distributions of MORPH-II, while\nensuring independence between training and testing sets. Finally, we contribute\na novel global framework for age estimation that utilizes posterior\nprobabilities from the race classification step to compute a racecomposite age\nestimate. Preliminary experimental results on MORPH-II are presented.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 16:07:49 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Yip", "Benjamin", ""], ["Bingham", "Garrett", ""], ["Kempfert", "Katherine", ""], ["Fabish", "Jonathan", ""], ["Kling", "Troy", ""], ["Chen", "Cuixian", ""], ["Wang", "Yishi", ""]]}, {"id": "1811.06458", "submitter": "David Berga", "authors": "David Berga, Xos\\'e Ram\\'on Fdez-Vidal, Xavier Otazu, V\\'ictor\n  Lebor\\'an, Xos\\'e M. Pardo", "title": "Psychophysical evaluation of individual low-level feature influences on\n  visual attention", "comments": "29 pages, 24 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.visres.2018.10.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we provide the analysis of eye movement behavior elicited by\nlow-level feature distinctiveness with a dataset of synthetically-generated\nimage patterns. Design of visual stimuli was inspired by the ones used in\nprevious psychophysical experiments, namely in free-viewing and visual\nsearching tasks, to provide a total of 15 types of stimuli, divided according\nto the task and feature to be analyzed. Our interest is to analyze the\ninfluences of low-level feature contrast between a salient region and the rest\nof distractors, providing fixation localization characteristics and reaction\ntime of landing inside the salient region. Eye-tracking data was collected from\n34 participants during the viewing of a 230 images dataset. Results show that\nsaliency is predominantly and distinctively influenced by: 1. feature type, 2.\nfeature contrast, 3. temporality of fixations, 4. task difficulty and 5. center\nbias. This experimentation proposes a new psychophysical basis for saliency\nmodel evaluation using synthetic images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 16:37:31 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Berga", "David", ""], ["Fdez-Vidal", "Xos\u00e9 Ram\u00f3n", ""], ["Otazu", "Xavier", ""], ["Lebor\u00e1n", "V\u00edctor", ""], ["Pardo", "Xos\u00e9 M.", ""]]}, {"id": "1811.06488", "submitter": "Ezra Webb", "authors": "Ezra Webb, Cheng Lei, Chun-Jung Huang, Hirofumi Kobayashi, Hideharu\n  Mikami, Keisuke Goda", "title": "Exploring the Deep Feature Space of a Cell Classification Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present contemporary techniques for visualising the feature\nspace of a deep learning image classification neural network. These techniques\nare viewed in the context of a feed-forward network trained to classify low\nresolution fluorescence images of white blood cells captured using optofluidic\nimaging. The model has two output classes corresponding to two different cell\ntypes, which are often difficult to distinguish by eye. This paper has two\nmajor sections. The first looks to develop the information space presented by\ndimension reduction techniques, such as t-SNE, used to embed high-dimensional\npre-softmax layer activations into a two-dimensional plane. The second section\nlooks at feature visualisation by optimisation to generate feature images\nrepresenting the learned features of the network. Using and developing these\ntechniques we visualise class separation and structures within the dataset at\nvarious depths using clustering algorithms and feature images; track the\ndevelopment of feature complexity as we ascend the network; and begin to\nextract the features the network has learnt by modulating single-channel\nfeature images with up-scaled neuron activation maps to distinguish their most\nsalient parts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:26:17 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Webb", "Ezra", ""], ["Lei", "Cheng", ""], ["Huang", "Chun-Jung", ""], ["Kobayashi", "Hirofumi", ""], ["Mikami", "Hideharu", ""], ["Goda", "Keisuke", ""]]}, {"id": "1811.06497", "submitter": "Kunal Nagpal", "authors": "Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan (Cameron) Chen, Ellery\n  Wulczyn, Fraser Tan, Niels Olson, Jenny L. Smith, Arash Mohtashamian, James\n  H. Wren, Greg S. Corrado, Robert MacDonald, Lily H. Peng, Mahul B. Amin,\n  Andrew J. Evans, Ankur R. Sangoi, Craig H. Mermel, Jason D. Hipp, Martin C.\n  Stumpe", "title": "Development and Validation of a Deep Learning Algorithm for Improving\n  Gleason Scoring of Prostate Cancer", "comments": null, "journal-ref": "Nature Partner Journal Digital Medicine (2019)", "doi": "10.1038/s41746-019-0112-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For prostate cancer patients, the Gleason score is one of the most important\nprognostic factors, potentially determining treatment independent of the stage.\nHowever, Gleason scoring is based on subjective microscopic examination of\ntumor morphology and suffers from poor reproducibility. Here we present a deep\nlearning system (DLS) for Gleason scoring whole-slide images of\nprostatectomies. Our system was developed using 112 million\npathologist-annotated image patches from 1,226 slides, and evaluated on an\nindependent validation dataset of 331 slides, where the reference standard was\nestablished by genitourinary specialist pathologists. On the validation\ndataset, the mean accuracy among 29 general pathologists was 0.61. The DLS\nachieved a significantly higher diagnostic accuracy of 0.70 (p=0.002) and\ntrended towards better patient risk stratification in correlations to clinical\nfollow-up data. Our approach could improve the accuracy of Gleason scoring and\nsubsequent therapy decisions, particularly where specialist expertise is\nunavailable. The DLS also goes beyond the current Gleason system to more finely\ncharacterize and quantitate tumor morphology, providing opportunities for\nrefinement of the Gleason system itself.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:49:50 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Nagpal", "Kunal", "", "Cameron"], ["Foote", "Davis", "", "Cameron"], ["Liu", "Yun", "", "Cameron"], ["Po-Hsuan", "", "", "Cameron"], ["Chen", "", ""], ["Wulczyn", "Ellery", ""], ["Tan", "Fraser", ""], ["Olson", "Niels", ""], ["Smith", "Jenny L.", ""], ["Mohtashamian", "Arash", ""], ["Wren", "James H.", ""], ["Corrado", "Greg S.", ""], ["MacDonald", "Robert", ""], ["Peng", "Lily H.", ""], ["Amin", "Mahul B.", ""], ["Evans", "Andrew J.", ""], ["Sangoi", "Ankur R.", ""], ["Mermel", "Craig H.", ""], ["Hipp", "Jason D.", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1811.06498", "submitter": "Michael Ferlaino", "authors": "Craig A. Glastonbury, Michael Ferlaino, Christoffer Nell\\r{a}ker,\n  Cecilia M. Lindgren", "title": "Adjusting for Confounding in Unsupervised Latent Representations of\n  Images", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological imaging data are often partially confounded or contain unwanted\nvariability. Examples of such phenomena include variable lighting across\nmicroscopy image captures, stain intensity variation in histological slides,\nand batch effects for high throughput drug screening assays. Therefore, to\ndevelop \"fair\" models which generalise well to unseen examples, it is crucial\nto learn data representations that are insensitive to nuisance factors of\nvariation. In this paper, we present a strategy based on adversarial training,\ncapable of learning unsupervised representations invariant to confounders. As\nan empirical validation of our method, we use deep convolutional autoencoders\nto learn unbiased cellular representations from microscopy imaging.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:53:21 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 10:35:32 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Glastonbury", "Craig A.", ""], ["Ferlaino", "Michael", ""], ["Nell\u00e5ker", "Christoffer", ""], ["Lindgren", "Cecilia M.", ""]]}, {"id": "1811.06529", "submitter": "Tomasz Kornuta", "authors": "Vincent Marois and T.S. Jayram and Vincent Albouy and Tomasz Kornuta\n  and Younes Bouhadjar and Ahmet S. Ozcan", "title": "On transfer learning using a MAC model variant", "comments": "Paper accepted for Visually Grounded Interaction and Language (ViGIL)\n  Workshop, NIPS 2018, Montreeal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\na simplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that, transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy as well.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:52:06 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 23:37:30 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Marois", "Vincent", ""], ["Jayram", "T. S.", ""], ["Albouy", "Vincent", ""], ["Kornuta", "Tomasz", ""], ["Bouhadjar", "Younes", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1811.06582", "submitter": "Neeti Narayan", "authors": "Neeti Narayan, Nishant Sankaran, Srirangaraj Setlur, Venu Govindaraju", "title": "CAN: Composite Appearance Network for Person Tracking and How to Model\n  Errors in a Tracking System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking multiple people across multiple cameras is an open problem. It is\ntypically divided into two tasks: (i) single-camera tracking (SCT) - identify\ntrajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify\ntrajectories across cameras for real surveillance scenes. Many methods cater to\nSCT, while ICT still remains a challenge. In this paper, we propose a tracking\nmethod which uses motion cues and a feature aggregation network for\ntemplate-based person re-identification by incorporating metadata such as\nperson bounding box and camera information. We present a feature aggregation\narchitecture called Composite Appearance Network (CAN) to address the above\nproblem. The key structure of this architecture is called EvalNet that pays\nattention to each feature vector and learns to weight them based on gradients\nit receives for the overall template for optimal re-identification performance.\nWe demonstrate the efficiency of our approach with experiments on the\nchallenging multi-camera tracking dataset, DukeMTMC. We also survey existing\ntracking measures and present an online error metric called \"Inference Error\"\n(IE) that provides a better estimate of tracking/re-identification error, by\ntreating SCT and ICT errors uniformly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 20:23:46 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 00:23:04 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 23:20:00 GMT"}, {"version": "v4", "created": "Thu, 3 Oct 2019 22:48:18 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Narayan", "Neeti", ""], ["Sankaran", "Nishant", ""], ["Setlur", "Srirangaraj", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1811.06604", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov", "title": "Conditional GANs for Multi-Illuminant Color Constancy: Revolution or Yet\n  Another Approach?", "comments": "Accepted to CVPR 2019 Workshop (NTIRE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-uniform and multi-illuminant color constancy are important tasks, the\nsolution of which will allow to discard information about lighting conditions\nin the image. Non-uniform illumination and shadows distort colors of real-world\nobjects and mostly do not contain valuable information. Thus, many computer\nvision and image processing techniques would benefit from automatic discarding\nof this information at the pre-processing step. In this work we propose novel\nview on this classical problem via generative end-to-end algorithm based on\nimage conditioned Generative Adversarial Network. We also demonstrate the\npotential of the given approach for joint shadow detection and removal. Forced\nby the lack of training data, we render the largest existing shadow removal\ndataset and make it publicly available. It consists of approximately 6,000\npairs of wide field of view synthetic images with and without shadows.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:58:16 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 10:32:35 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sidorov", "Oleksii", ""]]}, {"id": "1811.06641", "submitter": "Yazhou Liu", "authors": "Sen Cao and Yazhou Liu and Pongsak Lasang and Shengmei Shen", "title": "Detecting The Objects on The Road Using Modular Lightweight Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a modular lightweight network model for road objects\ndetection, such as car, pedestrian and cyclist, especially when they are far\naway from the camera and their sizes are small. Great advances have been made\nfor the deep networks, but small objects detection is still a challenging task.\nIn order to solve this problem, majority of existing methods utilize\ncomplicated network or bigger image size, which generally leads to higher\ncomputation cost. The proposed network model is referred to as modular feature\nfusion detector (MFFD), using a fast and efficient network architecture for\ndetecting small objects. The contribution lies in the following aspects: 1) Two\nbase modules have been designed for efficient computation: Front module reduce\nthe information loss from raw input images; Tinier module decrease model size\nand computation cost, while ensuring the detection accuracy. 2) By stacking the\nbase modules, we design a context features fusion framework for multi-scale\nobject detection. 3) The propose method is efficient in terms of model size and\ncomputation cost, which is applicable for resource limited devices, such as\nembedded systems for advanced driver assistance systems (ADAS). Comparisons\nwith the state-of-the-arts on the challenging KITTI dataset reveal the\nsuperiority of the proposed method. Especially, 100 fps can be achieved on the\nembedded GPUs such as Jetson TX2.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 01:14:59 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Cao", "Sen", ""], ["Liu", "Yazhou", ""], ["Lasang", "Pongsak", ""], ["Shen", "Shengmei", ""]]}, {"id": "1811.06660", "submitter": "Kostas Alexis PhD", "authors": "Sotirios Diamantas, Kostas Alexis", "title": "Optical Flow Based Background Subtraction with a Moving Camera:\n  Application to Autonomous Driving", "comments": "5 pages, 4 figures, presubmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research we present a novel algorithm for background subtraction\nusing a moving camera. Our algorithm is based purely on visual information\nobtained from a camera mounted on an electric bus, operating in downtown Reno\nwhich automatically detects moving objects of interest with the view to provide\na fully autonomous vehicle. In our approach we exploit the optical flow vectors\ngenerated by the motion of the camera while keeping parameter assumptions a\nminimum. At first, we estimate the Focus of Expansion, which is used to model\nand simulate 3D points given the intrinsic parameters of the camera, and\nperform multiple linear regression to estimate the regression equation\nparameters and implement on the real data set of every frame to identify moving\nobjects. We validated our algorithm using data taken from a common bus route.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 02:36:41 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Diamantas", "Sotirios", ""], ["Alexis", "Kostas", ""]]}, {"id": "1811.06666", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh and Mohan M. Trivedi", "title": "Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an approach to produce accurate 3D detection boxes for\nobjects on the ground using single monocular images. We do so by merging 2D\nvisual cues, 3D object dimensions, and ground plane constraints to produce\nboxes that are robust against small errors and incorrect predictions. First, we\ntrain a single-shot convolutional neural network (CNN) that produces multiple\nvisual and geometric cues of interest: 2D bounding boxes, 2D keypoints of\ninterest, coarse object orientations and object dimensions. Subsets of these\ncues are then used to poll probable ground planes from a pre-computed database\nof ground planes, to identify the \"best fit\" plane with highest consensus. Once\nidentified, the \"best fit\" plane provides enough constraints to successfully\nconstruct the desired 3D detection box, without directly predicting the 6DoF\npose of the object. The entire ground plane polling (GPP) procedure is\nconstructed as a non-parametrized layer of the CNN that outputs the desired\n\"best fit\" plane and the corresponding 3D keypoints, which together define the\nfinal 3D bounding box. Doing so allows us to poll thousands of different ground\nplane configurations without adding considerable overhead, while also creating\na single CNN that directly produces the desired output without the need for\npost processing. We evaluate our method on the 2D detection and orientation\nestimation benchmark from the challenging KITTI dataset, and provide additional\ncomparisons for 3D metrics of importance. This single-stage, single-pass CNN\nresults in superior localization and orientation estimation compared to more\ncomplex and computationally expensive monocular approaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:23:12 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 20:20:12 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 17:56:28 GMT"}, {"version": "v4", "created": "Fri, 7 Feb 2020 23:53:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1811.06668", "submitter": "You Qiaoben", "authors": "You Qiaoben, Zheng Wang, Jianguo Li, Yinpeng Dong, Yu-Gang Jiang, Jun\n  Zhu", "title": "Composite Binary Decomposition Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks have great resource and computing efficiency, while\nsuffer from long training procedure and non-negligible accuracy drops, when\ncomparing to the full-precision counterparts. In this paper, we propose the\ncomposite binary decomposition networks (CBDNet), which first compose\nreal-valued tensor of each layer with a limited number of binary tensors, and\nthen decompose some conditioned binary tensors into two low-rank binary\ntensors, so that the number of parameters and operations are greatly reduced\ncomparing to the original ones. Experiments demonstrate the effectiveness of\nthe proposed method, as CBDNet can approximate image classification network\nResNet-18 using 5.25 bits, VGG-16 using 5.47 bits, DenseNet-121 using 5.72\nbits, object detection networks SSD300 using 4.38 bits, and semantic\nsegmentation networks SegNet using 5.18 bits, all with minor accuracy drops.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:29:34 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Qiaoben", "You", ""], ["Wang", "Zheng", ""], ["Li", "Jianguo", ""], ["Dong", "Yinpeng", ""], ["Jiang", "Yu-Gang", ""], ["Zhu", "Jun", ""]]}, {"id": "1811.06679", "submitter": "Runmin Cong", "authors": "Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, and\n  Nam Ling", "title": "HSCS: Hierarchical Sparsity Based Co-saliency Detection for RGBD Images", "comments": "11 pages, 5 figures, Accepted by IEEE Transactions on Multimedia,\n  https://rmcong.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-saliency detection aims to discover common and salient objects in an image\ngroup containing more than two relevant images. Moreover, depth information has\nbeen demonstrated to be effective for many computer vision tasks. In this\npaper, we propose a novel co-saliency detection method for RGBD images based on\nhierarchical sparsity reconstruction and energy function refinement. With the\nassistance of the intra saliency map, the inter-image correspondence is\nformulated as a hierarchical sparsity reconstruction framework. The global\nsparsity reconstruction model with a ranking scheme focuses on capturing the\nglobal characteristics among the whole image group through a common foreground\ndictionary. The pairwise sparsity reconstruction model aims to explore the\ncorresponding relationship between pairwise images through a set of pairwise\ndictionaries. In order to improve the intra-image smoothness and inter-image\nconsistency, an energy function refinement model is proposed, which includes\nthe unary data term, spatial smooth term, and holistic consistency term.\nExperiments on two RGBD co-saliency detection benchmarks demonstrate that the\nproposed method outperforms the state-of-the-art algorithms both qualitatively\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 05:19:24 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Cong", "Runmin", ""], ["Lei", "Jianjun", ""], ["Fu", "Huazhu", ""], ["Huang", "Qingming", ""], ["Cao", "Xiaochun", ""], ["Ling", "Nam", ""]]}, {"id": "1811.06700", "submitter": "Lianwen Jin", "authors": "Lele Xie, Yuliang Liu, Lianwen Jin, Zecheng Xie", "title": "DeRPN: Taking a further step toward more general object detection", "comments": "8pages, 4 figures, 6 tables, accepted to appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current detection methods have adopted anchor boxes as regression\nreferences. However, the detection performance is sensitive to the setting of\nthe anchor boxes. A proper setting of anchor boxes may vary significantly\nacross different datasets, which severely limits the universality of the\ndetectors. To improve the adaptivity of the detectors, in this paper, we\npresent a novel dimension-decomposition region proposal network (DeRPN) that\ncan perfectly displace the traditional Region Proposal Network (RPN). DeRPN\nutilizes an anchor string mechanism to independently match object widths and\nheights, which is conducive to treating variant object shapes. In addition, a\nnovel scale-sensitive loss is designed to address the imbalanced loss\ncomputations of different scaled objects, which can avoid the small objects\nbeing overwhelmed by larger ones. Comprehensive experiments conducted on both\ngeneral object detection datasets (Pascal VOC 2007, 2012 and MS COCO) and scene\ntext detection datasets (ICDAR 2013 and COCO-Text) all prove that our DeRPN can\nsignificantly outperform RPN. It is worth mentioning that the proposed DeRPN\ncan be employed directly on different models, tasks, and datasets without any\nmodifications of hyperparameters or specialized optimization, which further\ndemonstrates its adaptivity. The code will be released at\nhttps://github.com/HCIILAB/DeRPN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 08:25:52 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Xie", "Lele", ""], ["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""], ["Xie", "Zecheng", ""]]}, {"id": "1811.06783", "submitter": "Hengyue Pan", "authors": "Hengyue Pan, Hui Jiang, Xin Niu and Yong Dou", "title": "DropFilter: A Novel Regularization Method for Learning Convolutional\n  Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed the fast development of different\nregularization methods for deep learning models such as fully-connected deep\nneural networks (DNNs) and Convolutional Neural Networks (CNNs). Most of\nprevious methods mainly consider to drop features from input data and hidden\nlayers, such as Dropout, Cutout and DropBlocks. DropConnect select to drop\nconnections between fully-connected layers. By randomly discard some features\nor connections, the above mentioned methods control the overfitting problem and\nimprove the performance of neural networks. In this paper, we proposed two\nnovel regularization methods, namely DropFilter and DropFilter-PLUS, for the\nlearning of CNNs. Different from the previous methods, DropFilter and\nDropFilter-PLUS selects to modify the convolution filters. For DropFilter-PLUS,\nwe find a suitable way to accelerate the learning process based on theoretical\nanalysis. Experimental results on MNIST show that using DropFilter and\nDropFilter-PLUS may improve performance on image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:40:39 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:28:42 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pan", "Hengyue", ""], ["Jiang", "Hui", ""], ["Niu", "Xin", ""], ["Dou", "Yong", ""]]}, {"id": "1811.06817", "submitter": "Rhiannon Michelmore", "authors": "Rhiannon Michelmore, Marta Kwiatkowska, Yarin Gal", "title": "Evaluating Uncertainty Quantification in End-to-End Autonomous Driving\n  Control", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rise in popularity of Deep Neural Networks (DNNs), attributed to more\npowerful GPUs and widely available datasets, has seen them being increasingly\nused within safety-critical domains. One such domain, self-driving, has\nbenefited from significant performance improvements, with millions of miles\nhaving been driven with no human intervention. Despite this, crashes and\nerroneous behaviours still occur, in part due to the complexity of verifying\nthe correctness of DNNs and a lack of safety guarantees.\n  In this paper, we demonstrate how quantitative measures of uncertainty can be\nextracted in real-time, and their quality evaluated in end-to-end controllers\nfor self-driving cars. To this end we utilise a recent method for gathering\napproximate uncertainty information from DNNs without changing the network's\narchitecture. We propose evaluation techniques for the uncertainty on two\nseparate architectures which use the uncertainty to predict crashes up to five\nseconds in advance. We find that mutual information, a measure of uncertainty\nin classification networks, is a promising indicator of forthcoming crashes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 14:30:30 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Michelmore", "Rhiannon", ""], ["Kwiatkowska", "Marta", ""], ["Gal", "Yarin", ""]]}, {"id": "1811.06846", "submitter": "Gabriel Dahia", "authors": "Gabriel Dahia, Maur\\'icio Pamplona Segundo", "title": "Improving Fingerprint Pore Detection with a Small FCN", "comments": "arXiv admin note: text overlap with arXiv:1809.10229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate if previously proposed CNNs for fingerprint pore\ndetection overestimate the number of required model parameters for this task.\nWe show that this is indeed the case by proposing a fully convolutional neural\nnetwork that has significantly fewer parameters. We evaluate this model using a\nrigorous and reproducible protocol, which was, prior to our work, not available\nto the community. Using our protocol, we show that the proposed model, when\ncombined with post-processing, performs better than previous methods, albeit\nbeing much more efficient. All our code is available at\nhttps://github.com/gdahia/fingerprint-pore-detection\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:29:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Dahia", "Gabriel", ""], ["Segundo", "Maur\u00edcio Pamplona", ""]]}, {"id": "1811.06861", "submitter": "Matthias Haselmann Dipl.-Ing.", "authors": "Matthias Haselmann, Dieter P. Gruber, Paul Tabatabai", "title": "Anomaly Detection using Deep Learning based Image Completion", "comments": "6 pages, 5 figures, Accepted for publication by IEEE, 17th\n  International Conference on Machine Learning and Applications (ICMLA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surface inspection is an important task in many manufacturing\nindustries and often requires machine learning driven solutions. Supervised\napproaches, however, can be challenging, since it is often difficult to obtain\nlarge amounts of labeled training data. In this work, we instead perform\none-class unsupervised learning on fault-free samples by training a deep\nconvolutional neural network to complete images whose center regions are cut\nout. Since the network is trained exclusively on fault-free data, it completes\nthe image patches with a fault-free version of the missing image region. The\npixel-wise reconstruction error within the cut out region is an anomaly image\nwhich can be used for anomaly detection. Results on surface images of decorated\nplastic parts demonstrate that this approach is suitable for detection of\nvisible anomalies and moreover surpasses all other tested methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:36:28 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Haselmann", "Matthias", ""], ["Gruber", "Dieter P.", ""], ["Tabatabai", "Paul", ""]]}, {"id": "1811.06868", "submitter": "Hanxiao Wang", "authors": "Hanxiao Wang, Venkatesh Saligrama, Stan Sclaroff, Vitaly Ablavsky", "title": "Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential\n  Fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fine-grained classification on an edge camera\ndevice that has limited power. The edge device must sparingly interact with the\ncloud to minimize communication bits to conserve power, and the cloud upon\nreceiving the edge inputs returns a classification label. To deal with\nfine-grained classification, we adopt the perspective of sequential fixation\nwith a foveated field-of-view to model cloud-edge interactions. We propose a\nnovel deep reinforcement learning-based foveation model, DRIFT, that\nsequentially generates and recognizes mixed-acuity images.Training of DRIFT\nrequires only image-level category labels and encourages fixations to contain\ntask-relevant information, while maintaining data efficiency. Specifically,\nwetrain a foveation actor network with a novel Deep Deterministic Policy\nGradient by Conditioned Critic and Coaching (DDPGC3) algorithm. In addition, we\npropose to shape the reward to provide informative feedback after each fixation\nto better guide RL training. We demonstrate the effectiveness of DRIFT on this\ntask by evaluating on five fine-grained classification benchmark datasets, and\nshow that the proposed approach achieves state-of-the-art performance with over\n3X reduction in transmitted pixels.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:41:24 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:52:54 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wang", "Hanxiao", ""], ["Saligrama", "Venkatesh", ""], ["Sclaroff", "Stan", ""], ["Ablavsky", "Vitaly", ""]]}, {"id": "1811.06878", "submitter": "HyoungHo Jung", "authors": "Jung HyoungHo, Lee Ryong, Lee Sanghwan, Hwang Wonjun", "title": "Residual Convolutional Neural Network Revisited with Active Weighted\n  Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual recognition, the key to the performance improvement of ResNet is\nthe success in establishing the stack of deep sequential convolutional layers\nusing identical mapping by a shortcut connection. It results in multiple paths\nof data flow under a network and the paths are merged with the equal weights.\nHowever, it is questionable whether it is correct to use the fixed and\npredefined weights at the mapping units of all paths. In this paper, we\nintroduce the active weighted mapping method which infers proper weight values\nbased on the characteristic of input data on the fly. The weight values of each\nmapping unit are not fixed but changed as the input image is changed, and the\nmost proper weight values for each mapping unit are derived according to the\ninput image. For this purpose, channel-wise information is embedded from both\nthe shortcut connection and convolutional block, and then the fully connected\nlayers are used to estimate the weight values for the mapping units. We train\nthe backbone network and the proposed module alternately for a more stable\nlearning of the proposed method. Results of the extensive experiments show that\nthe proposed method works successfully on the various backbone architectures\nfrom ResNet to DenseNet. We also verify the superiority and generality of the\nproposed method on various datasets in comparison with the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:51:20 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["HyoungHo", "Jung", ""], ["Ryong", "Lee", ""], ["Sanghwan", "Lee", ""], ["Wonjun", "Hwang", ""]]}, {"id": "1811.06879", "submitter": "Zan Gojcic", "authors": "Zan Gojcic, Caifa Zhou, Jan D. Wegner, Andreas Wieser", "title": "The Perfect Match: 3D Point Cloud Matching with Smoothed Densities", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose 3DSmoothNet, a full workflow to match 3D point clouds with a\nsiamese deep learning architecture and fully convolutional layers using a\nvoxelized smoothed density value (SDV) representation. The latter is computed\nper interest point and aligned to the local reference frame (LRF) to achieve\nrotation invariance. Our compact, learned, rotation invariant 3D point cloud\ndescriptor achieves 94.9% average recall on the 3DMatch benchmark data set,\noutperforming the state-of-the-art by more than 20 percent points with only 32\noutput dimensions. This very low output dimension allows for near realtime\ncorrespondence search with 0.1 ms per feature point on a standard PC. Our\napproach is sensor- and sceneagnostic because of SDV, LRF and learning highly\ndescriptive features with fully convolutional layers. We show that 3DSmoothNet\ntrained only on RGB-D indoor scenes of buildings achieves 79.0% average recall\non laser scans of outdoor vegetation, more than double the performance of our\nclosest, learning-based competitors. Code, data and pre-trained models are\navailable online at https://github.com/zgojcic/3DSmoothNet.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:53:02 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 12:52:25 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 12:08:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gojcic", "Zan", ""], ["Zhou", "Caifa", ""], ["Wegner", "Jan D.", ""], ["Wieser", "Andreas", ""]]}, {"id": "1811.06934", "submitter": "Cuixian Chen", "authors": "Benjamin Yip, Rachel Towner, Troy Kling, Cuixian Chen, and Yishi Wang", "title": "Image Pre-processing Using OpenCV Library on MORPH-II Face Database", "comments": "Project for NSF-REU site at UNCW for Summer 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper outlines the steps taken toward pre-processing the 55,134 images\nof the MORPH-II non-commercial dataset. Following the introduction, section two\nbegins with an overview of each step in the pre-processing pipeline. Section\nthree expands upon each stage of the process and includes details on all\ncalculations made, by providing the OpenCV functionality paired with each step.\nThe last portion of this paper discusses the potential improvements to this\npre-processing pipeline that became apparent in retrospect.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:36:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Yip", "Benjamin", ""], ["Towner", "Rachel", ""], ["Kling", "Troy", ""], ["Chen", "Cuixian", ""], ["Wang", "Yishi", ""]]}, {"id": "1811.06937", "submitter": "Yong Man Ro", "authors": "Wissam J. Baddar, Yong Man Ro", "title": "Mode Variational LSTM Robust to Unseen Modes of Variation: Application\n  to Facial Expression Recognition", "comments": "Accepted in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal feature encoding is essential for encoding the dynamics in\nvideo sequences. Recurrent neural networks, particularly long short-term memory\n(LSTM) units, have been popular as an efficient tool for encoding\nspatio-temporal features in sequences. In this work, we investigate the effect\nof mode variations on the encoded spatio-temporal features using LSTMs. We show\nthat the LSTM retains information related to the mode variation in the\nsequence, which is irrelevant to the task at hand (e.g. classification facial\nexpressions). Actually, the LSTM forget mechanism is not robust enough to mode\nvariations and preserves information that could negatively affect the encoded\nspatio-temporal features. We propose the mode variational LSTM to encode\nspatio-temporal features robust to unseen modes of variation. The mode\nvariational LSTM modifies the original LSTM structure by adding an additional\ncell state that focuses on encoding the mode variation in the input sequence.\nTo efficiently regulate what features should be stored in the additional cell\nstate, additional gating functionality is also introduced. The effectiveness of\nthe proposed mode variational LSTM is verified using the facial expression\nrecognition task. Comparative experiments on publicly available datasets\nverified that the proposed mode variational LSTM outperforms existing methods.\nMoreover, a new dynamic facial expression dataset with different modes of\nvariation, including various modes like pose and illumination variations, was\ncollected to comprehensively evaluate the proposed mode variational LSTM.\nExperimental results verified that the proposed mode variational LSTM encodes\nspatio-temporal features robust to unseen modes of variation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:40:13 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Baddar", "Wissam J.", ""], ["Ro", "Yong Man", ""]]}, {"id": "1811.06943", "submitter": "Shintaro Yamamoto", "authors": "Shintaro Yamamoto, Yoshihiro Fukuhara, Ryota Suzuki, Shigeo Morishima,\n  Hirokatsu Kataoka", "title": "Automatic Paper Summary Generation from Visual and Textual Information", "comments": "International Conference on Machine Vision 2018, Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent boom in artificial intelligence (AI) research, including\ncomputer vision (CV), it has become impossible for researchers in these fields\nto keep up with the exponentially increasing number of manuscripts. In response\nto this situation, this paper proposes the paper summary generation (PSG) task\nusing a simple but effective method to automatically generate an academic paper\nsummary from raw PDF data. We realized PSG by combination of vision-based\nsupervised components detector and language-based unsupervised important\nsentence extractor, which is applicable for a trained format of manuscripts. We\nshow the quantitative evaluation of ability of simple vision-based components\nextraction, and the qualitative evaluation that our system can extract both\nvisual item and sentence that are helpful for understanding. After processing\nvia our PSG, the 979 manuscripts accepted by the Conference on Computer Vision\nand Pattern Recognition (CVPR) 2018 are available. It is believed that the\nproposed method will provide a better way for researchers to stay caught with\nimportant academic papers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:52:25 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Yamamoto", "Shintaro", ""], ["Fukuhara", "Yoshihiro", ""], ["Suzuki", "Ryota", ""], ["Morishima", "Shigeo", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1811.06964", "submitter": "Coline Devin", "authors": "Eric Jang, Coline Devin, Vincent Vanhoucke, Sergey Levine", "title": "Grasp2Vec: Learning Object Representations from Self-Supervised Grasping", "comments": "CoRL 2018. Eric Jang and Coline Devin contributed equally to this\n  work", "journal-ref": "Proceedings of The 2nd Conference on Robot Learning, in PMLR\n  87:99-112 (2018)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well structured visual representations can make robot learning faster and can\nimprove generalization. In this paper, we study how we can acquire effective\nobject-centric representations for robotic manipulation tasks without human\nlabeling by using autonomous robot interaction with the environment. Such\nrepresentation learning methods can benefit from continuous refinement of the\nrepresentation as the robot collects more experience, allowing them to scale\neffectively without human intervention. Our representation learning approach is\nbased on object persistence: when a robot removes an object from a scene, the\nrepresentation of that scene should change according to the features of the\nobject that was removed. We formulate an arithmetic relationship between\nfeature vectors from this observation, and use it to learn a representation of\nscenes and objects that can then be used to identify object instances, localize\nthem in the scene, and perform goal-directed grasping tasks where the robot\nmust retrieve commanded objects from a bin. The same grasping procedure can\nalso be used to automatically collect training data for our method, by\nrecording images of scenes, grasping and removing an object, and recording the\noutcome. Our experiments demonstrate that this self-supervised approach for\ntasked grasping substantially outperforms direct reinforcement learning from\nimages and prior representation learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:42:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:25:51 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Jang", "Eric", ""], ["Devin", "Coline", ""], ["Vanhoucke", "Vincent", ""], ["Levine", "Sergey", ""]]}, {"id": "1811.06965", "submitter": "Yanping Huang", "authors": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen,\n  Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng\n  Chen", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline\n  Parallelism", "comments": "11 pages. Work in progress. Copyright 2018 by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling up deep neural network capacity has been known as an effective\napproach to improving model quality for several different machine learning\ntasks. In many cases, increasing model capacity beyond the memory limit of a\nsingle accelerator has required developing special algorithms or\ninfrastructure. These solutions are often architecture-specific and do not\ntransfer to other tasks. To address the need for efficient and task-independent\nmodel parallelism, we introduce GPipe, a pipeline parallelism library that\nallows scaling any network that can be expressed as a sequence of layers. By\npipelining different sub-sequences of layers on separate accelerators, GPipe\nprovides the flexibility of scaling a variety of different networks to gigantic\nsizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining\nalgorithm, resulting in almost linear speedup when a model is partitioned\nacross multiple accelerators. We demonstrate the advantages of GPipe by\ntraining large-scale neural networks on two different tasks with distinct\nnetwork architectures: (i) Image Classification: We train a\n557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on\nImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single\n6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100\nlanguages and achieve better quality than all bilingual models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:43:28 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:32:58 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 17:25:46 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 17:45:02 GMT"}, {"version": "v5", "created": "Thu, 25 Jul 2019 21:42:58 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Huang", "Yanping", ""], ["Cheng", "Youlong", ""], ["Bapna", "Ankur", ""], ["Firat", "Orhan", ""], ["Chen", "Mia Xu", ""], ["Chen", "Dehao", ""], ["Lee", "HyoukJoong", ""], ["Ngiam", "Jiquan", ""], ["Le", "Quoc V.", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""]]}, {"id": "1811.06969", "submitter": "Nicholas Frosst", "authors": "Nicholas Frosst, Sara Sabour, Geoffrey Hinton", "title": "DARCCC: Detecting Adversaries by Reconstruction from Class Conditional\n  Capsules", "comments": "To be presented at NIPS 2018 Workshop on Security in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple technique that allows capsule models to detect\nadversarial images. In addition to being trained to classify images, the\ncapsule model is trained to reconstruct the images from the pose parameters and\nidentity of the correct top-level capsule. Adversarial images do not look like\na typical member of the predicted class and they have much larger\nreconstruction errors when the reconstruction is produced from the top-level\ncapsule for that class. We show that setting a threshold on the $l2$ distance\nbetween the input image and its reconstruction from the winning capsule is very\neffective at detecting adversarial images for three different datasets. The\nsame technique works quite well for CNNs that have been trained to reconstruct\nthe image from all or part of the last hidden layer before the softmax. We then\nexplore a stronger, white-box attack that takes the reconstruction error into\naccount. This attack is able to fool our detection technique but in order to\nmake the model change its prediction to another class, the attack must\ntypically make the \"adversarial\" image resemble images of the other class.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:52:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Frosst", "Nicholas", ""], ["Sabour", "Sara", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1811.06981", "submitter": "Oren Rippel", "authors": "Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G.\n  Anderson, Lubomir Bourdev", "title": "Learned Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:29:51 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Rippel", "Oren", ""], ["Nair", "Sanjay", ""], ["Lew", "Carissa", ""], ["Branson", "Steve", ""], ["Anderson", "Alexander G.", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1811.06994", "submitter": "Chia-Wen Kuo", "authors": "Chia-Wen Kuo, Jacob Ashmore, David Huggins, Zsolt Kira", "title": "Data-Efficient Graph Embedding Learning for PCB Component Detection", "comments": "Paper accepted in WACV 2019. See\n  https://drive.google.com/open?id=1VkS8n1mKvAWjEPkiOA28XgY6VNfrsWYo for\n  supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a challenging computer vision task, namely the detection\nof generic components on a PCB, and a novel set of deep-learning methods that\nare able to jointly leverage the appearance of individual components and the\npropagation of information across the structure of the board to accurately\ndetect and identify various types of components on a PCB. Due to the expense of\nmanual data labeling, a highly unbalanced distribution of component types, and\nsignificant domain shift across boards, most earlier attempts based on\ntraditional image processing techniques fail to generalize well to PCB images\nwith various quality, lighting conditions, etc. Newer object detection\npipelines such as Faster R-CNN, on the other hand, require a large amount of\nlabeled data, do not deal with domain shift, and do not leverage structure. To\naddress these issues, we propose a three stage pipeline in which a\nclass-agnostic region proposal network is followed by a low-shot similarity\nprediction classifier. In order to exploit the data dependency within a PCB, we\ndesign a novel Graph Network block to refine the component features conditioned\non each PCB. To the best of our knowledge, this is one of the earliest attempts\nto train a deep learning based model for such tasks, and we demonstrate\nimprovements over recent graph networks for this task. We also provide in-depth\nanalysis and discussion for this challenging task, pointing to future research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 19:07:38 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 05:16:40 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kuo", "Chia-Wen", ""], ["Ashmore", "Jacob", ""], ["Huggins", "David", ""], ["Kira", "Zsolt", ""]]}, {"id": "1811.07013", "submitter": "Eirini Arvaniti", "authors": "Eirini Arvaniti, Manfred Claassen", "title": "Coupling weak and strong supervision for classification of prostate\n  cancer histopathology images", "comments": "Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated grading of prostate cancer histopathology images is a challenging\ntask, with one key challenge being the scarcity of annotations down to the\nlevel of regions of interest (strong labels), as typically the prostate cancer\nGleason score is known only for entire tissue slides (weak labels). In this\nstudy, we focus on automated Gleason score assignment of prostate cancer\nwhole-slide images on the basis of a large weakly-labeled dataset and a smaller\nstrongly-labeled one. We efficiently leverage information from both label\nsources by jointly training a classifier on the two datasets and by introducing\na gradient update scheme that assigns different relative importances to each\ntraining example, as a means of self-controlling the weak supervision signal.\nOur approach achieves superior performance when compared with standard Gleason\nscoring methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:05:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Arvaniti", "Eirini", ""], ["Claassen", "Manfred", ""]]}, {"id": "1811.07014", "submitter": "Konstantinos Zampogiannis", "authors": "Konstantinos Zampogiannis, Cornelia Fermuller, Yiannis Aloimonos", "title": "Topology-Aware Non-Rigid Point Cloud Registration", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2019.2940655", "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a non-rigid registration pipeline for pairs of\nunorganized point clouds that may be topologically different. Standard warp\nfield estimation algorithms, even under robust, discontinuity-preserving\nregularization, tend to produce erratic motion estimates on boundaries\nassociated with `close-to-open' topology changes. We overcome this limitation\nby exploiting backward motion: in the opposite motion direction, a\n`close-to-open' event becomes `open-to-close', which is by default handled\ncorrectly. At the core of our approach lies a general, topology-agnostic warp\nfield estimation algorithm, similar to those employed in recently introduced\ndynamic reconstruction systems from RGB-D input. We improve motion estimation\non boundaries associated with topology changes in an efficient post-processing\nphase. Based on both forward and (inverted) backward warp hypotheses, we\nexplicitly detect regions of the deformed geometry that undergo topological\nchanges by means of local deformation criteria and broadly classify them as\n`contacts' or `separations'. Subsequently, the two motion hypotheses are\nseamlessly blended on a local basis, according to the type and proximity of\ndetected events. Our method achieves state-of-the-art motion estimation\naccuracy on the MPI Sintel dataset. Experiments on a custom dataset with\ntopological event annotations demonstrate the effectiveness of our pipeline in\nestimating motion on event boundaries, as well as promising performance in\nexplicit topological event detection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:08:47 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 22:54:07 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 21:19:00 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zampogiannis", "Konstantinos", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1811.07023", "submitter": "Kathleen Greene", "authors": "K. G. Greene", "title": "An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:28:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Greene", "K. G.", ""]]}, {"id": "1811.07031", "submitter": "Viswanath Sivakumar", "authors": "Jing Huang, Viswanath Sivakumar, Mher Mnatsakanyan, Guan Pang", "title": "Improving Rotated Text Detection with Rotation Region Proposal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant number of images shared on social media platforms such as\nFacebook and Instagram contain text in various forms. It's increasingly\nbecoming commonplace for bad actors to share misinformation, hate speech or\nother kinds of harmful content as text overlaid on images on such platforms. A\nscene-text understanding system should hence be able to handle text in various\norientations that the adversary might use. Moreover, such a system can be\nincorporated into screen readers used to aid the visually impaired. In this\nwork, we extend the scene-text extraction system at Facebook, Rosetta, to\nefficiently handle text in various orientations. Specifically, we incorporate\nthe Rotation Region Proposal Networks (RRPN) in our text extraction pipeline\nand offer practical suggestions for building and deploying a model for\ndetecting and recognizing text in arbitrary orientations efficiently.\nExperimental results show a significant improvement on detecting rotated text.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:57:37 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Huang", "Jing", ""], ["Sivakumar", "Viswanath", ""], ["Mnatsakanyan", "Mher", ""], ["Pang", "Guan", ""]]}, {"id": "1811.07044", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "BLeSS: Bio-inspired Low-level Spatiochromatic Similarity Assisted Image\n  Quality Assessment", "comments": "7 pages, 3 figures, 3 tables", "journal-ref": "2016 IEEE International Conference on Multimedia and Expo (ICME),\n  Seattle, WA, 2016, pp. 1-6", "doi": "10.1109/ICME.2016.7552874", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a biologically-inspired low-level\nspatiochromatic-model-based similarity method (BLeSS) to assist full-reference\nimage-quality estimators that originally oversimplify color perception\nprocesses. More specifically, the spatiochromatic model is based on spatial\nfrequency, spatial orientation, and surround contrast effects. The assistant\nsimilarity method is used to complement image-quality estimators based on phase\ncongruency, gradient magnitude, and spectral residual. The effectiveness of\nBLeSS is validated using FSIM, FSIMc and SR-SIM methods on LIVE, Multiply\nDistorted LIVE, and TID 2013 databases. In terms of Spearman correlation, BLeSS\nenhances the performance of all quality estimators in color-based degradations\nand the enhancement is at 100% for both feature- and spectral residual-based\nsimilarity methods. Moreover, BleSS significantly enhances the performance of\nSR-SIM and FSIM in the full TID 2013 database.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 21:49:38 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.07056", "submitter": "Jiquan Ngiam", "authors": "Jiquan Ngiam and Daiyi Peng and Vijay Vasudevan and Simon Kornblith\n  and Quoc V. Le and Ruoming Pang", "title": "Domain Adaptive Transfer Learning with Specialist Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transfer learning is a widely used method to build high performing computer\nvision models. In this paper, we study the efficacy of transfer learning by\nexamining how the choice of data impacts performance. We find that more\npre-training data does not always help, and transfer performance depends on a\njudicious choice of pre-training data. These findings are important given the\ncontinued increase in dataset sizes. We further propose domain adaptive\ntransfer learning, a simple and effective pre-training method using importance\nweights computed based on the target dataset. Our method to compute importance\nweights follow from ideas in domain adaptation, and we show a novel application\nto transfer learning. Our methods achieve state-of-the-art results on multiple\nfine-grained classification datasets and are well-suited for use in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 22:52:27 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 22:09:00 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Ngiam", "Jiquan", ""], ["Peng", "Daiyi", ""], ["Vasudevan", "Vijay", ""], ["Kornblith", "Simon", ""], ["Le", "Quoc V.", ""], ["Pang", "Ruoming", ""]]}, {"id": "1811.07059", "submitter": "Zexi Chen", "authors": "Zexi Chen, Bharathkumar Ramachandra, Tianfu Wu, Ranga Raju Vatsavai", "title": "Relational Long Short-Term Memory for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and temporal relationships, both short-range and long-range, between\nobjects in videos, are key cues for recognizing actions. It is a challenging\nproblem to model them jointly. In this paper, we first present a new variant of\nLong Short-Term Memory, namely Relational LSTM, to address the challenge of\nrelation reasoning across space and time between objects. In our Relational\nLSTM module, we utilize a non-local operation similar in spirit to the recently\nproposed non-local network to substitute the fully connected operation in the\nvanilla LSTM. By doing this, our Relational LSTM is capable of capturing long\nand short-range spatio-temporal relations between objects in videos in a\nprincipled way. Then, we propose a two-branch neural architecture consisting of\nthe Relational LSTM module as the non-local branch and a spatio-temporal\npooling based local branch. The local branch is utilized for capturing local\nspatial appearance and/or short-term motion features. The two branches are\nconcatenated to learn video-level features from snippet-level ones which are\nthen used for classification. Experimental results on UCF-101 and HMDB-51\ndatasets show that our model achieves state-of-the-art results among LSTM-based\nmethods, while obtaining comparable performance with other state-of-the-art\nmethods (which use not directly comparable schema). Further, on the more\ncomplex large-scale Charades dataset, we obtain a large 3.2% gain over\nstate-of-the-art methods, verifying the effectiveness of our method in complex\nunderstanding.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 23:03:23 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 21:55:13 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Zexi", ""], ["Ramachandra", "Bharathkumar", ""], ["Wu", "Tianfu", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "1811.07070", "submitter": "Paden Tomasello", "authors": "Paden Tomasello, Sammy Sidhu, Anting Shen, Matthew W. Moskewicz, Nobie\n  Redmon, Gayatri Joshi, Romi Phadte, Paras Jain, Forrest Iandola", "title": "DSCnet: Replicating Lidar Point Clouds with Deep Sensor Cloning", "comments": "V2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have become increasingly popular for\nsolving a variety of computer vision tasks, ranging from image classification\nto image segmentation. Recently, autonomous vehicles have created a demand for\ndepth information, which is often obtained using hardware sensors such as Light\ndetection and ranging (LIDAR). Although it can provide precise distance\nmeasurements, most LIDARs are still far too expensive to sell in mass-produced\nconsumer vehicles, which has motivated methods to generate depth information\nfrom commodity automotive sensors like cameras.\n  In this paper, we propose an approach called Deep Sensor Cloning (DSC). The\nidea is to use Convolutional Neural Networks in conjunction with inexpensive\nsensors to replicate the 3D point-clouds that are created by expensive LIDARs.\nTo accomplish this, we develop a new dataset (DSDepth) and a new family of CNN\narchitectures (DSCnets). While previous tasks such as KITTI depth prediction\nuse an interpolated RGB-D images as ground-truth for training, we instead use\nDSCnets to directly predict LIDAR point-clouds. When we compare the output of\nour models to a $75,000 LIDAR, we find that our most accurate DSCnet achieves a\nrelative error of 5.77% using a single camera and 4.69% using stereo cameras.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 01:03:37 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 02:45:13 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tomasello", "Paden", ""], ["Sidhu", "Sammy", ""], ["Shen", "Anting", ""], ["Moskewicz", "Matthew W.", ""], ["Redmon", "Nobie", ""], ["Joshi", "Gayatri", ""], ["Phadte", "Romi", ""], ["Jain", "Paras", ""], ["Iandola", "Forrest", ""]]}, {"id": "1811.07073", "submitter": "Arash Vahdat", "authors": "Mostafa S. Ibrahim, Arash Vahdat, Mani Ranjbar, William G. Macready", "title": "Semi-Supervised Semantic Image Segmentation with Self-correcting\n  Networks", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 01:20:03 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 23:11:23 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 04:58:15 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ibrahim", "Mostafa S.", ""], ["Vahdat", "Arash", ""], ["Ranjbar", "Mani", ""], ["Macready", "William G.", ""]]}, {"id": "1811.07081", "submitter": "Xin Zhang", "authors": "Chenyang Li, Xin Zhang, Lufan Liao, Lianwen Jin, Weixin Yang", "title": "Skeleton-based Gesture Recognition Using Several Fully Connected Layers\n  with Path Signature Features and Temporal Transformer Module", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skeleton based gesture recognition is gaining more popularity due to its\nwide possible applications. The key issues are how to extract discriminative\nfeatures and how to design the classification model. In this paper, we first\nleverage a robust feature descriptor, path signature (PS), and propose three PS\nfeatures to explicitly represent the spatial and temporal motion\ncharacteristics, i.e., spatial PS (S_PS), temporal PS (T_PS) and temporal\nspatial PS (T_S_PS). Considering the significance of fine hand movements in the\ngesture, we propose an \"attention on hand\" (AOH) principle to define joint\npairs for the S_PS and select single joint for the T_PS. In addition, the\ndyadic method is employed to extract the T_PS and T_S_PS features that encode\nglobal and local temporal dynamics in the motion. Secondly, without the\nrecurrent strategy, the classification model still faces challenges on temporal\nvariation among different sequences. We propose a new temporal transformer\nmodule (TTM) that can match the sequence key frames by learning the temporal\nshifting parameter for each input. This is a learning-based module that can be\nincluded into standard neural network architecture. Finally, we design a\nmulti-stream fully connected layer based network to treat spatial and temporal\nfeatures separately and fused them together for the final result. We have\ntested our method on three benchmark gesture datasets, i.e., ChaLearn 2016,\nChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the\nstate-of-the-art performance on skeleton-based gesture recognition with high\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:41:38 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 13:38:26 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Li", "Chenyang", ""], ["Zhang", "Xin", ""], ["Liao", "Lufan", ""], ["Jin", "Lianwen", ""], ["Yang", "Weixin", ""]]}, {"id": "1811.07083", "submitter": "Van-Thanh Hoang Mr.", "authors": "Van-Thanh Hoang and Kang-Hyun Jo", "title": "PydMobileNet: Improved Version of MobileNets with Pyramid Depthwise\n  Separable Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown remarkable performance in\nvarious computer vision tasks in recent years. However, the increasing model\nsize has raised challenges in adopting them in real-time applications as well\nas mobile and embedded vision applications. Many works try to build networks as\nsmall as possible while still have acceptable performance. The state-of-the-art\narchitecture is MobileNets. They use Depthwise Separable Convolution\n(DWConvolution) in place of standard Convolution to reduce the size of\nnetworks. This paper describes an improved version of MobileNet, called Pyramid\nMobile Network. Instead of using just a $3\\times 3$ kernel size for\nDWConvolution like in MobileNet, the proposed network uses a pyramid kernel\nsize to capture more spatial information. The proposed architecture is\nevaluated on two highly competitive object recognition benchmark datasets\n(CIFAR-10, CIFAR-100). The experiments demonstrate that the proposed network\nachieves better performance compared with MobileNet as well as other\nstate-of-the-art networks. Additionally, it is more flexible in fine-tuning the\ntrade-off between accuracy, latency and model size than MobileNets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:58:31 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hoang", "Van-Thanh", ""], ["Jo", "Kang-Hyun", ""]]}, {"id": "1811.07087", "submitter": "Snehashis Roy", "authors": "Dzung L. Pham and Snehashis Roy", "title": "Alternating Segmentation and Simulation for Contrast Adaptive Tissue\n  Classification", "comments": "Proceedings Volume 10578, Medical Imaging 2018: Biomedical\n  Applications in Molecular, Structural, and Functional Imaging; SPIE Medical\n  Imaging 2018", "journal-ref": null, "doi": "10.1117/12.2295047", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A key feature of magnetic resonance (MR) imaging is its ability to manipulate\nhow the intrinsic tissue parameters of the anatomy ultimately contribute to the\ncontrast properties of the final, acquired image. This flexibility, however,\ncan lead to substantial challenges for segmentation algorithms, particularly\nsupervised methods. These methods require atlases or training data, which are\ncomposed of MR image and labeled image pairs. In most cases, the training data\nare obtained with a fixed acquisition protocol, leading to suboptimal\nperformance when an input data set that requires segmentation has differing\ncontrast properties. This drawback is increasingly significant with the recent\nmovement towards multi-center research studies involving multiple scanners and\nacquisition protocols. In this work, we propose a new framework for supervised\nsegmentation approaches that is robust to contrast differences between the\ntraining MR image and the input image. Our approach uses a generative\nsimulation model within the segmentation process to compensate for the contrast\ndifferences. We allow the contrast of the MR image in the training data to vary\nby simulating a new contrast from the corresponding label image. The model\nparameters are optimized by a cost function measuring the consistency between\nthe input MR image and its simulation based on a current estimate of the\nsegmentation labels. We provide a proof of concept of this approach by\ncombining a supervised classifier with a simple simulation model, and apply the\nresulting algorithm to synthetic images and actual MR images.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 03:28:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pham", "Dzung L.", ""], ["Roy", "Snehashis", ""]]}, {"id": "1811.07100", "submitter": "Flood Sung", "authors": "Xueting Zhang, Yuting Qiang, Flood Sung, Yongxin Yang, Timothy M.\n  Hospedales", "title": "RelationNet2: Deep Comparison Columns for Few-Shot Learning", "comments": "10 pages, 5 figures, Published in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 04:46:05 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:56:24 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 02:12:18 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhang", "Xueting", ""], ["Qiang", "Yuting", ""], ["Sung", "Flood", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1811.07103", "submitter": "Aydogan Ozcan", "authors": "Yichen Wu, Yilin Luo, Gunvant Chaudhari, Yair Rivenson, Ayfer Calis,\n  Kevin De Haan, Aydogan Ozcan", "title": "Cross-modality deep learning brings bright-field microscopy contrast to\n  holography", "comments": "3 pages", "journal-ref": "Light: Science & Applications (2019)", "doi": "10.1038/s41377-019-0139-9", "report-no": null, "categories": "cs.CV cs.LG physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning brings bright-field microscopy contrast to holographic images\nof a sample volume, bridging the volumetric imaging capability of holography\nwith the speckle- and artifact-free image contrast of bright-field incoherent\nmicroscopy.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 05:20:13 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wu", "Yichen", ""], ["Luo", "Yilin", ""], ["Chaudhari", "Gunvant", ""], ["Rivenson", "Yair", ""], ["Calis", "Ayfer", ""], ["De Haan", "Kevin", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1811.07104", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, Walter J. Scheirer, Kevin W. Bowyer, Patrick J.\n  Flynn", "title": "On Hallucinating Context and Background Pixels from a Face Mask using\n  Multi-scale GANs", "comments": "Extended version of WACV 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-scale GAN model to hallucinate realistic context\n(forehead, hair, neck, clothes) and background pixels automatically from a\nsingle input face mask. Instead of swapping a face on to an existing picture,\nour model directly generates realistic context and background pixels based on\nthe features of the provided face mask. Unlike face inpainting algorithms, it\ncan generate realistic hallucinations even for a large number of missing\npixels. Our model is composed of a cascaded network of GAN blocks, each tasked\nwith hallucination of missing pixels at a particular resolution while guiding\nthe synthesis process of the next GAN block. The hallucinated full face image\nis made photo-realistic by using a combination of reconstruction, perceptual,\nadversarial and identity preserving losses at each block of the network. With a\nset of extensive experiments, we demonstrate the effectiveness of our model in\nhallucinating context and background pixels from face masks varying in facial\npose, expression and lighting, collected from multiple datasets subject\ndisjoint with our training data. We also compare our method with two popular\nface swapping and face completion methods in terms of visual quality and\nrecognition performance. Additionally, we analyze our cascaded pipeline and\ncompare it with the recently proposed progressive growing of GANs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 05:52:13 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 05:56:43 GMT"}, {"version": "v3", "created": "Sun, 12 Jan 2020 02:45:51 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Scheirer", "Walter J.", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1811.07112", "submitter": "JIn Fang", "authors": "Jin Fang, Dingfu Zhou, Feilong Yan, Tongtong Zhao, Feihu Zhang, Yu Ma,\n  Liang Wang and Ruigang Yang", "title": "Augmented LiDAR Simulator for Autonomous Driving", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/LRA.2020.2969927", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Autonomous Driving (AD), detection and tracking of obstacles on the roads\nis a critical task. Deep-learning based methods using annotated LiDAR data have\nbeen the most widely adopted approach for this. Unfortunately, annotating 3D\npoint cloud is a very challenging, time- and money-consuming task. In this\npaper, we propose a novel LiDAR simulator that augments real point cloud with\nsynthetic obstacles (e.g., cars, pedestrians, and other movable objects).\nUnlike previous simulators that entirely rely on CG models and game engines,\nour augmented simulator bypasses the requirement to create high-fidelity\nbackground CAD models. Instead, we can simply deploy a vehicle with a LiDAR\nscanner to sweep the street of interests to obtain the background point cloud,\nbased on which annotated point cloud can be automatically generated. This\nunique \"scan-and-simulate\" capability makes our approach scalable and\npractical, ready for large-scale industrial applications. In this paper, we\ndescribe our simulator in detail, in particular the placement of obstacles that\nis critical for performance enhancement. We show that detectors with our\nsimulated LiDAR point cloud alone can perform comparably (within two percentage\npoints) with these trained with real data. Mixing real and simulated data can\nachieve over 95% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 07:09:13 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 09:59:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fang", "Jin", ""], ["Zhou", "Dingfu", ""], ["Yan", "Feilong", ""], ["Zhao", "Tongtong", ""], ["Zhang", "Feihu", ""], ["Ma", "Yu", ""], ["Wang", "Liang", ""], ["Yang", "Ruigang", ""]]}, {"id": "1811.07120", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust and Joachim Denzler", "title": "Not just a matter of semantics: the relationship between visual\n  similarity and semantic similarity", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge transfer, zero-shot learning and semantic image retrieval are\nmethods that aim at improving accuracy by utilizing semantic information, e.g.\nfrom WordNet. It is assumed that this information can augment or replace\nmissing visual data in the form of labeled training images because semantic\nsimilarity correlates with visual similarity. This assumption may seem trivial,\nbut is crucial for the application of such semantic methods. Any violation can\ncause mispredictions. Thus, it is important to examine the visual-semantic\nrelationship for a certain target problem. In this paper, we use five different\nsemantic and visual similarity measures each to thoroughly analyze the\nrelationship without relying too much on any single definition. We postulate\nand verify three highly consequential hypotheses on the relationship. Our\nresults show that it indeed exists and that WordNet semantic similarity carries\nmore information about visual similarity than just the knowledge of \"different\nclasses look different\". They suggest that classification is not the ideal\napplication for semantic methods and that wrong semantic information is much\nworse than none.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:00:41 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 10:00:12 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Denzler", "Joachim", ""]]}, {"id": "1811.07123", "submitter": "Xiao Sun", "authors": "Xiao Sun and Chuankang Li and Stephen Lin", "title": "Explicit Spatiotemporal Joint Relation Learning for Tracking Human Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for human pose tracking that is based on learning\nspatiotemporal relationships among joints. Beyond generating the heatmap of a\njoint in a given frame, our system also learns to predict the offset of the\njoint from a neighboring joint in the frame. Additionally, it is trained to\npredict the displacement of the joint from its position in the previous frame,\nin a manner that can account for possibly changing joint appearance, unlike\noptical flow. These relational cues in the spatial domain and temporal domain\nare inferred in a robust manner by attending only to relevant areas in the\nvideo frames. By explicitly learning and exploiting these joint relationships,\nour system achieves state-of-the-art performance on standard benchmarks for\nvarious pose tracking tasks including 3D body pose tracking in RGB video, 3D\nhand pose tracking in depth sequences, and 3D hand gesture tracking in RGB\nvideo.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:12:50 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 03:29:30 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 04:00:35 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Sun", "Xiao", ""], ["Li", "Chuankang", ""], ["Lin", "Stephen", ""]]}, {"id": "1811.07124", "submitter": "Haoxin Ma", "authors": "Haoxin Ma, Haotian Li, Zhiwen Qian, Shengxian Shi, and Tingting Mu", "title": "VommaNet: an End-to-End Network for Disparity Estimation from Reflective\n  and Texture-less Light Field Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise combination of image sensor and micro-lens array enables lenslet\nlight field cameras to record both angular and spatial information of incoming\nlight, therefore, one can calculate disparity and depth from light field\nimages. In turn, 3D models of the recorded objects can be recovered, which is a\ngreat advantage over other imaging system. However, reflective and texture-less\nareas in light field images have complicated conditions, making it hard to\ncorrectly calculate disparity with existing algorithms. To tackle this problem,\nwe introduce a novel end-to-end network VommaNet to retrieve multi-scale\nfeatures from reflective and texture-less regions for accurate disparity\nestimation. Meanwhile, our network has achieved similar or better performance\nin other regions for both synthetic light field images and real-world data\ncompared to the state-of-the-art algorithms. Currently, we achieve the best\nscore for mean squared error (MSE) on HCI 4D Light Field Benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:13:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ma", "Haoxin", ""], ["Li", "Haotian", ""], ["Qian", "Zhiwen", ""], ["Shi", "Shengxian", ""], ["Mu", "Tingting", ""]]}, {"id": "1811.07125", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust and Joachim Denzler", "title": "Integrating domain knowledge: using hierarchies to improve deep\n  classifiers", "comments": "Accepted at ACPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most prominent problems in machine learning in the age of deep\nlearning is the availability of sufficiently large annotated datasets. For\nspecific domains, e.g. animal species, a long-tail distribution means that some\nclasses are observed and annotated insufficiently. Additional labels can be\nprohibitively expensive, e.g. because domain experts need to be involved.\nHowever, there is more information available that is to the best of our\nknowledge not exploited accordingly. In this paper, we propose to make use of\npreexisting class hierarchies like WordNet to integrate additional domain\nknowledge into classification. We encode the properties of such a class\nhierarchy into a probabilistic model. From there, we derive a novel label\nencoding and a corresponding loss function. On the ImageNet and NABirds\ndatasets our method offers a relative improvement of 10.4% and 9.6% in accuracy\nover the baseline respectively. After less than a third of training time, it is\nalready able to match the baseline's fine-grained recognition performance. Both\nresults show that our suggested method is efficient and effective.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:23:32 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 16:27:23 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Denzler", "Joachim", ""]]}, {"id": "1811.07126", "submitter": "Xue Yang", "authors": "Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo,\n  Sun Xian and Kun Fu", "title": "SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated\n  Objects", "comments": "10 pages, 10 figures, 6 tables, ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has been a building block in computer vision. Though\nconsiderable progress has been made, there still exist challenges for objects\nwith small size, arbitrary direction, and dense distribution. Apart from\nnatural images, such issues are especially pronounced for aerial images of\ngreat importance. This paper presents a novel multi-category rotation detector\nfor small, cluttered and rotated objects, namely SCRDet. Specifically, a\nsampling fusion network is devised which fuses multi-layer feature with\neffective anchor sampling, to improve the sensitivity to small objects.\nMeanwhile, the supervised pixel attention network and the channel attention\nnetwork are jointly explored for small and cluttered object detection by\nsuppressing the noise and highlighting the objects feature. For more accurate\nrotation estimation, the IoU constant factor is added to the smooth L1 loss to\naddress the boundary problem for the rotating bounding box. Extensive\nexperiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as\nnatural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the\nstate-of-the-art performance of our detector. The code and models will be\navailable at https://github.com/DetectionTeamUCAS.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:24:25 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 08:22:24 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 06:50:29 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 02:53:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Xue", ""], ["Yang", "Jirui", ""], ["Yan", "Junchi", ""], ["Zhang", "Yue", ""], ["Zhang", "Tengfei", ""], ["Guo", "Zhi", ""], ["Xian", "Sun", ""], ["Fu", "Kun", ""]]}, {"id": "1811.07130", "submitter": "Zuozhuo Dai", "authors": "Zuozhuo Dai and Mingqiang Chen and Xiaodong Gu and Siyu Zhu and Ping\n  Tan", "title": "Batch DropBlock Network for Person Re-identification and Beyond", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the person re-identification task often suffers from the problem of\npose changes and occlusions, some attentive local features are often suppressed\nwhen training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network\nwhich is a two branch network composed of a conventional ResNet-50 as the\nglobal branch and a feature dropping branch. The global branch encodes the\nglobal salient representations. Meanwhile, the feature dropping branch consists\nof an attentive feature learning module called Batch DropBlock, which randomly\ndrops the same region of all input feature maps in a batch to reinforce the\nattentive feature learning of local regions. The network then concatenates\nfeatures from both branches and provides a more comprehensive and spatially\ndistributed feature representation. Albeit simple, our method achieves\nstate-of-the-art on person re-identification and it is also applicable to\ngeneral metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy\non the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online\nProducts dataset, outperforming the existing works by a large margin (more than\n6%).\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:49:04 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 09:37:37 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dai", "Zuozhuo", ""], ["Chen", "Mingqiang", ""], ["Gu", "Xiaodong", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "1811.07157", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh, Fabio Cuzzolin", "title": "Recurrent Convolutions for Causal 3D CNNs", "comments": "Workshop on Large Scale Holistic Video Understanding, ICCVW, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, three dimensional (3D) convolutional neural networks (CNNs) have\nemerged as dominant methods to capture spatiotemporal representations in\nvideos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D\nCNNs, however, are anti-causal (i.e., they exploit information from both the\npast and the future frames to produce feature representations, thus preventing\ntheir use in online settings), constrain the temporal reasoning horizon to the\nsize of the temporal convolution kernel, and are not temporal\nresolution-preserving for video sequence-to-sequence modelling, as, for\ninstance, in action detection. To address these serious limitations, here we\npresent a new 3D CNN architecture for the causal/online processing of videos.\n  Namely, we propose a novel Recurrent Convolutional Network (RCN), which\nrelies on recurrence to capture the temporal context across frames at each\nnetwork level. Our network decomposes 3D convolutions into (1) a 2D spatial\nconvolution component, and (2) an additional hidden state $1\\times 1$\nconvolution, applied across time. The hidden state at any time $t$ is assumed\nto depend on the hidden state at $t-1$ and on the current output of the spatial\nconvolution component. As a result, the proposed network: (i) produces causal\noutputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal\nresolution. Our experiments on the large-scale large Kinetics and MultiThumos\ndatasets show that the proposed method performs comparably to anti-causal 3D\nCNNs, while being causal and using fewer parameters.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 13:07:30 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 09:28:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1811.07161", "submitter": "Jing Yu", "authors": "Jing Yu and Zhenchun Chang and Chuangbai Xiao", "title": "Edge-Based Blur Kernel Estimation Using Sparse Representation and\n  Self-Similarity", "comments": "26 pages, 10 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deconvolution is the problem of recovering the latent image from\nthe only observed blurry image when the blur kernel is unknown. In this paper,\nwe propose an edge-based blur kernel estimation method for blind motion\ndeconvolution. In our previous work, we incorporate both sparse representation\nand self-similarity of image patches as priors into our blind deconvolution\nmodel to regularize the recovery of the latent image. Since almost any natural\nimage has properties of sparsity and multi-scale self-similarity, we construct\na sparsity regularizer and a cross-scale non-local regularizer based on our\npatch priors. It has been observed that our regularizers often favor sharp\nimages over blurry ones only for image patches of the salient edges and thus we\ndefine an edge mask to locate salient edges that we want to apply our\nregularizers. Experimental results on both simulated and real blurry images\ndemonstrate that our method outperforms existing state-of-the-art blind\ndeblurring methods even for handling of very large blurs, thanks to the use of\nthe edge mask.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 14:00:37 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yu", "Jing", ""], ["Chang", "Zhenchun", ""], ["Xiao", "Chuangbai", ""]]}, {"id": "1811.07170", "submitter": "Tobias Senst", "authors": "Gregory Schr\\\"oder, Tobias Senst, Erik Bochinski, Thomas Sikora", "title": "Optical Flow Dataset and Benchmark for Visual Crowd Analysis", "comments": "submission to International Conference on Advanced Video and\n  Signal-Based Surveillance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of optical flow algorithms greatly depends on the specifics\nof the content and the application for which it is used. Existing and well\nestablished optical flow datasets are limited to rather particular contents\nfrom which none is close to crowd behavior analysis; whereas such applications\nheavily utilize optical flow. We introduce a new optical flow dataset\nexploiting the possibilities of a recent video engine to generate sequences\nwith ground-truth optical flow for large crowds in different scenarios. We\nbreak with the development of the last decade of introducing ever increasing\ndisplacements to pose new difficulties. Instead we focus on real-world\nsurveillance scenarios where numerous small, partly independent, non rigidly\nmoving objects observed over a long temporal range pose a challenge. By\nevaluating different optical flow algorithms, we find that results of\nestablished datasets can not be transferred to these new challenges. In\nexhaustive experiments we are able to provide new insight into optical flow for\ncrowd analysis. Finally, the results have been validated on the real-world UCF\ncrowd tracking benchmark while achieving competitive results compared to more\nsophisticated state-of-the-art crowd tracking approaches.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 14:43:42 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Schr\u00f6der", "Gregory", ""], ["Senst", "Tobias", ""], ["Bochinski", "Erik", ""], ["Sikora", "Thomas", ""]]}, {"id": "1811.07173", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif, Fady Aziz, Karim Armanious, Bernhard Kleiner, Bin\n  Yang, Urs Schneider", "title": "Person Identification and Body Mass Index: A Deep Learning-Based Study\n  on Micro-Dopplers", "comments": "Accepted in IEEE Radarconf19", "journal-ref": null, "doi": "10.1109/RADAR.2019.8835652", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining a smart surveillance requires a sensing system that can capture\naccurate and detailed information for the human walking style. The radar\nmicro-Doppler ($\\boldsymbol{\\mu}$-D) analysis is proved to be a reliable metric\nfor studying human locomotions. Thus, $\\boldsymbol{\\mu}$-D signatures can be\nused to identify humans based on their walking styles. Additionally, the\nsignatures contain information about the radar cross section (RCS) of the\nmoving subject. This paper investigates the effect of human body\ncharacteristics on human identification based on their $\\boldsymbol{\\mu}$-D\nsignatures. In our proposed experimental setup, a treadmill is used to collect\n$\\boldsymbol{\\mu}$-D signatures of 22 subjects with different genders and body\ncharacteristics. Convolutional autoencoders (CAE) are then used to extract the\nlatent space representation from the $\\boldsymbol{\\mu}$-D signatures. It is\nthen interpreted in two dimensions using t-distributed stochastic neighbor\nembedding (t-SNE). Our study shows that the body mass index (BMI) has a\ncorrelation with the $\\boldsymbol{\\mu}$-D signature of the walking subject. A\n50-layer deep residual network is then trained to identify the walking subject\nbased on the $\\boldsymbol{\\mu}$-D signature. We achieve an accuracy of 98% on\nthe test set with high signal-to-noise-ratio (SNR) and 84% in case of different\nSNR levels.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 14:53:22 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 13:47:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Aziz", "Fady", ""], ["Armanious", "Karim", ""], ["Kleiner", "Bernhard", ""], ["Yang", "Bin", ""], ["Schneider", "Urs", ""]]}, {"id": "1811.07184", "submitter": "Cheng Yaw Low", "authors": "Cheng-Yaw Low, Jaewoo Park, and Andrew Beng-Jin Teoh", "title": "Stacking-Based Deep Neural Network: Deep Analytic Network for Pattern\n  Classification", "comments": "14 pages, 7 figures, 11 tables", "journal-ref": null, "doi": "10.1109/TCYB.2019.2908387", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking-based deep neural network (S-DNN) is aggregated with pluralities of\nbasic learning modules, one after another, to synthesize a deep neural network\n(DNN) alternative for pattern classification. Contrary to the DNNs trained end\nto end by backpropagation (BP), each S-DNN layer, i.e., a self-learnable\nmodule, is to be trained decisively and independently without BP intervention.\nIn this paper, a ridge regression-based S-DNN, dubbed deep analytic network\n(DAN), along with its kernelization (K-DAN), are devised for multilayer feature\nre-learning from the pre-extracted baseline features and the structured\nfeatures. Our theoretical formulation demonstrates that DAN/K-DAN re-learn by\nperturbing the intra/inter-class variations, apart from diminishing the\nprediction errors. We scrutinize the DAN/K-DAN performance for pattern\nclassification on datasets of varying domains - faces, handwritten digits,\ngeneric objects, to name a few. Unlike the typical BP-optimized DNNs to be\ntrained from gigantic datasets by GPU, we disclose that DAN/K-DAN are trainable\nusing only CPU even for small-scale training sets. Our experimental results\ndisclose that DAN/K-DAN outperform the present S-DNNs and also the BP-trained\nDNNs, including multiplayer perceptron, deep belief network, etc., without data\naugmentation applied.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 16:19:14 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 05:49:30 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Low", "Cheng-Yaw", ""], ["Park", "Jaewoo", ""], ["Teoh", "Andrew Beng-Jin", ""]]}, {"id": "1811.07190", "submitter": "Hochul Shin", "authors": "Hochul Shin, Hyeon Cho, Dongyi Kim, Daekwan Ko, Soochul Lim, Wonjun\n  Hwang", "title": "Sequential Image-based Attention Network for Inferring Force Estimation\n  without Haptic Sensor", "comments": "Accepted by IEEE Access on Oct. 08, 2019", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2947090", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can infer approximate interaction force between objects from only\nvision information because we already have learned it through experiences.\nBased on this idea, we propose a recurrent convolutional neural network-based\nmethod using sequential images for inferring interaction force without using a\nhaptic sensor. For training and validating deep learning methods, we collected\na large number of images and corresponding interaction forces through an\nelectronic motor-based device. To concentrate on changing shapes of a target\nobject by the external force in images, we propose a sequential image-based\nattention module, which learns a salient model from temporal dynamics. The\nproposed sequential image-based attention module consists of a sequential\nspatial attention module and a sequential channel attention module, which are\nextended to exploit multiple sequential images. For gaining better accuracy, we\nalso created a weighted average pooling layer for both spatial and channel\nattention modules. The extensive experimental results verified that the\nproposed method successfully infers interaction forces under the various\nconditions, such as different target materials, illumination changes, and\nexternal force directions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 17:12:59 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:49:34 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 11:02:38 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 11:26:21 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shin", "Hochul", ""], ["Cho", "Hyeon", ""], ["Kim", "Dongyi", ""], ["Ko", "Daekwan", ""], ["Lim", "Soochul", ""], ["Hwang", "Wonjun", ""]]}, {"id": "1811.07211", "submitter": "Jacob Springer", "authors": "Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim,\n  Garrett T. Kenyon", "title": "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep\n  Learning Transferable Examples", "comments": "8 pages, 8 figures, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 19:39:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 18:55:55 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Springer", "Jacob M.", ""], ["Strauss", "Charles S.", ""], ["Thresher", "Austin M.", ""], ["Kim", "Edward", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1811.07212", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Kevin J. Shih, Yichen Li, Ke Xu, Svetlana Lazebnik,\n  Stan Sclaroff, Kate Saenko", "title": "Revisiting Image-Language Networks for Open-ended Phrase Detection", "comments": "Accepted to TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3029008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing work that grounds natural language phrases in images starts\nwith the assumption that the phrase in question is relevant to the image. In\nthis paper we address a more realistic version of the natural language\ngrounding task where we must both identify whether the phrase is relevant to an\nimage and localize the phrase. This can also be viewed as a generalization of\nobject detection to an open-ended vocabulary, introducing elements of few- and\nzero-shot detection. We propose an approach for this task that extends Faster\nR-CNN to relate image regions and phrases. By carefully initializing the\nclassification layers of our network using canonical correlation analysis\n(CCA), we encourage a solution that is more discerning when reasoning between\nsimilar phrases, resulting in over double the performance compared to a naive\nadaptation on three popular phrase grounding datasets, Flickr30K Entities,\nReferIt Game, and Visual Genome, with test-time phrase vocabulary sizes of 5K,\n32K, and 159K, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 19:45:05 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 01:05:23 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 19:13:55 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Shih", "Kevin J.", ""], ["Li", "Yichen", ""], ["Xu", "Ke", ""], ["Lazebnik", "Svetlana", ""], ["Sclaroff", "Stan", ""], ["Saenko", "Kate", ""]]}, {"id": "1811.07222", "submitter": "Xinshuo Weng", "authors": "Yunze Man, Xinshuo Weng, Xi Li, Kris Kitani", "title": "GroundNet: Monocular Ground Plane Normal Estimation with Geometric\n  Consistency", "comments": "Camera Ready for ACM MM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on estimating the 3D orientation of the ground plane from a single\nimage. We formulate the problem as an inter-mingled multi-task prediction\nproblem by jointly optimizing for pixel-wise surface normal direction, ground\nplane segmentation, and depth estimates. Specifically, our proposed model,\nGroundNet, first estimates the depth and surface normal in two separate\nstreams, from which two ground plane normals are then computed\ndeterministically. To leverage the geometric correlation between depth and\nnormal, we propose to add a consistency loss on top of the computed ground\nplane normals. In addition, a ground segmentation stream is used to isolate the\nground regions so that we can selectively back-propagate parameter updates\nthrough only the ground regions in the image. Our method achieves the\ntop-ranked performance on ground plane normal estimation and horizon line\ndetection on the real-world outdoor datasets of ApolloScape and KITTI,\nimproving the performance of previous art by up to 17.7% relatively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 21:25:53 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 23:04:16 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 15:09:28 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 01:59:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Man", "Yunze", ""], ["Weng", "Xinshuo", ""], ["Li", "Xi", ""], ["Kitani", "Kris", ""]]}, {"id": "1811.07246", "submitter": "Wenxuan Wu", "authors": "Wenxuan Wu, Zhongang Qi, Li Fuxin", "title": "PointConv: Deep Convolutional Networks on 3D Point Clouds", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike images which are represented in regular dense grids, 3D point clouds\nare irregular and unordered, hence applying convolution on them can be\ndifficult. In this paper, we extend the dynamic filter to a new convolution\noperation, named PointConv. PointConv can be applied on point clouds to build\ndeep convolutional networks. We treat convolution kernels as nonlinear\nfunctions of the local coordinates of 3D points comprised of weight and density\nfunctions. With respect to a given point, the weight functions are learned with\nmulti-layer perceptron networks and density functions through kernel density\nestimation. The most important contribution of this work is a novel\nreformulation proposed for efficiently computing the weight functions, which\nallowed us to dramatically scale up the network and significantly improve its\nperformance. The learned convolution kernel can be used to compute\ntranslation-invariant and permutation-invariant convolution on any point set in\nthe 3D space. Besides, PointConv can also be used as deconvolution operators to\npropagate features from a subsampled point cloud back to its original\nresolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep\nconvolutional neural networks built on PointConv are able to achieve\nstate-of-the-art on challenging semantic segmentation benchmarks on 3D point\nclouds. Besides, our experiments converting CIFAR-10 into a point cloud showed\nthat networks built on PointConv can match the performance of convolutional\nnetworks in 2D images of a similar structure.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 23:42:13 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 17:22:11 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 21:20:22 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wu", "Wenxuan", ""], ["Qi", "Zhongang", ""], ["Fuxin", "Li", ""]]}, {"id": "1811.07249", "submitter": "Srikrishna Karanam", "authors": "Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jana Kosecka", "title": "Learning Local RGB-to-CAD Correspondences for Object Pose Estimation", "comments": "10 pages, 6 figures, 4 tables, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of 3D object pose estimation. While much recent work\nhas focused on the RGB domain, the reliance on accurately annotated images\nlimits their generalizability and scalability. On the other hand, the easily\navailable CAD models of objects are rich sources of data, providing a large\nnumber of synthetically rendered images. In this paper, we solve this key\nproblem of existing methods requiring expensive 3D pose annotations by\nproposing a new method that matches RGB images to CAD models for object pose\nestimation. Our key innovations compared to existing work include removing the\nneed for either real-world textures for CAD models or explicit 3D pose\nannotations for RGB images. We achieve this through a series of objectives that\nlearn how to select keypoints and enforce viewpoint and modality invariance\nacross RGB images and CAD model renderings. We conduct extensive experiments to\ndemonstrate that the proposed method can reliably estimate object pose in RGB\nimages, as well as generalize to object instances not seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 00:32:58 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:45:13 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:29:30 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 14:12:22 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Georgakis", "Georgios", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Kosecka", "Jana", ""]]}, {"id": "1811.07252", "submitter": "Adam Czajka", "authors": "Adam Czajka and Zhaoyuan Fang and Kevin W. Bowyer", "title": "Iris Presentation Attack Detection Based on Photometric Stereo Features", "comments": "Patent Pending. Paper accepted for WACV 2019, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new iris presentation attack detection method using\nthree-dimensional features of an observed iris region estimated by photometric\nstereo. Our implementation uses a pair of iris images acquired by a common\ncommercial iris sensor (LG 4000). No hardware modifications of any kind are\nrequired. Our approach should be applicable to any iris sensor that can\nilluminate the eye from two different directions. Each iris image in the pair\nis captured under near-infrared illumination at a different angle relative to\nthe eye. Photometric stereo is used to estimate surface normal vectors in the\nnon-occluded portions of the iris region. The variability of the normal vectors\nis used as the presentation attack detection score. This score is larger for a\ntexture that is irregularly opaque and printed on a convex contact lens, and is\nsmaller for an authentic iris texture. Thus the problem is formulated as binary\nclassification into (a) an eye wearing textured contact lens and (b) the\ntexture of an actual iris surface (possibly seen through a clear contact lens).\nExperiments were carried out on a database of approx. 2,900 iris image pairs\nacquired from approx. 100 subjects. Our method was able to correctly classify\nover 95% of samples when tested on contact lens brands unseen in training, and\nover 98% of samples when the contact lens brand was seen during training. The\nsource codes of the method are made available to other researchers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:21:22 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Czajka", "Adam", ""], ["Fang", "Zhaoyuan", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "1811.07256", "submitter": "Junjie Huang", "authors": "Junjie Huang, Wei Zou, Zheng Zhu, Jiagang Zhu", "title": "Optical Flow Based Online Moving Foreground Analysis", "comments": "6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtained by moving object detection, the foreground mask result is unshaped\nand can not be directly used in most subsequent processes. In this paper, we\nfocus on this problem and address it by constructing an optical flow based\nmoving foreground analysis framework. During the processing procedure, the\nforeground masks are analyzed and segmented through two complementary\nclustering algorithms. As a result, we obtain the instance-level information\nlike the number, location and size of moving objects. The experimental result\nshow that our method adapts itself to the problem and performs well enough for\npractical applications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:59:28 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Huang", "Junjie", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""], ["Zhu", "Jiagang", ""]]}, {"id": "1811.07258", "submitter": "Gaoang Wang", "authors": "Gaoang Wang, Yizhou Wang, Haotian Zhang, Renshu Gu, Jenq-Neng Hwang", "title": "Exploit the Connectivity: Multi-Object Tracking with TrackletNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking (MOT) is an important and practical task related to\nboth surveillance systems and moving camera applications, such as autonomous\ndriving and robotic vision. However, due to unreliable detection, occlusion and\nfast camera motion, tracked targets can be easily lost, which makes MOT very\nchallenging. Most recent works treat tracking as a re-identification (Re-ID)\ntask, but how to combine appearance and temporal features is still not well\naddressed. In this paper, we propose an innovative and effective tracking\nmethod called TrackletNet Tracker (TNT) that combines temporal and appearance\ninformation together as a unified framework. First, we define a graph model\nwhich treats each tracklet as a vertex. The tracklets are generated by\nappearance similarity with CNN features and intersection-over-union (IOU) with\nepipolar constraints to compensate camera movement between adjacent frames.\nThen, for every pair of two tracklets, the similarity is measured by our\ndesigned multi-scale TrackletNet. Afterwards, the tracklets are clustered into\ngroups which represent individual object IDs. Our proposed TNT has the ability\nto handle most of the challenges in MOT, and achieve promising results on MOT16\nand MOT17 benchmark datasets compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 02:23:27 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wang", "Gaoang", ""], ["Wang", "Yizhou", ""], ["Zhang", "Haotian", ""], ["Gu", "Renshu", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1811.07260", "submitter": "Zhizhong Wang", "authors": "Zhizhong Wang and Lei Zhao and Wei Xing and Dongming Lu", "title": "GLStyleNet: Higher Quality Style Transfer Combining Global and Local\n  Pyramid Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies using deep neural networks have shown remarkable success in\nstyle transfer especially for artistic and photo-realistic images. However, the\napproaches using global feature correlations fail to capture small, intricate\ntextures and maintain correct texture scales of the artworks, and the\napproaches based on local patches are defective on global effect. In this\npaper, we present a novel feature pyramid fusion neural network, dubbed\nGLStyleNet, which sufficiently takes into consideration multi-scale and\nmulti-level pyramid features by best aggregating layers across a VGG network,\nand performs style transfer hierarchically with multiple losses of different\nscales. Our proposed method retains high-frequency pixel information and low\nfrequency construct information of images from two aspects: loss function\nconstraint and feature fusion. Our approach is not only flexible to adjust the\ntrade-off between content and style, but also controllable between global and\nlocal. Compared to state-of-the-art methods, our method can transfer not just\nlarge-scale, obvious style cues but also subtle, exquisite ones, and\ndramatically improves the quality of style transfer. We demonstrate the\neffectiveness of our approach on portrait style transfer, artistic style\ntransfer, photo-realistic style transfer and Chinese ancient painting style\ntransfer tasks. Experimental results indicate that our unified approach\nimproves image style transfer quality over previous state-of-the-art methods,\nwhile also accelerating the whole process in a certain extent. Our code is\navailable at https://github.com/EndyWon/GLStyleNet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 02:39:45 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wang", "Zhizhong", ""], ["Zhao", "Lei", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "1811.07266", "submitter": "Yuchen Li", "authors": "Yuchen Li, Safwan Hossain, Kiarash Jamali, Frank Rudzicz", "title": "DeepConsensus: using the consensus of features from multiple layers to\n  attain robust image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a classifier whose test set is exposed to various perturbations\nthat are not present in the training set. These test samples still contain\nenough features to map them to the same class as their unperturbed counterpart.\nCurrent architectures exhibit rapid degradation of accuracy when trained on\nstandard datasets but then used to classify perturbed samples of that data. To\naddress this, we present a novel architecture named DeepConsensus that\nsignificantly improves generalization to these test-time perturbations. Our key\ninsight is that deep neural networks should directly consider summaries of low\nand high level features when making classifications. Existing convolutional\nneural networks can be augmented with DeepConsensus, leading to improved\nresistance against large and small perturbations on MNIST, EMNIST,\nFashionMNIST, CIFAR10 and SVHN datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 03:37:52 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:46:52 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 17:39:57 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Yuchen", ""], ["Hossain", "Safwan", ""], ["Jamali", "Kiarash", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1811.07268", "submitter": "Bolin Liu", "authors": "Bolin Liu, Xiao Shu, Xiaolin Wu", "title": "Deep Learning with Inaccurate Training Data for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of deep learning, particularly those in image\nrestoration, it is either very difficult, prohibitively expensive, or outright\nimpossible to obtain paired training data precisely as in the real world. In\nsuch cases, one is forced to use synthesized paired data to train the deep\nconvolutional neural network (DCNN). However, due to the unavoidable\ngeneralization error in statistical learning, the synthetically trained DCNN\noften performs poorly on real world data. To overcome this problem, we propose\na new general training method that can compensate for, to a large extent, the\ngeneralization errors of synthetically trained DCNNs.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 04:01:33 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liu", "Bolin", ""], ["Shu", "Xiao", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1811.07270", "submitter": "Tien Ho-Phuoc", "authors": "Tien Ho-Phuoc", "title": "CIFAR10 to Compare Visual Recognition Performance between Deep Neural\n  Networks and Humans", "comments": "paper, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object recognition plays an essential role in human daily life. This\nability is so efficient that we can recognize a face or an object seemingly\nwithout effort, though they may vary in position, scale, pose, and\nillumination. In the field of computer vision, a large number of studies have\nbeen carried out to build a human-like object recognition system. Recently,\ndeep neural networks have shown impressive progress in object classification\nperformance, and have been reported to surpass humans. Yet there is still lack\nof thorough and fair comparison between humans and artificial recognition\nsystems. While some studies consider artificially degraded images, human\nrecognition performance on dataset widely used for deep neural networks has not\nbeen fully evaluated. The present paper carries out an extensive experiment to\nevaluate human classification accuracy on CIFAR10, a well-known dataset of\nnatural images. This then allows for a fair comparison with the\nstate-of-the-art deep neural networks. Our CIFAR10-based evaluations show very\nefficient object recognition of recent CNNs but, at the same time, prove that\nthey are still far from human-level capability of generalization. Moreover, a\ndetailed investigation using multiple levels of difficulty reveals that easy\nimages for humans may not be easy for deep neural networks. Such images form a\nsubset of CIFAR10 that can be employed to evaluate and improve future neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 04:21:37 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 13:52:53 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ho-Phuoc", "Tien", ""]]}, {"id": "1811.07275", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, James Storer, Dinei Florencio, Cha Zhang", "title": "RePr: Improved Training of Convolutional Filters", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-trained Convolutional Neural Network can easily be pruned without\nsignificant loss of performance. This is because of unnecessary overlap in the\nfeatures captured by the network's filters. Innovations in network architecture\nsuch as skip/dense connections and Inception units have mitigated this problem\nto some extent, but these improvements come with increased computation and\nmemory requirements at run-time. We attempt to address this problem from\nanother angle - not by changing the network structure but by altering the\ntraining method. We show that by temporarily pruning and then restoring a\nsubset of the model's filters, and repeating this process cyclically, overlap\nin the learned features is reduced, producing improved generalization. We show\nthat the existing model-pruning criteria are not optimal for selecting filters\nto prune in this context and introduce inter-filter orthogonality as the\nranking criteria to determine under-expressive filters. Our method is\napplicable both to vanilla convolutional networks and more complex modern\narchitectures, and improves the performance across a variety of tasks,\nespecially when applied to smaller networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 05:15:27 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 04:10:34 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 06:04:16 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Prakash", "Aaditya", ""], ["Storer", "James", ""], ["Florencio", "Dinei", ""], ["Zhang", "Cha", ""]]}, {"id": "1811.07288", "submitter": "Jiaxin Cheng", "authors": "Jiaxin Cheng, Yue Wu, Wael Abd-Almageed and Prem Natarajan", "title": "Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image-to-GPS verification problem asks whether a given image is taken at\na claimed GPS location. In this paper, we treat it as an image verification\nproblem -- whether a query image is taken at the same place as a reference\nimage retrieved at the claimed GPS location. We make three major contributions:\n1) we propose a novel custom bottom-up pattern matching (BUPM) deep neural\nnetwork solution; 2) we demonstrate that the verification can be directly done\nby cross-checking a perspective-looking query image and a panorama reference\nimage, and 3) we collect and clean a dataset of 30K pairs query and reference.\nOur experimental results show that the proposed BUPM solution outperforms the\nstate-of-the-art solutions in terms of both verification and localization.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 06:59:36 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Cheng", "Jiaxin", ""], ["Wu", "Yue", ""], ["Abd-Almageed", "Wael", ""], ["Natarajan", "Prem", ""]]}, {"id": "1811.07296", "submitter": "Jianlin Su", "authors": "Jianlin Su", "title": "GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz\n  Constraint", "comments": "simplify some proofs; add reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We know SGAN may have a risk of gradient vanishing. A significant improvement\nis WGAN, with the help of 1-Lipschitz constraint on discriminator to prevent\nfrom gradient vanishing. Is there any GAN having no gradient vanishing and no\n1-Lipschitz constraint on discriminator? We do find one, called GAN-QP.\n  To construct a new framework of Generative Adversarial Network (GAN) usually\nincludes three steps: 1. choose a probability divergence; 2. convert it into a\ndual form; 3. play a min-max game. In this articles, we demonstrate that the\nfirst step is not necessary. We can analyse the property of divergence and even\nconstruct new divergence in dual space directly. As a reward, we obtain a\nsimpler alternative of WGAN: GAN-QP. We demonstrate that GAN-QP have a better\nperformance than WGAN in theory and practice.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 08:36:03 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 11:44:33 GMT"}, {"version": "v3", "created": "Sat, 8 Dec 2018 04:15:42 GMT"}, {"version": "v4", "created": "Sat, 15 Dec 2018 11:30:28 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Su", "Jianlin", ""]]}, {"id": "1811.07308", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Yilin Shen, Hongxia Jin, William Wang", "title": "A Variational Dirichlet Framework for Out-of-Distribution Detection", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recently rapid development in deep learning, deep neural networks\nhave been widely adopted in many real-life applications. However, deep neural\nnetworks are also known to have very little control over its uncertainty for\nunseen examples, which potentially causes very harmful and annoying\nconsequences in practical scenarios. In this paper, we are particularly\ninterested in designing a higher-order uncertainty metric for deep neural\nnetworks and investigate its effectiveness under the out-of-distribution\ndetection task proposed by~\\cite{hendrycks2016baseline}. Our method first\nassumes there exists an underlying higher-order distribution $\\mathbb{P}(z)$,\nwhich controls label-wise categorical distribution $\\mathbb{P}(y)$ over classes\non the K-dimension simplex, and then approximate such higher-order distribution\nvia parameterized posterior function $p_{\\theta}(z|x)$ under variational\ninference framework, finally we use the entropy of learned posterior\ndistribution $p_{\\theta}(z|x)$ as uncertainty measure to detect\nout-of-distribution examples. Further, we propose an auxiliary objective\nfunction to discriminate against synthesized adversarial examples to further\nincrease the robustness of the proposed uncertainty measure. Through\ncomprehensive experiments on various datasets, our proposed framework is\ndemonstrated to consistently outperform competing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 10:24:58 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 19:48:44 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 01:51:41 GMT"}, {"version": "v4", "created": "Sat, 20 Apr 2019 22:53:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chen", "Wenhu", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Wang", "William", ""]]}, {"id": "1811.07311", "submitter": "Yoel Shoshan", "authors": "Yoel Shoshan and Vadim Ratner", "title": "Regularized adversarial examples for model interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As machine learning algorithms continue to improve, there is an increasing\nneed for explaining why a model produces a certain prediction for a certain\ninput. In recent years, several methods for model interpretability have been\ndeveloped, aiming to provide explanation of which subset regions of the model\ninput is the main reason for the model prediction. In parallel, a significant\nresearch community effort is occurring in recent years for developing\nadversarial example generation methods for fooling models, while not altering\nthe true label of the input,as it would have been classified by a human\nannotator. In this paper, we bridge the gap between adversarial example\ngeneration and model interpretability, and introduce a modification to the\nadversarial example generation process which encourages better\ninterpretability. We analyze the proposed method on a public medical imaging\ndataset, both quantitatively and qualitatively, and show that it significantly\noutperforms the leading known alternative method. Our suggested method is\nsimple to implement, and can be easily plugged into most common adversarial\nexample generation frameworks. Additionally, we propose an explanation quality\nmetric - $APE$ - \"Adversarial Perturbative Explanation\", which measures how\nwell an explanation describes model decisions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 10:40:16 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 07:29:32 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Shoshan", "Yoel", ""], ["Ratner", "Vadim", ""]]}, {"id": "1811.07315", "submitter": "Miguel De Prado", "authors": "Miguel de Prado, Nuria Pazos and Luca Benini", "title": "Learning to infer: RL-based search for DNN primitive selection on\n  Heterogeneous Embedded Systems", "comments": null, "journal-ref": null, "doi": "10.23919/DATE.2019.8714959", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is increasingly being adopted by industry for computer vision\napplications running on embedded devices. While Convolutional Neural Networks'\naccuracy has achieved a mature and remarkable state, inference latency and\nthroughput are a major concern especially when targeting low-cost and low-power\nembedded platforms. CNNs' inference latency may become a bottleneck for Deep\nLearning adoption by industry, as it is a crucial specification for many\nreal-time processes. Furthermore, deployment of CNNs across heterogeneous\nplatforms presents major compatibility issues due to vendor-specific technology\nand acceleration libraries. In this work, we present QS-DNN, a fully automatic\nsearch based on Reinforcement Learning which, combined with an inference engine\noptimizer, efficiently explores through the design space and empirically finds\nthe optimal combinations of libraries and primitives to speed up the inference\nof CNNs on heterogeneous embedded devices. We show that, an optimized\ncombination can achieve 45x speedup in inference latency on CPU compared to a\ndependency-free baseline and 2x on average on GPGPU compared to the best vendor\nlibrary. Further, we demonstrate that, the quality of results and time\n\"to-solution\" is much better than with Random Search and achieves up to 15x\nbetter results for a short-time search.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 11:28:24 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["de Prado", "Miguel", ""], ["Pazos", "Nuria", ""], ["Benini", "Luca", ""]]}, {"id": "1811.07318", "submitter": "Saksham Suri", "authors": "Saksham Suri, Anush Sankaran, Mayank Vatsa, Richa Singh", "title": "On Matching Faces with Alterations due to Plastic Surgery and Disguise", "comments": "The 9th IEEE International Conference on Biometrics: Theory,\n  Applications, and Systems (BTAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plastic surgery and disguise variations are two of the most challenging\nco-variates of face recognition. The state-of-art deep learning models are not\nsufficiently successful due to the availability of limited training samples. In\nthis paper, a novel framework is proposed which transfers fundamental visual\nfeatures learnt from a generic image dataset to supplement a supervised face\nrecognition model. The proposed algorithm combines off-the-shelf supervised\nclassifier and a generic, task independent network which encodes information\nrelated to basic visual cues such as color, shape, and texture. Experiments are\nperformed on IIITD plastic surgery face dataset and Disguised Faces in the Wild\n(DFW) dataset. Results showcase that the proposed algorithm achieves state of\nthe art results on both the datasets. Specifically on the DFW database, the\nproposed algorithm yields over 87% verification accuracy at 1% false accept\nrate which is 53.8% better than baseline results computed using VGGFace.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 12:05:54 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Suri", "Saksham", ""], ["Sankaran", "Anush", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "1811.07335", "submitter": "Liu Sen", "authors": "Sen Liu and Jianxin Lin and Zhibo Chen", "title": "Distribution Discrepancy Maximization for Image Privacy Preserving", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in online photo sharing activities, image obfuscation\nalgorithms become particularly important for protecting the sensitive\ninformation in the shared photos. However, existing image obfuscation methods\nbased on hand-crafted principles are challenged by the dramatic development of\ndeep learning techniques. To address this problem, we propose to maximize the\ndistribution discrepancy between the original image domain and the encrypted\nimage domain. Accordingly, we introduce a collaborative training scheme: a\ndiscriminator $D$ is trained to discriminate the reconstructed image from the\nencrypted image, and an encryption model $G_e$ is required to generate these\ntwo kinds of images to maximize the recognition rate of $D$, leading to the\nsame training objective for both $D$ and $G_e$. We theoretically prove that\nsuch a training scheme maximizes two distributions' discrepancy. Compared with\ncommonly-used image obfuscation methods, our model can produce satisfactory\ndefense against the attack of deep recognition models indicated by significant\naccuracy decreases on FaceScrub, Casia-WebFace and LFW datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 14:53:49 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liu", "Sen", ""], ["Lin", "Jianxin", ""], ["Chen", "Zhibo", ""]]}, {"id": "1811.07339", "submitter": "Sangwhan Cha", "authors": "Yang Li and Sangwhan Cha", "title": "Implementation of Robust Face Recognition System Using Live Video Feed\n  Based on CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way to accurately and effectively identify people has always been an\ninteresting topic in research and industry. With the rapid development of\nartificial intelligence in recent years, facial recognition gains lots of\nattention due to prompting the development of emerging identification methods.\nCompared to traditional card recognition, fingerprint recognition and iris\nrecognition, face recognition has many advantages including non-contact\ninterface, high concurrency, and user-friendly usage. It has high potential to\nbe used in government, public facilities, security, e-commerce, retailing,\neducation and many other fields. With the development of deep learning and the\nintroduction of deep convolutional neural networks, the accuracy and speed of\nface recognition have made great strides. However, the results from different\nnetworks and models are very different with different system architecture.\nFurthermore, it could take significant amount of data storage space and data\nprocessing time for the face recognition system with video feed, if the system\nstores images and features of human faces. In this paper, facial features are\nextracted by merging and comparing multiple models, and then a deep neural\nnetwork is constructed to train and construct the combined features. In this\nway, the advantages of multiple models can be combined to mention the\nrecognition accuracy. After getting a model with high accuracy, we build a\nproduct model. The model will take a human face image and extract it into a\nvector. Then the distance between vectors are compared to determine if two\nfaces on different picture belongs to the same person. The proposed approach\nreduces data storage space and data processing time for the face recognition\nsystem with video feed scientifically with our proposed system architecture.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 15:31:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Yang", ""], ["Cha", "Sangwhan", ""]]}, {"id": "1811.07344", "submitter": "Cuixian Chen", "authors": "Philip Smith and Cuixian Chen", "title": "Transfer Learning with Deep CNNs for Gender Recognition and Age\n  Estimation", "comments": "It has been accepted in the 5th National Symposium for NSF REU\n  Research in Data Science, Systems, and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this project, competition-winning deep neural networks with pretrained\nweights are used for image-based gender recognition and age estimation.\nTransfer learning is explored using both VGG19 and VGGFace pretrained models by\ntesting the effects of changes in various design schemes and training\nparameters in order to improve prediction accuracy. Training techniques such as\ninput standardization, data augmentation, and label distribution age encoding\nare compared. Finally, a hierarchy of deep CNNs is tested that first classifies\nsubjects by gender, and then uses separate male and female age models to\npredict age. A gender recognition accuracy of 98.7% and an MAE of 4.1 years is\nachieved. This paper shows that, with proper training techniques, good results\ncan be obtained by retasking existing convolutional filters towards a new\npurpose.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 16:11:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Smith", "Philip", ""], ["Chen", "Cuixian", ""]]}, {"id": "1811.07376", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Bjorn Stenger, Tae-Kyun Kim", "title": "RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for hand pose estimation from RGB images that\nuses both external large-scale depth image datasets and paired depth and RGB\nimages as privileged information at training time. We show that providing depth\ninformation during training significantly improves performance of pose\nestimation from RGB images during testing. We explore different ways of using\nthis privileged information: (1) using depth data to initially train a\ndepth-based network, (2) using the features from the depth-based network of the\npaired depth images to constrain mid-level RGB network weights, and (3) using\nthe foreground mask, obtained from the depth data, to suppress the responses\nfrom the background area. By using paired RGB and depth images, we are able to\nsupervise the RGB-based network to learn middle layer features that mimic that\nof the corresponding depth-based network, which is trained on large-scale,\naccurately annotated depth data. During testing, when only an RGB image is\navailable, our method produces accurate 3D hand pose predictions. Our method is\nalso tested on 2D hand pose estimation. Experiments on three public datasets\nshow that the method outperforms the state-of-the-art methods for hand pose\nestimation using RGB image input.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 18:52:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yuan", "Shanxin", ""], ["Stenger", "Bjorn", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1811.07386", "submitter": "Anthony Rhodes", "authors": "Anthony D. Rhodes, Manan Goel", "title": "Deep Siamese Networks with Bayesian non-Parametrics for Video Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm utilizing a deep Siamese neural network as a\ngeneral object similarity function in combination with a Bayesian optimization\n(BO) framework to encode spatio-temporal information for efficient object\ntracking in video. In particular, we treat the video tracking problem as a\ndynamic (i.e. temporally-evolving) optimization problem. Using Gaussian Process\npriors, we model a dynamic objective function representing the location of a\ntracked object in each frame. By exploiting temporal correlations, the proposed\nmethod queries the search space in a statistically principled and efficient\nway, offering several benefits over current state of the art video tracking\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 19:32:48 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Rhodes", "Anthony D.", ""], ["Goel", "Manan", ""]]}, {"id": "1811.07391", "submitter": "Mingfei Gao", "authors": "Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S. Davis, David J.\n  Crandall", "title": "Temporal Recurrent Networks for Online Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on temporal action detection is formulated as an offline problem,\nin which the start and end times of actions are determined after the entire\nvideo is fully observed. However, important real-time applications including\nsurveillance and driver assistance systems require identifying actions as soon\nas each video frame arrives, based only on current and historical observations.\nIn this paper, we propose a novel framework, Temporal Recurrent Network (TRN),\nto model greater temporal context of a video frame by simultaneously performing\nonline action detection and anticipation of the immediate future. At each\nmoment in time, our approach makes use of both accumulated historical evidence\nand predicted future information to better recognize the action that is\ncurrently occurring, and integrates both of these into a unified end-to-end\narchitecture. We evaluate our approach on two popular online action detection\ndatasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14.\nThe results show that TRN significantly outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 20:03:55 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 18:58:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Xu", "Mingze", ""], ["Gao", "Mingfei", ""], ["Chen", "Yi-Ting", ""], ["Davis", "Larry S.", ""], ["Crandall", "David J.", ""]]}, {"id": "1811.07392", "submitter": "Sarah Ostadabbas", "authors": "Yu Yin, Mohsen Nabian, Miolin Fan, ChunAn Chou, Maria Gendron, Sarah\n  Ostadabbas", "title": "Facial Expression and Peripheral Physiology Fusion to Decode\n  Individualized Affective Experience", "comments": "2nd IJCAI Workshop on Artificial Intelligence in Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal approach to simultaneously analyze\nfacial movements and several peripheral physiological signals to decode\nindividualized affective experiences under positive and negative emotional\ncontexts, while considering their personalized resting dynamics. We propose a\nperson-specific recurrence network to quantify the dynamics present in the\nperson's facial movements and physiological data. Facial movement is\nrepresented using a robust head vs. 3D face landmark localization and tracking\napproach, and physiological data are processed by extracting known attributes\nrelated to the underlying affective experience. The dynamical coupling between\ndifferent input modalities is then assessed through the extraction of several\ncomplex recurrent network metrics. Inference models are then trained using\nthese metrics as features to predict individual's affective experience in a\ngiven context, after their resting dynamics are excluded from their response.\nWe validated our approach using a multimodal dataset consists of (i) facial\nvideos and (ii) several peripheral physiological signals, synchronously\nrecorded from 12 participants while watching 4 emotion-eliciting video-based\nstimuli. The affective experience prediction results signified that our\nmultimodal fusion method improves the prediction accuracy up to 19% when\ncompared to the prediction using only one or a subset of the input modalities.\nFurthermore, we gained prediction improvement for affective experience by\nconsidering the effect of individualized resting dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 20:10:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yin", "Yu", ""], ["Nabian", "Mohsen", ""], ["Fan", "Miolin", ""], ["Chou", "ChunAn", ""], ["Gendron", "Maria", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1811.07407", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Ziyun Yang, Thomas Ashley and Nicholas J. Durr", "title": "Multimodal Densenet", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make accurate decisions by interpreting complex data from multiple\nsources. Medical diagnostics, in particular, often hinge on human\ninterpretation of multi-modal information. In order for artificial intelligence\nto make progress in automated, objective, and accurate diagnosis and prognosis,\nmethods to fuse information from multiple medical imaging modalities are\nrequired. However, combining information from multiple data sources has several\nchallenges, as current deep learning architectures lack the ability to extract\nuseful representations from multimodal information, and often simple\nconcatenation is used to fuse such information. In this work, we propose\nMultimodal DenseNet, a novel architecture for fusing multimodal data. Instead\nof focusing on concatenation or early and late fusion, our proposed\narchitectures fuses information over several layers and gives the model\nflexibility in how it combines information from multiple sources. We apply this\narchitecture to the challenge of polyp characterization and landmark\nidentification in endoscopy. Features from white light images are fused with\nfeatures from narrow band imaging or depth maps. This study demonstrates that\nMultimodal DenseNet outperforms monomodal classification as well as other\nmultimodal fusion techniques by a significant margin on two different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 21:31:22 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Mahmood", "Faisal", ""], ["Yang", "Ziyun", ""], ["Ashley", "Thomas", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1811.07417", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "PerSIM: Multi-resolution Image Quality Assessment in the Perceptually\n  Uniform Color Domain", "comments": "5 pages, 1 figure, 3 tables", "journal-ref": "2015 IEEE International Conference on Image Processing (ICIP),\n  Quebec City, QC, 2015, pp. 1682-1686", "doi": "10.1109/ICIP.2015.7351087", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An average observer perceives the world in color instead of black and white.\nMoreover, the visual system focuses on structures and segments instead of\nindividual pixels. Based on these observations, we propose a full reference\nobjective image quality metric modeling visual system characteristics and\nchroma similarity in the perceptually uniform color domain (Lab). Laplacian of\nGaussian features are obtained in the L channel to model the retinal ganglion\ncells in human visual system and color similarity is calculated over the a and\nb channels. In the proposed perceptual similarity index (PerSIM), a\nmulti-resolution approach is followed to mimic the hierarchical nature of human\nvisual system. LIVE and TID2013 databases are used in the validation and PerSIM\noutperforms all the compared metrics in the overall databases in terms of\nranking, monotonic behavior and linearity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 22:42:32 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.07432", "submitter": "Yuan Li", "authors": "Yuan Li, Yuanjie Yu, Zefeng Li, Yangkun Lin, Meifang Xu, Jiwei Li, Xi\n  Zhou", "title": "Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks", "comments": "10 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, semantic segmentation and general object detection frameworks have\nbeen widely adopted by scene text detecting tasks. However, both of them alone\nhave obvious shortcomings in practice. In this paper, we propose a novel\nend-to-end trainable deep neural network framework, named Pixel-Anchor, which\ncombines semantic segmentation and SSD in one network by feature sharing and\nanchor-level attention mechanism to detect oriented scene text. To deal with\nscene text which has large variances in size and aspect ratio, we combine FPN\nand ASPP operation as our encoder-decoder structure in the semantic\nsegmentation part, and propose a novel Adaptive Predictor Layer in the SSD.\nPixel-Anchor detects scene text in a single network forward pass, no complex\npost-processing other than an efficient fusion Non-Maximum Suppression is\ninvolved. We have benchmarked the proposed Pixel-Anchor on the public datasets.\nPixel-Anchor outperforms the competing methods in terms of text localization\naccuracy and run speed, more specifically, on the ICDAR 2015 dataset, the\nproposed algorithm achieves an F-score of 0.8768 at 10 FPS for 960 x 1728\nresolution images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:26:42 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Yuan", ""], ["Yu", "Yuanjie", ""], ["Li", "Zefeng", ""], ["Lin", "Yangkun", ""], ["Xu", "Meifang", ""], ["Li", "Jiwei", ""], ["Zhou", "Xi", ""]]}, {"id": "1811.07441", "submitter": "Nadav Schor", "authors": "Nadav Schor, Oren Katzir, Hao Zhang and Daniel Cohen-Or", "title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition", "comments": "Accepted to ICCV 2019. Code: https://github.com/nschor/CompoNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven generative modeling has made remarkable progress by leveraging\nthe power of deep neural networks. A reoccurring challenge is how to enable a\nmodel to generate a rich variety of samples from the entire target\ndistribution, rather than only from a distribution confined to the training\ndata. In other words, we would like the generative model to go beyond the\nobserved samples and learn to generate ``unseen'', yet still plausible, data.\nIn our work, we present CompoNet, a generative neural network for 2D or 3D\nshapes that is based on a part-based prior, where the key idea is for the\nnetwork to synthesize shapes by varying both the shape parts and their\ncompositions. Treating a shape not as an unstructured whole, but as a\n(re-)composable set of deformable parts, adds a combinatorial dimension to the\ngenerative process to enrich the diversity of the output, encouraging the\ngenerator to venture more into the ``unseen''. We show that our part-based\nmodel generates richer variety of plausible shapes compared with baseline\ngenerative models. To this end, we introduce two quantitative metrics to\nevaluate the diversity of a generative model and assess how well the generated\ndata covers both the training data and unseen data from the same target\ndistribution. Code is available at https://github.com/nschor/CompoNet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:45:17 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 23:26:45 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 08:25:20 GMT"}, {"version": "v4", "created": "Sun, 1 Sep 2019 19:30:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Schor", "Nadav", ""], ["Katzir", "Oren", ""], ["Zhang", "Hao", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1811.07442", "submitter": "Armon Shariati", "authors": "Armon Shariati, Bernd Pfrommer, Camillo J. Taylor", "title": "Predictive and Semantic Layout Estimation for Robotic Applications in\n  Manhattan Worlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach to automatically extracting floor plans from\nthe kinds of incomplete measurements that could be acquired by an autonomous\nmobile robot. The approach proceeds by reasoning about extended structural\nlayout surfaces which are automatically extracted from the available data. The\nscheme can be run in an online manner to build water tight representations of\nthe environment. The system effectively speculates about room boundaries and\nfree space regions which provides useful guidance to subsequent motion planning\nsystems. Experimental results are presented on multiple data sets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:49:54 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Shariati", "Armon", ""], ["Pfrommer", "Bernd", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1811.07456", "submitter": "Guanbin Li", "authors": "Ruijia Xu, Guanbin Li, Jihan Yang, Liang Lin", "title": "Larger Norm More Transferable: An Adaptive Feature Norm Approach for\n  Unsupervised Domain Adaptation", "comments": "Accepted as an Oral presentation at ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation enables the learner to safely generalize into novel\nenvironments by mitigating domain shifts across distributions. Previous works\nmay not effectively uncover the underlying reasons that would lead to the\ndrastic model degradation on the target task. In this paper, we empirically\nreveal that the erratic discrimination of the target domain mainly stems from\nits much smaller feature norms with respect to that of the source domain. To\nthis end, we propose a novel parameter-free Adaptive Feature Norm approach. We\ndemonstrate that progressively adapting the feature norms of the two domains to\na large range of values can result in significant transfer gains, implying that\nthose task-specific features with larger norms are more transferable. Our\nmethod successfully unifies the computation of both standard and partial domain\nadaptation with more robustness against the negative transfer issue. Without\nbells and whistles but a few lines of code, our method substantially lifts the\nperformance on the target task and exceeds state-of-the-arts by a large margin\n(11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective\napproach will shed some light on the future research of transfer learning. Code\nis available at https://github.com/jihanyang/AFN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:04:58 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 15:25:11 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Xu", "Ruijia", ""], ["Li", "Guanbin", ""], ["Yang", "Jihan", ""], ["Lin", "Liang", ""]]}, {"id": "1811.07459", "submitter": "Tasfia Shermin", "authors": "Tasfia Shermin, Manzur Murshed, Guojun Lu and Shyh Wei Teng", "title": "Transfer Learning Using Classification Layer Features of CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although CNNs have gained the ability to transfer learned knowledge from\nsource task to target task by virtue of large annotated datasets but consume\nhuge processing time to fine-tune without GPU. In this paper, we propose a new\ncomputationally efficient transfer learning approach using classification layer\nfeatures of pre-trained CNNs by appending layer after existing classification\nlayer. We demonstrate that fine-tuning of the appended layer with existing\nclassification layer for new task converges much faster than baseline and in\naverage outperforms baseline classification accuracy. Furthermore, we execute\nthorough experiments to examine the influence of quantity, similarity, and\ndissimilarity of training sets in our classification outcomes to demonstrate\ntransferability of classification layer features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:11:08 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:55:09 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Shermin", "Tasfia", ""], ["Murshed", "Manzur", ""], ["Lu", "Guojun", ""], ["Teng", "Shyh Wei", ""]]}, {"id": "1811.07460", "submitter": "Zhanzhan Cheng", "authors": "Yunlu Xu, Chengwei Zhang, Zhanzhan Cheng, Jianwen Xie, Yi Niu,\n  Shiliang Pu and Fei Wu", "title": "Segregated Temporal Assembly Recurrent Networks for Weakly Supervised\n  Multiple Action Detection", "comments": "Accepted to Proc. AAAI Conference on Artificial Intelligence 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper proposes a segregated temporal assembly recurrent (STAR) network\nfor weakly-supervised multiple action detection. The model learns from\nuntrimmed videos with only supervision of video-level labels and makes\nprediction of intervals of multiple actions. Specifically, we first assemble\nvideo clips according to class labels by an attention mechanism that learns\nclass-variable attention weights and thus helps the noise relieving from\nbackground or other actions. Secondly, we build temporal relationship between\nactions by feeding the assembled features into an enhanced recurrent neural\nnetwork. Finally, we transform the output of recurrent neural network into the\ncorresponding action distribution. In order to generate more precise temporal\nproposals, we design a score term called segregated temporal gradient-weighted\nclass activation mapping (ST-GradCAM) fused with attention weights. Experiments\non THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the\nstate-of-the-art weakly-supervised method, and performs at par with the\nfully-supervised counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:12:06 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Xu", "Yunlu", ""], ["Zhang", "Chengwei", ""], ["Cheng", "Zhanzhan", ""], ["Xie", "Jianwen", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""]]}, {"id": "1811.07461", "submitter": "Sarah Ostadabbas", "authors": "Amirreza Farnoosh and Sarah Ostadabbas", "title": "Indoor GeoNet: Weakly Supervised Hybrid Learning for Depth and Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:17:10 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Farnoosh", "Amirreza", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1811.07465", "submitter": "Haoran You", "authors": "Haoran You, Yu Cheng, Tianheng Cheng, Chunliang Li, Pan Zhou", "title": "Bayesian Cycle-Consistent Generative Adversarial Networks via\n  Marginalizing Latent Sampling", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent techniques built on Generative Adversarial Networks (GANs), such as\nCycle-Consistent GANs, are able to learn mappings among different domains built\nfrom unpaired datasets, through min-max optimization games between generators\nand discriminators. However, it remains challenging to stabilize the training\nprocess and thus cyclic models fall into mode collapse accompanied by the\nsuccess of discriminator. To address this problem, we propose an novel Bayesian\ncyclic model and an integrated cyclic framework for inter-domain mappings. The\nproposed method motivated by Bayesian GAN explores the full posteriors of\ncyclic model via sampling latent variables and optimizes the model with maximum\na posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In\naddition, original CycleGAN cannot generate diversified results. But it is\nfeasible for Bayesian framework to diversify generated images by replacing\nrestricted latent variables in inference process. We evaluate the proposed\nBayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,\nand Monet2photo. The proposed method improve the per-pixel accuracy by 15% for\nthe Cityscapes semantic segmentation task within origin framework and improve\n20% within the proposed integrated framework, showing better resilience to\nimbalance confrontation. The diversified results of Monet2Photo style transfer\nalso demonstrate its superiority over original cyclic model. We provide codes\nfor all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:22:49 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 06:34:52 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 19:38:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["You", "Haoran", ""], ["Cheng", "Yu", ""], ["Cheng", "Tianheng", ""], ["Li", "Chunliang", ""], ["Zhou", "Pan", ""]]}, {"id": "1811.07468", "submitter": "Jianing Li", "authors": "Jianing Li, Shiliang Zhang, Tiejun Huang", "title": "Multi-scale 3D Convolution Network for Video Based Person\n  Re-Identification", "comments": "AAAI, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a two-stream convolution network to extract spatial and\ntemporal cues for video based person Re-Identification (ReID). A temporal\nstream in this network is constructed by inserting several Multi-scale 3D (M3D)\nconvolution layers into a 2D CNN network. The resulting M3D convolution network\nintroduces a fraction of parameters into the 2D CNN, but gains the ability of\nmulti-scale temporal feature learning. With this compact architecture, M3D\nconvolution network is also more efficient and easier to optimize than existing\n3D convolution networks. The temporal stream further involves Residual\nAttention Layers (RAL) to refine the temporal features. By jointly learning\nspatial-temporal attention masks in a residual manner, RAL identifies the\ndiscriminative spatial regions and temporal cues. The other stream in our\nnetwork is implemented with a 2D CNN for spatial feature extraction. The\nspatial and temporal features from two streams are finally fused for the video\nbased person ReID. Evaluations on three widely used benchmarks datasets, i.e.,\nMARS, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our\nmethod over existing 3D convolution networks and state-of-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:40:32 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Jianing", ""], ["Zhang", "Shiliang", ""], ["Huang", "Tiejun", ""]]}, {"id": "1811.07480", "submitter": "Ziqi Zhou", "authors": "Ziqi Zhou, Zheng Wang, Huchuan Lu, Song Wang and Meijun Sun", "title": "Global and Local Sensitivity Guided Key Salient Object Re-augmentation\n  for Video Saliency Detection", "comments": "6 figures, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing still-static deep learning based saliency researches do not\nconsider the weighting and highlighting of extracted features from different\nlayers, all features contribute equally to the final saliency decision-making.\nSuch methods always evenly detect all \"potentially significant regions\" and\nunable to highlight the key salient object, resulting in detection failure of\ndynamic scenes. In this paper, based on the fact that salient areas in videos\nare relatively small and concentrated, we propose a \\textbf{key salient object\nre-augmentation method (KSORA) using top-down semantic knowledge and bottom-up\nfeature guidance} to improve detection accuracy in video scenes. KSORA includes\ntwo sub-modules (WFE and KOS): WFE processes local salient feature selection\nusing bottom-up strategy, while KOS ranks each object in global fashion by\ntop-down statistical knowledge, and chooses the most critical object area for\nlocal enhancement. The proposed KSORA can not only strengthen the saliency\nvalue of the local key salient object but also ensure global saliency\nconsistency. Results on three benchmark datasets suggest that our model has the\ncapability of improving the detection accuracy on complex scenes. The\nsignificant performance of KSORA, with a speed of 17FPS on modern GPUs, has\nbeen verified by comparisons with other ten state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:27:23 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhou", "Ziqi", ""], ["Wang", "Zheng", ""], ["Lu", "Huchuan", ""], ["Wang", "Song", ""], ["Sun", "Meijun", ""]]}, {"id": "1811.07483", "submitter": "Honglun Zhang", "authors": "Honglun Zhang, Wenqing Chen, Jidong Tian, Yongkun Wang, Yaohui Jin", "title": "Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image\n  Translation with Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently unpaired multi-domain image-to-image translation has attracted great\ninterests and obtained remarkable progress, where a label vector is utilized to\nindicate multi-domain information. In this paper, we propose SAT (Show, Attend\nand Translate), an unified and explainable generative adversarial network\nequipped with visual attention that can perform unpaired image-to-image\ntranslation for multiple domains. By introducing an action vector, we treat the\noriginal translation tasks as problems of arithmetic addition and subtraction.\nVisual attention is applied to guarantee that only the regions relevant to the\ntarget domains are translated. Extensive experiments on a facial attribute\ndataset demonstrate the superiority of our approach and the generated attention\nmasks better explain what SAT attends when translating images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:37:52 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 09:09:15 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Honglun", ""], ["Chen", "Wenqing", ""], ["Tian", "Jidong", ""], ["Wang", "Yongkun", ""], ["Jin", "Yaohui", ""]]}, {"id": "1811.07484", "submitter": "Kuan-Chuan Peng", "authors": "Lezi Wang, Ziyan Wu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram\n  Singh, Bo Liu, Dimitris N. Metaxas", "title": "Sharpen Focus: Learning with Attention Separability and Consistency", "comments": "This paper is accepted to ICCV 2019. The supplementary material\n  (appendix) can be found after the main paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in gradient-based attention modeling have seen attention\nmaps emerge as a powerful tool for interpreting convolutional neural networks.\nDespite good localization for an individual class of interest, these techniques\nproduce attention maps with substantially overlapping responses among different\nclasses, leading to the problem of visual confusion and the need for\ndiscriminative attention. In this paper, we address this problem by means of a\nnew framework that makes class-discriminative attention a principled part of\nthe learning process. Our key innovations include new learning objectives for\nattention separability and cross-layer consistency, which result in improved\nattention discriminability and reduced visual confusion. Extensive experiments\non image classification benchmarks show the effectiveness of our approach in\nterms of improved classification accuracy, including CIFAR-100 (+3.33%),\nCaltech-256 (+1.64%), ILSVRC2012 (+0.92%), CUB-200-2011 (+4.8%) and PASCAL\nVOC2012 (+5.73%).\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:49:19 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:41:43 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 21:10:26 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wang", "Lezi", ""], ["Wu", "Ziyan", ""], ["Karanam", "Srikrishna", ""], ["Peng", "Kuan-Chuan", ""], ["Singh", "Rajat Vikram", ""], ["Liu", "Bo", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1811.07485", "submitter": "Chenchen Li", "authors": "Chenchen Li, Jialin Wang, Hongwei Wang, Miao Zhao, Wenjie Li, Xiaotie\n  Deng", "title": "Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural\n  Networks", "comments": "Draft, 25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User emotion analysis toward videos is to automatically recognize the general\nemotional status of viewers from the multimedia content embedded in the online\nvideo stream. Existing works fall in two categories: 1) visual-based methods,\nwhich focus on visual content and extract a specific set of features of videos.\nHowever, it is generally hard to learn a mapping function from low-level video\npixels to high-level emotion space due to great intra-class variance. 2)\ntextual-based methods, which focus on the investigation of user-generated\ncomments associated with videos. The learned word representations by\ntraditional linguistic approaches typically lack emotion information and the\nglobal comments usually reflect viewers' high-level understandings rather than\ninstantaneous emotions. To address these limitations, in this paper, we propose\nto jointly utilize video content and user-generated texts simultaneously for\nemotion analysis. In particular, we introduce exploiting a new type of\nuser-generated texts, i.e., \"danmu\", which are real-time comments floating on\nthe video and contain rich information to convey viewers' emotional opinions.\nTo enhance the emotion discriminativeness of words in textual feature\nextraction, we propose Emotional Word Embedding (EWE) to learn text\nrepresentations by jointly considering their semantics and emotions.\nAfterwards, we propose a novel visual-textual emotion analysis model with Deep\nCoupled Video and Danmu Neural networks (DCVDN), in which visual and textual\nfeatures are synchronously extracted and fused to form a comprehensive\nrepresentation by deep-canonically-correlated-autoencoder-based multi-view\nlearning. Through extensive experiments on a self-crawled real-world\nvideo-danmu dataset, we prove that DCVDN significantly outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:51:19 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Chenchen", ""], ["Wang", "Jialin", ""], ["Wang", "Hongwei", ""], ["Zhao", "Miao", ""], ["Li", "Wenjie", ""], ["Deng", "Xiaotie", ""]]}, {"id": "1811.07487", "submitter": "Srikrishna Karanam", "authors": "Meng Zheng, Srikrishna Karanam, Ziyan Wu, Richard J. Radke", "title": "Re-Identification with Consistent Attentive Siamese Networks", "comments": "10 pages, 8 figures, 3 tables, to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep architecture for person re-identification (re-id).\nWhile re-id has seen much recent progress, spatial localization and\nview-invariant representation learning for robust cross-view matching remain\nkey, unsolved problems. We address these questions by means of a new\nattention-driven Siamese learning architecture, called the Consistent Attentive\nSiamese Network. Our key innovations compared to existing, competing methods\ninclude (a) a flexible framework design that produces attention with only\nidentity labels as supervision, (b) explicit mechanisms to enforce attention\nconsistency among images of the same person, and (c) a new Siamese framework\nthat integrates attention and attention consistency, producing principled\nsupervisory signals as well as the first mechanism that can explain the\nreasoning behind the Siamese framework's predictions. We conduct extensive\nevaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and\nreport competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:59:51 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 17:07:25 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 18:37:34 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 14:25:28 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Radke", "Richard J.", ""]]}, {"id": "1811.07488", "submitter": "Seunghwan Cha", "authors": "Seunghwan Cha, James Ainooson, Maithilee Kunda", "title": "Quantifying Human Behavior on the Block Design Test Through Automated\n  Multi-Level Analysis of Overhead Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block design test is a standardized, widely used neuropsychological\nassessment of visuospatial reasoning that involves a person recreating a series\nof given designs out of a set of colored blocks. In current testing procedures,\nan expert neuropsychologist observes a person's accuracy and completion time as\nwell as overall impressions of the person's problem-solving procedures, errors,\netc., thus obtaining a holistic though subjective and often qualitative view of\nthe person's cognitive processes. We propose a new framework that combines room\nsensors and AI techniques to augment the information available to\nneuropsychologists from block design and similar tabletop assessments. In\nparticular, a ceiling-mounted camera captures an overhead view of the table\nsurface. From this video, we demonstrate how automated classification using\nmachine learning can produce a frame-level description of the state of the\nblock task and the person's actions over the course of each test problem. We\nalso show how a sequence-comparison algorithm can classify one individual's\nproblem-solving strategy relative to a database of simulated strategies, and\nhow these quantitative results can be visualized for use by neuropsychologists.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:03:03 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Cha", "Seunghwan", ""], ["Ainooson", "James", ""], ["Kunda", "Maithilee", ""]]}, {"id": "1811.07491", "submitter": "Yushan Feng", "authors": "Yushan Feng, Huitong Pan, Craig Meyer and Xue Feng", "title": "A Self-Adaptive Network For Multiple Sclerosis Lesion Segmentation From\n  Multi-Contrast MRI With Various Imaging Protocols", "comments": "This paper is submitted to IEEE ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have shown promises in the lesion segmentation of\nmultiple sclerosis (MS) from multicontrast MRI including T1, T2, proton density\n(PD) and FLAIR sequences. However, one challenge in deploying such networks\ninto clinical practice is the variability of imaging protocols, which often\ndiffer from the training dataset as certain MRI sequences may be unavailable or\nunusable. Therefore, trained networks need to adapt to practical situations\nwhen imaging protocols are different in deployment. In this paper, we propose a\nDNN-based MS lesion segmentation framework with a novel technique called\nsequence dropout which can adapt to various combinations of input MRI sequences\nduring deployment and achieve the maximal possible performance from the given\ninput. In addition, with this framework, we studied the quantitative impact of\neach MRI sequence on the MS lesion segmentation task without training separate\nnetworks. Experiments were performed using the IEEE ISBI 2015 Longitudinal MS\nLesion Challenge dataset and our method is currently ranked 2nd with a Dice\nsimilarity coefficient of 0.684. Furthermore, we showed our network achieved\nthe maximal possible performance when one sequence is unavailable during\ndeployment by comparing with separate networks trained on the corresponding\ninput MRI sequences. In particular, we discovered T1 and PD have minor impact\non segmentation performance while FLAIR is the predominant sequence.\nExperiments with multiple missing sequences were also performed and showed the\nrobustness of our network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:18:57 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Feng", "Yushan", ""], ["Pan", "Huitong", ""], ["Meyer", "Craig", ""], ["Feng", "Xue", ""]]}, {"id": "1811.07492", "submitter": "Yifan Peng", "authors": "Yifan Peng, Shazia Dharssi, Qingyu Chen, Tiarnan D. Keenan, Elvira\n  Agr\\'on, Wai T. Wong, Emily Y. Chew, Zhiyong Lu", "title": "DeepSeeNet: A deep learning model for automated classification of\n  patient-based age-related macular degeneration severity from color fundus\n  photographs", "comments": "Accepted for publication in Ophthalmology", "journal-ref": "Ophthalmology. 2018 Nov 22. pii: S0161-6420(18)32185-7", "doi": "10.1016/j.ophtha.2018.11.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In assessing the severity of age-related macular degeneration (AMD), the\nAge-Related Eye Disease Study (AREDS) Simplified Severity Scale predicts the\nrisk of progression to late AMD. However, its manual use requires the\ntime-consuming participation of expert practitioners. Although several\nautomated deep learning systems have been developed for classifying color\nfundus photographs (CFP) of individual eyes by AREDS severity score, none to\ndate has used a patient-based scoring system that uses images from both eyes to\nassign a severity score. DeepSeeNet, a deep learning model, was developed to\nclassify patients automatically by the AREDS Simplified Severity Scale (score\n0-5) using bilateral CFP. DeepSeeNet was trained on 58,402 and tested on 900\nimages from the longitudinal follow-up of 4549 participants from AREDS. Gold\nstandard labels were obtained using reading center grades. DeepSeeNet simulates\nthe human grading process by first detecting individual AMD risk factors\n(drusen size, pigmentary abnormalities) for each eye and then calculating a\npatient-based AMD severity score using the AREDS Simplified Severity Scale.\nDeepSeeNet performed better on patient-based classification (accuracy = 0.671;\nkappa = 0.558) than retinal specialists (accuracy = 0.599; kappa = 0.467) with\nhigh AUC in the detection of large drusen (0.94), pigmentary abnormalities\n(0.93), and late AMD (0.97). DeepSeeNet demonstrated high accuracy with\nincreased transparency in the automated assignment of individual patients to\nAMD risk categories based on the AREDS Simplified Severity Scale. These results\nhighlight the potential of deep learning to assist and enhance clinical\ndecision-making in patients with AMD, such as early AMD detection and risk\nprediction for developing late AMD. DeepSeeNet is publicly available on\nhttps://github.com/ncbi-nlp/DeepSeeNet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:19:34 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 17:34:07 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Peng", "Yifan", ""], ["Dharssi", "Shazia", ""], ["Chen", "Qingyu", ""], ["Keenan", "Tiarnan D.", ""], ["Agr\u00f3n", "Elvira", ""], ["Wong", "Wai T.", ""], ["Chew", "Emily Y.", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1811.07493", "submitter": "Gurjeet Singh", "authors": "Gurjeet Singh, Sun Miao, Shi Shi and Patrick Chiang", "title": "FotonNet: A HW-Efficient Object Detection System Using 3D-Depth\n  Segmentation and 2D-DNN Classifier", "comments": "7 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection and classification is one of the most important computer\nvision problems. Ever since the introduction of deep learning\n\\cite{krizhevsky2012imagenet}, we have witnessed a dramatic increase in the\naccuracy of this object detection problem. However, most of these improvements\nhave occurred using conventional 2D image processing. Recently, low-cost\n3D-image sensors, such as the Microsoft Kinect (Time-of-Flight) or the Apple\nFaceID (Structured-Light), can provide 3D-depth or point cloud data that can be\nadded to a convolutional neural network, acting as an extra set of dimensions.\nIn our proposed approach, we introduce a new 2D + 3D system that takes the\n3D-data to determine the object region followed by any conventional 2D-DNN,\nsuch as AlexNet. In this method, our approach can easily dissociate the\ninformation collection from the Point Cloud and 2D-Image data and combine both\noperations later. Hence, our system can use any existing trained 2D network on\na large image dataset, and does not require a large 3D-depth dataset for new\ntraining. Experimental object detection results across 30 images show an\naccuracy of 0.67, versus 0.54 and 0.51 for RCNN and YOLO, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:31:29 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Singh", "Gurjeet", ""], ["Miao", "Sun", ""], ["Shi", "Shi", ""], ["Chiang", "Patrick", ""]]}, {"id": "1811.07498", "submitter": "Peng Zhang", "authors": "Peng Zhang, Shujian Yu, Jiamiao Xu, Xinge You, Xiubao Jiang, Xiao-Yuan\n  Jing, and Dacheng Tao", "title": "Robust Visual Tracking using Multi-Frame Multi-Feature Joint Modeling", "comments": "This paper has been accepted by IEEE Transactions on Circuits and\n  Systems for Video Technology. The MATLAB code of our method is available from\n  our project homepage http://bmal.hust.edu.cn/project/KMF2JMTtracking.html", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2882339", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains a huge challenge to design effective and efficient trackers under\ncomplex scenarios, including occlusions, illumination changes and pose\nvariations. To cope with this problem, a promising solution is to integrate the\ntemporal consistency across consecutive frames and multiple feature cues in a\nunified model. Motivated by this idea, we propose a novel correlation\nfilter-based tracker in this work, in which the temporal relatedness is\nreconciled under a multi-task learning framework and the multiple feature cues\nare modeled using a multi-view learning approach. We demonstrate the resulting\nregression model can be efficiently learned by exploiting the structure of\nblockwise diagonal matrix. A fast blockwise diagonal matrix inversion algorithm\nis developed thereafter for efficient online tracking. Meanwhile, we\nincorporate an adaptive scale estimation mechanism to strengthen the stability\nof scale variation tracking. We implement our tracker using two types of\nfeatures and test it on two benchmark datasets. Experimental results\ndemonstrate the superiority of our proposed approach when compared with other\nstate-of-the-art trackers. project homepage\nhttp://bmal.hust.edu.cn/project/KMF2JMTtracking.html\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:44:00 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Zhang", "Peng", ""], ["Yu", "Shujian", ""], ["Xu", "Jiamiao", ""], ["You", "Xinge", ""], ["Jiang", "Xiubao", ""], ["Jing", "Xiao-Yuan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1811.07502", "submitter": "Shivanthan Yohanandan", "authors": "Shivanthan Yohanandan, Andy Song, Adrian G. Dyer, Angela Faragasso,\n  Subhrajit Roy and Dacheng Tao", "title": "Fast Efficient Object Detection Using Selective Attention", "comments": "Retraction due to significant oversight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retraction due to significant oversight\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 05:07:50 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 01:19:49 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 01:38:30 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Yohanandan", "Shivanthan", ""], ["Song", "Andy", ""], ["Dyer", "Adrian G.", ""], ["Faragasso", "Angela", ""], ["Roy", "Subhrajit", ""], ["Tao", "Dacheng", ""]]}, {"id": "1811.07503", "submitter": "Yu Pan", "authors": "Yu Pan, Jing Xu, Maolin Wang, Jinmian Ye, Fei Wang, Kun Bai, Zenglin\n  Xu", "title": "Compressing Recurrent Neural Networks with Tensor Ring for Action\n  Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term\nMemory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved\npromising performance in sequential data modeling. The hidden layers in RNNs\ncan be regarded as the memory units, which are helpful in storing information\nin sequential contexts. However, when dealing with high dimensional input data,\nsuch as video and text, the input-to-hidden linear transformation in RNNs\nbrings high memory usage and huge computational cost. This makes the training\nof RNNs unscalable and difficult. To address this challenge, we propose a novel\ncompact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring\ndecomposition (TRD) to reformulate the input-to-hidden transformation. Compared\nwith other tensor decomposition methods, TR-LSTM is more stable. In addition,\nTR-LSTM can complete an end-to-end training and also provide a fundamental\nbuilding block for RNNs in handling large input data. Experiments on real-world\naction recognition datasets have demonstrated the promising performance of the\nproposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art\ncompetitors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 05:10:14 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pan", "Yu", ""], ["Xu", "Jing", ""], ["Wang", "Maolin", ""], ["Ye", "Jinmian", ""], ["Wang", "Fei", ""], ["Bai", "Kun", ""], ["Xu", "Zenglin", ""]]}, {"id": "1811.07516", "submitter": "Rahma Fourati", "authors": "Rahma Fourati, Boudour Ammar, Javier Sanchez-Medina and Adel M. Alimi", "title": "Unsupervised Learning in Reservoir Computing for EEG-based Emotion\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications such as emotion recognition from recorded brain\nactivity, data are captured from electrodes over time. These signals constitute\na multidimensional time series. In this paper, Echo State Network (ESN), a\nrecurrent neural network with a great success in time series prediction and\nclassification, is optimized with different neural plasticity rules for\nclassification of emotions based on electroencephalogram (EEG) time series.\nActually, the neural plasticity rules are a kind of unsupervised learning\nadapted for the reservoir, i.e. the hidden layer of ESN. More specifically, an\ninvestigation of Oja's rule, BCM rule and gaussian intrinsic plasticity rule\nwas carried out in the context of EEG-based emotion recognition. The study,\nalso, includes a comparison of the offline and online training of the ESN. When\ntesting on the well-known affective benchmark \"DEAP dataset\" which contains EEG\nsignals from 32 subjects, we find that pretraining ESN with gaussian intrinsic\nplasticity enhanced the classification accuracy and outperformed the results\nachieved with an ESN pretrained with synaptic plasticity. Four classification\nproblems were conducted in which the system complexity is increased and the\ndiscrimination is more challenging, i.e. inter-subject emotion discrimination.\nOur proposed method achieves higher performance over the state of the art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:07:33 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 03:24:25 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Fourati", "Rahma", ""], ["Ammar", "Boudour", ""], ["Sanchez-Medina", "Javier", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1811.07519", "submitter": "Kai Hu", "authors": "Kai Hu, Bhiksha Raj", "title": "Higher-order Network for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing spatiotemporal dynamics is an essential topic in video recognition.\nIn this paper, we present learnable higher-order operations as a generic family\nof building blocks for capturing spatiotemporal dynamics from RGB input video\nspace. Similar to higher-order functions, the weights of higher-order\noperations are themselves derived from the data with learnable parameters.\nClassical architectures such as residual learning and network-in-network are\nfirst-order operations where weights are directly learned from the data.\nHigher-order operations make it easier to capture context-sensitive patterns,\nsuch as motion. Self-attention models are also higher-order operations, but the\nattention weights are mostly computed from an affine operation or dot product.\nLearnable higher-order operations can be more generic and flexible.\nExperimentally, we show that on the task of video recognition, our higher-order\nmodels can achieve results on par with or better than the existing\nstate-of-the-art methods on Something-Something (V1 and V2), Kinetics and\nCharades datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:22:50 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 20:05:22 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 18:50:30 GMT"}, {"version": "v4", "created": "Tue, 19 Nov 2019 02:13:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Hu", "Kai", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1811.07542", "submitter": "Jean Stawiaski", "authors": "Jean Stawiaski", "title": "A Pretrained DenseNet Encoder for Brain Tumor Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1710.02316", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a convolutional neural network for the automatic\nsegmentation of brain tumors in multimodal 3D MR images based on a U-net\narchitecture.We evaluate the use of a densely connected convolutional network\nencoder (DenseNet) which was pretrained on the ImageNet data set. We detail two\nnetwork architectures that can take into account multiple 3D images as inputs.\nThis work aims to identify if a generic pretrained network can be used for very\nspecific medical applications where the target data differ both in the number\nof spatial dimensions as well as in the number of inputs channels. Moreover in\norder to regularize this transfer learning task we only train the decoder part\nof the U-net architecture. We evaluate the effectiveness of the proposed\napproach on the BRATS 2018 segmentation challenge where we obtained dice scores\nof 0.79, 0.90, 0.85 and 95/% Hausdorff distance of 2.9mm, 3.95mm, and 6.48mm\nfor enhanced tumor core, whole tumor and tumor core respectively on the\nvalidation set. This scores degrades to 0.77, 0.88, 0.78 and 95 /% Hausdorff\ndistance of 3.6mm, 5.72mm, and 5.83mm on the testing set.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:00:22 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Stawiaski", "Jean", ""]]}, {"id": "1811.07544", "submitter": "Jiawei Liu", "authors": "Jiawei Liu and Zheng-Jun Zha and Hongtao Xie and Zhiwei Xiong and\n  Yongdong Zhang", "title": "CA3Net: Contextual-Attentional Attribute-Appearance Network for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to identify the same pedestrian across\nnon-overlapping camera views. Deep learning techniques have been applied for\nperson re-identification recently, towards learning representation of\npedestrian appearance. This paper presents a novel Contextual-Attentional\nAttribute-Appearance Network (CA3Net) for person re-identification. The CA3Net\nsimultaneously exploits the complementarity between semantic attributes and\nvisual appearance, the semantic context among attributes, visual attention on\nattributes as well as spatial dependencies among body parts, leading to\ndiscriminative and robust pedestrian representation. Specifically, an attribute\nnetwork within CA3Net is designed with an Attention-LSTM module. It\nconcentrates the network on latent image regions related to each attribute as\nwell as exploits the semantic context among attributes by a LSTM module. An\nappearance network is developed to learn appearance features from the full\nbody, horizontal and vertical body parts of pedestrians with spatial\ndependencies among body parts. The CA3Net jointly learns the attribute and\nappearance features in a multi-task learning manner, generating comprehensive\nrepresentation of pedestrians. Extensive experiments on two challenging\nbenchmarks, i.e., Market-1501 and DukeMTMC-reID datasets, have demonstrated the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:05:07 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liu", "Jiawei", ""], ["Zha", "Zheng-Jun", ""], ["Xie", "Hongtao", ""], ["Xiong", "Zhiwei", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1811.07548", "submitter": "Yuanliu Liu", "authors": "Yuanliu Liu, Bo Peng, Peipei Shi, He Yan, Yong Zhou, Bing Han, Yi\n  Zheng, Chao Lin, Jianbin Jiang, Yin Fan, Tingwei Gao, Ganwen Wang, Jian Liu,\n  Xiangju Lu, Danming Xie", "title": "iQIYI-VID: A Large Dataset for Multi-modal Person Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person identification in the wild is very challenging due to great variation\nin poses, face quality, clothes, makeup and so on. Traditional research, such\nas face recognition, person re-identification, and speaker recognition, often\nfocuses on a single modal of information, which is inadequate to handle all the\nsituations in practice. Multi-modal person identification is a more promising\nway that we can jointly utilize face, head, body, audio features, and so on. In\nthis paper, we introduce iQIYI-VID, the largest video dataset for multi-modal\nperson identification. It is composed of 600K video clips of 5,000 celebrities.\nThese video clips are extracted from 400K hours of online videos of various\ntypes, ranging from movies, variety shows, TV series, to news broadcasting. All\nvideo clips pass through a careful human annotation process, and the error rate\nof labels is lower than 0.2\\%. We evaluated the state-of-art models of face\nrecognition, person re-identification, and speaker recognition on the iQIYI-VID\ndataset. Experimental results show that these models are still far from being\nperfect for the task of person identification in the wild. We proposed a\nMulti-modal Attention module to fuse multi-modal features that can improve\nperson identification considerably. We have released the dataset online to\npromote multi-modal person identification research.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:16:42 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 05:41:27 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Yuanliu", ""], ["Peng", "Bo", ""], ["Shi", "Peipei", ""], ["Yan", "He", ""], ["Zhou", "Yong", ""], ["Han", "Bing", ""], ["Zheng", "Yi", ""], ["Lin", "Chao", ""], ["Jiang", "Jianbin", ""], ["Fan", "Yin", ""], ["Gao", "Tingwei", ""], ["Wang", "Ganwen", ""], ["Liu", "Jian", ""], ["Lu", "Xiangju", ""], ["Xie", "Danming", ""]]}, {"id": "1811.07555", "submitter": "Yuxin Zhang", "authors": "Yuxin Zhang, Huan Wang, Yang Luo, Lu Yu, Haoji Hu, Hangguan Shan, Tony\n  Q. S. Quek", "title": "Three Dimensional Convolutional Neural Network Pruning with\n  Regularization-Based Method", "comments": "ICIP 2019", "journal-ref": "ICIP 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite enjoying extensive applications in video analysis, three-dimensional\nconvolutional neural networks (3D CNNs)are restricted by their massive\ncomputation and storage consumption. To solve this problem, we propose a\nthreedimensional regularization-based neural network pruning method to assign\ndifferent regularization parameters to different weight groups based on their\nimportance to the network. Further we analyze the redundancy and computation\ncost for each layer to determine the different pruning ratios. Experiments show\nthat pruning based on our method can lead to 2x theoretical speedup with only\n0.41% accuracy loss for 3DResNet18 and 3.28% accuracy loss for C3D. The\nproposed method performs favorably against other popular methods for model\ncompression and acceleration.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:40:00 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 03:48:09 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhang", "Yuxin", ""], ["Wang", "Huan", ""], ["Luo", "Yang", ""], ["Yu", "Lu", ""], ["Hu", "Haoji", ""], ["Shan", "Hangguan", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "1811.07567", "submitter": "Li Niu", "authors": "Li Niu, Ashok Veeraraghavan, Ashu Sabharwal", "title": "Fine-grained Classification using Heterogeneous Web Data and Auxiliary\n  Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification remains a very challenging problem, because of\nthe absence of well-labeled training data caused by the high cost of annotating\na large number of fine-grained categories. In the extreme case, given a set of\ntest categories without any well-labeled training data, the majority of\nexisting works can be grouped into the following two research directions: 1)\ncrawl noisy labeled web data for the test categories as training data, which is\ndubbed as webly supervised learning; 2) transfer the knowledge from auxiliary\ncategories with well-labeled training data to the test categories, which\ncorresponds to zero-shot learning setting. Nevertheless, the above two research\ndirections still have critical issues to be addressed. For the first direction,\nweb data have noisy labels and considerably different data distribution from\ntest data. For the second direction, zero-shot learning is struggling to\nachieve compelling results compared with conventional supervised learning. The\nissues of the above two directions motivate us to develop a novel approach\nwhich can jointly exploit both noisy web training data from test categories and\nwell-labeled training data from auxiliary categories. In particular, on one\nhand, we crawl web data for test categories as noisy training data. On the\nother hand, we transfer the knowledge from auxiliary categories with\nwell-labeled training data to test categories by virtue of free semantic\ninformation (e.g., word vector) of all categories. Moreover, given the fact\nthat web data are generally associated with additional textual information\n(e.g., title and tag), we extend our method by using the surrounding textual\ninformation of web data as privileged information. Extensive experiments show\nthe effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 09:28:15 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Niu", "Li", ""], ["Veeraraghavan", "Ashok", ""], ["Sabharwal", "Ashu", ""]]}, {"id": "1811.07583", "submitter": "Jaime Spencer Martin Mr.", "authors": "Jaime Spencer, Oscar Mendez, Richard Bowden, Simon Hadfield", "title": "Localisation via Deep Imagination: learn the features not the map", "comments": "VNAD @ ECCV2018", "journal-ref": null, "doi": "10.1007/978-3-030-11021-5_44", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many times does a human have to drive through the same area to become\nfamiliar with it? To begin with, we might first build a mental model of our\nsurroundings. Upon revisiting this area, we can use this model to extrapolate\nto new unseen locations and imagine their appearance. Based on this, we propose\nan approach where an agent is capable of modelling new environments after a\nsingle visitation. To this end, we introduce \"Deep Imagination\", a combination\nof classical Visual-based Monte Carlo Localisation and deep learning. By making\nuse of a feature embedded 3D map, the system can \"imagine\" the view from any\nnovel location. These \"imagined\" views are contrasted with the current\nobservation in order to estimate the agent's current location. In order to\nbuild the embedded map, we train a deep Siamese Fully Convolutional U-Net to\nperform dense feature extraction. By training these features to be generic, no\nadditional training or fine tuning is required to adapt to new environments.\nOur results demonstrate the generality and transfer capability of our learnt\ndense features by training and evaluating on multiple datasets. Additionally,\nwe include several visualizations of the feature representations and resulting\n3D maps, as well as their application to localisation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 09:52:34 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Spencer", "Jaime", ""], ["Mendez", "Oscar", ""], ["Bowden", "Richard", ""], ["Hadfield", "Simon", ""]]}, {"id": "1811.07598", "submitter": "Xu Lan", "authors": "Xu Lan, Xiatian Zhu, Shaogang Gong", "title": "Self-Referenced Deep Learning", "comments": "To Appear in Asian Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is an effective approach to transferring knowledge\nfrom a teacher neural network to a student target network for satisfying the\nlow-memory and fast running requirements in practice use. Whilst being able to\ncreate stronger target networks compared to the vanilla non-teacher based\nlearning strategy, this scheme needs to train additionally a large teacher\nmodel with expensive computational cost. In this work, we present a\nSelf-Referenced Deep Learning (SRDL) strategy. Unlike both vanilla optimisation\nand existing knowledge distillation, SRDL distils the knowledge discovered by\nthe in-training target model back to itself to regularise the subsequent\nlearning procedure therefore eliminating the need for training a large teacher\nmodel. SRDL improves the model generalisation performance compared to vanilla\nlearning and conventional knowledge distillation approaches with negligible\nextra computational cost. Extensive evaluations show that a variety of deep\nnetworks benefit from SRDL resulting in enhanced deployment performance on both\ncoarse-grained object categorisation tasks (CIFAR10, CIFAR100, Tiny ImageNet,\nand ImageNet) and fine-grained person instance identification tasks\n(Market-1501).\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:41:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lan", "Xu", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1811.07605", "submitter": "Maciej Zamorski", "authors": "Maciej Zamorski, Maciej Zi\\k{e}ba, Piotr Klukowski, Rafa{\\l} Nowak,\n  Karol Kurach, Wojciech Stokowiec, Tomasz Trzci\\'nski", "title": "Adversarial Autoencoders for Compact Representations of 3D Point Clouds", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative architectures provide a way to model not only images but also\ncomplex, 3-dimensional objects, such as point clouds. In this work, we present\na novel method to obtain meaningful representations of 3D shapes that can be\nused for challenging tasks including 3D points generation, reconstruction,\ncompression, and clustering. Contrary to existing methods for 3D point cloud\ngeneration that train separate decoupled models for representation learning and\ngeneration, our approach is the first end-to-end solution that allows to\nsimultaneously learn a latent space of representation and generate 3D shape out\nof it. Moreover, our model is capable of learning meaningful compact binary\ndescriptors with adversarial training conducted on a latent space. To achieve\nthis goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D\ninput and create 3D output. Thanks to our end-to-end training regime, the\nresulting method called 3D Adversarial Autoencoder (3dAAE) obtains either\nbinary or continuous latent space that covers a much wider portion of training\ndata distribution. Finally, our quantitative evaluation shows that 3dAAE\nprovides state-of-the-art results for 3D points clustering and 3D object\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:51:09 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 18:00:49 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 19:22:36 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Klukowski", "Piotr", ""], ["Nowak", "Rafa\u0142", ""], ["Kurach", "Karol", ""], ["Stokowiec", "Wojciech", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "1811.07619", "submitter": "Jian Xu", "authors": "Jian Xu, Chunheng Wang, Cunzhao Shi, and Baihua Xiao", "title": "Adversarial Soft-detection-based Aggregation Network for Image Retrieval", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent year, the compact representations based on activations of\nConvolutional Neural Network (CNN) achieve remarkable performance in image\nretrieval. However, retrieval of some interested object that only takes up a\nsmall part of the whole image is still a challenging problem. Therefore, it is\nsignificant to extract the discriminative representations that contain regional\ninformation of the pivotal small object. In this paper, we propose a novel\nadversarial soft-detection-based aggregation (ASDA) method free from bounding\nbox annotations for image retrieval, based on adversarial detector and soft\nregion proposal layer. Our trainable adversarial detector generates semantic\nmaps based on adversarial erasing strategy to preserve more discriminative and\ndetailed information. Computed based on semantic maps corresponding to various\ndiscriminative patterns and semantic contents, our soft region proposal is\narbitrary shape rather than only rectangle and it reflects the significance of\nobjects. The aggregation based on trainable soft region proposal highlights\ndiscriminative semantic contents and suppresses the noise of background.\n  We conduct comprehensive experiments on standard image retrieval datasets.\nOur weakly supervised ASDA method achieves state-of-the-art performance on most\ndatasets. The results demonstrate that the proposed ASDA method is effective\nfor image retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:22:37 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 07:22:31 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 05:25:55 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Xu", "Jian", ""], ["Wang", "Chunheng", ""], ["Shi", "Cunzhao", ""], ["Xiao", "Baihua", ""]]}, {"id": "1811.07626", "submitter": "Xiao-Bo Jin", "authors": "Xiao-Bo Jin, Kai-Zhu Huang and Jianyu Miao", "title": "Beyond Attributes: Adversarial Erasing Embedding Network for Zero-shot\n  Learning", "comments": "12 pages, 10 figures,working report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an adversarial erasing embedding network with the guidance of\nhigh-order attributes (AEEN-HOA) is proposed for going further to solve the\nchallenging ZSL/GZSL task. AEEN-HOA consists of two branches, i.e., the upper\nstream is capable of erasing some initially discovered regions, then the\nhigh-order attribute supervision is incorporated to characterize the\nrelationship between the class attributes. Meanwhile, the bottom stream is\ntrained by taking the current background regions to train the same attribute.\nAs far as we know, it is the first time of introducing the erasing operations\ninto the ZSL task. In addition, we first propose a class attribute activation\nmap for the visualization of ZSL output, which shows the relationship between\nclass attribute feature and attention map. Experiments on four standard\nbenchmark datasets demonstrate the superiority of AEEN-HOA framework.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:39:02 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 07:33:09 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Huang", "Kai-Zhu", ""], ["Miao", "Jianyu", ""]]}, {"id": "1811.07628", "submitter": "Goutam Bhat", "authors": "Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg", "title": "ATOM: Accurate Tracking by Overlap Maximization", "comments": "CVPR 2019 (Oral). Complete code and models are available at\n  https://github.com/visionml/pytracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent years have witnessed astonishing improvements in visual tracking\nrobustness, the advancements in tracking accuracy have been limited. As the\nfocus has been directed towards the development of powerful classifiers, the\nproblem of accurate target state estimation has been largely overlooked. In\nfact, most trackers resort to a simple multi-scale search in order to estimate\nthe target bounding box. We argue that this approach is fundamentally limited\nsince target estimation is a complex task, requiring high-level knowledge about\nthe object.\n  We address this problem by proposing a novel tracking architecture,\nconsisting of dedicated target estimation and classification components. High\nlevel knowledge is incorporated into the target estimation through extensive\noffline learning. Our target estimation component is trained to predict the\noverlap between the target object and an estimated bounding box. By carefully\nintegrating target-specific information, our approach achieves previously\nunseen bounding box accuracy. We further introduce a classification component\nthat is trained online to guarantee high discriminative power in the presence\nof distractors. Our final tracking framework sets a new state-of-the-art on\nfive challenging benchmarks. On the new large-scale TrackingNet dataset, our\ntracker ATOM achieves a relative gain of 15% over the previous best approach,\nwhile running at over 30 FPS. Code and models are available at\nhttps://github.com/visionml/pytracking.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:40:17 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 17:56:18 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Danelljan", "Martin", ""], ["Bhat", "Goutam", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1811.07630", "submitter": "Elizaveta Logacheva", "authors": "Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva, Oleg Khomenko,\n  Sergey I. Nikolenko", "title": "SEIGAN: Towards Compositional Image Generation by Simultaneously\n  Learning to Segment, Enhance, and Inpaint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to image manipulation and understanding by\nsimultaneously learning to segment object masks, paste objects to another\nbackground image, and remove them from original images. For this purpose, we\ndevelop a novel generative model for compositional image generation, SEIGAN\n(Segment-Enhance-Inpaint Generative Adversarial Network), which learns these\nthree operations together in an adversarial architecture with additional cycle\nconsistency losses. To train, SEIGAN needs only bounding box supervision and\ndoes not require pairing or ground truth masks. SEIGAN produces better\ngenerated images (evaluated by human assessors) than other approaches and\nproduces high-quality segmentation masks, improving over other adversarially\ntrained approaches and getting closer to the results of fully supervised\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:50:20 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:33:07 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ostyakov", "Pavel", ""], ["Suvorov", "Roman", ""], ["Logacheva", "Elizaveta", ""], ["Khomenko", "Oleg", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1811.07632", "submitter": "Louis Gallagher", "authors": "Louis Gallagher and John B. McDonald", "title": "Collaborative Dense SLAM", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new system for live collaborative dense surface\nreconstruction. Cooperative robotics, multi participant augmented reality and\nhuman-robot interaction are all examples of situations where collaborative\nmapping can be leveraged for greater agent autonomy. Our system builds on\nElasticFusion to allow a number of cameras starting with unknown initial\nrelative positions to maintain local maps utilising the original algorithm.\nCarrying out visual place recognition across these local maps the system can\nidentify when two maps overlap in space, providing an inter-map constraint from\nwhich the system can derive the relative poses of the two maps. Using these\nresulting pose constraints, our system performs map merging, allowing multiple\ncameras to fuse their measurements into a single shared reconstruction. The\nadvantage of this approach is that it avoids replication of structures\nsubsequent to loop closures, where multiple cameras traverse the same regions\nof the environment. Furthermore, it allows cameras to directly exploit and\nupdate regions of the environment previously mapped by other cameras within the\nsystem. We provide both quantitative and qualitative analyses using the\nsynthetic ICL-NUIM dataset and the real-world Freiburg dataset including the\nimpact of multi-camera mapping on surface reconstruction accuracy, camera pose\nestimation accuracy and overall processing time. We also include qualitative\nresults in the form of sample reconstructions of room sized environments with\nup to 3 cameras undergoing intersecting and loopy trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:54:40 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 12:12:59 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Gallagher", "Louis", ""], ["McDonald", "John B.", ""]]}, {"id": "1811.07640", "submitter": "Ioannis Ivrissimtzis", "authors": "Xin Zhang and Qian Wang and Toby Breckon and Ioannis Ivrissimtzis", "title": "Watermark Retrieval from 3D Printed Objects via Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for reading digital data embedded in planar 3D printed\nsurfaces. The data are organised in binary arrays and embedded as surface\ntextures in a way inspired by QR codes. At the core of the retrieval method\nlies a Convolutional Neural Network, outputting a confidence map of the\nlocation of the surface textures encoding value 1 bits. Subsequently, the bit\narray is retrieved through a series of simple image processing and statistical\noperations applied on the confidence map. Extensive experimentation with images\ncaptured from various camera views, under various illumination conditions and\nfrom objects printed with various material colours, shows that the proposed\nmethod generalizes well and achieves the level of accuracy required in\npractical applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 12:20:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhang", "Xin", ""], ["Wang", "Qian", ""], ["Breckon", "Toby", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "1811.07662", "submitter": "Yue Zheng", "authors": "Yue Zheng, Yali Li, Shengjin Wang", "title": "Intention Oriented Image Captions with Guiding Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although existing image caption models can produce promising results using\nrecurrent neural networks (RNNs), it is difficult to guarantee that an object\nwe care about is contained in generated descriptions, for example in the case\nthat the object is inconspicuous in the image. Problems become even harder when\nthese objects did not appear in training stage. In this paper, we propose a\nnovel approach for generating image captions with guiding objects (CGO). The\nCGO constrains the model to involve a human-concerned object when the object is\nin the image. CGO ensures that the object is in the generated description while\nmaintaining fluency. Instead of generating the sequence from left to right, we\nstart the description with a selected object and generate other parts of the\nsequence based on this object. To achieve this, we design a novel framework\ncombining two LSTMs in opposite directions. We demonstrate the characteristics\nof our method on MSCOCO where we generate descriptions for each detected object\nin the images. With CGO, we can extend the ability of description to the\nobjects being neglected in image caption labels and provide a set of more\ncomprehensive and diverse descriptions for an image. CGO shows advantages when\napplied to the task of describing novel objects. We show experimental results\non both MSCOCO and ImageNet datasets. Evaluations show that our method\noutperforms the state-of-the-art models in the task with average F1 75.8,\nleading to better descriptions in terms of both content accuracy and fluency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:12:07 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 08:36:21 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zheng", "Yue", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1811.07665", "submitter": "Fei Peng", "authors": "Fei Peng, Le-bing Zhang, Min Long", "title": "FD-GAN: Face-demorphing generative adversarial network for restoring\n  accomplice's facial image", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face morphing attack is proved to be a serious threat to the existing face\nrecognition systems. Although a few face morphing detection methods have been\nput forward, the face morphing accomplice's facial restoration remains a\nchallenging problem. In this paper, a face de-morphing generative adversarial\nnetwork (FD-GAN) is proposed to restore the accomplice's facial image. It\nutilizes a symmetric dual network architecture and two levels of restoration\nlosses to separate the identity feature of the morphing accomplice. By\nexploiting the captured facial image (containing the criminal's identity) from\nthe face recognition system and the morphed image stored in the e-passport\nsystem (containing both criminal and accomplice's identities), the FD-GAN can\neffectively restore the accomplice's facial image. Experimental results and\nanalysis demonstrate the effectiveness of the proposed scheme. It has great\npotential to be implemented for detecting the face morphing accomplice in a\nreal identity verification scenario.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:19:48 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 02:07:21 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Peng", "Fei", ""], ["Zhang", "Le-bing", ""], ["Long", "Min", ""]]}, {"id": "1811.07727", "submitter": "Ping Luo", "authors": "Ping Luo, Zhanglin Peng, Jiamin Ren, Ruimao Zhang", "title": "Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct?", "comments": "Preprint. Work in Progress. 14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, they do. This work investigates a perspective for deep learning: whether\ndifferent normalization layers in a ConvNet require different normalizers. This\nis the first step towards understanding this phenomenon. We allow each\nconvolutional layer to be stacked before a switchable normalization (SN) that\nlearns to choose a normalizer from a pool of normalization methods. Through\nsystematic experiments in ImageNet, COCO, Cityscapes, and ADE20K, we answer\nthree questions: (a) Is it useful to allow each normalization layer to select\nits own normalizer? (b) What impacts the choices of normalizers? (c) Do\ndifferent tasks and datasets prefer different normalizers? Our results suggest\nthat (1) using distinct normalizers improves both learning and generalization\nof a ConvNet; (2) the choices of normalizers are more related to depth and\nbatch size, but less relevant to parameter initialization, learning rate decay,\nand solver; (3) different tasks and datasets have different behaviors when\nlearning to select normalizers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:36:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Luo", "Ping", ""], ["Peng", "Zhanglin", ""], ["Ren", "Jiamin", ""], ["Zhang", "Ruimao", ""]]}, {"id": "1811.07738", "submitter": "Tim Laibacher", "authors": "Tim Laibacher, Tillman Weyde, Sepehr Jalali", "title": "M2U-Net: Effective and Efficient Retinal Vessel Segmentation for\n  Resource-Constrained Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel neural network architecture for retinal\nvessel segmentation that improves over the state of the art on two benchmark\ndatasets, is the first to run in real time on high resolution images, and its\nsmall memory and processing requirements make it deployable in mobile and\nembedded systems. The M2U-Net has a new encoder-decoder architecture that is\ninspired by the U-Net. It adds pretrained components of MobileNetV2 in the\nencoder part and novel contractive bottleneck blocks in the decoder part that,\ncombined with bilinear upsampling, drastically reduce the parameter count to\n0.55M compared to 31.03M in the original U-Net. We have evaluated its\nperformance against a wide body of previously published results on three public\ndatasets. On two of them, the M2U-Net achieves new state-of-the-art performance\nby a considerable margin. When implemented on a GPU, our method is the first to\nachieve real-time inference speeds on high-resolution fundus images. We also\nimplemented our proposed network on an ARM-based embedded system where it\nsegments images in between 0.6 and 15 sec, depending on the resolution. Thus,\nthe M2U-Net enables a number of applications of retinal vessel structure\nextraction, such as early diagnosis of eye diseases, retinal biometric\nauthentication systems, and robot assisted microsurgery.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:51:56 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 12:21:07 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 07:51:28 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Laibacher", "Tim", ""], ["Weyde", "Tillman", ""], ["Jalali", "Sepehr", ""]]}, {"id": "1811.07749", "submitter": "Stefan Schneider", "authors": "Stefan Schneider, Graham W. Taylor, Stefan S. Linquist, Stefan C.\n  Kremer", "title": "Past, Present, and Future Approaches Using Computer Vision for Animal\n  Re-Identification from Camera Trap Data", "comments": "25 pages, 1 picture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a researcher to re-identify (re-ID) an individual animal upon\nre-encounter is fundamental for addressing a broad range of questions in the\nstudy of ecosystem function, community and population dynamics, and behavioural\necology. In this review, we describe a brief history of camera traps for re-ID,\npresent a collection of computer vision feature engineering methodologies\npreviously used for animal re-ID, provide an introduction to the underlying\nmechanisms of deep learning relevant to animal re-ID, highlight the success of\ndeep learning methods for human re-ID, describe the few ecological studies\ncurrently utilizing deep learning for camera trap analyses, and our predictions\nfor near future methodologies based on the rapid development of deep learning\nmethods. By utilizing novel deep learning methods for object detection and\nsimilarity comparisons, ecologists can extract animals from an image/video data\nand train deep learning classifiers to re-ID animal individuals beyond the\ncapabilities of a human observer. This methodology will allow ecologists with\ncamera/video trap data to re-identify individuals that exit and re-enter the\ncamera frame. Our expectation is that this is just the beginning of a major\ntrend that could stand to revolutionize the analysis of camera trap data and,\nultimately, our approach to animal ecology.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:30:06 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Schneider", "Stefan", ""], ["Taylor", "Graham W.", ""], ["Linquist", "Stefan S.", ""], ["Kremer", "Stefan C.", ""]]}, {"id": "1811.07753", "submitter": "Daniel C. Castro", "authors": "Daniel C. Castro, Sebastian Nowozin", "title": "Contextual Face Recognition with a Nested-Hierarchical Nonparametric\n  Identity Model", "comments": "NeurIPS 2018 Workshop on All of Bayesian Nonparametrics (BNP@NeurIPS\n  2018). arXiv admin note: substantial text overlap with arXiv:1807.07872", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current face recognition systems typically operate via classification into\nknown identities obtained from supervised identity annotations. There are two\nproblems with this paradigm: (1) current systems are unable to benefit from\noften abundant unlabelled data; and (2) they equate successful recognition with\nlabelling a given input image. Humans, on the other hand, regularly perform\nidentification of individuals completely unsupervised, recognising the identity\nof someone they have seen before even without being able to name that\nindividual. How can we go beyond the current classification paradigm towards a\nmore human understanding of identities? In previous work, we proposed an\nintegrated Bayesian model that coherently reasons about the observed images,\nidentities, partial knowledge about names, and the situational context of each\nobservation. Here, we propose extensions of the contextual component of this\nmodel, enabling unsupervised discovery of an unbounded number of contexts for\nimproved face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:45:59 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Castro", "Daniel C.", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1811.07767", "submitter": "Anton Becker", "authors": "Anton S. Becker, Lukas Jendele, Ondrej Skopek, Nicole Berger, Soleen\n  Ghafoor, Magda Marcon, Ender Konukoglu", "title": "Injecting and removing malignant features in mammography with CycleGAN:\n  Investigation of an automated adversarial attack using neural networks", "comments": "To be presented at RSNA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\textbf{Purpose}$ To train a cycle-consistent generative adversarial network\n(CycleGAN) on mammographic data to inject or remove features of malignancy, and\nto determine whether these AI-mediated attacks can be detected by radiologists.\n$\\textbf{Material and Methods}$ From the two publicly available datasets, BCDR\nand INbreast, we selected images from cancer patients and healthy controls. An\ninternal dataset served as test data, withheld during training. We ran two\nexperiments training CycleGAN on low and higher resolution images ($256 \\times\n256$ px and $512 \\times 408$ px). Three radiologists read the images and rated\nthe likelihood of malignancy on a scale from 1-5 and the likelihood of the\nimage being manipulated. The readout was evaluated by ROC analysis (Area under\nthe ROC curve = AUC). $\\textbf{Results}$ At the lower resolution, only one\nradiologist exhibited markedly lower detection of cancer (AUC=0.85 vs 0.63,\np=0.06), while the other two were unaffected (0.67 vs. 0.69 and 0.75 vs. 0.77,\np=0.55). Only one radiologist could discriminate between original and modified\nimages slightly better than guessing/chance (0.66, p=0.008). At the higher\nresolution, all radiologists showed significantly lower detection rate of\ncancer in the modified images (0.77-0.84 vs. 0.59-0.69, p=0.008), however, they\nwere now able to reliably detect modified images due to better visibility of\nartifacts (0.92, 0.92 and 0.97). $\\textbf{Conclusion}$ A CycleGAN can\nimplicitly learn malignant features and inject or remove them so that a\nsubstantial proportion of small mammographic images would consequently be\nmisdiagnosed. At higher resolutions, however, the method is currently limited\nand has a clear trade-off between manipulation of images and introduction of\nartifacts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:08:11 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Becker", "Anton S.", ""], ["Jendele", "Lukas", ""], ["Skopek", "Ondrej", ""], ["Berger", "Nicole", ""], ["Ghafoor", "Soleen", ""], ["Marcon", "Magda", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1811.07768", "submitter": "Edgard Chammas", "authors": "Edgard Chammas, Chafic Mokbel, Laurence Likforman-Sulem", "title": "Handwriting Recognition of Historical Documents with few labeled data", "comments": null, "journal-ref": null, "doi": "10.1109/DAS.2018.15", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical documents present many challenges for offline handwriting\nrecognition systems, among them, the segmentation and labeling steps. Carefully\nannotated textlines are needed to train an HTR system. In some scenarios,\ntranscripts are only available at the paragraph level with no text-line\ninformation. In this work, we demonstrate how to train an HTR system with few\nlabeled data. Specifically, we train a deep convolutional recurrent neural\nnetwork (CRNN) system on only 10% of manually labeled text-line data from a\ndataset and propose an incremental training procedure that covers the rest of\nthe data. Performance is further increased by augmenting the training set with\nspecially crafted multiscale data. We also propose a model-based normalization\nscheme which considers the variability in the writing scale at the recognition\nphase. We apply this approach to the publicly available READ dataset. Our\nsystem achieved the second best result during the ICDAR2017 competition.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 23:21:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chammas", "Edgard", ""], ["Mokbel", "Chafic", ""], ["Likforman-Sulem", "Laurence", ""]]}, {"id": "1811.07769", "submitter": "Ilke Demir", "authors": "Ilke Demir, Ramesh Raskar", "title": "Addressing the Invisible: Street Address Generation for Developing\n  Countries with Deep Learning", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than half of the world's roads lack adequate street addressing systems.\nLack of addresses is even more visible in daily lives of people in developing\ncountries. We would like to object to the assumption that having an address is\na luxury, by proposing a generative address design that maps the world in\naccordance with streets. The addressing scheme is designed considering several\ntraditional street addressing methodologies employed in the urban development\nscenarios around the world. Our algorithm applies deep learning to extract\nroads from satellite images, converts the road pixel confidences into a road\nnetwork, partitions the road network to find neighborhoods, and labels the\nregions, roads, and address units using graph- and proximity-based algorithms.\nWe present our results on a sample US city, and several developing cities,\ncompare travel times of users using current ad hoc and new complete addresses,\nand contrast our addressing solution to current industrial and open geocoding\nalternatives.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 07:34:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Demir", "Ilke", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1811.07770", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of human affect using visual signals is a problem\nthat has attracted significant interest over the past 20 years. However, human\nemotional states are quite complex. To appraise such states displayed in\nreal-world settings, we need expressive emotional descriptors that are capable\nof capturing and describing this complexity. The circumplex model of affect,\nwhich is described in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the activation of the emotion), can be\nused for this purpose. Recent progress in the emotion recognition domain has\nbeen achieved through the development of deep neural architectures and the\navailability of very large training databases. To this end, Aff-Wild has been\nthe first large-scale \"in-the-wild\" database, containing around 1,200,000\nframes. In this paper, we build upon this database, extending it with 260 more\nsubjects and 1,413,000 new video frames. We call the union of Aff-Wild with the\nadditional data, Aff-Wild2. The videos are downloaded from Youtube and have\nlarge variations in pose, age, illumination conditions, ethnicity and\nprofession. Both database-specific as well as cross-database experiments are\nperformed in this paper, by utilizing the Aff-Wild2, along with the RECOLA\ndatabase. The developed deep neural architectures are based on the joint\ntraining of state-of-the-art convolutional and recurrent neural networks with\nattention mechanism; thus exploiting both the invariant properties of\nconvolutional features, while modeling temporal dynamics that arise in human\nbehaviour via the recurrent layers. The obtained results show premise for\nutilization of the extended Aff-Wild, as well as of the developed deep neural\narchitectures for visual analysis of human behaviour in terms of continuous\nemotion dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:57:15 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:44:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.07771", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "A Multi-Task Learning & Generation Framework: Valence-Arousal, Action\n  Units & Primary Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years many research efforts have been devoted to the field\nof affect analysis. Various approaches have been proposed for: i) discrete\nemotion recognition in terms of the primary facial expressions; ii) emotion\nanalysis in terms of facial Action Units (AUs), assuming a fixed expression\nintensity; iii) dimensional emotion analysis, in terms of valence and arousal\n(VA). These approaches can only be effective, if they are developed using\nlarge, appropriately annotated databases, showing behaviors of people\nin-the-wild, i.e., in uncontrolled environments. Aff-Wild has been the first,\nlarge-scale, in-the-wild database (including around 1,200,000 frames of 300\nvideos), annotated in terms of VA. In the vast majority of existing emotion\ndatabases, their annotation is limited to either primary expressions, or\nvalence-arousal, or action units. In this paper, we first annotate a part\n(around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and\nanother part (around $288,000$ frames) in terms of the $7$ basic emotion\ncategories, so that parts of this database are annotated in terms of VA, as\nwell as AUs, or primary expressions. Then, we set up and tackle multi-task\nlearning for emotion recognition, as well as for facial image generation.\nMulti-task learning is performed using: i) a deep neural network with shared\nhidden layers, which learns emotional attributes by exploiting their\ninter-dependencies; ii) a discriminator of a generative adversarial network\n(GAN). On the other hand, image generation is implemented through the generator\nof the GAN. For these two tasks, we carefully design loss functions that fit\nthe examined set-up. Experiments are presented which illustrate the good\nperformance of the proposed approach when applied to the new annotated parts of\nthe Aff-Wild database.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 15:40:23 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:39:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.07782", "submitter": "Ruichi Yu", "authors": "Shiyi Lan, Ruichi Yu, Gang Yu, Larry S. Davis", "title": "Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep convolutional neural networks (CNNs) have motivated\nresearchers to adapt CNNs to directly model points in 3D point clouds. Modeling\nlocal structure has been proven to be important for the success of\nconvolutional architectures, and researchers exploited the modeling of local\npoint sets in the feature extraction hierarchy. However, limited attention has\nbeen paid to explicitly model the geometric structure amongst points in a local\nregion. To address this problem, we propose Geo-CNN, which applies a generic\nconvolution-like operation dubbed as GeoConv to each point and its local\nneighborhood. Local geometric relationships among points are captured when\nextracting edge features between the center and its neighboring points. We\nfirst decompose the edge feature extraction process onto three orthogonal\nbases, and then aggregate the extracted features based on the angles between\nthe edge vector and the bases. This encourages the network to preserve the\ngeometric structure in Euclidean space throughout the feature extraction\nhierarchy. GeoConv is a generic and efficient operation that can be easily\nintegrated into 3D point cloud analysis pipelines for multiple applications. We\nevaluate Geo-CNN on ModelNet40 and KITTI and achieve state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:30:20 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lan", "Shiyi", ""], ["Yu", "Ruichi", ""], ["Yu", "Gang", ""], ["Davis", "Larry S.", ""]]}, {"id": "1811.07789", "submitter": "Varun Manjunatha", "authors": "Varun Manjunatha and Nirat Saini and Larry S. Davis", "title": "Explicit Bias Discovery in Visual Question Answering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have observed that Visual Question Answering (VQA) models tend to\nanswer questions by learning statistical biases in the data. For example, their\nanswer to the question \"What is the color of the grass?\" is usually \"Green\",\nwhereas a question like \"What is the title of the book?\" cannot be answered by\ninferring statistical biases. It is of interest to the community to explicitly\ndiscover such biases, both for understanding the behavior of such models, and\ntowards debugging them. Our work address this problem. In a database, we store\nthe words of the question, answer and visual words corresponding to regions of\ninterest in attention maps. By running simple rule mining algorithms on this\ndatabase, we discover human-interpretable rules which give us unique insight\ninto the behavior of such models. Our results also show examples of unusual\nbehaviors learned by models in attempting VQA tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:39:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Manjunatha", "Varun", ""], ["Saini", "Nirat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1811.07791", "submitter": "David Fuentes-Jimenez", "authors": "David Fuentes-Jimenez, David Casillas-Perez, Daniel Pizarro, Toby\n  Collins and Adrien Bartoli", "title": "Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and\n  Deformable Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network\n(DNN) method for solving real-time automatic registration and 3D reconstruction\nof a deformable object viewed in a single monocular image.DeepSfT advances the\nstate-of-the-art in various aspects. Compared to existing DNN SfT methods, it\nis the first fully convolutional real-time approach that handles an arbitrary\nobject geometry, topology and surface representation. It also does not require\nground truth registration with real data and scales well to very complex object\nmodels with large numbers of elements. Compared to previous non-DNN SfT\nmethods, it does not involve numerical optimization at run-time, and is a\ndense, wide-baseline solution that does not demand, and does not suffer from,\nfeature-based matching. It is able to process a single image with significant\ndeformation and viewpoint changes, and handles well the core challenges of\nocclusions, weak texture and blur. DeepSfT is based on residual encoder-decoder\nstructures and refining blocks. It is trained end-to-end with a novel\ncombination of supervised learning from simulated renderings of the object\nmodel and semi-supervised automatic fine-tuning using real data captured with a\nstandard RGB-D camera. The cameras used for fine-tuning and run-time can be\ndifferent, making DeepSfT practical for real-world use. We show that DeepSfT\nsignificantly outperforms state-of-the-art wide-baseline approaches for\nnon-trivial templates, with quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:39:27 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:13:33 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 03:12:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fuentes-Jimenez", "David", ""], ["Casillas-Perez", "David", ""], ["Pizarro", "Daniel", ""], ["Collins", "Toby", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1811.07793", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Tiankuang Zhou, Zhibo Chen", "title": "DeepIR: A Deep Semantics Driven Framework for Image Retargeting", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\emph{Deep Image Retargeting} (\\emph{DeepIR}), a coarse-to-fine\nframework for content-aware image retargeting. Our framework first constructs\nthe semantic structure of input image with a deep convolutional neural network.\nThen a uniform re-sampling that suits for semantic structure preserving is\ndevised to resize feature maps to target aspect ratio at each feature layer.\nThe final retargeting result is generated by coarse-to-fine nearest neighbor\nfield search and step-by-step nearest neighbor field fusion. We empirically\ndemonstrate the effectiveness of our model with both qualitative and\nquantitative results on widely used RetargetMe dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:43:28 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 14:15:36 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 03:16:05 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Lin", "Jianxin", ""], ["Zhou", "Tiankuang", ""], ["Chen", "Zhibo", ""]]}, {"id": "1811.07802", "submitter": "Jean-Matthieu Maro", "authors": "Jean-Matthieu Maro, Ryad Benosman", "title": "Event-based Gesture Recognition with Dynamic Background Suppression\n  using Smartphone Computational Capabilities", "comments": "Draft version; final version published in Frontiers in Neuroscience\n  (open access)", "journal-ref": "Frontiers in Neuroscience 14 (2020) 275", "doi": "10.3389/fnins.2020.00275", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework of gesture recognition operating on the\noutput of an event based camera using the computational resources of a mobile\nphone. We will introduce a new development around the concept of time-surfaces\nmodified and adapted to run on the limited computational resources of a mobile\nplatform. We also introduce a new method to remove dynamically backgrounds that\nmakes full use of the high temporal resolution of event-based cameras. We\nassess the performances of the framework by operating on several dynamic\nscenarios in uncontrolled lighting conditions indoors and outdoors. We also\nintroduce a new publicly available event-based dataset for gesture recognition\nselected through a clinical process to allow human-machine interactions for the\nvisually-impaired and the elderly. We finally report comparisons with prior\nworks that tackled event-based gesture recognition reporting comparable if not\nsuperior results if taking into account the limited computational and memory\nconstraints of the used hardware.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:03:01 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 13:58:03 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 08:36:39 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Maro", "Jean-Matthieu", ""], ["Benosman", "Ryad", ""]]}, {"id": "1811.07807", "submitter": "Tian Xu", "authors": "Tian Xu, Jiayu Zhan, Oliver G.B. Garrod, Philip H.S. Torr, Song-Chun\n  Zhu, Robin A.A. Ince, Philippe G. Schyns", "title": "Deeper Interpretability of Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been one of the most\ninfluential recent developments in computer vision, particularly for\ncategorization. There is an increasing demand for explainable AI as these\nsystems are deployed in the real world. However, understanding the information\nrepresented and processed in CNNs remains in most cases challenging. Within\nthis paper, we explore the use of new information theoretic techniques\ndeveloped in the field of neuroscience to enable novel understanding of how a\nCNN represents information. We trained a 10-layer ResNet architecture to\nidentify 2,000 face identities from 26M images generated using a rigorously\ncontrolled 3D face rendering model that produced variations of intrinsic (i.e.\nface morphology, gender, age, expression and ethnicity) and extrinsic factors\n(i.e. 3D pose, illumination, scale and 2D translation). With our methodology,\nwe demonstrate that unlike human's network overgeneralizes face identities even\nwith extreme changes of face shape, but it is more sensitive to changes of\ntexture. To understand the processing of information underlying these\ncounterintuitive properties, we visualize the features of shape and texture\nthat the network processes to identify faces. Then, we shed a light into the\ninner workings of the black box and reveal how hidden layers represent these\nfeatures and whether the representations are invariant to pose. We hope that\nour methodology will provide an additional valuable tool for interpretability\nof CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:10:44 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 09:43:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Xu", "Tian", ""], ["Zhan", "Jiayu", ""], ["Garrod", "Oliver G. B.", ""], ["Torr", "Philip H. S.", ""], ["Zhu", "Song-Chun", ""], ["Ince", "Robin A. A.", ""], ["Schyns", "Philippe G.", ""]]}, {"id": "1811.07818", "submitter": "Amarnath R", "authors": "BV Divyashree, Amarnath R, Naveen M, G Hemantha Kumar", "title": "Novel approach to locate region of interest in mammograms for Breast\n  cancer", "comments": "ROI, breast cancer, mammographic images, segmentation, entropy, quad\n  tree", "journal-ref": "International Journal of Intelligent Systems and Applications in\n  Engineering.(ISSN:2147-6799) Vol 6, No 3 (2018)", "doi": "10.18201/ijisae.2018644775", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locating region of interest for breast cancer masses in the mammographic\nimage is a challenging problem in medical image processing. In this research\nwork, the keen idea is to efficiently extract suspected mass region for further\nexamination. In particular to this fact breast boundary segmentation on sliced\nrgb image using modified intensity based approach followed by quad tree based\ndivision to spot out suspicious area are proposed in the paper. To evaluate the\nperformance DDSM standard dataset are experimented and achieved acceptable\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:01:40 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Divyashree", "BV", ""], ["R", "Amarnath", ""], ["M", "Naveen", ""], ["Kumar", "G Hemantha", ""]]}, {"id": "1811.07839", "submitter": "Laurent Dardelet", "authors": "Laurent Dardelet and Sio-Hoi Ieng and Ryad Benosman", "title": "Event-Based Features Selection and Tracking from Intertwined Estimation\n  of Velocity and Generative Contours", "comments": "9 pages, 6 figures, 2 algorithms, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new event-based method for detecting and tracking\nfeatures from the output of an event-based camera. Unlike many tracking\nalgorithms from the computer vision community, this process does not aim for\nparticular predefined shapes such as corners. It relies on a dual intertwined\niterative continuous -- pure event-based -- estimation of the velocity vector\nand a bayesian description of the generative feature contours. By projecting\nalong estimated speeds updated for each incoming event it is possible to\nidentify and determine the spatial location and generative contour of the\ntracked feature while iteratively updating the estimation of the velocity\nvector. Results on several environments are shown taking into account large\nvariations in terms of luminosity, speed, nature and size of the tracked\nfeatures. The usage of speed instead of positions allows for a much faster\nfeedback allowing for very fast convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:53:45 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Dardelet", "Laurent", ""], ["Ieng", "Sio-Hoi", ""], ["Benosman", "Ryad", ""]]}, {"id": "1811.07859", "submitter": "Kumar Shreshtha", "authors": "Pankaj Bodani, Kumar Shreshtha, Shashikant Sharma", "title": "OrthoSeg: A Deep Multimodal Convolutional Neural Network for Semantic\n  Segmentation of Orthoimagery", "comments": "8 pages, 9 figures, 3 tables", "journal-ref": "Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-5,\n  621-628, 2018", "doi": "10.5194/isprs-archives-XLII-5-621-2018", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the task of semantic segmentation of orthoimagery using\nmultimodal data e.g. optical RGB, infrared and digital surface model. We\npropose a deep convolutional neural network architecture termed OrthoSeg for\nsemantic segmentation using multimodal, orthorectified and coregistered data.\nWe also propose a training procedure for supervised training of OrthoSeg. The\ntraining procedure complements the inherent architectural characteristics of\nOrthoSeg for preventing complex co-adaptations of learned features, which may\narise due to probable high dimensionality and spatial correlation in multimodal\nand/or multispectral coregistered data. OrthoSeg consists of parallel encoding\nnetworks for independent encoding of multimodal feature maps and a decoder\ndesigned for efficiently fusing independently encoded multimodal feature maps.\nA softmax layer at the end of the network uses the features generated by the\ndecoder for pixel-wise classification. The decoder fuses feature maps from the\nparallel encoders locally as well as contextually at multiple scales to\ngenerate per-pixel feature maps for final pixel-wise classification resulting\nin segmented output. We experimentally show the merits of OrthoSeg by\ndemonstrating state-of-the-art accuracy on the ISPRS Potsdam 2D Semantic\nSegmentation dataset. Adaptability is one of the key motivations behind\nOrthoSeg so that it serves as a useful architectural option for a wide range of\nproblems involving the task of semantic segmentation of coregistered multimodal\nand/or multispectral imagery. Hence, OrthoSeg is designed to enable independent\nscaling of parallel encoder networks and decoder network to better match\napplication requirements, such as the number of input channels, the effective\nfield-of-view, and model capacity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:21:41 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:45:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Bodani", "Pankaj", ""], ["Shreshtha", "Kumar", ""], ["Sharma", "Shashikant", ""]]}, {"id": "1811.07889", "submitter": "Sung Ho Kang", "authors": "Sung Ho Kang, Kiwan Jeon, Hak-Jin Kim, Jin Keun Seo, and Sang-Hwy Lee", "title": "Automatic Three-Dimensional Cephalometric Annotation System Using\n  Three-Dimensional Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Three-dimensional (3D) cephalometric analysis using computerized\ntomography data has been rapidly adopted for dysmorphosis and anthropometry.\nSeveral different approaches to automatic 3D annotation have been proposed to\novercome the limitations of traditional cephalometry. The purpose of this study\nwas to evaluate the accuracy of our newly-developed system using a deep\nlearning algorithm for automatic 3D cephalometric annotation. Methods: To\novercome current technical limitations, some measures were developed to\ndirectly annotate 3D human skull data. Our deep learning-based model system\nmainly consisted of a 3D convolutional neural network and image data\nresampling. Results: The discrepancies between the referenced and predicted\ncoordinate values in three axes and in 3D distance were calculated to evaluate\nsystem accuracy. Our new model system yielded prediction errors of 3.26, 3.18,\nand 4.81 mm (for three axes) and 7.61 mm (for 3D). Moreover, there was no\ndifference among the landmarks of the three groups, including the midsagittal\nplane, horizontal plane, and mandible (p>0.05). Conclusion: A new 3D\nconvolutional neural network-based automatic annotation system for 3D\ncephalometry was developed. The strategies used to implement the system were\ndetailed and measurement results were evaluated for accuracy. Further\ndevelopment of this system is planned for full clinical application of\nautomatic 3D cephalometric annotation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 05:47:34 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kang", "Sung Ho", ""], ["Jeon", "Kiwan", ""], ["Kim", "Hak-Jin", ""], ["Seo", "Jin Keun", ""], ["Lee", "Sang-Hwy", ""]]}, {"id": "1811.07896", "submitter": "Shishira Maiya", "authors": "Shishira R Maiya and Sudharshan Chandra Babu", "title": "Slum Segmentation and Change Detection : A Deep Learning Approach", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than one billion people live in slums around the world. In some\ndeveloping countries, slum residents make up for more than half of the\npopulation and lack reliable sanitation services, clean water, electricity,\nother basic services. Thus, slum rehabilitation and improvement is an important\nglobal challenge, and a significant amount of effort and resources have been\nput into this endeavor. These initiatives rely heavily on slum mapping and\nmonitoring, and it is essential to have robust and efficient methods for\nmapping and monitoring existing slum settlements. In this work, we introduce an\napproach to segment and map individual slums from satellite imagery, leveraging\nregional convolutional neural networks for instance segmentation using transfer\nlearning. In addition, we also introduce a method to perform change detection\nand monitor slum change over time. We show that our approach effectively learns\nslum shape and appearance, and demonstrates strong quantitative results,\nresulting in a maximum AP of 80.0.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:45:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Maiya", "Shishira R", ""], ["Babu", "Sudharshan Chandra", ""]]}, {"id": "1811.07945", "submitter": "Shuai Li", "authors": "Mo Deng, Shuai Li and George Barbastathis", "title": "Learning to synthesize: splitting and recombining low and high spatial\n  frequencies for image recovery", "comments": "10 pages, 10 figures. Supplement file can be provided upon reasonable\n  request", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN)-based image reconstruction, despite many successes,\noften exhibits uneven fidelity between high and low spatial frequency bands. In\nthis paper we propose the Learning Synthesis by DNN (LS-DNN) approach where two\nDNNs process the low and high spatial frequencies, respectively, and, improving\nover [30], the two DNNs are trained separately and a third DNN combines them\ninto an image with high fidelity at all bands. We demonstrate LS-DNN in two\ncanonical inverse problems: super-resolution (SR) in diffraction-limited\nimaging (DLI), and quantitative phase retrieval (QPR). Our results also show\ncomparable or improved performance over perceptual-loss based SR [21], and can\nbe generalized to a wider range of image recovery problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 19:29:20 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Deng", "Mo", ""], ["Li", "Shuai", ""], ["Barbastathis", "George", ""]]}, {"id": "1811.07950", "submitter": "Yao Li", "authors": "Yao Li, Martin Renqiang Min, Wenchao Yu, Cho-Jui Hsieh, Thomas C.M.\n  Lee, and Erik Kruus", "title": "Optimal Transport Classifier: Defending Against Adversarial Attacks by\n  Regularized Deep Embedding", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the vulnerability of deep convolutional\nneural networks against adversarial examples. Inspired by the observation that\nthe intrinsic dimension of image data is much smaller than its pixel space\ndimension and the vulnerability of neural networks grows with the input\ndimension, we propose to embed high-dimensional input images into a\nlow-dimensional space to perform classification. However, arbitrarily\nprojecting the input images to a low-dimensional space without regularization\nwill not improve the robustness of deep neural networks. Leveraging optimal\ntransport theory, we propose a new framework, Optimal Transport Classifier\n(OT-Classifier), and derive an objective that minimizes the discrepancy between\nthe distribution of the true label and the distribution of the OT-Classifier\noutput. Experimental results on several benchmark datasets show that, our\nproposed framework achieves state-of-the-art performance against strong\nadversarial attack methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 19:42:38 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 17:30:27 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Li", "Yao", ""], ["Min", "Martin Renqiang", ""], ["Yu", "Wenchao", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Thomas C. M.", ""], ["Kruus", "Erik", ""]]}, {"id": "1811.07958", "submitter": "Brent Griffin", "authors": "Brent A. Griffin and Jason J. Corso", "title": "Tukey-Inspired Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of strictly unsupervised video object\nsegmentation, i.e., the separation of a primary object from background in video\nwithout a user-provided object mask or any training on an annotated dataset. We\nfind foreground objects in low-level vision data using a John Tukey-inspired\nmeasure of \"outlierness\". This Tukey-inspired measure also estimates the\nreliability of each data source as video characteristics change (e.g., a camera\nstarts moving). The proposed method achieves state-of-the-art results for\nstrictly unsupervised video object segmentation on the challenging DAVIS\ndataset. Finally, we use a variant of the Tukey-inspired measure to combine the\noutput of multiple segmentation methods, including those using supervision\nduring training, runtime, or both. This collectively more robust method of\nsegmentation improves the Jaccard measure of its constituent methods by as much\nas 28%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:15:27 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:37:11 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Griffin", "Brent A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1811.07966", "submitter": "Alexander Wong", "authors": "Audrey Chung, Paul Fieguth, Alexander Wong", "title": "Mitigating Architectural Mismatch During the Evolutionary Synthesis of\n  Deep Neural Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary deep intelligence has recently shown great promise for producing\nsmall, powerful deep neural network models via the organic synthesis of\nincreasingly efficient architectures over successive generations. Existing\nevolutionary synthesis processes, however, have allowed the mating of parent\nnetworks independent of architectural alignment, resulting in a mismatch of\nnetwork structures. We present a preliminary study into the effects of\narchitectural alignment during evolutionary synthesis using a gene tagging\nsystem. Surprisingly, the network architectures synthesized using the gene\ntagging approach resulted in slower decreases in performance accuracy and\nstorage size; however, the resultant networks were comparable in size and\nperformance accuracy to the non-gene tagging networks. Furthermore, we\nspeculate that there is a noticeable decrease in network variability for\nnetworks synthesized with gene tagging, indicating that enforcing a\nlike-with-like mating policy potentially restricts the exploration of the\nsearch space of possible network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:36:16 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chung", "Audrey", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1811.07969", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Mario Wieser, Andreas Morel-Forster, Aleksander\n  Wieczorek, Sonali Parbhoo, Volker Roth, Thomas Vetter", "title": "Informed MCMC with Bayesian Neural Networks for Facial Image Analysis", "comments": "Accepted to the Bayesian Deep Learning Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision tasks are difficult because of the large variability in the\ndata that is induced by changes in light, background, partial occlusion as well\nas the varying pose, texture, and shape of objects. Generative approaches to\ncomputer vision allow us to overcome this difficulty by explicitly modeling the\nphysical image formation process. Using generative object models, the analysis\nof an observed image is performed via Bayesian inference of the posterior\ndistribution. This conceptually simple approach tends to fail in practice\nbecause of several difficulties stemming from sampling the posterior\ndistribution: high-dimensionality and multi-modality of the posterior\ndistribution as well as expensive simulation of the rendering process. The main\ndifficulty of sampling approaches in a computer vision context is choosing the\nproposal distribution accurately so that maxima of the posterior are explored\nearly and the algorithm quickly converges to a valid image interpretation. In\nthis work, we propose to use a Bayesian Neural Network for estimating an image\ndependent proposal distribution. Compared to a standard Gaussian random walk\nproposal, this accelerates the sampler in finding regions of the posterior with\nhigh value. In this way, we can significantly reduce the number of samples\nneeded to perform facial image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:47:04 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 15:05:50 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kortylewski", "Adam", ""], ["Wieser", "Mario", ""], ["Morel-Forster", "Andreas", ""], ["Wieczorek", "Aleksander", ""], ["Parbhoo", "Sonali", ""], ["Roth", "Volker", ""], ["Vetter", "Thomas", ""]]}, {"id": "1811.07982", "submitter": "Minzhong Luo", "authors": "Mincong Luo, Xinfu He, Li Liu", "title": "Generative Model for Material Experiments Based on Prior Knowledge and\n  Attention Mechanism", "comments": "Accepted by NIPS2018 MMLM workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Material irradiation experiment is dangerous and complex, thus it requires\nthose with a vast advanced expertise to process the images and data manually.\nIn this paper, we propose a generative adversarial model based on prior\nknowledge and attention mechanism to achieve the generation of irradiated\nmaterial images (data-to-image model), and a prediction model for corresponding\nindustrial performance (image-to-data model). With the proposed models,\nresearchers can skip the dangerous and complex irradiation experiments and\nobtain the irradiation images and industrial performance parameters directly by\ninputing some experimental parameters only. We also introduce a new dataset\nISMD which contains 22000 irradiated images with 22,143 sets of corresponding\nparameters. Our model achieved high quality results by compared with several\nbaseline models. The evaluation and detailed analysis are also performed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 02:40:00 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Luo", "Mincong", ""], ["He", "Xinfu", ""], ["Liu", "Li", ""]]}, {"id": "1811.07985", "submitter": "Krit Pongpirul", "authors": "Seelwan Sathitratanacheewin (1 and 2) and Krit Pongpirul (1, 2, and 3)\n  ((1) Department of Preventive and Social Medicine, Faculty of Medicine,\n  Chulalongkorn University, Bangkok, Thailand, (2) Thai Health AI Foundation,\n  Bangkok, Thailand, (3) Department of International Health and Department of\n  Health, Behavior, and Society, Johns Hopkins Bloomberg School of Public\n  Health, Baltimore, MD, USA)", "title": "Deep Learning for Automated Classification of Tuberculosis-Related Chest\n  X-Ray: Dataset Specificity Limits Diagnostic Performance Generalizability", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning has been an emerging tool for various aspects of infectious\ndiseases including tuberculosis surveillance and detection. However, WHO\nprovided no recommendations on using computer-aided tuberculosis detection\nsoftware because of the small number of studies, methodological limitations,\nand limited generalizability of the findings. To quantify the generalizability\nof the machine-learning model, we developed a Deep Convolutional Neural Network\n(DCNN) model using a TB-specific CXR dataset of one population (National\nLibrary of Medicine Shenzhen No.3 Hospital) and tested it with non-TB-specific\nCXR dataset of another population (National Institute of Health Clinical\nCenters). The findings suggested that a supervised deep learning model\ndeveloped by using the training dataset from one population may not have the\nsame diagnostic performance in another population. Technical specification of\nCXR images, disease severity distribution, overfitting, and overdiagnosis\nshould be examined before implementation in other settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:32:42 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 08:44:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sathitratanacheewin", "Seelwan", "", "1 and 2"], ["Pongpirul", "Krit", "", "1, 2, and 3"]]}, {"id": "1811.07987", "submitter": "Li Conghui", "authors": "Conghui Li, Zhaocheng Zhu, and Yuming Zhao", "title": "Saliency Supervision: An Intuitive and Effective Approach for Pain\n  Intensity Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting pain intensity from face images is an important problem in autonomous\nnursing systems. However, due to the limitation in data sources and the\nsubjectiveness in pain intensity values, it is hard to adopt modern deep neural\nnetworks for this problem without domain-specific auxiliary design. Inspired by\nhuman vision priori, we propose a novel approach called saliency supervision,\nwhere we directly regularize deep networks to focus on facial area that is\ndiscriminative for pain regression. Through alternative training between\nsaliency supervision and global loss, our method can learn sparse and robust\nfeatures, which is proved helpful for pain intensity regression. We verified\nsaliency supervision with face-verification network backbone on the widely-used\ndataset, and achieved state-of-art performance without bells and whistles. Our\nsaliency supervision is intuitive in spirit, yet effective in performance. We\nbelieve such saliency supervision is essential in dealing with ill-posed\ndatasets, and has potential in a wide range of vision tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:18:09 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Li", "Conghui", ""], ["Zhu", "Zhaocheng", ""], ["Zhao", "Yuming", ""]]}, {"id": "1811.07988", "submitter": "Zhanli Chen", "authors": "Zhanli Chen, Rashid Ansari, Diana Wilkie", "title": "Automated Pain Detection from Facial Expressions using FACS: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial pain expression is an important modality for assessing pain,\nespecially when the patient's verbal ability to communicate is impaired. The\nfacial muscle-based action units (AUs), which are defined by the Facial Action\nCoding System (FACS), have been widely studied and are highly reliable as a\nmethod for detecting facial expressions (FE) including valid detection of pain.\nUnfortunately, FACS coding by humans is a very time-consuming task that makes\nits clinical use prohibitive. Significant progress on automated facial\nexpression recognition (AFER) has led to its numerous successful applications\nin FACS-based affective computing problems. However, only a handful of studies\nhave been reported on automated pain detection (APD), and its application in\nclinical settings is still far from a reality. In this paper, we review the\nprogress in research that has contributed to automated pain detection, with\nfocus on 1) the framework-level similarity between spontaneous AFER and APD\nproblems; 2) the evolution of system design including the recent development of\ndeep learning methods; 3) the strategies and considerations in developing a\nFACS-based pain detection framework from existing research; and 4) introduction\nof the most relevant databases that are available for AFER and APD studies. We\nattempt to present key considerations in extending a general AFER framework to\nan APD framework in clinical settings. In addition, the performance metrics are\nalso highlighted in evaluating an AFER or an APD system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:59:24 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Zhanli", ""], ["Ansari", "Rashid", ""], ["Wilkie", "Diana", ""]]}, {"id": "1811.07993", "submitter": "Pengkai Zhu", "authors": "Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama", "title": "Generalized Zero-Shot Recognition based on Visually Semantic Embedding", "comments": "Accepted in CVPR209. 9 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Generalized Zero-Shot learning (GZSL) method that is\nagnostic to both unseen images and unseen semantic vectors during training.\nPrior works in this context propose to map high-dimensional visual features to\nthe semantic domain, we believe contributes to the semantic gap. To bridge the\ngap, we propose a novel low-dimensional embedding of visual instances that is\n\"visually semantic.\" Analogous to semantic data that quantifies the existence\nof an attribute in the presented instance, components of our visual embedding\nquantifies existence of a prototypical part-type in the presented instance. In\nparallel, as a thought experiment, we quantify the impact of noisy semantic\ndata by utilizing a novel visual oracle to visually supervise a learner. These\nfactors, namely semantic noise, visual-semantic gap and label noise lead us to\npropose a new graphical model for inference with pairwise interactions between\nlabel, semantic data, and inputs. We tabulate results on a number of benchmark\ndatasets demonstrating significant improvement in accuracy over\nstate-of-the-art under both semantic and visual supervision.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 21:38:28 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:48:27 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhu", "Pengkai", ""], ["Wang", "Hanxiao", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1811.07996", "submitter": "Abon Chaudhuri", "authors": "Abon Chaudhuri, Paolo Messina, Samrat Kokkula, Aditya Subramanian,\n  Abhinandan Krishnan, Shreyansh Gandhi, Alessandro Magnani, Venkatesh\n  Kandaswamy", "title": "A Smart System for Selection of Optimal Product Images in E-Commerce", "comments": "Accepted in IEEE Big Data Conference 2018 (Industry & Government\n  Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, content quality of the product catalog plays a key role in\ndelivering a satisfactory experience to the customers. In particular, visual\ncontent such as product images influences customers' engagement and purchase\ndecisions. With the rapid growth of e-commerce and the advent of artificial\nintelligence, traditional content management systems are giving way to\nautomated scalable systems. In this paper, we present a machine learning driven\nvisual content management system for extremely large e-commerce catalogs. For a\ngiven product, the system aggregates images from various suppliers, understands\nand analyzes them to produce a superior image set with optimal image count and\nquality, and arranges them in an order tailored to the demands of the\ncustomers. The system makes use of an array of technologies, ranging from deep\nlearning to traditional computer vision, at different stages of analysis. In\nthis paper, we outline how the system works and discuss the unique challenges\nrelated to applying machine learning techniques to real-world data from\ne-commerce domain. We emphasize how we tune state-of-the-art image\nclassification techniques to develop solutions custom made for a massive,\ndiverse, and constantly evolving product catalog. We also provide the details\nof how we measure the system's impact on various customer engagement metrics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 02:35:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chaudhuri", "Abon", ""], ["Messina", "Paolo", ""], ["Kokkula", "Samrat", ""], ["Subramanian", "Aditya", ""], ["Krishnan", "Abhinandan", ""], ["Gandhi", "Shreyansh", ""], ["Magnani", "Alessandro", ""], ["Kandaswamy", "Venkatesh", ""]]}, {"id": "1811.07998", "submitter": "Hamed Alemohammad", "authors": "Yoni Nachmany, Hamed Alemohammad", "title": "Generating a Training Dataset for Land Cover Classification to Advance\n  Global Development", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation of land cover classes is fundamental for agricultural\nand economic development work, from sustainable forestry to urban planning, yet\nexisting training datasets have significant limitations. To generate an open\nand comprehensive training library of high resolution Earth imagery and high\nquality land cover classifications, public Sentinel-2 data at 10 m spatial\nresolution was matched with accurate GlobeLand30 labels from 2010, which were\nfiltered by agreement with an intermediary Sentinel-2 classification at 20 m\nproduced during atmospheric correction. Scene-level classifications were\npredicted by Random Forests trained on valid reflectance data and the filtered\nlabels, and achieved over 80% model accuracy for a variety of locations.\nFurther work is required to aggregate individual scene classifications for\nannual labels and to test the approach in more locations, before crowdsourcing\nhuman validation. The goal is to create a sustained community-wide effort to\ngenerate image labels not only for land cover, but also very specific images\nfor major agriculture crops across the world and other thematic categories of\ninterest to the global development community.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:50:46 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Nachmany", "Yoni", ""], ["Alemohammad", "Hamed", ""]]}, {"id": "1811.07999", "submitter": "Steven Kommrusch", "authors": "Steve Kommrusch and Louis-No\\\"el Pouchet", "title": "Synthetic Lung Nodule 3D Image Generation Using Autoencoders", "comments": "19 pages, 12 figures, full paper for work initially presented at\n  IJCAI 2018", "journal-ref": null, "doi": null, "report-no": "CS-18-101", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges of using machine learning techniques with medical data\nis the frequent dearth of source image data on which to train. A representative\nexample is automated lung cancer diagnosis, where nodule images need to be\nclassified as suspicious or benign. In this work we propose an automatic\nsynthetic lung nodule image generator. Our 3D shape generator is designed to\naugment the variety of 3D images. Our proposed system takes root in autoencoder\ntechniques, and we provide extensive experimental characterization that\ndemonstrates its ability to produce quality synthetic images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 21:51:38 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 05:58:20 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 05:58:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kommrusch", "Steve", ""], ["Pouchet", "Louis-No\u00ebl", ""]]}, {"id": "1811.08004", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Shiyang Cheng and Maja Pantic and Stefanos\n  Zafeiriou", "title": "Photorealistic Facial Synthesis in the Dimensional Affect Space", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.05027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for synthesizing facial affect, which is\nbased on our annotating 600,000 frames of the 4DFAB database in terms of\nvalence and arousal. The input of this approach is a pair of these emotional\nstate descriptors and a neutral 2D image of a person to whom the corresponding\naffect will be synthesized. Given this target pair, a set of 3D facial meshes\nis selected, which is used to build a blendshape model and generate the new\nfacial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting\nis performed and the reconstructed face is deformed to generate the target\nfacial expressions. Last, the new face is rendered into the original image.\nBoth qualitative and quantitative experimental studies illustrate the\ngeneration of realistic images, when the neutral image is sampled from a\nvariety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,\nAFEW-VA, BU-3DFE, Bosphorus.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:30:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Cheng", "Shiyang", ""], ["Pantic", "Maja", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.08009", "submitter": "Istvan Fehervari", "authors": "Istvan Fehervari and Srikar Appalaraju", "title": "Scalable Logo Recognition using Proxies", "comments": "Accepted at IEEE WACV 2019, Hawaii USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo recognition is the task of identifying and classifying logos. Logo\nrecognition is a challenging problem as there is no clear definition of a logo\nand there are huge variations of logos, brands and re-training to cover every\nvariation is impractical. In this paper, we formulate logo recognition as a\nfew-shot object detection problem. The two main components in our pipeline are\nuniversal logo detector and few-shot logo recognizer. The universal logo\ndetector is a class-agnostic deep object detector network which tries to learn\nthe characteristics of what makes a logo. It predicts bounding boxes on likely\nlogo regions. These logo regions are then classified by logo recognizer using\nnearest neighbor search, trained by triplet loss using proxies. We also\nannotated a first of its kind product logo dataset containing 2000 logos from\n295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall\nwith 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the\npublicly available FlickrLogos-32 test set without fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:28:13 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Fehervari", "Istvan", ""], ["Appalaraju", "Srikar", ""]]}, {"id": "1811.08011", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, Yasunori Ishii,\n  Sotaro Tsukizawa", "title": "Explain to Fix: A Framework to Interpret and Correct DNN Object Detector\n  Predictions", "comments": "Systems for ML Workshop @ NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining predictions of deep neural networks (DNNs) is an important and\nnontrivial task. In this paper, we propose a practical approach to interpret\ndecisions made by a DNN object detector that has fidelity comparable to\nstate-of-the-art methods and sufficient computational efficiency to process\nlarge datasets. Our method relies on recent theory and approximates Shapley\nfeature importance values. We qualitatively and quantitatively show that the\nproposed explanation method can be used to find image features which cause\nfailures in DNN object detection. The developed software tool combined into the\n\"Explain to Fix\" (E2X) framework has a factor of 10 higher computational\nefficiency than prior methods and can be used for cluster processing using\ngraphics processing units (GPUs). Lastly, we propose a potential extension of\nthe E2X framework where the discovered missing features can be added into\ntraining dataset to overcome failures after model retraining.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:41:49 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Hodgkinson", "Alec", ""], ["Yamaguchi", "Takuya", ""], ["Ishii", "Yasunori", ""], ["Tsukizawa", "Sotaro", ""]]}, {"id": "1811.08012", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "A Comparative Study of Computational Aesthetics", "comments": "6 pages, 5 figures, 1 table", "journal-ref": "2014 IEEE International Conference on Image Processing (ICIP),\n  Paris, 2014, pp. 590-594", "doi": "10.1109/ICIP.2014.7025118", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective metrics model image quality by quantifying image degradations or\nestimating perceived image quality. However, image quality metrics do not model\nwhat makes an image more appealing or beautiful. In order to quantify the\naesthetics of an image, we need to take it one step further and model the\nperception of aesthetics. In this paper, we examine computational aesthetics\nmodels that use hand-crafted, generic and hybrid descriptors. We show that\ngeneric descriptors can perform as well as state of the art hand-crafted\naesthetics models that use global features. However, neither generic nor\nhand-crafted features is sufficient to model aesthetics when we only use global\nfeatures without considering spatial composition or distribution. We also\nfollow a visual dictionary approach similar to state of the art methods and\nshow that it performs poorly without the spatial pyramid step.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:46:12 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08015", "submitter": "Shuhui Jiang", "authors": "Shuhui Jiang, Zhaowen Wang, Aaron Hertzmann, Hailin Jin, Yun Fu", "title": "Visual Font Pairing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 23:07:38 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Jiang", "Shuhui", ""], ["Wang", "Zhaowen", ""], ["Hertzmann", "Aaron", ""], ["Jin", "Hailin", ""], ["Fu", "Yun", ""]]}, {"id": "1811.08032", "submitter": "Andrey Filippov", "authors": "Andrey Filippov, Oleg Dzhimiev", "title": "See far with TPNET: a Tile Processor and a CNN Symbiosis", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the evolution of the neural networks more specialized cells were\nadded to the set of basic building blocks. These cells aim to improve training\nconvergence, increase the overall performance, and reduce the number of\nrequired labels, all while preserving the expressive power of the universal\nnetwork. Inspired by the partitioning of the human visual perception system\nbetween the eyes and the cerebral cortex, we present TPNET, which offloads\nuniversal and application-specific CNN from the bulk processing of the high\nresolution pixel data and performs the translation-variant image correction\nwhile delegating all non-linear decision making to the network.\n  In this work, we explore application of TPNET to 3D perception with a\nnarrow-baseline (0.0001-0.0025) quad stereo camera and prove that a trained\nnetwork provides a disparity prediction from the 2D phase correlation output by\nthe Tile Processor (TP) that is twice as accurate as the prediction from a\ncarefully hand-crafted algorithm. The TP in turn reduces the dimensions of the\ninput features of the network and provides instrument-invariant and\ntranslation-invariant data, making real-time high resolution stereo 3D\nperception feasible and easing the requirement to have a complete end-to-end\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 00:26:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Filippov", "Andrey", ""], ["Dzhimiev", "Oleg", ""]]}, {"id": "1811.08043", "submitter": "Md Amirul Islam", "authors": "Rezaul Karim, Md Amirul Islam, Neil D. B. Bruce", "title": "Recurrent Iterative Gating Networks for Semantic Segmentation", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for Recurrent Iterative Gating called\nRIGNet. The core elements of RIGNet involve recurrent connections that control\nthe flow of information in neural networks in a top-down manner, and different\nvariants on the core structure are considered. The iterative nature of this\nmechanism allows for gating to spread in both spatial extent and feature space.\nThis is revealed to be a powerful mechanism with broad compatibility with\ncommon existing networks. Analysis shows how gating interacts with different\nnetwork characteristics, and we also show that more shallow networks with\ngating may be made to perform better than much deeper networks that do not\ninclude RIGNet modules.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:26:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Karim", "Rezaul", ""], ["Islam", "Md Amirul", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1811.08051", "submitter": "Kuan-Chuan Peng", "authors": "Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, Rama\n  Chellappa", "title": "Learning without Memorizing", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning (IL) is an important task aimed at increasing the\ncapability of a trained model, in terms of the number of classes recognizable\nby the model. The key problem in this task is the requirement of storing data\n(e.g. images) associated with existing classes, while teaching the classifier\nto learn new classes. However, this is impractical as it increases the memory\nrequirement at every incremental step, which makes it impossible to implement\nIL algorithms on edge devices with limited memory. Hence, we propose a novel\napproach, called `Learning without Memorizing (LwM)', to preserve the\ninformation about existing (base) classes, without storing any of their data,\nwhile making the classifier progressively learn the new classes. In LwM, we\npresent an information preserving penalty: Attention Distillation Loss\n($L_{AD}$), and demonstrate that penalizing the changes in classifiers'\nattention maps helps to retain information of the base classes, as new classes\nare added. We show that adding $L_{AD}$ to the distillation loss which is an\nexisting information preserving loss consistently outperforms the\nstate-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in\nterms of the overall accuracy of base and incrementally learned classes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:20:16 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:39:30 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Dhar", "Prithviraj", ""], ["Singh", "Rajat Vikram", ""], ["Peng", "Kuan-Chuan", ""], ["Wu", "Ziyan", ""], ["Chellappa", "Rama", ""]]}, {"id": "1811.08056", "submitter": "Dae Hoon Park", "authors": "Dae Hoon Park, Chiu Man Ho, Yi Chang, Huaqing Zhang", "title": "Gradient-Coherent Strong Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization plays an important role in generalization of deep neural\nnetworks, which are often prone to overfitting with their numerous parameters.\nL1 and L2 regularizers are common regularization tools in machine learning with\ntheir simplicity and effectiveness. However, we observe that imposing strong L1\nor L2 regularization with stochastic gradient descent on deep neural networks\neasily fails, which limits the generalization ability of the underlying neural\nnetworks. To understand this phenomenon, we first investigate how and why\nlearning fails when strong regularization is imposed on deep neural networks.\nWe then propose a novel method, gradient-coherent strong regularization, which\nimposes regularization only when the gradients are kept coherent in the\npresence of strong regularization. Experiments are performed with multiple deep\narchitectures on three benchmark data sets for image recognition. Experimental\nresults show that our proposed approach indeed endures strong regularization\nand significantly improves both accuracy and compression (up to 9.9x), which\ncould not be achieved otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:41:56 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 01:52:12 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Park", "Dae Hoon", ""], ["Ho", "Chiu Man", ""], ["Chang", "Yi", ""], ["Zhang", "Huaqing", ""]]}, {"id": "1811.08063", "submitter": "Dzung Doan Anh", "authors": "Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Yu Liu, Shin-Fang Ch'ng,\n  Thanh-Toan Do, Ian Reid", "title": "Visual Localization Under Appearance Change: Filtering Approaches", "comments": "To appear in Neural Computing and Applications-Special Issue on Best\n  of DICTA 2019. Its conference version won APRS/IAPR Best paper award at DICTA\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major focus of current research on place recognition is visual localization\nfor autonomous driving. In this scenario, as cameras will be operating\ncontinuously, it is realistic to expect videos as an input to visual\nlocalization algorithms, as opposed to the single-image querying approach used\nin other visual localization works. In this paper, we show that exploiting\ntemporal continuity in the testing sequence significantly improves visual\nlocalization - qualitatively and quantitatively. Although intuitive, this idea\nhas not been fully explored in recent works. To this end, we propose two\nfiltering approaches to exploit the temporal smoothness of image sequences: i)\nfiltering on discrete domain with Hidden Markov Model, and ii) filtering on\ncontinuous domain with Monte Carlo-based visual localization. Our approaches\nrely on local features with an encoding technique to represent an image as a\nsingle vector. The experimental results on synthetic and real datasets show\nthat our proposed methods achieve better results than state of the art (i.e.,\ndeep learning-based pose regression approaches) for the task on visual\nlocalization under significant appearance change. Our synthetic dataset and\nsource code are made publicly available at\nhttps://sites.google.com/view/g2d-software/home and\nhttps://github.com/dadung/Visual-Localization-Filtering.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:02:12 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 00:51:56 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 02:52:54 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 06:21:03 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Doan", "Anh-Dzung", ""], ["Latif", "Yasir", ""], ["Chin", "Tat-Jun", ""], ["Liu", "Yu", ""], ["Ch'ng", "Shin-Fang", ""], ["Do", "Thanh-Toan", ""], ["Reid", "Ian", ""]]}, {"id": "1811.08067", "submitter": "Arpit Agarwal Mr.", "authors": "Ricson Cheng, Arpit Agarwal, Katerina Fragkiadaki", "title": "Reinforcement Learning of Active Vision for Manipulating Objects under\n  Occlusions", "comments": "The paper was present in Conference of Robot Learning 2018", "journal-ref": "Proceedings of Machine Learning Research 87 (2018) 422--431", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider artificial agents that learn to jointly control their gripperand\ncamera in order to reinforcement learn manipulation policies in the presenceof\nocclusions from distractor objects. Distractors often occlude the object of\nin-terest and cause it to disappear from the field of view. We propose hand/eye\ncon-trollers that learn to move the camera to keep the object within the field\nof viewand visible, in coordination to manipulating it to achieve the desired\ngoal, e.g.,pushing it to a target location. We incorporate structural biases of\nobject-centricattention within our actor-critic architectures, which our\nexperiments suggest tobe a key for good performance. Our results further\nhighlight the importance ofcurriculum with regards to environment difficulty.\nThe resulting active vision /manipulation policies outperform static camera\nsetups for a variety of clutteredenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:24:38 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 15:10:51 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Cheng", "Ricson", ""], ["Agarwal", "Arpit", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1811.08073", "submitter": "Pengyuan Ren", "authors": "Pengyuan Ren, Jianmin Li", "title": "Factorized Distillation: Training Holistic Person Re-identification\n  Model by Distilling an Ensemble of Partial ReID Models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is aimed at identifying the same person\nacross videos captured from different cameras. In the view that networks\nextracting global features using ordinary network architectures are difficult\nto extract local features due to their weak attention mechanisms, researchers\nhave proposed a lot of elaborately designed ReID networks, while greatly\nimproving the accuracy, the model size and the feature extraction latency are\nalso soaring. We argue that a relatively compact ordinary network extracting\nglobally pooled features has the capability to extract discriminative local\nfeatures and can achieve state-of-the-art precision if only the model's\nparameters are properly learnt. In order to reduce the difficulty in learning\nhard identity labels, we propose a novel knowledge distillation method:\nFactorized Distillation, which factorizes both feature maps and retrieval\nfeatures of holistic ReID network to mimic representations of multiple partial\nReID models, thus transferring the knowledge from partial ReID models to the\nholistic network. Experiments show that the performance of model trained with\nthe proposed method can outperform state-of-the-art with relatively few network\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:50:09 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ren", "Pengyuan", ""], ["Li", "Jianmin", ""]]}, {"id": "1811.08075", "submitter": "Weilin Cong", "authors": "Weilin Cong, William Wang, Wang-Chien Lee", "title": "Scene Graph Generation via Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success object detection and segmentation models have\nachieved in recognizing individual objects in images, performance on cognitive\ntasks such as image caption, semantic image retrieval, and visual QA is far\nfrom satisfactory. To achieve better performance on these cognitive tasks,\nmerely recognizing individual object instances is insufficient. Instead, the\ninteractions between object instances need to be captured in order to\nfacilitate reasoning and understanding of the visual scenes in an image. Scene\ngraph, a graph representation of images that captures object instances and\ntheir relationships, offers a comprehensive understanding of an image. However,\nexisting techniques on scene graph generation fail to distinguish subjects and\nobjects in the visual scenes of images and thus do not perform well with\nreal-world datasets where exist ambiguous object instances. In this work, we\npropose a novel scene graph generation model for predicting object instances\nand its corresponding relationships in an image. Our model, SG-CRF, learns the\nsequential order of subject and object in a relationship triplet, and the\nsemantic compatibility of object instance nodes and relationship nodes in a\nscene graph efficiently. Experiments empirically show that SG-CRF outperforms\nthe state-of-the-art methods, on three different datasets, i.e., CLEVR, VRD,\nand Visual Genome, raising the Recall@100 from 24.99% to 49.95%, from 41.92% to\n50.47%, and from 54.69% to 54.77%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:55:07 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Cong", "Weilin", ""], ["Wang", "William", ""], ["Lee", "Wang-Chien", ""]]}, {"id": "1811.08080", "submitter": "Kazuya Kakizaki", "authors": "Hajime Ono, Tsubasa Takahashi, Kazuya Kakizaki", "title": "Lightweight Lipschitz Margin Training for Certified Defense against\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we make machine learning provably robust against adversarial examples\nin a scalable way? Since certified defense methods, which ensure\n$\\epsilon$-robust, consume huge resources, they can only achieve small degree\nof robustness in practice. Lipschitz margin training (LMT) is a scalable\ncertified defense, but it can also only achieve small robustness due to\nover-regularization. How can we make certified defense more efficiently? We\npresent LC-LMT, a light weight Lipschitz margin training which solves the above\nproblem. Our method has the following properties; (a) efficient: it can achieve\n$\\epsilon$-robustness at early epoch, and (b) robust: it has a potential to get\nhigher robustness than LMT. In the evaluation, we demonstrate the benefits of\nthe proposed method. LC-LMT can achieve required robustness more than 30 epoch\nearlier than LMT in MNIST, and shows more than 90 $\\%$ accuracy against both\nlegitimate and adversarial inputs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:22:55 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ono", "Hajime", ""], ["Takahashi", "Tsubasa", ""], ["Kakizaki", "Kazuya", ""]]}, {"id": "1811.08081", "submitter": "Yuchen Li", "authors": "Safwan Hossain, Kiarash Jamali, Yuchen Li, Frank Rudzicz", "title": "ChainGAN: A sequential approach to GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture and training methodology for generative\nadversarial networks. Current approaches attempt to learn the transformation\nfrom a noise sample to a generated data sample in one shot. Our proposed\ngenerator architecture, called $\\textit{ChainGAN}$, uses a two-step process. It\nfirst attempts to transform a noise vector into a crude sample, similar to a\ntraditional generator. Next, a chain of networks, called $\\textit{editors}$,\nattempt to sequentially enhance this sample. We train each of these units\nindependently, instead of with end-to-end backpropagation on the entire chain.\nOur model is robust, efficient, and flexible as we can apply it to various\nnetwork architectures. We provide rationale for our choices and experimentally\nevaluate our model, achieving competitive results on several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:30:32 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:56:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Hossain", "Safwan", ""], ["Jamali", "Kiarash", ""], ["Li", "Yuchen", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1811.08103", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Yanwei Pang, Jichang Guo, Zhongfei Zhang, and\n  Fei Wu", "title": "Bi-Adversarial Auto-Encoder for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing generative Zero-Shot Learning (ZSL) methods only consider the\nunidirectional alignment from the class semantics to the visual features while\nignoring the alignment from the visual features to the class semantics, which\nfails to construct the visual-semantic interactions well. In this paper, we\npropose to synthesize visual features based on an auto-encoder framework paired\nwith bi-adversarial networks respectively for visual and semantic modalities to\nreinforce the visual-semantic interactions with a bi-directional alignment,\nwhich ensures the synthesized visual features to fit the real visual\ndistribution and to be highly related to the semantics. The encoder aims at\nsynthesizing real-like visual features while the decoder forces both the real\nand the synthesized visual features to be more related to the class semantics.\nTo further capture the discriminative information of the synthesized visual\nfeatures, both the real and synthesized visual features are forced to be\nclassified into the correct classes via a classification network. Experimental\nresults on four benchmark datasets show that the proposed approach is\nparticularly competitive on both the traditional ZSL and the generalized ZSL\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:29:28 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Yu", "Yunlong", ""], ["Ji", "Zhong", ""], ["Pang", "Yanwei", ""], ["Guo", "Jichang", ""], ["Zhang", "Zhongfei", ""], ["Wu", "Fei", ""]]}, {"id": "1811.08106", "submitter": "Donghui Sun", "authors": "Donghui Sun, Qing Zhang and Jun Yang", "title": "Pyramid Embedded Generative Adversarial Network for Automated Font\n  Generation", "comments": "6 pages, 7 figures, accepted by International Conference on Pattern\n  Recognition (ICPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the Chinese font synthesis problem and propose\na Pyramid Embedded Generative Adversarial Network (PEGAN) to automatically\ngenerate Chinese character images. The PEGAN consists of one generator and one\ndiscriminator. The generator is built using one encoder-decoder structure with\ncascaded refinement connections and mirror skip connections. The cascaded\nrefinement connections embed a multiscale pyramid of downsampled original input\ninto the encoder feature maps of different layers, and multi-scale feature maps\nfrom the encoder are connected to the corresponding feature maps in the decoder\nto make the mirror skip connections. Through combining the generative\nadversarial loss, pixel-wise loss, category loss and perceptual loss, the\ngenerator and discriminator can be trained alternately to synthesize character\nimages. In order to verify the effectiveness of our proposed PEGAN, we first\nbuild one evaluation set, in which the characters are selected according to\ntheir stroke number and frequency of use, and then use both qualitative and\nquantitative metrics to measure the performance of our model comparing with the\nbaseline method. The experimental results demonstrate the effectiveness of our\nproposed model, it shows the potential to automatically extend small font banks\ninto complete ones.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:37:46 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sun", "Donghui", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""]]}, {"id": "1811.08115", "submitter": "Jingjing Wu", "authors": "Hao Liu and Jingjing Wu and Jianguo Jiang and Meibin Qi and Bo Ren", "title": "Sequence-based Person Attribute Recognition with Joint CTC-Attention\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute recognition has become crucial because of its wide applications in\nmany computer vision tasks, such as person re-identification. Like many object\nrecognition problems, variations in viewpoints, illumination, and recognition\nat far distance, all make this task challenging. In this work, we propose a\njoint CTC-Attention model (JCM), which maps attribute labels into sequences to\nlearn the semantic relationship among attributes. Besides, this network uses\nneural network to encode images into sequences, and employs connectionist\ntemporal classification (CTC) loss to train the network with the aim of\nimproving the encoding performance of the network. At the same time, it adopts\nthe attention model to decode the sequences, which can realize aligning the\nsequences and better learning the semantic information from attributes.\nExtensive experiments on three public datasets, i.e., Market-1501 attribute\ndataset, Duke attribute dataset and PETA dataset, demonstrate the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:18:31 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 07:48:28 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liu", "Hao", ""], ["Wu", "Jingjing", ""], ["Jiang", "Jianguo", ""], ["Qi", "Meibin", ""], ["Ren", "Bo", ""]]}, {"id": "1811.08126", "submitter": "Roey Mechrez", "authors": "Firas Shama, Roey Mechrez, Alon Shoshan, Lihi Zelnik-Manor", "title": "Adversarial Feedback Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their remarkable generative capabilities, GANs have gained great\npopularity, and are used abundantly in state-of-the-art methods and\napplications. In a GAN based model, a discriminator is trained to learn the\nreal data distribution. To date, it has been used only for training purposes,\nwhere it's utilized to train the generator to provide real-looking outputs. In\nthis paper we propose a novel method that makes an explicit use of the\ndiscriminator in test-time, in a feedback manner in order to improve the\ngenerator results. To the best of our knowledge it is the first time a\ndiscriminator is involved in test-time. We claim that the discriminator holds\nsignificant information on the real data distribution, that could be useful for\ntest-time as well, a potential that has not been explored before.\n  The approach we propose does not alter the conventional training stage. At\ntest-time, however, it transfers the output from the generator into the\ndiscriminator, and uses feedback modules (convolutional blocks) to translate\nthe features of the discriminator layers into corrections to the features of\nthe generator layers, which are used eventually to get a better generator\nresult. Our method can contribute to both conditional and unconditional GANs.\nAs demonstrated by our experiments, it can improve the results of\nstate-of-the-art networks for super-resolution, and image generation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:53:27 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Shama", "Firas", ""], ["Mechrez", "Roey", ""], ["Shoshan", "Alon", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1811.08138", "submitter": "Chao Chen", "authors": "Chao Chen, Sheng Zhang, Cuibing Du", "title": "Learning to Detect Instantaneous Changes with Retrospective Convolution\n  and Static Sample Synthesis", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection has been a challenging visual task due to the dynamic nature\nof real-world scenes. Good performance of existing methods depends largely on\nprior background images or a long-term observation. These methods, however,\nsuffer severe degradation when they are applied to detection of instantaneously\noccurred changes with only a few preceding frames provided. In this paper, we\nexploit spatio-temporal convolutional networks to address this challenge, and\npropose a novel retrospective convolution, which features efficient change\ninformation extraction between the current frame and frames from historical\nobservation. To address the problem of foreground-specific over-fitting in\nlearning-based methods, we further propose a data augmentation method, named\nstatic sample synthesis, to guide the network to focus on learning change-cued\ninformation rather than specific spatial features of foreground. Trained\nend-to-end with complex scenarios, our framework proves to be accurate in\ndetecting instantaneous changes and robust in combating diverse noises.\nExtensive experiments demonstrate that our proposed method significantly\noutperforms existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:17:30 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Chao", ""], ["Zhang", "Sheng", ""], ["Du", "Cuibing", ""]]}, {"id": "1811.08139", "submitter": "Sergei Divakov", "authors": "Sergei Divakov and Ivan Oseledets", "title": "Adversarial point set registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to point set registration which is based on\none-shot adversarial learning. The idea of the algorithm is inspired by recent\nsuccesses of generative adversarial networks. Treating the point clouds as\nthree-dimensional probability distributions, we develop a one-shot adversarial\noptimization procedure, in which we train a critic neural network to\ndistinguish between source and target point sets, while simultaneously learning\nthe parameters of the transformation to trick the critic into confusing the\npoints. In contrast to most existing algorithms for point set registration,\nours does not rely on any correspondences between the point clouds. We\ndemonstrate the performance of the algorithm on several challenging benchmarks\nand compare it to the existing baselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:20:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Divakov", "Sergei", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1811.08152", "submitter": "Rohit Gandikota", "authors": "Rohit Gandikota and Deepak Mishra", "title": "How You See Me", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution Neural Networks is one of the most powerful tools in the present\nera of science. There has been a lot of research done to improve their\nperformance and robustness while their internal working was left unexplored to\nmuch extent. They are often defined as black boxes that can map non-linear data\nvery effectively. This paper tries to show how CNN has learned to look at an\nimage. The proposed algorithm exploits the basic math of CNN to backtrack the\nimportant pixels it is considering to predict. This is a simple algorithm which\ndoes not involve any training of its own over a pre-trained CNN which can\nclassify.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:44:10 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gandikota", "Rohit", ""], ["Mishra", "Deepak", ""]]}, {"id": "1811.08164", "submitter": "Qingjie Meng", "authors": "Qingjie Meng, Matthew Sinclair, Veronika Zimmer, Benjamin Hou, Martin\n  Rajchl, Nicolas Toussaint, Ozan Oktay, Jo Schlemper, Alberto Gomez, James\n  Housden, Jacqueline Matthew, Daniel Rueckert, Julia Schnabel and Bernhard\n  Kainz", "title": "Weakly Supervised Estimation of Shadow Confidence Maps in Fetal\n  Ultrasound Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting acoustic shadows in ultrasound images is important in many clinical\nand engineering applications. Real-time feedback of acoustic shadows can guide\nsonographers to a standardized diagnostic viewing plane with minimal artifacts\nand can provide additional information for other automatic image analysis\nalgorithms. However, automatically detecting shadow regions using\nlearning-based algorithms is challenging because pixel-wise ground truth\nannotation of acoustic shadows is subjective and time consuming. In this paper\nwe propose a weakly supervised method for automatic confidence estimation of\nacoustic shadow regions. Our method is able to generate a dense shadow-focused\nconfidence map. In our method, a shadow-seg module is built to learn general\nshadow features for shadow segmentation, based on global image-level\nannotations as well as a small number of coarse pixel-wise shadow annotations.\nA transfer function is introduced to extend the obtained binary shadow\nsegmentation to a reference confidence map. Additionally, a confidence\nestimation network is proposed to learn the mapping between input images and\nthe reference confidence maps. This network is able to predict shadow\nconfidence maps directly from input images during inference. We use evaluation\nmetrics such as DICE, inter-class correlation and etc. to verify the\neffectiveness of our method. Our method is more consistent than human\nannotation, and outperforms the state-of-the-art quantitatively in shadow\nsegmentation and qualitatively in confidence estimation of shadow regions. We\nfurther demonstrate the applicability of our method by integrating shadow\nconfidence maps into tasks such as ultrasound image classification, multi-view\nimage fusion and automated biometric measurements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:20:39 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 19:31:06 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 17:03:28 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Meng", "Qingjie", ""], ["Sinclair", "Matthew", ""], ["Zimmer", "Veronika", ""], ["Hou", "Benjamin", ""], ["Rajchl", "Martin", ""], ["Toussaint", "Nicolas", ""], ["Oktay", "Ozan", ""], ["Schlemper", "Jo", ""], ["Gomez", "Alberto", ""], ["Housden", "James", ""], ["Matthew", "Jacqueline", ""], ["Rueckert", "Daniel", ""], ["Schnabel", "Julia", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1811.08165", "submitter": "Christian A. Mueller", "authors": "Christian A. Mueller and Andreas Birk", "title": "Unsupervised Learning of Shape Concepts - From Real-World Objects to\n  Mental Simulation", "comments": "Submitted (preprint version) to IEEE Transactions on Cognitive and\n  Developmental Systems. arXiv admin note: substantial text overlap with\n  arXiv:1803.02140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised shape analysis is proposed to learn concepts reflecting shape\ncommonalities. Our approach is two-fold: i) a spatial topology analysis of\npoint cloud segment constellations within objects is used in which\nconstellations are decomposed and described in a hierarchical and symbolic\nmanner. ii) A topology analysis of the description space is used in which\nsegment decompositions are exposed in. Inspired by Persistent Homology, groups\nof shape commonality are revealed. Experiments show that extracted persistent\ncommonality groups can feature semantically meaningful shape concepts; the\ngeneralization of the proposed approach is evaluated by different real-world\ndatasets. We extend this by not only learning shape concepts using real-world\ndata, but by also using mental simulation of artificial abstract objects for\ntraining purposes. This extended approach is unsupervised in two respects:\nlabel-agnostic (no label information is used) and instance-agnostic (no\ninstances preselected by human supervision are used for training). Experiments\nshow that concepts generated with mental simulation, generalize and\ndiscriminate real object observations. Consequently, a robot may train and\nlearn its own internal representation of concepts regarding shape appearance in\na self-driven and machine-centric manner while omitting the tedious process of\nsupervised dataset generation including the ambiguity in instance labeling and\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:21:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Mueller", "Christian A.", ""], ["Birk", "Andreas", ""]]}, {"id": "1811.08170", "submitter": "Lei Li", "authors": "Lei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, Chiew-Lan\n  Tai", "title": "Sketch-R2CNN: An Attentive Network for Vector Sketch Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand sketching is a dynamic process where points are sequentially sampled\nand grouped as strokes for sketch acquisition on electronic devices. To\nrecognize a sketched object, most existing methods discard such important\ntemporal ordering and grouping information from human and simply rasterize\nsketches into binary images for classification. In this paper, we propose a\nnovel single-branch attentive network architecture RNN-Rasterization-CNN\n(Sketch-R2CNN for short) to fully leverage the dynamics in sketches for\nrecognition. Sketch-R2CNN takes as input only a vector sketch with grouped\nsequences of points, and uses an RNN for stroke attention estimation in the\nvector space and a CNN for 2D feature extraction in the pixel space\nrespectively. To bridge the gap between these two spaces in neural networks, we\npropose a neural line rasterization module to convert the vector sketch along\nwith the attention estimated by RNN into a bitmap image, which is subsequently\nconsumed by CNN. The neural line rasterization module is designed in a\ndifferentiable way to yield a unified pipeline for end-to-end learning. We\nperform experiments on existing large-scale sketch recognition benchmarks and\nshow that by exploiting the sketch dynamics with the attention mechanism, our\nmethod is more robust and achieves better performance than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:44:34 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Li", "Lei", ""], ["Zou", "Changqing", ""], ["Zheng", "Youyi", ""], ["Su", "Qingkun", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "1811.08180", "submitter": "Ning Yu", "authors": "Ning Yu, Larry Davis, Mario Fritz", "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints", "comments": "Accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nincreasing success in generating photorealistic images. But they also raise\nchallenges to visual forensics and model attribution. We present the first\nstudy of learning GAN fingerprints towards image attribution and using them to\nclassify an image as real or GAN-generated. For GAN-generated images, we\nfurther identify their sources. Our experiments show that (1) GANs carry\ndistinct model fingerprints and leave stable fingerprints in their generated\nimages, which support image attribution; (2) even minor differences in GAN\ntraining can result in different fingerprints, which enables fine-grained model\nauthentication; (3) fingerprints persist across different image frequencies and\npatches and are not biased by GAN artifacts; (4) fingerprint finetuning is\neffective in immunizing against five types of adversarial image perturbations;\nand (5) comparisons also show our learned fingerprints consistently outperform\nseveral baselines in a variety of setups.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:11:21 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 13:19:40 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 17:11:32 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Yu", "Ning", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "1811.08188", "submitter": "Thomas Roddick", "authors": "Thomas Roddick, Alex Kendall, Roberto Cipolla", "title": "Orthographic Feature Transform for Monocular 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection from monocular images has proven to be an enormously\nchallenging task, with the performance of leading systems not yet achieving\neven 10\\% of that of LiDAR-based counterparts. One explanation for this\nperformance gap is that existing systems are entirely at the mercy of the\nperspective image-based representation, in which the appearance and scale of\nobjects varies drastically with depth and meaningful distances are difficult to\ninfer. In this work we argue that the ability to reason about the world in 3D\nis an essential element of the 3D object detection task. To this end, we\nintroduce the orthographic feature transform, which enables us to escape the\nimage domain by mapping image-based features into an orthographic 3D space.\nThis allows us to reason holistically about the spatial configuration of the\nscene in a domain where scale is consistent and distances between objects are\nmeaningful. We apply this transformation as part of an end-to-end deep learning\narchitecture and achieve state-of-the-art performance on the KITTI 3D object\nbenchmark.\\footnote{We will release full source code and pretrained models upon\nacceptance of this manuscript for publication.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:31:53 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Roddick", "Thomas", ""], ["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1811.08196", "submitter": "YeonKun Lee", "authors": "Yeonkun Lee, Jaeseok Jeong, Jongseob Yun, Wonjune Cho, Kuk-Jin Yoon", "title": "SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360\n  degree Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omni-directional cameras have many advantages overconventional cameras in\nthat they have a much wider field-of-view (FOV). Accordingly, several\napproaches have beenproposed recently to apply convolutional neural\nnetworks(CNNs) to omni-directional images for various visual tasks.However,\nmost of them use image representations defined inthe Euclidean space after\ntransforming the omni-directionalviews originally formed in the non-Euclidean\nspace. Thistransformation leads to shape distortion due to nonuniformspatial\nresolving power and the loss of continuity. Theseeffects make existing\nconvolution kernels experience diffi-culties in extracting meaningful\ninformation.This paper presents a novel method to resolve such prob-lems of\napplying CNNs to omni-directional images. Theproposed method utilizes a\nspherical polyhedron to rep-resent omni-directional views. This method\nminimizes thevariance of the spatial resolving power on the sphere sur-face,\nand includes new convolution and pooling methodsfor the proposed\nrepresentation. The proposed method canalso be adopted by any existing\nCNN-based methods. Thefeasibility of the proposed method is demonstrated\nthroughclassification, detection, and semantic segmentation taskswith synthetic\nand real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:56:52 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 10:19:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lee", "Yeonkun", ""], ["Jeong", "Jaeseok", ""], ["Yun", "Jongseob", ""], ["Cho", "Wonjune", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1811.08201", "submitter": "Tianyi Wu", "authors": "Tianyi Wu, Sheng Tang, Rui Zhang, Yongdong Zhang", "title": "CGNet: A Light-weight Context Guided Network for Semantic Segmentation", "comments": "Code: https://github.com/wutianyiRosun/CGNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand of applying semantic segmentation model on mobile devices has been\nincreasing rapidly. Current state-of-the-art networks have enormous amount of\nparameters hence unsuitable for mobile devices, while other small memory\nfootprint models follow the spirit of classification network and ignore the\ninherent characteristic of semantic segmentation. To tackle this problem, we\npropose a novel Context Guided Network (CGNet), which is a light-weight and\nefficient network for semantic segmentation. We first propose the Context\nGuided (CG) block, which learns the joint feature of both local feature and\nsurrounding context, and further improves the joint feature with the global\ncontext. Based on the CG block, we develop CGNet which captures contextual\ninformation in all stages of the network and is specially tailored for\nincreasing segmentation accuracy. CGNet is also elaborately designed to reduce\nthe number of parameters and save memory footprint. Under an equivalent number\nof parameters, the proposed CGNet significantly outperforms existing\nsegmentation networks. Extensive experiments on Cityscapes and CamVid datasets\nverify the effectiveness of the proposed approach. Specifically, without any\npost-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean\nIoU on Cityscapes with less than 0.5 M parameters. The source code for the\ncomplete system can be found at https://github.com/wutianyiRosun/CGNet.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:04:50 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 07:32:07 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wu", "Tianyi", ""], ["Tang", "Sheng", ""], ["Zhang", "Rui", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1811.08223", "submitter": "Ramanarayan Mohanty", "authors": "Ramanarayan Mohanty, SL Happy and Aurobinda Routray", "title": "A Semi-supervised Spatial Spectral Regularized Manifold Local Scaling\n  Cut With HGF for Dimensionality Reduction of Hyperspectral Images", "comments": null, "journal-ref": "IEEE Transaction on Geoscience and Remote Sensing, 2018", "doi": "10.1109/TGRS.2018.2884771", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSI) contain a wealth of information over hundreds of\ncontiguous spectral bands, making it possible to classify materials through\nsubtle spectral discrepancies. However, the classification of this rich\nspectral information is accompanied by the challenges like high dimensionality,\nsingularity, limited training samples, lack of labeled data samples,\nheteroscedasticity and nonlinearity. To address these challenges, we propose a\nsemi-supervised graph based dimensionality reduction method named\n`semi-supervised spatial spectral regularized manifold local scaling cut'\n(S3RMLSC). The underlying idea of the proposed method is to exploit the limited\nlabeled information from both the spectral and spatial domains along with the\nabundant unlabeled samples to facilitate the classification task by retaining\nthe original distribution of the data. In S3RMLSC, a hierarchical guided filter\n(HGF) is initially used to smoothen the pixels of the HSI data to preserve the\nspatial pixel consistency. This step is followed by the construction of linear\npatches from the nonlinear manifold by using the maximal linear patch (MLP)\ncriterion. Then the inter-patch and intra-patch dissimilarity matrices are\nconstructed in both spectral and spatial domains by regularized manifold local\nscaling cut (RMLSC) and neighboring pixel manifold local scaling cut (NPMLSC)\nrespectively. Finally, we obtain the projection matrix by optimizing the\nupdated semi-supervised spatial-spectral between-patch and total-patch\ndissimilarity. The effectiveness of the proposed DR algorithm is illustrated\nwith publicly available real-world HSI datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:56:39 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mohanty", "Ramanarayan", ""], ["Happy", "SL", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1811.08230", "submitter": "Lin Wang", "authors": "S. Mohammad Mostafavi I., Lin Wang, Yo-Sung Ho, Kuk-Jin Yoon", "title": "Event-based High Dynamic Range Image and Very High Frame Rate Video\n  Generation using Conditional Generative Adversarial Networks", "comments": "10 pages,", "journal-ref": null, "doi": "10.1109/CVPR.2019.01032", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras have a lot of advantages over traditional cameras, such as low\nlatency, high temporal resolution, and high dynamic range. However, since the\noutputs of event cameras are the sequences of asynchronous events overtime\nrather than actual intensity images, existing algorithms could not be directly\napplied. Therefore, it is demanding to generate intensity images from events\nfor other tasks. In this paper, we unlock the potential of event camera-based\nconditional generative adversarial networks to create images/videos from an\nadjustable portion of the event data stream. The stacks of space-time\ncoordinates of events are used as inputs and the network is trained to\nreproduce images based on the spatio-temporal intensity changes. The usefulness\nof event cameras to generate high dynamic range(HDR) images even in extreme\nillumination conditions and also non blurred images under rapid motion is also\nshown.In addition, the possibility of generating very high frame rate videos is\ndemonstrated, theoretically up to 1 million frames per second (FPS) since the\ntemporal resolution of event cameras are about 1{\\mu}s. Proposed methods are\nevaluated by comparing the results with the intensity images captured on the\nsame pixel grid-line of events using online available real datasets and\nsynthetic datasets produced by the event camera simulator.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:05:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["I.", "S. Mohammad Mostafavi", ""], ["Wang", "Lin", ""], ["Ho", "Yo-Sung", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1811.08264", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang,\n  Yan-Feng Wang, Cewu Lu", "title": "Transferable Interactiveness Knowledge for Human-Object Interaction\n  Detection", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) Detection is an important problem to\nunderstand how humans interact with objects. In this paper, we explore\nInteractiveness Knowledge which indicates whether human and object interact\nwith each other or not. We found that interactiveness knowledge can be learned\nacross HOI datasets, regardless of HOI category settings. Our core idea is to\nexploit an Interactiveness Network to learn the general interactiveness\nknowledge from multiple HOI datasets and perform Non-Interaction Suppression\nbefore HOI classification in inference. On account of the generalization of\ninteractiveness, interactiveness network is a transferable knowledge learner\nand can be cooperated with any HOI detection models to achieve desirable\nresults. We extensively evaluate the proposed method on HICO-DET and V-COCO\ndatasets. Our framework outperforms state-of-the-art HOI detection results by a\ngreat margin, verifying its efficacy and flexibility. Code is available at\nhttps://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 14:20:55 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 09:39:10 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 05:44:36 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 14:08:41 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Li", "Yong-Lu", ""], ["Zhou", "Siyuan", ""], ["Huang", "Xijie", ""], ["Xu", "Liang", ""], ["Ma", "Ze", ""], ["Fang", "Hao-Shu", ""], ["Wang", "Yan-Feng", ""], ["Lu", "Cewu", ""]]}, {"id": "1811.08278", "submitter": "Feiyang Chen", "authors": "Feiyang Chen, Nan Chen, Hanyang Mao, Hanlin Hu", "title": "Assessing four Neural Networks on Handwritten Digit Recognition Dataset\n  (MNIST)", "comments": "TPW course essay. arXiv admin note: text overlap with\n  arXiv:1709.04219 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the image recognition has been a research topic for many years, many\nresearchers still have a keen interest in it[1]. In some papers[2][3][4],\nhowever, there is a tendency to compare models only on one or two datasets,\neither because of time restraints or because the model is tailored to a\nspecific task. Accordingly, it is hard to understand how well a certain model\ngeneralizes across image recognition field[6]. In this paper, we compare four\nneural networks on MNIST dataset[5] with different division. Among them, three\nare Convolutional Neural Networks (CNN)[7], Deep Residual Network (ResNet)[2]\nand Dense Convolutional Network (DenseNet)[3] respectively, and the other is\nour improvement on CNN baseline through introducing Capsule Network\n(CapsNet)[1] to image recognition area. We show that the previous models\ndespite do a quite good job in this area, our retrofitting can be applied to\nget a better performance. The result obtained by CapsNet is an accuracy rate of\n99.75\\%, and it is the best result published so far. Another inspiring result\nis that CapsNet only needs a small amount of data to get excellent performance.\nFinally, we will apply CapsNet's ability to generalize in other image\nrecognition field in the future.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 23:55:57 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 03:24:24 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Feiyang", ""], ["Chen", "Nan", ""], ["Mao", "Hanyang", ""], ["Hu", "Hanlin", ""]]}, {"id": "1811.08290", "submitter": "Junjie Huang", "authors": "Junjie Huang, Wei Zou, Zheng Zhu, Jiagang Zhu", "title": "An Efficient Optical Flow Based Motion Detection Method for\n  Non-stationary Scenes", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1807.04890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time motion detection in non-stationary scenes is a difficult task due\nto dynamic background, changing foreground appearance and limited computational\nresource. These challenges degrade the performance of the existing methods in\npractical applications. In this paper, an optical flow based framework is\nproposed to address this problem. By applying a novel strategy to utilize\noptical flow, we enable our method being free of model constructing, training\nor updating and can be performed efficiently. Besides, a dual judgment\nmechanism with adaptive intervals and adaptive thresholds is designed to\nheighten the system's adaptation to different situations. In experiment part,\nwe quantitatively and qualitatively validate the effectiveness and feasibility\nof our method with videos in various scene conditions. The experimental results\nshow that our method adapts itself to different situations and outperforms the\nstate-of-the-art real-time methods, indicating the advantages of our optical\nflow based method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:57:44 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 01:27:27 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Huang", "Junjie", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""], ["Zhu", "Jiagang", ""]]}, {"id": "1811.08305", "submitter": "Jose Dolz", "authors": "Jose Dolz and Christian Desrosiers and Ismail Ben Ayed", "title": "IVD-Net: Intervertebral disc localization and segmentation in MRI with a\n  multi-modal UNet", "comments": "Manuscript submitted to the Proceedings of the MICCAI 2018 IVD\n  Challenge. arXiv admin note: text overlap with arXiv:1810.07003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization and segmentation of intervertebral disc (IVD) is\ncrucial for the assessment of spine disease diagnosis. Despite the\ntechnological advances in medical imaging, IVD localization and segmentation\nare still manually performed, which is time-consuming and prone to errors. If,\nin addition, multi-modal imaging is considered, the burden imposed on disease\nassessments increases substantially. In this paper, we propose an architecture\nfor IVD localization and segmentation in multi-modal MRI, which extends the\nwell-known UNet. Compared to single images, multi-modal data brings\ncomplementary information, contributing to better data representation and\ndiscriminative power. Our contributions are three-fold. First, how to\neffectively integrate and fully leverage multi-modal data remains almost\nunexplored. In this work, each MRI modality is processed in a different path to\nbetter exploit their unique information. Second, inspired by HyperDenseNet, the\nnetwork is densely-connected both within each path and across different paths,\ngranting the model the freedom to learn where and how the different modalities\nshould be processed and combined. Third, we improved standard U-Net modules by\nextending inception modules with two dilated convolutions blocks of different\nscale, which helps handling multi-scale context. We report experiments over the\ndata set of the public MICCAI 2018 Challenge on Automatic Intervertebral Disc\nLocalization and Segmentation, with 13 multi-modal MRI images used for training\nand 3 for validation. We trained IVD-Net on an NVidia TITAN XP GPU with 16 GBs\nRAM, using ADAM as optimizer and a learning rate of 10e-5 during 200 epochs.\nTraining took about 5 hours, and segmentation of a whole volume about 2-3\nseconds, on average. Several baselines, with different multi-modal fusion\nstrategies, were used to demonstrate the effectiveness of the proposed\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:35:28 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1811.08321", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Vinay Sameer Raja Kadi, Nikhil Verma, Vinay P.\n  Namboodiri", "title": "Stability Based Filter Pruning for Accelerating Deep CNNs", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have achieved impressive performance on\nthe wide variety of tasks (classification, detection, etc.) across multiple\ndomains at the cost of high computational and memory requirements. Thus,\nleveraging CNNs for real-time applications necessitates model compression\napproaches that not only reduce the total number of parameters but reduce the\noverall computation as well. In this work, we present a stability-based\napproach for filter-level pruning of CNNs. We evaluate our proposed approach on\ndifferent architectures (LeNet, VGG-16, ResNet, and Faster RCNN) and datasets\nand demonstrate its generalizability through extensive experiments. Moreover,\nour compressed models can be used at run-time without requiring any special\nlibraries or hardware. Our model compression method reduces the number of FLOPS\nby an impressive factor of 6.03X and GPU memory footprint by more than 17X,\nsignificantly outperforming other state-of-the-art filter pruning methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 15:44:36 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Singh", "Pravendra", ""], ["Kadi", "Vinay Sameer Raja", ""], ["Verma", "Nikhil", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1811.08328", "submitter": "Marc Bosch", "authors": "Marc Bosch and Gordon A. Christie and Christopher M. Gifford", "title": "Sensor Adaptation for Improved Semantic Segmentation of Overhead Imagery", "comments": "Accepted publication at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a powerful method to facilitate visual scene\nunderstanding. Each pixel is assigned a label according to a pre-defined list\nof object classes and semantic entities. This becomes very useful as a means to\nsummarize large scale overhead imagery. In this paper we present our work on\nsemantic segmentation with applications to overhead imagery. We propose an\nalgorithm that builds and extends upon the DeepLab framework to be able to\nrefine and resolve small objects (relative to the image size) such as vehicles.\nWe have also investigated sensor adaptation as a means to augment available\ntraining data to be able to reduce some of the shortcomings of neural networks\nwhen deployed in new environments and to new sensors. We report results on\nseveral datasets and compare performance with other state-of-the-art\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 15:53:38 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Bosch", "Marc", ""], ["Christie", "Gordon A.", ""], ["Gifford", "Christopher M.", ""]]}, {"id": "1811.08342", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Manikandan R, Neeraj Matiyali, Vinay P. Namboodiri", "title": "Multi-layer Pruning Framework for Compressing Single Shot MultiBox\n  Detector", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for compressing state-of-the-art Single Shot MultiBox\nDetector (SSD). The framework addresses compression in the following stages:\nSparsity Induction, Filter Selection, and Filter Pruning. In the Sparsity\nInduction stage, the object detector model is sparsified via an improved global\nthreshold. In Filter Selection & Pruning stage, we select and remove filters\nusing sparsity statistics of filter weights in two consecutive convolutional\nlayers. This results in the model with the size smaller than most existing\ncompact architectures. We evaluate the performance of our framework with\nmultiple datasets and compare over multiple methods. Experimental results show\nthat our method achieves state-of-the-art compression of 6.7X and 4.9X on\nPASCAL VOC dataset on models SSD300 and SSD512 respectively. We further show\nthat the method produces maximum compression of 26X with SSD512 on German\nTraffic Sign Detection Benchmark (GTSDB). Additionally, we also empirically\nshow our method's adaptability for classification based architecture VGG16 on\ndatasets CIFAR and German Traffic Sign Recognition Benchmark (GTSRB) achieving\na compression rate of 125X and 200X with the reduction in flops by 90.50% and\n96.6% respectively with no loss of accuracy. In addition to this, our method\ndoes not require any special libraries or hardware support for the resulting\ncompressed models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:19:44 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Singh", "Pravendra", ""], ["R", "Manikandan", ""], ["Matiyali", "Neeraj", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1811.08345", "submitter": "Chaorong Li", "authors": "Chaorong Li, Huang Wei, Huafu Chen", "title": "LGLG-WPCA: An Effective Texture-based Method for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed an effective face feature extraction method by\nLearning Gabor Log-Euclidean Gaussian with Whitening Principal Component\nAnalysis (WPCA), called LGLG-WPCA. The proposed method learns face features\nfrom the embedded multivariate Gaussian in Gabor wavelet domain; it has the\nrobust performance to adverse conditions such as varying poses, skin aging and\nuneven illumination. Because the space of Gaussian is a Riemannian manifold and\nit is difficult to incorporate learning mechanism in the model. To address this\nissue, we use L2EMG to map the multidimensional Gaussian model to the linear\nspace, and then use WPCA to learn face features. We also implemented the\nkey-point-based version of LGLG-WPCA, called LGLG(KP)-WPCA. Experiments show\nthe proposed methods are effective and promising for face texture feature\nextraction and the combination of the feature of the proposed methods and the\nfeatures of Deep Convolutional Network (DCNN) achieved the best recognition\naccuracies on FERET database compared to the state-of-the-art methods. In the\nnext version of this paper, we will test the performance of the proposed\nmethods on the large-varying pose databases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:21:20 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 07:23:40 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 01:12:32 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 06:22:10 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Li", "Chaorong", ""], ["Wei", "Huang", ""], ["Chen", "Huafu", ""]]}, {"id": "1811.08362", "submitter": "Yunbo Wang", "authors": "Zhiyu Yao, Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S Yu,\n  Jiaguang Sun", "title": "Multi-Task Learning of Generalizable Representations for Video Action\n  Recognition", "comments": "ICME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic video action recognition, labels may not contain enough\ninformation about the diverse video appearance and dynamics, thus, existing\nmodels that are trained under the standard supervised learning paradigm may\nextract less generalizable features. We evaluate these models under a\ncross-dataset experiment setting, as the above label bias problem in video\nanalysis is even more prominent across different data sources. We find that\nusing the optical flows as model inputs harms the generalization ability of\nmost video recognition models.\n  Based on these findings, we present a multi-task learning paradigm for video\nclassification. Our key idea is to avoid label bias and improve the\ngeneralization ability by taking data as its own supervision or supervising\nconstraints on the data. First, we take the optical flows and the RGB frames by\ntaking them as auxiliary supervisions, and thus naming our model as Reversed\nTwo-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow\nprediction task and the frame reconstruction task by introducing a new training\nobjective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which\nconstraints the discrepancy of the multi-task features in a self-supervised\nmanner. Rev2Net is shown to be effective on the classic action recognition\ntask. It specifically shows a strong generalization ability in the\ncross-dataset experiments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:49:17 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 02:56:41 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yao", "Zhiyu", ""], ["Wang", "Yunbo", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S", ""], ["Sun", "Jiaguang", ""]]}, {"id": "1811.08383", "submitter": "Song Han", "authors": "Ji Lin, Chuang Gan, Song Han", "title": "TSM: Temporal Shift Module for Efficient Video Understanding", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth in video streaming gives rise to challenges on\nperforming video understanding at high accuracy and low computation cost.\nConventional 2D CNNs are computationally cheap but cannot capture temporal\nrelationships; 3D CNN based methods can achieve good performance but are\ncomputationally intensive, making it expensive to deploy. In this paper, we\npropose a generic and effective Temporal Shift Module (TSM) that enjoys both\nhigh efficiency and high performance. Specifically, it can achieve the\nperformance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the\nchannels along the temporal dimension; thus facilitate information exchanged\namong neighboring frames. It can be inserted into 2D CNNs to achieve temporal\nmodeling at zero computation and zero parameters. We also extended TSM to\nonline setting, which enables real-time low-latency online video recognition\nand video object detection. TSM is accurate and efficient: it ranks the first\nplace on the Something-Something leaderboard upon publication; on Jetson Nano\nand Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video\nrecognition. The code is available at:\nhttps://github.com/mit-han-lab/temporal-shift-module.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:39:26 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 04:04:00 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 16:31:16 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Lin", "Ji", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "1811.08390", "submitter": "Huan Wang", "authors": "Huan Wang, Qiming Zhang, Yuehai Wang, Haoji Hu", "title": "Structured Pruning for Efficient ConvNets via Incremental Regularization", "comments": "Accepted by NIPS 2018 workshop on \"Compact Deep Neural Network\n  Representation with Industrial Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter pruning is a promising approach for CNN compression and\nacceleration by eliminating redundant model parameters with tolerable\nperformance loss. Despite its effectiveness, existing regularization-based\nparameter pruning methods usually drive weights towards zero with large and\nconstant regularization factors, which neglects the fact that the\nexpressiveness of CNNs is fragile and needs a more gentle way of regularization\nfor the networks to adapt during pruning. To solve this problem, we propose a\nnew regularization-based pruning method (named IncReg) to incrementally assign\ndifferent regularization factors to different weight groups based on their\nrelative importance, whose effectiveness is proved on popular CNNs compared\nwith state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:52:54 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 23:49:32 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wang", "Huan", ""], ["Zhang", "Qiming", ""], ["Wang", "Yuehai", ""], ["Hu", "Haoji", ""]]}, {"id": "1811.08398", "submitter": "Charlie Hewitt", "authors": "Charlie Hewitt and Marwa Mahmoud", "title": "Shape-only Features for Plant Leaf Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel feature set for shape-only leaf identification\nmotivated by real-world, mobile deployment. The feature set includes basic\nshape features, as well as signal features extracted from local area integral\ninvariants (LAIIs), similar to curvature maps, at multiple scales. The proposed\nmethodology is evaluated on a number of publicly available leaf datasets with\ncomparable results to existing methods which make use of colour and texture\nfeatures in addition to shape. Over 90% classification accuracy is achieved on\nmost datasets, with top-four accuracy for these datasets reaching over 98%.\nRotation and scale invariance of the proposed features are demonstrated, along\nwith an evaluation of the generalisability of the approach for generic shape\nmatching.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:09:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Hewitt", "Charlie", ""], ["Mahmoud", "Marwa", ""]]}, {"id": "1811.08400", "submitter": "Qi Dong", "authors": "Qi Dong and Xiatian Zhu and Shaogang Gong", "title": "Single-Label Multi-Class Image Classification by Deep Logistic\n  Regression", "comments": "Accepted by AAAI-19, code at\n  https://github.com/qd301/FocusRectificationLogisticRegression", "journal-ref": "The Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective learning formulation is essential for the success of\nconvolutional neural networks. In this work, we analyse thoroughly the standard\nlearning objective functions for multi-class classification CNNs: softmax\nregression (SR) for single-label scenario and logistic regression (LR) for\nmulti-label scenario. Our analyses lead to an inspiration of exploiting LR for\nsingle-label classification learning, and then the disclosing of the negative\nclass distraction problem in LR. To address this problem, we develop two novel\nLR based objective functions that not only generalise the conventional LR but\nimportantly turn out to be competitive alternatives to SR in single label\nclassification. Extensive comparative evaluations demonstrate the model\nlearning advantages of the proposed LR functions over the commonly adopted SR\nin single-label coarse-grained object categorisation and cross-class\nfine-grained person instance identification tasks. We also show the performance\nsuperiority of our method on clothing attribute classification in comparison to\nthe vanilla LR function.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:19:36 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 11:29:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Dong", "Qi", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1811.08404", "submitter": "Daniel Nkemelu", "authors": "Daniel K. Nkemelu, Daniel Omeiza and Nancy Lubalo", "title": "Deep Convolutional Neural Network for Plant Seedlings Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agriculture is vital for human survival and remains a major driver of several\neconomies around the world; more so in underdeveloped and developing economies.\nWith increasing demand for food and cash crops, due to a growing global\npopulation and the challenges posed by climate change, there is a pressing need\nto increase farm outputs while incurring minimal costs. Previous machine vision\ntechnologies developed for selective weeding have faced the challenge of\nreliable and accurate weed detection. We present approaches for plant seedlings\nclassification with a dataset that contains 4,275 images of approximately 960\nunique plants belonging to 12 species at several growth stages. We compare the\nperformances of two traditional algorithms and a Convolutional Neural Network\n(CNN), a deep learning technique widely applied to image recognition, for this\ntask. Our findings show that CNN-driven seedling classification applications\nwhen used in farming automation has the potential to optimize crop yield and\nimprove productivity and efficiency when designed appropriately.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:22:54 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Nkemelu", "Daniel K.", ""], ["Omeiza", "Daniel", ""], ["Lubalo", "Nancy", ""]]}, {"id": "1811.08412", "submitter": "Qian Wang", "authors": "Qian Wang, Ning Jia, Toby P. Breckon", "title": "A Baseline for Multi-Label Image Classification Using An Ensemble of\n  Deep Convolutional Neural Networks", "comments": "IEEE International Conference on Image Processing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on multi-label image classification have focused on designing\nmore complex architectures of deep neural networks such as the use of attention\nmechanisms and region proposal networks. Although performance gains have been\nreported, the backbone deep models of the proposed approaches and the\nevaluation metrics employed in different works vary, making it difficult to\ncompare each fairly. Moreover, due to the lack of properly investigated\nbaselines, the advantage introduced by the proposed techniques are often\nambiguous. To address these issues, we make a thorough investigation of the\nmainstream deep convolutional neural network architectures for multi-label\nimage classification and present a strong baseline. With the use of proper data\naugmentation techniques and model ensembles, the basic deep architectures can\nachieve better performance than many existing more complex ones on three\nbenchmark datasets, providing great insight for the future studies on\nmulti-label image classification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:34:22 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 15:30:10 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 10:06:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Qian", ""], ["Jia", "Ning", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1811.08414", "submitter": "Mat\\'ias Mattamala", "authors": "Cristopher G\\'omez and Mat\\'ias Mattamala and Tim Resink and Javier\n  Ruiz-del-Solar", "title": "Visual SLAM-based Localization and Navigation for Service Robots: The\n  Pepper Case", "comments": "12 pages, 6 figures. Presented in RoboCup Symposium 2018. Final\n  version will appear in Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Visual-SLAM based localization and navigation system for service\nrobots. Our system is built on top of the ORB-SLAM monocular system but\nextended by the inclusion of wheel odometry in the estimation procedures. As a\ncase study, the proposed system is validated using the Pepper robot, whose\nshort-range LIDARs and RGB-D camera do not allow the robot to self-localize in\nlarge environments. The localization system is tested in navigation tasks using\nPepper in two different environments: a medium-size laboratory, and a\nlarge-size hall.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:41:35 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["G\u00f3mez", "Cristopher", ""], ["Mattamala", "Mat\u00edas", ""], ["Resink", "Tim", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "1811.08429", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Boosting in Image Quality Assessment", "comments": "Paper: 6 pages, 5 tables, 1 figure, Presentation: 16 slides\n  [Ancillary files]", "journal-ref": "D. Temel and G. AlRegib, \"Boosting in image quality assessment,\"\n  2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP),\n  Montreal, QC, 2016, pp. 1-6", "doi": "10.1109/MMSP.2016.7813335", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the effect of boosting in image quality assessment\nthrough multi-method fusion. Existing multi-method studies focus on proposing a\nsingle quality estimator. On the contrary, we investigate the generalizability\nof multi-method fusion as a framework. In addition to support vector machines\nthat are commonly used in the multi-method fusion, we propose using neural\nnetworks in the boosting. To span different types of image quality assessment\nalgorithms, we use quality estimators based on fidelity, perceptually-extended\nfidelity, structural similarity, spectral similarity, color, and learning. In\nthe experiments, we perform k-fold cross validation using the LIVE, the\nmultiply distorted LIVE, and the TID 2013 databases and the performance of\nimage quality assessment algorithms are measured via accuracy-, linearity-, and\nranking-based metrics. Based on the experiments, we show that boosting methods\ngenerally improve the performance of image quality assessment and the level of\nimprovement depends on the type of the boosting algorithm. Our experimental\nresults also indicate that boosting the worst performing quality estimator with\ntwo or more additional methods leads to statistically significant performance\nenhancements independent of the boosting technique and neural network-based\nboosting outperforms support vector machine-based boosting when two or more\nmethods are fused.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:16:16 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08456", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov", "title": "Artificial Color Constancy via GoogLeNet with Angular Loss Function", "comments": null, "journal-ref": "International Journal of Imaging and Robotics, 19(3):1-10, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color Constancy is the ability of the human visual system to perceive colors\nunchanged independently of the illumination. Giving a machine this feature will\nbe beneficial in many fields where chromatic information is used. Particularly,\nit significantly improves scene understanding and object recognition. In this\npaper, we propose transfer learning-based algorithm, which has two main\nfeatures: accuracy higher than many state-of-the-art algorithms and simplicity\nof implementation. Despite the fact that GoogLeNet was used in the experiments,\ngiven approach may be applied to any CNN. Additionally, we discuss design of a\nnew loss function oriented specifically to this problem, and propose a few the\nmost suitable options.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:34:18 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:07:52 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Sidorov", "Oleksii", ""]]}, {"id": "1811.08458", "submitter": "Isay Katsman", "authors": "Qian Huang, Zeqi Gu, Isay Katsman, Horace He, Pian Pawakapan, Zhiqiu\n  Lin, Serge Belongie, Ser-Nam Lim", "title": "Intermediate Level Adversarial Attack for Enhanced Transferability", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples may be overfit to exploit the particular\narchitecture and feature representation of a source model, resulting in\nsub-optimal black-box transfer attacks to other target models. This leads us to\nintroduce the Intermediate Level Attack (ILA), which attempts to fine-tune an\nexisting adversarial example for greater black-box transferability by\nincreasing its perturbation on a pre-specified layer of the source model. We\nshow that our method can effectively achieve this goal and that we can decide a\nnearly-optimal layer of the source model to perturb without any knowledge of\nthe target models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:40:24 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Huang", "Qian", ""], ["Gu", "Zeqi", ""], ["Katsman", "Isay", ""], ["He", "Horace", ""], ["Pawakapan", "Pian", ""], ["Lin", "Zhiqiu", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1811.08466", "submitter": "Nikita Durasov", "authors": "Nikita Durasov, Mikhail Romanov, Valeriya Bubnova, Pavel Bogomolov,\n  Anton Konushin", "title": "Double Refinement Network for Efficient Indoor Monocular Depth\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is the task of obtaining a measure of distance for\neach pixel using a single image. It is an important problem in computer vision\nand is usually solved using neural networks. Though recent works in this area\nhave shown significant improvement in accuracy, the state-of-the-art methods\ntend to require massive amounts of memory and time to process an image. The\nmain purpose of this work is to improve the performance of the latest solutions\nwith no decrease in accuracy. To this end, we introduce the Double Refinement\nNetwork architecture. The proposed method achieves state-of-the-art results on\nthe standard benchmark RGB-D dataset NYU Depth v2, while its frames per second\nrate is significantly higher (up to 18 times speedup per image at batch size 1)\nand the RAM usage per image is lower.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:56:10 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 17:17:16 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Durasov", "Nikita", ""], ["Romanov", "Mikhail", ""], ["Bubnova", "Valeriya", ""], ["Bogomolov", "Pavel", ""], ["Konushin", "Anton", ""]]}, {"id": "1811.08481", "submitter": "Ben Zion Vatashsky", "authors": "Ben-Zion Vatashsky and Shimon Ullman", "title": "VQA with no questions-answers training", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for teaching machines to answer visual questions have made\nsignificant progress in recent years, but current methods still lack important\nhuman capabilities, including integrating new visual classes and concepts in a\nmodular manner, providing explanations for the answers and handling new domains\nwithout explicit examples. We propose a novel method that consists of two main\nparts: generating a question graph representation, and an answering procedure,\nguided by the abstract structure of the question graph to invoke an extendable\nset of visual estimators. Training is performed for the language part and the\nvisual part on their own, but unlike existing schemes, the method does not\nrequire any training using images with associated questions and answers. This\napproach is able to handle novel domains (extended question types and new\nobject classes, properties and relations) as long as corresponding visual\nestimators are available. In addition, it can provide explanations to its\nanswers and suggest alternatives when questions are not grounded in the image.\nWe demonstrate that this approach achieves both high performance and domain\nextensibility without any questions-answers training.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:52:46 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 09:53:47 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Vatashsky", "Ben-Zion", ""], ["Ullman", "Shimon", ""]]}, {"id": "1811.08484", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo\n  Bremer", "title": "MimicGAN: Corruption-Mimicking for Blind Image Recovery & Adversarial\n  Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems continues to be a central challenge in computer\nvision. Existing techniques either explicitly construct an inverse mapping\nusing prior knowledge about the corruption, or learn the inverse directly using\na large collection of examples. However, in practice, the nature of corruption\nmay be unknown, and thus it is challenging to regularize the problem of\ninferring a plausible solution. On the other hand, collecting task-specific\ntraining data is tedious for known corruptions and impossible for unknown ones.\nWe present MimicGAN, an unsupervised technique to solve general inverse\nproblems based on image priors in the form of generative adversarial networks\n(GANs). Using a GAN prior, we show that one can reliably recover solutions to\nunderdetermined inverse problems through a surrogate network that learns to\nmimic the corruption at test time. Our system successively estimates the\ncorruption and the clean image without the need for supervisory training, while\noutperforming existing baselines in blind image recovery. We also demonstrate\nthat MimicGAN improves upon recent GAN-based defenses against adversarial\nattacks and represents one of the strongest test-time defenses available today.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:59:38 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Kailkhura", "Bhavya", ""], ["Bremer", "Timo", ""]]}, {"id": "1811.08489", "submitter": "Tianlu Wang", "authors": "Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, Vicente Ordonez", "title": "Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias\n  in Deep Image Representations", "comments": "10 pages, 7 figures, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a framework to measure and mitigate intrinsic biases\nwith respect to protected variables --such as gender-- in visual recognition\ntasks. We show that trained models significantly amplify the association of\ntarget labels with gender beyond what one would expect from biased datasets.\nSurprisingly, we show that even when datasets are balanced such that each label\nco-occurs equally with each gender, learned models amplify the association\nbetween labels and gender, as much as if data had not been balanced! To\nmitigate this, we adopt an adversarial approach to remove unwanted features\ncorresponding to protected variables from intermediate representations in a\ndeep neural network -- and provide a detailed analysis of its effectiveness.\nExperiments on two datasets: the COCO dataset (objects), and the imSitu dataset\n(actions), show reductions in gender bias amplification while maintaining most\nof the accuracy of the original models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 21:11:53 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 02:35:42 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 20:02:52 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 00:18:49 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Tianlu", ""], ["Zhao", "Jieyu", ""], ["Yatskar", "Mark", ""], ["Chang", "Kai-Wei", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1811.08495", "submitter": "Tiago Santana De Nazare", "authors": "Tiago S. Nazare, Rodrigo F. de Mello, Moacir A. Ponti", "title": "Are pre-trained CNNs good feature extractors for anomaly detection in\n  surveillance videos?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several techniques have been explored to detect unusual behaviour\nin surveillance videos. Nevertheless, few studies leverage features from\npre-trained CNNs and none of then present a comparison of features generate by\ndifferent models. Motivated by this gap, we compare features extracted by four\nstate-of-the-art image classification networks as a way of describing patches\nfrom security video frames. We carry out experiments on the Ped1 and Ped2\ndatasets and analyze the usage of different feature normalization techniques.\nOur results indicate that choosing the appropriate normalization is crucial to\nimprove the anomaly detection performance when working with CNN features. Also,\nin the Ped2 dataset our approach was able to obtain results comparable to the\nones of several state-of-the-art methods. Lastly, as our method only considers\nthe appearance of each frame, we believe that it can be combined with\napproaches that focus on motion patterns to further improve performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 21:29:58 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Nazare", "Tiago S.", ""], ["de Mello", "Rodrigo F.", ""], ["Ponti", "Moacir A.", ""]]}, {"id": "1811.08496", "submitter": "Rajeev Ranjan", "authors": "Joshua Gleason, Rajeev Ranjan, Steven Schwarcz, Carlos D. Castillo,\n  Jun-Chen Cheng, and Rama Chellappa", "title": "A Proposal-Based Solution to Spatio-Temporal Action Detection in\n  Untrimmed Videos", "comments": "To appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for spatio-temporal action detection in videos are\nlimited by the spatial extent and temporal duration of the actions. In this\npaper, we present a modular system for spatio-temporal action detection in\nuntrimmed security videos. We propose a two stage approach. The first stage\ngenerates dense spatio-temporal proposals using hierarchical clustering and\ntemporal jittering techniques on frame-wise object detections. The second stage\nis a Temporal Refinement I3D (TRI-3D) network that performs action\nclassification and temporal refinement on the generated proposals. The object\ndetection-based proposal generation step helps in detecting actions occurring\nin a small spatial region of a video frame, while temporal jittering and\nrefinement helps in detecting actions of variable lengths. Experimental results\non the spatio-temporal action detection dataset - DIVA - show the effectiveness\nof our system. For comparison, the performance of our system is also evaluated\non the THUMOS14 temporal action detection dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 21:35:07 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 03:06:34 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Gleason", "Joshua", ""], ["Ranjan", "Rajeev", ""], ["Schwarcz", "Steven", ""], ["Castillo", "Carlos D.", ""], ["Cheng", "Jun-Chen", ""], ["Chellappa", "Rama", ""]]}, {"id": "1811.08513", "submitter": "Naofumi Tomita", "authors": "Naofumi Tomita, Behnaz Abdollahi, Jason Wei, Bing Ren, Arief\n  Suriawinata, Saeed Hassanpour", "title": "Attention-Based Deep Neural Networks for Detection of Cancerous and\n  Precancerous Esophagus Tissue on Histopathological Slides", "comments": "Accepted for publication at the Journal of JAMA Network Open", "journal-ref": "JAMA Netw Open. 2019;2(11):e1914645", "doi": "10.1001/jamanetworkopen.2019.14645", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods, such as the sliding window approach for\ncropped-image classification and heuristic aggregation for whole-slide\ninference, for analyzing histological patterns in high-resolution microscopy\nimages have shown promising results. These approaches, however, require a\nlaborious annotation process and are fragmented. This diagnostic study\ncollected deidentified high-resolution histological images (N = 379) for\ntraining a new model composed of a convolutional neural network and a\ngrid-based attention network, trainable without region-of-interest annotations.\nHistological images of patients who underwent endoscopic esophagus and\ngastroesophageal junction mucosal biopsy between January 1, 2016, and December\n31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were\ncollected. The method achieved a mean accuracy of 0.83 in classifying 123 test\nimages. These results were comparable with or better than the performance from\nthe current state-of-the-art sliding window approach, which was trained with\nregions of interest. Results of this study suggest that the proposed\nattention-based deep neural network framework for Barrett esophagus and\nesophageal adenocarcinoma detection is important because it is based solely on\ntissue-level annotations, unlike existing methods that are based on regions of\ninterest. This new model is expected to open avenues for applying deep learning\nto digital pathology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 22:19:44 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 20:41:23 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Tomita", "Naofumi", ""], ["Abdollahi", "Behnaz", ""], ["Wei", "Jason", ""], ["Ren", "Bing", ""], ["Suriawinata", "Arief", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1811.08537", "submitter": "Till Hartmann", "authors": "Till S. Hartmann", "title": "Seeing in the dark with recurrent convolutional neural networks", "comments": "12 pages (with appendix), 6 figure (main text), 3 supplementary\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical convolutional neural networks (cCNNs) are very good at categorizing\nobjects in images. But, unlike human vision which is relatively robust to noise\nin images, the performance of cCNNs declines quickly as image quality worsens.\nHere we propose to use recurrent connections within the convolutional layers to\nmake networks robust against pixel noise such as could arise from imaging at\nlow light levels, and thereby significantly increase their performance when\ntested with simulated noisy video sequences. We show that cCNNs classify images\nwith high signal to noise ratios (SNRs) well, but are easily outperformed when\ntested with low SNR images (high noise levels) by convolutional neural networks\nthat have recurrency added to convolutional layers, henceforth referred to as\ngruCNNs. Addition of Bayes-optimal temporal integration to allow the cCNN to\nintegrate multiple image frames still does not match gruCNN performance.\nAdditionally, we show that at low SNRs, the probabilities predicted by the\ngruCNN (after calibration) have higher confidence than those predicted by the\ncCNN. We propose to consider recurrent connections in the early stages of\nneural networks as a solution to computer vision under imperfect lighting\nconditions and noisy environments; challenges faced during real-time video\nstreams of autonomous driving at night, during rain or snow, and other\nnon-ideal situations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:05:48 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Hartmann", "Till S.", ""]]}, {"id": "1811.08557", "submitter": "Wanxin Tian", "authors": "Wanxin Tian, Zixuan Wang, Haifeng Shen, Weihong Deng, Yiping Meng,\n  Binghui Chen, Xiubao Zhang, Yuan Zhao, Xiehe Huang", "title": "Learning Better Features for Face Detection with Feature Fusion and\n  Segmentation Supervision", "comments": "10 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1711.07246, arXiv:1712.00721 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of face detectors has been largely improved with the\ndevelopment of convolutional neural network. However, it remains challenging\nfor face detectors to detect tiny, occluded or blurry faces. Besides, most face\ndetectors can't locate face's position precisely and can't achieve high\nIntersection-over-Union (IoU) scores. We assume that problems inside are\ninadequate use of supervision information and imbalance between semantics and\ndetails at all level feature maps in CNN even with Feature Pyramid Networks\n(FPN). In this paper, we present a novel single-shot face detection network,\nnamed DF$^2$S$^2$ (Detection with Feature Fusion and Segmentation Supervision),\nwhich introduces a more effective feature fusion pyramid and a more efficient\nsegmentation branch on ResNet-50 to handle mentioned problems. Specifically,\ninspired by FPN and SENet, we apply semantic information from higher-level\nfeature maps as contextual cues to augment low-level feature maps via a spatial\nand channel-wise attention style, preventing details from being covered by too\nmuch semantics and making semantics and details complement each other. We\nfurther propose a semantic segmentation branch to best utilize detection\nsupervision information meanwhile applying attention mechanism in a\nself-supervised manner. The segmentation branch is supervised by weak\nsegmentation ground-truth (no extra annotation is required) in a hierarchical\nmanner, deprecated in the inference time so it wouldn't compromise the\ninference speed. We evaluate our model on WIDER FACE dataset and achieved\nstate-of-art results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:31:03 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 03:27:57 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 10:10:12 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tian", "Wanxin", ""], ["Wang", "Zixuan", ""], ["Shen", "Haifeng", ""], ["Deng", "Weihong", ""], ["Meng", "Yiping", ""], ["Chen", "Binghui", ""], ["Zhang", "Xiubao", ""], ["Zhao", "Yuan", ""], ["Huang", "Xiehe", ""]]}, {"id": "1811.08560", "submitter": "Mohammad Babaeizadeh", "authors": "Mohammad Babaeizadeh, Golnaz Ghiasi", "title": "Adjustable Real-time Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer is the problem of synthesizing an image with content\nsimilar to a given image and style similar to another. Although recent\nfeed-forward neural networks can generate stylized images in real-time, these\nmodels produce a single stylization given a pair of style/content images, and\nthe user doesn't have control over the synthesized output. Moreover, the style\ntransfer depends on the hyper-parameters of the model with varying \"optimum\"\nfor different input images. Therefore, if the stylized output is not appealing\nto the user, she/he has to try multiple models or retrain one with different\nhyper-parameters to get a favorite stylization. In this paper, we address these\nissues by proposing a novel method which allows adjustment of crucial\nhyper-parameters, after the training and in real-time, through a set of\nmanually adjustable parameters. These parameters enable the user to modify the\nsynthesized outputs from the same pair of style/content images, in search of a\nfavorite stylized image. Our quantitative and qualitative experiments indicate\nhow adjusting these parameters is comparable to retraining the model with\ndifferent hyper-parameters. We also demonstrate how these parameters can be\nrandomized to generate results which are diverse but still very similar in\nstyle and content.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:20:05 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Babaeizadeh", "Mohammad", ""], ["Ghiasi", "Golnaz", ""]]}, {"id": "1811.08561", "submitter": "Yong Liu", "authors": "Yong Liu, Lin Shang, Andy Song", "title": "Adaptive Re-ranking of Deep Feature for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical person re-identification (re-ID) methods train a deep CNN to extract\ndeep features and combine them with a distance metric for the final evaluation.\nIn this work, we focus on exploiting the full information encoded in the deep\nfeature to boost the re-ID performance. First, we propose a Deep Feature Fusion\n(DFF) method to exploit the diverse information embedded in a deep feature. DFF\ntreats each sub-feature as an information carrier and employs a diffusion\nprocess to exchange their information. Second, we propose an Adaptive\nRe-Ranking (ARR) method to exploit the contextual information encoded in the\nfeatures of neighbors. ARR utilizes the contextual information to re-rank the\nretrieval results in an iterative manner. Particularly, it adds more contextual\ninformation after each iteration automatically to consider more matches. Third,\nwe propose a strategy that combines DFF and ARR to enhance the performance.\nExtensive comparative evaluations demonstrate the superiority of the proposed\nmethods on three large benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:22:03 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Liu", "Yong", ""], ["Shang", "Lin", ""], ["Song", "Andy", ""]]}, {"id": "1811.08564", "submitter": "Zhiyan Cui", "authors": "Zhiyan Cui, Na Lu", "title": "Feature Selection Convolutional Neural Networks for Visual Tracking", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.03132", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing tracking methods based on CNN(convolutional neural\nnetworks) are too slow for real-time application despite the excellent tracking\nprecision compared with the traditional ones. Moreover, neural networks are\nmemory intensive which will take up lots of hardware resources. In this paper,\na feature selection visual tracking algorithm combining CNN based\nMDNet(Multi-Domain Network) and RoIAlign was developed. We find that there is a\nlot of redundancy in feature maps from convolutional layers. So valid feature\nmaps are selected by mutual information and others are abandoned which can\nreduce the complexity and computation of the network and do not affect the\nprecision. The major problem of MDNet also lies in the time efficiency.\nConsidering the computational complexity of MDNet is mainly caused by the large\namount of convolution operations and fine-tuning of the network during\ntracking, a RoIAlign layer which could conduct the convolution over the whole\nimage instead of each RoI is added to accelerate the convolution and a new\nstrategy of fine-tuning the fully-connected layers is used to accelerate the\nupdate. With RoIAlign employed, the computation speed has been increased and it\nshows greater precision than RoIPool. Because RoIAlign can process float number\ncoordinates by bilinear interpolation. These strategies can accelerate the\nprocessing, reduce the complexity with very low impact on precision and it can\nrun at around 10 fps(while the speed of MDNet is about 1 fps). The proposed\nalgorithm has been evaluated on a benchmark: OTB100, on which high precision\nand speed have been obtained.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 13:12:21 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Cui", "Zhiyan", ""], ["Lu", "Na", ""]]}, {"id": "1811.08565", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Bernhard Egger, Andreas Morel-Forster, Andreas\n  Schneider, Thomas Gerig, Clemens Blumer, Corius Reyneke, Thomas Vetter", "title": "Can Synthetic Faces Undo the Damage of Dataset Bias to Face Recognition\n  and Facial Landmark Detection?", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that deep learning approaches to face recognition and facial\nlandmark detection suffer from biases in modern training datasets. In this\nwork, we propose to use synthetic face images to reduce the negative effects of\ndataset biases on these tasks. Using a 3D morphable face model, we generate\nlarge amounts of synthetic face images with full control over facial shape and\ncolor, pose, illumination, and background. With a series of experiments, we\nextensively test the effects of priming deep nets by pre-training them with\nsynthetic faces. We observe the following positive effects for face recognition\nand facial landmark detection tasks: 1) Priming with synthetic face images\nimproves the performance consistently across all benchmarks because it reduces\nthe negative effects of biases in the training data. 2) Traditional approaches\nfor reducing the damage of dataset bias, such as data augmentation and transfer\nlearning, are less effective than training with synthetic faces. 3) Using\nsynthetic data, we can reduce the size of real-world datasets by 75% for face\nrecognition and by 50% for facial landmark detection while maintaining\nperformance. Thus, offering a means to focus the data collection process on\nless but higher quality data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 21:17:21 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 00:26:34 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kortylewski", "Adam", ""], ["Egger", "Bernhard", ""], ["Morel-Forster", "Andreas", ""], ["Schneider", "Andreas", ""], ["Gerig", "Thomas", ""], ["Blumer", "Clemens", ""], ["Reyneke", "Corius", ""], ["Vetter", "Thomas", ""]]}, {"id": "1811.08575", "submitter": "Xin Jin", "authors": "Xin Jin, Zhibo Chen, Jianxin Lin, Zhikai Chen, Wei Zhou", "title": "Unsupervised Single Image Deraining with Self-supervised Constraints", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing single image deraining methods require learning supervised\nmodels from a large set of paired synthetic training data, which limits their\ngenerality, scalability and practicality in real-world multimedia applications.\nBesides, due to lack of labeled-supervised constraints, directly applying\nexisting unsupervised frameworks to the image deraining task will suffer from\nlow-quality recovery. Therefore, we propose an Unsupervised Deraining\nGenerative Adversarial Network (UD-GAN) to tackle above problems by introducing\nself-supervised constraints from the intrinsic statistics of unpaired rainy and\nclean images. Specifically, we firstly design two collaboratively optimized\nmodules, namely Rain Guidance Module (RGM) and Background Guidance Module\n(BGM), to take full advantage of rainy image characteristics: The RGM is\ndesigned to discriminate real rainy images from fake rainy images which are\ncreated based on outputs of the generator with BGM. Simultaneously, the BGM\nexploits a hierarchical Gaussian-Blur gradient error to ensure background\nconsistency between rainy input and de-rained output. Secondly, a novel\nluminance-adjusting adversarial loss is integrated into the clean image\ndiscriminator considering the built-in luminance difference between real clean\nimages and derained images. Comprehensive experiment results on various\nbenchmarking datasets and different training settings show that UD-GAN\noutperforms existing image deraining methods in both quantitative and\nqualitative comparisons.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:50:06 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Jin", "Xin", ""], ["Chen", "Zhibo", ""], ["Lin", "Jianxin", ""], ["Chen", "Zhikai", ""], ["Zhou", "Wei", ""]]}, {"id": "1811.08585", "submitter": "Chaoqi Chen", "authors": "Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue\n  Huang, Tingyang Xu, Junzhou Huang", "title": "Progressive Feature Alignment for Unsupervised Domain Adaptation", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich\nsource domain to a fully-unlabeled target domain. To tackle this task, recent\napproaches resort to discriminative domain transfer in virtue of pseudo-labels\nto enforce the class-level distribution alignment across the source and target\ndomains. These methods, however, are vulnerable to the error accumulation and\nthus incapable of preserving cross-domain category consistency, as the\npseudo-labeling accuracy is not guaranteed explicitly. In this paper, we\npropose the Progressive Feature Alignment Network (PFAN) to align the\ndiscriminative features across domains progressively and effectively, via\nexploiting the intra-class variation in the target domain. To be specific, we\nfirst develop an Easy-to-Hard Transfer Strategy (EHTS) and an Adaptive\nPrototype Alignment (APA) step to train our model iteratively and\nalternatively. Moreover, upon observing that a good domain adaptation usually\nrequires a non-saturated source classifier, we consider a simple yet efficient\nway to retard the convergence speed of the source classification loss by\nfurther involving a temperature variate into the soft-max function. The\nextensive experimental results reveal that the proposed PFAN exceeds the\nstate-of-the-art performance on three UDA datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:32:31 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 11:43:21 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chen", "Chaoqi", ""], ["Xie", "Weiping", ""], ["Huang", "Wenbing", ""], ["Rong", "Yu", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Xu", "Tingyang", ""], ["Huang", "Junzhou", ""]]}, {"id": "1811.08588", "submitter": "Yoshinori Konishi", "authors": "Yoshinori Konishi and Kosuke Hattori and Manabu Hashimoto", "title": "Real-Time 6D Object Pose Estimation on CPU", "comments": "accepted to IROS 2019", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Macau, China, 2019, pp. 3451-3458", "doi": "10.1109/IROS40897.2019.8967967", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast and accurate 6D object pose estimation from a RGB-D image.\nOur proposed method is template matching based and consists of three main\ntechnical components, PCOF-MOD (multimodal PCOF), balanced pose tree (BPT) and\noptimum memory rearrangement for a coarse-to-fine search. Our model templates\non densely sampled viewpoints and PCOF-MOD which explicitly handles a certain\nrange of 3D object pose improve the robustness against background clutters. BPT\nwhich is an efficient tree-based data structures for a large number of\ntemplates and template matching on rearranged feature maps where nearby\nfeatures are linearly aligned accelerate the pose estimation. The experimental\nevaluation on tabletop and bin-picking dataset showed that our method achieved\nhigher accuracy and faster speed in comparison with state-of-the-art techniques\nincluding recent CNN based approaches. Moreover, our model templates can be\ntrained only from 3D CAD in a few minutes and the pose estimation run in near\nreal-time (23 fps) on CPU. These features are suitable for any real\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:42:20 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 09:01:59 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 05:45:04 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Konishi", "Yoshinori", ""], ["Hattori", "Kosuke", ""], ["Hashimoto", "Manabu", ""]]}, {"id": "1811.08589", "submitter": "Mengdi Wang", "authors": "Mengdi Wang, Qing Zhang, Jun Yang, Xiaoyuan Cui and Wei Lin", "title": "Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural\n  Networks", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a graph-adaptive pruning (GAP) method for efficient\ninference of convolutional neural networks (CNNs). In this method, the network\nis viewed as a computational graph, in which the vertices denote the\ncomputation nodes and edges represent the information flow. Through topology\nanalysis, GAP is capable of adapting to different network structures,\nespecially the widely used cross connections and multi-path data flow in recent\nnovel convolutional models. The models can be adaptively pruned at vertex-level\nas well as edge-level without any post-processing, thus GAP can directly get\npractical model compression and inference speed-up. Moreover, it does not need\nany customized computation library or hardware support. Finetuning is conducted\nafter pruning to restore the model performance. In the finetuning step, we\nadopt a self-taught knowledge distillation (KD) strategy by utilizing\ninformation from the original model, through which, the performance of the\noptimized model can be sufficiently improved, without introduction of any other\nteacher model. Experimental results show the proposed GAP can achieve promising\nresult to make inference more efficient, e.g., for ResNeXt-29 on CIFAR10, it\ncan get 13X model compression and 4.3X practical speed-up with marginal loss of\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:43:38 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wang", "Mengdi", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""], ["Cui", "Xiaoyuan", ""], ["Lin", "Wei", ""]]}, {"id": "1811.08592", "submitter": "Albert Haque", "authors": "Albert Haque, Michelle Guo, Adam S Miner, Li Fei-Fei", "title": "Measuring Depression Symptom Severity from Spoken Language and 3D Facial\n  Expressions", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/9", "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more than 300 million people depressed worldwide, depression is a global\nproblem. Due to access barriers such as social stigma, cost, and treatment\navailability, 60% of mentally-ill adults do not receive any mental health\nservices. Effective and efficient diagnosis relies on detecting clinical\nsymptoms of depression. Automatic detection of depressive symptoms would\npotentially improve diagnostic accuracy and availability, leading to faster\nintervention. In this work, we present a machine learning method for measuring\nthe severity of depressive symptoms. Our multi-modal method uses 3D facial\nexpressions and spoken language, commonly available from modern cell phones. It\ndemonstrates an average error of 3.67 points (15.3% relative) on the\nclinically-validated Patient Health Questionnaire (PHQ) scale. For detecting\nmajor depressive disorder, our model demonstrates 83.3% sensitivity and 82.6%\nspecificity. Overall, this paper shows how speech recognition, computer vision,\nand natural language processing can be combined to assist mental health\npatients and practitioners. This technology could be deployed to cell phones\nworldwide and facilitate low-cost universal access to mental health care.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:52:31 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:49:11 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Haque", "Albert", ""], ["Guo", "Michelle", ""], ["Miner", "Adam S", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1811.08594", "submitter": "Thanh Nguyen Tang", "authors": "Thanh T. Nguyen, Dung Nguyen", "title": "Learning to Attend Relevant Regions in Videos from Eye Fixations", "comments": "This is an incomplete work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentively important regions in video frames account for a majority part of\nthe semantics in each frame. This information is helpful in many applications\nnot only for entertainment (such as auto generating commentary and tourist\nguide) but also for robotic control which holds a larascope supported for\nlaparoscopic surgery. However, it is not always straightforward to define and\nlocate such semantic regions in videos. In this work, we attempt to address the\nproblem of attending relevant regions in videos by leveraging the eye fixations\nlabels with a RNN-based visual attention model. Our experimental results\nsuggest that this approach holds a good potential to learn to attend semantic\nregions in videos while its performance also heavily relies on the quality of\neye fixations labels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 04:20:03 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 12:29:29 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 11:38:46 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nguyen", "Thanh T.", ""], ["Nguyen", "Dung", ""]]}, {"id": "1811.08599", "submitter": "Zhonghua Wu", "authors": "Zhonghua Wu, Guosheng Lin, Qingyi Tao and Jianfei Cai", "title": "M2E-Try On Net: Fashion from Model to Everyone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing virtual try-on applications require clean clothes images.\nInstead, we present a novel virtual Try-On network, M2E-Try On Net, which\ntransfers the clothes from a model image to a person image without the need of\nany clean product images. To obtain a realistic image of person wearing the\ndesired model clothes, we aim to solve the following challenges: 1) non-rigid\nnature of clothes - we need to align poses between the model and the user; 2)\nrichness in textures of fashion items - preserving the fine details and\ncharacteristics of the clothes is critical for photo-realistic transfer; 3)\nvariation of identity appearances - it is required to fit the desired model\nclothes to the person identity seamlessly. To tackle these challenges, we\nintroduce three key components, including the pose alignment network (PAN), the\ntexture refinement network (TRN) and the fitting network (FTN). Since it is\nunlikely to gather image pairs of input person image and desired output image\n(i.e. person wearing the desired clothes), our framework is trained in a\nself-supervised manner to gradually transfer the poses and textures of the\nmodel's clothes to the desired appearance. In the experiments, we verify on the\nDeep Fashion dataset and MVC dataset that our method can generate\nphoto-realistic images for the person to try-on the model clothes. Furthermore,\nwe explore the model capability for different fashion items, including both\nupper and lower garments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 05:11:42 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 13:06:59 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 09:34:57 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Wu", "Zhonghua", ""], ["Lin", "Guosheng", ""], ["Tao", "Qingyi", ""], ["Cai", "Jianfei", ""]]}, {"id": "1811.08605", "submitter": "Enze Xie", "authors": "Enze Xie, Yuhang Zang, Shuai Shao, Gang Yu, Cong Yao, Guangyao Li", "title": "Scene Text Detection with Supervised Pyramid Context Network", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection methods based on deep learning have achieved remarkable\nresults over the past years. However, due to the high diversity and complexity\nof natural scenes, previous state-of-the-art text detection methods may still\nproduce a considerable amount of false positives, when applied to images\ncaptured in real-world environments. To tackle this issue, mainly inspired by\nMask R-CNN, we propose in this paper an effective model for scene text\ndetection, which is based on Feature Pyramid Network (FPN) and instance\nsegmentation. We propose a supervised pyramid context network (SPCNET) to\nprecisely locate text regions while suppressing false positives. Benefited from\nthe guidance of semantic information and sharing FPN, SPCNET obtains\nsignificantly enhanced performance while introducing marginal extra\ncomputation. Experiments on standard datasets demonstrate that our SPCNET\nclearly outperforms start-of-the-art methods. Specifically, it achieves an\nF-measure of 92.1% on ICDAR2013, 87.2% on ICDAR2015, 74.1% on ICDAR2017 MLT and\n82.9% on Total-Text.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:13:03 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Xie", "Enze", ""], ["Zang", "Yuhang", ""], ["Shao", "Shuai", ""], ["Yu", "Gang", ""], ["Yao", "Cong", ""], ["Li", "Guangyao", ""]]}, {"id": "1811.08611", "submitter": "Wanchen Sui", "authors": "Wanchen Sui, Qing Zhang, Jun Yang, Wei Chu", "title": "A Novel Integrated Framework for Learning both Text Detection and\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel integrated framework for learning both text\ndetection and recognition. For most of the existing methods, detection and\nrecognition are treated as two isolated tasks and trained separately, since\nparameters of detection and recognition models are different and two models\ntarget to optimize their own loss functions during individual training\nprocesses. In contrast to those methods, by sharing model parameters, we merge\nthe detection model and recognition model into a single end-to-end trainable\nmodel and train the joint model for two tasks simultaneously. The shared\nparameters not only help effectively reduce the computational load in inference\nprocess, but also improve the end-to-end text detection-recognition accuracy.\nIn addition, we design a simpler and faster sequence learning method for the\nrecognition network based on a succession of stacked convolutional layers\nwithout any recurrent structure, this is proved feasible and dramatically\nimproves inference speed. Extensive experiments on different datasets\ndemonstrate that the proposed method achieves very promising results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:14:34 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Sui", "Wanchen", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""], ["Chu", "Wei", ""]]}, {"id": "1811.08618", "submitter": "Seungjoon Yang", "authors": "Jinhyeok Jang, Jaehong Kim, Jaeyeon Lee, and Seungjoon Yang", "title": "Neural Networks with Activation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an adaptive activation method for neural networks that\nexploits the interdependency of features. Each pixel, node, and layer is\nassigned with a polynomial activation function, whose coefficients are provided\nby an auxiliary activation network. The activation of a feature depends on the\nfeatures of neighboring pixels in a convolutional layer and other nodes in a\ndense layer. The dependency is learned from data by the activation networks. In\nour experiments, networks with activation networks provide significant\nperformance improvement compared to the baseline networks on which they are\nbuilt. The proposed method can be used to improve the network performance as an\nalternative to increasing the number of nodes and layers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:54:41 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Jang", "Jinhyeok", ""], ["Kim", "Jaehong", ""], ["Lee", "Jaeyeon", ""], ["Yang", "Seungjoon", ""]]}, {"id": "1811.08622", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li, Cheng Xu, Biao Leng", "title": "Angular Triplet-Center Loss for Multi-view 3D Shape Retrieval", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to obtain the desirable representation of a 3D shape, which is\ndiscriminative across categories and polymerized within classes, is a\nsignificant challenge in 3D shape retrieval. Most existing 3D shape retrieval\nmethods focus on capturing strong discriminative shape representation with\nsoftmax loss for the classification task, while the shape feature learning with\nmetric loss is neglected for 3D shape retrieval. In this paper, we address this\nproblem based on the intuition that the cosine distance of shape embeddings\nshould be close enough within the same class and far away across categories.\nSince most of 3D shape retrieval tasks use cosine distance of shape features\nfor measuring shape similarity, we propose a novel metric loss named angular\ntriplet-center loss, which directly optimizes the cosine distances between the\nfeatures. It inherits the triplet-center loss property to achieve larger\ninter-class distance and smaller intra-class distance simultaneously. Unlike\nprevious metric loss utilized in 3D shape retrieval methods, where Euclidean\ndistance is adopted and the margin design is difficult, the proposed method is\nmore convenient to train feature embeddings and more suitable for 3D shape\nretrieval. Moreover, the angle margin is adopted to replace the cosine margin\nin order to provide more explicit discriminative constraints on an embedding\nspace. Extensive experimental results on two popular 3D object retrieval\nbenchmarks, ModelNet40 and ShapeNetCore 55, demonstrate the effectiveness of\nour proposed loss, and our method has achieved state-of-the-art results on\nvarious 3D shape datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 08:07:04 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 14:55:34 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 08:52:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li", "Zhaoqun", ""], ["Xu", "Cheng", ""], ["Leng", "Biao", ""]]}, {"id": "1811.08632", "submitter": "Xueyang Fu", "authors": "Xueyang Fu, Qi Qi, Yue Huang, Xinghao Ding, Feng Wu, John Paisley", "title": "A Deep Tree-Structured Fusion Model for Single Image Deraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective deep tree-structured fusion model based on\nfeature aggregation for the deraining problem. We argue that by effectively\naggregating features, a relatively simple network can still handle tough image\nderaining problems well. First, to capture the spatial structure of rain we use\ndilated convolutions as our basic network block. We then design a\ntree-structured fusion architecture which is deployed within each block\n(spatial information) and across all blocks (content information). Our method\nis based on the assumption that adjacent features contain redundant\ninformation. This redundancy obstructs generation of new representations and\ncan be reduced by hierarchically fusing adjacent features. Thus, the proposed\nmodel is more compact and can effectively use spatial and content information.\nExperiments on synthetic and real-world datasets show that our network achieves\nbetter deraining results with fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 08:36:30 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Fu", "Xueyang", ""], ["Qi", "Qi", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Wu", "Feng", ""], ["Paisley", "John", ""]]}, {"id": "1811.08634", "submitter": "Yifan Yang", "authors": "Yifan Yang, Qijing Huang, Bichen Wu, Tianjun Zhang, Liang Ma, Giulio\n  Gambardella, Michaela Blott, Luciano Lavagno, Kees Vissers, John Wawrzynek,\n  Kurt Keutzer", "title": "Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on\n  Embedded FPGAs", "comments": "Update to the latest results", "journal-ref": null, "doi": "10.1145/3289602.3293902", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using FPGAs to accelerate ConvNets has attracted significant attention in\nrecent years. However, FPGA accelerator design has not leveraged the latest\nprogress of ConvNets. As a result, the key application characteristics such as\nframes-per-second (FPS) are ignored in favor of simply counting GOPs, and\nresults on accuracy, which is critical to application success, are often not\neven reported. In this work, we adopt an algorithm-hardware co-design approach\nto develop a ConvNet accelerator called Synetgy and a novel ConvNet model\ncalled DiracDeltaNet$^{\\dagger}$. Both the accelerator and ConvNet are tailored\nto FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with\nonly $1\\times 1$ convolutions while spatial convolutions are replaced by more\nefficient shift operations. DiracDeltaNet achieves competitive accuracy on\nImageNet (88.7\\% top-5), but with 42$\\times$ fewer parameters and 48$\\times$\nfewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 4-bit and\nactivations to 4-bits, with less than 1\\% accuracy loss. These quantizations\nexploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model\nsize, low computational OP count, low precision and simplified operators allow\nus to co-design a highly customized computing unit for an FPGA. We implement\nthe computing units for DiracDeltaNet on an Ultra96 SoC system through\nhigh-level synthesis. Our accelerator's final top-5 accuracy of 88.1\\% on\nImageNet, is higher than all the previously reported embedded FPGA\naccelerators. In addition, the accelerator reaches an inference speed of 66.3\nFPS on the ImageNet classification task, surpassing prior works with similar\naccuracy by at least 11.6$\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 08:42:30 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 13:00:11 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 03:06:40 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 01:45:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yang", "Yifan", ""], ["Huang", "Qijing", ""], ["Wu", "Bichen", ""], ["Zhang", "Tianjun", ""], ["Ma", "Liang", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Lavagno", "Luciano", ""], ["Vissers", "Kees", ""], ["Wawrzynek", "John", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1811.08645", "submitter": "Gwang-Il Ri", "authors": "Gwang-Il Ri, Chol-Gyun Ri, Su-Rim Ji", "title": "A Fingerprint Indexing Method Based on Minutia Descriptor and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we propose a novel fingerprint indexing approach for speeding\nup in the fingerprint recognition system. What kind of features are used for\nindexing and how to employ the extracted features for searching are crucial for\nthe fingerprint indexing. In this paper, we select a minutia descriptor, which\nhas been used to improve the accuracy of the fingerprint matching, as a local\nfeature for indexing and construct a fixed-length feature vector which will be\nused for searching from the minutia descriptors of the fingerprint image using\na clustering. And we propose a fingerprint searching approach that uses the\nEuclidean distance between two feature vectors as the similarity between two\nindexing features. Our indexing approach has several benefits. It reduces\nsearching time significantly and is irrespective of the existence of singular\npoints and robust even though the size of the fingerprint image is small or the\nquality is low. And the constructed indexing vector by this approach is\nindependent of the features which are used for indexing based on the\ngeometrical relations between the minutiae, like one based on the minutiae\ntriplets. Thus, the proposed approach could be combined with other indexing\napproaches to gain a better indexing performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 09:19:54 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ri", "Gwang-Il", ""], ["Ri", "Chol-Gyun", ""], ["Ji", "Su-Rim", ""]]}, {"id": "1811.08657", "submitter": "Songyou Peng", "authors": "Le Zhang, Songyou Peng, Stefan Winkler", "title": "PersEmoN: A Deep Network for Joint Analysis of Apparent Personality,\n  Emotion and Their Relationship", "comments": "Accepted to IEEE Transactions on Affective Computing", "journal-ref": null, "doi": "10.1109/TAFFC.2019.2951656", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apparent personality and emotion analysis are both central to affective\ncomputing. Existing works solve them individually. In this paper we investigate\nif such high-level affect traits and their relationship can be jointly learned\nfrom face images in the wild. To this end, we introduce PersEmoN, an end-to-end\ntrainable and deep Siamese-like network. It consists of two convolutional\nnetwork branches, one for emotion and the other for apparent personality. Both\nnetworks share their bottom feature extraction module and are optimized within\na multi-task learning framework. Emotion and personality networks are dedicated\nto their own annotated dataset. Furthermore, an adversarial-like loss function\nis employed to promote representation coherence among heterogeneous dataset\nsources. Based on this, we also explore the emotion-to-apparent-personality\nrelationship. Extensive experiments demonstrate the effectiveness of PersEmoN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:01:46 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 15:31:43 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhang", "Le", ""], ["Peng", "Songyou", ""], ["Winkler", "Stefan", ""]]}, {"id": "1811.08661", "submitter": "Paul Jaeger", "authors": "Paul F. Jaeger, Simon A. A. Kohl, Sebastian Bickelhaupt, Fabian\n  Isensee, Tristan Anselm Kuder, Heinz-Peter Schlemmer, and Klaus H. Maier-Hein", "title": "Retina U-Net: Embarrassingly Simple Exploitation of Segmentation\n  Supervision for Medical Object Detection", "comments": null, "journal-ref": "Neruips ML4H Workshop 2019 PLMR", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of localizing and categorizing objects in medical images often\nremains formulated as a semantic segmentation problem. This approach, however,\nonly indirectly solves the coarse localization task by predicting pixel-level\nscores, requiring ad-hoc heuristics when mapping back to object-level scores.\nState-of-the-art object detectors on the other hand, allow for individual\nobject scoring in an end-to-end fashion, while ironically trading in the\nability to exploit the full pixel-wise supervision signal. This can be\nparticularly disadvantageous in the setting of medical image analysis, where\ndata sets are notoriously small. In this paper, we propose Retina U-Net, a\nsimple architecture, which naturally fuses the Retina Net one-stage detector\nwith the U-Net architecture widely used for semantic segmentation in medical\nimages. The proposed architecture recaptures discarded supervision signals by\ncomplementing object detection with an auxiliary task in the form of semantic\nsegmentation without introducing the additional complexity of previously\nproposed two-stage detectors. We evaluate the importance of full segmentation\nsupervision on two medical data sets, provide an in-depth analysis on a series\nof toy experiments and show how the corresponding performance gain grows in the\nlimit of small data sets. Retina U-Net yields strong detection performance only\nreached by its more complex two-staged counterparts. Our framework including\nall methods implemented for operation on 2D and 3D images is available at\ngithub.com/pfjaeger/medicaldetectiontoolkit.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:12:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jaeger", "Paul F.", ""], ["Kohl", "Simon A. A.", ""], ["Bickelhaupt", "Sebastian", ""], ["Isensee", "Fabian", ""], ["Kuder", "Tristan Anselm", ""], ["Schlemmer", "Heinz-Peter", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1811.08668", "submitter": "Minchao Li", "authors": "Minchao Li, Shikui Tu, Lei Xu", "title": "Computational Decomposition of Style for Controllable and Enhanced Style\n  Transfer", "comments": "9 pages for main body and 11 pages for appendix. One picture is\n  replaces compared to the last version. Some typos are corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer has been demonstrated to be powerful in creating\nartistic image with help of Convolutional Neural Networks (CNN). However, there\nis still lack of computational analysis of perceptual components of the\nartistic style. Different from some early attempts which studied the style by\nsome pre-processing or post-processing techniques, we investigate the\ncharacteristics of the style systematically based on feature map produced by\nCNN. First, we computationally decompose the style into basic elements using\nnot only spectrum based methods including Fast Fourier Transform (FFT),\nDiscrete Cosine Transform (DCT) but also latent variable models such Principal\nComponent Analysis (PCA), Independent Component Analysis (ICA). Then, the\ndecomposition of style induces various ways of controlling the style elements\nwhich could be embedded as modules in state-of-the-art style transfer\nalgorithms. Such decomposition of style brings several advantages. It enables\nthe computational coding of different artistic styles by our style basis with\nsimilar styles clustering together, and thus it facilitates the mixing or\nintervention of styles based on the style basis from more than one styles so\nthat compound style or new style could be generated to produce styled images.\nExperiments demonstrate the effectiveness of our method on not only painting\nstyle transfer but also sketch style transfer which indicates possible\napplications on picture-to-sketch problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:23:36 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 09:34:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Minchao", ""], ["Tu", "Shikui", ""], ["Xu", "Lei", ""]]}, {"id": "1811.08674", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Thomas Kipf, Max Welling, Antonio Garcia-Uceda\n  Juarez, Jesper H Pedersen, Jens Petersen, Marleen de Bruijne", "title": "Graph Refinement based Airway Extraction using Mean-Field Networks and\n  Graph Neural Networks", "comments": "Accepted for publication at Medical Image Analysis. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph refinement, or the task of obtaining subgraphs of interest from\nover-complete graphs, can have many varied applications. In this work, we\nextract trees or collection of sub-trees from image data by, first deriving a\ngraph-based representation of the volumetric data and then, posing the tree\nextraction as a graph refinement task. We present two methods to perform graph\nrefinement. First, we use mean-field approximation (MFA) to approximate the\nposterior density over the subgraphs from which the optimal subgraph of\ninterest can be estimated. Mean field networks (MFNs) are used for inference\nbased on the interpretation that iterations of MFA can be seen as feed-forward\noperations in a neural network. This allows us to learn the model parameters\nusing gradient descent. Second, we present a supervised learning approach using\ngraph neural networks (GNNs) which can be seen as generalisations of MFNs.\nSubgraphs are obtained by training a GNN-based graph refinement model to\ndirectly predict edge probabilities. We discuss connections between the two\nclasses of methods and compare them for the task of extracting airways from 3D,\nlow-dose, chest CT data. We show that both the MFN and GNN models show\nsignificant improvement when compared to one baseline method, that is similar\nto a top performing method in the EXACT'09 Challenge, and a 3D U-Net based\nairway segmentation model, in detecting more branches with fewer false\npositives.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:50:31 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:14:58 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Kipf", "Thomas", ""], ["Welling", "Max", ""], ["Juarez", "Antonio Garcia-Uceda", ""], ["Pedersen", "Jesper H", ""], ["Petersen", "Jens", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1811.08728", "submitter": "Christian Wilms", "authors": "Christian Wilms and Simone Frintrop", "title": "AttentionMask: Attentive, Efficient Object Proposal Generation Focusing\n  on Small Objects", "comments": "Accepted at ACCV 2018. Code is available at\n  https://github.com/chwilms/AttentionMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for class-agnostic object proposal generation,\nwhich is efficient and especially well-suited to detect small objects.\nEfficiency is achieved by scale-specific objectness attention maps which focus\nthe processing on promising parts of the image and reduce the amount of sampled\nwindows strongly. This leads to a system, which is $33\\%$ faster than the\nstate-of-the-art and clearly outperforming state-of-the-art in terms of average\nrecall. Secondly, we add a module for detecting small objects, which are often\nmissed by recent models. We show that this module improves the average recall\nfor small objects by about $53\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 13:43:43 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wilms", "Christian", ""], ["Frintrop", "Simone", ""]]}, {"id": "1811.08737", "submitter": "Honghui Shi", "authors": "Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana\n  Rosing, Rogerio Feris", "title": "SpotTune: Transfer Learning through Adaptive Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, which allows a source task to affect the inductive bias of\nthe target task, is widely used in computer vision. The typical way of\nconducting transfer learning with deep neural networks is to fine-tune a model\npre-trained on the source task using data from the target task. In this paper,\nwe propose an adaptive fine-tuning approach, called SpotTune, which finds the\noptimal fine-tuning strategy per instance for the target data. In SpotTune,\ngiven an image from the target task, a policy network is used to make routing\ndecisions on whether to pass the image through the fine-tuned layers or the\npre-trained layers. We conduct extensive experiments to demonstrate the\neffectiveness of the proposed approach. Our method outperforms the traditional\nfine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune\nwith other state-of-the-art fine-tuning strategies, showing superior\nperformance. On the Visual Decathlon datasets, our method achieves the highest\nscore across the board without bells and whistles.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:02:03 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Guo", "Yunhui", ""], ["Shi", "Honghui", ""], ["Kumar", "Abhishek", ""], ["Grauman", "Kristen", ""], ["Rosing", "Tajana", ""], ["Feris", "Rogerio", ""]]}, {"id": "1811.08739", "submitter": "Myron Brown", "authors": "Marc Bosch, Kevin Foster, Gordon Christie, Sean Wang, Gregory D Hager,\n  and Myron Brown", "title": "Semantic Stereo for Incidental Satellite Images", "comments": "Accepted publication at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly common use of incidental satellite images for stereo\nreconstruction versus rigidly tasked binocular or trinocular coincident\ncollection is helping to enable timely global-scale 3D mapping; however,\nreliable stereo correspondence from multi-date image pairs remains very\nchallenging due to seasonal appearance differences and scene change. Promising\nrecent work suggests that semantic scene segmentation can provide a robust\nregularizing prior for resolving ambiguities in stereo correspondence and\nreconstruction problems. To enable research for pairwise semantic stereo and\nmulti-view semantic 3D reconstruction with incidental satellite images, we have\nestablished a large-scale public dataset including multi-view, multi-band\nsatellite images and ground truth geometric and semantic labels for two large\ncities. To demonstrate the complementary nature of the stereo and segmentation\ntasks, we present lightweight public baselines adapted from recent state of the\nart convolutional neural network models and assess their performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:05:06 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Bosch", "Marc", ""], ["Foster", "Kevin", ""], ["Christie", "Gordon", ""], ["Wang", "Sean", ""], ["Hager", "Gregory D", ""], ["Brown", "Myron", ""]]}, {"id": "1811.08747", "submitter": "Dongdong Chen", "authors": "Dongdong Chen and Mingming He and Qingnan Fan and Jing Liao and Liheng\n  Zhang and Dongdong Hou and Lu Yuan and Gang Hua", "title": "Gated Context Aggregation Network for Image Dehazing and Deraining", "comments": "Accepted by WACV 2019, Code released at\n  \"https://github.com/cddlyf/GCANet\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing aims to recover the uncorrupted content from a hazy image.\nInstead of leveraging traditional low-level or handcrafted image priors as the\nrestoration constraints, e.g., dark channels and increased contrast, we propose\nan end-to-end gated context aggregation network to directly restore the final\nhaze-free image. In this network, we adopt the latest smoothed dilation\ntechnique to help remove the gridding artifacts caused by the widely-used\ndilated convolution with negligible extra parameters, and leverage a gated\nsub-network to fuse the features from different levels. Extensive experiments\ndemonstrate that our method can surpass previous state-of-the-art methods by a\nlarge margin both quantitatively and qualitatively. In addition, to demonstrate\nthe generality of the proposed method, we further apply it to the image\nderaining task, which also achieves the state-of-the-art performance. Code has\nbeen made available at https://github.com/cddlyf/GCANet.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:22:51 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 13:39:41 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Dongdong", ""], ["He", "Mingming", ""], ["Fan", "Qingnan", ""], ["Liao", "Jing", ""], ["Zhang", "Liheng", ""], ["Hou", "Dongdong", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""]]}, {"id": "1811.08751", "submitter": "Michael Roberts", "authors": "Michael Roberts and Jack Spencer", "title": "Chan-Vese Reformulation for Selective Image Segmentation", "comments": "To appear in the Journal of Mathematical Imaging and Vision 2019. (23\n  pages, 19 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective segmentation involves incorporating user input to partition an\nimage into foreground and background, by discriminating between objects of a\nsimilar type. Typically, such methods involve introducing additional\nconstraints to generic segmentation approaches. However, we show that this is\noften inconsistent with respect to common assumptions about the image. The\nproposed method introduces a new fitting term that is more useful in practice\nthan the Chan-Vese framework. In particular, the idea is to define a term that\nallows for the background to consist of multiple regions of inhomogeneity. We\nprovide comparitive experimental results to alternative approaches to\ndemonstrate the advantages of the proposed method, broadening the possible\napplication of these methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:29:14 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 18:44:26 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Roberts", "Michael", ""], ["Spencer", "Jack", ""]]}, {"id": "1811.08760", "submitter": "Alon Shoshan", "authors": "Alon Shoshan, Roey Mechrez, Lihi Zelnik-Manor", "title": "Dynamic-Net: Tuning the Objective Without Re-training for Synthesis\n  Tasks", "comments": "version update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key ingredients for successful optimization of modern CNNs is\nidentifying a suitable objective. To date, the objective is fixed a-priori at\ntraining time, and any variation to it requires re-training a new network. In\nthis paper we present a first attempt at alleviating the need for re-training.\nRather than fixing the network at training time, we train a \"Dynamic-Net\" that\ncan be modified at inference time. Our approach considers an \"objective-space\"\nas the space of all linear combinations of two objectives, and the Dynamic-Net\nis emulating the traversing of this objective-space at test-time, without any\nfurther training. We show that this upgrades pre-trained networks by providing\nan out-of-learning extension, while maintaining the performance quality. The\nsolution we propose is fast and allows a user to interactively modify the\nnetwork, in real-time, in order to obtain the result he/she desires. We show\nthe benefits of such an approach via several different applications.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:49:34 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 11:28:38 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Shoshan", "Alon", ""], ["Mechrez", "Roey", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1811.08815", "submitter": "Khoi-Nguyen Mac", "authors": "Khoi-Nguyen C. Mac, Dhiraj Joshi, Raymond A. Yeh, Jinjun Xiong,\n  Rogerio S. Feris, Minh N. Do", "title": "Learning Motion in Feature Space: Locally-Consistent Deformable\n  Convolution Networks for Fine-Grained Action Detection", "comments": "Accepted at ICCV 2019 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained action detection is an important task with numerous applications\nin robotics and human-computer interaction. Existing methods typically utilize\na two-stage approach including extraction of local spatio-temporal features\nfollowed by temporal modeling to capture long-term dependencies. While most\nrecent papers have focused on the latter (long-temporal modeling), here, we\nfocus on producing features capable of modeling fine-grained motion more\nefficiently. We propose a novel locally-consistent deformable convolution,\nwhich utilizes the change in receptive fields and enforces a local coherency\nconstraint to capture motion information effectively. Our model jointly learns\nspatio-temporal features (instead of using independent spatial and temporal\nstreams). The temporal component is learned from the feature space instead of\npixel space, e.g. optical flow. The produced features can be flexibly used in\nconjunction with other long-temporal modeling networks, e.g. ST-CNN,\nDilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the\noriginal long-temporal models on two fine-grained action datasets: 50 Salads\nand GTEA, achieving F1 scores of 80.22% and 75.39% respectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:34:53 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 14:59:45 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 16:48:15 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2019 01:16:30 GMT"}, {"version": "v5", "created": "Wed, 6 Nov 2019 21:37:45 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Mac", "Khoi-Nguyen C.", ""], ["Joshi", "Dhiraj", ""], ["Yeh", "Raymond A.", ""], ["Xiong", "Jinjun", ""], ["Feris", "Rogerio S.", ""], ["Do", "Minh N.", ""]]}, {"id": "1811.08820", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson", "title": "Trajectory PHD and CPHD filters", "comments": "MATLAB implementations are provided here:\n  https://github.com/Agarciafernandez/MTT", "journal-ref": "In IEEE Transactions on Signal Processing, vol. 67, no. 22, pp.\n  5702-5714, Nov. 2019", "doi": "10.1109/TSP.2019.2943234", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the probability hypothesis density filter (PHD) and the\ncardinality PHD (CPHD) filter for sets of trajectories, which are referred to\nas the trajectory PHD (TPHD) and trajectory CPHD (TCPHD) filters. Contrary to\nthe PHD/CPHD filters, the TPHD/TCPHD filters are able to produce trajectory\nestimates from first principles. The TPHD filter is derived by recursively\nobtaining the best Poisson multitrajectory density approximation to the\nposterior density over the alive trajectories by minimising the\nKullback-Leibler divergence. The TCPHD is derived in the same way but\npropagating an independent identically distributed (IID) cluster\nmultitrajectory density approximation. We also propose the Gaussian mixture\nimplementations of the TPHD and TCPHD recursions, the Gaussian mixture TPHD\n(GMTPHD) and the Gaussian mixture TCPHD (GMTCPHD), and the L-scan\ncomputationally efficient implementations, which only update the density of the\ntrajectory states of the last L time steps.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:48:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:28:32 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 12:43:38 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1811.08824", "submitter": "Aaron Walsman", "authors": "Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav\n  Artzi, Yejin Choi, Dieter Fox", "title": "Early Fusion for Goal Directed Robotic Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building perceptual systems for robotics which perform well under tight\ncomputational budgets requires novel architectures which rethink the\ntraditional computer vision pipeline. Modern vision architectures require the\nagent to build a summary representation of the entire scene, even if most of\nthe input is irrelevant to the agent's current goal. In this work, we flip this\nparadigm, by introducing EarlyFusion vision models that condition on a goal to\nbuild custom representations for downstream tasks. We show that these goal\nspecific representations can be learned more quickly, are substantially more\nparameter efficient, and more robust than existing attention mechanisms in our\ndomain. We demonstrate the effectiveness of these methods on a simulated\nrobotic item retrieval problem that is trained in a fully end-to-end manner via\nimitation learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:55:17 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 23:56:55 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 18:16:59 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Walsman", "Aaron", ""], ["Bisk", "Yonatan", ""], ["Gabriel", "Saadia", ""], ["Misra", "Dipendra", ""], ["Artzi", "Yoav", ""], ["Choi", "Yejin", ""], ["Fox", "Dieter", ""]]}, {"id": "1811.08837", "submitter": "Maneet Singh", "authors": "Maneet Singh, Richa Singh, Mayank Vatsa, Nalini Ratha, Rama Chellappa", "title": "Recognizing Disguised Faces in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in face recognition has seen tremendous growth over the past couple\nof decades. Beginning from algorithms capable of performing recognition in\nconstrained environments, the current face recognition systems achieve very\nhigh accuracies on large-scale unconstrained face datasets. While upcoming\nalgorithms continue to achieve improved performance, a majority of the face\nrecognition systems are susceptible to failure under disguise variations, one\nof the most challenging covariate of face recognition. Most of the existing\ndisguise datasets contain images with limited variations, often captured in\ncontrolled settings. This does not simulate a real world scenario, where both\nintentional and unintentional unconstrained disguises are encountered by a face\nrecognition system. In this paper, a novel Disguised Faces in the Wild (DFW)\ndataset is proposed which contains over 11000 images of 1000 identities with\ndifferent types of disguise accessories. The dataset is collected from the\nInternet, resulting in unconstrained face images similar to real world\nsettings. This is the first-of-a-kind dataset with the availability of\nimpersonator and genuine obfuscated face images for each subject. The proposed\ndataset has been analyzed in terms of three levels of difficulty: (i) easy,\n(ii) medium, and (iii) hard in order to showcase the challenging nature of the\nproblem. It is our view that the research community can greatly benefit from\nthe DFW dataset in terms of developing algorithms robust to such adversaries.\nThe proposed dataset was released as part of the First International Workshop\nand Competition on Disguised Faces in the Wild at CVPR, 2018. This paper\npresents the DFW dataset in detail, including the evaluation protocols,\nbaseline results, performance analysis of the submissions received as part of\nthe competition, and three levels of difficulties of the DFW challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:28:35 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Singh", "Maneet", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Ratha", "Nalini", ""], ["Chellappa", "Rama", ""]]}, {"id": "1811.08839", "submitter": "Anuroop Sriram", "authors": "Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan\n  Huang, Matthew J. Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary\n  Bruno, Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana,\n  Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael Rabbat, Pascal\n  Vincent, Nafissa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C.\n  Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, Yvonne W. Lui", "title": "fastMRI: An Open Dataset and Benchmarks for Accelerated MRI", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements\nhas the potential to reduce medical costs, minimize stress to patients and make\nMRI possible in applications where it is currently prohibitively slow or\nexpensive. We introduce the fastMRI dataset, a large-scale collection of both\nraw MR measurements and clinical MR images, that can be used for training and\nevaluation of machine-learning approaches to MR image reconstruction. By\nintroducing standardized evaluation criteria and a freely-accessible dataset,\nour goal is to help the community make rapid advances in the state of the art\nfor MR image reconstruction. We also provide a self-contained introduction to\nMRI for machine learning researchers with no medical imaging background.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:32:14 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:31:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zbontar", "Jure", ""], ["Knoll", "Florian", ""], ["Sriram", "Anuroop", ""], ["Murrell", "Tullie", ""], ["Huang", "Zhengnan", ""], ["Muckley", "Matthew J.", ""], ["Defazio", "Aaron", ""], ["Stern", "Ruben", ""], ["Johnson", "Patricia", ""], ["Bruno", "Mary", ""], ["Parente", "Marc", ""], ["Geras", "Krzysztof J.", ""], ["Katsnelson", "Joe", ""], ["Chandarana", "Hersh", ""], ["Zhang", "Zizhao", ""], ["Drozdzal", "Michal", ""], ["Romero", "Adriana", ""], ["Rabbat", "Michael", ""], ["Vincent", "Pascal", ""], ["Yakubova", "Nafissa", ""], ["Pinkerton", "James", ""], ["Wang", "Duo", ""], ["Owens", "Erich", ""], ["Zitnick", "C. Lawrence", ""], ["Recht", "Michael P.", ""], ["Sodickson", "Daniel K.", ""], ["Lui", "Yvonne W.", ""]]}, {"id": "1811.08883", "submitter": "Kaiming He", "authors": "Kaiming He, Ross Girshick, Piotr Doll\\'ar", "title": "Rethinking ImageNet Pre-training", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report competitive results on object detection and instance segmentation\non the COCO dataset using standard models trained from random initialization.\nThe results are no worse than their ImageNet pre-training counterparts even\nwhen using the hyper-parameters of the baseline system (Mask R-CNN) that were\noptimized for fine-tuning pre-trained models, with the sole exception of\nincreasing the number of training iterations so the randomly initialized models\nmay converge. Training from random initialization is surprisingly robust; our\nresults hold even when: (i) using only 10% of the training data, (ii) for\ndeeper and wider models, and (iii) for multiple tasks and metrics. Experiments\nshow that ImageNet pre-training speeds up convergence early in training, but\ndoes not necessarily provide regularization or improve final target task\naccuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection\nwithout using any external data---a result on par with the top COCO 2017\ncompetition results that used ImageNet pre-training. These observations\nchallenge the conventional wisdom of ImageNet pre-training for dependent tasks\nand we expect these discoveries will encourage people to rethink the current de\nfacto paradigm of `pre-training and fine-tuning' in computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:55:58 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["He", "Kaiming", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1811.08886", "submitter": "Zhijian Liu", "authors": "Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han", "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision", "comments": "CVPR 2019. The first three authors contributed equally to this work.\n  Project page: https://hanlab.mit.edu/projects/haq/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference. Emergent DNN hardware accelerators begin to\nsupport mixed precision (1-8 bits) to further improve the computation\nefficiency, which raises a great challenge to find the optimal bitwidth for\neach layer: it requires domain experts to explore the vast design space trading\noff among accuracy, latency, energy, and model size, which is both\ntime-consuming and sub-optimal. Conventional quantization algorithm ignores the\ndifferent hardware architectures and quantizes all the layers in a uniform way.\nIn this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\nframework which leverages the reinforcement learning to automatically determine\nthe quantization policy, and we take the hardware accelerator's feedback in the\ndesign loop. Rather than relying on proxy signals such as FLOPs and model size,\nwe employ a hardware simulator to generate direct feedback signals (latency and\nenergy) to the RL agent. Compared with conventional methods, our framework is\nfully automated and can specialize the quantization policy for different neural\nnetwork architectures and hardware architectures. Our framework effectively\nreduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\nnegligible loss of accuracy compared with the fixed bitwidth (8 bits)\nquantization. Our framework reveals that the optimal policies on different\nhardware architectures (i.e., edge and cloud architectures) under different\nresource constraints (i.e., latency, energy and model size) are drastically\ndifferent. We interpreted the implication of different quantization policies,\nwhich offer insights for both neural network architecture design and hardware\narchitecture design.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:58:14 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 10:25:30 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 20:35:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Kuan", ""], ["Liu", "Zhijian", ""], ["Lin", "Yujun", ""], ["Lin", "Ji", ""], ["Han", "Song", ""]]}, {"id": "1811.08890", "submitter": "Nils Holzenberger", "authors": "Nils Holzenberger, Shruti Palaskar, Pranava Madhyastha, Florian Metze,\n  Raman Arora", "title": "Learning from Multiview Correlations in Open-Domain Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of datasets contain multiple views, such as video, sound\nand automatic captions. A basic challenge in representation learning is how to\nleverage multiple views to learn better representations. This is further\ncomplicated by the existence of a latent alignment between views, such as\nbetween speech and its transcription, and by the multitude of choices for the\nlearning objective. We explore an advanced, correlation-based representation\nlearning method on a 4-way parallel, multimodal dataset, and assess the quality\nof the learned representations on retrieval-based tasks. We show that the\nproposed approach produces rich representations that capture most of the\ninformation shared across views. Our best models for speech and textual\nmodalities achieve retrieval rates from 70.7% to 96.9% on open-domain,\nuser-generated instructional videos. This shows it is possible to learn\nreliable representations across disparate, unaligned and noisy modalities, and\nencourages using the proposed approach on larger datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:57:11 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 18:21:28 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Holzenberger", "Nils", ""], ["Palaskar", "Shruti", ""], ["Madhyastha", "Pranava", ""], ["Metze", "Florian", ""], ["Arora", "Raman", ""]]}, {"id": "1811.08891", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "A Comparative Study of Quality and Content-Based Spatial Pooling\n  Strategies in Image Quality Assessment", "comments": "Paper: 5 pages, 8 figures, Presentation: 21 slides [Ancillary files]", "journal-ref": "2015 IEEE GlobalSIP, Orlando, FL, 2015, pp. 732-736", "doi": "10.1109/GlobalSIP.2015.7418293", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of quantifying image quality consists of engineering the quality\nfeatures and pooling these features to obtain a value or a map. There has been\na significant research interest in designing the quality features but pooling\nis usually overlooked compared to feature design. In this work, we compare the\nstate of the art quality and content-based spatial pooling strategies and show\nthat although features are the key in any image quality assessment, pooling\nalso matters. We also propose a quality-based spatial pooling strategy that is\nbased on linearly weighted percentile pooling (WPP). Pooling strategies are\nanalyzed for squared error, SSIM and PerSIM in LIVE, multiply distorted LIVE\nand TID2013 image databases.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:36:01 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08925", "submitter": "Runzhou Ge", "authors": "Runzhou Ge, Jiyang Gao, Kan Chen, Ram Nevatia", "title": "MAC: Mining Activity Concepts for Language-based Temporal Localization", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of language-based temporal localization in untrimmed\nvideos. Compared to temporal localization with fixed categories, this problem\nis more challenging as the language-based queries not only have no pre-defined\nactivity list but also may contain complex descriptions. Previous methods\naddress the problem by considering features from video sliding windows and\nlanguage queries and learning a subspace to encode their correlation, which\nignore rich semantic cues about activities in videos and queries. We propose to\nmine activity concepts from both video and language modalities by applying the\nactionness score enhanced Activity Concepts based Localizer (ACL).\nSpecifically, the novel ACL encodes the semantic concepts from verb-obj pairs\nin language queries and leverages activity classifiers' prediction scores to\nencode visual concepts. Besides, ACL also has the capability to regress sliding\nwindows as localization results. Experiments show that ACL significantly\noutperforms state-of-the-arts under the widely used metric, with more than 5%\nincrease on both Charades-STA and TACoS datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:53:03 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Ge", "Runzhou", ""], ["Gao", "Jiyang", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "1811.08927", "submitter": "Dogancan Temel", "authors": "Mohit Prabhushankar and Dogancan Temel and Ghassan AlRegib", "title": "Generating Adaptive and Robust Filter Sets Using an Unsupervised\n  Learning Framework", "comments": "Paper:5 pages, 5 figures, 3 tables and Poster [Ancillary files]", "journal-ref": "2017 IEEE International Conference on Image Processing (ICIP),\n  Beijing, 2017, pp. 3041-3045", "doi": "10.1109/ICIP.2017.8296841", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an adaptive unsupervised learning framework,\nwhich utilizes natural images to train filter sets. The applicability of these\nfilter sets is demonstrated by evaluating their performance in two contrasting\napplications - image quality assessment and texture retrieval. While assessing\nimage quality, the filters need to capture perceptual differences based on\ndissimilarities between a reference image and its distorted version. In texture\nretrieval, the filters need to assess similarity between texture images to\nretrieve closest matching textures. Based on experiments, we show that the\nfilter responses span a set in which a monotonicity-based metric can measure\nboth the perceptual dissimilarity of natural images and the similarity of\ntexture images. In addition, we corrupt the images in the test set and\ndemonstrate that the proposed method leads to robust and reliable retrieval\nperformance compared to existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:02:33 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08939", "submitter": "Jameson Merkow", "authors": "The DeepRadiology Team", "title": "Pneumonia Detection in Chest Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe our approach to pneumonia classification and\nlocalization in chest radiographs. This method uses only \\emph{open-source}\ndeep learning object detection and is based on CoupleNet, a fully convolutional\nnetwork which incorporates global and local features for object detection. Our\napproach achieves robustness through critical modifications of the training\nprocess and a novel ensembling algorithm which merges bounding boxes from\nseveral models. We tested our detection algorithm tested on a dataset of 3000\nchest radiographs as part of the 2018 RSNA Pneumonia Challenge; our solution\nwas recognized as a winning entry in a contest which attracted more than 1400\nparticipants worldwide.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:33:40 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["The DeepRadiology Team", "", ""]]}, {"id": "1811.08947", "submitter": "Dogancan Temel", "authors": "Mohit Prabhushankar and Dogancan Temel and Ghassan AlRegib", "title": "MS-UNIQUE: Multi-model and Sharpness-weighted Unsupervised Image Quality\n  Estimation", "comments": "Paper: 6 pages, 6 figures, 2 tables and Presentation: 21 slides\n  [Ancillary files]", "journal-ref": "The Electronic Imaging, IQSP XIV, Burlingame, California, USA,\n  Jan. 29 Feb. 2, 2017", "doi": "10.2352/ISSN.2470-1173.2017.12.IQSP-223", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we train independent linear decoder models to estimate the\nperceived quality of images. More specifically, we calculate the responses of\nindividual non-overlapping image patches to each of the decoders and scale\nthese responses based on the sharpness characteristics of filter set. We use\nmultiple linear decoders to capture different abstraction levels of the image\npatches. Training each model is carried out on 100,000 image patches from the\nImageNet database in an unsupervised fashion. Color space selection and ZCA\nWhitening are performed over these patches to enhance the descriptiveness of\nthe data. The proposed quality estimator is tested on the LIVE and the TID 2013\nimage quality assessment databases. Performance of the proposed method is\ncompared against eleven other state of the art methods in terms of accuracy,\nconsistency, linearity, and monotonic behavior. Based on experimental results,\nthe proposed method is generally among the top performing quality estimators in\nall categories.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:55:56 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08965", "submitter": "Zhiyi Cheng", "authors": "Zhiyi Cheng, Xiatian Zhu and Shaogang Gong", "title": "Low-Resolution Face Recognition", "comments": "Accepted by 14th Asian Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst recent face-recognition (FR) techniques have made significant progress\non recognising constrained high-resolution web images, the same cannot be said\non natively unconstrained low-resolution images at large scales. In this work,\nwe examine systematically this under-studied FR problem, and introduce a novel\nComplement Super-Resolution and Identity (CSRI) joint deep learning method with\na unified end-to-end network architecture. We further construct a new\nlarge-scale dataset TinyFace of native unconstrained low-resolution face images\nfrom selected public datasets, because none benchmark of this nature exists in\nthe literature. With extensive experiments we show there is a significant gap\nbetween the reported FR performances on popular benchmarks and the results on\nTinyFace, and the advantages of the proposed CSRI over a variety of\nstate-of-the-art FR and super-resolution deep models on solving this largely\nignored FR scenario. The TinyFace dataset is released publicly at:\nhttps://qmul-tinyface.github.io/.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 22:14:24 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 23:55:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Cheng", "Zhiyi", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1811.08982", "submitter": "Shafin Rahman", "authors": "Shafin Rahman, Salman Khan, and Nick Barnes", "title": "Polarity Loss for Zero-shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional object detection models require large amounts of training data.\nIn comparison, humans can recognize previously unseen objects by merely knowing\ntheir semantic description. To mimic similar behaviour, zero-shot object\ndetection aims to recognize and localize 'unseen' object instances by using\nonly their semantic information. The model is first trained to learn the\nrelationships between visual and semantic domains for seen objects, later\ntransferring the acquired knowledge to totally unseen objects. This setting\ngives rise to the need for correct alignment between visual and semantic\nconcepts, so that the unseen objects can be identified using only their\nsemantic attributes. In this paper, we propose a novel loss function called\n'Polarity loss', that promotes correct visual-semantic alignment for an\nimproved zero-shot object detection. On one hand, it refines the noisy semantic\nembeddings via metric learning on a 'Semantic vocabulary' of related concepts\nto establish a better synergy between visual and semantic domains. On the other\nhand, it explicitly maximizes the gap between positive and negative predictions\nto achieve better discrimination between seen, unseen and background objects.\nOur approach is inspired by embodiment theories in cognitive science, that\nclaim human semantic understanding to be grounded in past experiences (seen\nobjects), related linguistic concepts (word vocabulary) and visual perception\n(seen/unseen object images). We conduct extensive evaluations on MS-COCO and\nPascal VOC datasets, showing significant improvements over state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 01:01:19 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 05:24:14 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 05:53:10 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Rahman", "Shafin", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""]]}, {"id": "1811.08988", "submitter": "Minhyuk Sung", "authors": "Lingxiao Li and Minhyuk Sung and Anastasia Dubrovina and Li Yi and\n  Leonidas Guibas", "title": "Supervised Fitting of Geometric Primitives to 3D Point Clouds", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": "10.1109/CVPR.2019.00276", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting geometric primitives to 3D point cloud data bridges a gap between\nlow-level digitized 3D data and high-level structural information on the\nunderlying 3D shapes. As such, it enables many downstream applications in 3D\ndata processing. For a long time, RANSAC-based methods have been the gold\nstandard for such primitive fitting problems, but they require careful\nper-input parameter tuning and thus do not scale well for large datasets with\ndiverse shapes. In this work, we introduce Supervised Primitive Fitting Network\n(SPFN), an end-to-end neural network that can robustly detect a varying number\nof primitives at different scales without any user control. The network is\nsupervised using ground truth primitive surfaces and primitive membership for\nthe input points. Instead of directly predicting the primitives, our\narchitecture first predicts per-point properties and then uses a differential\nmodel estimation module to compute the primitive type and parameters. We\nevaluate our approach on a novel benchmark of ANSI 3D mechanical component\nmodels and demonstrate a significant improvement over both the state-of-the-art\nRANSAC-based methods and the direct neural prediction.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 01:45:54 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 05:46:14 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 06:01:28 GMT"}, {"version": "v4", "created": "Tue, 28 Jan 2020 17:59:19 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Li", "Lingxiao", ""], ["Sung", "Minhyuk", ""], ["Dubrovina", "Anastasia", ""], ["Yi", "Li", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1811.09012", "submitter": "Feiran Li", "authors": "Feiran Li, Gustavo Alfonso Garcia Ricardez, Jun Takamatsu and Tsukasa\n  Ogasawara", "title": "Multi-View Inpainting for RGB-D Sequence", "comments": "10 pages", "journal-ref": "3DV (2018) 464--47", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel approach to remove undesired objects from\nRGB-D sequences captured with freely moving cameras, which enables static 3D\nreconstruction. Our method jointly uses existing information from multiple\nframes as well as generates new one via inpainting techniques. We use balanced\nrules to select source frames; local homography based image warping method for\nalignment and Markov random field (MRF) based approach for combining existing\ninformation. For the left holes, we employ exemplar based multi-view inpainting\nmethod to deal with the color image and coherently use it as guidance to\ncomplete the depth correspondence. Experiments show that our approach is\nqualified for removing the undesired objects and inpainting the holes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 03:57:32 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Li", "Feiran", ""], ["Ricardez", "Gustavo Alfonso Garcia", ""], ["Takamatsu", "Jun", ""], ["Ogasawara", "Tsukasa", ""]]}, {"id": "1811.09019", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Lijun Gong, Shengfeng He, Linchao Bao,\n  Jinshan Pan, Qingxiong Yang and Ming-Hsuan Yang", "title": "Joint Face Hallucination and Deblurring via Structure Generation and\n  Detail Enhancement", "comments": "In IJCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of restoring a high-resolution face image from a\nblurry low-resolution input. This problem is difficult as super-resolution and\ndeblurring need to be tackled simultaneously. Moreover, existing algorithms\ncannot handle face images well as low-resolution face images do not have much\ntexture which is especially critical for deblurring. In this paper, we propose\nan effective algorithm by utilizing the domain-specific knowledge of human\nfaces to recover high-quality faces. We first propose a facial component guided\ndeep Convolutional Neural Network (CNN) to restore a coarse face image, which\nis denoted as the base image where the facial component is automatically\ngenerated from the input face image. However, the CNN based method cannot\nhandle image details well. We further develop a novel exemplar-based detail\nenhancement algorithm via facial component matching. Extensive experiments show\nthat the proposed method outperforms the state-of-the-art algorithms both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:33:29 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["Gong", "Lijun", ""], ["He", "Shengfeng", ""], ["Bao", "Linchao", ""], ["Pan", "Jinshan", ""], ["Yang", "Qingxiong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1811.09020", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman H. Khan, Shafin Rahman, Fatih Porikli", "title": "Task-generalizable Adversarial Attack based on Perceptual Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) can be easily fooled by adding human\nimperceptible perturbations to the images. These perturbed images are known as\n`adversarial examples' and pose a serious threat to security and safety\ncritical systems. A litmus test for the strength of adversarial examples is\ntheir transferability across different DNN models in a black box setting (i.e.\nwhen the target model's architecture and parameters are not known to attacker).\nCurrent attack algorithms that seek to enhance adversarial transferability work\non the decision level i.e. generate perturbations that alter the network\ndecisions. This leads to two key limitations: (a) An attack is dependent on the\ntask-specific loss function (e.g. softmax cross-entropy for object recognition)\nand therefore does not generalize beyond its original task. (b) The adversarial\nexamples are specific to the network architecture and demonstrate poor\ntransferability to other network architectures. We propose a novel approach to\ncreate adversarial examples that can broadly fool different networks on\nmultiple tasks. Our approach is based on the following intuition: \"Perpetual\nmetrics based on neural network features are highly generalizable and show\nexcellent performance in measuring and stabilizing input distortions. Therefore\nan ideal attack that creates maximum distortions in the network feature space\nshould realize highly transferable examples\". We report extensive experiments\nto show how adversarial examples generalize across multiple networks for\nclassification, object detection and segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:35:26 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:54:36 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 06:09:30 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman H.", ""], ["Rahman", "Shafin", ""], ["Porikli", "Fatih", ""]]}, {"id": "1811.09022", "submitter": "Ashkan Abbasi", "authors": "Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani, Yi\n  Zhang", "title": "Three-dimensional Optical Coherence Tomography Image Denoising through\n  Multi-input Fully-Convolutional Networks", "comments": "This is the last version of our paper published in Computers in\n  Biology and Medicine, Elsevier", "journal-ref": null, "doi": "10.1016/j.compbiomed.2019.01.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in applying convolutional\nneural networks (CNNs) to low-level vision tasks such as denoising and\nsuper-resolution. Due to the coherent nature of the image formation process,\noptical coherence tomography (OCT) images are inevitably affected by noise.\nThis paper proposes a new method named the multi-input fully-convolutional\nnetworks (MIFCN) for denoising of OCT images. In contrast to recently proposed\nnatural image denoising CNNs, the proposed architecture allows the exploitation\nof high degrees of correlation and complementary information among neighboring\nOCT images through pixel by pixel fusion of multiple FCNs. The parameters of\nthe proposed multi-input architecture are learned by considering the\nconsistency between the overall output and the contribution of each input\nimage. The proposed MIFCN method is compared with the state-of-the-art\ndenoising methods adopted on OCT images of normal and age-related macular\ndegeneration eyes in a quantitative and qualitative manner.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:40:21 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 10:57:29 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Abbasi", "Ashkan", ""], ["Monadjemi", "Amirhassan", ""], ["Fang", "Leyuan", ""], ["Rabbani", "Hossein", ""], ["Zhang", "Yi", ""]]}, {"id": "1811.09030", "submitter": "Takashi Matsubara", "authors": "Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara", "title": "Data Augmentation using Random Image Cropping and Patching for Deep CNNs", "comments": "accepted version, 16 pages", "journal-ref": "EEE Transactions on Circuits and Systems for Video Technology,\n  2019", "doi": "10.1109/TCSVT.2019.2935128", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable results in\nimage processing tasks. However, their high expression ability risks\noverfitting. Consequently, data augmentation techniques have been proposed to\nprevent overfitting while enriching datasets. Recent CNN architectures with\nmore parameters are rendering traditional data augmentation techniques\ninsufficient. In this study, we propose a new data augmentation technique\ncalled random image cropping and patching (RICAP) which randomly crops four\nimages and patches them to create a new training image. Moreover, RICAP mixes\nthe class labels of the four images, resulting in an advantage similar to label\nsmoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the\nshake-shake regularization model) by comparison with competitive data\naugmentation techniques such as cutout and mixup. RICAP achieves a new\nstate-of-the-art test error of $2.19\\%$ on CIFAR-10. We also confirmed that\ndeep CNNs with RICAP achieve better results on classification tasks using\nCIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft\nCOCO.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 06:07:40 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:21:32 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Takahashi", "Ryo", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "1811.09036", "submitter": "Isidro Cortes-Ciriano PhD", "authors": "Isidro Cortes Ciriano and Andreas Bender", "title": "KekuleScope: prediction of cancer cell line sensitivity and compound\n  potency using convolutional neural networks trained on compound images", "comments": null, "journal-ref": null, "doi": "10.1186/s13321-019-0364-5", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of convolutional neural networks (ConvNets) to harness\nhigh-content screening images or 2D compound representations is gaining\nincreasing attention in drug discovery. However, existing applications often\nrequire large data sets for training, or sophisticated pretraining schemes.\nHere, we show using 33 IC50 data sets from ChEMBL 23 that the in vitro activity\nof compounds on cancer cell lines and protein targets can be accurately\npredicted on a continuous scale from their Kekule structure representations\nalone by extending existing architectures, which were pretrained on unrelated\nimage data sets. We show that the predictive power of the generated models is\ncomparable to that of Random Forest (RF) models and fully-connected Deep Neural\nNetworks trained on circular (Morgan) fingerprints. Notably, including\nadditional fully-connected layers further increases the predictive power of the\nConvNets by up to 10%. Analysis of the predictions generated by RF models and\nConvNets shows that by simply averaging the output of the RF models and\nConvNets we obtain significantly lower errors in prediction for multiple data\nsets, although the effect size is small, than those obtained with either model\nalone, indicating that the features extracted by the convolutional layers of\nthe ConvNets provide complementary predictive signal to Morgan fingerprints.\nLastly, we show that multi-task ConvNets trained on compound images permit to\nmodel COX isoform selectivity on a continuous scale with errors in prediction\ncomparable to the uncertainty of the data. Overall, in this work we present a\nset of ConvNet architectures for the prediction of compound activity from their\nKekule structure representations with state-of-the-art performance, that\nrequire no generation of compound descriptors or use of sophisticated image\nprocessing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 06:45:22 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 10:50:20 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ciriano", "Isidro Cortes", ""], ["Bender", "Andreas", ""]]}, {"id": "1811.09038", "submitter": "Peng Jiang", "authors": "Peng Jiang, Zhiyi Pan, Nuno Vasconcelos, Baoquan Chen, Jingliang Peng", "title": "Super Diffusion for Salient Object Detection", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 11/25/2019", "doi": "10.1109/TIP.2019.2954209", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major branch of saliency object detection methods is diffusion-based\nwhich construct a graph model on a given image and diffuse seed saliency values\nto the whole graph by a diffusion matrix. While their performance is sensitive\nto specific feature spaces and scales used for the diffusion matrix definition,\nlittle work has been published to systematically promote the robustness and\naccuracy of salient object detection under the generic mechanism of diffusion.\nIn this work, we firstly present a novel view of the working mechanism of the\ndiffusion process based on mathematical analysis, which reveals that the\ndiffusion process is actually computing the similarity of nodes with respect to\nthe seeds based on diffusion maps. Following this analysis, we propose super\ndiffusion, a novel inclusive learning-based framework for salient object\ndetection, which makes the optimum and robust performance by integrating a\nlarge pool of feature spaces, scales and even features originally computed for\nnon-diffusion-based salient object detection. A closed-form solution of the\noptimal parameters for the integration is determined through supervised\nlearning. At the local level, we propose to promote each individual diffusion\nbefore the integration. Our mathematical analysis reveals the close\nrelationship between saliency diffusion and spectral clustering. Based on this,\nwe propose to re-synthesize each individual diffusion matrix from the most\ndiscriminative eigenvectors and the constant eigenvector (for saliency\nnormalization). The proposed framework is implemented and experimented on\nprevalently used benchmark datasets, consistently leading to state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 06:50:28 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Jiang", "Peng", ""], ["Pan", "Zhiyi", ""], ["Vasconcelos", "Nuno", ""], ["Chen", "Baoquan", ""], ["Peng", "Jingliang", ""]]}, {"id": "1811.09043", "submitter": "Ziv Katzir", "authors": "Ziv Katzir, Yuval Elovici", "title": "Detecting Adversarial Perturbations Through Spatial Behavior in\n  Activation Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based classifiers are still prone to manipulation through\nadversarial perturbations. State of the art attacks can overcome most of the\ndefense or detection mechanisms suggested so far, and adversaries have the\nupper hand in this arms race. Adversarial examples are designed to resemble the\nnormal input from which they were constructed, while triggering an incorrect\nclassification. This basic design goal leads to a characteristic spatial\nbehavior within the context of Activation Spaces, a term coined by the authors\nto refer to the hyperspaces formed by the activation values of the network's\nlayers. Within the output of the first layers of the network, an adversarial\nexample is likely to resemble normal instances of the source class, while in\nthe final layers such examples will diverge towards the adversary's target\nclass. The steps below enable us to leverage this inherent shift from one class\nto another in order to form a novel adversarial example detector. We construct\nEuclidian spaces out of the activation values of each of the deep neural\nnetwork layers. Then, we induce a set of k-nearest neighbor classifiers (k-NN),\none per activation space of each neural network layer, using the\nnon-adversarial examples. We leverage those classifiers to produce a sequence\nof class labels for each nonperturbed input sample and estimate the a priori\nprobability for a class label change between one activation space and another.\nDuring the detection phase we compute a sequence of classification labels for\neach input using the trained classifiers. We then estimate the likelihood of\nthose classification sequences and show that adversarial sequences are far less\nlikely than normal ones. We evaluated our detection method against the state of\nthe art C&W attack method, using two image classification datasets (MNIST,\nCIFAR-10) reaching an AUC 0f 0.95 for the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 07:17:32 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 10:33:27 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Katzir", "Ziv", ""], ["Elovici", "Yuval", ""]]}, {"id": "1811.09058", "submitter": "Zhuoyao Zhong", "authors": "Zhida Huang, Zhuoyao Zhong, Lei Sun, Qiang Huo", "title": "Mask R-CNN with Pyramid Attention Network for Scene Text Detection", "comments": "Accepted by WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new Mask R-CNN based text detection approach\nwhich can robustly detect multi-oriented and curved text from natural scene\nimages in a unified manner. To enhance the feature representation ability of\nMask R-CNN for text detection tasks, we propose to use the Pyramid Attention\nNetwork (PAN) as a new backbone network of Mask R-CNN. Experiments demonstrate\nthat PAN can suppress false alarms caused by text-like backgrounds more\neffectively. Our proposed approach has achieved superior performance on both\nmulti-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text\ndetection benchmark tasks by only using single-scale and single-model testing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 08:17:40 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Huang", "Zhida", ""], ["Zhong", "Zhuoyao", ""], ["Sun", "Lei", ""], ["Huo", "Qiang", ""]]}, {"id": "1811.09063", "submitter": "Bas van der Velden", "authors": "Bas H.M. van der Velden, Bob D. de Vos, Claudette E. Loo, Hugo J.\n  Kuijf, Ivana Isgum, Kenneth G.A. Gilhuijs", "title": "Response monitoring of breast cancer on DCE-MRI using convolutional\n  neural network-generated seed points and constrained volume growing", "comments": "This work has been accepted for SPIE Medical Imaging 2019,\n  Computer-Aided Diagnosis conference, Paper 10950-12", "journal-ref": "Medical Imaging 2019: Computer-Aided Diagnosis (Vol. 10950, p.\n  109500D). International Society for Optics and Photonics", "doi": "10.1117/12.2508358", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response of breast cancer to neoadjuvant chemotherapy (NAC) can be monitored\nusing the change in visible tumor on magnetic resonance imaging (MRI). In our\ncurrent workflow, seed points are manually placed in areas of enhancement\nlikely to contain cancer. A constrained volume growing method uses these\nmanually placed seed points as input and generates a tumor segmentation. This\nmethod is rigorously validated using complete pathological embedding. In this\nstudy, we propose to exploit deep learning for fast and automatic seed point\ndetection, replacing manual seed point placement in our existing and\nwell-validated workflow. The seed point generator was developed in early breast\ncancer patients with pathology-proven segmentations (N=100), operated shortly\nafter MRI. It consisted of an ensemble of three independently trained fully\nconvolutional dilated neural networks that classified breast voxels as tumor or\nnon-tumor. Subsequently, local maxima were used as seed points for volume\ngrowing in patients receiving NAC (N=10). The percentage of tumor volume change\nwas evaluated against semi-automatic segmentations. The primary cancer was\nlocalized in 95% of the tumors at the cost of 0.9 false positive per patient.\nFalse positives included focally enhancing regions of unknown origin and parts\nof the intramammary blood vessels. Volume growing from the seed points showed a\nmedian tumor volume decrease of 70% (interquartile range: 50%-77%), comparable\nto the semi-automatic segmentations (median: 70%, interquartile range 23%-76%).\nTo conclude, a fast and automatic seed point generator was developed, fully\nautomating a well-validated semi-automatic workflow for response monitoring of\nbreast cancer to neoadjuvant chemotherapy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 09:02:41 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["van der Velden", "Bas H. M.", ""], ["de Vos", "Bob D.", ""], ["Loo", "Claudette E.", ""], ["Kuijf", "Hugo J.", ""], ["Isgum", "Ivana", ""], ["Gilhuijs", "Kenneth G. A.", ""]]}, {"id": "1811.09081", "submitter": "Sebastian Zambanini", "authors": "Sebastian Zambanini", "title": "Feature-based groupwise registration of historical aerial images to\n  present-day ortho-photo maps", "comments": "Under review at Elsevier Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2019.01.024", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the registration of historical WWII images to\npresent-day ortho-photo maps for the purpose of geolocalization. Due to the\nchallenging nature of this problem, we propose to register the images jointly\nas a group rather than in a step-by-step manner. To this end, we exploit Hough\nVoting spaces as pairwise registration estimators and show how they can be\nintegrated into a probabilistic groupwise registration framework that can be\nefficiently optimized. The feature-based nature of our registration framework\nallows to register images with a-priori unknown translational and rotational\nrelations, and is also able to handle scale changes of up to 30% in our test\ndata due to a final geometrically guided matching step. The superiority of the\nproposed method over existing pairwise and groupwise registration methods is\ndemonstrated on eight highly challenging sets of historical images with\ncorresponding ortho-photo maps.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 10:04:58 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Zambanini", "Sebastian", ""]]}, {"id": "1811.09128", "submitter": "Paul Patras", "authors": "Chaoyun Zhang, Rui Li, Woojin Kim, Daesub Yoon, Paul Patras", "title": "Driver Behavior Recognition via Interwoven Deep Convolutional Neural\n  Nets with Multi-stream Inputs", "comments": "13 pages, 15 figures", "journal-ref": "IEEE Access, vol. 8, pp. 191138-191151, 2020", "doi": "10.1109/ACCESS.2020.3032344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding driver activity is vital for in-vehicle systems that aim to\nreduce the incidence of car accidents rooted in cognitive distraction.\nAutomating real-time behavior recognition while ensuring actions classification\nwith high accuracy is however challenging, given the multitude of circumstances\nsurrounding drivers, the unique traits of individuals, and the computational\nconstraints imposed by in-vehicle embedded platforms. Prior work fails to\njointly meet these runtime/accuracy requirements and mostly rely on a single\nsensing modality, which in turn can be a single point of failure. In this\npaper, we harness the exceptional feature extraction abilities of deep learning\nand propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN)\narchitecture to tackle the problem of accurate classification of driver\nbehaviors in real-time. The proposed solution exploits information from\nmulti-stream inputs, i.e., in-vehicle cameras with different fields of view and\noptical flows computed based on recorded images, and merges through multiple\nfusion layers abstract features that it extracts. This builds a tight\nensembling system, which significantly improves the robustness of the model. In\naddition, we introduce a temporal voting scheme based on historical inference\ninstances, to enhance the classification accuracy. Experiments conducted with a\ndataset that we collect in a mock-up car environment demonstrate that the\nproposed InterCNN with MobileNet convolutional blocks can classify 9 different\nbehaviors with 73.97% accuracy, and 5 'aggregated' behaviors with 81.66%\naccuracy. We further show that our architecture is highly computationally\nefficient, as it performs inferences within 15ms, which satisfies the real-time\nconstraints of intelligent cars. Nevertheless, our InterCNN is robust to lossy\ninput, as the classification remains accurate when two input streams are\noccluded.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:05:23 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 22:28:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhang", "Chaoyun", ""], ["Li", "Rui", ""], ["Kim", "Woojin", ""], ["Yoon", "Daesub", ""], ["Patras", "Paul", ""]]}, {"id": "1811.09131", "submitter": "Dan Casas", "authors": "Raquel Vidaurre, Dan Casas, Elena Garces, Jorge Lopez-Moreno", "title": "BRDF Estimation of Complex Materials with Nested Learning", "comments": "Accepted to IEEE Winter Conference on Applications of Computer Vision\n  2019 (WACV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the optical properties of a material from RGB-images is an\nimportant but extremely ill-posed problem in Computer Graphics. While recent\nworks have successfully approached this problem even from just a single\nphotograph, significant simplifications of the material model are assumed,\nlimiting the usability of such methods. The detection of complex material\nproperties such as anisotropy or Fresnel effect remains an unsolved challenge.\nWe propose a novel method that predicts the model parameters of an\nartist-friendly, physically-based BRDF, from only two low-resolution shots of\nthe material. Thanks to a novel combination of deep neural networks in a nested\narchitecture, we are able to handle the ambiguities given by the\nnon-orthogonality and non-convexity of the parameter space. To train the\nnetwork, we generate a novel dataset of physically-based synthetic images. We\nprove that our model can recover new properties like anisotropy, index of\nrefraction and a second reflectance color, for materials that have tinted\nspecular reflections or whose albedo changes at glancing angles.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:15:30 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Vidaurre", "Raquel", ""], ["Casas", "Dan", ""], ["Garces", "Elena", ""], ["Lopez-Moreno", "Jorge", ""]]}, {"id": "1811.09132", "submitter": "Sami Brandt", "authors": "Sami Sebastian Brandt, Hanno Ackermann and Stella Grasshof", "title": "Uncalibrated Non-Rigid Factorisation by Independent Subspace Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general, prior-free approach for the uncalibrated non-rigid\nstructure-from-motion problem for modelling and analysis of non-rigid objects\nsuch as human faces. The word general refers to an approach that recovers the\nnon-rigid affine structure and motion from 2D point correspondences by assuming\nthat (1) the non-rigid shapes are generated by a linear combination of rigid 3D\nbasis shapes, (2) that the non-rigid shapes are affine in nature, i.e., they\ncan be modelled as deviations from the mean, rigid shape, (3) and that the\nbasis shapes are statistically independent. In contrast to the majority of\nexisting works, no prior information is assumed for the structure and motion\napart from the assumption the that underlying basis shapes are statistically\nindependent. The independent 3D shape bases are recovered by independent\nsubspace analysis (ISA). Likewise, in contrast to the most previous approaches,\nno calibration information is assumed for affine cameras; the reconstruction is\nsolved up to a global affine ambiguity that makes our approach simple but\nefficient. In the experiments, we evaluated the method with several standard\ndata sets including a real face expression data set of 7200 faces with 2D point\ncorrespondences and unknown 3D structure and motion for which we obtained\npromising results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:17:21 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Brandt", "Sami Sebastian", ""], ["Ackermann", "Hanno", ""], ["Grasshof", "Stella", ""]]}, {"id": "1811.09134", "submitter": "Soumya Shubhra Ghosh", "authors": "Soumya Shubhra Ghosh, Yang Hua, Sankha Subhra Mukherjee, Neil\n  Robertson", "title": "IEGAN: Multi-purpose Perceptual Quality Image Enhancement Using\n  Generative Adversarial Network", "comments": "Accepted at IEEE WACV 2019", "journal-ref": null, "doi": "10.1109/WACV.2019.00070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the breakthroughs in quality of image enhancement, an end-to-end\nsolution for simultaneous recovery of the finer texture details and sharpness\nfor degraded images with low resolution is still unsolved. Some existing\napproaches focus on minimizing the pixel-wise reconstruction error which\nresults in a high peak signal-to-noise ratio. The enhanced images fail to\nprovide high-frequency details and are perceptually unsatisfying, i.e., they\nfail to match the quality expected in a photo-realistic image. In this paper,\nwe present Image Enhancement Generative Adversarial Network (IEGAN), a\nversatile framework capable of inferring photo-realistic natural images for\nboth artifact removal and super-resolution simultaneously. Moreover, we propose\na new loss function consisting of a combination of reconstruction loss, feature\nloss and an edge loss counterpart. The feature loss helps to push the output\nimage to the natural image manifold and the edge loss preserves the sharpness\nof the output image. The reconstruction loss provides low-level semantic\ninformation to the generator regarding the quality of the generated images\ncompared to the original. Our approach has been experimentally proven to\nrecover photo-realistic textures from heavily compressed low-resolution images\non public benchmarks and our proposed high-resolution World100 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:24:42 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ghosh", "Soumya Shubhra", ""], ["Hua", "Yang", ""], ["Mukherjee", "Sankha Subhra", ""], ["Robertson", "Neil", ""]]}, {"id": "1811.09150", "submitter": "Chen Chen", "authors": "Xiandong Meng, Xuan Deng, Shuyuan Zhu, Shuaicheng Liu, Chuan Wang,\n  Chen Chen, Bing Zeng", "title": "MGANet: A Robust Model for Quality Enhancement of Compressed Video", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video compression, most of the existing deep learning approaches\nconcentrate on the visual quality of a single frame, while ignoring the useful\npriors as well as the temporal information of adjacent frames. In this paper,\nwe propose a multi-frame guided attention network (MGANet) to enhance the\nquality of compressed videos. Our network is composed of a temporal encoder\nthat discovers inter-frame relations, a guided encoder-decoder subnet that\nencodes and enhances the visual patterns of target frame, and a\nmulti-supervised reconstruction component that aggregates information to\npredict details. We design a bidirectional residual convolutional LSTM unit to\nimplicitly discover frames variations over time with respect to the target\nframe. Meanwhile, the guided map is proposed to guide our network to\nconcentrate more on the block boundary. Our approach takes advantage of\nintra-frame prior information and inter-frame information to improve the\nquality of compressed video. Experimental results show the robustness and\nsuperior performance of the proposed method.Code is available at\nhttps://github.com/mengab/MGANet\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:58:44 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 01:46:34 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 02:37:01 GMT"}, {"version": "v4", "created": "Tue, 15 Jan 2019 12:42:36 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Meng", "Xiandong", ""], ["Deng", "Xuan", ""], ["Zhu", "Shuyuan", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Chuan", ""], ["Chen", "Chen", ""], ["Zeng", "Bing", ""]]}, {"id": "1811.09171", "submitter": "Thalaiyasingam Ajanthan", "authors": "Richard Hartley and Thalaiyasingam Ajanthan", "title": "Generalized Range Moves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider move-making algorithms for energy minimization of multi-label\nMarkov Random Fields (MRFs). Since this is not a tractable problem in general,\na commonly used heuristic is to minimize over subsets of labels and variables\nin an iterative procedure. Such methods include {\\alpha}-expansion,\n{\\alpha}{\\beta}-swap, and range-moves. In each iteration, a small subset of\nvariables are active in the optimization, which diminishes their effectiveness,\nand increases the required number of iterations. In this paper, we present a\nmethod in which optimization can be carried out over all labels, and most, or\nall variables at once. Experiments show substantial improvement with respect to\nprevious move-making algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 13:42:33 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Hartley", "Richard", ""], ["Ajanthan", "Thalaiyasingam", ""]]}, {"id": "1811.09173", "submitter": "Huiwen Dong", "authors": "Huiwen Dong, Jing Yu, Chuangbai Xiao", "title": "Dual Reweighted Lp-Norm Minimization for Salt-and-pepper Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust principal component analysis (RPCA), which aims to estimate\nunderlying low-rank and sparse structures from the degraded observation data,\nhas found wide applications in computer vision. It is usually replaced by the\nprincipal component pursuit (PCP) model in order to pursue the convex property,\nleading to the undesirable overshrink problem. In this paper, we propose a dual\nweighted lp-norm (DWLP) model with a more reasonable weighting rule and weaker\npowers, which greatly generalizes the previous work and provides a better\napproximation to the rank minimization problem for original matrix as well as\nthe l0-norm minimization problem for sparse data. Moreover, an approximate\nclosed-form solution is introduced to solve the lp-norm minimization, which has\nmore stability in the nonconvex optimization and provides a more accurate\nestimation for the low-rank and sparse matrix recovery problem. We then apply\nthe DWLP model to remove salt-and-pepper noise by exploiting the image nonlocal\nself-similarity. Both qualitative and quantitative experiments demonstrate that\nthe proposed method outperforms other state-of-the-art methods. In terms of\nPSNR evaluation, our DWLP achieves about 7.188dB, 5.078dB, 3.854dB, 2.536dB and\n0.158dB improvements over the current WSNM-RPCA under 10\\% to 50\\%\nsalt-and-pepper noise with an interval 10\\% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 13:50:30 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 08:15:33 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 05:54:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Dong", "Huiwen", ""], ["Yu", "Jing", ""], ["Xiao", "Chuangbai", ""]]}, {"id": "1811.09178", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck and St\\'ephane Dupont", "title": "Object-oriented Targets for Visual Navigation using Rich Semantic\n  Representations", "comments": "Presented at NIPS workshop (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching for an object humans navigate through a scene using semantic\ninformation and spatial relationships. We look for an object using our\nknowledge of its attributes and relationships with other objects to infer the\nprobable location. In this paper, we propose to tackle the visual navigation\nproblem using rich semantic representations of the observed scene and\nobject-oriented targets to train an agent. We show that both allows the agent\nto generalize to new targets and unseen scene in a short amount of training\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:07:16 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 16:33:30 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "1811.09192", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Oleksiy Ostapenko, Patrick J\\\"ahnichen, Tassilo Klein,\n  Moin Nabi", "title": "Self Paced Adversarial Training for Multimodal Few-shot Learning", "comments": "To appear at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning algorithms yield remarkable results in many\nvisual recognition tasks. However, they still fail to provide satisfactory\nresults in scarce data regimes. To a certain extent this lack of data can be\ncompensated by multimodal information. Missing information in one modality of a\nsingle data point (e.g. an image) can be made up for in another modality (e.g.\na textual description). Therefore, we design a few-shot learning task that is\nmultimodal during training (i.e. image and text) and single-modal during test\ntime (i.e. image). In this regard, we propose a self-paced class-discriminative\ngenerative adversarial network incorporating multimodality in the context of\nfew-shot learning. The proposed approach builds upon the idea of cross-modal\ndata generation in order to alleviate the data sparsity problem. We improve\nfew-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:29:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Pahde", "Frederik", ""], ["Ostapenko", "Oleksiy", ""], ["J\u00e4hnichen", "Patrick", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1811.09236", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, Gokhan Yildirim", "title": "Copy the Old or Paint Anew? An Adversarial Framework for (non-)\n  Parametric Image Stylization", "comments": "Accepted at the NIPS 2018 workshop on Machine Learning for Creativity\n  and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric generative deep models are state-of-the-art for photo and\nnon-photo realistic image stylization. However, learning complicated image\nrepresentations requires compute-intense models parametrized by a huge number\nof weights, which in turn requires large datasets to make learning successful.\nNon-parametric exemplar-based generation is a technique that works well to\nreproduce style from small datasets, but is also compute-intensive. These\naspects are a drawback for the practice of digital AI artists: typically one\nwants to use a small set of stylization images, and needs a fast flexible model\nin order to experiment with it. With this motivation, our work has these\ncontributions: (i) a novel stylization method called Fully Adversarial Mosaics\n(FAMOS) that combines the strengths of both parametric and non-parametric\napproaches; (ii) multiple ablations and image examples that analyze the method\nand show its capabilities; (iii) source code that will empower artists and\nmachine learning researchers to use and modify FAMOS.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 16:54:12 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Yildirim", "Gokhan", ""]]}, {"id": "1811.09243", "submitter": "Dongyang Kuang", "authors": "Dongyang Kuang and Tanya Schmah", "title": "FAIM -- A ConvNet Method for Unsupervised 3D Medical Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new unsupervised learning algorithm, \"FAIM\", for 3D medical\nimage registration. With a different architecture than the popular \"U-net\", the\nnetwork takes a pair of full image volumes and predicts the displacement fields\nneeded to register source to target. Compared with \"U-net\" based registration\nnetworks such as VoxelMorph, FAIM has fewer trainable parameters but can\nachieve higher registration accuracy as judged by Dice score on region labels\nin the Mindboggle-101 dataset. Moreover, with the proposed penalty loss on\nnegative Jacobian determinants, FAIM produces deformations with many fewer\n\"foldings\", i.e. regions of non-invertibility where the surface folds over\nitself. In our experiment, we varied the strength of this penalty and\ninvestigated changes in registration accuracy and non-invertibility in terms of\nnumber of \"folding\" locations. We found that FAIM is able to maintain both the\nadvantages of higher accuracy and fewer \"folding\" locations over VoxelMorph,\nover a range of hyper-parameters (with the same values used for both networks).\nFurther, when trading off registration accuracy for better invertibility, FAIM\nrequired less sacrifice of registration accuracy. Codes for this paper will be\nreleased upon publication.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:29:04 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 20:43:34 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kuang", "Dongyang", ""], ["Schmah", "Tanya", ""]]}, {"id": "1811.09244", "submitter": "Fahdi Kanavati", "authors": "Fahdi Kanavati, Shah Islam, Eric O. Aboagye, Andrea Rockall", "title": "Automatic L3 slice detection in 3D CT images using fully-convolutional\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of single CT slices extracted at the third lumbar vertebra (L3)\nhas garnered significant clinical interest in the past few years, in particular\nin regards to quantifying sarcopenia (muscle loss). In this paper, we propose\nan efficient method to automatically detect the L3 slice in 3D CT images. Our\nmethod works with images with a variety of fields of view, occlusions, and\nslice thicknesses. 3D CT images are first converted into 2D via Maximal\nIntensity Projection (MIP), reducing the dimensionality of the problem. The MIP\nimages are then used as input to a 2D fully-convolutional network to predict\nthe L3 slice locations in the form of 2D confidence maps. In addition we\npropose a variant architecture with less parameters allowing 1D confidence map\nprediction and slightly faster prediction time without loss of accuracy.\nQuantitative evaluation of our method on a dataset of 1006 3D CT images yields\na median error of 1mm, similar to the inter-rater median error of 1mm obtained\nfrom two annotators, demonstrating the effectiveness of our method in\nefficiently and accurately detecting the L3 slice.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:31:18 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Kanavati", "Fahdi", ""], ["Islam", "Shah", ""], ["Aboagye", "Eric O.", ""], ["Rockall", "Andrea", ""]]}, {"id": "1811.09245", "submitter": "Masaki Saito", "authors": "Masaki Saito, Shunta Saito, Masanori Koyama, Sosuke Kobayashi", "title": "Train Sparsely, Generate Densely: Memory-efficient Unsupervised Training\n  of High-resolution Temporal GAN", "comments": "Accepted at International Journal of Computer Vision. The source code\n  is available at https://github.com/pfnet-research/tgan2", "journal-ref": null, "doi": "10.1007/s11263-020-01333-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of Generative Adversarial Network (GAN) on a video dataset is a\nchallenge because of the sheer size of the dataset and the complexity of each\nobservation. In general, the computational cost of training GAN scales\nexponentially with the resolution. In this study, we present a novel memory\nefficient method of unsupervised learning of high-resolution video dataset\nwhose computational cost scales only linearly with the resolution. We achieve\nthis by designing the generator model as a stack of small sub-generators and\ntraining the model in a specific way. We train each sub-generator with its own\nspecific discriminator. At the time of the training, we introduce between each\npair of consecutive sub-generators an auxiliary subsampling layer that reduces\nthe frame-rate by a certain ratio. This procedure can allow each sub-generator\nto learn the distribution of the video at different levels of resolution. We\nalso need only a few GPUs to train a highly complex generator that far\noutperforms the predecessor in terms of inception scores.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:31:26 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 11:47:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Saito", "Masaki", ""], ["Saito", "Shunta", ""], ["Koyama", "Masanori", ""], ["Kobayashi", "Sosuke", ""]]}, {"id": "1811.09310", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin and Zhezhi He, Deliang Fan", "title": "Parametric Noise Injection: Trainable Randomness to Improve Deep Neural\n  Network Robustness against Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in the field of Deep Learning have exposed the underlying\nvulnerability of Deep Neural Network (DNN) against adversarial examples. In\nimage classification, an adversarial example is a carefully modified image that\nis visually imperceptible to the original image but can cause DNN model to\nmisclassify it. Training the network with Gaussian noise is an effective\ntechnique to perform model regularization, thus improving model robustness\nagainst input variation. Inspired by this classical method, we explore to\nutilize the regularization characteristic of noise injection to improve DNN's\nrobustness against adversarial attack. In this work, we propose\nParametric-Noise-Injection (PNI) which involves trainable Gaussian noise\ninjection at each layer on either activation or weights through solving the\nmin-max optimization problem, embedded with adversarial training. These\nparameters are trained explicitly to achieve improved robustness. To the best\nof our knowledge, this is the first work that uses trainable noise injection to\nimprove network robustness against adversarial attacks, rather than manually\nconfiguring the injected noise level through cross-validation. The extensive\nresults show that our proposed PNI technique effectively improves the\nrobustness against a variety of powerful white-box and black-box attacks such\nas PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the\nleast, PNI method improves both clean- and perturbed-data accuracy in\ncomparison to the state-of-the-art defense methods, which outperforms current\nunbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test\ndata respectively using Resnet-20 architecture.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 21:10:52 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""]]}, {"id": "1811.09341", "submitter": "Ruizhe Zhao", "authors": "Ruizhe Zhao and Wayne Luk", "title": "Efficient Structured Pruning and Architecture Searching for Group\n  Convolution", "comments": "Published as an ICCV'19 NEUARCH workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient inference of Convolutional Neural Networks is a thriving topic\nrecently. It is desirable to achieve the maximal test accuracy under given\ninference budget constraints when deploying a pre-trained model. Network\npruning is a commonly used technique while it may produce irregular sparse\nmodels that can hardly gain actual speed-up. Group convolution is a promising\npruning target due to its regular structure; however, incorporating such\nstructure into the pruning procedure is challenging. It is because structural\nconstraints are hard to describe and can make pruning intractable to solve. The\nneed for configuring group convolution architecture, i.e., the number of\ngroups, that maximises test accuracy also increases difficulty.\n  This paper presents an efficient method to address this challenge. We\nformulate group convolution pruning as finding the optimal channel permutation\nto impose structural constraints and solve it efficiently by heuristics. We\nalso apply local search to exploring group configuration based on estimated\npruning cost to maximise test accuracy. Compared to prior work, results show\nthat our method produces competitive group convolution models for various tasks\nwithin a shorter pruning period and enables rapid group configuration\nexploration subject to inference budget constraints.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 01:45:44 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 19:36:46 GMT"}, {"version": "v3", "created": "Sat, 3 Aug 2019 08:24:57 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2019 05:14:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Ruizhe", ""], ["Luk", "Wayne", ""]]}, {"id": "1811.09347", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Yunchao Wei, Jiahui Yu, Shiyu Chang, Jinjun Xiong,\n  Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi", "title": "A Simple Non-i.i.d. Sampling Approach for Efficient Training and Better\n  Generalization", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While training on samples drawn from independent and identical distribution\nhas been a de facto paradigm for optimizing image classification networks,\nhumans learn new concepts in an easy-to-hard manner and on the selected\nexamples progressively. Driven by this fact, we investigate the training\nparadigms where the samples are not drawn from independent and identical\ndistribution. We propose a data sampling strategy, named Drop-and-Refresh\n(DaR), motivated by the learning behaviors of humans that selectively drop easy\nsamples and refresh them only periodically. We show in our experiments that the\nproposed DaR strategy can maintain (and in many cases improve) the predictive\naccuracy even when the training cost is reduced by 15% on various datasets\n(CIFAR 10, CIFAR 100 and ImageNet) and with different backbone architectures\n(ResNets, DenseNets and MobileNets). Furthermore and perhaps more importantly,\nwe find the ImageNet pre-trained models using our DaR sampling strategy\nachieves better transferability for the downstream tasks including object\ndetection (+0.3 AP), instance segmentation (+0.3 AP), scene parsing (+0.5 mIoU)\nand human pose estimation (+0.6 AP). Our investigation encourages people to\nrethink the connections between the sampling strategy for training and the\ntransferability of its learned features for pre-training ImageNet models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 02:49:47 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 03:39:15 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cheng", "Bowen", ""], ["Wei", "Yunchao", ""], ["Yu", "Jiahui", ""], ["Chang", "Shiyu", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-Mei", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "1811.09358", "submitter": "Li Shen", "authors": "Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang and Wei Liu", "title": "A Sufficient Condition for Convergences of Adam and RMSProp", "comments": "Accepted by CVPR2019 as an Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adam and RMSProp are two of the most influential adaptive stochastic\nalgorithms for training deep neural networks, which have been pointed out to be\ndivergent even in the convex setting via a few simple counterexamples. Many\nattempts, such as decreasing an adaptive learning rate, adopting a big batch\nsize, incorporating a temporal decorrelation technique, seeking an analogous\nsurrogate, etc., have been tried to promote Adam/RMSProp-type algorithms to\nconverge. In contrast with existing approaches, we introduce an alternative\neasy-to-check sufficient condition, which merely depends on the parameters of\nthe base learning rate and combinations of historical second-order moments, to\nguarantee the global convergence of generic Adam/RMSProp for solving\nlarge-scale non-convex stochastic optimization. Moreover, we show that the\nconvergences of several variants of Adam, such as AdamNC, AdaEMA, etc., can be\ndirectly implied via the proposed sufficient condition in the non-convex\nsetting. In addition, we illustrate that Adam is essentially a specifically\nweighted AdaGrad with exponential moving average momentum, which provides a\nnovel perspective for understanding Adam and RMSProp. This observation coupled\nwith this sufficient condition gives much deeper interpretations on their\ndivergences. At last, we validate the sufficient condition by applying Adam and\nRMSProp to tackle a certain counterexample and train deep neural networks.\nNumerical results are exactly in accord with our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 04:26:47 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 08:59:14 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 03:39:53 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zou", "Fangyu", ""], ["Shen", "Li", ""], ["Jie", "Zequn", ""], ["Zhang", "Weizhong", ""], ["Liu", "Wei", ""]]}, {"id": "1811.09361", "submitter": "Yang You", "authors": "Yang You, Yujing Lou, Qi Liu, Yu-Wing Tai, Lizhuang Ma, Cewu Lu,\n  Weiming Wang", "title": "Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D\n  Spherical Voxel Convolution", "comments": "8 pages, to appear on AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis without pose priors is very challenging in real\napplications, as the orientations of point clouds are often unknown. In this\npaper, we propose a brand new point-set learning framework PRIN, namely,\nPointwise Rotation-Invariant Network, focusing on rotation-invariant feature\nextraction in point clouds analysis. We construct spherical signals by Density\nAware Adaptive Sampling to deal with distorted point distributions in spherical\nspace. In addition, we propose Spherical Voxel Convolution and Point\nRe-sampling to extract rotation-invariant features for each point. Our network\ncan be applied to tasks ranging from object classification, part segmentation,\nto 3D feature matching and label alignment. We show that, on the dataset with\nrandomly rotated point clouds, PRIN demonstrates better performance than\nstate-of-the-art methods without any data augmentation. We also provide\ntheoretical analysis for the rotation-invariance achieved by our methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 05:11:14 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 03:12:53 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 02:34:50 GMT"}, {"version": "v4", "created": "Fri, 22 Mar 2019 07:54:03 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2019 15:54:34 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["You", "Yang", ""], ["Lou", "Yujing", ""], ["Liu", "Qi", ""], ["Tai", "Yu-Wing", ""], ["Ma", "Lizhuang", ""], ["Lu", "Cewu", ""], ["Wang", "Weiming", ""]]}, {"id": "1811.09393", "submitter": "Mengyu Chu", "authors": "Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix\\'e, Nils Thuerey", "title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video\n  Generation", "comments": "Project page: https://ge.in.tum.de/publications/2019-tecogan-chu/,\n  code link: https://github.com/thunil/TecoGAN", "journal-ref": null, "doi": "10.1145/3386569.3392457", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work explores temporal self-supervision for GAN-based video generation\ntasks. While adversarial training successfully yields generative models for a\nvariety of areas, temporal relationships in the generated data are much less\nexplored. Natural temporal changes are crucial for sequential generation tasks,\ne.g. video super-resolution and unpaired video translation. For the former,\nstate-of-the-art methods often favor simpler norm losses such as $L^2$ over\nadversarial training. However, their averaging nature easily leads to\ntemporally smooth results with an undesirable lack of spatial detail. For\nunpaired video translation, existing approaches modify the generator networks\nto form spatio-temporal cycle consistencies. In contrast, we focus on improving\nlearning objectives and propose a temporally self-supervised algorithm. For\nboth tasks, we show that temporal adversarial learning is key to achieving\ntemporally coherent solutions without sacrificing spatial detail. We also\npropose a novel Ping-Pong loss to improve the long-term temporal consistency.\nIt effectively prevents recurrent networks from accumulating artifacts\ntemporally without depressing detailed features. Additionally, we propose a\nfirst set of metrics to quantitatively evaluate the accuracy as well as the\nperceptual quality of the temporal evolution. A series of user studies confirm\nthe rankings computed with these metrics. Code, data, models, and results are\nprovided at https://github.com/thunil/TecoGAN. The project page\nhttps://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental\nmaterials.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 09:16:22 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:10:51 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 08:31:06 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 15:06:30 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Chu", "Mengyu", ""], ["Xie", "You", ""], ["Mayer", "Jonas", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Thuerey", "Nils", ""]]}, {"id": "1811.09410", "submitter": "Jinglu Wang", "authors": "Jinglu Wang, Bo Sun, Yan Lu", "title": "MVPNet: Multi-View Point Regression Networks for 3D Object\n  Reconstruction from A Single Image", "comments": "8 pages; accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of reconstructing an object's surface\nfrom a single image using generative networks. First, we represent a 3D surface\nwith an aggregation of dense point clouds from multiple views. Each point cloud\nis embedded in a regular 2D grid aligned on an image plane of a viewpoint,\nmaking the point cloud convolution-favored and ordered so as to fit into deep\nnetwork architectures. The point clouds can be easily triangulated by\nexploiting connectivities of the 2D grids to form mesh-based surfaces. Second,\nwe propose an encoder-decoder network that generates such kind of multiple\nview-dependent point clouds from a single image by regressing their 3D\ncoordinates and visibilities. We also introduce a novel geometric loss that is\nable to interpret discrepancy over 3D surfaces as opposed to 2D projective\nplanes, resorting to the surface discretization on the constructed meshes. We\ndemonstrate that the multi-view point regression network outperforms\nstate-of-the-art methods with a significant improvement on challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 09:56:48 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wang", "Jinglu", ""], ["Sun", "Bo", ""], ["Lu", "Yan", ""]]}, {"id": "1811.09426", "submitter": "Chen Yukang", "authors": "Yukang Chen, Gaofeng Meng, Qian Zhang, Xinbang Zhang, Liangchen Song,\n  Shiming Xiang and Chunhong Pan", "title": "Joint Neural Architecture Search and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing neural architectures is a fundamental step in deep learning\napplications. As a partner technique, model compression on neural networks has\nbeen widely investigated to gear the needs that the deep learning algorithms\ncould be run with the limited computation resources on mobile devices.\nCurrently, both the tasks of architecture design and model compression require\nexpertise tricks and tedious trials. In this paper, we integrate these two\ntasks into one unified framework, which enables the joint architecture search\nwith quantization (compression) policies for neural networks. This method is\nnamed JASQ. Here our goal is to automatically find a compact neural network\nmodel with high performance that is suitable for mobile devices. Technically, a\nmulti-objective evolutionary search algorithm is introduced to search the\nmodels under the balance between model size and performance accuracy. In\nexperiments, we find that our approach outperforms the methods that search only\nfor architectures or only for quantization policies. 1) Specifically, given\nexisting networks, our approach can provide them with learning-based\nquantization policies, and outperforms their 2 bits, 4 bits, 8 bits, and 16\nbits counterparts. It can yield higher accuracies than the float models, for\nexample, over 1.02% higher accuracy on MobileNet-v1. 2) What is more, under the\nbalance between model size and performance accuracy, two models are obtained\nwith joint search of architectures and quantization policies: a high-accuracy\nmodel and a small model, JASQNet and JASQNet-Small that achieves 2.97% error\nrate with 0.9 MB on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 10:58:46 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Chen", "Yukang", ""], ["Meng", "Gaofeng", ""], ["Zhang", "Qian", ""], ["Zhang", "Xinbang", ""], ["Song", "Liangchen", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1811.09461", "submitter": "Michael Gygli", "authors": "Michael Gygli, Vittorio Ferrari", "title": "Fast Object Class Labelling via Speech", "comments": "to be published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object class labelling is the task of annotating images with labels on the\npresence or absence of objects from a given class vocabulary. Simply asking one\nyes/no question per class, however, has a cost that is linear in the vocabulary\nsize and is thus inefficient for large vocabularies. Modern approaches rely on\na hierarchical organization of the vocabulary to reduce annotation time, but\nremain expensive (several minutes per image for the 200 classes in ILSVRC).\nInstead, we propose a new interface where classes are annotated via speech.\nSpeaking is fast and allows for direct access to the class name, without\nsearching through a list or hierarchy. As additional advantages, annotators can\nsimultaneously speak and scan the image for objects, the interface can be kept\nextremely simple, and using it requires less mouse movement. As annotators\nusing our interface should only say words from a given class vocabulary, we\npropose a dedicated task that trains them to do so. Through experiments on COCO\nand ILSVRC, we show our method yields high-quality annotations at 2.3x - 14.9x\nless annotation time than existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:08:39 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 15:33:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1811.09473", "submitter": "Meng Lan", "authors": "Meng Lan, Yipeng Zhang, Lefei Zhang, Bo Du", "title": "Defect Detection from UAV Images based on Region-Based CNNs", "comments": "received as best paper of The first International Workshop on\n  Developmental Learning Workshop in ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide applications of Unmanned Aerial Vehicle (UAV) in engineering\nsuch as the inspection of the electrical equipment from distance, the demands\nof efficient object detection algorithms for abundant images acquired by UAV\nhave also been significantly increased in recent years. In this work, we study\nthe performance of the region-based CNN for the electrical equipment defect\ndetection by using the UAV images. In order to train the detection model, we\ncollect a UAV images dataset composes of four classes of electrical equipment\ndefects with thousands of annotated labels. Then, based on the region-based\nfaster R-CNN model, we present a multi-class defects detection model for\nelectrical equipment which is more efficient and accurate than traditional\nsingle class detection methods. Technically, we have replaced the RoI pooling\nlayer with a similar operation in Tensorflow and promoted the mini-batch to 128\nper image in the training procedure. These improvements have slightly increased\nthe speed of detection without any accuracy loss. Therefore, the modified\nregion-based CNN could simultaneously detect multi-class of defects of the\nelectrical devices in nearly real time. Experimental results on the real word\nelectrical equipment images demonstrate that the proposed method achieves\nbetter performance than the traditional object detection algorithms in defect\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:38:17 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Lan", "Meng", ""], ["Zhang", "Yipeng", ""], ["Zhang", "Lefei", ""], ["Du", "Bo", ""]]}, {"id": "1811.09485", "submitter": "Janne Mustaniemi", "authors": "Janne Mustaniemi, Juho Kannala, Jiri Matas, Simo S\\\"arkk\\\"a and Janne\n  Heikkil\\\"a", "title": "LSD$_2$ -- Joint Denoising and Deblurring of Short and Long Exposure\n  Images with CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of acquiring high-quality photographs with\nhandheld smartphone cameras in low-light imaging conditions. We propose an\napproach based on capturing pairs of short and long exposure images in rapid\nsuccession and fusing them into a single high-quality photograph. Unlike\nexisting methods, we take advantage of both images simultaneously and perform a\njoint denoising and deblurring using a convolutional neural network. A novel\napproach is introduced to generate realistic short-long exposure image pairs.\nThe method produces good images in extremely challenging conditions and\noutperforms existing denoising and deblurring methods. It also enables exposure\nfusion in the presence of motion blur.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 14:13:33 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:15:19 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 20:24:46 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mustaniemi", "Janne", ""], ["Kannala", "Juho", ""], ["Matas", "Jiri", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1811.09497", "submitter": "Georg Poier", "authors": "Georg Poier, Michael Opitz, David Schinagl, Horst Bischof", "title": "MURAUER: Mapping Unlabeled Real Data for Label AUstERity", "comments": "WACV 2019; Project page at https://poier.github.io/murauer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data labeling for learning 3D hand pose estimation models is a huge effort.\nReadily available, accurately labeled synthetic data has the potential to\nreduce the effort. However, to successfully exploit synthetic data, current\nstate-of-the-art methods still require a large amount of labeled real data. In\nthis work, we remove this requirement by learning to map from the features of\nreal data to the features of synthetic data mainly using a large amount of\nsynthetic and unlabeled real data. We exploit unlabeled data using two\nauxiliary objectives, which enforce that (i) the mapped representation is pose\nspecific and (ii) at the same time, the distributions of real and synthetic\ndata are aligned. While pose specifity is enforced by a self-supervisory signal\nrequiring that the representation is predictive for the appearance from\ndifferent views, distributions are aligned by an adversarial term. In this way,\nwe can significantly improve the results of the baseline system, which does not\nuse unlabeled data and outperform many recent approaches already with about 1%\nof the labeled real data. This presents a step towards faster deployment of\nlearning based hand pose estimation, making it accessible for a larger range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 14:40:07 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 12:30:49 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Poier", "Georg", ""], ["Opitz", "Michael", ""], ["Schinagl", "David", ""], ["Bischof", "Horst", ""]]}, {"id": "1811.09521", "submitter": "Jia Li", "authors": "Jia Li, Junjie Wu, Anlin Zheng, Yafei Song, Yu Zhang, Xiaowu Chen", "title": "Complementary Segmentation of Primary Video Objects with Reversible\n  Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting primary objects in a video is an important yet challenging problem\nin computer vision, as it exhibits various levels of foreground/background\nambiguities. To reduce such ambiguities, we propose a novel formulation via\nexploiting foreground and background context as well as their complementary\nconstraint. Under this formulation, a unified objective function is further\ndefined to encode each cue. For implementation, we design a Complementary\nSegmentation Network (CSNet) with two separate branches, which can\nsimultaneously encode the foreground and background information along with\njoint spatial constraints. The CSNet is trained on massive images with manually\nannotated salient objects in an end-to-end manner. By applying CSNet on each\nvideo frame, the spatial foreground and background maps can be initialized. To\nenforce temporal consistency effectively and efficiently, we divide each frame\ninto superpixels and construct neighborhood reversible flow that reflects the\nmost reliable temporal correspondences between superpixels in far-away frames.\nWith such flow, the initialized foregroundness and backgroundness can be\npropagated along the temporal dimension so that primary video objects gradually\npop-out and distractors are well suppressed. Extensive experimental results on\nthree video datasets show that the proposed approach achieves impressive\nperformance in comparisons with 18 state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 15:29:38 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Li", "Jia", ""], ["Wu", "Junjie", ""], ["Zheng", "Anlin", ""], ["Song", "Yafei", ""], ["Zhang", "Yu", ""], ["Chen", "Xiaowu", ""]]}, {"id": "1811.09543", "submitter": "Ji Zhang", "authors": "Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal", "title": "An Interpretable Model for Scene Graph Generation", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.00662", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient and interpretable scene graph generator. We consider\nthree types of features: visual, spatial and semantic, and we use a late fusion\nstrategy such that each feature's contribution can be explicitly investigated.\nWe study the key factors about these features that have the most impact on the\nperformance, and also visualize the learned visual features for relationships\nand investigate the efficacy of our model. We won the champion of the\nOpenImages Visual Relationship Detection Challenge on Kaggle, where we\noutperform the 2nd place by 5\\% (20\\% relatively). We believe an accurate scene\ngraph generator is a fundamental stepping stone for higher-level\nvision-language tasks such as image captioning and visual QA, since it provides\na semantic, structured comprehension of an image that is beyond pixels and\nobjects.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:51:01 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Zhang", "Ji", ""], ["Shih", "Kevin", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1811.09561", "submitter": "Volker Steinhage", "authors": "Jonatan Grimm, Katja Herzog, Florian Rist, Anna Kicherer, Reinhard\n  T\\\"opfer, Volker Steinhage", "title": "An Adaptive Approach for Automated Grapevine Phenotyping using VGG-based\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (grapevine) breeding programs and research, periodic phenotyping and\nmulti-year monitoring of different grapevine traits, like growth or yield, is\nneeded especially in the field. This demand imply objective, precise and\nautomated methods using sensors and adaptive software. This work presents a\nproof-of-concept analyzing RGB images of different growth stages of grapevines\nwith the aim to detect and quantify promising plant organs which are related to\nyield. The input images are segmented by a Fully Convolutional Neural Network\n(FCN) into object and background pixels. The objects are plant organs like\nyoung shoots, pedicels, flower buds or grapes, which are principally suitable\nfor yield estimation. In the ground truth of the training images, each object\nis separately annotated as a connected segment of object pixels, which enables\nend-to-end learning of the object features. Based on the CNN-based\nsegmentation, the number of objects is determined by detecting and counting\nconnected components of object pixels using region labeling. In an evaluation\non six different data sets, the system achieves an IoU of up to 87.3% for the\nsegmentation and an F1 score of up to 88.6% for the object detection.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:05:31 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 09:56:45 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Grimm", "Jonatan", ""], ["Herzog", "Katja", ""], ["Rist", "Florian", ""], ["Kicherer", "Anna", ""], ["T\u00f6pfer", "Reinhard", ""], ["Steinhage", "Volker", ""]]}, {"id": "1811.09567", "submitter": "Yipeng Qin", "authors": "Yipeng Qin, Niloy Mitra, Peter Wonka", "title": "How does Lipschitz Regularization Influence GAN Training?", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Lipschitz regularization in stabilizing GAN training,\nthe exact reason of its effectiveness remains poorly understood. The direct\neffect of $K$-Lipschitz regularization is to restrict the $L2$-norm of the\nneural network gradient to be smaller than a threshold $K$ (e.g., $K=1$) such\nthat $\\|\\nabla f\\| \\leq K$. In this work, we uncover an even more important\neffect of Lipschitz regularization by examining its impact on the loss\nfunction: It degenerates GAN loss functions to almost linear ones by\nrestricting their domain and interval of attainable gradient values. Our\nanalysis shows that loss functions are only successful if they are degenerated\nto almost linear ones. We also show that loss functions perform poorly if they\nare not degenerated and that a wide range of functions can be used as loss\nfunction as long as they are sufficiently degenerated by regularization.\nBasically, Lipschitz regularization ensures that all loss functions effectively\nwork in the same way. Empirically, we verify our proposition on the MNIST,\nCIFAR10 and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:18:00 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 14:00:53 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 07:39:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Qin", "Yipeng", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "1811.09600", "submitter": "Luiz Gustavo Hafemann", "authors": "J\\'er\\^ome Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed,\n  Robert Sabourin, Eric Granger", "title": "Decoupling Direction and Norm for Efficient Gradient-Based L2\n  Adversarial Attacks and Defenses", "comments": "Accepted as a conference paper to the 2019 IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition (CVPR oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on adversarial examples in computer vision tasks has shown that\nsmall, often imperceptible changes to an image can induce misclassification,\nwhich has security implications for a wide range of image processing systems.\nConsidering $L_2$ norm distortions, the Carlini and Wagner attack is presently\nthe most effective white-box attack in the literature. However, this method is\nslow since it performs a line-search for one of the optimization terms, and\noften requires thousands of iterations. In this paper, an efficient approach is\nproposed to generate gradient-based attacks that induce misclassifications with\nlow $L_2$ norm, by decoupling the direction and the norm of the adversarial\nperturbation that is added to the image. Experiments conducted on the MNIST,\nCIFAR-10 and ImageNet datasets indicate that our attack achieves comparable\nresults to the state-of-the-art (in terms of $L_2$ norm) with considerably\nfewer iterations (as few as 100 iterations), which opens the possibility of\nusing these attacks for adversarial training. Models trained with our attack\nachieve state-of-the-art robustness against white-box gradient-based $L_2$\nattacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense\nwhen the attacks are limited to a maximum norm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 18:54:47 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 21:11:22 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 21:11:11 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Rony", "J\u00e9r\u00f4me", ""], ["Hafemann", "Luiz G.", ""], ["Oliveira", "Luiz S.", ""], ["Ayed", "Ismail Ben", ""], ["Sabourin", "Robert", ""], ["Granger", "Eric", ""]]}, {"id": "1811.09618", "submitter": "Shenlong Lou", "authors": "Shenlong Lou, Yan Luo, Qiancong Fan, Feng Chen, Yiping Chen, Cheng\n  Wang, Jonathan Li", "title": "NeuroTreeNet: A New Method to Explore Horizontal Expansion Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized that the deeper networks or networks with more\nfeature maps have better performance. Existing studies mainly focus on\nextending the network depth and increasing the feature maps of networks. At the\nsame time, horizontal expansion network (e.g. Inception Model) as an\nalternative way to improve network performance has not been fully investigated.\nAccordingly, we proposed NeuroTreeNet (NTN), as a new horizontal extension\nnetwork through the combination of random forest and Inception Model. Based on\nthe tree structure, in which each branch represents a network and the root node\nfeatures are shared to child nodes, network parameters are effectively reduced.\nBy combining all features of leaf nodes, even less feature maps achieved better\nperformance. In addition, the relationship between tree structure and the\nperformance of NTN was investigated in depth. Comparing to other networks (e.g.\nVDSR\\_5) with equal magnitude parameters, our model showed preferable\nperformance in super resolution reconstruction task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:16:04 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Lou", "Shenlong", ""], ["Luo", "Yan", ""], ["Fan", "Qiancong", ""], ["Chen", "Feng", ""], ["Chen", "Yiping", ""], ["Wang", "Cheng", ""], ["Li", "Jonathan", ""]]}, {"id": "1811.09651", "submitter": "Hady Ahmady Phoulady", "authors": "Hady Ahmady Phoulady and Peter R. Mouton", "title": "A New Cervical Cytology Dataset for Nucleus Detection and Image\n  Classification (Cervix93) and Methods for Cervical Nucleus Detection", "comments": "5 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing Pap cytology slides is an important tasks in detecting and grading\nprecancerous and cancerous cervical cancer stages. Processing cytology images\nusually involve segmenting nuclei and overlapping cells. We introduce a\ncervical cytology dataset that can be used to evaluate nucleus detection, as\nwell as image classification methods in the cytology image processing area.\nThis dataset contains 93 real image stacks with their grade labels and manually\nannotated nuclei within images. We also present two methods: a baseline method\nbased on a previously proposed approach, and a deep learning method, and\ncompare their results with other state-of-the-art methods. Both the baseline\nmethod and the deep learning method outperform other state-of-the-art methods\nby significant margins. Along with the dataset, we publicly make the evaluation\ncode and the baseline method available to download for further benchmarking.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 19:41:47 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Phoulady", "Hady Ahmady", ""], ["Mouton", "Peter R.", ""]]}, {"id": "1811.09655", "submitter": "Hans Atlason", "authors": "Hans E. Atlason, Askell Love, Sigurdur Sigurdsson, Vilmundur Gudnason\n  and Lotta M. Ellingsen", "title": "Unsupervised brain lesion segmentation from MRI using a convolutional\n  autoencoder", "comments": null, "journal-ref": null, "doi": "10.1117/12.2512953", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesions that appear hyperintense in both Fluid Attenuated Inversion Recovery\n(FLAIR) and T2-weighted magnetic resonance images (MRIs) of the human brain are\ncommon in the brains of the elderly population and may be caused by ischemia or\ndemyelination. Lesions are biomarkers for various neurodegenerative diseases,\nmaking accurate quantification of them important for both disease diagnosis and\nprogression. Automatic lesion detection using supervised learning requires\nmanually annotated images, which can often be impractical to acquire.\nUnsupervised lesion detection, on the other hand, does not require any manual\ndelineation; however, these methods can be challenging to construct due to the\nvariability in lesion load, placement of lesions, and voxel intensities. Here\nwe present a novel approach to address this problem using a convolutional\nautoencoder, which learns to segment brain lesions as well as the white matter,\ngray matter, and cerebrospinal fluid by reconstructing FLAIR images as conical\ncombinations of softmax layer outputs generated from the corresponding T1, T2,\nand FLAIR images. Some of the advantages of this model are that it accurately\nlearns to segment lesions regardless of lesion load, and it can be used to\nquickly and robustly segment new images that were not in the training set.\nComparisons with state-of-the-art segmentation methods evaluated on ground\ntruth manual labels indicate that the proposed method works well for generating\naccurate lesion segmentations without the need for manual annotations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 19:53:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Atlason", "Hans E.", ""], ["Love", "Askell", ""], ["Sigurdsson", "Sigurdur", ""], ["Gudnason", "Vilmundur", ""], ["Ellingsen", "Lotta M.", ""]]}, {"id": "1811.09675", "submitter": "Kazuto Ichimaru", "authors": "Kazuto Ichimaru and Ryo Furukawa and Hiroshi Kawasaki", "title": "CNN based dense underwater 3D scene reconstruction by transfer learning\n  using bubble database", "comments": "IEEE Winter Conference on Applications of Computer Vision. arXiv\n  admin note: text overlap with arXiv:1808.08348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense 3D shape acquisition of swimming human or live fish is an important\nresearch topic for sports, biological science and so on. For this purpose,\nactive stereo sensor is usually used in the air, however it cannot be applied\nto the underwater environment because of refraction, strong light attenuation\nand severe interference of bubbles. Passive stereo is a simple solution for\ncapturing dynamic scenes at underwater environment, however the shape with\ntextureless surfaces or irregular reflections cannot be recovered. Recently,\nthe stereo camera pair with a pattern projector for adding artificial textures\non the objects is proposed. However, to use the system for underwater\nenvironment, several problems should be compensated, i.e., disturbance by\nfluctuation and bubbles. Simple solution is to use convolutional neural network\nfor stereo to cancel the effects of bubbles and/or water fluctuation. Since it\nis not easy to train CNN with small size of database with large variation, we\ndevelop a special bubble generation device to efficiently create real bubble\ndatabase of multiple size and density. In addition, we propose a transfer\nlearning technique for multi-scale CNN to effectively remove bubbles and\nprojected-patterns on the object. Further, we develop a real system and\nactually captured live swimming human, which has not been done before.\nExperiments are conducted to show the effectiveness of our method compared with\nthe state of the art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:08:51 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ichimaru", "Kazuto", ""], ["Furukawa", "Ryo", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1811.09681", "submitter": "Ahmad S. Tarawneh", "authors": "Ahmad S. Tarawneh, Ceyhun Celik, Ahmad B. Hassanat, Dmitry Chetverikov", "title": "Detailed Investigation of Deep Features with Sparse Representation and\n  Dimensionality Reduction in CBIR: A Comparative Study", "comments": null, "journal-ref": "Intelligent Data Analysis, vol. 24, no. 1, 2020", "doi": "10.3233/IDA-180895", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on content-based image retrieval (CBIR) has been under development\nfor decades, and numerous methods have been competing to extract the most\ndiscriminative features for improved representation of the image content.\nRecently, deep learning methods have gained attention in computer vision,\nincluding CBIR. In this paper, we present a comparative investigation of\ndifferent features, including low-level and high-level features, for CBIR. We\ncompare the performance of CBIR systems using different deep features with\nstate-of-the-art low-level features such as SIFT, SURF, HOG, LBP, and LTP,\nusing different dictionaries and coefficient learning techniques. Furthermore,\nwe conduct comparisons with a set of primitive and popular features that have\nbeen used in this field, including colour histograms and Gabor features. We\nalso investigate the discriminative power of deep features using certain\nsimilarity measures under different validation approaches. Furthermore, we\ninvestigate the effects of the dimensionality reduction of deep features on the\nperformance of CBIR systems using principal component analysis, discrete\nwavelet transform, and discrete cosine transform. Unprecedentedly, the\nexperimental results demonstrate high (95\\% and 93\\%) mean average precisions\nwhen using the VGG-16 FC7 deep features of Corel-1000 and Coil-20 datasets with\n10-D and 20-D K-SVD, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 20:53:13 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Tarawneh", "Ahmad S.", ""], ["Celik", "Ceyhun", ""], ["Hassanat", "Ahmad B.", ""], ["Chetverikov", "Dmitry", ""]]}, {"id": "1811.09699", "submitter": "Hossein Adeli", "authors": "Hossein Adeli, Gregory Zelinsky", "title": "Learning to attend in a brain-inspired deep neural network", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning models have shown that including attention as a\ncomponent results in improved model accuracy and interpretability, despite the\nconcept of attention in these approaches only loosely approximating the brain's\nattention mechanism. Here we extend this work by building a more brain-inspired\ndeep network model of the primate ATTention Network (ATTNet) that learns to\nshift its attention so as to maximize the reward. Using deep reinforcement\nlearning, ATTNet learned to shift its attention to the visual features of a\ntarget category in the context of a search task. ATTNet's dorsal layers also\nlearned to prioritize these shifts of attention so as to maximize success of\nthe ventral pathway classification and receive greater reward. Model behavior\nwas tested against the fixations made by subjects searching images for the same\ncued category. Both subjects and ATTNet showed evidence for attention being\npreferentially directed to target goals, behaviorally measured as oculomotor\nguidance to targets. More fundamentally, ATTNet learned to shift its attention\nto target like objects and spatially route its visual inputs to accomplish the\ntask. This work makes a step toward a better understanding of the role of\nattention in the brain and other computational systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 21:23:56 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Adeli", "Hossein", ""], ["Zelinsky", "Gregory", ""]]}, {"id": "1811.09716", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato,\n  Pascal Frossard", "title": "Robustness via curvature regularization, and vice versa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:03:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Uesato", "Jonathan", ""], ["Frossard", "Pascal", ""]]}, {"id": "1811.09729", "submitter": "Peng Zhou", "authors": "Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi, Abhinav\n  Shrivastava, Ser Nam Lim and Larry S. Davis", "title": "Generate, Segment and Refine: Towards Generic Manipulation Segmentation", "comments": null, "journal-ref": "AAAI-2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulated images has become a significant emerging challenge. The\nadvent of image sharing platforms and the easy availability of advanced photo\nediting software have resulted in a large quantities of manipulated images\nbeing shared on the internet. While the intent behind such manipulations varies\nwidely, concerns on the spread of fake news and misinformation is growing.\nCurrent state of the art methods for detecting these manipulated images suffers\nfrom the lack of training data due to the laborious labeling process. We\naddress this problem in this paper, for which we introduce a manipulated image\ngeneration process that creates true positives using currently available\ndatasets. Drawing from traditional work on image blending, we propose a novel\ngenerator for creating such examples. In addition, we also propose to further\ncreate examples that force the algorithm to focus on boundary artifacts during\ntraining. Strong experimental results validate our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 00:06:10 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 21:52:57 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 18:29:06 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhou", "Peng", ""], ["Chen", "Bor-Chun", ""], ["Han", "Xintong", ""], ["Najibi", "Mahyar", ""], ["Shrivastava", "Abhinav", ""], ["Lim", "Ser Nam", ""], ["Davis", "Larry S.", ""]]}, {"id": "1811.09745", "submitter": "Zhongwen Zhang", "authors": "Zhongwen Zhang, Egor Chesakov, Dmitrii Marin, Yuri Boykov", "title": "Divergence Prior and Vessel-tree Reconstruction", "comments": "10 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 02:14:43 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Zhongwen", ""], ["Chesakov", "Egor", ""], ["Marin", "Dmitrii", ""], ["Boykov", "Yuri", ""]]}, {"id": "1811.09750", "submitter": "Siddique Latif", "authors": "Siddique Latif, Muhammad Asim, Muhammad Usman, Junaid Qadir, and Rajib\n  Rana", "title": "Automating Motion Correction in Multishot MRI Using Generative\n  Adversarial Networks", "comments": null, "journal-ref": "MED-NIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multishot Magnetic Resonance Imaging (MRI) has recently gained popularity as\nit accelerates the MRI data acquisition process without compromising the\nquality of final MR image. However, it suffers from motion artifacts caused by\npatient movements which may lead to misdiagnosis. Modern state-of-the-art\nmotion correction techniques are able to counter small degree motion, however,\ntheir adoption is hindered by their time complexity. This paper proposes a\nGenerative Adversarial Network (GAN) for reconstructing motion free\nhigh-fidelity images while reducing the image reconstruction time by an\nimpressive two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 03:13:27 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Latif", "Siddique", ""], ["Asim", "Muhammad", ""], ["Usman", "Muhammad", ""], ["Qadir", "Junaid", ""], ["Rana", "Rajib", ""]]}, {"id": "1811.09763", "submitter": "Pak Lun Kevin Ding", "authors": "Pak Lun Kevin Ding, Yikang Li, Baoxin Li", "title": "Mean Local Group Average Precision (mLGAP): A New Performance Metric for\n  Hashing-based Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 04:31:41 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ding", "Pak Lun Kevin", ""], ["Li", "Yikang", ""], ["Li", "Baoxin", ""]]}, {"id": "1811.09780", "submitter": "Huangxing Lin", "authors": "Huangxing Lin, Xueyang Fu, Changxing Jing, Xinghao Ding, Yue Huang", "title": "A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for single images raindrop removal either have poor\nrobustness or suffer from parameter burdens. In this paper, we propose a new\nAdjacent Aggregation Network (A^2Net) with lightweight architectures to remove\nraindrops from single images. Instead of directly cascading convolutional\nlayers, we design an adjacent aggregation architecture to better fuse features\nfor rich representations generation, which can lead to high quality images\nreconstruction. To further simplify the learning process, we utilize a\nproblem-specific knowledge to force the network focus on the luminance channel\nin the YUV color space instead of all RGB channels. By combining adjacent\naggregating operation with color space transformation, the proposed A^2Net can\nachieve state-of-the-art performances on raindrop removal with significant\nparameters reduction.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 07:08:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Lin", "Huangxing", ""], ["Fu", "Xueyang", ""], ["Jing", "Changxing", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""]]}, {"id": "1811.09783", "submitter": "Zhengping Zhou", "authors": "Song-Hai Zhang, Zhengping Zhou, Bin Liu, Xin Dong, Dun Liang, Peter\n  Hall, Shi-Min Hu", "title": "What and Where: A Context-based Recommendation System for Object\n  Insertion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel topic consisting of two dual tasks: 1) given\na scene, recommend objects to insert, 2) given an object category, retrieve\nsuitable background scenes. A bounding box for the inserted object is predicted\nin both tasks, which helps downstream applications such as semi-automated\nadvertising and video composition. The major challenge lies in the fact that\nthe target object is neither present nor localized at test time, whereas\navailable datasets only provide scenes with existing objects. To tackle this\nproblem, we build an unsupervised algorithm based on object-level contexts,\nwhich explicitly models the joint probability distribution of object categories\nand bounding boxes with a Gaussian mixture model. Experiments on our newly\nannotated test set demonstrate that our system outperforms existing baselines\non all subtasks, and do so under a unified framework. Our contribution promises\nfuture extensions and applications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 07:48:35 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Song-Hai", ""], ["Zhou", "Zhengping", ""], ["Liu", "Bin", ""], ["Dong", "Xin", ""], ["Liang", "Dun", ""], ["Hall", "Peter", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1811.09789", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris", "title": "Senti-Attend: Image Captioning using Sentiment and Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:47:16 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Wan", "Stephen", ""], ["Paris", "Cecile", ""]]}, {"id": "1811.09790", "submitter": "Yaochen Li", "authors": "Yaochen Li, Yuehu Liu, Jihua Zhu, Shiqi Ma, Zhenning Niu and Rui Guo", "title": "Spatio-Temporal Road Scene Reconstruction using Superpixel Markov Random\n  Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene model construction based on image rendering is an indispensable but\nchallenging technique in computer vision and intelligent transportation\nsystems. In this paper, we propose a framework for constructing 3D\ncorridor-based road scene models. This consists of two successive stages: road\ndetection and scene construction. The road detection is realized by a new\nsuperpixel Markov random field (MRF) algorithm. The data fidelity term in the\nMRF's energy function is jointly computed according to the superpixel features\nof color, texture and location. The smoothness term is established on the basis\nof the interaction of spatio-temporally adjacent superpixels. In the subsequent\nscene construction, the foreground and background regions are modeled\nindependently. Experiments for road detection demonstrate the proposed method\noutperforms the state-of-the-art in both accuracy and speed. The scene\nconstruction experiments confirm that the proposed scene models show better\ncorrectness ratios, and have the potential to support a range of applications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:48:39 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 06:17:32 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 09:46:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Li", "Yaochen", ""], ["Liu", "Yuehu", ""], ["Zhu", "Jihua", ""], ["Ma", "Shiqi", ""], ["Niu", "Zhenning", ""], ["Guo", "Rui", ""]]}, {"id": "1811.09791", "submitter": "Yunjae Jung", "authors": "Yunjae Jung, Donghyeon Cho, Dahun Kim, Sanghyun Woo, In So Kweon", "title": "Discriminative Feature Learning for Unsupervised Video Summarization", "comments": "Accepted to AAAI 2019 !!!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of unsupervised video summarization\nthat automatically extracts key-shots from an input video. Specifically, we\ntackle two critical issues based on our empirical observations: (i) Ineffective\nfeature learning due to flat distributions of output importance scores for each\nframe, and (ii) training difficulty when dealing with long-length video inputs.\nTo alleviate the first problem, we propose a simple yet effective\nregularization loss term called variance loss. The proposed variance loss\nallows a network to predict output scores for each frame with high discrepancy\nwhich enables effective feature learning and significantly improves model\nperformance. For the second problem, we design a novel two-stream network named\nChunk and Stride Network (CSNet) that utilizes local (chunk) and global\n(stride) temporal view on the video features. Our CSNet gives better\nsummarization results for long-length videos compared to the existing methods.\nIn addition, we introduce an attention mechanism to handle the dynamic\ninformation in videos. We demonstrate the effectiveness of the proposed methods\nby conducting extensive ablation studies and show that our final model achieves\nnew state-of-the-art results on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:49:06 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Jung", "Yunjae", ""], ["Cho", "Donghyeon", ""], ["Kim", "Dahun", ""], ["Woo", "Sanghyun", ""], ["Kweon", "In So", ""]]}, {"id": "1811.09795", "submitter": "Dahun Kim", "authors": "Dahun Kim, Donghyeon Cho, In So Kweon", "title": "Self-Supervised Video Representation Learning with Space-Time Cubic\n  Puzzles", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have\nbeen utilized for visual representation learning for still images, when the\nnumber of labeled images is limited or absent at all. Recently, this worthwhile\nstream of study extends to video domain where the cost of human labeling is\neven more expensive. However, the most of existing methods are still based on\n2D CNN architectures that can not directly capture spatio-temporal information\nfor video applications. In this paper, we introduce a new self-supervised task\ncalled as \\textit{Space-Time Cubic Puzzles} to train 3D CNNs using large scale\nvideo dataset. This task requires a network to arrange permuted 3D\nspatio-temporal crops. By completing \\textit{Space-Time Cubic Puzzles}, the\nnetwork learns both spatial appearance and temporal relation of video frames,\nwhich is our final goal. In experiments, we demonstrate that our learned 3D\nrepresentation is well transferred to action recognition tasks, and outperforms\nstate-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 09:08:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Kweon", "In So", ""]]}, {"id": "1811.09796", "submitter": "Dinesh Khandelwal", "authors": "Dinesh Khandelwal, Suyash Agrawal, Parag Singla, Chetan Arora", "title": "A Novel Technique for Evidence based Conditional Inference in Deep\n  Neural Networks via Latent Feature Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auxiliary information can be exploited in machine learning models using the\nparadigm of evidence based conditional inference. Multi-modal techniques in\nDeep Neural Networks (DNNs) can be seen as perturbing the latent feature\nrepresentation for incorporating evidence from the auxiliary modality. However,\nthey require training a specialized network which can map sparse evidence to a\nhigh dimensional latent space vector. Designing such a network, as well as\ncollecting jointly labeled data for training is a non-trivial task. In this\npaper, we present a novel multi-task learning (MTL) based framework to perform\nevidence based conditional inference in DNNs which can overcome both these\nshortcomings. Our framework incorporates evidence as the output of secondary\ntask(s), while modeling the original problem as the primary task of interest.\nDuring inference, we employ a novel Bayesian formulation to change the joint\nlatent feature representation so as to maximize the probability of the observed\nevidence. Since our approach models evidence as prediction from a DNN, this can\noften be achieved using standard pre-trained backbones for popular tasks,\neliminating the need for training altogether. Even when training is required,\nour MTL architecture ensures the same can be done without any need for jointly\nlabeled data. Exploiting evidence using our framework, we show an improvement\nof 3.9% over the state-of-the-art, for predicting semantic segmentation given\nthe image tags, and 2.8% for predicting instance segmentation given image\ncaptions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 09:17:57 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 06:58:17 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 05:59:04 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 03:45:45 GMT"}, {"version": "v5", "created": "Wed, 4 Dec 2019 06:59:15 GMT"}, {"version": "v6", "created": "Fri, 6 Dec 2019 06:36:51 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Khandelwal", "Dinesh", ""], ["Agrawal", "Suyash", ""], ["Singla", "Parag", ""], ["Arora", "Chetan", ""]]}, {"id": "1811.09800", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab, Christian Wachinger", "title": "Bayesian QuickNAT: Model Uncertainty in Deep Whole-Brain Segmentation\n  for Structure-wise Quality Control", "comments": "Under Review in NeuroImage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian QuickNAT for the automated quality control of\nwhole-brain segmentation on MRI T1 scans. Next to the Bayesian fully\nconvolutional neural network, we also present inherent measures of segmentation\nuncertainty that allow for quality control per brain structure. For estimating\nmodel uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC)\nsamples from the posterior distribution are generated by keeping the dropout\nlayers active at test time. Entropy over the MC samples provides a voxel-wise\nmodel uncertainty map, whereas expectation over the MC predictions provides the\nfinal segmentation. Next to voxel-wise uncertainty, we introduce four metrics\nto quantify structure-wise uncertainty in segmentation for quality control. We\nreport experiments on four out-of-sample datasets comprising of diverse age\nrange, pathology and imaging artifacts. The proposed structure-wise uncertainty\nmetrics are highly correlated with the Dice score estimated with manual\nannotation and therefore present an inherent measure of segmentation quality.\nIn particular, the intersection over union over all the MC samples is a\nsuitable proxy for the Dice score. In addition to quality control at\nscan-level, we propose to incorporate the structure-wise uncertainty as a\nmeasure of confidence to do reliable group analysis on large data repositories.\nWe envisage that the introduced uncertainty metrics would help assess the\nfidelity of automated deep learning based segmentation methods for large-scale\npopulation studies, as they enable automated quality control and group analyses\nin processing large data repositories.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 09:41:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Conjeti", "Sailesh", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1811.09822", "submitter": "Rongcheng Tu", "authors": "Rong-Cheng Tu and Xian-Ling Mao and Bo-Si Feng and Bing-Bing Bian and\n  Yu-shu Ying", "title": "Object Detection based Deep Unsupervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve\nthe performance of unsupervised hashing methods if we can first mine some\nsupervised semantic 'label information' from unlabeled data and then\nincorporate the 'label information' into the training process. Thus, in this\npaper, we propose a novel Object Detection based Deep Unsupervised Hashing\nmethod (ODDUH). Specifically, a pre-trained object detection model is utilized\nto mining supervised 'label information', which is used to guide the learning\nprocess to generate high-quality hash codes.Extensive experiments on two public\ndatasets demonstrate that the proposed method outperforms the state-of-the-art\nunsupervised hashing methods in the image retrieval task.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 12:30:55 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tu", "Rong-Cheng", ""], ["Mao", "Xian-Ling", ""], ["Feng", "Bo-Si", ""], ["Bian", "Bing-Bing", ""], ["Ying", "Yu-shu", ""]]}, {"id": "1811.09831", "submitter": "Shangxi Wu", "authors": "Shangxi Wu, Jitao Sang, Kaiyuan Xu, Jiaming Zhang, Yanfeng Sun, Liping\n  Jing and Jian Yu", "title": "Attention, Please! Adversarial Defense via Attention Rectification and\n  Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study provides a new understanding of the adversarial attack problem by\nexamining the correlation between adversarial attack and visual attention\nchange. In particular, we observed that: (1) images with incomplete attention\nregions are more vulnerable to adversarial attacks; and (2) successful\nadversarial attacks lead to deviated and scattered attention map. Accordingly,\nan attention-based adversarial defense framework is designed to simultaneously\nrectify the attention map for prediction and preserve the attention area\nbetween adversarial and original images. The problem of adding iteratively\nattacked samples is also discussed in the context of visual attention change.\nWe hope the attention-related data analysis and defense solution in this study\nwill shed some light on the mechanism behind the adversarial attack and also\nfacilitate future adversarial defense/attack model design.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 13:14:08 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 09:20:18 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 05:42:30 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wu", "Shangxi", ""], ["Sang", "Jitao", ""], ["Xu", "Kaiyuan", ""], ["Zhang", "Jiaming", ""], ["Sun", "Yanfeng", ""], ["Jing", "Liping", ""], ["Yu", "Jian", ""]]}, {"id": "1811.09834", "submitter": "Yongxi Lu", "authors": "Ziyao Tang, Yongxi Lu and Tara Javidi", "title": "Efficient Video Understanding via Layered Multi Frame-Rate Analysis", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest challenges in the design of a real-time perception system\nfor autonomous driving vehicles and drones is the conflicting requirement of\nsafety (high prediction accuracy) and efficiency. Traditional approaches use a\nsingle frame rate for the entire system. Motivated by the observation that the\nlack of robustness against environmental factors is the major weakness of\ncompact ConvNet architectures, we propose a dual frame-rate system that brings\nin the best of both worlds: A modulator stream that executes an expensive\nmodels robust to environmental factors at a low frame rate to extract slowly\nchanging features describing the environment, and a prediction stream that\nexecutes a light-weight model at real-time to extract transient signals that\ndescribes particularities of the current frame. The advantage of our design is\nvalidated by our extensive empirical study, showing that our solution leads to\nconsistent improvements using a variety of backbone architecture choice and\ninput resolutions. These findings suggest multiple frame-rate systems as a\npromising direction in designing efficient perception for autonomous agents.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 13:43:34 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tang", "Ziyao", ""], ["Lu", "Yongxi", ""], ["Javidi", "Tara", ""]]}, {"id": "1811.09845", "submitter": "Shikhar Sharma", "authors": "Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla\n  El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, Graham W.Taylor", "title": "Tell, Draw, and Repeat: Generating and Modifying Images Based on\n  Continual Linguistic Instruction", "comments": "Accepted at ICCV 2019", "journal-ref": "Proceedings of the 2019 IEEE International Conference on Computer\n  Vision (ICCV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional text-to-image generation is an active area of research, with many\npossible applications. Existing research has primarily focused on generating a\nsingle image from available conditioning information in one step. One practical\nextension beyond one-step generation is a system that generates an image\niteratively, conditioned on ongoing linguistic input or feedback. This is\nsignificantly more challenging than one-step generation tasks, as such a system\nmust understand the contents of its generated images with respect to the\nfeedback history, the current feedback, as well as the interactions among\nconcepts present in the feedback history. In this work, we present a recurrent\nimage generation model which takes into account both the generated output up to\nthe current step as well as all past instructions for generation. We show that\nour model is able to generate the background, add new objects, and apply simple\ntransformations to existing objects. We believe our approach is an important\nstep toward interactive generation. Code and data is available at:\nhttps://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/ .\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 14:42:18 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 17:34:25 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 15:14:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["El-Nouby", "Alaaeldin", ""], ["Sharma", "Shikhar", ""], ["Schulz", "Hannes", ""], ["Hjelm", "Devon", ""], ["Asri", "Layla El", ""], ["Kahou", "Samira Ebrahimi", ""], ["Bengio", "Yoshua", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1811.09847", "submitter": "Bailin Deng", "authors": "Luo Jiang, Juyong Zhang, Bailin Deng", "title": "Robust RGB-D Face Recognition Using Attribute-Aware Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing convolutional neural network (CNN) based face recognition algorithms\ntypically learn a discriminative feature mapping, using a loss function that\nenforces separation of features from different classes and/or aggregation of\nfeatures within the same class. However, they may suffer from bias in the\ntraining data such as uneven sampling density, because they optimize the\nadjacency relationship of the learned features without considering the\nproximity of the underlying faces. Moreover, since they only use facial images\nfor training, the learned feature mapping may not correctly indicate the\nrelationship of other attributes such as gender and ethnicity, which can be\nimportant for some face recognition applications. In this paper, we propose a\nnew CNN-based face recognition approach that incorporates such attributes into\nthe training process. Using an attribute-aware loss function that regularizes\nthe feature mapping using attribute proximity, our approach learns more\ndiscriminative features that are correlated with the attributes. We train our\nface recognition model on a large-scale RGB-D data set with over 100K\nidentities captured under real application conditions. By comparing our\napproach with other methods on a variety of experiments, we demonstrate that\ndepth channel and attribute-aware loss greatly improve the accuracy and\nrobustness of face recognition.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 15:07:12 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 15:13:03 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Jiang", "Luo", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""]]}, {"id": "1811.09855", "submitter": "Zhu Yabin", "authors": "Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang", "title": "FANet: Quality-Aware Feature Aggregation Network for Robust RGB-T\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to perform robust visual tracking in adverse and\nchallenging conditions using complementary visual and thermal infrared data\n(RGBT tracking). We propose a novel deep network architecture called\nqualityaware Feature Aggregation Network (FANet) for robust RGBT tracking.\nUnlike existing RGBT trackers, our FANet aggregates hierarchical deep features\nwithin each modality to handle the challenge of significant appearance changes\ncaused by deformation, low illumination, background clutter and occlusion. In\nparticular, we employ the operations of max pooling to transform these\nhierarchical and multi-resolution features into uniform space with the same\nresolution, and use 1x1 convolution operation to compress feature dimensions to\nachieve more effective hierarchical feature aggregation. To model the\ninteractions between RGB and thermal modalities, we elaborately design an\nadaptive aggregation subnetwork to integrate features from different modalities\nbased on their reliabilities and thus are able to alleviate noise effects\nintroduced by low-quality sources. The whole FANet is trained in an end-to-end\nmanner. Extensive experiments on large-scale benchmark datasets demonstrate the\nhigh-accurate performance against other state-of-the-art RGBT tracking methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 16:54:28 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 07:37:57 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhu", "Yabin", ""], ["Li", "Chenglong", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1811.09862", "submitter": "Maxim Naumov", "authors": "Maxim Naumov and Utku Diril and Jongsoo Park and Benjamin Ray and\n  Jedrzej Jablonski and Andrew Tulloch", "title": "On Periodic Functions as Regularizers for Quantization of Neural\n  Networks", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been successfully used in computer vision and many\nother fields. We propose an unorthodox algorithm for performing quantization of\nthe model parameters. In contrast with popular quantization schemes based on\nthresholds, we use a novel technique based on periodic functions, such as\ncontinuous trigonometric sine or cosine as well as non-continuous hat\nfunctions. We apply these functions component-wise and add the sum over the\nmodel parameters as a regularizer to the model loss during training. The\nfrequency and amplitude hyper-parameters of these functions can be adjusted\nduring training. The regularization pushes the weights into discrete points\nthat can be encoded as integers. We show that using this technique the\nresulting quantized models exhibit the same accuracy as the original ones on\nCIFAR-10 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:24:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Naumov", "Maxim", ""], ["Diril", "Utku", ""], ["Park", "Jongsoo", ""], ["Ray", "Benjamin", ""], ["Jablonski", "Jedrzej", ""], ["Tulloch", "Andrew", ""]]}, {"id": "1811.09885", "submitter": "Linan Zhang", "authors": "Linan Zhang and Hayden Schaeffer", "title": "Forward Stability of ResNet and Its Variants", "comments": "35 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The residual neural network (ResNet) is a popular deep network architecture\nwhich has the ability to obtain high-accuracy results on several image\nprocessing problems. In order to analyze the behavior and structure of ResNet,\nrecent work has been on establishing connections between ResNets and\ncontinuous-time optimal control problems. In this work, we show that the\npost-activation ResNet is related to an optimal control problem with\ndifferential inclusions, and provide continuous-time stability results for the\ndifferential inclusion associated with ResNet. Motivated by the stability\nconditions, we show that alterations of either the architecture or the\noptimization problem can generate variants of ResNet which improve the\ntheoretical stability bounds. In addition, we establish stability bounds for\nthe full (discrete) network associated with two variants of ResNet, in\nparticular, bounds on the growth of the features and a measure of the\nsensitivity of the features with respect to perturbations. These results also\nhelp to show the relationship between the depth, regularization, and stability\nof the feature space. Computational experiments on the proposed variants show\nthat the accuracy of ResNet is preserved and that the accuracy seems to be\nmonotone with respect to the depth and various corruptions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 19:43:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Linan", ""], ["Schaeffer", "Hayden", ""]]}, {"id": "1811.09889", "submitter": "Arun CS Kumar", "authors": "Shefali Srivastava, Abhimanyu Chopra, Arun CS Kumar, Suchendra M.\n  Bhandarkar, Deepak Sharma", "title": "Matching Disparate Image Pairs Using Shape-Aware ConvNets", "comments": "First two authors contributed equally, to Appear in the IEEE Winter\n  Conference on Applications of Computer Vision (WACV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An end-to-end trainable ConvNet architecture, that learns to harness the\npower of shape representation for matching disparate image pairs, is proposed.\nDisparate image pairs are deemed those that exhibit strong affine variations in\nscale, viewpoint and projection parameters accompanied by the presence of\npartial or complete occlusion of objects and extreme variations in ambient\nillumination. Under these challenging conditions, neither local nor global\nfeature-based image matching methods, when used in isolation, have been\nobserved to be effective. The proposed correspondence determination scheme for\nmatching disparate images exploits high-level shape cues that are derived from\nlow-level local feature descriptors, thus combining the best of both worlds. A\ngraph-based representation for the disparate image pair is generated by\nconstructing an affinity matrix that embeds the distances between feature\npoints in two images, thus modeling the correspondence determination problem as\none of graph matching. The eigenspectrum of the affinity matrix, i.e., the\nlearned global shape representation, is then used to further regress the\ntransformation or homography that defines the correspondence between the source\nimage and target image. The proposed scheme is shown to yield state-of-the-art\nresults for both, coarse-level shape matching as well as fine point-wise\ncorrespondence determination.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 20:09:15 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Srivastava", "Shefali", ""], ["Chopra", "Abhimanyu", ""], ["Kumar", "Arun CS", ""], ["Bhandarkar", "Suchendra M.", ""], ["Sharma", "Deepak", ""]]}, {"id": "1811.09897", "submitter": "Seong Jae Hwang", "authors": "Seong Jae Hwang, Zirui Tao, Won Hwa Kim, Vikas Singh", "title": "Conditional Recurrent Flow: Conditional Generation of Longitudinal\n  Samples with Applications to Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models using neural network have opened a door to large-scale\nstudies for various application domains, especially for studies that suffer\nfrom lack of real samples to obtain statistically robust inference. Typically,\nthese generative models would train on existing data to learn the underlying\ndistribution of the measurements (e.g., images) in latent spaces conditioned on\ncovariates (e.g., image labels), and generate independent samples that are\nidentically distributed in the latent space. Such models may work for\ncross-sectional studies, however, they are not suitable to generate data for\nlongitudinal studies that focus on \"progressive\" behavior in a sequence of\ndata. In practice, this is a quite common case in various neuroimaging studies\nwhose goal is to characterize a trajectory of pathologies of a specific disease\neven from early stages. This may be too ambitious especially when the sample\nsize is small (e.g., up to a few hundreds). Motivated from the setup above, we\nseek to develop a conditional generative model for longitudinal data generation\nby designing an invertable neural network. Inspired by recurrent nature of\nlongitudinal data, we propose a novel neural network that incorporates\nrecurrent subnetwork and context gating to include smooth transition in a\nsequence of generated data. Our model is validated on a video sequence dataset\nand a longitudinal AD dataset with various experimental settings for\nqualitative and quantitative evaluations of the generated samples. The results\nwith the AD dataset captures AD specific group differences with sufficiently\ngenerated longitudinal samples that are consistent with existing literature,\nwhich implies a great potential to be applicable to other disease studies.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 21:12:42 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 04:40:49 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Hwang", "Seong Jae", ""], ["Tao", "Zirui", ""], ["Kim", "Won Hwa", ""], ["Singh", "Vikas", ""]]}, {"id": "1811.09908", "submitter": "Chunhua Shen", "authors": "Haokui Zhang, Ying Li, Peng Wang, Yu Liu, Chunhua Shen", "title": "RGB-D Based Action Recognition with Light-weight 3D Convolutional\n  Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from RGB videos, depth data in RGB-D videos provide key\ncomplementary information for tristimulus visual data which potentially could\nachieve accuracy improvement for action recognition. However, most of the\nexisting action recognition models solely using RGB videos limit the\nperformance capacity. Additionally, the state-of-the-art action recognition\nmodels, namely 3D convolutional neural networks (3D-CNNs) contain tremendous\nparameters suffering from computational inefficiency. In this paper, we propose\na series of 3D light-weight architectures for action recognition based on RGB-D\ndata. Compared with conventional 3D-CNN models, the proposed light-weight\n3D-CNNs have considerably less parameters involving lower computation cost,\nwhile it results in favorable recognition performance. Experimental results on\ntwo public benchmark datasets show that our models can approximate or\noutperform the state-of-the-art approaches. Specifically, on the RGB+D-NTU\n(NTU) dataset, we achieve 93.2% and 97.6% for cross-subject and cross-view\nmeasurement, and on the Northwestern-UCLA Multiview Action 3D (N-UCLA) dataset,\nwe achieve 95.5% accuracy of cross-view.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 23:35:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Haokui", ""], ["Li", "Ying", ""], ["Wang", "Peng", ""], ["Liu", "Yu", ""], ["Shen", "Chunhua", ""]]}, {"id": "1811.09910", "submitter": "Wenzheng Chen", "authors": "Wenzheng Chen and Simon Daneau and Fahim Mannan and Felix Heide", "title": "Steady-state Non-Line-of-Sight Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional intensity cameras recover objects in the direct line-of-sight of\nthe camera, whereas occluded scene parts are considered lost in this process.\nNon-line-of-sight imaging (NLOS) aims at recovering these occluded objects by\nanalyzing their indirect reflections on visible scene surfaces. Existing NLOS\nmethods temporally probe the indirect light transport to unmix light paths\nbased on their travel time, which mandates specialized instrumentation that\nsuffers from low photon efficiency, high cost, and mechanical scanning. We\ndepart from temporal probing and demonstrate steady-state NLOS imaging using\nconventional intensity sensors and continuous illumination. Instead of assuming\nperfectly isotropic scattering, the proposed method exploits directionality in\nthe hidden surface reflectance, resulting in (small) spatial variation of their\nindirect reflections for varying illumination. To tackle the shape-dependence\nof these variations, we propose a trainable architecture which learns to map\ndiffuse indirect reflections to scene reflectance using only synthetic training\ndata. Relying on consumer color image sensors, with high fill factor, high\nquantum efficiency and low read-out noise, we demonstrate high-fidelity color\nNLOS imaging for scene configurations tackled before with picosecond time\nresolution.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 23:39:54 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 02:09:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Wenzheng", ""], ["Daneau", "Simon", ""], ["Mannan", "Fahim", ""], ["Heide", "Felix", ""]]}, {"id": "1811.09916", "submitter": "Liangjian Chen", "authors": "Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue,\n  Xiaohui Xie, Yen-Yu Lin, Wei Fan", "title": "Generating Realistic Training Images Based on Tonality-Alignment\n  Generative Adversarial Networks for Hand Pose Estimation", "comments": null, "journal-ref": "30th British Machine Vision Conference 2019, BMVC 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from a monocular RGB image is an important but\nchallenging task. The main factor affecting its performance is the lack of a\nsufficiently large training dataset with accurate hand-keypoint annotations. In\nthis work, we circumvent this problem by proposing an effective method for\ngenerating realistic hand poses and show that state-of-the-art algorithms for\nhand pose estimation can be greatly improved by utilizing the generated hand\nposes as training data. Specifically, we first adopt an augmented reality (AR)\nsimulator to synthesize hand poses with accurate hand-keypoint labels. Although\nthe synthetic hand poses come with precise joint labels, eliminating the need\nof manual annotations, they look unnatural and are not the ideal training data.\nTo produce more realistic hand poses, we propose to blend a synthetic hand pose\nwith a real background, such as arms and sleeves. To this end, we develop\ntonality-alignment generative adversarial networks (TAGANs), which align the\ntonality and color distributions between synthetic hand poses and real\nbackgrounds, and can generate high quality hand poses. We evaluate TAGAN on\nthree benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With\nthe aid of the synthesized poses, our method performs favorably against the\nstate-of-the-arts in both 2D and 3D hand pose estimations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:18:13 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 02:41:33 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 10:21:09 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 02:09:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chen", "Liangjian", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Tang", "Hui", ""], ["Xue", "Yufan", ""], ["Xie", "Xiaohui", ""], ["Lin", "Yen-Yu", ""], ["Fan", "Wei", ""]]}, {"id": "1811.09928", "submitter": "Dong Liang", "authors": "Dong Liang, Rui Wang, Xiaowei Tian, Cong Zou", "title": "PCGAN: Partition-Controlled Human Image Generation", "comments": "AAAI 2019 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human image generation is a very challenging task since it is affected by\nmany factors. Many human image generation methods focus on generating human\nimages conditioned on a given pose, while the generated backgrounds are often\nblurred.In this paper,we propose a novel Partition-Controlled GAN to generate\nhuman images according to target pose and background. Firstly, human poses in\nthe given images are extracted, and foreground/background are partitioned for\nfurther use. Secondly, we extract and fuse appearance features, pose features\nand background features to generate the desired images. Experiments on\nMarket-1501 and DeepFashion datasets show that our model not only generates\nrealistic human images but also produce the human pose and background as we\nwant. Extensive experiments on COCO and LIP datasets indicate the potential of\nour method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 02:07:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Liang", "Dong", ""], ["Wang", "Rui", ""], ["Tian", "Xiaowei", ""], ["Zou", "Cong", ""]]}, {"id": "1811.09935", "submitter": "Fei Xue", "authors": "Fei Xue, Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, Hongbin Zha", "title": "Guided Feature Selection for Deep Visual Odometry", "comments": "Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end visual odometry architecture with guided\nfeature selection based on deep convolutional recurrent neural networks.\nDifferent from current monocular visual odometry methods, our approach is\nestablished on the intuition that features contribute discriminately to\ndifferent motion patterns. Specifically, we propose a dual-branch recurrent\nnetwork to learn the rotation and translation separately by leveraging current\nConvolutional Neural Network (CNN) for feature representation and Recurrent\nNeural Network (RNN) for image sequence reasoning. To enhance the ability of\nfeature selection, we further introduce an effective context-aware guidance\nmechanism to force each branch to distill related information for specific\nmotion pattern explicitly. Experiments demonstrate that on the prevalent KITTI\nand ICL_NUIM benchmarks, our method outperforms current state-of-the-art model-\nand learning-based methods for both decoupled and joint camera pose recovery.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 03:20:37 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Xue", "Fei", ""], ["Wang", "Qiuyuan", ""], ["Wang", "Xin", ""], ["Dong", "Wei", ""], ["Wang", "Junqiu", ""], ["Zha", "Hongbin", ""]]}, {"id": "1811.09938", "submitter": "Qianhao Zhang", "authors": "Zhang Qianhao, Alexander Mai, Joseph Menke, Allen Yang", "title": "Loop Closure Detection with RGB-D Feature Pyramid Siamese Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual Simultaneous Localization And Mapping (SLAM), detecting loop\nclosures has been an important but difficult task. Currently, most solutions\nare based on the bag-of-words approach. Yet the possibility of deep neural\nnetwork application to this task has not been fully explored due to the lack of\nappropriate architecture design and of sufficient training data. In this paper\nwe demonstrate the applicability of deep neural networks by addressing both\nissues. Specifically we show that a feature pyramid Siamese neural network can\nachieve state-of-the-art performance on pairwise loop closure detection. The\nnetwork is trained and tested on large-scale RGB-D datasets with a novel\nautomatic loop closure labeling algorithm. Each image pair is labelled by how\nmuch the images overlap, allowing loop closure to be computed directly rather\nthan by labor intensive manual labeling. We present an algorithm to adopt any\nlarge-scale generic RGB-D dataset for use in training deep loop-closure\nnetworks. We show for the first time that deep neural networks are capable of\ndetecting loop closures, and we provide a method for generating large-scale\ndatasets for use in evaluating and training loop closure detectors.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 03:39:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Qianhao", "Zhang", ""], ["Mai", "Alexander", ""], ["Menke", "Joseph", ""], ["Yang", "Allen", ""]]}, {"id": "1811.09950", "submitter": "Edward Chou Mr.", "authors": "Edward Chou, Matthew Tan, Cherry Zou, Michelle Guo, Albert Haque,\n  Arnold Milstein, Li Fei-Fei", "title": "Privacy-Preserving Action Recognition for Smart Hospitals using\n  Low-Resolution Depth Images", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/154", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-vision hospital systems can greatly assist healthcare workers and\nimprove medical facility treatment, but often face patient resistance due to\nthe perceived intrusiveness and violation of privacy associated with visual\nsurveillance. We downsample video frames to extremely low resolutions to\ndegrade private information from surveillance videos. We measure the amount of\nactivity-recognition information retained in low resolution depth images, and\nalso apply a privately-trained DCSCN super-resolution model to enhance the\nutility of our images. We implement our techniques with two actual\nhealthcare-surveillance scenarios, hand-hygiene compliance and ICU\nactivity-logging, and show that our privacy-preserving techniques preserve\nenough information for realistic healthcare tasks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 05:55:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Chou", "Edward", ""], ["Tan", "Matthew", ""], ["Zou", "Cherry", ""], ["Guo", "Michelle", ""], ["Haque", "Albert", ""], ["Milstein", "Arnold", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1811.09961", "submitter": "Bo Pang", "authors": "Bo Pang, Kaiwen Zha, Hanwen Cao, Chen Shi, Cewu Lu", "title": "Deep RNN Framework for Visual Sequential Applications", "comments": "10 pages, 7 figures, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting temporal and representation features efficiently plays a pivotal\nrole in understanding visual sequence information. To deal with this, we\npropose a new recurrent neural framework that can be stacked deep effectively.\nThere are mainly two novel designs in our deep RNN framework: one is a new RNN\nmodule called Context Bridge Module (CBM) which splits the information flowing\nalong the sequence (temporal direction) and along depth (spatial representation\ndirection), making it easier to train when building deep by balancing these two\ndirections; the other is the Overlap Coherence Training Scheme that reduces the\ntraining complexity for long visual sequential tasks on account of the\nlimitation of computing resources.\n  We provide empirical evidence to show that our deep RNN framework is easy to\noptimize and can gain accuracy from the increased depth on several visual\nsequence problems. On these tasks, we evaluate our deep RNN framework with 15\nlayers, 7* than conventional RNN networks, but it is still easy to train. Our\ndeep framework achieves more than 11% relative improvements over shallow RNN\nmodels on Kinetics, UCF-101, and HMDB-51 for video classification. For\nauxiliary annotation, after replacing the shallow RNN part of Polygon-RNN with\nour 15-layer deep CBM, the performance improves by 14.7%. For video future\nprediction, our deep RNN improves the state-of-the-art shallow model's\nperformance by 2.4% on PSNR and SSIM. The code and trained models are published\naccompanied by this paper: https://github.com/BoPang1996/Deep-RNN-Framework.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 06:34:29 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 08:04:56 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 09:34:28 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 03:55:16 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Pang", "Bo", ""], ["Zha", "Kaiwen", ""], ["Cao", "Hanwen", ""], ["Shi", "Chen", ""], ["Lu", "Cewu", ""]]}, {"id": "1811.09962", "submitter": "Zhipeng Cai", "authors": "Zhipeng Cai, Tat-Jun Chin, Alvaro Parra Bustos, Konrad Schindler", "title": "Practical optimal registration of terrestrial LiDAR scan pairs", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 147,\n  January 2019, Pages 118-131", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is a fundamental problem in 3D scanning. In this\npaper, we address the frequent special case of registering terrestrial LiDAR\nscans (or, more generally, levelled point clouds). Many current solutions still\nrely on the Iterative Closest Point (ICP) method or other heuristic procedures,\nwhich require good initializations to succeed and/or provide no guarantees of\nsuccess. On the other hand, exact or optimal registration algorithms can\ncompute the best possible solution without requiring initializations; however,\nthey are currently too slow to be practical in realistic applications.\n  Existing optimal approaches ignore the fact that in routine use the relative\nrotations between scans are constrained to the azimuth, via the built-in level\ncompensation in LiDAR scanners. We propose a novel, optimal and computationally\nefficient registration method for this 4DOF scenario. Our approach operates on\ncandidate 3D keypoint correspondences, and contains two main steps: (1) a\ndeterministic selection scheme that significantly reduces the candidate\ncorrespondence set in a way that is guaranteed to preserve the optimal\nsolution; and (2) a fast branch-and-bound (BnB) algorithm with a novel\npolynomial-time subroutine for 1D rotation search, that quickly finds the\noptimal alignment for the reduced set. We demonstrate the practicality of our\nmethod on realistic point clouds from multiple LiDAR surveys.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 06:36:28 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:45:23 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 04:45:42 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Cai", "Zhipeng", ""], ["Chin", "Tat-Jun", ""], ["Bustos", "Alvaro Parra", ""], ["Schindler", "Konrad", ""]]}, {"id": "1811.09971", "submitter": "Bo Jiang", "authors": "Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang", "title": "Graph Learning-Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph Convolutional Neural Networks (graph CNNs) have been widely\nused for graph data representation and semi-supervised learning tasks. However,\nexisting graph CNNs generally use a fixed graph which may be not optimal for\nsemi-supervised learning tasks. In this paper, we propose a novel Graph\nLearning-Convolutional Network (GLCN) for graph data representation and\nsemi-supervised learning. The aim of GLCN is to learn an optimal graph\nstructure that best serves graph CNNs for semi-supervised learning by\nintegrating both graph learning and graph convolution together in a unified\nnetwork architecture. The main advantage is that in GLCN, both given labels and\nthe estimated labels are incorporated and thus can provide useful 'weakly'\nsupervised information to refine (or learn) the graph construction and also to\nfacilitate the graph convolution operation in GLCN for unknown label\nestimation. Experimental results on seven benchmarks demonstrate that GLCN\nsignificantly outperforms state-of-the-art traditional fixed structure based\ngraph CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 08:21:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Jiang", "Bo", ""], ["Zhang", "Ziyan", ""], ["Lin", "Doudou", ""], ["Tang", "Jin", ""]]}, {"id": "1811.09974", "submitter": "Yanghao Li", "authors": "Yanghao Li, Sijie Song, Yuqi Li, Jiaying Liu", "title": "Temporal Bilinear Networks for Video Action Recognition", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal modeling in videos is a fundamental yet challenging problem in\ncomputer vision. In this paper, we propose a novel Temporal Bilinear (TB) model\nto capture the temporal pairwise feature interactions between adjacent frames.\nCompared with some existing temporal methods which are limited in linear\ntransformations, our TB model considers explicit quadratic bilinear\ntransformations in the temporal domain for motion evolution and sequential\nrelation modeling. We further leverage the factorized bilinear model in linear\ncomplexity and a bottleneck network design to build our TB blocks, which also\nconstrains the parameters and computation cost. We consider two schemes in\nterms of the incorporation of TB blocks and the original 2D spatial\nconvolutions, namely wide and deep Temporal Bilinear Networks (TBN). Finally,\nwe perform experiments on several widely adopted datasets including Kinetics,\nUCF101 and HMDB51. The effectiveness of our TBNs is validated by comprehensive\nablation analyses and comparisons with various state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 09:16:37 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Yanghao", ""], ["Song", "Sijie", ""], ["Li", "Yuqi", ""], ["Liu", "Jiaying", ""]]}, {"id": "1811.09982", "submitter": "Battista Biggio", "authors": "Battista Biggio, Ignazio Pillai, Samuel Rota Bul\\`o, Davide Ariu,\n  Marcello Pelillo, Fabio Roli", "title": "Is Data Clustering in Adversarial Settings Secure?", "comments": null, "journal-ref": "Proceedings of the 2013 ACM Workshop on Artificial Intelligence\n  and Security, AISec '13, pages 87-98, New York, NY, USA, 2013. ACM", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms have been increasingly adopted in security applications\nto spot dangerous or illicit activities. However, they have not been originally\ndevised to deal with deliberate attack attempts that may aim to subvert the\nclustering process itself. Whether clustering can be safely adopted in such\nsettings remains thus questionable. In this work we propose a general framework\nthat allows one to identify potential attacks against clustering algorithms,\nand to evaluate their impact, by making specific assumptions on the adversary's\ngoal, knowledge of the attacked system, and capabilities of manipulating the\ninput data. We show that an attacker may significantly poison the whole\nclustering process by adding a relatively small percentage of attack samples to\nthe input data, and that some attack samples may be obfuscated to be hidden\nwithin some existing clusters. We present a case study on single-linkage\nhierarchical clustering, and report experiments on clustering of malware\nsamples and handwritten digits.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 10:21:59 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Biggio", "Battista", ""], ["Pillai", "Ignazio", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Ariu", "Davide", ""], ["Pelillo", "Marcello", ""], ["Roli", "Fabio", ""]]}, {"id": "1811.09986", "submitter": "Shih-Yao Lin", "authors": "Shih-Yao Lin, Yen-Yu Lin, Chu-Song Chen, Yi-Ping Hung", "title": "Learning Conditional Random Fields with Augmented Observations for\n  Partially Observed Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at recognizing partially observed human actions in videos.\nAction videos acquired in uncontrolled environments often contain corrupt\nframes, which make actions partially observed. Furthermore, these frames can\nlast for arbitrary lengths of time and appear irregularly. They are\ninconsistent with training data and degrade the performance of pre-trained\naction recognition systems. We present an approach to address this issue. For\neach training and testing actions, we divide it into segments and explore the\nmutual dependency between temporal segments. This property states that the\nsimilarity of two actions at one segment often implies their similarity at\nanother. We augment each segment with extra alternatives retrieved from\ntraining data. The augmentation algorithm is designed in a way where a few\nalternatives are good enough to replace the original segment where corrupt\nframes occur. Our approach is developed upon hidden conditional random fields\nand leverages the flexibility of hidden variables for uncertainty handling. It\nturns out that our approach integrates corrupt segment detection and\nalternative selection into the process of prediction, and can recognize\npartially observed actions more accurately. It is evaluated on both fully\nobserved actions and partially observed ones with either synthetic or real\ncorrupt frames. The experimental results manifest its general applicability and\nsuperior performance, especially when corrupt frames are present in the action\nvideos.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 10:59:19 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 05:38:29 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 09:35:07 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lin", "Shih-Yao", ""], ["Lin", "Yen-Yu", ""], ["Chen", "Chu-Song", ""], ["Hung", "Yi-Ping", ""]]}, {"id": "1811.09998", "submitter": "Jia Li", "authors": "Shiming Ge, Shengwei Zhao, Chenyu Li, Jia Li", "title": "Low-resolution Face Recognition in the Wild via Selective Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2883743", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, the deployment of face recognition models in the wild needs to\nidentify low-resolution faces with extremely low computational cost. To address\nthis problem, a feasible solution is compressing a complex face model to\nachieve higher speed and lower memory at the cost of minimal performance drop.\nInspired by that, this paper proposes a learning approach to recognize\nlow-resolution faces via selective knowledge distillation. In this approach, a\ntwo-stream convolutional neural network (CNN) is first initialized to recognize\nhigh-resolution faces and resolution-degraded faces with a teacher stream and a\nstudent stream, respectively. The teacher stream is represented by a complex\nCNN for high-accuracy recognition, and the student stream is represented by a\nmuch simpler CNN for low-complexity recognition. To avoid significant\nperformance drop at the student stream, we then selectively distil the most\ninformative facial features from the teacher stream by solving a sparse graph\noptimization problem, which are then used to regularize the fine-tuning process\nof the student stream. In this way, the student stream is actually trained by\nsimultaneously handling two tasks with limited computational resources:\napproximating the most informative facial cues via feature regression, and\nrecovering the missing facial cues via low-resolution face classification.\nExperimental results show that the student stream performs impressively in\nrecognizing low-resolution faces and costs only 0.15MB memory and runs at 418\nfaces per second on CPU and 9,433 faces per second on GPU.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 12:46:21 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 01:43:51 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Ge", "Shiming", ""], ["Zhao", "Shengwei", ""], ["Li", "Chenyu", ""], ["Li", "Jia", ""]]}, {"id": "1811.10002", "submitter": "Hwann-Tzong Chen", "authors": "Shou-Yao Roy Tseng, Hwann-Tzong Chen, Shao-Heng Tai, Tyng-Luh Liu", "title": "Non-local RoI for Cross-Object Perception", "comments": "NIPS 2018 Workshop on Relational Representation Learning. arXiv admin\n  note: substantial text overlap with arXiv:1807.05361", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic and flexible module that encodes region proposals by\nboth their intrinsic features and the extrinsic correlations to the others. The\nproposed non-local region of interest (NL-RoI) can be seamlessly adapted into\ndifferent generalized R-CNN architectures to better address various perception\ntasks. Observe that existing techniques from R-CNN treat RoIs independently and\nperform the prediction solely based on image features within each region\nproposal. However, the pairwise relationships between proposals could further\nprovide useful information for detection and segmentation. NL-RoI is thus\nformulated to enrich each RoI representation with the information from all\nother RoIs, and yield a simple, low-cost, yet effective module for region-based\nconvolutional networks. Our experimental results show that NL-RoI can improve\nthe performance of Faster/Mask R-CNN for object detection and instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 13:05:49 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tseng", "Shou-Yao Roy", ""], ["Chen", "Hwann-Tzong", ""], ["Tai", "Shao-Heng", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1811.10003", "submitter": "Dinh Nguyen Van", "authors": "Dinh NguyenVan, Shijian Lu, Shangxuan Tian, Nizar Ouarti, Mounir\n  Mokhtari", "title": "A pooling based scene text proposal technique for scene text reading in\n  the wild", "comments": "The article has 34 pages with nine figures, six tables. It has been\n  accepted to publish in Journal of Pattern Recognition", "journal-ref": "Journal of Pattern Recognition, Volume 87, March 2019, Pages\n  118-129, ISSN: 0031-3203", "doi": "10.1016/j.patcog.2018.10.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic reading texts in scenes has attracted increasing interest in recent\nyears as texts often carry rich semantic information that is useful for scene\nunderstanding. In this paper, we propose a novel scene text proposal technique\naiming for accurate reading texts in scenes. Inspired by the pooling layer in\nthe deep neural network architecture, a pooling based scene text proposal\ntechnique is developed. A novel score function is designed which exploits the\nhistogram of oriented gradients and is capable of ranking the proposals\naccording to their probabilities of being text. An end-to-end scene text\nreading system has also been developed by incorporating the proposed scene text\nproposal technique where false alarms elimination and words recognition are\nperformed simultaneously. Extensive experiments over several public datasets\nshow that the proposed technique can handle multi-orientation and\nmulti-language scene texts and obtains outstanding proposal performance. The\ndeveloped end-to-end systems also achieve very competitive scene text spotting\nand reading performance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 13:14:53 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["NguyenVan", "Dinh", ""], ["Lu", "Shijian", ""], ["Tian", "Shangxuan", ""], ["Ouarti", "Nizar", ""], ["Mokhtari", "Mounir", ""]]}, {"id": "1811.10004", "submitter": "Jia Li", "authors": "Jia Li, Daowei Li, Kui Fu, Long Xu", "title": "Visual Attention on the Sun: What Do Existing Models Actually Predict?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention prediction is a classic problem that seems to be well\naddressed in the deep learning era. One compelling concern, however, gradually\narise along with the rapidly growing performance scores over existing visual\nattention datasets: do existing deep models really capture the inherent\nmechanism of human visual attention? To address this concern, this paper\nproposes a new dataset, named VASUN, that records the free-viewing human\nattention on solar images. Different from previous datasets, images in VASUN\ncontain many irregular visual patterns that existing deep models have never\nseen. By benchmarking existing models on VASUN, we find the performances of\nmany state-of-the-art deep models drop remarkably, while many classic shallow\nmodels perform impressively. From these results, we find that the significant\nperformance advance of existing deep attention models may come from their\ncapabilities of memorizing and predicting the occurrence of some specific\nvisual patterns other than learning the inherent mechanism of human visual\nattention. In addition, we also train several baseline models on VASUN to\ndemonstrate the feasibility and key issues of predicting visual attention on\nthe sun. These baseline models, together with the proposed dataset, can be used\nto revisit the problem of visual attention prediction from a novel perspective\nthat are complementary to existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 13:15:37 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Jia", ""], ["Li", "Daowei", ""], ["Fu", "Kui", ""], ["Xu", "Long", ""]]}, {"id": "1811.10014", "submitter": "Chenglong Li", "authors": "Xiao Wang, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang and Bin Luo", "title": "Describe and Attend to Track: Learning Natural Language guided\n  Structural Representation and Visual Attention for Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tracking-by-detection framework requires a set of positive and negative\ntraining samples to learn robust tracking models for precise localization of\ntarget objects. However, existing tracking models mostly treat different\nsamples independently while ignores the relationship information among them. In\nthis paper, we propose a novel structure-aware deep neural network to overcome\nsuch limitations. In particular, we construct a graph to represent the pairwise\nrelationships among training samples, and additionally take the natural\nlanguage as the supervised information to learn both feature representations\nand classifiers robustly. To refine the states of the target and re-track the\ntarget when it is back to view from heavy occlusion and out of view, we\nelaborately design a novel subnetwork to learn the target-driven visual\nattentions from the guidance of both visual and natural language cues.\nExtensive experiments on five tracking benchmark datasets validated the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 14:00:05 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 02:54:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wang", "Xiao", ""], ["Li", "Chenglong", ""], ["Yang", "Rui", ""], ["Zhang", "Tianzhu", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1811.10016", "submitter": "Aditya Arun", "authors": "Aditya Arun, C.V. Jawahar, M. Pawan Kumar", "title": "Dissimilarity Coefficient based Weakly Supervised Object Detection", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of weakly supervised object detection, where the\ntraining samples are annotated using only image-level labels that indicate the\npresence or absence of an object category. In order to model the uncertainty in\nthe location of the objects, we employ a dissimilarity coefficient based\nprobabilistic learning objective. The learning objective minimizes the\ndifference between an annotation agnostic prediction distribution and an\nannotation aware conditional distribution. The main computational challenge is\nthe complex nature of the conditional distribution, which consists of terms\nover hundreds or thousands of variables. The complexity of the conditional\ndistribution rules out the possibility of explicitly modeling it. Instead, we\nexploit the fact that deep learning frameworks rely on stochastic optimization.\nThis allows us to use a state of the art discrete generative model that can\nprovide annotation consistent samples from the conditional distribution.\nExtensive experiments on PASCAL VOC 2007 and 2012 data sets demonstrate the\nefficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 14:09:57 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Arun", "Aditya", ""], ["Jawahar", "C. V.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1811.10020", "submitter": "Dongdong Zeng", "authors": "Dongdong Zeng, Xiang Chen, Ming Zhu, Michael Goesele, Arjan Kuijper", "title": "Background Subtraction with Real-time Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fast foreground object extraction is very important for object\ntracking and recognition in video surveillance. Although many background\nsubtraction (BGS) methods have been proposed in the recent past, it is still\nregarded as a tough problem due to the variety of challenging situations that\noccur in real-world scenarios. In this paper, we explore this problem from a\nnew perspective and propose a novel background subtraction framework with\nreal-time semantic segmentation (RTSS). Our proposed framework consists of two\ncomponents, a traditional BGS segmenter $\\mathcal{B}$ and a real-time semantic\nsegmenter $\\mathcal{S}$. The BGS segmenter $\\mathcal{B}$ aims to construct\nbackground models and segments foreground objects. The real-time semantic\nsegmenter $\\mathcal{S}$ is used to refine the foreground segmentation outputs\nas feedbacks for improving the model updating accuracy. $\\mathcal{B}$ and\n$\\mathcal{S}$ work in parallel on two threads. For each input frame $I_t$, the\nBGS segmenter $\\mathcal{B}$ computes a preliminary foreground/background\n(FG/BG) mask $B_t$. At the same time, the real-time semantic segmenter\n$\\mathcal{S}$ extracts the object-level semantics ${S}_t$. Then, some specific\nrules are applied on ${B}_t$ and ${S}_t$ to generate the final detection\n${D}_t$. Finally, the refined FG/BG mask ${D}_t$ is fed back to update the\nbackground model. Comprehensive experiments evaluated on the CDnet 2014 dataset\ndemonstrate that our proposed method achieves state-of-the-art performance\namong all unsupervised background subtraction methods while operating at\nreal-time, and even performs better than some deep learning based supervised\nalgorithms. In addition, our proposed framework is very flexible and has the\npotential for generalization.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 14:20:44 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 15:20:04 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zeng", "Dongdong", ""], ["Chen", "Xiang", ""], ["Zhu", "Ming", ""], ["Goesele", "Michael", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1811.10026", "submitter": "Yaochen Li", "authors": "Yaochen Li, Ying Liu, Rui Sun, Rui Guo, Li Zhu and Yong Qi", "title": "Multi-view Point Cloud Registration with Adaptive Convergence Threshold\n  and its Application on 3D Model Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view point cloud registration is a hot topic in the communities of\nmultimedia technology and artificial intelligence (AI). In this paper, we\npropose a framework to reconstruct the 3D models by the multi-view point cloud\nregistration algorithm with adaptive convergence threshold, and subsequently\napply it to 3D model retrieval. The iterative closest point (ICP) algorithm is\nimplemented combining with the motion average algorithm for the registration of\nmulti-view point clouds. After the registration process, we design applications\nfor 3D model retrieval. The geometric saliency map is computed based on the\nvertex curvature. The test facial triangle is then generated based on the\nsaliency map, which is applied to compare with the standard facial triangle.\nThe face and non-face models are then discriminated. The experiments and\ncomparisons prove the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 14:51:03 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 02:05:58 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Li", "Yaochen", ""], ["Liu", "Ying", ""], ["Sun", "Rui", ""], ["Guo", "Rui", ""], ["Zhu", "Li", ""], ["Qi", "Yong", ""]]}, {"id": "1811.10048", "submitter": "Antoine Fond", "authors": "Antoine Fond, Marie-Odile Berger, Gilles Simon", "title": "Joint Facade Registration and Segmentation for Urban Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient approach for solving jointly facade\nregistration and semantic segmentation. Progress in facade detection and\nrecognition enable good initialization for the registration of a reference\nfacade to a newly acquired target image. We propose here to rely on semantic\nsegmentation to improve the accuracy of that initial registration.\nSimultaneously we aim to improve the quality of the semantic segmentation\nthrough the registration. These two problems are jointly solved in a\nExpectation-Maximization framework. We especially introduce a bayesian model\nthat use prior semantic segmentation as well as geometric structure of the\nfacade reference modeled by $L_p$ Gaussian Mixtures. We show the advantages of\nour method in term of robustness to clutter and change of illumination on urban\nimages from various database.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 16:24:00 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 16:09:39 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Fond", "Antoine", ""], ["Berger", "Marie-Odile", ""], ["Simon", "Gilles", ""]]}, {"id": "1811.10052", "submitter": "Alexander Lundervold", "authors": "Alexander Selvikv{\\aa}g Lundervold and Arvid Lundervold", "title": "An overview of deep learning in medical imaging focusing on MRI", "comments": "Minor updates. Close to the version published in Zeitschrift f\\\"ur\n  Medizinische Physik (Available online 13 December 2018)", "journal-ref": "Zeitschrift f\\\"ur Medizinische Physik, Volume 29, Issue 2, May\n  2019", "doi": "10.1016/j.zemedi.2018.11.002", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What has happened in machine learning lately, and what does it mean for the\nfuture of medical image analysis? Machine learning has witnessed a tremendous\namount of attention over the last few years. The current boom started around\n2009 when so-called deep artificial neural networks began outperforming other\nestablished models on a number of important benchmarks. Deep neural networks\nare now the state-of-the-art machine learning models across a variety of areas,\nfrom image analysis to natural language processing, and widely deployed in\nacademia and industry. These developments have a huge potential for medical\nimaging technology, medical data analysis, medical diagnostics and healthcare\nin general, slowly being realized. We provide a short overview of recent\nadvances and some associated challenges in machine learning applied to medical\nimage processing and image analysis. As this has become a very broad and fast\nexpanding field we will not survey the entire landscape of applications, but\nput particular focus on deep learning in MRI.\n  Our aim is threefold: (i) give a brief introduction to deep learning with\npointers to core references; (ii) indicate how deep learning has been applied\nto the entire MRI processing chain, from acquisition to image retrieval, from\nsegmentation to disease prediction; (iii) provide a starting point for people\ninterested in experimenting and perhaps contributing to the field of machine\nlearning for medical imaging by pointing out good educational resources,\nstate-of-the-art open-source code, and interesting sources of data and problems\nrelated medical imaging.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 16:40:42 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 09:58:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lundervold", "Alexander Selvikv\u00e5g", ""], ["Lundervold", "Arvid", ""]]}, {"id": "1811.10066", "submitter": "Andrey Kuehlkamp", "authors": "Andrey Kuehlkamp and Kevin Bowyer", "title": "Predicting Gender from Iris Texture May Be Harder Than It Seems", "comments": "Paper accepted for publication at the IEEE Winter Conference on\n  Applications of Computer Vision - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting gender from iris images has been reported by several researchers\nas an application of machine learning in biometrics. Recent works on this topic\nhave suggested that the preponderance of the gender cues is located in the\nperiocular region rather than in the iris texture itself. This paper focuses on\nteasing out whether the information for gender prediction is in the texture of\nthe iris stroma, the periocular region, or both. We present a larger dataset\nfor gender from iris, and evaluate gender prediction accuracy using linear SVM\nand CNN, comparing hand-crafted and deep features. We use probabilistic\nocclusion masking to gain insight on the problem. Results suggest the\ndiscriminative power of the iris texture for gender is weaker than previously\nthought, and that the gender-related information is primarily in the periocular\nregion.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 18:23:21 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kuehlkamp", "Andrey", ""], ["Bowyer", "Kevin", ""]]}, {"id": "1811.10068", "submitter": "Andrey Kuehlkamp", "authors": "Andrey Kuehlkamp, Allan Pinto, Anderson Rocha, Kevin Bowyer, Adam\n  Czajka", "title": "Ensemble of Multi-View Learning Classifiers for Cross-Domain Iris\n  Presentation Attack Detection", "comments": "IEEE Transactions on Information Forensics and Security (Early\n  Access), 2018", "journal-ref": null, "doi": "10.1109/TIFS.2018.2878542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of large-scale iris recognition systems around the world has\nbrought to light the importance of detecting presentation attack images\n(textured contact lenses and printouts). This work presents a new approach in\niris Presentation Attack Detection (PAD), by exploring combinations of\nConvolutional Neural Networks (CNNs) and transformed input spaces through\nbinarized statistical image features (BSIF). Our method combines lightweight\nCNNs to classify multiple BSIF views of the input image. Following explorations\non complementary input spaces leading to more discriminative features to detect\npresentation attacks, we also propose an algorithm to select the best (and most\ndiscriminative) predictors for the task at hand.An ensemble of predictors makes\nuse of their expected individual performances to aggregate their results into a\nfinal prediction. Results show that this technique improves on the current\nstate of the art in iris PAD, outperforming the winner of LivDet-Iris2017\ncompetition both for intra- and cross-dataset scenarios, and illustrating the\nvery difficult nature of the cross-dataset scenario.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 18:32:35 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kuehlkamp", "Andrey", ""], ["Pinto", "Allan", ""], ["Rocha", "Anderson", ""], ["Bowyer", "Kevin", ""], ["Czajka", "Adam", ""]]}, {"id": "1811.10080", "submitter": "Mingda Zhang", "authors": "Keren Ye, Mingda Zhang, Wei Li, Danfeng Qin, Adriana Kovashka, Jesse\n  Berent", "title": "Learning to discover and localize visual objects with open vocabulary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the cost of obtaining accurate bounding boxes for training\ntoday's state-of-the-art object detection models, recent weakly supervised\ndetection work has proposed techniques to learn from image-level labels.\nHowever, requiring discrete image-level labels is both restrictive and\nsuboptimal. Real-world \"supervision\" usually consists of more unstructured\ntext, such as captions. In this work we learn association maps between images\nand captions. We then use a novel objectness criterion to rank the resulting\ncandidate boxes, such that high-ranking boxes have strong gradients along all\nedges. Thus, we can detect objects beyond a fixed object category vocabulary,\nif those objects are frequent and distinctive enough. We show that our\nobjectness criterion improves the proposed bounding boxes in relation to prior\nweakly supervised detection methods. Further, we show encouraging results on\nobject detection from image-level captions only.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 19:55:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ye", "Keren", ""], ["Zhang", "Mingda", ""], ["Li", "Wei", ""], ["Qin", "Danfeng", ""], ["Kovashka", "Adriana", ""], ["Berent", "Jesse", ""]]}, {"id": "1811.10092", "submitter": "Xin Wang", "authors": "Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen,\n  Yuan-Fang Wang, William Yang Wang, Lei Zhang", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning\n  for Vision-Language Navigation", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language navigation (VLN) is the task of navigating an embodied agent\nto carry out natural language instructions inside real 3D environments. In this\npaper, we study how to address three critical challenges for this task: the\ncross-modal grounding, the ill-posed feedback, and the generalization problems.\nFirst, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that\nenforces cross-modal grounding both locally and globally via reinforcement\nlearning (RL). Particularly, a matching critic is used to provide an intrinsic\nreward to encourage global matching between instructions and trajectories, and\na reasoning navigator is employed to perform cross-modal grounding in the local\nvisual scene. Evaluation on a VLN benchmark dataset shows that our RCM model\nsignificantly outperforms previous methods by 10% on SPL and achieves the new\nstate-of-the-art performance. To improve the generalizability of the learned\npolicy, we further introduce a Self-Supervised Imitation Learning (SIL) method\nto explore unseen environments by imitating its own past, good decisions. We\ndemonstrate that SIL can approximate a better and more efficient policy, which\ntremendously minimizes the success rate performance gap between seen and unseen\nenvironments (from 30.7% to 11.7%).\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 20:49:58 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 05:43:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Xin", ""], ["Huang", "Qiuyuan", ""], ["Celikyilmaz", "Asli", ""], ["Gao", "Jianfeng", ""], ["Shen", "Dinghan", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""], ["Zhang", "Lei", ""]]}, {"id": "1811.10100", "submitter": "Yichun Shi", "authors": "Yichun Shi, Debayan Deb, Anil K. Jain", "title": "WarpGAN: Automatic Caricature Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose, WarpGAN, a fully automatic network that can generate caricatures\ngiven an input face photo. Besides transferring rich texture styles, WarpGAN\nlearns to automatically predict a set of control points that can warp the photo\ninto a caricature, while preserving identity. We introduce an\nidentity-preserving adversarial loss that aids the discriminator to distinguish\nbetween different subjects. Moreover, WarpGAN allows customization of the\ngenerated caricatures by controlling the exaggeration extent and the visual\nstyles. Experimental results on a public domain dataset, WebCaricature, show\nthat WarpGAN is capable of generating a diverse set of caricatures while\npreserving the identities. Five caricature experts suggest that caricatures\ngenerated by WarpGAN are visually similar to hand-drawn ones and only prominent\nfacial features are exaggerated.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 21:36:01 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 18:52:57 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 07:10:36 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Shi", "Yichun", ""], ["Deb", "Debayan", ""], ["Jain", "Anil K.", ""]]}, {"id": "1811.10121", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "Foreground Clustering for Joint Segmentation and Localization in Videos\n  and Images", "comments": "In Proceedings of NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework in which video/image segmentation and\nlocalization are cast into a single optimization problem that integrates\ninformation from low level appearance cues with that of high level localization\ncues in a very weakly supervised manner. The proposed framework leverages two\nrepresentations at different levels, exploits the spatial relationship between\nbounding boxes and superpixels as linear constraints and simultaneously\ndiscriminates between foreground and background at bounding box and superpixel\nlevel. Different from previous approaches that mainly rely on discriminative\nclustering, we incorporate a foreground model that minimizes the histogram\ndifference of an object across all image frames. Exploiting the geometric\nrelation between the superpixels and bounding boxes enables the transfer of\nsegmentation cues to improve localization output and vice-versa. Inclusion of\nthe foreground model generalizes our discriminative framework to video data\nwhere the background tends to be similar and thus, not discriminative. We\ndemonstrate the effectiveness of our unified framework on the YouTube Object\nvideo dataset, Internet Object Discovery dataset and Pascal VOC 2007.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:00:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "1811.10126", "submitter": "Richard Wood", "authors": "Richard Wood, Alexander McGlashan, C.B. Moon, W.Y.Kim", "title": "Artificial Retina Using A Hybrid Neural Network With Spatial Transform\n  Capability", "comments": "16 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the design and programming of a hybrid (digital/analog)\nneural network to function as an artificial retina with the ability to perform\na spatial discrete cosine transform. We describe the structure of the circuit,\nwhich uses an analog cell that is interlinked using a programmable digital\narray. The paper is broken into three main parts. First, we present the results\nof a Matlab simulation. Then we show the circuit simulation in Spice. This is\nfollowed by a demonstration of the practical device. This system has\nintentionally separated components with the specialty analog circuits being\nseparated from the readily available digital field programmable gate array\n(FPGA) components. Further development includes the use of rapid\nmanufacture-able organic electronics used for the analog components. The\nplanned uses for this platform include crowd development of software that uses\nthe underlying pulse based processing. The development package will include\nsimulators in the form of Matlab and Spice type software platforms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:32:53 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wood", "Richard", ""], ["McGlashan", "Alexander", ""], ["Moon", "C. B.", ""], ["Kim", "W. Y.", ""]]}, {"id": "1811.10136", "submitter": "Wei Gao", "authors": "Wei Gao and Russ Tedrake", "title": "FilterReg: Robust and Efficient Probabilistic Point-Set Registration\n  using Gaussian Filter and Twist Parameterization", "comments": "CVPR 2019. The video demo and source code are on\n  https://sites.google.com/view/filterreg/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic point-set registration methods have been gaining more attention\nfor their robustness to noise, outliers and occlusions. However, these methods\ntend to be much slower than the popular iterative closest point (ICP)\nalgorithms, which severely limits their usability. In this paper, we contribute\na novel probabilistic registration method that achieves state-of-the-art\nrobustness as well as substantially faster computational performance than\nmodern ICP implementations. This is achieved using a rigorous yet\ncomputationally-efficient probabilistic formulation. Point-set registration is\ncast as a maximum likelihood estimation and solved using the EM algorithm. We\nshow that with a simple augmentation, the E step can be formulated as a\nfiltering problem, allowing us to leverage advances in efficient Gaussian\nfiltering methods. We also propose a customized permutohedral filter for\nimproved efficiency while retaining sufficient accuracy for our task.\nAdditionally, we present a simple and efficient twist parameterization that\ngeneralizes our method to the registration of articulated and deformable\nobjects. For articulated objects, the complexity of our method is almost\nindependent of the Degrees Of Freedom (DOFs), which makes it highly efficient\neven for high DOF systems. The results demonstrate the proposed method\nconsistently outperforms many competitive baselines on a variety of\nregistration tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 01:23:47 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 15:51:10 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 17:34:47 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Gao", "Wei", ""], ["Tedrake", "Russ", ""]]}, {"id": "1811.10144", "submitter": "Yang Fu", "authors": "Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, Thomas\n  Huang", "title": "Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation\n  Approach for Person Re-identification", "comments": "This work has been accepted as an Oral presentation at ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation in person re-identification (re-ID) has always been a\nchallenging task. In this work, we explore how to harness the natural similar\ncharacteristics existing in the samples from the target domain for learning to\nconduct person re-ID in an unsupervised manner. Concretely, we propose a\nSelf-similarity Grouping (SSG) approach, which exploits the potential\nsimilarity (from global body to local parts) of unlabeled samples to\nautomatically build multiple clusters from different views. These independent\nclusters are then assigned with labels, which serve as the pseudo identities to\nsupervise the training process. We repeatedly and alternatively conduct such a\ngrouping and training process until the model is stable. Despite the apparent\nsimplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC\nto Market1501) and 4.4% (Market1501 to DukeMTMC) in mAP, respectively. Upon our\nSSG, we further introduce a clustering-guided semisupervised approach named SSG\n++ to conduct the one-shot domain adaption in an open set setting (i.e. the\nnumber of independent identities from the target domain is unknown). Without\nspending much effort on labeling, our SSG ++ can further promote the mAP upon\nSSG by 10.7% and 6.9%, respectively. Our Code is available at:\nhttps://github.com/OasisYang/SSG .\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 02:17:17 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 21:34:49 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 03:43:29 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fu", "Yang", ""], ["Wei", "Yunchao", ""], ["Wang", "Guanshuo", ""], ["Zhou", "Yuqian", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas", ""]]}, {"id": "1811.10153", "submitter": "Ryohei Suzuki", "authors": "Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji,\n  Huachun Zhu", "title": "Spatially Controllable Image Synthesis with Internal Representation\n  Collaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel CNN-based image editing strategy that allows the user to\nchange the semantic information of an image over an arbitrary region by\nmanipulating the feature-space representation of the image in a trained GAN\nmodel. We will present two variants of our strategy: (1) spatial conditional\nbatch normalization (sCBN), a type of conditional batch normalization with\nuser-specifiable spatial weight maps, and (2) feature-blending, a method of\ndirectly modifying the intermediate features. Our methods can be used to edit\nboth artificial image and real image, and they both can be used together with\nany GAN with conditional normalization layers. We will demonstrate the power of\nour method through experiments on various types of GANs trained on different\ndatasets. Code will be available at\nhttps://github.com/pfnet-research/neural-collage.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:00:08 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 06:19:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Suzuki", "Ryohei", ""], ["Koyama", "Masanori", ""], ["Miyato", "Takeru", ""], ["Yonetsuji", "Taizan", ""], ["Zhu", "Huachun", ""]]}, {"id": "1811.10166", "submitter": "Charlotte Pelletier Dr", "authors": "Charlotte Pelletier, Geoffrey I. Webb, Francois Petitjean", "title": "Temporal Convolutional Neural Network for the Classification of\n  Satellite Image Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New remote sensing sensors now acquire high spatial and spectral Satellite\nImage Time Series (SITS) of the world. These series of images are a key\ncomponent of classification systems that aim at obtaining up-to-date and\naccurate land cover maps of the Earth's surfaces. More specifically, the\ncombination of the temporal, spectral and spatial resolutions of new SITS makes\npossible to monitor vegetation dynamics. Although traditional classification\nalgorithms, such as Random Forest (RF), have been successfully applied for SITS\nclassification, these algorithms do not make the most of the temporal domain.\nConversely, some approaches that take into account the temporal dimension have\nrecently been tested, especially Recurrent Neural Networks (RNNs). This paper\nproposes an exhaustive study of another deep learning approaches, namely\nTemporal Convolutional Neural Networks (TempCNNs) where convolutions are\napplied in the temporal dimension. The goal is to quantitatively and\nqualitatively evaluate the contribution of TempCNNs for SITS classification.\nThis paper proposes a set of experiments performed on one million time series\nextracted from 46 Formosat-2 images. The experimental results show that\nTempCNNs are more accurate than RF and RNNs, that are the current state of the\nart for SITS classification. We also highlight some differences with results\nobtained in computer vision, e.g. about pooling layers. Moreover, we provide\nsome general guidelines on the network architecture, common regularization\nmechanisms, and hyper-parameter values such as batch size. Finally, we assess\nthe visual quality of the land cover maps produced by TempCNNs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:42:52 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 01:00:29 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Pelletier", "Charlotte", ""], ["Webb", "Geoffrey I.", ""], ["Petitjean", "Francois", ""]]}, {"id": "1811.10180", "submitter": "Liyuan Pan Miss", "authors": "Liyuan Pan, Cedric Scheerlinck, Xin Yu, Richard Hartley, Miaomiao Liu,\n  and Yuchao Dai", "title": "Bringing a Blurry Frame Alive at High Frame-Rate with an Event Camera", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras can measure intensity changes (called `{\\it events}')\nwith microsecond accuracy under high-speed motion and challenging lighting\nconditions. With the active pixel sensor (APS), the event camera allows\nsimultaneous output of the intensity frames. However, the output images are\ncaptured at a relatively low frame-rate and often suffer from motion blur. A\nblurry image can be regarded as the integral of a sequence of latent images,\nwhile the events indicate the changes between the latent images. Therefore, we\nare able to model the blur-generation process by associating event data to a\nlatent image. In this paper, we propose a simple and effective approach, the\n\\textbf{Event-based Double Integral (EDI)} model, to reconstruct a high\nframe-rate, sharp video from a single blurry frame and its event data. The\nvideo generation is based on solving a simple non-convex optimization problem\nin a single scalar variable. Experimental results on both synthetic and real\nimages demonstrate the superiority of our EDI model and optimization method in\ncomparison to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 05:19:57 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 06:06:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Pan", "Liyuan", ""], ["Scheerlinck", "Cedric", ""], ["Yu", "Xin", ""], ["Hartley", "Richard", ""], ["Liu", "Miaomiao", ""], ["Dai", "Yuchao", ""]]}, {"id": "1811.10185", "submitter": "Liyuan Pan Miss", "authors": "Liyuan Pan, Richard Hartley, Miaomiao Liu and Yuchao Dai", "title": "Phase-only Image Based Kernel Estimation for Single-image Blind\n  Deblurring", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image blurring process is generally modelled as the convolution of a blur\nkernel with a latent image. Therefore, the estimation of the blur kernel is\nessentially important for blind image deblurring. Unlike existing approaches\nwhich focus on approaching the problem by enforcing various priors on the blur\nkernel and the latent image, we are aiming at obtaining a high quality blur\nkernel directly by studying the problem in the frequency domain. We show that\nthe auto-correlation of the absolute phase-only image can provide faithful\ninformation about the motion (e.g. the motion direction and magnitude, we call\nit the motion pattern in this paper.) that caused the blur, leading to a new\nand efficient blur kernel estimation approach. The blur kernel is then refined\nand the sharp image is estimated by solving an optimization problem by\nenforcing a regularization on the blur kernel and the latent image. We further\nextend our approach to handle non-uniform blur, which involves spatially\nvarying blur kernels. Our approach is evaluated extensively on synthetic and\nreal data and shows good results compared to the state-of-the-art deblurring\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 05:40:32 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 06:03:25 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 07:34:52 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Pan", "Liyuan", ""], ["Hartley", "Richard", ""], ["Liu", "Miaomiao", ""], ["Dai", "Yuchao", ""]]}, {"id": "1811.10199", "submitter": "Naranchimeg Bold", "authors": "Bold Naranchimeg, Chao Zhang, Takuya Akashi", "title": "Cross-domain Deep Feature Combination for Bird Species Classification\n  with Audio-visual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent decade, many state-of-the-art algorithms on image classification as\nwell as audio classification have achieved noticeable successes with the\ndevelopment of deep convolutional neural network (CNN). However, most of the\nworks only exploit single type of training data. In this paper, we present a\nstudy on classifying bird species by exploiting the combination of both visual\n(images) and audio (sounds) data using CNN, which has been sparsely treated so\nfar. Specifically, we propose CNN-based multimodal learning models in three\ntypes of fusion strategies (early, middle, late) to settle the issues of\ncombining training data cross domains. The advantage of our proposed method\nlies on the fact that We can utilize CNN not only to extract features from\nimage and audio data (spectrogram) but also to combine the features across\nmodalities. In the experiment, we train and evaluate the network structure on a\ncomprehensive CUB-200-2011 standard data set combing our originally collected\naudio data set with respect to the data species. We observe that a model which\nutilizes the combination of both data outperforms models trained with only an\neither type of data. We also show that transfer learning can significantly\nincrease the classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:28:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Naranchimeg", "Bold", ""], ["Zhang", "Chao", ""], ["Akashi", "Takuya", ""]]}, {"id": "1811.10200", "submitter": "Girish Varma", "authors": "Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan\n  Chandraker, C V Jawahar", "title": "IDD: A Dataset for Exploring Problems of Autonomous Navigation in\n  Unconstrained Environments", "comments": "WACV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several datasets for autonomous navigation have become available in\nrecent years, they tend to focus on structured driving environments. This\nusually corresponds to well-delineated infrastructure such as lanes, a small\nnumber of well-defined categories for traffic participants, low variation in\nobject or background appearance and strict adherence to traffic rules. We\npropose IDD, a novel dataset for road scene understanding in unstructured\nenvironments where the above assumptions are largely not satisfied. It consists\nof 10,004 images, finely annotated with 34 classes collected from 182 drive\nsequences on Indian roads. The label set is expanded in comparison to popular\nbenchmarks such as Cityscapes, to account for new classes. It also reflects\nlabel distributions of road scenes significantly different from existing\ndatasets, with most classes displaying greater within-class diversity.\nConsistent with real driving behaviours, it also identifies new classes such as\ndrivable areas besides the road. We propose a new four-level label hierarchy,\nwhich allows varying degrees of complexity and opens up possibilities for new\ntraining methods. Our empirical study provides an in-depth analysis of the\nlabel characteristics. State-of-the-art methods for semantic segmentation\nachieve much lower accuracies on our dataset, demonstrating its distinction\ncompared to Cityscapes. Finally, we propose that our dataset is an ideal\nopportunity for new problems such as domain adaptation, few-shot learning and\nbehaviour prediction in road scenes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:29:26 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Varma", "Girish", ""], ["Subramanian", "Anbumani", ""], ["Namboodiri", "Anoop", ""], ["Chandraker", "Manmohan", ""], ["Jawahar", "C V", ""]]}, {"id": "1811.10201", "submitter": "AnChieh Cheng", "authors": "An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, Min Sun", "title": "InstaNAS: Instance-aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Neural Architecture Search (NAS) aims at finding a single\narchitecture that achieves the best performance, which usually optimizes task\nrelated learning objectives such as accuracy. However, a single architecture\nmay not be representative enough for the whole dataset with high diversity and\nvariety. Intuitively, electing domain-expert architectures that are proficient\nin domain-specific features can further benefit architecture related objectives\nsuch as latency. In this paper, we propose InstaNAS---an instance-aware NAS\nframework---that employs a controller trained to search for a \"distribution of\narchitectures\" instead of a single architecture; This allows the model to use\nsophisticated architectures for the difficult samples, which usually comes with\nlarge architecture related cost, and shallow architectures for those easy\nsamples. During the inference phase, the controller assigns each of the unseen\ninput samples with a domain expert architecture that can achieve high accuracy\nwith customized inference costs. Experiments within a search space inspired by\nMobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without\ncompromising accuracy on a series of datasets against MobileNetV2.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:29:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 14:12:40 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 09:25:04 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Cheng", "An-Chieh", ""], ["Lin", "Chieh Hubert", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Sun", "Min", ""]]}, {"id": "1811.10203", "submitter": "Dan Levi", "authors": "Noa Garnett, Rafi Cohen, Tomer Pe'er, Roee Lahav, Dan Levi", "title": "3D-LaneNet: End-to-End 3D Multiple Lane Detection", "comments": "To be presented in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a network that directly predicts the 3D layout of lanes in a\nroad scene from a single image. This work marks a first attempt to address this\ntask with on-board sensing without assuming a known constant lane width or\nrelying on pre-mapped environments. Our network architecture, 3D-LaneNet,\napplies two new concepts: intra-network inverse-perspective mapping (IPM) and\nanchor-based lane representation. The intra-network IPM projection facilitates\na dual-representation information flow in both regular image-view and top-view.\nAn anchor-per-column output representation enables our end-to-end approach\nwhich replaces common heuristics such as clustering and outlier rejection,\ncasting lane estimation as an object detection problem. In addition, our\napproach explicitly handles complex situations such as lane merges and splits.\nResults are shown on two new 3D lane datasets, a synthetic and a real one. For\ncomparison with existing methods, we test our approach on the image-only\ntuSimple lane detection benchmark, achieving performance competitive with\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:34:28 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:59:05 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 06:48:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Garnett", "Noa", ""], ["Cohen", "Rafi", ""], ["Pe'er", "Tomer", ""], ["Lahav", "Roee", ""], ["Levi", "Dan", ""]]}, {"id": "1811.10210", "submitter": "Girish Varma", "authors": "Sudhir Yarram, Girish Varma, C.V. Jawahar", "title": "City-Scale Road Audit System using Deep Learning", "comments": "IROS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road networks in cities are massive and is a critical component of mobility.\nFast response to defects, that can occur not only due to regular wear and tear\nbut also because of extreme events like storms, is essential. Hence there is a\nneed for an automated system that is quick, scalable and cost-effective for\ngathering information about defects. We propose a system for city-scale road\naudit, using some of the most recent developments in deep learning and semantic\nsegmentation. For building and benchmarking the system, we curated a dataset\nwhich has annotations required for road defects. However, many of the labels\nrequired for road audit have high ambiguity which we overcome by proposing a\nlabel hierarchy. We also propose a multi-step deep learning model that segments\nthe road, subdivide the road further into defects, tags the frame for each\ndefect and finally localizes the defects on a map gathered using GPS. We\nanalyze and evaluate the models on image tagging as well as segmentation at\ndifferent levels of the label hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:49:11 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Yarram", "Sudhir", ""], ["Varma", "Girish", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1811.10216", "submitter": "Aniruddha Patil", "authors": "Aniruddha V Patil and Pankaj Rabha", "title": "A Survey on Joint Object Detection and Pose Estimation using Monocular\n  Vision", "comments": "Accepted at the International Joint Conference on Computer Vision and\n  Pattern Recognition (CCVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey we present a complete landscape of joint object detection and\npose estimation methods that use monocular vision. Descriptions of traditional\napproaches that involve descriptors or models and various estimation methods\nhave been provided. These descriptors or models include chordiograms,\nshape-aware deformable parts model, bag of boundaries, distance transform\ntemplates, natural 3D markers and facet features whereas the estimation methods\ninclude iterative clustering estimation, probabilistic networks and iterative\ngenetic matching. Hybrid approaches that use handcrafted feature extraction\nfollowed by estimation by deep learning methods have been outlined. We have\ninvestigated and compared, wherever possible, pure deep learning based\napproaches (single stage and multi stage) for this problem. Comprehensive\ndetails of the various accuracy measures and metrics have been illustrated. For\nthe purpose of giving a clear overview, the characteristics of relevant\ndatasets are discussed. The trends that prevailed from the infancy of this\nproblem until now have also been highlighted.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 07:43:23 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Patil", "Aniruddha V", ""], ["Rabha", "Pankaj", ""]]}, {"id": "1811.10228", "submitter": "Ravid Shwartz Ziv", "authors": "Itamar Ben-Ari and Ravid Shwartz-Ziv", "title": "Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection\n  in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semi-supervised model for detecting anomalies in videos\ninspiredby the Video Pixel Network [van den Oord et al., 2016]. VPN is a\nprobabilisticgenerative model based on a deep neural network that estimates the\ndiscrete jointdistribution of raw pixels in video frames. Our model extends the\nConvolutional-LSTM video encoder part of the VPN with a novel convolutional\nbased attentionmechanism. We also modify the Pixel-CNN decoder part of the VPN\nto a frameinpainting task where a partially masked version of the frame to\npredict is given asinput. The frame reconstruction error is used as an anomaly\nindicator. We test ourmodel on a modified version of the moving mnist dataset\n[Srivastava et al., 2015]. Our model is shown to be effective in detecting\nanomalies in videos. This approachcould be a component in applications\nrequiring visual common sense.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 08:25:14 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ben-Ari", "Itamar", ""], ["Shwartz-Ziv", "Ravid", ""]]}, {"id": "1811.10240", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio and George Azzopardi and Nicolai Petkov", "title": "Brain-inspired robust delineation operator", "comments": "Accepted at Brain-driven Computer Vision workshop at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel filter, based on the existing COSFIRE\nfilter, for the delineation of patterns of interest. It includes a mechanism of\npush-pull inhibition that improves robustness to noise in terms of spurious\ntexture. Push-pull inhibition is a phenomenon that is observed in neurons in\narea V1 of the visual cortex, which suppresses the response of certain simple\ncells for stimuli of preferred orientation but of non-preferred contrast. This\ntype of inhibition allows for sharper detection of the patterns of interest and\nimproves the quality of delineation especially in images with spurious texture.\n  We performed experiments on images from different applications, namely the\ndetection of rose stems for automatic gardening, the delineation of cracks in\npavements and road surfaces, and the segmentation of blood vessels in retinal\nimages. Push-pull inhibition helped to improve results considerably in all\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:24:58 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Strisciuglio", "Nicola", ""], ["Azzopardi", "George", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1811.10247", "submitter": "Zengyi Qin", "authors": "Zengyi Qin, Jinglu Wang, Yan Lu", "title": "MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object\n  Localization", "comments": "8 pages, accepted by AAAI 2019, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and localizing objects in the real 3D space, which plays a crucial\nrole in scene understanding, is particularly challenging given only a single\nRGB image due to the geometric information loss during imagery projection. We\npropose MonoGRNet for the amodal 3D object detection from a monocular RGB image\nvia geometric reasoning in both the observed 2D projection and the unobserved\ndepth dimension. MonoGRNet is a single, unified network composed of four\ntask-specific subnetworks, responsible for 2D object detection, instance depth\nestimation (IDE), 3D localization and local corner regression. Unlike the\npixel-level depth estimation that needs per-pixel annotations, we propose a\nnovel IDE method that directly predicts the depth of the targeting 3D bounding\nbox's center using sparse supervision. The 3D localization is further achieved\nby estimating the position in the horizontal and vertical dimensions. Finally,\nMonoGRNet is jointly learned by optimizing the locations and poses of the 3D\nbounding boxes in the global context. We demonstrate that MonoGRNet achieves\nstate-of-the-art performance on challenging datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:36:40 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:52:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Qin", "Zengyi", ""], ["Wang", "Jinglu", ""], ["Lu", "Yan", ""]]}, {"id": "1811.10261", "submitter": "Santosh Vipparthi Kumar", "authors": "Monu Verma, Santosh. K. Vipparthi, Girdhari Singh", "title": "Region Based Extensive Response Index Pattern for Facial Expression\n  Recognition", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel descriptor named Region based Extensive Response\nIndex Pattern (RETRaIN) for facial expression recognition. The RETRaIN encodes\nthe relation among the reference and neighboring pixels of facial active\nregions. These relations are computed by using directional compass mask on an\ninput image and extract the high edge responses in foremost directions. Further\nextreme edge index positions are selected and encoded into six-bit compact code\nto reduce feature dimensionality and distinguish between the uniform and\nnon-uniform patterns in the facial features. The performance of the proposed\ndescriptor is tested and evaluated on three benchmark datasets Extended Cohn\nKanade, JAFFE, and MUG. The RETRaIN achieves superior recognition accuracy in\ncomparison to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:03:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Verma", "Monu", ""], ["Vipparthi", "Santosh. K.", ""], ["Singh", "Girdhari", ""]]}, {"id": "1811.10302", "submitter": "Shuai Bai", "authors": "Shuai Bai, Zhiqun He, Ting-Bing Xu, Zheng Zhu, Yuan Dong, Hongliang\n  Bai", "title": "Multi-hierarchical Independent Correlation Filters for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visual tracking, most of the traditional correlation filters (CF) based\nmethods suffer from the bottleneck of feature redundancy and lack of motion\ninformation. In this paper, we design a novel tracking framework, called\nmulti-hierarchical independent correlation filters (MHIT). The framework\nconsists of motion estimation module, hierarchical features selection,\nindependent CF online learning, and adaptive multi-branch CF fusion.\nSpecifically, the motion estimation module is introduced to capture motion\ninformation, which effectively alleviates the object partial occlusion in the\ntemporal video. The multi-hierarchical deep features of CNN representing\ndifferent semantic information can be fully excavated to track multi-scale\nobjects. To better overcome the deep feature redundancy, each hierarchical\nfeatures are independently fed into a single branch to implement the online\nlearning of parameters. Finally, an adaptive weight scheme is integrated into\nthe framework to fuse these independent multi-branch CFs for the better and\nmore robust visual object tracking. Extensive experiments on OTB and VOT\ndatasets show that the proposed MHIT tracker can significantly improve the\ntracking performance. Especially, it obtains a 20.1% relative performance gain\ncompared to the top trackers on the VOT2017 challenge, and also achieves new\nstate-of-the-art performance on the VOT2018 challenge.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 11:41:23 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:39:58 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bai", "Shuai", ""], ["He", "Zhiqun", ""], ["Xu", "Ting-Bing", ""], ["Zhu", "Zheng", ""], ["Dong", "Yuan", ""], ["Bai", "Hongliang", ""]]}, {"id": "1811.10323", "submitter": "Tarun Kalluri Mr.", "authors": "Tarun Kalluri, Girish Varma, Manmohan Chandraker, C V Jawahar", "title": "Universal Semi-Supervised Semantic Segmentation", "comments": "Accepted as poster presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the need for semantic segmentation has arisen across several\ndifferent applications and environments. However, the expense and redundancy of\nannotation often limits the quantity of labels available for training in any\ndomain, while deployment is easier if a single model works well across domains.\nIn this paper, we pose the novel problem of universal semi-supervised semantic\nsegmentation and propose a solution framework, to meet the dual needs of lower\nannotation and deployment costs. In contrast to counterpoints such as fine\ntuning, joint training or unsupervised domain adaptation, universal\nsemi-supervised segmentation ensures that across all domains: (i) a single\nmodel is deployed, (ii) unlabeled data is used, (iii) performance is improved,\n(iv) only a few labels are needed and (v) label spaces may differ. To address\nthis, we minimize supervised as well as within and cross-domain unsupervised\nlosses, introducing a novel feature alignment objective based on pixel-aware\nentropy regularization for the latter. We demonstrate quantitative advantages\nover other approaches on several combinations of segmentation datasets across\ndifferent geographies (Germany, England, India) and environments (outdoors,\nindoors), as well as qualitative insights on the aligned representations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 12:36:03 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:55:56 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 06:13:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kalluri", "Tarun", ""], ["Varma", "Girish", ""], ["Chandraker", "Manmohan", ""], ["Jawahar", "C V", ""]]}, {"id": "1811.10343", "submitter": "Tianwei Shen", "authors": "Tianwei Shen, Zixin Luo, Lei Zhou, Runze Zhang, Siyu Zhu, Tian Fang,\n  Long Quan", "title": "Matchable Image Retrieval by Learning from Surface Reconstruction", "comments": "accepted by ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved superior performance on\nobject image retrieval, while Bag-of-Words (BoW) models with handcrafted local\nfeatures still dominate the retrieval of overlapping images in 3D\nreconstruction. In this paper, we narrow down this gap by presenting an\nefficient CNN-based method to retrieve images with overlaps, which we refer to\nas the matchable image retrieval problem. Different from previous methods that\ngenerates training data based on sparse reconstruction, we create a large-scale\nimage database with rich 3D geometrics and exploit information from surface\nreconstruction to obtain fine-grained training data. We propose a batched\ntriplet-based loss function combined with mesh re-projection to effectively\nlearn the CNN representation. The proposed method significantly accelerates the\nimage retrieval process in 3D reconstruction and outperforms the\nstate-of-the-art CNN-based and BoW methods for matchable image retrieval. The\ncode and data are available at https://github.com/hlzz/mirror.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:03:15 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 17:26:18 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Shen", "Tianwei", ""], ["Luo", "Zixin", ""], ["Zhou", "Lei", ""], ["Zhang", "Runze", ""], ["Zhu", "Siyu", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1811.10352", "submitter": "Zhijie Wu", "authors": "Zhijie Wu and Chunjin Song and Yang Zhou and Minglun Gong and Hui\n  Huang", "title": "EFANet: Exchangeable Feature Alignment Network for Arbitrary Style\n  Transfer", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer has been an important topic both in computer vision and\ngraphics. Since the seminal work of Gatys et al. first demonstrates the power\nof stylization through optimization in the deep feature space, quite a few\napproaches have achieved real-time arbitrary style transfer with\nstraightforward statistic matching techniques. In this work, our key\nobservation is that only considering features in the input style image for the\nglobal deep feature statistic matching or local patch swap may not always\nensure a satisfactory style transfer; see e.g., Figure 1. Instead, we propose a\nnovel transfer framework, EFANet, that aims to jointly analyze and better align\nexchangeable features extracted from content and style image pair. In this way,\nthe style features from the style image seek for the best compatibility with\nthe content information in the content image, leading to more structured\nstylization results. In addition, a new whitening loss is developed for\npurifying the computed content features and better fusion with styles in\nfeature space. Qualitative and quantitative experiments demonstrate the\nadvantages of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:15:23 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 12:16:25 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2019 18:36:11 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wu", "Zhijie", ""], ["Song", "Chunjin", ""], ["Zhou", "Yang", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1811.10355", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Unsupervised learning with sparse space-and-time autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use spatially-sparse two, three and four dimensional convolutional\nautoencoder networks to model sparse structures in 2D space, 3D space, and\n3+1=4 dimensional space-time. We evaluate the resulting latent spaces by\ntesting their usefulness for downstream tasks. Applications are to handwriting\nrecognition in 2D, segmentation for parts in 3D objects, segmentation for\nobjects in 3D scenes, and body-part segmentation for 4D wire-frame models\ngenerated from motion capture data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:22:17 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1811.10374", "submitter": "Macarena D\\'iaz Gonz\\'alez", "authors": "Macarena D\\'iaz, Jorge Novo, Paula Cutr\\'in, Francisco G\\'omez-Ulla,\n  Manuel G. Penedo, Marcos Ortega", "title": "Automatic segmentation of the Foveal Avascular Zone in ophthalmological\n  OCT-A images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0212364", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angiography by Optical Coherence Tomography is a non-invasive retinal imaging\nmodality of recent appearance that allows the visualization of the vascular\nstructure at predefined depths based on the detection of the blood movement.\nOCT-A images constitute a suitable scenario to analyse the retinal vascular\nproperties of regions of interest, measuring the characteristics of the foveal\nvascular and avascular zones. Extracted parameters of this region can be used\nas prognostic factors that determine if the patient suffers from certain\npathologies, indicating the associated pathological degree. The manual\nextraction of these biomedical parameters is a long, tedious and subjective\nprocess, introducing a significant intra and inter-expert variability, which\npenalizes the utility of the measurements. In addition, the absence of tools\nthat automatically facilitate these calculations encourages the creation of\ncomputer-aided diagnosis frameworks that ease the doctor's work, increasing\ntheir productivity and making viable the use of this type of vascular\nbiomarkers.\n  We propose a fully automatic system that identifies and precisely segments\nthe region of the foveal avascular zone (FAZ) using a novel ophthalmological\nimage modality as is OCT-A. The system combines different image processing\ntechniques to firstly identify the region where the FAZ is contained and,\nsecondly, proceed with the extraction of its precise contour. The system was\nvalidated using a representative set of 168 OCT-A images, providing accurate\nresults with the best correlation with the manual measurements of two experts\nclinician of 0.93 as well as a Jaccard's index of 0.82 of the best experimental\ncase. This tool provides an accurate FAZ measurement with the desired\nobjectivity and reproducibility, being very useful for the analysis of relevant\nvascular diseases through the study of the retinal microcirculation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:07:28 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["D\u00edaz", "Macarena", ""], ["Novo", "Jorge", ""], ["Cutr\u00edn", "Paula", ""], ["G\u00f3mez-Ulla", "Francisco", ""], ["Penedo", "Manuel G.", ""], ["Ortega", "Marcos", ""]]}, {"id": "1811.10399", "submitter": "Kedar Potdar", "authors": "Kedar Potdar, Chinmay D. Pai, Sukrut Akolkar", "title": "A Convolutional Neural Network based Live Object Recognition System as\n  Blind Aid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a live object recognition system that serves as a blind\naid. Visually impaired people heavily rely on their other senses such as touch\nand auditory signals for understanding the environment around them. The act of\nknowing what object is in front of the blind person without touching it (by\nhand or some other tool) is very difficult. In some cases, the physical contact\nbetween the person and object can be dangerous, and even lethal.\n  This project employs a Convolutional Neural Network for recognition of\npre-trained objects on the ImageNet dataset. A camera, aligned with the\nsystem's predetermined orientation serves as input to the computer system,\nwhich has the object recognition Neural Network deployed to carry out real-time\nobject detection. Output from the network can then be parsed to present to the\nvisually impaired person either in the form of audio or Braille text.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:38:25 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Potdar", "Kedar", ""], ["Pai", "Chinmay D.", ""], ["Akolkar", "Sukrut", ""]]}, {"id": "1811.10413", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, Ian Reid", "title": "Structured Binary Neural Networks for Accurate Image Classification and\n  Semantic Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1808.02631", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to train convolutional neural networks (CNNs) with\nboth binarized weights and activations, leading to quantized models\nspecifically} for mobile devices with limited power capacity and computation\nresources. Previous works on quantizing CNNs seek to approximate the\nfloating-point information using a set of discrete values, which we call value\napproximation, but typically assume the same architecture as the full-precision\nnetworks. In this paper, however, we take a novel 'structure approximation'\nview for quantization---it is very likely that a different architecture may be\nbetter for best performance. In particular, we propose a `network\ndecomposition' strategy, named \\textbf{Group-Net}, in which we divide the\nnetwork into groups. In this way, each full-precision group can be effectively\nreconstructed by aggregating a set of homogeneous binary branches.\n  In addition, we learn effective connections among groups to improve the\nrepresentational capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for highly\naccurate semantic segmentation by embedding rich context into the binary\nstructure.\n  Experiments on both classification and semantic segmentation tasks\ndemonstrate the superior performance of the proposed methods over various\npopular architectures. In particular, we outperform the previous best binary\nneural networks in terms of accuracy and major computation savings.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 05:24:17 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 03:32:02 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""], ["Tan", "Mingkui", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""]]}, {"id": "1811.10415", "submitter": "Cam Bermudez", "authors": "Camilo Bermudez, William Rodriguez, Yuankai Huo, Allison E. Hainline,\n  Rui Li, Robert Shults, Pierre D. DHaese, Peter E. Konrad, Benoit M. Dawant,\n  Bennett A. Landman", "title": "Towards Machine Learning Prediction of Deep Brain Stimulation (DBS)\n  Intra-operative Efficacy Maps", "comments": "Accepted to SPIE: Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep brain stimulation (DBS) has the potential to improve the quality of life\nof people with a variety of neurological diseases. A key challenge in DBS is in\nthe placement of a stimulation electrode in the anatomical location that\nmaximizes efficacy and minimizes side effects. Pre-operative localization of\nthe optimal stimulation zone can reduce surgical times and morbidity. Current\nmethods of producing efficacy probability maps follow an anatomical guidance on\nmagnetic resonance imaging (MRI) to identify the areas with the highest\nefficacy in a population. In this work, we propose to revisit this problem as a\nclassification problem, where each voxel in the MRI is a sample informed by the\nsurrounding anatomy. We use a patch-based convolutional neural network to\nclassify a stimulation coordinate as having a positive reduction in symptoms\nduring surgery. We use a cohort of 187 patients with a total of 2,869\nstimulation coordinates, upon which 3D patches were extracted and associated\nwith an efficacy score. We compare our results with a registration-based method\nof surgical planning. We show an improvement in the classification of\nintraoperative stimulation coordinates as a positive response in reduction of\nsymptoms with AUC of 0.670 compared to a baseline registration-based approach,\nwhich achieves an AUC of 0.627 (p < 0.01). Although additional validation is\nneeded, the proposed classification framework and deep learning method appear\nwell-suited for improving pre-surgical planning and personalize treatment\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:50:06 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Bermudez", "Camilo", ""], ["Rodriguez", "William", ""], ["Huo", "Yuankai", ""], ["Hainline", "Allison E.", ""], ["Li", "Rui", ""], ["Shults", "Robert", ""], ["DHaese", "Pierre D.", ""], ["Konrad", "Peter E.", ""], ["Dawant", "Benoit M.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1811.10419", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Haojin Yang, Christoph Meinel", "title": "Multi-Task Generative Adversarial Network for Handling Imbalanced\n  Clinical Data", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216. arXiv admin note: text overlap with arXiv:1810.03871", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/21", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative adversarial architecture to mitigate imbalance\ndata problem for the task of medical image semantic segmentation where the\nmajority of pixels belong to a healthy region and few belong to lesion or\nnon-health region. A model trained with imbalanced data tends to bias towards\nhealthy data which is not desired in clinical applications. We design a new\nconditional GAN with two components: a generative model and a discriminative\nmodel to mitigate imbalanced data problem through selective weighted loss.\nWhile the generator is trained on sequential magnetic resonance images (MRI) to\nlearn semantic segmentation and disease classification, the discriminator\nclassifies whether a generated output is real or fake. The proposed\narchitecture achieved state-of-the-art results on ACDC-2017 for cardiac\nsegmentation and diseases classification. We have achieved competitive results\non BraTS-2017 for brain tumor segmentation and brain diseases classification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 10:19:22 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Rezaei", "Mina", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1811.10427", "submitter": "Qunwei Li", "authors": "Qunwei Li, Bhavya Kailkhura, Rushil Anirudh, Yi Zhou, Yingbin Liang,\n  Pramod Varshney", "title": "MR-GAN: Manifold Regularized Generative Adversarial Networks", "comments": "arXiv admin note: text overlap with arXiv:1706.04156 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing interest in generative adversarial networks (GANs),\ntraining GANs remains a challenging problem, both from a theoretical and a\npractical standpoint. To address this challenge, in this paper, we propose a\nnovel way to exploit the unique geometry of the real data, especially the\nmanifold information. More specifically, we design a method to regularize GAN\ntraining by adding an additional regularization term referred to as manifold\nregularizer. The manifold regularizer forces the generator to respect the\nunique geometry of the real data manifold and generate high quality data.\nFurthermore, we theoretically prove that the addition of this regularization\nterm in any class of GANs including DCGAN and Wasserstein GAN leads to improved\nperformance in terms of generalization, existence of equilibrium, and\nstability. Preliminary experiments show that the proposed manifold\nregularization helps in avoiding mode collapse and leads to stable training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 21:21:02 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Qunwei", ""], ["Kailkhura", "Bhavya", ""], ["Anirudh", "Rushil", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Varshney", "Pramod", ""]]}, {"id": "1811.10437", "submitter": "Jiang Zhang", "authors": "Jiang Zhang, Yuanqing Xia, Ganghui Shen", "title": "A Novel Learning-based Global Path Planning Algorithm for Planetary\n  Rovers", "comments": "Submitted to Neurocomputing. arXiv admin note: text overlap with\n  arXiv:1808.08395", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous path planning algorithms are significant to planetary exploration\nrovers, since relying on commands from Earth will heavily reduce their\nefficiency of executing exploration missions. This paper proposes a novel\nlearning-based algorithm to deal with global path planning problem for\nplanetary exploration rovers. Specifically, a novel deep convolutional neural\nnetwork with double branches (DB-CNN) is designed and trained, which can plan\npath directly from orbital images of planetary surfaces without implementing\nenvironment mapping. Moreover, the planning procedure requires no prior\nknowledge about planetary surface terrains. Finally, experimental results\ndemonstrate that DB-CNN achieves better performance on global path planning and\nfaster convergence during training compared with the existing Value Iteration\nNetwork (VIN).\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 15:20:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Jiang", ""], ["Xia", "Yuanqing", ""], ["Shen", "Ganghui", ""]]}, {"id": "1811.10449", "submitter": "Tien Ho-Phuoc", "authors": "Hanh T. M. Tran and Tien Ho-Phuoc", "title": "Deep Laplacian Pyramid Network for Text Images Super-Resolution", "comments": "paper, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently demonstrated interesting results\nfor single image super-resolution. However, these networks were trained to deal\nwith super-resolution problem on natural images. In this paper, we adapt a deep\nnetwork, which was proposed for natural images superresolution, to single text\nimage super-resolution. To evaluate the network, we present our database for\nsingle text image super-resolution. Moreover, we propose to combine Gradient\nDifference Loss (GDL) with L1/L2 loss to enhance edges in super-resolution\nimage. Quantitative and qualitative evaluations on our dataset show that adding\nthe GDL improves the super-resolution results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:29:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tran", "Hanh T. M.", ""], ["Ho-Phuoc", "Tien", ""]]}, {"id": "1811.10452", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Mathieu Salzmann, Pascal Fua", "title": "Context-Aware Crowd Counting", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for counting people in crowded scenes rely on deep\nnetworks to estimate crowd density. They typically use the same filters over\nthe whole image or over large image patches. Only then do they estimate local\nscale to compensate for perspective distortion. This is typically achieved by\ntraining an auxiliary classifier to select, for predefined image patches, the\nbest kernel size among a limited set of choices. As such, these methods are not\nend-to-end trainable and restricted in the scope of context they can leverage.\n  In this paper, we introduce an end-to-end trainable deep architecture that\ncombines features obtained using multiple receptive field sizes and learns the\nimportance of each such feature at each image location. In other words, our\napproach adaptively encodes the scale of the contextual information required to\naccurately predict crowd density. This yields an algorithm that outperforms\nstate-of-the-art crowd counting methods, especially when perspective effects\nare strong.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:31:22 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 08:32:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Weizhe", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1811.10464", "submitter": "Angela Dai", "authors": "Angela Dai and Matthias Nie{\\ss}ner", "title": "Scan2Mesh: From Unstructured Range Scans to 3D Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Scan2Mesh, a novel data-driven generative approach which\ntransforms an unstructured and potentially incomplete range scan into a\nstructured 3D mesh representation. The main contribution of this work is a\ngenerative neural network architecture whose input is a range scan of a 3D\nobject and whose output is an indexed face set conditioned on the input scan.\nIn order to generate a 3D mesh as a set of vertices and face indices, the\ngenerative model builds on a series of proxy losses for vertices, edges, and\nfaces. At each stage, we realize a one-to-one discrete mapping between the\npredicted and ground truth data points with a combination of convolutional- and\ngraph neural network architectures. This enables our algorithm to predict a\ncompact mesh representation similar to those created through manual artist\neffort using 3D modeling software. Our generated mesh results thus produce\nsharper, cleaner meshes with a fundamentally different structure from those\ngenerated through implicit functions, a first step in bridging the gap towards\nartist-created CAD models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:54:15 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 15:51:51 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1811.10493", "submitter": "Bingzhang Hu", "authors": "BingZhang Hu, Yu Guan, Yan Gao, Yang Long, Nicholas Lane and Thomas\n  Ploetz", "title": "Robust Cross-View Gait Recognition with Evidence: A Discriminant Gait\n  GAN (DiGGAN) Approach", "comments": "Submitted to ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait as a biometric trait has attracted much attention in many security and\nprivacy applications such as identity recognition and authentication, during\nthe last few decades. Because of its nature as a long-distance biometric trait,\ngait can be easily collected and used to identify individuals non-intrusively\nthrough CCTV cameras. However, it is very difficult to develop robust automated\ngait recognition systems, since gait may be affected by many covariate factors\nsuch as clothing, walking speed, camera view angle etc. Out of them, large view\nangle changes has been deemed as the most challenging factor as it can alter\nthe overall gait appearance substantially.\n  Existing works on gait recognition are far from enough to provide satisfying\nperformances because of such view changes. Furthermore, very few works have\nconsidered evidences -- the demonstrable information revealing the\nreliabilities of decisions, which are regarded as important demands in machine\nlearning-based recognition/authentication applications. To address these\nissues, in this paper we propose a Discriminant Gait Generative Adversarial\nNetwork, namely DiGGAN, which can effectively extract view-invariant features\nfor cross-view gait recognition; and more importantly, to transfer gait images\nto different views -- serving as evidences and showing how the decisions have\nbeen made. Quantitative experiments have been conducted on the two most popular\ncross-view gait datasets, the OU-MVLP and CASIA-B, where the proposed DiGGAN\nhas outperformed state-of-the-art methods. Qualitative analysis has also been\nprovided and demonstrates the proposed DiGGAN's capability in providing\nevidences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:37:29 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 16:10:26 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 10:17:12 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Hu", "BingZhang", ""], ["Guan", "Yu", ""], ["Gao", "Yan", ""], ["Long", "Yang", ""], ["Lane", "Nicholas", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1811.10495", "submitter": "Shuxuan Guo", "authors": "Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann", "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce an approach to training a given compact network. To this end, we\nleverage over-parameterization, which typically improves both neural network\noptimization and generalization. Specifically, we propose to expand each linear\nlayer of the compact network into multiple consecutive linear layers, without\nadding any nonlinearity. As such, the resulting expanded network, or ExpandNet,\ncan be contracted back to the compact one algebraically at inference. In\nparticular, we introduce two convolutional expansion strategies and demonstrate\ntheir benefits on several tasks, including image classification, object\ndetection, and semantic segmentation. As evidenced by our experiments, our\napproach outperforms both training the compact network from scratch and\nperforming knowledge distillation from a teacher. Furthermore, our linear\nover-parameterization empirically reduces gradient confusion during training\nand improves the network generalization.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:40:24 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 13:33:58 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 11:08:09 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 17:26:52 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 11:55:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Guo", "Shuxuan", ""], ["Alvarez", "Jose M.", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1811.10508", "submitter": "Mateusz Kozi\\'nski", "authors": "Mateusz Kozi\\'nski, Agata Mosinska, Mathieu Salzmann and Pascal Fua", "title": "Tracing in 2D to Reduce the Annotation Effort for 3D Deep Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of obtaining annotations to build training databases still\nslows down the adoption of recent deep learning approaches for biomedical image\nanalysis. In this paper, we show that we can train a Deep Net to perform 3D\nvolumetric delineation given only 2D annotations in Maximum Intensity\nProjections (MIP). As a consequence, we can decrease the amount of time spent\nannotating by a factor of two while maintaining similar performance.\n  Our approach is inspired by space carving, a classical technique of\nreconstructing complex 3D shapes from arbitrarily-positioned cameras. We will\ndemonstrate its effectiveness on 3D light microscopy images of neurons and\nretinal blood vessels and on Magnetic Resonance Angiography (MRA) brain scans.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:01:29 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kozi\u0144ski", "Mateusz", ""], ["Mosinska", "Agata", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1811.10515", "submitter": "Xintao Wang", "authors": "Xintao Wang, Ke Yu, Chao Dong, Xiaoou Tang, Chen Change Loy", "title": "Deep Network Interpolation for Continuous Imagery Effect Transition", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network has demonstrated its capability of learning\na deterministic mapping for the desired imagery effect. However, the large\nvariety of user flavors motivates the possibility of continuous transition\namong different output effects. Unlike existing methods that require a specific\ndesign to achieve one particular transition (e.g., style transfer), we propose\na simple yet universal approach to attain a smooth control of diverse imagery\neffects in many low-level vision tasks, including image restoration,\nimage-to-image translation, and style transfer. Specifically, our method,\nnamely Deep Network Interpolation (DNI), applies linear interpolation in the\nparameter space of two or more correlated networks. A smooth control of imagery\neffects can be achieved by tweaking the interpolation coefficients. In addition\nto DNI and its broad applications, we also investigate the mechanism of network\ninterpolation from the perspective of learned filters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:14:27 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Xintao", ""], ["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1811.10519", "submitter": "Attila Szab\\'o", "authors": "Attila Szab\\'o, Paolo Favaro", "title": "Unsupervised 3D Shape Learning from Image Collections in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to learn the 3D surface of objects directly from a\ncollection of images. Previous work achieved this capability by exploiting\nadditional manual annotation, such as object pose, 3D surface templates,\ntemporal continuity of videos, manually selected landmarks, and\nforeground/background masks. In contrast, our method does not make use of any\nsuch annotation. Rather, it builds a generative model, a convolutional neural\nnetwork, which, given a noise vector sample, outputs the 3D surface and texture\nof an object and a background image. These 3 components combined with an\nadditional random viewpoint vector are then fed to a differential renderer to\nproduce a view of the sampled object and background. Our general principle is\nthat if the output of the renderer, the generated image, is realistic, then its\ninput, the generated 3D and texture, should also be realistic. To achieve\nrealism, the generative model is trained adversarially against a discriminator\nthat tries to distinguish between the output of the renderer and real images\nfrom the given data set. Moreover, our generative model can be paired with an\nencoder and trained as an autoencoder, to automatically extract the 3D shape,\ntexture and pose of the object in an image. Our trained generative model and\nencoder show promising results both on real and synthetic data, which\ndemonstrate for the first time that fully unsupervised 3D learning from image\ncollections is possible.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:21:30 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:50:01 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Szab\u00f3", "Attila", ""], ["Favaro", "Paolo", ""]]}, {"id": "1811.10520", "submitter": "Yusuf Roohani", "authors": "Yusuf H. Roohani, Noor Sajid, Pranava Madhyastha, Cathy J. Price,\n  Thomas M. H. Hope", "title": "Predicting Language Recovery after Stroke with Convolutional Networks on\n  Stitched MRI", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/144", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One third of stroke survivors have language difficulties. Emerging evidence\nsuggests that their likelihood of recovery depends mainly on the damage to\nlanguage centers. Thus previous research for predicting language recovery\npost-stroke has focused on identifying damaged regions of the brain. In this\npaper, we introduce a novel method where we only make use of stitched\n2-dimensional cross-sections of raw MRI scans in a deep convolutional neural\nnetwork setup to predict language recovery post-stroke. Our results show: a)\nthe proposed model that only uses MRI scans has comparable performance to\nmodels that are dependent on lesion specific information; b) the features\nlearned by our model are complementary to the lesion specific information and\nthe combination of both appear to outperform previously reported results in\nsimilar settings. We further analyse the CNN model for understanding regions in\nbrain that are responsible for arriving at these predictions using gradient\nbased saliency maps. Our findings are in line with previous lesion studies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:23:28 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Roohani", "Yusuf H.", ""], ["Sajid", "Noor", ""], ["Madhyastha", "Pranava", ""], ["Price", "Cathy J.", ""], ["Hope", "Thomas M. H.", ""]]}, {"id": "1811.10524", "submitter": "Morteza Rezanejad", "authors": "Morteza Rezanejad, Gabriel Downs, John Wilder, Dirk B. Walther, Allan\n  Jepson, Sven Dickinson, and Kaleem Siddiqi", "title": "Scene Categorization from Contours: Medial Axis Based Salience Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision community has witnessed recent advances in scene\ncategorization from images, with the state-of-the art systems now achieving\nimpressive recognition rates on challenging benchmarks such as the Places365\ndataset. Such systems have been trained on photographs which include color,\ntexture and shading cues. The geometry of shapes and surfaces, as conveyed by\nscene contours, is not explicitly considered for this task. Remarkably, humans\ncan accurately recognize natural scenes from line drawings, which consist\nsolely of contour-based shape cues. Here we report the first computer vision\nstudy on scene categorization of line drawings derived from popular databases\nincluding an artist scene database, MIT67, and Places365. Specifically, we use\noff-the-shelf pre-trained CNNs to perform scene classification given only\ncontour information as input and find performance levels well above chance. We\nalso show that medial-axis based contour salience methods can be used to select\nmore informative subsets of contour pixels and that the variation in CNN\nclassification performance on various choices for these subsets is\nqualitatively similar to that observed in human performance. Moreover, when the\nsalience measures are used to weight the contours, as opposed to pruning them,\nwe find that these weights boost our CNN performance above that for unweighted\ncontour input. That is, the medial axis based salience weights appear to add\nuseful information that is not available when CNNs are trained to use contours\nalone.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:27:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Rezanejad", "Morteza", ""], ["Downs", "Gabriel", ""], ["Wilder", "John", ""], ["Walther", "Dirk B.", ""], ["Jepson", "Allan", ""], ["Dickinson", "Sven", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "1811.10541", "submitter": "Florian Bernard", "authors": "Florian Bernard, Johan Thunberg, Paul Swoboda, Christian Theobalt", "title": "Higher-order Projected Power Iterations for Scalable Multi-Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matching of multiple objects (e.g. shapes or images) is a fundamental\nproblem in vision and graphics. In order to robustly handle ambiguities, noise\nand repetitive patterns in challenging real-world settings, it is essential to\ntake geometric consistency between points into account. Computationally, the\nmulti-matching problem is difficult. It can be phrased as simultaneously\nsolving multiple (NP-hard) quadratic assignment problems (QAPs) that are\ncoupled via cycle-consistency constraints. The main limitations of existing\nmulti-matching methods are that they either ignore geometric consistency and\nthus have limited robustness, or they are restricted to small-scale problems\ndue to their (relatively) high computational cost. We address these\nshortcomings by introducing a Higher-order Projected Power Iteration method,\nwhich is (i) efficient and scales to tens of thousands of points, (ii)\nstraightforward to implement, (iii) able to incorporate geometric consistency,\n(iv) guarantees cycle-consistent multi-matchings, and (iv) comes with\ntheoretical convergence guarantees. Experimentally we show that our approach is\nsuperior to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:44:48 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 09:11:59 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Bernard", "Florian", ""], ["Thunberg", "Johan", ""], ["Swoboda", "Paul", ""], ["Theobalt", "Christian", ""]]}, {"id": "1811.10551", "submitter": "Deng Weijian", "authors": "Weijian Deng, Liang Zheng, Qixiang Ye, Yi Yang, Jianbin Jiao", "title": "Similarity-preserving Image-image Domain Adaptation for Person\n  Re-identification", "comments": "14 pages, 7 tables, 14 figures, this version is not fully edited and\n  will be updated soon. arXiv admin note: text overlap with arXiv:1711.07027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the domain adaptation problem in person\nre-identification (re-ID) under a \"learning via translation\" framework,\nconsisting of two components, 1) translating the labeled images from the source\nto the target domain in an unsupervised manner, 2) learning a re-ID model using\nthe translated images. The objective is to preserve the underlying human\nidentity information after image translation, so that translated images with\nlabels are effective for feature learning on the target domain. To this end, we\npropose a similarity preserving generative adversarial network (SPGAN) and its\nend-to-end trainable version, eSPGAN. Both aiming at similarity preserving,\nSPGAN enforces this property by heuristic constraints, while eSPGAN does so by\noptimally facilitating the re-ID model learning. More specifically, SPGAN\nseparately undertakes the two components in the \"learning via translation\"\nframework. It first preserves two types of unsupervised similarity, namely,\nself-similarity of an image before and after translation, and\ndomain-dissimilarity of a translated source image and a target image. It then\nlearns a re-ID model using existing networks. In comparison, eSPGAN seamlessly\nintegrates image translation and re-ID model learning. During the end-to-end\ntraining of eSPGAN, re-ID learning guides image translation to preserve the\nunderlying identity information of an image. Meanwhile, image translation\nimproves re-ID learning by providing identity-preserving training samples of\nthe target domain style. In the experiment, we show that identities of the fake\nimages generated by SPGAN and eSPGAN are well preserved. Based on this, we\nreport the new state-of-the-art domain adaptation results on two large-scale\nperson re-ID datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:56:32 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 12:11:01 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Deng", "Weijian", ""], ["Zheng", "Liang", ""], ["Ye", "Qixiang", ""], ["Yang", "Yi", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1811.10553", "submitter": "Alvaro Ulloa Cerna", "authors": "Alvaro Ulloa, Linyuan Jing, Christopher W Good, David P vanMaanen,\n  Sushravya Raghunath, Jonathan D Suever, Christopher D Nevius, Gregory J\n  Wehner, Dustin Hartzel, Joseph B Leader, Amro Alsaid, Aalpen A Patel, H\n  Lester Kirchner, Marios S Pattichis, Christopher M Haggerty, Brandon K\n  Fornwalt", "title": "A deep neural network to enhance prediction of 1-year mortality using\n  echocardiographic videos of the heart", "comments": "We updated results with improved performance after dropout bug in\n  tensorflow v1.12. We also added learning curves showing promise in video\n  model with more samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future clinical events helps physicians guide appropriate\nintervention. Machine learning has tremendous promise to assist physicians with\npredictions based on the discovery of complex patterns from historical data,\nsuch as large, longitudinal electronic health records (EHR). This study is a\nfirst attempt to demonstrate such capabilities using raw echocardiographic\nvideos of the heart. We show that a large dataset of 723,754\nclinically-acquired echocardiographic videos (~45 million images) linked to\nlongitudinal follow-up data in 27,028 patients can be used to train a deep\nneural network to predict 1-year mortality with good accuracy (area under the\ncurve (AUC) in an independent test set = 0.839). Prediction accuracy was\nfurther improved by adding EHR data (AUC = 0.858). Finally, we demonstrate that\nthe trained neural network was more accurate in mortality prediction than two\nexpert cardiologists. These results highlight the potential of neural networks\nto add new power to clinical predictions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:58:57 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 20:36:00 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Ulloa", "Alvaro", ""], ["Jing", "Linyuan", ""], ["Good", "Christopher W", ""], ["vanMaanen", "David P", ""], ["Raghunath", "Sushravya", ""], ["Suever", "Jonathan D", ""], ["Nevius", "Christopher D", ""], ["Wehner", "Gregory J", ""], ["Hartzel", "Dustin", ""], ["Leader", "Joseph B", ""], ["Alsaid", "Amro", ""], ["Patel", "Aalpen A", ""], ["Kirchner", "H Lester", ""], ["Pattichis", "Marios S", ""], ["Haggerty", "Christopher M", ""], ["Fornwalt", "Brandon K", ""]]}, {"id": "1811.10559", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Vinay Kumar Verma, Piyush Rai and Vinay P. Namboodiri", "title": "Leveraging Filter Correlations for Deep Model Compression", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a filter correlation based model compression approach for deep\nconvolutional neural networks. Our approach iteratively identifies pairs of\nfilters with the largest pairwise correlations and drops one of the filters\nfrom each such pair. However, instead of discarding one of the filters from\neach such pair na\\\"{i}vely, the model is re-optimized to make the filters in\nthese pairs maximally correlated, so that discarding one of the filters from\nthe pair results in minimal information loss. Moreover, after discarding the\nfilters in each round, we further finetune the model to recover from the\npotential small loss incurred by the compression. We evaluate our proposed\napproach using a comprehensive set of experiments and ablation studies. Our\ncompression method yields state-of-the-art FLOPs compression rates on various\nbenchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving\nexcellent predictive performance for tasks such as object detection on\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:05:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:16:50 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Singh", "Pravendra", ""], ["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1811.10564", "submitter": "Chenyu You", "authors": "Chenyu You, Linfeng Yang, Yi Zhang, Ge Wang", "title": "Low-Dose CT via Deep CNN with Skip Connection and Network in Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in computed tomography (CT) is how to minimize patient\nradiation exposure without compromising image quality and diagnostic\nperformance. The use of deep convolutional (Conv) neural networks for noise\nreduction in Low-Dose CT (LDCT) images has recently shown a great potential in\nthis important application. In this paper, we present a highly efficient and\neffective neural network model for LDCT image noise reduction. Specifically, to\ncapture local anatomical features we integrate Deep Convolutional Neural\nNetworks (CNNs) and Skip connection layers for feature extraction. Also, we\nintroduce parallelized $1\\times 1$ CNN, called Network in Network, to lower the\ndimensionality of the output from the previous layer, achieving faster\ncomputational speed at less feature loss. To optimize the performance of the\nnetwork, we adopt a Wasserstein generative adversarial network (WGAN)\nframework. Quantitative and qualitative comparisons demonstrate that our\nproposed network model can produce images with lower noise and more structural\ndetails than state-of-the-art noise-reduction methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:08:44 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 02:53:26 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["You", "Chenyu", ""], ["Yang", "Linfeng", ""], ["Zhang", "Yi", ""], ["Wang", "Ge", ""]]}, {"id": "1811.10565", "submitter": "Alexander Gomez Villa A. Gomez-Villa", "authors": "Alexander Gomez-Villa, Adri\\'an Mart\\'in, Javier Vazquez-Corral,\n  Marcelo Bertalm\\'io", "title": "Convolutional Neural Networks Deceived by Visual Illusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual illusions teach us that what we see is not always what it is\nrepresented in the physical world. Its special nature make them a fascinating\ntool to test and validate any new vision model proposed. In general, current\nvision models are based on the concatenation of linear convolutions and\nnon-linear operations. In this paper we get inspiration from the similarity of\nthis structure with the operations present in Convolutional Neural Networks\n(CNNs). This motivated us to study if CNNs trained for low-level visual tasks\nare deceived by visual illusions. In particular, we show that CNNs trained for\nimage denoising, image deblurring, and computational color constancy are able\nto replicate the human response to visual illusions, and that the extent of\nthis replication varies with respect to variation in architecture and spatial\npattern size. We believe that this CNNs behaviour appears as a by-product of\nthe training for the low level vision tasks of denoising, color constancy or\ndeblurring. Our work opens a new bridge between human perception and CNNs: in\norder to obtain CNNs that better replicate human behaviour, we may need to\nstart aiming for them to better replicate visual illusions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:09:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Gomez-Villa", "Alexander", ""], ["Mart\u00edn", "Adri\u00e1n", ""], ["Vazquez-Corral", "Javier", ""], ["Bertalm\u00edo", "Marcelo", ""]]}, {"id": "1811.10575", "submitter": "Pallabi Ghosh", "authors": "Pallabi Ghosh, Yi Yao, Larry S. Davis, Ajay Divakaran", "title": "Stacked Spatio-Temporal Graph Convolutional Networks for Action\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel Stacked Spatio-Temporal Graph Convolutional Networks\n(Stacked-STGCN) for action segmentation, i.e., predicting and localizing a\nsequence of actions over long videos. We extend the Spatio-Temporal Graph\nConvolutional Network (STGCN) originally proposed for skeleton-based action\nrecognition to enable nodes with different characteristics (e.g., scene, actor,\nobject, action, etc.), feature descriptors with varied lengths, and arbitrary\ntemporal edge connections to account for large graph deformation commonly\nassociated with complex activities. We further introduce the stacked hourglass\narchitecture to STGCN to leverage the advantages of an encoder-decoder design\nfor improved generalization performance and localization accuracy. We explore\nvarious descriptors such as frame-level VGG, segment-level I3D, RCNN-based\nobject, etc. as node descriptors to enable action segmentation based on joint\ninference over comprehensive contextual information. We show results on CAD120\n(which provides pre-computed node features and edge weights for fair\nperformance comparison across algorithms) as well as a more complex real-world\nactivity dataset, Charades. Our Stacked-STGCN in general achieves 4.0%\nperformance improvement over the best reported results in F1 score on CAD120\nand 1.3% in mAP on Charades using VGG features.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:28:24 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:32:30 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 18:52:24 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 15:53:41 GMT"}, {"version": "v5", "created": "Sun, 14 Apr 2019 18:15:23 GMT"}, {"version": "v6", "created": "Sun, 2 Jun 2019 18:21:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ghosh", "Pallabi", ""], ["Yao", "Yi", ""], ["Davis", "Larry S.", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1811.10582", "submitter": "Farley Lai", "authors": "Ning Xie, Farley Lai, Derek Doran, Asim Kadav", "title": "Visual Entailment Task for Visually-Grounded Language Learning", "comments": "4 pages, accepted by Visually Grounded Interaction and Language\n  (ViGIL) workshop in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new inference task - Visual Entailment (VE) - which differs\nfrom traditional Textual Entailment (TE) tasks whereby a premise is defined by\nan image, rather than a natural language sentence as in TE tasks. A novel\ndataset SNLI-VE (publicly available at https://github.com/necla-ml/SNLI-VE) is\nproposed for VE tasks based on the Stanford Natural Language Inference corpus\nand Flickr30k. We introduce a differentiable architecture called the\nExplainable Visual Entailment model (EVE) to tackle the VE problem. EVE and\nseveral other state-of-the-art visual question answering (VQA) based models are\nevaluated on the SNLI-VE dataset, facilitating grounded language understanding\nand providing insights on how modern VQA based models perform.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:37:25 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 03:07:54 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Xie", "Ning", ""], ["Lai", "Farley", ""], ["Doran", "Derek", ""], ["Kadav", "Asim", ""]]}, {"id": "1811.10597", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B.\n  Tenenbaum, William T. Freeman, Antonio Torralba", "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial\n  Networks", "comments": "18 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:59:07 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 22:56:10 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Strobelt", "Hendrik", ""], ["Zhou", "Bolei", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""], ["Torralba", "Antonio", ""]]}, {"id": "1811.10636", "submitter": "Michael S. Ryoo", "authors": "AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo", "title": "Evolving Space-Time Neural Architectures for Videos", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for finding video CNN architectures that capture rich\nspatio-temporal information in videos. Previous work, taking advantage of 3D\nconvolutions, obtained promising results by manually designing video CNN\narchitectures. We here develop a novel evolutionary search algorithm that\nautomatically explores models with different types and combinations of layers\nto jointly learn interactions between spatial and temporal aspects of video\nrepresentations. We demonstrate the generality of this algorithm by applying it\nto two meta-architectures, obtaining new architectures superior to manually\ndesigned architectures. Further, we propose a new component, the iTGM layer,\nwhich more efficiently utilizes its parameters to allow learning of space-time\ninteractions over longer time horizons. The iTGM layer is often preferred by\nthe evolutionary algorithm and allows building cost-efficient networks. The\nproposed approach discovers new and diverse video architectures that were\npreviously unknown. More importantly they are both more accurate and faster\nthan prior models, and outperform the state-of-the-art results on multiple\ndatasets we test, including HMDB, Kinetics, and Moments in Time. We will open\nsource the code and models, to encourage future model development.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:00:12 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 18:17:46 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Toshev", "Alexander", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1811.10648", "submitter": "Xiao Ma", "authors": "Xiao Ma, Lina Mezghani, Kimberly Wilber, Hui Hong, Robinson Piramuthu,\n  Mor Naaman, Serge Belongie", "title": "Understanding Image Quality and Trust in Peer-to-Peer Marketplaces", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As any savvy online shopper knows, second-hand peer-to-peer marketplaces are\nfilled with images of mixed quality. How does image quality impact marketplace\noutcomes, and can quality be automatically predicted? In this work, we\nconducted a large-scale study on the quality of user-generated images in\npeer-to-peer marketplaces. By gathering a dataset of common second-hand\nproducts (~75,000 images) and annotating a subset with human-labeled quality\njudgments, we were able to model and predict image quality with decent accuracy\n(~87%). We then conducted two studies focused on understanding the relationship\nbetween these image quality scores and two marketplace outcomes: sales and\nperceived trustworthiness. We show that image quality is associated with higher\nlikelihood that an item will be sold, though other factors such as view count\nwere better predictors of sales. Nonetheless, we show that high quality\nuser-generated images selected by our models outperform stock imagery in\neliciting perceptions of trust from users. Our findings can inform the design\nof future marketplaces and guide potential sellers to take better product\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:10:34 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ma", "Xiao", ""], ["Mezghani", "Lina", ""], ["Wilber", "Kimberly", ""], ["Hong", "Hui", ""], ["Piramuthu", "Robinson", ""], ["Naaman", "Mor", ""], ["Belongie", "Serge", ""]]}, {"id": "1811.10649", "submitter": "Minghai Qin", "authors": "Minghai Qin, Dejan Vucinic", "title": "Noisy Computations during Inference: Harmful or Helpful?", "comments": "20 pages, 11 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two aspects of noisy computations during inference. The first aspect\nis how to mitigate their side effects for naturally trained deep learning\nsystems. One of the motivations for looking into this problem is to reduce the\nhigh power cost of conventional computing of neural networks through the use of\nanalog neuromorphic circuits. Traditional GPU/CPU-centered deep learning\narchitectures exhibit bottlenecks in power-restricted applications (e.g.,\nembedded systems). The use of specialized neuromorphic circuits, where analog\nsignals passed through memory-cell arrays are sensed to accomplish\nmatrix-vector multiplications, promises large power savings and speed gains but\nbrings with it the problems of limited precision of computations and\nunavoidable analog noise. We manage to improve inference accuracy from 21.1% to\n99.5% for MNIST images, from 29.9% to 89.1% for CIFAR10, and from 15.5% to\n89.6% for MNIST stroke sequences with the presence of strong noise (with\nsignal-to-noise power ratio being 0 dB) by noise-injected training and a voting\nmethod. This observation promises neural networks that are insensitive to\ninference noise, which reduces the quality requirements on neuromorphic\ncircuits and is crucial for their practical usage. The second aspect is how to\nutilize the noisy inference as a defensive architecture against black-box\nadversarial attacks. During inference, by injecting proper noise to signals in\nthe neural networks, the robustness of adversarially-trained neural networks\nagainst black-box attacks has been further enhanced by 0.5% and 1.13% for two\nadversarially trained models for MNIST and CIFAR10, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:18:18 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Qin", "Minghai", ""], ["Vucinic", "Dejan", ""]]}, {"id": "1811.10652", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "Show, Control and Tell: A Framework for Generating Controllable and\n  Grounded Captions", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current captioning approaches can describe images using black-box\narchitectures whose behavior is hardly controllable and explainable from the\nexterior. As an image can be described in infinite ways depending on the goal\nand the context at hand, a higher degree of controllability is needed to apply\ncaptioning algorithms in complex scenarios. In this paper, we introduce a novel\nframework for image captioning which can generate diverse descriptions by\nallowing both grounding and controllability. Given a control signal in the form\nof a sequence or set of image regions, we generate the corresponding caption\nthrough a recurrent architecture which predicts textual chunks explicitly\ngrounded on regions, following the constraints of the given control.\nExperiments are conducted on Flickr30k Entities and on COCO Entities, an\nextended version of COCO in which we add grounding annotations collected in a\nsemi-automatic manner. Results demonstrate that our method achieves state of\nthe art performances on controllable image captioning, in terms of caption\nquality and diversity. Code and annotations are publicly available at:\nhttps://github.com/aimagelab/show-control-and-tell.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:23:33 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 14:18:54 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 07:58:20 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1811.10666", "submitter": "Matteo Tomei", "authors": "Matteo Tomei, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "Art2Real: Unfolding the Reality of Artworks via Semantically-Aware\n  Image-to-Image Translation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applicability of computer vision to real paintings and artworks has been\nrarely investigated, even though a vast heritage would greatly benefit from\ntechniques which can understand and process data from the artistic domain. This\nis partially due to the small amount of annotated artistic data, which is not\neven comparable to that of natural images captured by cameras. In this paper,\nwe propose a semantic-aware architecture which can translate artworks to\nphoto-realistic visualizations, thus reducing the gap between visual features\nof artistic and realistic data. Our architecture can generate natural images by\nretrieving and learning details from real photos through a similarity matching\nstrategy which leverages a weakly-supervised semantic understanding of the\nscene. Experimental results show that the proposed technique leads to increased\nrealism and to a reduction in domain shift, which improves the performance of\npre-trained architectures for classification, detection, and segmentation. Code\nis publicly available at: https://github.com/aimagelab/art2real.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:51:47 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 09:18:30 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 09:14:40 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Tomei", "Matteo", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1811.10669", "submitter": "Christopher Bowles", "authors": "Christopher Bowles, Roger Gunn, Alexander Hammers, Daniel Rueckert", "title": "GANsfer Learning: Combining labelled and unlabelled data for GAN based\n  data augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is a domain which suffers from a paucity of manually\nannotated data for the training of learning algorithms. Manually delineating\npathological regions at a pixel level is a time consuming process, especially\nin 3D images, and often requires the time of a trained expert. As a result,\nsupervised machine learning solutions must make do with small amounts of\nlabelled data, despite there often being additional unlabelled data available.\nWhilst of less value than labelled images, these unlabelled images can contain\npotentially useful information. In this paper we propose combining both\nlabelled and unlabelled data within a GAN framework, before using the resulting\nnetwork to produce images for use when training a segmentation network. We\nexplore the task of deep grey matter multi-class segmentation in an AD dataset\nand show that the proposed method leads to a significant improvement in\nsegmentation results, particularly in cases where the amount of labelled data\nis restricted. We show that this improvement is largely driven by a greater\nability to segment the structures known to be the most affected by AD, thereby\ndemonstrating the benefits of exposing the system to more examples of\npathological anatomical variation. We also show how a shift in domain of the\ntraining data from young and healthy towards older and more pathological\nexamples leads to better segmentations of the latter cases, and that this leads\nto a significant improvement in the ability for the computed segmentations to\nstratify cases of AD.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 20:08:44 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bowles", "Christopher", ""], ["Gunn", "Roger", ""], ["Hammers", "Alexander", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1811.10673", "submitter": "Christos Bampis", "authors": "Sungsoo Kim and Jin Soo Park and Christos G. Bampis and Jaeseong Lee\n  and Mia K. Markey and Alexandros G. Dimakis and Alan C. Bovik", "title": "Adversarial Video Compression Guided by Soft Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a video compression framework using conditional Generative\nAdversarial Networks (GANs). We rely on two encoders: one that deploys a\nstandard video codec and another which generates low-level maps via a pipeline\nof down-sampling, a newly devised soft edge detector, and a novel lossless\ncompression scheme. For decoding, we use a standard video decoder as well as a\nneural network based one, which is trained using a conditional GAN. Recent\n\"deep\" approaches to video compression require multiple videos to pre-train\ngenerative networks to conduct interpolation. In contrast to this prior work,\nour scheme trains a generative decoder on pairs of a very limited number of key\nframes taken from a single video and corresponding low-level maps. The trained\ndecoder produces reconstructed frames relying on a guidance of low-level maps,\nwithout any interpolation. Experiments on a diverse set of 131 videos\ndemonstrate that our proposed GAN-based compression engine achieves much higher\nquality reconstructions at very low bitrates than prevailing standard codecs\nsuch as H.264 or HEVC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 20:22:04 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kim", "Sungsoo", ""], ["Park", "Jin Soo", ""], ["Bampis", "Christos G.", ""], ["Lee", "Jaeseong", ""], ["Markey", "Mia K.", ""], ["Dimakis", "Alexandros G.", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1811.10681", "submitter": "Titus Cieslewski", "authors": "Titus Cieslewski and Michael Bloesch and Davide Scaramuzza", "title": "Matching Features without Descriptors: Implicitly Matched Interest\n  Points", "comments": "10 pages without references, accepted for publication at the British\n  Machine Vision Conference (BMVC), Cardiff, 2019. v2 contains additional\n  results, and a bug in the evaluation of LF-NET has been fixed", "journal-ref": "British Machine Vision Conference (BMVC), Cardiff, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction and matching of interest points is a prerequisite for many\ngeometric computer vision problems. Traditionally, matching has been achieved\nby assigning descriptors to interest points and matching points that have\nsimilar descriptors. In this paper, we propose a method by which interest\npoints are instead already implicitly matched at detection time. With this,\ndescriptors do not need to be calculated, stored, communicated, or matched any\nmore. This is achieved by a convolutional neural network with multiple output\nchannels and can be thought of as a collection of a variety of detectors, each\nspecialized to specific visual features. This paper describes how to design and\ntrain such a network in a way that results in successful relative pose\nestimation performance despite the limitation on interest point count. While\nthe overall matching score is slightly lower than with traditional methods, the\napproach is descriptor free and thus enables localization systems with a\nsignificantly smaller memory footprint and multi-agent localization systems\nwith lower bandwidth requirements. The network also outputs the confidence for\na specific interest point resulting in a valid match. We evaluate performance\nrelative to state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 20:45:58 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 13:17:11 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Cieslewski", "Titus", ""], ["Bloesch", "Michael", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1811.10696", "submitter": "Mengshi Qi", "authors": "Mengshi Qi, Weijian Li, Zhengyuan Yang, Yunhong Wang, Jiebo Luo", "title": "Attentive Relational Networks for Mapping Images to Scene Graphs", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation refers to the task of automatically mapping an image\ninto a semantic structural graph, which requires correctly labeling each\nextracted object and their interaction relationships. Despite the recent\nsuccess in object detection using deep learning techniques, inferring complex\ncontextual relationships and structured graph representations from visual data\nremains a challenging topic. In this study, we propose a novel Attentive\nRelational Network that consists of two key modules with an object detection\nbackbone to approach this problem. The first module is a semantic\ntransformation module utilized to capture semantic embedded relation features,\nby translating visual features and linguistic features into a common semantic\nspace. The other module is a graph self-attention module introduced to embed a\njoint graph representation through assigning various importance weights to\nneighboring nodes. Finally, accurate scene graphs are produced by the relation\ninference module to recognize all entities and the corresponding relations. We\nevaluate our proposed method on the widely-adopted Visual Genome Dataset, and\nthe results demonstrate the effectiveness and superiority of our model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:36:49 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 13:11:09 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Qi", "Mengshi", ""], ["Li", "Weijian", ""], ["Yang", "Zhengyuan", ""], ["Wang", "Yunhong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1811.10698", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz", "title": "LSTA: Long Short-Term Attention for Egocentric Action Recognition", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric activity recognition is one of the most challenging tasks in video\nanalysis. It requires a fine-grained discrimination of small objects and their\nmanipulation. While some methods base on strong supervision and attention\nmechanisms, they are either annotation consuming or do not take spatio-temporal\npatterns into account. In this paper we propose LSTA as a mechanism to focus on\nfeatures from spatial relevant parts while attention is being tracked smoothly\nacross the video sequence. We demonstrate the effectiveness of LSTA on\negocentric activity recognition with an end-to-end trainable two-stream\narchitecture, achieving state of the art performance on four standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:40:03 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 15:22:34 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 09:39:00 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "1811.10699", "submitter": "Shruti Vyas", "authors": "Shruti Vyas, Yogesh S Rawat, Mubarak Shah", "title": "Time-Aware and View-Aware Video Rendering for Unsupervised\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success in deep learning has lead to various effective\nrepresentation learning methods for videos. However, the current approaches for\nvideo representation require large amount of human labeled datasets for\neffective learning. We present an unsupervised representation learning\nframework to encode scene dynamics in videos captured from multiple viewpoints.\nThe proposed framework has two main components: Representation Learning Network\n(RL-NET), which learns a representation with the help of Blending Network\n(BL-NET), and Video Rendering Network (VR-NET), which is used for video\nsynthesis. The framework takes as input video clips from different viewpoints\nand time, learns an internal representation and uses this representation to\nrender a video clip from an arbitrary given viewpoint and time. The ability of\nthe proposed network to render video frames from arbitrary viewpoints and time\nenable it to learn a meaningful and robust representation of the scene\ndynamics. We demonstrate the effectiveness of the proposed method in rendering\nview-aware as well as time-aware video clips on two different real-world\ndatasets including UCF-101 and NTU-RGB+D. To further validate the effectiveness\nof the learned representation, we use it for the task of view-invariant\nactivity classification where we observe a significant improvement (~26%) in\nthe performance on NTU-RGB+D dataset compared to the existing state-of-the art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:40:38 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 15:24:39 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Vyas", "Shruti", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "1811.10714", "submitter": "Taylor Killian", "authors": "Justin A. Goodwin, Olivia M. Brown, Taylor W. Killian, Sung-Hyun Son", "title": "Learning Robust Representations for Automatic Target Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio frequency (RF) sensors are used alongside other sensing modalities to\nprovide rich representations of the world. Given the high variability of\ncomplex-valued target responses, RF systems are susceptible to attacks masking\ntrue target characteristics from accurate identification. In this work, we\nevaluate different techniques for building robust classification architectures\nexploiting learned physical structure in received synthetic aperture radar\nsignals of simulated 3D targets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:08:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Goodwin", "Justin A.", ""], ["Brown", "Olivia M.", ""], ["Killian", "Taylor W.", ""], ["Son", "Sung-Hyun", ""]]}, {"id": "1811.10716", "submitter": "Jianyu Wang", "authors": "Jianyu Wang and Haichao Zhang", "title": "Bilateral Adversarial Training: Towards Fast Training of More Robust\n  Models Against Adversarial Attacks", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study fast training of adversarially robust models. From\nthe analyses of the state-of-the-art defense method, i.e., the multi-step\nadversarial training, we hypothesize that the gradient magnitude links to the\nmodel robustness. Motivated by this, we propose to perturb both the image and\nthe label during training, which we call Bilateral Adversarial Training (BAT).\nTo generate the adversarial label, we derive an closed-form heuristic solution.\nTo generate the adversarial image, we use one-step targeted attack with the\ntarget label being the most confusing class. In the experiment, we first show\nthat random start and the most confusing target attack effectively prevent the\nlabel leaking and gradient masking problem. Then coupled with the adversarial\nlabel part, our model significantly improves the state-of-the-art results. For\nexample, against PGD100 white-box attack with cross-entropy loss, on CIFAR10,\nwe achieve 63.7\\% versus 47.2\\%; on SVHN, we achieve 59.1\\% versus 42.1\\%. At\nlast, the experiment on the very (computationally) challenging ImageNet dataset\nfurther demonstrates the effectiveness of our fast method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:15:44 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 03:13:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Wang", "Jianyu", ""], ["Zhang", "Haichao", ""]]}, {"id": "1811.10719", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato, Tatsuya Harada", "title": "Learning View Priors for Single-view 3D Reconstruction", "comments": "CVPR 2019. Project page:\n  http://hiroharu-kato.com/projects_en/view_prior_learning.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is some ambiguity in the 3D shape of an object when the number of\nobserved views is small. Because of this ambiguity, although a 3D object\nreconstructor can be trained using a single view or a few views per object,\nreconstructed shapes only fit the observed views and appear incorrect from the\nunobserved viewpoints. To reconstruct shapes that look reasonable from any\nviewpoint, we propose to train a discriminator that learns prior knowledge\nregarding possible views. The discriminator is trained to distinguish the\nreconstructed views of the observed viewpoints from those of the unobserved\nviewpoints. The reconstructor is trained to correct unobserved views by fooling\nthe discriminator. Our method outperforms current state-of-the-art methods on\nboth synthetic and natural image datasets; this validates the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:23:44 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 15:18:48 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.10720", "submitter": "Justus Thies", "authors": "Justus Thies and Michael Zollh\\\"ofer and Christian Theobalt and Marc\n  Stamminger and Matthias Nie{\\ss}ner", "title": "IGNOR: Image-guided Neural Object Rendering", "comments": "Video: https://youtu.be/s79HG9yn7QM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learned image-guided rendering technique that combines the\nbenefits of image-based rendering and GAN-based image synthesis. The goal of\nour method is to generate photo-realistic re-renderings of reconstructed\nobjects for virtual and augmented reality applications (e.g., virtual\nshowrooms, virtual tours \\& sightseeing, the digital inspection of historical\nartifacts). A core component of our work is the handling of view-dependent\neffects. Specifically, we directly train an object-specific deep neural network\nto synthesize the view-dependent appearance of an object. As input data we are\nusing an RGB video of the object. This video is used to reconstruct a proxy\ngeometry of the object via multi-view stereo. Based on this 3D proxy, the\nappearance of a captured view can be warped into a new target view as in\nclassical image-based rendering. This warping assumes diffuse surfaces, in case\nof view-dependent effects, such as specular highlights, it leads to artifacts.\nTo this end, we propose EffectsNet, a deep neural network that predicts\nview-dependent effects. Based on these estimations, we are able to convert\nobserved images to diffuse images. These diffuse images can be projected into\nother views. In the target view, our pipeline reinserts the new view-dependent\neffects. To composite multiple reprojected images to a final output, we learn a\ncomposition network that outputs photo-realistic results. Using this\nimage-guided approach, the network does not have to allocate capacity on\n``remembering'' object appearance, instead it learns how to combine the\nappearance of captured images. We demonstrate the effectiveness of our approach\nboth qualitatively and quantitatively on synthetic as well as on real data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:24:25 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 15:30:46 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""], ["Stamminger", "Marc", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1811.10725", "submitter": "Yuhe Jin", "authors": "Baptiste Angles and Yuhe Jin and Simon Kornblith and Andrea\n  Tagliasacchi and Kwang Moo Yi", "title": "MIST: Multiple Instance Spatial Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep network that can be trained to tackle image reconstruction\nand classification problems that involve detection of multiple object\ninstances, without any supervision regarding their whereabouts. The network\nlearns to extract the most significant top-K patches, and feeds these patches\nto a task-specific network -- e.g., auto-encoder or classifier -- to solve a\ndomain specific problem. The challenge in training such a network is the\nnon-differentiable top-K selection process. To address this issue, we lift the\ntraining optimization problem by treating the result of top-K selection as a\nslack variable, resulting in a simple, yet effective, multi-stage training. Our\nmethod is able to learn to detect recurrent structures in the training dataset\nby learning to reconstruct images. It can also learn to localize structures\nwhen only knowledge on the occurrence of the object is provided, and in doing\nso it outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:49:20 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 06:11:30 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 00:36:09 GMT"}, {"version": "v4", "created": "Thu, 30 Jan 2020 21:28:53 GMT"}, {"version": "v5", "created": "Fri, 4 Dec 2020 19:19:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Angles", "Baptiste", ""], ["Jin", "Yuhe", ""], ["Kornblith", "Simon", ""], ["Tagliasacchi", "Andrea", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1811.10742", "submitter": "Hou-Ning Hu", "authors": "Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp\n  Kr\\\"ahenb\\\"uhl, Trevor Darrell, Fisher Yu", "title": "Joint Monocular 3D Vehicle Detection and Tracking", "comments": "18 pages, 12 figures. Add supplementary material. Accepted by ICCV\n  2019. Website: https://eborboihuc.github.io/Mono-3DT Code:\n  https://github.com/ucbdrive/3d-vehicle-tracking Video:\n  https://youtu.be/EJAtOCKI31g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle 3D extents and trajectories are critical cues for predicting the\nfuture location of vehicles and planning future agent ego-motion based on those\npredictions. In this paper, we propose a novel online framework for 3D vehicle\ndetection and tracking from monocular videos. The framework can not only\nassociate detections of vehicles in motion over time, but also estimate their\ncomplete 3D bounding box information from a sequence of 2D images captured on a\nmoving platform. Our method leverages 3D box depth-ordering matching for robust\ninstance association and utilizes 3D trajectory prediction for\nre-identification of occluded vehicles. We also design a motion learning module\nbased on an LSTM for more accurate long-term motion extrapolation. Our\nexperiments on simulation, KITTI, and Argoverse datasets show that our 3D\ntracking pipeline offers robust data association and tracking. On Argoverse,\nour image-based method is significantly better for tracking 3D vehicles within\n30 meters than the LiDAR-centric baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:29:46 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 07:24:32 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 08:50:53 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Hu", "Hou-Ning", ""], ["Cai", "Qi-Zhi", ""], ["Wang", "Dequan", ""], ["Lin", "Ji", ""], ["Sun", "Min", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""], ["Yu", "Fisher", ""]]}, {"id": "1811.10762", "submitter": "Chengjiang Long", "authors": "Chengjiang Long, Arslan Basharat, Anthony Hoogs", "title": "A Coarse-to-fine Deep Convolutional Neural Network Framework for Frame\n  Duplication Detection and Localization in Forged Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos can be manipulated by duplicating a sequence of consecutive frames\nwith the goal of concealing or imitating a specific content in the same video.\nIn this paper, we propose a novel coarse-to-fine framework based on deep\nConvolutional Neural Networks to automatically detect and localize such frame\nduplication. First, an I3D network finds coarse-level matches between candidate\nduplicated frame sequences and the corresponding selected original frame\nsequences. Then a Siamese network based on ResNet architecture identifies\nfine-level correspondences between an individual duplicated frame and the\ncorresponding selected frame. We also propose a robust statistical approach to\ncompute a video-level score indicating the likelihood of manipulation or\nforgery. Additionally, for providing manipulation localization information we\ndevelop an inconsistency detector based on the I3D network to distinguish the\nduplicated frames from the selected original frames. Quantified evaluation on\ntwo challenging video forgery datasets clearly demonstrates that this approach\nperforms significantly better than four recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 01:08:05 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 00:52:46 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Long", "Chengjiang", ""], ["Basharat", "Arslan", ""], ["Hoogs", "Anthony", ""]]}, {"id": "1811.10763", "submitter": "Chenglong Li", "authors": "Xiao Wang, Tao Sun, Rui Yang, Chenglong Li, Bin Luo and Jin Tang", "title": "Quality-Aware Multimodal Saliency Detection via Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating various modes of information into the machine learning\nprocedure is becoming a new trend. And data from various source can provide\nmore information than single one no matter they are heterogeneous or\nhomogeneous. Existing deep learning based algorithms usually directly\nconcatenate features from each domain to represent the input data. Seldom of\nthem take the quality of data into consideration which is a key issue in\nrelated multimodal problems. In this paper, we propose an efficient\nquality-aware deep neural network to model the weight of data from each domain\nusing deep reinforcement learning (DRL). Specifically, we take the weighting of\neach domain as a decision-making problem and teach an agent learn to interact\nwith the environment. The agent can tune the weight of each domain through\ndiscrete action selection and obtain a positive reward if the saliency results\nare improved. The target of the agent is to achieve maximum rewards after\nfinished its sequential action selection. We validate the proposed algorithms\non multimodal saliency detection in a coarse-to-fine way. The coarse saliency\nmaps are generated from an encoder-decoder framework which is trained with\ncontent loss and adversarial loss. The final results can be obtained via\nadaptive weighting of maps from each domain. Experiments conducted on two kinds\nof salient object detection benchmarks validated the effectiveness of our\nproposed quality-aware deep neural network.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 01:10:34 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wang", "Xiao", ""], ["Sun", "Tao", ""], ["Yang", "Rui", ""], ["Li", "Chenglong", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1811.10770", "submitter": "Wei Shen", "authors": "Wei Shen and Rujie Liu", "title": "Generating Attention from Classifier Activations for Fine-grained\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in fine-grained recognition utilize attention maps to\nlocalize objects of interest. Although there are many ways to generate\nattention maps, most of them rely on sophisticated loss functions or complex\ntraining processes. In this work, we propose a simple and straightforward\nattention generation model based on the output activations of classifiers. The\nadvantage of our model is that it can be easily trained with image level labels\nand softmax loss functions. More specifically, multiple linear local\nclassifiers are firstly adopted to perform fine-grained classification at each\nlocation of high level CNN feature maps. The attention map is generated by\naggregating and max-pooling the output activations. Then the attention map\nserves as a surrogate target object mask to train those local classifiers,\nsimilar to training models for semantic segmentation. Our model achieves\nstate-of-the-art results on three heavily benchmarked datasets, i.e. 87.9% on\nCUB-200-2011 dataset, 94.1% on Stanford Cars dataset and 92.1% on FGVC-Aircraft\ndataset, demonstrating its effectiveness on fine-grained recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 02:02:50 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Shen", "Wei", ""], ["Liu", "Rujie", ""]]}, {"id": "1811.10771", "submitter": "Ryad Benjamin Benosman", "authors": "T. Leroux and S.-H. Ieng and R. Benosman", "title": "Event-Based Structured Light for Depth Reconstruction using Frequency\n  Tagged Light Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a new method for 3D depth estimation using the output of\nan asynchronous time driven image sensor. In association with a high speed\nDigital Light Processing projection system, our method achieves real-time\nreconstruction of 3D points cloud, up to several hundreds of hertz. Unlike\nstate of the art methodology, we introduce a method that relies on the use of\nfrequency tagged light pattern that make use of the high temporal resolution of\nevent based sensors. This approch eases matching as each pattern unique\nfrequency allow for any easy matching between displayed patterns and the event\nbased sensor. Results are show on real scenes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 02:03:44 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Leroux", "T.", ""], ["Ieng", "S. -H.", ""], ["Benosman", "R.", ""]]}, {"id": "1811.10779", "submitter": "Wei Shen", "authors": "Wei Shen and Rujie Liu", "title": "Tackling Early Sparse Gradients in Softmax Activation Using Leaky\n  Squared Euclidean Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax activation is commonly used to output the probability distribution\nover categories based on certain distance metric. In scenarios like one-shot\nlearning, the distance metric is often chosen to be squared Euclidean distance\nbetween the query sample and the category prototype. This practice works well\nin most time. However, we find that choosing squared Euclidean distance may\ncause distance explosion leading gradients to be extremely sparse in the early\nstage of back propagation. We term this phenomena as the early sparse gradients\nproblem. Though it doesn't deteriorate the convergence of the model, it may set\nup a barrier to further model improvement. To tackle this problem, we propose\nto use leaky squared Euclidean distance to impose a restriction on distances.\nIn this way, we can avoid distance explosion and increase the magnitude of\ngradients. Extensive experiments are conducted on Omniglot and miniImageNet\ndatasets. We show that using leaky squared Euclidean distance can improve\none-shot classification accuracy on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 02:37:03 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Shen", "Wei", ""], ["Liu", "Rujie", ""]]}, {"id": "1811.10787", "submitter": "Yang Feng", "authors": "Yang Feng, Lin Ma, Wei Liu, Jiebo Luo", "title": "Unsupervised Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved great successes on the image captioning\ntask. However, most of the existing models depend heavily on paired\nimage-sentence datasets, which are very expensive to acquire. In this paper, we\nmake the first attempt to train an image captioning model in an unsupervised\nmanner. Instead of relying on manually labeled image-sentence pairs, our\nproposed model merely requires an image set, a sentence corpus, and an existing\nvisual concept detector. The sentence corpus is used to teach the captioning\nmodel how to generate plausible sentences. Meanwhile, the knowledge in the\nvisual concept detector is distilled into the captioning model to guide the\nmodel to recognize the visual concepts in an image. In order to further\nencourage the generated captions to be semantically consistent with the image,\nthe image and caption are projected into a common latent space so that they can\nreconstruct each other. Given that the existing sentence corpora are mainly\ndesigned for linguistic research and are thus with little reference to image\ncontents, we crawl a large-scale image description corpus of two million\nnatural sentences to facilitate the unsupervised image captioning scenario.\nExperimental results show that our proposed model is able to produce quite\npromising results without any caption annotations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:16:20 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 10:57:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Feng", "Yang", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1811.10788", "submitter": "Shirsendu Halder", "authors": "Shirsendu Sukanta Halder, Sanchayan Santra and Bhabatosh Chanda", "title": "Reconstruction Loss Minimized FCN for Single Image Dehazing", "comments": "12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze and fog reduce the visibility of outdoor scenes as a veil like\nsemi-transparent layer appears over the objects. As a result, images captured\nunder such conditions lack contrast. Image dehazing methods try to alleviate\nthis problem by recovering a clear version of the image. In this paper, we\npropose a Fully Convolutional Neural Network based model to recover the clear\nscene radiance by estimating the environmental illumination and the scene\ntransmittance jointly from a hazy image. The method uses a relaxed haze imaging\nmodel to allow for the situations with non-uniform illumination. We have\ntrained the network by minimizing a custom-defined loss that measures the error\nof reconstructing the hazy image in three different ways. Additionally, we use\na multilevel approach to determine the scene transmittance and the\nenvironmental illumination in order to reduce the dependence of the estimate on\nimage scale. Evaluations show that our model performs well compared to the\nexisting state-of-the-art methods. It also verifies the potential of our model\nin diverse situations and various lighting conditions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:26:26 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Halder", "Shirsendu Sukanta", ""], ["Santra", "Sanchayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "1811.10798", "submitter": "Yiwen Huang", "authors": "Yiwen Huang, Rihui Wu, Pinglai Ou, Ziyong Feng", "title": "Sequentially Aggregated Convolutional Networks", "comments": "To appear in ICCV 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep networks generally implement a certain form of shortcut\nconnections to alleviate optimization difficulties. However, we observe that\nsuch network topology alters the nature of deep networks. In many ways, these\nnetworks behave similarly to aggregated wide networks. We thus exploit the\naggregation nature of shortcut connections at a finer architectural level and\nplace them within wide convolutional layers. We end up with a sequentially\naggregated convolutional (SeqConv) layer that combines the benefits of both\nwide and deep representations by aggregating features of various depths in\nsequence. The proposed SeqConv serves as a drop-in replacement of regular wide\nconvolutional layers and thus could be handily integrated into any backbone\nnetwork. We apply SeqConv to widely adopted backbones including ResNet and\nResNeXt, and conduct experiments for image classification on public benchmark\ndatasets. Our ResNet based network with a model size of ResNet-50 easily\nsurpasses the performance of the 2.35$\\times$ larger ResNet-152, while our\nResNeXt based model sets a new state-of-the-art accuracy on ImageNet\nclassification for networks with similar model complexity. The code and\npre-trained models of our work are publicly available at\nhttps://github.com/GroupOfAlchemists/SeqConv.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:15:35 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 12:24:18 GMT"}, {"version": "v3", "created": "Sat, 31 Aug 2019 07:22:32 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Huang", "Yiwen", ""], ["Wu", "Rihui", ""], ["Ou", "Pinglai", ""], ["Feng", "Ziyong", ""]]}, {"id": "1811.10800", "submitter": "David Hall", "authors": "David Hall, Feras Dayoub, John Skinner, Haoyang Zhang, Dimity Miller,\n  Peter Corke, Gustavo Carneiro, Anelia Angelova, Niko S\\\"underhauf", "title": "Probabilistic Object Detection: Definition and Evaluation", "comments": "21 pages, 25 figures, to appear in the proceedings of the winter\n  conference on applications of computer vision WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Probabilistic Object Detection, the task of detecting objects in\nimages and accurately quantifying the spatial and semantic uncertainties of the\ndetections. Given the lack of methods capable of assessing such probabilistic\nobject detections, we present the new Probability-based Detection Quality\nmeasure (PDQ).Unlike AP-based measures, PDQ has no arbitrary thresholds and\nrewards spatial and label quality, and foreground/background separation quality\nwhile explicitly penalising false positive and false negative detections. We\ncontrast PDQ with existing mAP and moLRP measures by evaluating\nstate-of-the-art detectors and a Bayesian object detector based on Monte Carlo\nDropout. Our experiments indicate that conventional object detectors tend to be\nspatially overconfident and thus perform poorly on the task of probabilistic\nobject detection. Our paper aims to encourage the development of new object\ndetection approaches that provide detections with accurately estimated spatial\nand label uncertainties and are of critical importance for deployment on robots\nand embodied AI systems in the real world.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:27:40 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 01:33:29 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 06:33:57 GMT"}, {"version": "v4", "created": "Thu, 30 Jan 2020 05:49:50 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Hall", "David", ""], ["Dayoub", "Feras", ""], ["Skinner", "John", ""], ["Zhang", "Haoyang", ""], ["Miller", "Dimity", ""], ["Corke", "Peter", ""], ["Carneiro", "Gustavo", ""], ["Angelova", "Anelia", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "1811.10801", "submitter": "Shirsendu Halder", "authors": "Shirsendu Sukanta Halder, Kanjar De and Partha Pratim Roy", "title": "Perceptual Conditional Generative Adversarial Networks for End-to-End\n  Image Colourization", "comments": "16 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colours are everywhere. They embody a significant part of human visual\nperception. In this paper, we explore the paradigm of hallucinating colours\nfrom a given gray-scale image. The problem of colourization has been dealt in\nprevious literature but mostly in a supervised manner involving\nuser-interference. With the emergence of Deep Learning methods numerous tasks\nrelated to computer vision and pattern recognition have been automatized and\ncarried in an end-to-end fashion due to the availability of large data-sets and\nhigh-power computing systems. We investigate and build upon the recent success\nof Conditional Generative Adversarial Networks (cGANs) for Image-to-Image\ntranslations. In addition to using the training scheme in the basic cGAN, we\npropose an encoder-decoder generator network which utilizes the class-specific\ncross-entropy loss as well as the perceptual loss in addition to the original\nobjective function of cGAN. We train our model on a large-scale dataset and\npresent illustrative qualitative and quantitative analysis of our results. Our\nresults vividly display the versatility and proficiency of our methods through\nlife-like colourization outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:28:08 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Halder", "Shirsendu Sukanta", ""], ["De", "Kanjar", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1811.10813", "submitter": "Suwon Shon", "authors": "Suwon Shon, Tae-Hyun Oh, James Glass", "title": "Noise-tolerant Audio-visual Online Person Verification using an\n  Attention-based Neural Network Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-modal online person verification system\nusing both speech and visual signals. Inspired by neuroscientific findings on\nthe association of voice and face, we propose an attention-based end-to-end\nneural network that learns multi-sensory associations for the task of person\nverification. The attention mechanism in our proposed network learns to\nconditionally select a salient modality between speech and facial\nrepresentations that provides a balance between complementary inputs. By virtue\nof this capability, the network is robust to missing or corrupted data from\neither modality. In the VoxCeleb2 dataset, we show that our method performs\nfavorably against competing multi-modal methods. Even for extreme cases of\nlarge corruption or an entirely missing modality, our method demonstrates\nrobustness over other unimodal methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:58:10 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Shon", "Suwon", ""], ["Oh", "Tae-Hyun", ""], ["Glass", "James", ""]]}, {"id": "1811.10830", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi", "title": "From Recognition to Cognition: Visual Commonsense Reasoning", "comments": "CVPR 2019 oral. Project page at https://visualcommonsense.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual understanding goes well beyond object recognition. With one glance at\nan image, we can effortlessly imagine the world beyond the pixels: for\ninstance, we can infer people's actions, goals, and mental states. While this\ntask is easy for humans, it is tremendously difficult for today's vision\nsystems, requiring higher-order cognition and commonsense reasoning about the\nworld. We formalize this task as Visual Commonsense Reasoning. Given a\nchallenging question about an image, a machine must answer correctly and then\nprovide a rationale justifying its answer.\n  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA\nproblems derived from 110k movie scenes. The key recipe for generating\nnon-trivial and high-quality problems at scale is Adversarial Matching, a new\napproach to transform rich annotations into multiple choice questions with\nminimal bias. Experimental results show that while humans find VCR easy (over\n90% accuracy), state-of-the-art vision models struggle (~45%).\n  To move towards cognition-level understanding, we present a new reasoning\nengine, Recognition to Cognition Networks (R2C), that models the necessary\nlayered inferences for grounding, contextualization, and reasoning. R2C helps\nnarrow the gap between humans and machines (~65%); still, the challenge is far\nfrom solved, and we provide analysis that suggests avenues for future work.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:22:26 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 17:50:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zellers", "Rowan", ""], ["Bisk", "Yonatan", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "1811.10837", "submitter": "Xinyu Huang", "authors": "Qichuan Geng and Hong Zhang and Xinyu Huang and Sen Wang and Feixiang\n  Lu and Xinjing Cheng and Zhong Zhou and Ruigang Yang", "title": "Part-level Car Parsing and Reconstruction from Single Street View", "comments": "Version 2: 1. A major revision; 2. Experiments based on ApolloScape\n  dataset are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part information has been shown to be resistant to occlusions and viewpoint\nchanges, which is beneficial for various vision-related tasks. However, we\nfound very limited work in car pose estimation and reconstruction from street\nviews leveraging the part information. There are two major contributions in\nthis paper. Firstly, we make the first attempt to build a framework to\nsimultaneously estimate shape, translation, orientation, and semantic parts of\ncars in 3D space from a single street view. As it is labor-intensive to\nannotate semantic parts on real street views, we propose a specific approach to\nimplicitly transfer part features from synthesized images to real street views.\nFor pose and shape estimation, we propose a novel network structure that\nutilizes both part features and 3D losses. Secondly, we are the first to\nconstruct a high-quality dataset that contains 348 different car models with\nphysical dimensions and part-level annotations based on global and local\ndeformations. Given these models, we further generate 60K synthesized images\nwith randomization of orientation, illumination, occlusion, and texture. Our\nresults demonstrate that our part segmentation performance is significantly\nimproved after applying our implicit transfer approach. Our network for pose\nand shape estimation achieves the state-of-the-art performance on the\nApolloCar3D dataset and outperforms 3D-RCNN and DeepMANTA by 12.57 and 8.91\npercentage points in terms of mean A3DP-Abs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:38:36 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 14:46:00 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Geng", "Qichuan", ""], ["Zhang", "Hong", ""], ["Huang", "Xinyu", ""], ["Wang", "Sen", ""], ["Lu", "Feixiang", ""], ["Cheng", "Xinjing", ""], ["Zhou", "Zhong", ""], ["Yang", "Ruigang", ""]]}, {"id": "1811.10842", "submitter": "Junsong Fan", "authors": "Junsong Fan, Zhaoxiang Zhang, Tieniu Tan, Chunfeng Song, Jun Xiao", "title": "CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic\n  Segmentation", "comments": "9 pages, 4 figures, AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation with only image-level labels saves\nlarge human effort to annotate pixel-level labels. Cutting-edge approaches rely\non various innovative constraints and heuristic rules to generate the masks for\nevery single image. Although great progress has been achieved by these methods,\nthey treat each image independently and do not take account of the\nrelationships across different images. In this paper, however, we argue that\nthe cross-image relationship is vital for weakly supervised segmentation.\nBecause it connects related regions across images, where supplementary\nrepresentations can be propagated to obtain more consistent and integral\nregions. To leverage this information, we propose an end-to-end cross-image\naffinity module, which exploits pixel-level cross-image relationships with only\nimage-level labels. By means of this, our approach achieves 64.3% and 65.3%\nmIoU on Pascal VOC 2012 validation and test set respectively, which is a new\nstate-of-the-art result by only using image-level labels for weakly supervised\nsemantic segmentation, demonstrating the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 07:03:12 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 06:10:51 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Fan", "Junsong", ""], ["Zhang", "Zhaoxiang", ""], ["Tan", "Tieniu", ""], ["Song", "Chunfeng", ""], ["Xiao", "Jun", ""]]}, {"id": "1811.10847", "submitter": "Byung-Cheol Min", "authors": "Arabinda Samantaray, Baijian Yang, J. Eric Dietz, Byung-Cheol Min", "title": "Algae Detection Using Computer Vision and Deep Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disconcerting ramification of water pollution caused by burgeoning\npopulations, rapid industrialization and modernization of agriculture, has been\nthe exponential increase in the incidence of algal growth across the globe.\nHarmful algal blooms (HABs) have devastated fisheries, contaminated drinking\nwater and killed livestock, resulting in economic losses to the tune of\nmillions of dollars. Therefore, it is important to constantly monitor water\nbodies and identify any algae build-up so that prompt action against its\naccumulation can be taken and the harmful consequences can be avoided. In this\npaper, we propose a computer vision system based on deep learning for algae\nmonitoring. The proposed system is fast, accurate and cheap, and it can be\ninstalled on any robotic platforms such as USVs and UAVs for autonomous algae\nmonitoring. The experimental results demonstrate that the proposed system can\ndetect algae in distinct environments regardless of the underlying hardware\nwith high accuracy and in real time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 07:31:26 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Samantaray", "Arabinda", ""], ["Yang", "Baijian", ""], ["Dietz", "J. Eric", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "1811.10862", "submitter": "Yusuke Niitani", "authors": "Yusuke Niitani, Takuya Akiba, Tommi Kerola, Toru Ogawa, Shotaro Sano,\n  Shuji Suzuki", "title": "Sampling Techniques for Large-Scale Object Detection from Sparsely\n  Annotated Objects", "comments": "CVPR2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and reliable methods for training of object detectors are in higher\ndemand than ever, and more and more data relevant to the field is becoming\navailable. However, large datasets like Open Images Dataset v4 (OID) are\nsparsely annotated, and some measure must be taken in order to ensure the\ntraining of a reliable detector. In order to take the incompleteness of these\ndatasets into account, one possibility is to use pretrained models to detect\nthe presence of the unverified objects. However, the performance of such a\nstrategy depends largely on the power of the pretrained model. In this study,\nwe propose part-aware sampling, a method that uses human intuition for the\nhierarchical relation between objects. In terse terms, our method works by\nmaking assumptions like \"a bounding box for a car should contain a bounding box\nfor a tire\". We demonstrate the power of our method on OID and compare the\nperformance against a method based on a pretrained model. Our method also won\nthe first and second place on the public and private test sets of the Google AI\nOpen Images Competition 2018.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:14:02 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 06:52:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Niitani", "Yusuke", ""], ["Akiba", "Takuya", ""], ["Kerola", "Tommi", ""], ["Ogawa", "Toru", ""], ["Sano", "Shotaro", ""], ["Suzuki", "Shuji", ""]]}, {"id": "1811.10863", "submitter": "U\\u{g}ur Kart", "authors": "Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri\n  Matas", "title": "Object Tracking by Reconstruction with View-Specific Discriminative\n  Correlation Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard RGB-D trackers treat the target as an inherently 2D structure, which\nmakes modelling appearance changes related even to simple out-of-plane rotation\nhighly challenging. We address this limitation by proposing a novel long-term\nRGB-D tracker - Object Tracking by Reconstruction (OTR). The tracker performs\nonline 3D target reconstruction to facilitate robust learning of a set of\nview-specific discriminative correlation filters (DCFs). The 3D reconstruction\nsupports two performance-enhancing features: (i) generation of accurate spatial\nsupport for constrained DCF learning from its 2D projection and (ii) point\ncloud based estimation of 3D pose change for selection and storage of\nview-specific DCFs which are used to robustly localize the target after\nout-of-view rotation or heavy occlusion. Extensive evaluation of OTR on the\nchallenging Princeton RGB-D tracking and STC Benchmarks shows it outperforms\nthe state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:16:29 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kart", "Ugur", ""], ["Lukezic", "Alan", ""], ["Kristan", "Matej", ""], ["Kamarainen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "1811.10870", "submitter": "Yiding Liu", "authors": "Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu, Houqiang Li\n  and Yan Lu", "title": "Affinity Derivation and Graph Merge for Instance Segmentation", "comments": "Published in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an instance segmentation scheme based on pixel affinity\ninformation, which is the relationship of two pixels belonging to a same\ninstance. In our scheme, we use two neural networks with similar structure. One\nis to predict pixel level semantic score and the other is designed to derive\npixel affinities.\n  Regarding pixels as the vertexes and affinities as edges, we then propose a\nsimple yet effective graph merge algorithm to cluster pixels into instances.\nExperimental results show that our scheme can generate fine-grained instance\nmask.\n  With Cityscapes training data, the proposed scheme achieves 27.3 AP on test\nset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:34:28 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liu", "Yiding", ""], ["Yang", "Siyu", ""], ["Li", "Bin", ""], ["Zhou", "Wengang", ""], ["Xu", "Jizheng", ""], ["Li", "Houqiang", ""], ["Lu", "Yan", ""]]}, {"id": "1811.10872", "submitter": "Feida Zhu", "authors": "Feida Zhu, Yizhou Yu", "title": "Automatic Image Stylization Using Deep Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color and tone stylization strives to enhance unique themes with artistic\ncolor and tone adjustments. It has a broad range of applications from\nprofessional image postprocessing to photo sharing over social networks.\nMainstream photo enhancement softwares provide users with predefined styles,\nwhich are often hand-crafted through a trial-and-error process. Such photo\nadjustment tools lack a semantic understanding of image contents and the\nresulting global color transform limits the range of artistic styles it can\nrepresent. On the other hand, stylistic enhancement needs to apply distinct\nadjustments to various semantic regions. Such an ability enables a broader\nrange of visual styles. In this paper, we propose a novel deep learning\narchitecture for automatic image stylization, which learns local enhancement\nstyles from image pairs. Our deep learning architecture is an end-to-end deep\nfully convolutional network performing semantics-aware feature extraction as\nwell as automatic image adjustment prediction. Image stylization can be\nefficiently accomplished with a single forward pass through our deep network.\nExperiments on existing datasets for image stylization demonstrate the\neffectiveness of our deep learning architecture.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:39:48 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhu", "Feida", ""], ["Yu", "Yizhou", ""]]}, {"id": "1811.10884", "submitter": "Anima Majumder", "authors": "Madhu Babu V, Swagat Kumar, Anima Majumder, Kaushik Das", "title": "UnDEMoN 2.0: Improved Depth and Ego Motion Estimation through Deep Image\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an improved version of UnDEMoN model for depth and\nego motion estimation from monocular images. The improvement is achieved by\ncombining the standard bi-linear sampler with a deep network based image\nsampling model (DIS-NET) to provide better image reconstruction capabilities on\nwhich the depth estimation accuracy depends in un-supervised learning models.\nWhile DIS-NET provides higher order regression and larger input search space,\nthe bi-linear sampler provides geometric constraints necessary for reducing the\nsize of the solution space for an ill-posed problem of this kind. This\ncombination is shown to provide significant improvement in depth and pose\nestimation accuracy outperforming all existing state-of-the-art methods in this\ncategory. In addition, the modified network uses far less number of tunable\nparameters making it one of the lightest deep network model for depth\nestimation. The proposed model is labeled as \"UnDEMoN 2.0\" indicating an\nimprovement over the existing UnDEMoN model. The efficacy of the proposed model\nis demonstrated through rigorous experimental analysis on the standard KITTI\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 09:19:03 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Babu", "Madhu", "V"], ["Kumar", "Swagat", ""], ["Majumder", "Anima", ""], ["Das", "Kaushik", ""]]}, {"id": "1811.10893", "submitter": "Renqiang Li", "authors": "Renqiang Li, Hong Liu, Xiangdong Wan, Yueliang Qian", "title": "DSBI: Double-Sided Braille Image Dataset and Algorithm Evaluation for\n  Braille Dots Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Braille is an effective way for the visually impaired to learn knowledge and\nobtain information. Braille image recognition aims to automatically detect\nBraille dots in the whole Braille image. There is no available public datasets\nfor Braille image recognition to push relevant research and evaluate\nalgorithms. This paper constructs a large-scale Double-Sided Braille Image\ndataset DSBI with detailed Braille recto dots, verso dots and Braille cells\nannotation. To quickly annotate Braille images, an auxiliary annotation\nstrategy is proposed, which adopts initial automatic detection of Braille dots\nand modifies annotation results by convenient human-computer interaction\nmethod. This labeling strategy can averagely increase label efficiency by six\ntimes for recto dots annotation in one Braille image. Braille dots detection is\nthe core and basic step for Braille image recognition. This paper also\nevaluates some Braille dots detection methods on our dataset DSBI and gives the\nbenchmark performance of recto dots detection. We have released our Braille\nimages dataset on the GitHub website.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 09:49:08 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 02:43:53 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Li", "Renqiang", ""], ["Liu", "Hong", ""], ["Wan", "Xiangdong", ""], ["Qian", "Yueliang", ""]]}, {"id": "1811.10899", "submitter": "Bastien Moysset", "authors": "Bastien Moysset and Ronaldo Messina", "title": "Are 2D-LSTM really dead for offline text recognition?", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent trend in handwritten text recognition with deep neural\nnetworks to replace 2D recurrent layers with 1D, and in some cases even\ncompletely remove the recurrent layers, relying on simple feed-forward\nconvolutional only architectures. The most used type of recurrent layer is the\nLong-Short Term Memory (LSTM). The motivations to do so are many: there are few\nopen-source implementations of 2D-LSTM, even fewer supporting GPU\nimplementations (currently cuDNN only implements 1D-LSTM); 2D recurrences\nreduce the amount of computations that can be parallelized, and thus possibly\nincrease the training/inference time; recurrences create global dependencies\nwith respect to the input, and sometimes this may not be desirable.\n  Many recent competitions were won by systems that employed networks that use\n2D-LSTM layers. Most previous work that compared 1D or pure feed-forward\narchitectures to 2D recurrent models have done so on simple datasets or did not\nfully optimize the \"baseline\" 2D model compared to the challenger model, which\nwas dully optimized.\n  In this work, we aim at a fair comparison between 2D and competing models and\nalso extensively evaluate them on more complex datasets that are more\nrepresentative of challenging \"real-world\" data, compared to \"academic\"\ndatasets that are more restricted in their complexity. We aim at determining\nwhen and why the 1D and 2D recurrent models have different results. We also\ncompare the results with a language model to assess if linguistic constraints\ndo level the performance of the different networks.\n  Our results show that for challenging datasets, 2D-LSTM networks still seem\nto provide the highest performances and we propose a visualization strategy to\nexplain it.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 10:13:01 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Moysset", "Bastien", ""], ["Messina", "Ronaldo", ""]]}, {"id": "1811.10907", "submitter": "Fan Yang", "authors": "Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, Shin'ichi Satoh", "title": "Efficient Image Retrieval via Decoupling Diffusion into Online and\n  Offline Processing", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion is commonly used as a ranking or re-ranking method in retrieval\ntasks to achieve higher retrieval performance, and has attracted lots of\nattention in recent years. A downside to diffusion is that it performs slowly\nin comparison to the naive k-NN search, which causes a non-trivial online\ncomputational cost on large datasets. To overcome this weakness, we propose a\nnovel diffusion technique in this paper. In our work, instead of applying\ndiffusion to the query, we pre-compute the diffusion results of each element in\nthe database, making the online search a simple linear combination on top of\nthe k-NN search process. Our proposed method becomes 10~ times faster in terms\nof online search speed. Moreover, we propose to use late truncation instead of\nearly truncation in previous works to achieve better retrieval performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 10:52:26 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 11:12:31 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Yang", "Fan", ""], ["Hinami", "Ryota", ""], ["Matsui", "Yusuke", ""], ["Ly", "Steven", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1811.10914", "submitter": "Kaicheng Yu", "authors": "Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann", "title": "Beyond One Glance: Gated Recurrent Architecture for Hand Segmentation", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As mixed reality is gaining increased momentum, the development of effective\nand efficient solutions to egocentric hand segmentation is becoming critical.\nTraditional segmentation techniques typically follow a one-shot approach, where\nthe image is passed forward only once through a model that produces a\nsegmentation mask. This strategy, however, does not reflect the perception of\nhumans, who continuously refine their representation of the world. In this\npaper, we therefore introduce a novel gated recurrent architecture. It goes\nbeyond both iteratively passing the predicted segmentation mask through the\nnetwork and adding a standard recurrent unit to it. Instead, it incorporates\nmultiple encoder-decoder layers of the segmentation network, so as to keep\ntrack of its internal state in the refinement process. As evidenced by our\nresults on standard hand segmentation benchmarks and on our own dataset, our\napproach outperforms these other, simpler recurrent segmentation techniques, as\nwell as the state-of-the-art hand segmentation one. Furthermore, we demonstrate\nthe generality of our approach by applying it to road segmentation, where it\nalso outperforms other baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 11:16:41 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 13:10:49 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 13:26:20 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Wang", "Wei", ""], ["Yu", "Kaicheng", ""], ["Hugonot", "Joachim", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1811.10943", "submitter": "Francis Williams", "authors": "Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan\n  Bruna, Daniele Panozzo", "title": "Deep Geometric Prior for Surface Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a discrete surface from a point cloud is a fundamental\ngeometry processing problem that has been studied for decades, with many\nmethods developed. We propose the use of a deep neural network as a geometric\nprior for surface reconstruction. Specifically, we overfit a neural network\nrepresenting a local chart parameterization to part of an input point cloud\nusing the Wasserstein distance as a measure of approximation. By jointly\nfitting many such networks to overlapping parts of the point cloud, while\nenforcing a consistency condition, we compute a manifold atlas. By sampling\nthis atlas, we can produce a dense reconstruction of the surface approximating\nthe input cloud. The entire procedure does not require any training data or\nexplicit regularization, yet, we show that it is able to perform remarkably\nwell: not introducing typical overfitting artifacts, and approximating sharp\nfeatures closely at the same time. We experimentally show that this geometric\nprior produces good results for both man-made objects containing sharp features\nand smoother organic objects, as well as noisy inputs. We compare our method\nwith a number of well-known reconstruction methods on a standard surface\nreconstruction benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:50:46 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 00:31:43 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Williams", "Francis", ""], ["Schneider", "Teseo", ""], ["Silva", "Claudio", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1811.10946", "submitter": "Serkan S\\\"ul\\\"un", "authors": "Serkan Sulun", "title": "Deep Learned Frame Prediction for Video Compression", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.28590.54085", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion compensation is one of the most essential methods for any video\ncompression algorithm. Video frame prediction is a task analogous to motion\ncompensation. In recent years, the task of frame prediction is undertaken by\ndeep neural networks (DNNs). In this thesis we create a DNN to perform learned\nframe prediction and additionally implement a codec that contains our DNN. We\ntrain our network using two methods for two different goals. Firstly we train\nour network based on mean square error (MSE) only, aiming to obtain highest\nPSNR values at frame prediction and video compression. Secondly we use\nadversarial training to produce visually more realistic frame predictions. For\nframe prediction, we compare our method with the baseline methods of frame\ndifference and 16x16 block motion compensation. For video compression we\nfurther include x264 video codec in the comparison. We show that in frame\nprediction, adversarial training produces frames that look sharper and more\nrealistic, compared MSE based training, but in video compression it\nconsistently performs worse. This proves that even though adversarial training\nis useful for generating video frames that are more pleasing to the human eye,\nthey should not be employed for video compression. Moreover, our network\ntrained with MSE produces accurate frame predictions, and in quantitative\nresults, for both tasks, it produces comparable results in all videos and\noutperforms other methods on average. More specifically, learned frame\nprediction outperforms other methods in terms of rate-distortion performance in\ncase of high motion video, while the rate-distortion performance of our method\nis competitive with that of x264 in low motion video.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:53:30 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Sulun", "Serkan", ""]]}, {"id": "1811.10964", "submitter": "Jian Jiao", "authors": "Jian Jiao, Jichao Jiao, Yaokai Mo, Weilun Liu, Zhongliang Deng", "title": "MagicVO: End-to-End Monocular Visual Odometry through Deep\n  Bi-directional Recurrent Convolutional Neural Network", "comments": "9 pages,5 figures,CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework to solve the problem of monocular visual\nodometry, called MagicVO . Based on Convolutional Neural Network (CNN) and\nBi-directional LSTM (Bi-LSTM), MagicVO outputs a 6-DoF absolute-scale pose at\neach position of the camera with a sequence of continuous monocular images as\ninput. It not only utilizes the outstanding performance of CNN in image feature\nprocessing to extract the rich features of image frames fully but also learns\nthe geometric relationship from image sequences pre and post through Bi-LSTM to\nget a more accurate prediction. A pipeline of the MagicVO is shown in Fig. 1.\nThe MagicVO system is end-to-end, and the results of experiments on the KITTI\ndataset and the ETH-asl cla dataset show that MagicVO has a better performance\nthan traditional visual odometry (VO) systems in the accuracy of pose and the\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:22:32 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 02:13:06 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Jiao", "Jian", ""], ["Jiao", "Jichao", ""], ["Mo", "Yaokai", ""], ["Liu", "Weilun", ""], ["Deng", "Zhongliang", ""]]}, {"id": "1811.10969", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Junghun James Kim, Daekyu Shin", "title": "One-Shot Item Search with Multimodal Data", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of near similar image search, features from Deep Neural Network\nis often used to compare images and measure similarity. In the past, we only\nfocused visual search in image dataset without text data. However, since deep\nneural network emerged, the performance of visual search becomes high enough to\napply it in many industries from 3D data to multimodal data. Compared to the\nneeds of multimodal search, there has not been sufficient researches.\n  In this paper, we present a method of near similar search with image and text\nmultimodal dataset. Earlier time, similar image search, especially when\nsearching shopping items, treated image and text separately to search similar\nitems and reorder the results. This regards two tasks of image search and text\nmatching as two different tasks. Our method, however, explore the vast data to\ncompute k-nearest neighbors using both image and text.\n  In our experiment of similar item search, our system using multimodal data\nshows better performance than single data while it only increases minute\ncomputing time. For the experiment, we collected more than 15 million of\naccessory and six million of digital product items from online shopping\nwebsites, in which the product item comprises item images, titles, categories,\nand descriptions. Then we compare the performance of multimodal searching to\nsingle space searching in these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:30:13 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 16:42:18 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Yim", "Jonghwa", ""], ["Kim", "Junghun James", ""], ["Shin", "Daekyu", ""]]}, {"id": "1811.10980", "submitter": "Alexander Krull", "authors": "Alexander Krull, Tim-Oliver Buchholz, Florian Jug", "title": "Noise2Void - Learning Denoising from Single Noisy Images", "comments": "9 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of image denoising is currently dominated by discriminative deep\nlearning methods that are trained on pairs of noisy input and clean target\nimages. Recently it has been shown that such methods can also be trained\nwithout clean targets. Instead, independent pairs of noisy images can be used,\nin an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V),\na training scheme that takes this idea one step further. It does not require\nnoisy image pairs, nor clean target images. Consequently, N2V allows us to\ntrain directly on the body of data to be denoised and can therefore be applied\nwhen other methods cannot. Especially interesting is the application to\nbiomedical image data, where the acquisition of training targets, clean or\nnoisy, is frequently not possible. We compare the performance of N2V to\napproaches that have either clean target images and/or noisy image pairs\navailable. Intuitively, N2V cannot be expected to outperform methods that have\nmore information available during training. Still, we observe that the\ndenoising performance of Noise2Void drops in moderation and compares favorably\nto training-free denoising methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:48:42 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 16:07:38 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Krull", "Alexander", ""], ["Buchholz", "Tim-Oliver", ""], ["Jug", "Florian", ""]]}, {"id": "1811.10983", "submitter": "Erhan Gundogdu", "authors": "Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang,\n  Mathieu Salzmann, Pascal Fua", "title": "GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a\n3D body, it remains too costly for real-time applications, such as virtual\ntry-on. By contrast, inference in a deep network, requiring a single forward\npass, is much faster. Taking advantage of this, we propose a novel architecture\nto fit a 3D garment template to a 3D body. Specifically, we build upon the\nrecent progress in 3D point cloud processing with deep networks to extract\ngarment features at varying levels of detail, including point-wise, patch-wise\nand global features. We fuse these features with those extracted in parallel\nfrom the 3D body, so as to model the cloth-body interactions. The resulting\ntwo-stream architecture, which we call as GarNet, is trained using a loss\nfunction inspired by physics-based modeling, and delivers visually plausible\ngarment shapes whose 3D points are, on average, less than 1 cm away from those\nof a PBS method, while running 100 times faster. Moreover, the proposed method\ncan model various garment types with different cutting patterns when parameters\nof those patterns are given as input to the network.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:55:01 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 14:25:41 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 13:07:58 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Gundogdu", "Erhan", ""], ["Constantin", "Victor", ""], ["Seifoddini", "Amrollah", ""], ["Dang", "Minh", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1811.10984", "submitter": "Andrii Maksai", "authors": "Andrii Maksai and Pascal Fua", "title": "Eliminating Exposure Bias and Loss-Evaluation Mismatch in Multiple\n  Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity Switching remains one of the main difficulties Multiple Object\nTracking (MOT) algorithms have to deal with. Many state-of-the-art approaches\nnow use sequence models to solve this problem but their training can be\naffected by biases that decrease their efficiency. In this paper, we introduce\na new training procedure that confronts the algorithm to its own mistakes while\nexplicitly attempting to minimize the number of switches, which results in\nbetter training. We propose an iterative scheme of building a rich training set\nand using it to learn a scoring function that is an explicit proxy for the\ntarget tracking metric. Whether using only simple geometric features or more\nsophisticated ones that also take appearance into account, our approach\noutperforms the state-of-the-art on several MOT benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:57:17 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Maksai", "Andrii", ""], ["Fua", "Pascal", ""]]}, {"id": "1811.11051", "submitter": "Idan Kligvasser", "authors": "Idan Kligvasser and Tomer Michaeli", "title": "Dense xUnit Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep net architectures have constantly evolved over the past few years,\nleading to significant advancements in a wide array of computer vision tasks.\nHowever, besides high accuracy, many applications also require a low\ncomputational load and limited memory footprint. To date, efficiency has\ntypically been achieved either by architectural choices at the macro level\n(e.g. using skip connections or pruning techniques) or modifications at the\nlevel of the individual layers (e.g. using depth-wise convolutions or channel\nshuffle operations). Interestingly, much less attention has been devoted to the\nrole of the activation functions in constructing efficient nets. Recently,\nKligvasser et al. showed that incorporating spatial connections within the\nactivation functions, enables a significant boost in performance in image\nrestoration tasks, at any given budget of parameters. However, the\neffectiveness of their xUnit module has only been tested on simple small\nmodels, which are not characteristic of those used in high-level vision tasks.\nIn this paper, we adopt and improve the xUnit activation, show how it can be\nincorporated into the DenseNet architecture, and illustrate its high\neffectiveness for classification and image restoration tasks alike. While the\nDenseNet architecture is extremely efficient to begin with, our dense xUnit net\n(DxNet) can typically achieve the same performance with far fewer parameters.\nFor example, on ImageNet, our DxNet outperforms a ReLU-based DenseNet having\n30% more parameters and achieves state-of-the-art results for this budget of\nparameters. Furthermore, in denoising and super-resolution, DxNet significantly\nimproves upon all existing lightweight solutions, including the xUnit-based\nnets of Kligvasser et al.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 15:21:50 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kligvasser", "Idan", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1811.11053", "submitter": "Haifeng Li", "authors": "Li Chen, Hailun Ding, Qi Li, Zhuo Li, Jian Peng, Haifeng Li", "title": "Understanding the Importance of Single Directions via Representative\n  Substitution", "comments": "4 pages, 6 figures", "journal-ref": "AAAI-19 Workshop on Network Interpretability for Deep Learning,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the internal representations of deep neural networks (DNNs) is\ncrucal to explain their behavior. The interpretation of individual units, which\nare neurons in MLPs or convolution kernels in convolutional networks, has been\npaid much attention given their fundamental role. However, recent research\n(Morcos et al. 2018) presented a counterintuitive phenomenon, which suggests\nthat an individual unit with high class selectivity, called interpretable\nunits, has poor contributions to generalization of DNNs. In this work, we\nprovide a new perspective to understand this counterintuitive phenomenon, which\nmakes sense when we introduce Representative Substitution (RS). Instead of\nindividually selective units with classes, the RS refers to the independence of\na unit's representations in the same layer without any annotation. Our\nexperiments demonstrate that interpretable units have high RS which are not\ncritical to network's generalization. The RS provides new insights into the\ninterpretation of DNNs and suggests that we need to focus on the independence\nand relationship of the representations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 15:25:03 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 22:34:56 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Chen", "Li", ""], ["Ding", "Hailun", ""], ["Li", "Qi", ""], ["Li", "Zhuo", ""], ["Peng", "Jian", ""], ["Li", "Haifeng", ""]]}, {"id": "1811.11057", "submitter": "Shiyao Wang", "authors": "Shiyao Wang, Hongchao Lu, Zhidong Deng", "title": "Fast Object Detection in Compressed Video", "comments": "10 pages, 7 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in videos has drawn increasing attention since it is more\npractical in real scenarios. Most of the deep learning methods use CNNs to\nprocess each decoded frame in a video stream individually. However, the free of\ncharge yet valuable motion information already embedded in the video\ncompression format is usually overlooked. In this paper, we propose a fast\nobject detection method by taking advantage of this with a novel Motion aided\nMemory Network (MMNet). The MMNet has two major advantages: 1) It significantly\naccelerates the procedure of feature extraction for compressed videos. It only\nneed to run a complete recognition network for I-frames, i.e. a few reference\nframes in a video, and it produces the features for the following P frames\n(predictive frames) with a light weight memory network, which runs fast; 2)\nUnlike existing methods that establish an additional network to model motion of\nframes, we take full advantage of both motion vectors and residual errors that\nare freely available in video streams. To our best knowledge, the MMNet is the\nfirst work that investigates a deep convolutional detector on compressed\nvideos. Our method is evaluated on the large-scale ImageNet VID dataset, and\nthe results show that it is 3x times faster than single image detector R-FCN\nand 10x times faster than high-performance detector MANet at a minor accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 15:35:53 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 04:47:37 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 08:00:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Shiyao", ""], ["Lu", "Hongchao", ""], ["Deng", "Zhidong", ""]]}, {"id": "1811.11080", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, Khoa Luu", "title": "MobiFace: A Lightweight Deep Learning Face Recognition on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely used in numerous computer vision\napplications, particularly in face recognition. However, deploying deep neural\nnetwork face recognition on mobile devices has recently become a trend but\nstill limited since most high-accuracy deep models are both time and GPU\nconsumption in the inference stage. Therefore, developing a lightweight deep\nneural network is one of the most practical solutions to deploy face\nrecognition on mobile devices. Such the lightweight deep neural network\nrequires efficient memory with small number of weights representation and low\ncost operators. In this paper, a novel deep neural network named MobiFace, a\nsimple but effective approach, is proposed for productively deploying face\nrecognition on mobile devices. The experimental results have shown that our\nlightweight MobiFace is able to achieve high performance with 99.73% on LFW\ndatabase and 91.3% on large-scale challenging Megaface database. It is also\neventually competitive against large-scale deep-networks face recognition while\nsignificant reducing computational time and memory consumption.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:34:01 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:00:16 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Quach", "Kha Gia", ""], ["Jalata", "Ibsa", ""], ["Le", "Ngan", ""], ["Luu", "Khoa", ""]]}, {"id": "1811.11082", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Nghia Nguyen, Eric Patterson,\n  Tien D. Bui, Ngan Le", "title": "Automatic Face Aging in Videos via Deep Reinforcement Learning", "comments": "CVPR2019 Camera Ready, https://face-aging.github.io/RL-VAP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to synthesize automatically\nage-progressed facial images in video sequences using Deep Reinforcement\nLearning. The proposed method models facial structures and the longitudinal\nface-aging process of given subjects coherently across video frames. The\napproach is optimized using a long-term reward, Reinforcement Learning function\nwith deep feature extraction from Deep Convolutional Neural Network. Unlike\nprevious age-progression methods that are only able to synthesize an aged\nlikeness of a face from a single input image, the proposed approach is capable\nof age-progressing facial likenesses in videos with consistently synthesized\nfacial features across frames. In addition, the deep reinforcement learning\nmethod guarantees preservation of the visual identity of input faces after\nage-progression. Results on videos of our new collected aging face AGFW-v2\ndatabase demonstrate the advantages of the proposed solution in terms of both\nquality of age-progressed faces, temporal smoothness, and cross-age face\nverification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:41:39 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 06:03:56 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Quach", "Kha Gia", ""], ["Nguyen", "Nghia", ""], ["Patterson", "Eric", ""], ["Bui", "Tien D.", ""], ["Le", "Ngan", ""]]}, {"id": "1811.11088", "submitter": "Carl Olsson", "authors": "Marcus Valtonen \\\"Ornhag and Carl Olsson and Anders Heyden", "title": "Bilinear Parameterization For Differentiable Rank-Regularization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank approximation is a commonly occurring problem in many computer\nvision and machine learning applications. There are two common ways of\noptimizing the resulting models. Either the set of matrices with a given rank\ncan be explicitly parametrized using a bilinear factorization, or low rank can\nbe implicitly enforced using regularization terms penalizing non-zero singular\nvalues. While the former approach results in differentiable problems that can\nbe efficiently optimized using local quadratic approximation, the latter is\ntypically not differentiable (sometimes even discontinuous) and requires first\norder subgradient or splitting methods. It is well known that gradient based\nmethods exhibit slow convergence for ill-conditioned problems.\n  In this paper we show how many non-differentiable regularization methods can\nbe reformulated into smooth objectives using bilinear parameterization. This\nallows us to use standard second order methods, such as Levenberg--Marquardt\n(LM) and Variable Projection (VarPro), to achieve accurate solutions for\nill-conditioned cases. We show on several real and synthetic experiments that\nour second order formulation converges to substantially more accurate solutions\nthan competing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:48:11 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 15:26:19 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 13:32:59 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["\u00d6rnhag", "Marcus Valtonen", ""], ["Olsson", "Carl", ""], ["Heyden", "Anders", ""]]}, {"id": "1811.11127", "submitter": "Tim Brooks", "authors": "Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet,\n  Jonathan T. Barron", "title": "Unprocessing Images for Learned Raw Denoising", "comments": "http://timothybrooks.com/tech/unprocessing/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning techniques work best when the data used for training\nresembles the data used for evaluation. This holds true for learned\nsingle-image denoising algorithms, which are applied to real raw camera sensor\nreadings but, due to practical constraints, are often trained on synthetic\nimage data. Though it is understood that generalizing from synthetic to real\ndata requires careful consideration of the noise properties of image sensors,\nthe other aspects of a camera's image processing pipeline (gain, color\ncorrection, tone mapping, etc) are often overlooked, despite their significant\neffect on how raw measurements are transformed into finished images. To address\nthis, we present a technique to \"unprocess\" images by inverting each step of an\nimage processing pipeline, thereby allowing us to synthesize realistic raw\nsensor measurements from commonly available internet photos. We additionally\nmodel the relevant components of an image processing pipeline when evaluating\nour loss function, which allows training to be aware of all relevant\nphotometric processing that will occur after denoising. By processing and\nunprocessing model outputs and training data in this way, we are able to train\na simple convolutional neural network that has 14%-38% lower error rates and is\n9x-18x faster than the previous state of the art on the Darmstadt Noise\nDataset, and generalizes to sensors outside of that dataset as well.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 17:38:14 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Brooks", "Tim", ""], ["Mildenhall", "Ben", ""], ["Xue", "Tianfan", ""], ["Chen", "Jiawen", ""], ["Sharlet", "Dillon", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1811.11135", "submitter": "Himanshu Akolkar", "authors": "Himanshu Akolkar, SioHoi Ieng and Ryad Benosman", "title": "Real-time high speed motion prediction using fast aperture-robust\n  event-driven visual flow", "comments": "Pre-print version, 12 pages, 12 figures. Accepted in IEEE tPAMI July\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow is a crucial component of the feature space for early visual\nprocessing of dynamic scenes especially in new applications such as\nself-driving vehicles, drones and autonomous robots. The dynamic vision sensors\nare well suited for such applications because of their asynchronous, sparse and\ntemporally precise representation of the visual dynamics. Many algorithms\nproposed for computing visual flow for these sensors suffer from the aperture\nproblem as the direction of the estimated flow is governed by the curvature of\nthe object rather than the true motion direction. Some methods that do overcome\nthis problem by temporal windowing under-utilize the true precise temporal\nnature of the dynamic sensors. In this paper, we propose a novel multi-scale\nplane fitting based visual flow algorithm that is robust to the aperture\nproblem and also computationally fast and efficient. Our algorithm performs\nwell in many scenarios ranging from fixed camera recording simple geometric\nshapes to real world scenarios such as camera mounted on a moving car and can\nsuccessfully perform event-by-event motion estimation of objects in the scene\nto allow for predictions of upto 500 ms i.e. equivalent to 10 to 25 frames with\ntraditional cameras.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 17:52:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 23:02:27 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Akolkar", "Himanshu", ""], ["Ieng", "SioHoi", ""], ["Benosman", "Ryad", ""]]}, {"id": "1811.11147", "submitter": "Arun Mukundan", "authors": "Arun Mukundan, Giorgos Tolias, Andrei Bursuc, Herv\\'e J\\'egou,\n  Ond\\v{r}ej Chum", "title": "Understanding and Improving Kernel Local Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple-kernel local-patch descriptor based on efficient match\nkernels from pixel gradients. It combines two parametrizations of gradient\nposition and direction, each parametrization provides robustness to a different\ntype of patch mis-registration: polar parametrization for noise in the patch\ndominant orientation detection, Cartesian for imprecise location of the feature\npoint. Combined with whitening of the descriptor space, that is learned with or\nwithout supervision, the performance is significantly improved. We analyze the\neffect of the whitening on patch similarity and demonstrate its semantic\nmeaning. Our unsupervised variant is the best performing descriptor constructed\nwithout the need of labeled data. Despite the simplicity of the proposed\ndescriptor, it competes well with deep learning approaches on a number of\ndifferent tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:18:13 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Mukundan", "Arun", ""], ["Tolias", "Giorgos", ""], ["Bursuc", "Andrei", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1811.11155", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "title": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained\n  Object Generation and Discovery", "comments": null, "journal-ref": "CVPR 2019 (Oral Presentation)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FineGAN, a novel unsupervised GAN framework, which disentangles\nthe background, object shape, and object appearance to hierarchically generate\nimages of fine-grained object categories. To disentangle the factors without\nsupervision, our key idea is to use information theory to associate each factor\nto a latent code, and to condition the relationships between the codes in a\nspecific way to induce the desired hierarchy. Through extensive experiments, we\nshow that FineGAN achieves the desired disentanglement to generate realistic\nand diverse images belonging to fine-grained classes of birds, dogs, and cars.\nUsing FineGAN's automatically learned features, we also cluster real images as\na first attempt at solving the novel problem of unsupervised fine-grained\nobject category discovery. Our code/models/demo can be found at\nhttps://github.com/kkanshul/finegan\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:44:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:44:24 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Ojha", "Utkarsh", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1811.11163", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada", "title": "Class-Distinct and Class-Mutual Image Generation with GANs", "comments": "Accepted to BMVC 2019 (Spotlight). Project page:\n  https://takuhirok.github.io/CP-GAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-conditional extensions of generative adversarial networks (GANs), such\nas auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered\nattention owing to their ability to decompose representations into class labels\nand other factors and to boost the training stability. However, a limitation is\nthat they assume that each class is separable and ignore the relationship\nbetween classes even though class overlapping frequently occurs in a real-world\nscenario when data are collected on the basis of diverse or ambiguous criteria.\nTo overcome this limitation, we address a novel problem called class-distinct\nand class-mutual image generation, in which the goal is to construct a\ngenerator that can capture between-class relationships and generate an image\nselectively conditioned on the class specificity. To solve this problem without\nadditional supervision, we propose classifier's posterior GAN (CP-GAN), in\nwhich we redesign the generator input and the objective function of AC-GAN for\nclass-overlapping data. Precisely, we incorporate the classifier's posterior\ninto the generator input and optimize the generator so that the classifier's\nposterior of generated data corresponds with that of real data. We demonstrate\nthe effectiveness of CP-GAN using both controlled and real-world\nclass-overlapping data with a model configuration analysis and comparative\nstudy. Our code is available at https://github.com/takuhirok/CP-GAN/.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:56:19 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 17:51:04 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.11165", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada", "title": "Label-Noise Robust Generative Adversarial Networks", "comments": "Accepted to CVPR 2019 (Oral). Project page:\n  https://takuhirok.github.io/rGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a framework that learns a\ngenerative distribution through adversarial training. Recently, their\nclass-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary\nclassifier GAN (AC-GAN)) have attracted much attention owing to their ability\nto learn the disentangled representations and to improve the training\nstability. However, their training requires the availability of large-scale\naccurate class-labeled data, which are often laborious or impractical to\ncollect in a real-world scenario. To remedy this, we propose a novel family of\nGANs called label-noise robust GANs (rGANs), which, by incorporating a noise\ntransition model, can learn a clean label conditional generative distribution\neven when training labels are noisy. In particular, we propose two variants:\nrAC-GAN, which is a bridging model between AC-GAN and the label-noise robust\nclassification model, and rcGAN, which is an extension of cGAN and solves this\nproblem with no reliance on any classifier. In addition to providing the\ntheoretical background, we demonstrate the effectiveness of our models through\nextensive experiments using diverse GAN configurations, various noise settings,\nand multiple evaluation metrics (in which we tested 402 conditions in total).\nOur code is available at https://github.com/takuhirok/rGAN/.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:56:21 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 18:42:42 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.11167", "submitter": "Jifeng Dai", "authors": "Zheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, Jifeng Dai", "title": "Integrated Object Detection and Tracking with Tracklet-Conditioned\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and tracking of objects is vital for effective video\nunderstanding. In previous work, the two tasks have been combined in a way that\ntracking is based heavily on detection, but the detection benefits marginally\nfrom the tracking. To increase synergy, we propose to more tightly integrate\nthe tasks by conditioning the object detection in the current frame on\ntracklets computed in prior frames. With this approach, the object detection\nresults not only have high detection responses, but also improved coherence\nwith the existing tracklets. This greater coherence leads to estimated object\ntrajectories that are smoother and more stable than the jittered paths obtained\nwithout tracklet-conditioned detection. Over extensive experiments, this\napproach is shown to achieve state-of-the-art performance in terms of both\ndetection and tracking accuracy, as well as noticeable improvements in tracking\nstability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:58:07 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Zheng", ""], ["Cheng", "Dazhi", ""], ["Zhu", "Xizhou", ""], ["Lin", "Stephen", ""], ["Dai", "Jifeng", ""]]}, {"id": "1811.11168", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai", "title": "Deformable ConvNets v2: More Deformable, Better Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superior performance of Deformable Convolutional Networks arises from its\nability to adapt to the geometric variations of objects. Through an examination\nof its adaptive behavior, we observe that while the spatial support for its\nneural features conforms more closely than regular ConvNets to object\nstructure, this support may nevertheless extend well beyond the region of\ninterest, causing features to be influenced by irrelevant image content. To\naddress this problem, we present a reformulation of Deformable ConvNets that\nimproves its ability to focus on pertinent image regions, through increased\nmodeling power and stronger training. The modeling power is enhanced through a\nmore comprehensive integration of deformable convolution within the network,\nand by introducing a modulation mechanism that expands the scope of deformation\nmodeling. To effectively harness this enriched modeling capability, we guide\nnetwork training via a proposed feature mimicking scheme that helps the network\nto learn features that reflect the object focus and classification power of\nR-CNN features. With the proposed contributions, this new version of Deformable\nConvNets yields significant performance gains over the original model and\nproduces leading results on the COCO benchmark for object detection and\ninstance segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:58:11 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 09:33:04 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zhu", "Xizhou", ""], ["Hu", "Han", ""], ["Lin", "Stephen", ""], ["Dai", "Jifeng", ""]]}, {"id": "1811.11187", "submitter": "Armen Avetisyan", "authors": "Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X.\n  Chang, Matthias Nie{\\ss}ner", "title": "Scan2CAD: Learning CAD Model Alignment in RGB-D Scans", "comments": "Video: https://youtu.be/PiHSYpgLTfA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Scan2CAD, a novel data-driven method that learns to align clean 3D\nCAD models from a shape database to the noisy and incomplete geometry of a\ncommodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method\ntakes as input a set of CAD models, and predicts a 9DoF pose that aligns each\nmodel to the underlying scan geometry. To tackle this problem, we create a new\nscan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated\nkeypoint pairs between 14225 CAD models from ShapeNet and their counterpart\nobjects in the scans. Our method selects a set of representative keypoints in a\n3D scan for which we find correspondences to the CAD geometry. To this end, we\ndesign a novel 3D CNN architecture that learns a joint embedding between real\nand synthetic objects, and from this predicts a correspondence heatmap. Based\non these correspondence heatmaps, we formulate a variational energy\nminimization that aligns a given set of CAD models to the reconstruction. We\nevaluate our approach on our newly introduced Scan2CAD benchmark where we\noutperform both handcrafted feature descriptor as well as state-of-the-art CNN\nbased methods by 21.39%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:00:06 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Avetisyan", "Armen", ""], ["Dahnert", "Manuel", ""], ["Dai", "Angela", ""], ["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1811.11205", "submitter": "Zhourong Chen", "authors": "Zhourong Chen, Yang Li, Samy Bengio, Si Si", "title": "You Look Twice: GaterNet for Dynamic Filter Selection in CNNs", "comments": "CVPR2019; Google Research, The Hong Kong University of Science and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of conditional computation for deep nets has been proposed\npreviously to improve model performance by selectively using only parts of the\nmodel conditioned on the sample it is processing. In this paper, we investigate\ninput-dependent dynamic filter selection in deep convolutional neural networks\n(CNNs). The problem is interesting because the idea of forcing different parts\nof the model to learn from different types of samples may help us acquire\nbetter filters in CNNs, improve the model generalization performance and\npotentially increase the interpretability of model behavior. We propose a novel\nyet simple framework called GaterNet, which involves a backbone and a gater\nnetwork. The backbone network is a regular CNN that performs the major\ncomputation needed for making a prediction, while a global gater network is\nintroduced to generate binary gates for selectively activating filters in the\nbackbone network based on each input. Extensive experiments on CIFAR and\nImageNet datasets show that our models consistently outperform the original\nmodels with a large margin. On CIFAR-10, our model also improves upon\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:14:49 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 17:57:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Zhourong", ""], ["Li", "Yang", ""], ["Bengio", "Samy", ""], ["Si", "Si", ""]]}, {"id": "1811.11209", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, David Held, Christoph Mertz, Martial Hebert", "title": "Iterative Transformer Network for 3D Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:22:24 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 02:48:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Yuan", "Wentao", ""], ["Held", "David", ""], ["Mertz", "Christoph", ""], ["Hebert", "Martial", ""]]}, {"id": "1811.11212", "submitter": "Neil Houlsby", "authors": "Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby", "title": "Self-Supervised GANs via Auxiliary Rotation Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:30:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 14:25:35 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Ting", ""], ["Zhai", "Xiaohua", ""], ["Ritter", "Marvin", ""], ["Lucic", "Mario", ""], ["Houlsby", "Neil", ""]]}, {"id": "1811.11226", "submitter": "Blaine Rister", "authors": "Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi and\n  Daniel L. Rubin", "title": "CT organ segmentation using GPU data augmentation, unsupervised labels\n  and IOU loss", "comments": "Journal submission pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-convolutional neural networks have achieved superior performance in a\nvariety of image segmentation tasks. However, their training requires laborious\nmanual annotation of large datasets, as well as acceleration by parallel\nprocessors with high-bandwidth memory, such as GPUs. We show that simple models\ncan achieve competitive accuracy for organ segmentation on CT images when\ntrained with extensive data augmentation, which leverages existing graphics\nhardware to quickly apply geometric and photometric transformations to 3D image\ndata. On 3 mm^3 CT volumes, our GPU implementation is 2.6-8X faster than a\nwidely-used CPU version, including communication overhead. We also show how to\nautomatically generate training labels using rudimentary morphological\noperations, which are efficiently computed by 3D Fourier transforms. We\ncombined fully-automatic labels for the lungs and bone with semi-automatic ones\nfor the liver, kidneys and bladder, to create a dataset of 130 labeled CT\nscans. To achieve the best results from data augmentation, our model uses the\nintersection-over-union (IOU) loss function, a close relative of the Dice loss.\nWe discuss its mathematical properties and explain why it outperforms the usual\nweighted cross-entropy loss for unbalanced segmentation tasks. We conclude that\nthere is no unique IOU loss function, as the naive one belongs to a broad\nfamily of functions with the same essential properties. When combining data\naugmentation with the IOU loss, our model achieves a Dice score of 78-92% for\neach organ. The trained model, code and dataset will be made publicly\navailable, to further medical imaging research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:53:59 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Rister", "Blaine", ""], ["Yi", "Darvin", ""], ["Shivakumar", "Kaushik", ""], ["Nobashi", "Tomomi", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1811.11239", "submitter": "Wei Tang", "authors": "Wei Tang, John Corring, Ying Wu and Gang Hua", "title": "A Compositional Textual Model for Recognition of Imperfect Word Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Printed text recognition is an important problem for industrial OCR systems.\nPrinted text is constructed in a standard procedural fashion in most settings.\nWe develop a mathematical model for this process that can be applied to the\nbackward inference problem of text recognition from an image. Through ablation\nexperiments we show that this model is realistic and that a multi-task\nobjective setting can help to stabilize estimation of its free parameters,\nenabling use of conventional deep learning methods. Furthermore, by directly\nmodeling the geometric perturbations of text synthesis we show that our model\ncan help recover missing characters from incomplete text regions, the bane of\nmulticomponent OCR systems, enabling recognition even when the detection\nreturns incomplete information.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:23:02 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Tang", "Wei", ""], ["Corring", "John", ""], ["Wu", "Ying", ""], ["Hua", "Gang", ""]]}, {"id": "1811.11243", "submitter": "Chao-Hui Huang", "authors": "Chao-Hui Huang and Daniel Racoceanu", "title": "eXclusive Autoencoder (XAE) for Nucleus Detection and Classification on\n  Hematoxylin and Eosin (H&E) Stained Histopathological Images", "comments": "11 pages, 7 figures, 5 tables, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduced a novel feature extraction approach, named\nexclusive autoencoder (XAE), which is a supervised version of autoencoder (AE),\nable to largely improve the performance of nucleus detection and classification\non hematoxylin and eosin (H&E) histopathological images. The proposed XAE can\nbe used in any AE-based algorithm, as long as the data labels are also provided\nin the feature extraction phase. In the experiments, we evaluated the\nperformance of an approach which is the combination of an XAE and a fully\nconnected neural network (FCN) and compared with some AE-based methods. For a\nnucleus detection problem (considered as a nucleus/non-nucleus classification\nproblem) on breast cancer H&E images, the F-score of the proposed XAE+FCN\napproach achieved 96.64% while the state-of-the-art was at 84.49%. For nucleus\nclassification on colorectal cancer H&E images, with the annotations of four\ncategories of epithelial, inflammatory, fibroblast and miscellaneous nuclei.\nThe F-score of the proposed method reached 70.4%. We also proposed a lymphocyte\nsegmentation method. In the step of lymphocyte detection, we have compared with\ncutting-edge technology and gained improved performance from 90% to 98.67%. We\nalso proposed an algorithm for lymphocyte segmentation based on nucleus\ndetection and classification. The obtained Dice coefficient achieved 88.31%\nwhile the cutting-edge approach was at 74%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:27:46 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Huang", "Chao-Hui", ""], ["Racoceanu", "Daniel", ""]]}, {"id": "1811.11251", "submitter": "Yilun Zhang", "authors": "Yilun Zhang, Hyun Soo Park", "title": "Multiview Supervision By Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a semi-supervised learning framework to train a keypoint\ndetector using multiview image streams given the limited labeled data\n(typically $<$4\\%). We leverage the complementary relationship between\nmultiview geometry and visual tracking to provide three types of supervisionary\nsignals to utilize the unlabeled data: (1) keypoint detection in one view can\nbe supervised by other views via the epipolar geometry; (2) a keypoint moves\nsmoothly over time where its optical flow can be used to temporally supervise\nconsecutive image frames to each other; (3) visible keypoint in one view is\nlikely to be visible in the adjacent view. We integrate these three signals in\na differentiable fashion to design a new end-to-end neural network composed of\nthree pathways. This design allows us to extensively use the unlabeled data to\ntrain the keypoint detector. We show that our approach outperforms existing\ndetectors including DeepLabCut tailored to the keypoint detection of non-human\nspecies such as monkeys, dogs, and mice.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:46:11 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 03:00:13 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Yilun", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1811.11254", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Junlin Yang, Lin Gu, Nicha Dvornek", "title": "ShelfNet for Fast Semantic Segmentation", "comments": null, "journal-ref": "2019 IEEE International Conference on Computer Vision (ICCV),\n  CVRSUAD", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ShelfNet, a novel architecture for accurate fast\nsemantic segmentation. Different from the single encoder-decoder structure,\nShelfNet has multiple encoder-decoder branch pairs with skip connections at\neach spatial level, which looks like a shelf with multiple columns. The\nshelf-shaped structure can be viewed as an ensemble of multiple deep and\nshallow paths, thus improving accuracy. We significantly reduce computation\nburden by reducing channel number, at the same time achieving high accuracy\nwith this unique structure. In addition, we propose a shared-weight strategy in\nthe residual block which reduces parameter number without sacrificing\nperformance. Compared with popular non real-time methods such as PSPNet, our\nShelfNet achieves 4$\\times$ faster inference speed with similar accuracy on\nPASCAL VOC dataset. Compared with real-time segmentation models such as\nBiSeNet, our model achieves higher accuracy at comparable speed on the\nCityscapes Dataset, enabling the application in speed-demanding tasks such as\nstreet-scene understanding for autonomous driving. Furthermore, our ShelfNet\nachieves 79.0\\% mIoU on Cityscapes Dataset with ResNet34 backbone,\noutperforming PSPNet and BiSeNet with large backbones such as ResNet101.\nThrough extensive experiments, we validated the superior performance of\nShelfNet. We provide link to the implementation\n\\url{https://github.com/juntang-zhuang/ShelfNet-lw-cityscapes}.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:57:48 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 20:39:06 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 02:24:06 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 09:51:49 GMT"}, {"version": "v5", "created": "Sat, 15 Dec 2018 14:13:15 GMT"}, {"version": "v6", "created": "Wed, 4 Sep 2019 03:59:15 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhuang", "Juntang", ""], ["Yang", "Junlin", ""], ["Gu", "Lin", ""], ["Dvornek", "Nicha", ""]]}, {"id": "1811.11269", "submitter": "Greg Olmschenk", "authors": "Greg Olmschenk, Zhigang Zhu, Hao Tang", "title": "Generalizing semi-supervised generative adversarial networks to\n  regression using feature contrasting", "comments": null, "journal-ref": "Computer Vision and Image Understanding, Volume 186, September\n  2019, Pages 1-12", "doi": "10.1016/j.cviu.2019.06.004", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we generalize semi-supervised generative adversarial networks\n(GANs) from classification problems to regression problems. In the last few\nyears, the importance of improving the training of neural networks using\nsemi-supervised training has been demonstrated for classification problems. We\npresent a novel loss function, called feature contrasting, resulting in a\ndiscriminator which can distinguish between fake and real data based on feature\nstatistics. This method avoids potential biases and limitations of alternative\napproaches. The generalization of semi-supervised GANs to the regime of\nregression problems of opens their use to countless applications as well as\nproviding an avenue for a deeper understanding of how GANs function. We first\ndemonstrate the capabilities of semi-supervised regression GANs on a toy\ndataset which allows for a detailed understanding of how they operate in\nvarious circumstances. This toy dataset is used to provide a theoretical basis\nof the semi-supervised regression GAN. We then apply the semi-supervised\nregression GANs to a number of real-world computer vision applications: age\nestimation, driving steering angle prediction, and crowd counting from single\nimages. We perform extensive tests of what accuracy can be achieved with\nsignificantly reduced annotated data. Through the combination of the\ntheoretical example and real-world scenarios, we demonstrate how\nsemi-supervised GANs can be generalized to regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:31:33 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 21:37:15 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 17:36:13 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Olmschenk", "Greg", ""], ["Zhu", "Zhigang", ""], ["Tang", "Hao", ""]]}, {"id": "1811.11283", "submitter": "Raviteja Vemulapalli", "authors": "Raviteja Vemulapalli and Aseem Agarwala", "title": "A Compact Embedding for Facial Expression Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:00:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 22:46:44 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Agarwala", "Aseem", ""]]}, {"id": "1811.11286", "submitter": "Wang Yifan", "authors": "Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga\n  Sorkine-Hornung", "title": "Patch-based Progressive 3D Point Set Upsampling", "comments": "accepted to cvpr2019, code available at https://github.com/yifita/P3U", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detail-driven deep neural network for point set upsampling. A\nhigh-resolution point set is essential for point-based rendering and surface\nreconstruction. Inspired by the recent success of neural image super-resolution\ntechniques, we progressively train a cascade of patch-based upsampling networks\non different levels of detail end-to-end. We propose a series of architectural\ndesign contributions that lead to a substantial performance boost. The effect\nof each technical contribution is demonstrated in an ablation study.\nQualitative and quantitative experiments show that our method significantly\noutperforms the state-of-the-art learning-based and optimazation-based\napproaches, both in terms of handling low-resolution inputs and revealing\nhigh-fidelity details.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:01:55 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 12:42:31 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 17:08:50 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Yifan", "Wang", ""], ["Wu", "Shihao", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1811.11296", "submitter": "Richard Marriott", "authors": "Richard T. Marriott, Sami Romdhani and Liming Chen", "title": "Taking Control of Intra-class Variation in Conditional GANs Under Weak\n  Supervision", "comments": null, "journal-ref": "in 2020 15th IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2020) (FG), Buenos Aires, AR, 2020 pp. 283-290", "doi": "10.1109/FG47880.2020.00042", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are able to learn mappings between\nsimple, relatively low-dimensional, random distributions and points on the\nmanifold of realistic images in image-space. The semantics of this mapping,\nhowever, are typically entangled such that meaningful image properties cannot\nbe controlled independently of one another. Conditional GANs (cGANs) provide a\npotential solution to this problem, allowing specific semantics to be enforced\nduring training. This solution, however, depends on the availability of precise\nlabels, which are sometimes difficult or near impossible to obtain, e.g. labels\nrepresenting lighting conditions or describing the background. In this paper we\nintroduce a new formulation of the cGAN that is able to learn disentangled,\nmultivariate models of semantically meaningful variation and which has the\nadvantage of requiring only the weak supervision of binary attribute labels.\nFor example, given only labels of ambient / non-ambient lighting, our method is\nable to learn multivariate lighting models disentangled from other factors such\nas the identity and pose. We coin the method intra-class variation isolation\n(IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA\ndataset and on synthetic 3D morphable model data, learning to disentangle\nattributes such as lighting, pose, expression, and even the background.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:38:29 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 00:10:05 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marriott", "Richard T.", ""], ["Romdhani", "Sami", ""], ["Chen", "Liming", ""]]}, {"id": "1811.11304", "submitter": "Mahyar Najibi", "authors": "Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S. Davis,\n  Tom Goldstein", "title": "Universal Adversarial Training", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard adversarial attacks change the predicted class label of a selected\nimage by adding specially tailored small perturbations to its pixels. In\ncontrast, a universal perturbation is an update that can be added to any image\nin a broad class of images, while still changing the predicted class label. We\nstudy the efficient generation of universal adversarial perturbations, and also\nefficient methods for hardening networks to these attacks. We propose a simple\noptimization-based universal attack that reduces the top-1 accuracy of various\nnetwork architectures on ImageNet to less than 20%, while learning the\nuniversal perturbation 13X faster than the standard method.\n  To defend against these perturbations, we propose universal adversarial\ntraining, which models the problem of robust classifier generation as a\ntwo-player min-max game, and produces robust models with only 2X the cost of\nnatural training. We also propose a simultaneous stochastic gradient method\nthat is almost free of extra computation, which allows us to do universal\nadversarial training on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:09:27 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:57:36 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shafahi", "Ali", ""], ["Najibi", "Mahyar", ""], ["Xu", "Zheng", ""], ["Dickerson", "John", ""], ["Davis", "Larry S.", ""], ["Goldstein", "Tom", ""]]}, {"id": "1811.11314", "submitter": "Frederico Guth", "authors": "Fred Guth and Teofilo E. deCampos", "title": "Skin lesion segmentation using U-Net and good training strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we approach the problem of skin lesion segmentation using a\nconvolutional neural network based on the U-Net architecture. We present a set\nof training strategies that had a significant impact on the performance of this\nmodel. We evaluated this method on the ISIC Challenge 2018 - Skin Lesion\nAnalysis Towards Melanoma Detection, obtaining threshold Jaccard index of\n77.5%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:45:58 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Guth", "Fred", ""], ["deCampos", "Teofilo E.", ""]]}, {"id": "1811.11318", "submitter": "Hongyu Xu", "authors": "Hongyu Xu, Xutao Lv, Xiaoyu Wang, Zhou Ren, Navaneeth Bodla and Rama\n  Chellappa", "title": "Deep Regionlets: Blended Representation and Deep Learning for Generic\n  Object Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.02408", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel object detection algorithm named \"Deep\nRegionlets\" by integrating deep neural networks and a conventional detection\nschema for accurate generic object detection. Motivated by the effectiveness of\nregionlets for modeling object deformations and multiple aspect ratios, we\nincorporate regionlets into an end-to-end trainable deep learning framework.\nThe deep regionlets framework consists of a region selection network and a deep\nregionlet learning module. Specifically, given a detection bounding box\nproposal, the region selection network provides guidance on where to select\nsub-regions from which features can be learned from. An object proposal\ntypically contains 3-16 sub-regions. The regionlet learning module focuses on\nlocal feature selection and transformations to alleviate the effects of\nappearance variations. To this end, we first realize non-rectangular region\nselection within the detection framework to accommodate variations in object\nappearance. Moreover, we design a \"gating network\" within the regionlet leaning\nmodule to enable instance dependent soft feature selection and pooling. The\nDeep Regionlets framework is trained end-to-end without additional efforts. We\npresent ablation studies and extensive experiments on the PASCAL VOC dataset\nand the Microsoft COCO dataset. The proposed method yields competitive\nperformance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN,\neven without additional segmentation labels.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:13:00 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 05:37:18 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Xu", "Hongyu", ""], ["Lv", "Xutao", ""], ["Wang", "Xiaoyu", ""], ["Ren", "Zhou", ""], ["Bodla", "Navaneeth", ""], ["Chellappa", "Rama", ""]]}, {"id": "1811.11323", "submitter": "Xinshuo Weng", "authors": "Shangxuan Wu, Xinshuo Weng", "title": "Image Labeling with Markov Random Fields and Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods for object segmentation in computer vision are\nformulated as a labeling task. This, in general, could be transferred to a\npixel-wise label assignment task, which is quite similar to the structure of\nhidden Markov random field. In terms of Markov random field, each pixel can be\nregarded as a state and has a transition probability to its neighbor pixel, the\nlabel behind each pixel is a latent variable and has an emission probability\nfrom its corresponding state. In this paper, we reviewed several modern image\nlabeling methods based on Markov random field and conditional random Field. And\nwe compare the result of these methods with some classical image labeling\nmethods. The experiment demonstrates that the introduction of Markov random\nfield and conditional random field make a big difference in the segmentation\nresult.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:36:58 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 20:20:43 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wu", "Shangxuan", ""], ["Weng", "Xinshuo", ""]]}, {"id": "1811.11325", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Wentao Han", "title": "CyLKs: Unsupervised Cycle Lucas-Kanade Network for Landmark Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across a majority of modern learning-based tracking systems, expensive\nannotations are needed to achieve state-of-the-art performance. In contrast,\nthe Lucas-Kanade (LK) algorithm works well without any annotation. However, LK\nhas a strong assumption of photometric (brightness) consistency on image\nintensity and is easy to drift because of large motion, occlusion, and aperture\nproblem. To relax the assumption and alleviate the drift problem, we propose\nCyLKs, a data-driven way of training Lucas-Kanade in an unsupervised manner.\nCyLKs learns a feature transformation through CNNs, transforming the input\nimages to a feature space which is especially favorable to LK tracking. During\ntraining, we perform differentiable Lucas-Kanade forward and backward on the\nconvolutional feature maps, and then minimize the re-projection error. During\ntesting, we perform the LK tracking on the learned features. We apply our model\nto the task of landmark tracking and perform experiments on datasets of THUMOS\nand 300VW.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:41:29 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 17:32:59 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 20:35:02 GMT"}, {"version": "v4", "created": "Tue, 21 May 2019 01:50:54 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Weng", "Xinshuo", ""], ["Han", "Wentao", ""]]}, {"id": "1811.11329", "submitter": "Xinshuo Weng", "authors": "Sen Wang, Daoyuan Jia, Xinshuo Weng", "title": "Deep Reinforcement Learning for Autonomous Driving", "comments": "no time for further improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has steadily improved and outperform human in lots of\ntraditional games since the resurgence of deep neural network. However, these\nsuccess is not easy to be copied to autonomous driving because the state spaces\nin real world are extreme complex and action spaces are continuous and fine\ncontrol is required. Moreover, the autonomous driving vehicles must also keep\nfunctional safety under the complex environments. To deal with these\nchallenges, we first adopt the deep deterministic policy gradient (DDPG)\nalgorithm, which has the capacity to handle complex state and action spaces in\ncontinuous domain. We then choose The Open Racing Car Simulator (TORCS) as our\nenvironment to avoid physical damage. Meanwhile, we select a set of appropriate\nsensor information from TORCS and design our own rewarder. In order to fit DDPG\nalgorithm to TORCS, we design our network architecture for both actor and\ncritic inside DDPG paradigm. To demonstrate the effectiveness of our model, We\nevaluate on different modes in TORCS and show both quantitative and qualitative\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:56:57 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 02:57:33 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 20:51:26 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Sen", ""], ["Jia", "Daoyuan", ""], ["Weng", "Xinshuo", ""]]}, {"id": "1811.11356", "submitter": "Drew Linsley", "authors": "Drew Linsley, Junkyung Kim, David Berson, and Thomas Serre", "title": "Robust neural circuit reconstruction from serial electron microscopy\n  with convolutional recurrent networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in deep learning have started to impact neuroscience. Of\nparticular significance are claims that current segmentation algorithms achieve\n\"super-human\" accuracy in an area known as connectomics. However, as we will\nshow, these algorithms do not effectively generalize beyond the particular\nsource and brain tissues used for training -- severely limiting their usability\nby the broader neuroscience community. To fill this gap, we describe a novel\nconnectomics challenge for source- and tissue-agnostic reconstruction of\nneurons (STAR), which favors broad generalization over fitting specific\ndatasets. We first demonstrate that current state-of-the-art approaches to\nneuron segmentation perform poorly on the challenge. We further describe a\nnovel convolutional recurrent neural network module that combines short-range\nhorizontal connections within a processing stage and long-range top-down\nconnections between stages. The resulting architecture establishes the state of\nthe art on the STAR challenge and represents a significant step towards widely\nusable and fully-automated connectomics analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:26:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 17:03:59 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 00:45:27 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2020 22:04:45 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Linsley", "Drew", ""], ["Kim", "Junkyung", ""], ["Berson", "David", ""], ["Serre", "Thomas", ""]]}, {"id": "1811.11358", "submitter": "Suhani Vora", "authors": "Suhani Vora, Reza Mahjourian, Soeren Pirk, Anelia Angelova", "title": "Future Segmentation Using 3D Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future to anticipate the outcome of events and actions is a\ncritical attribute of autonomous agents; particularly for agents which must\nrely heavily on real time visual data for decision making. Working towards this\ncapability, we address the task of predicting future frame segmentation from a\nstream of monocular video by leveraging the 3D structure of the scene. Our\nframework is based on learnable sub-modules capable of predicting pixel-wise\nscene semantic labels, depth, and camera ego-motion of adjacent frames. We\nfurther propose a recurrent neural network based model capable of predicting\nfuture ego-motion trajectory as a function of a series of past ego-motion\nsteps. Ultimately, we observe that leveraging 3D structure in the model\nfacilitates successful prediction, achieving state of the art accuracy in\nfuture semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:31:13 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Vora", "Suhani", ""], ["Mahjourian", "Reza", ""], ["Pirk", "Soeren", ""], ["Angelova", "Anelia", ""]]}, {"id": "1811.11365", "submitter": "Kai Fan Dr", "authors": "Yuanhang Su, Kai Fan, Nguyen Bach, C.-C. Jay Kuo, Fei Huang", "title": "Unsupervised Multi-modal Neural Machine Translation", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results with only large monolingual corpora in each language.\nHowever, the uncertainty of associating target with source sentences makes UNMT\ntheoretically an ill-posed problem. This work investigates the possibility of\nutilizing images for disambiguation to improve the performance of UNMT. Our\nassumption is intuitively based on the invariant property of image, i.e., the\ndescription of the same visual content by different languages should be\napproximately similar. We propose an unsupervised multi-modal machine\ntranslation (UMNMT) framework based on the language translation cycle\nconsistency loss conditional on the image, targeting to learn the bidirectional\nmulti-modal translation simultaneously. Through an alternate training between\nmulti-modal and uni-modal, our inference model can translate with or without\nthe image. On the widely used Multi30K dataset, the experimental results of our\napproach are significantly better than those of the text-only UNMT on the 2016\ntest dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:48:25 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 02:00:51 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Su", "Yuanhang", ""], ["Fan", "Kai", ""], ["Bach", "Nguyen", ""], ["Kuo", "C. -C. Jay", ""], ["Huang", "Fei", ""]]}, {"id": "1811.11373", "submitter": "Alessio Lomuscio", "authors": "Panagiotis Kouvaros and Alessio Lomuscio", "title": "Formal Verification of CNN-based Perception Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying neural-based perception systems\nimplemented by convolutional neural networks. We define a notion of local\nrobustness based on affine and photometric transformations. We show the notion\ncannot be captured by previously employed notions of robustness. The method\nproposed is based on reachability analysis for feed-forward neural networks and\nrelies on MILP encodings of both the CNNs and transformations under question.\nWe present an implementation and discuss the experimental results obtained for\na CNN trained from the MNIST data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 03:36:25 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kouvaros", "Panagiotis", ""], ["Lomuscio", "Alessio", ""]]}, {"id": "1811.11375", "submitter": "Peng Lu", "authors": "Peng Lu, Hangyu Lin, Yanwei Fu, Shaogang Gong, Yu-Gang Jiang,\n  Xiangyang Xue", "title": "Instance-level Sketch-based Retrieval by Deep Triplet Classification\n  Siamese Network", "comments": "we will significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decide to withdraw it now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch has been employed as an effective communicative tool to express the\nabstract and intuitive meanings of object. Recognizing the free-hand sketch\ndrawing is extremely useful in many real-world applications. While\ncontent-based sketch recognition has been studied for several decades, the\ninstance-level Sketch-Based Image Retrieval (SBIR) tasks have attracted\nsignificant research attention recently. The existing datasets such as\nQMUL-Chair and QMUL-Shoe, focus on the retrieval tasks of chairs and shoes.\nHowever, there are several key limitations in previous instance-level SBIR\nworks. The state-of-the-art works have to heavily rely on the pre-training\nprocess, quality of edge maps, multi-cropping testing strategy, and augmenting\nsketch images. To efficiently solve the instance-level SBIR, we propose a new\nDeep Triplet Classification Siamese Network (DeepTCNet) which employs\nDenseNet-169 as the basic feature extractor and is optimized by the triplet\nloss and classification loss. Critically, our proposed DeepTCNet can break the\nlimitations existed in previous works. The extensive experiments on five\nbenchmark sketch datasets validate the effectiveness of the proposed model.\nAdditionally, to study the tasks of sketch-based hairstyle retrieval, this\npaper contributes a new instance-level photo-sketch dataset - Hairstyle\nPhoto-Sketch dataset, which is composed of 3600 sketches and photos, and 2400\nsketch-photo pairs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 03:52:18 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 11:05:17 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lu", "Peng", ""], ["Lin", "Hangyu", ""], ["Fu", "Yanwei", ""], ["Gong", "Shaogang", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1811.11387", "submitter": "Longlong Jing", "authors": "Longlong Jing, Xiaodong Yang, Jingen Liu, Yingli Tian", "title": "Self-Supervised Spatiotemporal Feature Learning via Video Rotation\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:04:34 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 02:51:59 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Jing", "Longlong", ""], ["Yang", "Xiaodong", ""], ["Liu", "Jingen", ""], ["Tian", "Yingli", ""]]}, {"id": "1811.11389", "submitter": "Bo Zhao", "authors": "Bo Zhao and Lili Meng and Weidong Yin and Leonid Sigal", "title": "Image Generation from Layout", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent progress on generative models, controlled\ngeneration of images depicting multiple and complex object layouts is still a\ndifficult problem. Among the core challenges are the diversity of appearance a\ngiven object may possess and, as a result, exponential set of images consistent\nwith a specified layout. To address these challenges, we propose a novel\napproach for layout-based image generation; we call it Layout2Im. Given the\ncoarse spatial layout (bounding boxes + object categories), our model can\ngenerate a set of realistic images which have the correct objects in the\ndesired locations. The representation of each object is disentangled into a\nspecified/certain part (category) and an unspecified/uncertain part\n(appearance). The category is encoded using a word embedding and the appearance\nis distilled into a low-dimensional vector sampled from a normal distribution.\nIndividual object representations are composed together using convolutional\nLSTM, to obtain an encoding of the complete layout, and then decoded to an\nimage. Several loss terms are introduced to encourage accurate and diverse\ngeneration. The proposed Layout2Im model significantly outperforms the previous\nstate of the art, boosting the best reported inception score by 24.66% and\n28.57% on the very challenging COCO-Stuff and Visual Genome datasets,\nrespectively. Extensive experiments also demonstrate our method's ability to\ngenerate complex and diverse images with multiple objects.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:11:14 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 04:35:56 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 21:17:06 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhao", "Bo", ""], ["Meng", "Lili", ""], ["Yin", "Weidong", ""], ["Sigal", "Leonid", ""]]}, {"id": "1811.11397", "submitter": "Li Ding", "authors": "Li Ding, Chen Feng", "title": "DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds", "comments": "This paper has been accepted for CVPR'19 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepMapping, a novel registration framework using deep neural\nnetworks (DNNs) as auxiliary functions to align multiple point clouds from\nscratch to a globally consistent frame. We use DNNs to model the highly\nnon-convex mapping process that traditionally involves hand-crafted data\nassociation, sensor pose initialization, and global refinement. Our key novelty\nis that \"training\" these DNNs with properly defined unsupervised losses is\nequivalent to solving the underlying registration problem, but less sensitive\nto good initialization than ICP. Our framework contains two DNNs: a\nlocalization network that estimates the poses for input point clouds, and a map\nnetwork that models the scene structure by estimating the occupancy status of\nglobal coordinates. This allows us to convert the registration problem to a\nbinary occupancy classification, which can be solved efficiently using\ngradient-based optimization. We further show that DeepMapping can be readily\nextended to address the problem of Lidar SLAM by imposing geometric constraints\nbetween consecutive point clouds. Experiments are conducted on both simulated\nand real datasets. Qualitative and quantitative comparisons demonstrate that\nDeepMapping often enables more robust and accurate global registration of\nmultiple point clouds than existing techniques. Our code is available at\nhttps://ai4ce.github.io/DeepMapping/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:51:38 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 04:01:20 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ding", "Li", ""], ["Feng", "Chen", ""]]}, {"id": "1811.11405", "submitter": "Naiyan Wang", "authors": "Chuanchen Luo, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "Spectral Feature Transformation for Person Re-identification", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the surge of deep learning techniques, the field of person\nre-identification has witnessed rapid progress in recent years. Deep learning\nbased methods focus on learning a feature space where samples are clustered\ncompactly according to their corresponding identities. Most existing methods\nrely on powerful CNNs to transform the samples individually. In contrast, we\npropose to consider the sample relations in the transformation. To achieve this\ngoal, we incorporate spectral clustering technique into CNN. We derive a novel\nmodule named Spectral Feature Transformation and seamlessly integrate it into\nexisting CNN pipeline with negligible cost,which makes our method enjoy the\nbest of two worlds. Empirical studies show that the proposed approach\noutperforms previous state-of-the-art methods on four public benchmarks by a\nconsiderable margin without bells and whistles.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 06:46:38 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Luo", "Chuanchen", ""], ["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1811.11424", "submitter": "Yutong Feng", "authors": "Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, Yue Gao", "title": "MeshNet: Mesh Neural Network for 3D Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh is an important and powerful type of data for 3D shapes and widely\nstudied in the field of computer vision and computer graphics. Regarding the\ntask of 3D shape representation, there have been extensive research efforts\nconcentrating on how to represent 3D shapes well using volumetric grid,\nmulti-view and point cloud. However, there is little effort on using mesh data\nin recent years, due to the complexity and irregularity of mesh data. In this\npaper, we propose a mesh neural network, named MeshNet, to learn 3D shape\nrepresentation from mesh data. In this method, face-unit and feature splitting\nare introduced, and a general architecture with available and effective blocks\nare proposed. In this way, MeshNet is able to solve the complexity and\nirregularity problem of mesh and conduct 3D shape representation well. We have\napplied the proposed MeshNet method in the applications of 3D shape\nclassification and retrieval. Experimental results and comparisons with the\nstate-of-the-art methods demonstrate that the proposed MeshNet can achieve\nsatisfying 3D shape classification and retrieval performance, which indicates\nthe effectiveness of the proposed method on 3D shape representation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:48:51 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Feng", "Yutong", ""], ["Feng", "Yifan", ""], ["You", "Haoxuan", ""], ["Zhao", "Xibin", ""], ["Gao", "Yue", ""]]}, {"id": "1811.11431", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi", "title": "ESPNetv2: A Light-weight, Power Efficient, and General Purpose\n  Convolutional Neural Network", "comments": "Accepted at CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a light-weight, power efficient, and general purpose\nconvolutional neural network, ESPNetv2, for modeling visual and sequential\ndata. Our network uses group point-wise and depth-wise dilated separable\nconvolutions to learn representations from a large effective receptive field\nwith fewer FLOPs and parameters. The performance of our network is evaluated on\nfour different tasks: (1) object classification, (2) semantic segmentation, (3)\nobject detection, and (4) language modeling. Experiments on these tasks,\nincluding image classification on the ImageNet and language modeling on the\nPenTree bank dataset, demonstrate the superior performance of our method over\nthe state-of-the-art methods. Our network outperforms ESPNet by 4-5% and has\n2-4x fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared to\nYOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracy\nwith 6x fewer FLOPs. Our experiments show that ESPNetv2 is much more power\nefficient than existing state-of-the-art efficient methods including\nShuffleNets and MobileNets. Our code is open-source and available at\nhttps://github.com/sacmehta/ESPNetv2\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 08:01:09 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 04:48:39 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2019 04:34:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mehta", "Sachin", ""], ["Rastegari", "Mohammad", ""], ["Shapiro", "Linda", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1811.11436", "submitter": "Chang Jo Kim", "authors": "Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, Choongsang Cho", "title": "Neural Sign Language Translation based on Human Keypoint Estimation", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sign language translation system based on human keypoint\nestimation. It is well-known that many problems in the field of computer vision\nrequire a massive amount of dataset to train deep neural network models. The\nsituation is even worse when it comes to the sign language translation problem\nas it is far more difficult to collect high-quality training data. In this\npaper, we introduce the KETI (short for Korea Electronics Technology Institute)\nsign language dataset which consists of 14,672 videos of high resolution and\nquality. Considering the fact that each country has a different and unique sign\nlanguage, the KETI sign language dataset can be the starting line for further\nresearch on the Korean sign language translation. Using the KETI sign language\ndataset, we develop a neural network model for translating sign videos into\nnatural language sentences by utilizing the human keypoints extracted from a\nface, hands, and body parts. The obtained human keypoint vector is normalized\nby the mean and standard deviation of the keypoints and used as input to our\ntranslation model based on the sequence-to-sequence architecture. As a result,\nwe show that our approach is robust even when the size of the training data is\nnot sufficient. Our translation model achieves 93.28% (55.28%, respectively)\ntranslation accuracy on the validation set (test set, respectively) for 105\nsentences that can be used in emergency situations. We compare several types of\nour neural sign translation models based on different attention mechanisms in\nterms of classical metrics for measuring the translation performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 08:28:54 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 06:27:34 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ko", "Sang-Ki", ""], ["Kim", "Chang Jo", ""], ["Jung", "Hyedong", ""], ["Cho", "Choongsang", ""]]}, {"id": "1811.11455", "submitter": "Nir Zarrabi", "authors": "Nir Zarrabi, Shai Avidan, Yael Moses", "title": "CrowdCam: Dynamic Region Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting dynamic regions in CrowdCam images,\nwhere a dynamic region is the projection of a moving 3D object on the image\nplane. Quite often, these regions are the most interesting parts of an image.\nCrowdCam images is a set of images of the same dynamic event, captured by a\ngroup of non-collaborating users. Almost every event of interest today is\ncaptured this way. This new type of images raises the need to develop new\nalgorithms tailored specifically for it. We propose a comprehensive solution to\nthe problem. Our solution combines cues that are based on geometry, appearance\nand proximity. First, geometric reasoning is used to produce rough score maps\nthat determine, for every pixel, how likely it is to be the projection of a\nstatic or dynamic scene point. These maps are noisy because CrowdCam images are\nusually few and far apart both in space and in time. Then, we use similarity in\nappearance space and proximity in the image plane to encourage neighboring\npixels to be labeled similarly as either static or dynamic. We collected a new,\nand challenging, data set to evaluate our algorithm. Results show that the\nsuccess score of our algorithm is nearly double that of the current state of\nthe art approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:19:42 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 19:15:29 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zarrabi", "Nir", ""], ["Avidan", "Shai", ""], ["Moses", "Yael", ""]]}, {"id": "1811.11459", "submitter": "Artem Sevastopolsky", "authors": "Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, Victor\n  Lempitsky", "title": "Coordinate-based Texture Inpainting for Pose-Guided Image Generation", "comments": "Published in Proceedings of the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR). 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep learning approach to pose-guided resynthesis of human\nphotographs. At the heart of the new approach is the estimation of the complete\nbody surface texture based on a single photograph. Since the input photograph\nalways observes only a part of the surface, we suggest a new inpainting method\nthat completes the texture of the human body. Rather than working directly with\ncolors of texture elements, the inpainting network estimates an appropriate\nsource location in the input image for each element of the body surface. This\ncorrespondence field between the input image and the texture is then further\nwarped into the target image coordinate frame based on the desired pose,\neffectively establishing the correspondence between the source and the target\nview even when the pose change is drastic. The final convolutional network then\nuses the established correspondence and all other available information to\nsynthesize the output image. A fully-convolutional architecture with deformable\nskip connections guided by the estimated correspondence field is used. We show\nstate-of-the-art result for pose-guided image synthesis. Additionally, we\ndemonstrate the performance of our system for garment transfer and pose-guided\nface resynthesis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:33:00 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 08:50:27 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Grigorev", "Artur", ""], ["Sevastopolsky", "Artem", ""], ["Vakhitov", "Alexander", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1811.11465", "submitter": "Arianna Rampini", "authors": "Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael\n  M. Bronstein, Emanuele Rodol\\`a", "title": "Isospectralization, or how to hear shape, style, and correspondence", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": "10.1109/CVPR.2019.00771", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question whether one can recover the shape of a geometric object from its\nLaplacian spectrum ('hear the shape of the drum') is a classical problem in\nspectral geometry with a broad range of implications and applications. While\ntheoretically the answer to this question is negative (there exist examples of\niso-spectral but non-isometric manifolds), little is known about the practical\npossibility of using the spectrum for shape reconstruction and optimization. In\nthis paper, we introduce a numerical procedure called isospectralization,\nconsisting of deforming one shape to make its Laplacian spectrum match that of\nanother. We implement the isospectralization procedure using modern\ndifferentiable programming techniques and exemplify its applications in some of\nthe classical and notoriously hard problems in geometry processing, computer\nvision, and graphics such as shape reconstruction, pose and style transfer, and\ndense deformable correspondence.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:51:28 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:17:46 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cosmo", "Luca", ""], ["Panine", "Mikhail", ""], ["Rampini", "Arianna", ""], ["Ovsjanikov", "Maks", ""], ["Bronstein", "Michael M.", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "1811.11482", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Image Reconstruction with Predictive Filter Flow", "comments": "https://www.ics.uci.edu/~skong2/pff.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, interpretable framework for solving a wide range of\nimage reconstruction problems such as denoising and deconvolution. Given a\ncorrupted input image, the model synthesizes a spatially varying linear filter\nwhich, when applied to the input image, reconstructs the desired output. The\nmodel parameters are learned using supervised or self-supervised training. We\ntest this model on three tasks: non-uniform motion blur removal,\nlossy-compression artifact reduction and single image super resolution. We\ndemonstrate that our model substantially outperforms state-of-the-art methods\non all these tasks and is significantly faster than optimization-based\napproaches to deconvolution. Unlike models that directly predict output pixel\nvalues, the predicted filter flow is controllable and interpretable, which we\ndemonstrate by visualizing the space of predicted filters for different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:17:14 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1811.11489", "submitter": "Jun Beom Kho", "authors": "Jun Beom Kho, Andrew B. J. Teoh, Wonjune Lee, Jaihie Kim", "title": "Fixed-length Bit-string Representation of Fingerprint by Normalized\n  Local Structures", "comments": "16 pages, 15 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to represent a fingerprint image by an\nordered, fixed-length bit-string providing improved accuracy performance,\nfaster matching time and compressibility. First, we devise a novel\nminutia-based local structure modeled by a mixture of 2D elliptical Gaussian\nfunctions in the pixel space. Each local structure is mapped to the Euclidean\nspace by normalizing the local structure with the number of minutiae that\nassociates to it. This simple yet crucial crux enables fast dissimilarity\ncomputation of two local structures with Euclidean distance without distortion.\nA complementary texture-based local structure to the minutia-based local\nstructure is also introduced whereby both can be compressed via principal\ncomponent analysis and fused easily in the Euclidean space. The fused local\nstructure is then converted to a K-bit ordered string via a K-means clustering\nalgorithm. This chain of computation with sole use of Euclidean distance is\nvital for speedy and discriminative bit-string conversion. The accuracy can be\nfurther improved by a finger-specific bit-training algorithm in which two\ncriteria are leveraged to select useful bit positions for matching. Experiments\nare performed on Fingerprint Verification Competition (FVC) databases for\ncomparison with existing techniques to show the superiority of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:54:30 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kho", "Jun Beom", ""], ["Teoh", "Andrew B. J.", ""], ["Lee", "Wonjune", ""], ["Kim", "Jaihie", ""]]}, {"id": "1811.11507", "submitter": "Claudio Michaelis", "authors": "Claudio Michaelis, Ivan Ustyuzhaninov, Matthias Bethge, Alexander S.\n  Ecker", "title": "One-Shot Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of one-shot instance segmentation: Given an example\nimage of a novel, previously unknown object category, find and segment all\nobjects of this category within a complex scene. To address this challenging\nnew task, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese\nbackbone encoding both reference image and scene, allowing it to target\ndetection and segmentation towards the reference category. We demonstrate\nempirical results on MS Coco highlighting challenges of the one-shot setting:\nwhile transferring knowledge about instance segmentation to novel object\ncategories works very well, targeting the detection network towards the\nreference category appears to be more difficult. Our work provides a first\nstrong baseline for one-shot instance segmentation and will hopefully inspire\nfurther research into more powerful and flexible scene analysis algorithms.\nCode is available at: https://github.com/bethgelab/siamese-mask-rcnn\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:36:21 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 08:48:08 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Michaelis", "Claudio", ""], ["Ustyuzhaninov", "Ivan", ""], ["Bethge", "Matthias", ""], ["Ecker", "Alexander S.", ""]]}, {"id": "1811.11510", "submitter": "Jialun Liu", "authors": "Jialun Liu", "title": "Identity Preserving Generative Adversarial Network for Cross-Domain\n  Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is to retrieval pedestrian images from no-overlap\ncamera views detected by pedestrian detectors. Most existing person\nre-identification (re-ID) models often fail to generalize well from the source\ndomain where the models are trained to a new target domain without labels,\nbecause of the bias between the source and target domain. This issue\nsignificantly limits the scalability and usability of the models in the real\nworld. Providing a labeled source training set and an unlabeled target training\nset, the aim of this paper is to improve the generalization ability of re-ID\nmodels to the target domain. To this end, we propose an image generative\nnetwork named identity preserving generative adversarial network (IPGAN). The\nproposed method has two excellent properties: 1) only a single model is\nemployed to translate the labeled images from the source domain to the target\ncamera domains in an unsupervised manner; 2) The identity information of images\nfrom the source domain is preserved before and after translation. Furthermore,\nwe propose IBN-reID model for the person re-identification task. It has better\ngeneralization ability than baseline models, especially in the cases without\nany domain adaptation. The IBN-reID model is trained on the translated images\nby supervised methods. Experimental results on Market-1501 and DukeMTMC-reID\nshow that the images generated by IPGAN are more suitable for cross-domain\nperson re-identification. Very competitive re-ID accuracy is achieved by our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:52:34 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Liu", "Jialun", ""]]}, {"id": "1811.11524", "submitter": "Yuan Liu", "authors": "Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, Shih-Fu Chang", "title": "Multi-granularity Generator for Temporal Action Proposal", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temporal action proposal generation is an important task, aiming to localize\nthe video segments containing human actions in an untrimmed video. In this\npaper, we propose a multi-granularity generator (MGG) to perform the temporal\naction proposal from different granularity perspectives, relying on the video\nvisual features equipped with the position embedding information. First, we\npropose to use a bilinear matching model to exploit the rich local information\nwithin the video sequence. Afterwards, two components, namely segment proposal\nproducer (SPP) and frame actionness producer (FAP), are combined to perform the\ntask of temporal action proposal at two distinct granularities. SPP considers\nthe whole video in the form of feature pyramid and generates segment proposals\nfrom one coarse perspective, while FAP carries out a finer actionness\nevaluation for each video frame. Our proposed MGG can be trained in an\nend-to-end fashion. By temporally adjusting the segment proposals with\nfine-grained frame actionness information, MGG achieves the superior\nperformance over state-of-the-art methods on the public THUMOS-14 and\nActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to\nperform the classification of the proposals generated by MGG, leading to\nsignificant improvements compared against the competing methods for the video\ndetection task.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 12:47:16 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 06:06:42 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Liu", "Yuan", ""], ["Ma", "Lin", ""], ["Zhang", "Yifeng", ""], ["Liu", "Wei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1811.11553", "submitter": "Michael Alcorn", "authors": "Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai,\n  Wei-Shinn Ku, Anh Nguyen", "title": "Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses\n  of Familiar Objects", "comments": "Poster at the 2019 Conference on Computer Vision and Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite excellent performance on stationary test sets, deep neural networks\n(DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including\nnatural, non-adversarial ones, which are common in real-world settings. In this\npaper, we present a framework for discovering DNN failures that harnesses 3D\nrenderers and 3D models. That is, we estimate the parameters of a 3D renderer\nthat cause a target DNN to misbehave in response to the rendered image. Using\nour framework and a self-assembled dataset of 3D objects, we investigate the\nvulnerability of DNNs to OoD poses of well-known objects in ImageNet. For\nobjects that are readily recognized by DNNs in their canonical poses, DNNs\nincorrectly classify 97% of their pose space. In addition, DNNs are highly\nsensitive to slight pose perturbations. Importantly, adversarial poses transfer\nacross models and datasets. We find that 99.9% and 99.4% of the poses\nmisclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image\nclassifiers trained on the same ImageNet dataset, respectively, and 75.5%\ntransfer to the YOLOv3 object detector trained on MS COCO.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 13:39:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 23:55:45 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 13:54:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Alcorn", "Michael A.", ""], ["Li", "Qi", ""], ["Gong", "Zhitao", ""], ["Wang", "Chengfei", ""], ["Mai", "Long", ""], ["Ku", "Wei-Shinn", ""], ["Nguyen", "Anh", ""]]}, {"id": "1811.11566", "submitter": "Savas Ozkan", "authors": "Bora Baydar, Savas Ozkan, Gozde Bozdagi Akar", "title": "Automatic Liver Segmentation with Adversarial Loss and Convolutional\n  Neural Network", "comments": "Technical Report(In Turkish)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of medical images is among most demanded works in the\nmedical information field since it saves time of the experts in the field and\navoids human error factors. In this work, a method based on Conditional\nAdversarial Networks and Fully Convolutional Networks is proposed for the\nautomatic segmentation of the liver MRIs. The proposed method, without any\npost-processing, is achieved the second place in the SIU Liver Segmentation\nChallenge 2018, data of which is provided by Dokuz Eyl\\\"ul University. In this\npaper, some improvements for the post-processing step are also proposed and it\nis shown that with these additions, the method outperforms other baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 13:57:36 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Baydar", "Bora", ""], ["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1811.11582", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu", "title": "Continuous Trade-off Optimization between Fast and Accurate Deep Face\n  Detectors", "comments": "Accepted at ICONIP 2018. arXiv admin note: substantial text overlap\n  with arXiv:1803.08707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks offer better face detection results than\nshallow or handcrafted models, their complex architectures come with higher\ncomputational requirements and slower inference speeds than shallow neural\nnetworks. In this context, we study five straightforward approaches to achieve\nan optimal trade-off between accuracy and speed in face detection. All the\napproaches are based on separating the test images in two batches, an easy\nbatch that is fed to a faster face detector and a difficult batch that is fed\nto a more accurate yet slower detector. We conduct experiments on the AFW and\nthe FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD\n(Single Shot Scale-invariant Face Detector) as the accurate face detector, both\nmodels being pre-trained on the WIDER FACE data set. Our experiments show that\nthe proposed difficulty metrics compare favorably to a random split of the\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:16:22 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1811.11594", "submitter": "Gusi Te", "authors": "Wei Hu, Gusi Te, Ju He, Dong Chen, Zongming Guo", "title": "Exploring Hypergraph Representation on Face Anti-spoofing Beyond 2D\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing plays a crucial role in protecting face recognition\nsystems from various attacks. Previous model-based and deep learning approaches\nachieve satisfactory performance for 2D face spoofs, but remain limited for\nmore advanced 3D attacks such as vivid masks. In this paper, we address 3D face\nanti-spoofing via the proposed Hypergraph Convolutional Neural Networks\n(HGCNN). Firstly, we construct a computation-efficient and posture-invariant\nface representation with only a few key points on hypergraphs. The hypergraph\nrepresentation is then fed into the designed HGCNN with hypergraph convolution\nfor feature extraction, while the depth auxiliary is also exploited for 3D mask\nanti-spoofing. Further, we build a 3D face attack database with color, depth\nand infrared light information to overcome the deficiency of 3D face\nanti-spoofing data. Experiments show that our method achieves the\nstate-of-the-art performance over widely used 3D and 2D databases as well as\nthe proposed one under various tests.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:36:55 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 12:09:07 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Hu", "Wei", ""], ["Te", "Gusi", ""], ["He", "Ju", ""], ["Chen", "Dong", ""], ["Guo", "Zongming", ""]]}, {"id": "1811.11606", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Niloy Mitra, Tobias Ritschel", "title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PlatonicGAN to discover the 3D structure of an object class from\nan unstructured collection of 2D images, i.e., where no relation between photos\nis known, except that they are showing instances of the same category. The key\nidea is to train a deep neural network to generate 3D shapes which, when\nrendered to images, are indistinguishable from ground truth images (for a\ndiscriminator) under various camera poses. Discriminating 2D images instead of\n3D shapes allows tapping into unstructured 2D photo collections instead of\nrelying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish\nconstraints between 2D image observation and their 3D interpretation, we\nsuggest a family of rendering layers that are effectively differentiable. This\nfamily includes visual hull, absorption-only (akin to x-ray), and\nemission-absorption. We can successfully reconstruct 3D shapes from\nunstructured 2D images and extensively evaluate PlatonicGAN on a range of\nsynthetic and real data sets achieving consistent improvements over baseline\nmethods. We further show that PlatonicGAN can be combined with 3D supervision\nto improve on and in some cases even surpass the quality of 3D-supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:58:22 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 09:37:44 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 15:04:04 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 09:17:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Henzler", "Philipp", ""], ["Mitra", "Niloy", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1811.11611", "submitter": "Joakim Johnander", "authors": "Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan,\n  Michael Felsberg", "title": "A Generative Appearance Model for End-to-end Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental challenges in video object segmentation is to find an\neffective representation of the target and background appearance. The best\nperforming approaches resort to extensive fine-tuning of a convolutional neural\nnetwork for this purpose. Besides being prohibitively expensive, this strategy\ncannot be truly trained end-to-end since the online fine-tuning procedure is\nnot integrated into the offline training of the network.\n  To address these issues, we propose a network architecture that learns a\npowerful representation of the target and background appearance in a single\nforward pass. The introduced appearance module learns a probabilistic\ngenerative model of target and background feature distributions. Given a new\nimage, it predicts the posterior class probabilities, providing a highly\ndiscriminative cue, which is processed in later network modules. Both the\nlearning and prediction stages of our appearance module are fully\ndifferentiable, enabling true end-to-end training of the entire segmentation\npipeline. Comprehensive experiments demonstrate the effectiveness of the\nproposed approach on three video object segmentation benchmarks. We close the\ngap to approaches based on online fine-tuning on DAVIS17, while operating at 15\nFPS on a single GPU. Furthermore, our method outperforms all published\napproaches on the large-scale YouTube-VOS dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:11:43 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 13:50:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Johnander", "Joakim", ""], ["Danelljan", "Martin", ""], ["Brissman", "Emil", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1811.11623", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Martin Boyer, Andrew Lindley, David Schreiber,\n  Thomas Philipp", "title": "Large Scale Audio-Visual Video Analytics Platform for Forensic\n  Investigations of Terroristic Attacks", "comments": null, "journal-ref": "25th International Conference on MultiMedia Modeling (MMM2019)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forensic investigation of a terrorist attack poses a huge challenge to\nthe investigative authorities, as several thousand hours of video footage need\nto be spotted. To assist law enforcement agencies (LEA) in identifying suspects\nand securing evidences, we present a platform which fuses information of\nsurveillance cameras and video uploads from eyewitnesses. The platform\nintegrates analytical modules for different input-modalities on a scalable\narchitecture. Videos are analyzed according their acoustic and visual content.\nSpecifically, Audio Event Detection is applied to index the content according\nto attack-specific acoustic concepts. Audio similarity search is utilized to\nidentify similar video sequences recorded from different perspectives. Visual\nobject detection and tracking are used to index the content according to\nrelevant concepts. The heterogeneous results of the analytical modules are\nfused into a distributed index of visual and acoustic concepts to facilitate\nrapid start of investigations, following traits and investigating witness\nreports.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:22:03 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Schindler", "Alexander", ""], ["Boyer", "Martin", ""], ["Lindley", "Andrew", ""], ["Schreiber", "David", ""], ["Philipp", "Thomas", ""]]}, {"id": "1811.11644", "submitter": "Li Jing", "authors": "Li Jing, Rumen Dangovski, Marin Soljacic", "title": "WaveletNet: Logarithmic Scale Efficient Convolutional Neural Networks\n  for Edge Devices", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a logarithmic-scale efficient convolutional neural network\narchitecture for edge devices, named WaveletNet. Our model is based on the\nwell-known depthwise convolution, and on two new layers, which we introduce in\nthis work: a wavelet convolution and a depthwise fast wavelet transform. By\nbreaking the symmetry in channel dimensions and applying a fast algorithm,\nWaveletNet shrinks the complexity of convolutional blocks by an O(logD/D)\nfactor, where D is the number of channels. Experiments on CIFAR-10 and ImageNet\nclassification show superior and comparable performances of WaveletNet compared\nto state-of-the-art models such as MobileNetV2.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:04:30 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Jing", "Li", ""], ["Dangovski", "Rumen", ""], ["Soljacic", "Marin", ""]]}, {"id": "1811.11662", "submitter": "Zhishuai Zhang", "authors": "Zhishuai Zhang, Wei Shen, Siyuan Qiao, Yan Wang, Bo Wang, Alan Yuille", "title": "Robust Face Detection via Learning Small Faces on Hard Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent anchor-based deep face detectors have achieved promising performance,\nbut they are still struggling to detect hard faces, such as small, blurred and\npartially occluded faces. A reason is that they treat all images and faces\nequally, without putting more effort on hard ones; however, many training\nimages only contain easy faces, which are less helpful to achieve better\nperformance on hard images. In this paper, we propose that the robustness of a\nface detector against hard faces can be improved by learning small faces on\nhard images. Our intuitions are (1) hard images are the images which contain at\nleast one hard face, thus they facilitate training robust face detectors; (2)\nmost hard faces are small faces and other types of hard faces can be easily\nconverted to small faces by shrinking. We build an anchor-based deep face\ndetector, which only output a single feature map with small anchors, to\nspecifically learn small faces and train it by a novel hard image mining\nstrategy. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal\nFaces, and AFW datasets to show the effectiveness of our method. Our method\nachieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val\ndataset respectively, which surpass the previous state-of-the-arts, especially\non the hard subset. Code and model are available at\nhttps://github.com/bairdzhang/smallhardface.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:43:06 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zhang", "Zhishuai", ""], ["Shen", "Wei", ""], ["Qiao", "Siyuan", ""], ["Wang", "Yan", ""], ["Wang", "Bo", ""], ["Yuille", "Alan", ""]]}, {"id": "1811.11683", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl\n  Vondrick, and Shih-Fu Chang", "title": "Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of phrase grounding by lear ing a multi-level common\nsemantic space shared by the textual and visual modalities. We exploit multiple\nlevels of feature maps of a Deep Convolutional Neural Network, as well as\ncontextualized word and sentence embeddings extracted from a character-based\nlanguage model. Following dedicated non-linear mappings for visual features at\neach level, word, and sentence embeddings, we obtain multiple instantiations of\nour common semantic space in which comparisons between any target text and the\nvisual content is performed with cosine similarity. We guide the model by a\nmulti-level multimodal attention mechanism which outputs attended visual\nfeatures at each level. The best level is chosen to be compared with text\ncontent for maximizing the pertinence scores of image-sentence pairs of the\nground truth. Experiments conducted on three publicly available datasets show\nsignificant performance gains (20%-60% relative) over the state-of-the-art in\nphrase localization and set a new performance record on those datasets. We\nprovide a detailed ablation study to show the contribution of each element of\nour approach and release our code on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:05:27 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:49:53 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Akbari", "Hassan", ""], ["Karaman", "Svebor", ""], ["Bhargava", "Surabhi", ""], ["Chen", "Brian", ""], ["Vondrick", "Carl", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1811.11718", "submitter": "Guilin Liu", "authors": "Guilin Liu, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan\n  Sapra, Zhiding Yu, Andrew Tao, Bryan Catanzaro", "title": "Partial Convolution based Padding", "comments": "11 pages; code is available at https://github.com/NVIDIA/partialconv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple yet effective padding scheme that can be\nused as a drop-in module for existing convolutional neural networks. We call it\npartial convolution based padding, with the intuition that the padded region\ncan be treated as holes and the original input as non-holes. Specifically,\nduring the convolution operation, the convolution results are re-weighted near\nimage borders based on the ratios between the padded area and the convolution\nsliding window area. Extensive experiments with various deep network models on\nImageNet classification and semantic segmentation demonstrate that the proposed\npadding scheme consistently outperforms standard zero padding with better\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:13:02 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Liu", "Guilin", ""], ["Shih", "Kevin J.", ""], ["Wang", "Ting-Chun", ""], ["Reda", "Fitsum A.", ""], ["Sapra", "Karan", ""], ["Yu", "Zhiding", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1811.11721", "submitter": "Yunchao Wei", "authors": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi,\n  Wenyu Liu, Thomas S. Huang", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation", "comments": "IEEE TPAMI 2020 & ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information is vital in visual understanding problems, such as\nsemantic segmentation and object detection. We propose a Criss-Cross Network\n(CCNet) for obtaining full-image contextual information in a very effective and\nefficient way. Concretely, for each pixel, a novel criss-cross attention module\nharvests the contextual information of all the pixels on its criss-cross path.\nBy taking a further recurrent operation, each pixel can finally capture the\nfull-image dependencies. Besides, a category consistent loss is proposed to\nenforce the criss-cross attention module to produce more discriminative\nfeatures. Overall, CCNet is with the following merits: 1) GPU memory friendly.\nCompared with the non-local block, the proposed recurrent criss-cross attention\nmodule requires 11x less GPU memory usage. 2) High computational efficiency.\nThe recurrent criss-cross attention significantly reduces FLOPs by about 85% of\nthe non-local block. 3) The state-of-the-art performance. We conduct extensive\nexperiments on semantic segmentation benchmarks including Cityscapes, ADE20K,\nhuman parsing benchmark LIP, instance segmentation benchmark COCO, video\nsegmentation benchmark CamVid. In particular, our CCNet achieves the mIoU\nscores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K\nvalidation set and the LIP validation set respectively, which are the new\nstate-of-the-art results. The source codes are available at\n\\url{https://github.com/speedinghzl/CCNet}.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:18:27 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:17:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Huang", "Zilong", ""], ["Wang", "Xinggang", ""], ["Wei", "Yunchao", ""], ["Huang", "Lichao", ""], ["Shi", "Humphrey", ""], ["Liu", "Wenyu", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1811.11727", "submitter": "Siddharth Kannan", "authors": "Siddharth Kannan, Gaurav Yengera, Didier Mutter, Jacques Marescaux,\n  Nicolas Padoy", "title": "Future-State Predicting LSTM for Early Surgery Type Recognition", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TMI.2019.2931158", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach for the early recognition of the type of\na laparoscopic surgery from its video. Early recognition algorithms can be\nbeneficial to the development of 'smart' OR systems that can provide automatic\ncontext-aware assistance, and also enable quick database indexing. The task is\nhowever ridden with challenges specific to videos belonging to the domain of\nlaparoscopy, such as high visual similarity across surgeries and large\nvariations in video durations. To capture the spatio-temporal dependencies in\nthese videos, we choose as our model a combination of a Convolutional Neural\nNetwork (CNN) and Long Short-Term Memory (LSTM) network. We then propose two\ncomplementary approaches for improving early recognition performance. The first\napproach is a CNN fine-tuning method that encourages surgeries to be\ndistinguished based on the initial frames of laparoscopic videos. The second\napproach, referred to as 'Future-State Predicting LSTM', trains an LSTM to\npredict information related to future frames, which helps in distinguishing\nbetween the different types of surgeries. We evaluate our approaches on a large\ndataset of 425 laparoscopic videos containing 9 types of surgeries (Laparo425),\nand achieve on average an accuracy of 75% having observed only the first 10\nminutes of a surgery. These results are quite promising from a practical\nstandpoint and also encouraging for other types of image-guided surgeries.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:26:24 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 03:00:53 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Kannan", "Siddharth", ""], ["Yengera", "Gaurav", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1811.11729", "submitter": "Enze Zhang", "authors": "Enze Zhang, Fa Zhang, Zhiyong Liu, Xiaohua Wan, Lifa Zhu", "title": "SegET: Deep Neural Network with Rich Contextual Features for Cellular\n  Structures Segmentation in Electron Tomography Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron tomography (ET) allows high-resolution reconstructions of\nmacromolecular complexes at nearnative state. Cellular structures segmentation\nin the reconstruction data from electron tomographic images is often required\nfor analyzing and visualizing biological structures, making it a powerful tool\nfor quantitative descriptions of whole cell structures and understanding\nbiological functions. However, these cellular structures are rather difficult\nto automatically separate or quantify from view owing to complex molecular\nenvironment and the limitations of reconstruction data of ET. In this paper, we\npropose a single end-to-end deep fully-convolutional semantic segmentation\nnetwork dubbed SegET with rich contextual features which fully exploitsthe\nmulti-scale and multi-level contextual information and reduces the loss of\ndetails of cellular structures in ET images. We trained and evaluated our\nnetwork on the electron tomogram of the CTL Immunological Synapse from Cell\nImage library. Our results demonstrate that SegET can automatically segment\naccurately and outperform all other baseline methods on each individual\nstructure in our ET dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:30:37 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zhang", "Enze", ""], ["Zhang", "Fa", ""], ["Liu", "Zhiyong", ""], ["Wan", "Xiaohua", ""], ["Zhu", "Lifa", ""]]}, {"id": "1811.11731", "submitter": "Navaneet K L", "authors": "Navaneet K L, Priyanka Mandikal, Mayank Agarwal and R. Venkatesh Babu", "title": "CAPNet: Continuous Approximation Projection For 3D Point Cloud\n  Reconstruction Using 2D Supervision", "comments": "Accepted at AAAI-2019; Codes are available at\n  https://github.com/val-iisc/capnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of 3D properties of objects is a necessity in order to build\neffective computer vision systems. However, lack of large scale 3D datasets can\nbe a major constraint for data-driven approaches in learning such properties.\nWe consider the task of single image 3D point cloud reconstruction, and aim to\nutilize multiple foreground masks as our supervisory data to alleviate the need\nfor large scale 3D datasets. A novel differentiable projection module, called\n'CAPNet', is introduced to obtain such 2D masks from a predicted 3D point\ncloud. The key idea is to model the projections as a continuous approximation\nof the points in the point cloud. To overcome the challenges of sparse\nprojection maps, we propose a loss formulation termed 'affinity loss' to\ngenerate outlier-free reconstructions. We significantly outperform the existing\nprojection based approaches on a large-scale synthetic dataset. We show the\nutility and generalizability of such a 2D supervised approach through\nexperiments on a real-world dataset, where lack of 3D data can be a serious\nconcern. To further enhance the reconstructions, we also propose a test stage\noptimization procedure to obtain reconstructions that display high\ncorrespondence with the observed input image.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:31:46 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["L", "Navaneet K", ""], ["Mandikal", "Priyanka", ""], ["Agarwal", "Mayank", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1811.11742", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli", "title": "3D human pose estimation in video with temporal convolutions and\n  semi-supervised training", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we demonstrate that 3D poses in video can be effectively\nestimated with a fully convolutional model based on dilated temporal\nconvolutions over 2D keypoints. We also introduce back-projection, a simple and\neffective semi-supervised training method that leverages unlabeled video data.\nWe start with predicted 2D keypoints for unlabeled video, then estimate 3D\nposes and finally back-project to the input 2D keypoints. In the supervised\nsetting, our fully-convolutional model outperforms the previous best result\nfrom the literature by 6 mm mean per-joint position error on Human3.6M,\ncorresponding to an error reduction of 11%, and the model also shows\nsignificant improvements on HumanEva-I. Moreover, experiments with\nback-projection show that it comfortably outperforms previous state-of-the-art\nresults in semi-supervised settings where labeled data is scarce. Code and\nmodels are available at https://github.com/facebookresearch/VideoPose3D\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:56:36 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 13:36:46 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Pavllo", "Dario", ""], ["Feichtenhofer", "Christoph", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1811.11745", "submitter": "Tim Brooks", "authors": "Tim Brooks, Jonathan T. Barron", "title": "Learning to Synthesize Motion Blur", "comments": "http://timothybrooks.com/tech/motion-blur/ . IEEE Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a technique for synthesizing a motion blurred image from a pair of\nunblurred images captured in succession. To build this system we motivate and\ndesign a differentiable \"line prediction\" layer to be used as part of a neural\nnetwork architecture, with which we can learn a system to regress from image\npairs to motion blurred images that span the capture time of the input image\npair. Training this model requires an abundance of data, and so we design and\nexecute a strategy for using frame interpolation techniques to generate a\nlarge-scale synthetic dataset of motion blurred images and their respective\ninputs. We additionally capture a high quality test set of real motion blurred\nimages, synthesized from slow motion videos, with which we evaluate our model\nagainst several baseline techniques that can be used to synthesize motion blur.\nOur model produces higher accuracy output than our baselines, and is\nsignificantly faster than baselines with competitive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:55:55 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 07:28:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Brooks", "Tim", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1811.11788", "submitter": "Steven McDonagh", "authors": "Steven McDonagh, Sarah Parisot, Fengwei Zhou, Xing Zhang, Ales\n  Leonardis, Zhenguo Li, Gregory Slabaugh", "title": "Formulating Camera-Adaptive Color Constancy as a Few-shot Meta-Learning\n  Problem", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital camera pipelines employ color constancy methods to estimate an\nunknown scene illuminant, in order to re-illuminate images as if they were\nacquired under an achromatic light source. Fully-supervised learning approaches\nexhibit state-of-the-art estimation accuracy with camera-specific labelled\ntraining imagery. Resulting models typically suffer from domain gaps and fail\nto generalise across imaging devices. In this work, we propose a new approach\nthat affords fast adaptation to previously unseen cameras, and robustness to\nchanges in capture device by leveraging annotated samples across different\ncameras and datasets. We present a general approach that utilizes the concept\nof color temperature to frame color constancy as a set of distinct, homogeneous\nfew-shot regression tasks, each associated with an intuitive physical meaning.\nWe integrate this novel formulation within a meta-learning framework, enabling\nfast generalisation to previously unseen cameras using only handfuls of camera\nspecific training samples. Consequently, the time spent for data collection and\nannotation substantially diminishes in practice whenever a new sensor is used.\nTo quantify this gain, we evaluate our pipeline on three publicly available\ndatasets comprising 12 different cameras and diverse scene content. Our\napproach delivers competitive results both qualitatively and quantitatively\nwhile requiring a small fraction of the camera-specific samples compared to\nstandard approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 19:16:41 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 19:08:48 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["McDonagh", "Steven", ""], ["Parisot", "Sarah", ""], ["Zhou", "Fengwei", ""], ["Zhang", "Xing", ""], ["Leonardis", "Ales", ""], ["Li", "Zhenguo", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "1811.11796", "submitter": "Mohammad Imrul Jubair", "authors": "K M Arefeen Sultan, Labiba Kanij Rupty, Nahidul Islam Pranto, Sayed\n  Khan Shuvo, Mohammad Imrul Jubair", "title": "Cartoon-to-real: An Approach to Translate Cartoon to Realistic Images\n  using GAN", "comments": "This is an ongoing work and this draft contains the future plan to\n  accomplish the tasks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to translate cartoon images to real world images using\nGenerative Aderserial Network (GAN). Existing GAN-based image-to-image\ntranslation methods which are trained on paired datasets are impractical as the\ndata is difficult to accumulate. Therefore, in this paper we exploit the\nCycle-Consistent Adversarial Networks (CycleGAN) method for images translation\nwhich needs an unpaired dataset. By applying CycleGAN we show that our model is\nable to generate meaningful real world images from cartoon images. However, we\nimplement another state of the art technique $-$ Deep Analogy $-$ to compare\nthe performance of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 19:40:32 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 13:16:39 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 17:03:09 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Sultan", "K M Arefeen", ""], ["Rupty", "Labiba Kanij", ""], ["Pranto", "Nahidul Islam", ""], ["Shuvo", "Sayed Khan", ""], ["Jubair", "Mohammad Imrul", ""]]}, {"id": "1811.11814", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng, Lingxi Xie, Tianwei Ni, Ya Zhang, Yan-Feng Wang, Qi\n  Tian, Elliot K. Fishman, Alan L. Yuille", "title": "Phase Collaborative Network for Two-Phase Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world practice, medical images acquired in different phases possess\ncomplementary information, {\\em e.g.}, radiologists often refer to both\narterial and venous scans in order to make the diagnosis. However, in medical\nimage analysis, fusing prediction from two phases is often difficult, because\n(i) there is a domain gap between two phases, and (ii) the semantic labels are\nnot pixel-wise corresponded even for images scanned from the same patient. This\npaper studies organ segmentation in two-phase CT scans. We propose Phase\nCollaborative Network (PCN), an end-to-end framework that contains both\ngenerative and discriminative modules. PCN can be mathematically explained to\nformulate phase-to-phase and data-to-label relations jointly. Experiments are\nperformed on a two-phase CT dataset, on which PCN outperforms the baselines\nworking with one-phase data by a large margin, and we empirically verify that\nthe gain comes from inter-phase collaboration. Besides, PCN transfers well to\ntwo public single-phase datasets, demonstrating its potential applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:26:36 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 06:53:50 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 15:12:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zheng", "Huangjie", ""], ["Xie", "Lingxi", ""], ["Ni", "Tianwei", ""], ["Zhang", "Ya", ""], ["Wang", "Yan-Feng", ""], ["Tian", "Qi", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1811.11815", "submitter": "Georgios Mastorakis", "authors": "Georgios Mastorakis", "title": "Unrepresentative video data: A review and evaluation", "comments": "35 pages, 14 figures. Submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the quality and quantity of training data are\nsignificant factors which affect the development and performance of machine\nintelligence algorithms. Without representative data, neither scientists nor\nalgorithms would be able to accurately capture the visual details of objects,\nactions or scenes. An evaluation methodology which filters data quality does\nnot yet exist, and currently, the validation of the data depends solely on\nhuman factor. This study reviews several public datasets and discusses their\nlimitations and issues regarding quality, feasibility, adaptation and\navailability of training data. A simple approach to evaluate (i.e.\nautomatically \"clean\" samples) training data is proposed with the use of real\nevents recorded on the YouTube platform. This study focuses on action\nrecognition data and particularly on human fall detection datasets. However,\nthe limitations described in this paper apply in virtually all datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:28:11 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 21:14:00 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Mastorakis", "Georgios", ""]]}, {"id": "1811.11816", "submitter": "Tsang Ing Ren", "authors": "Hector N. B. Pinheiro, Tsang Ing Ren, Stefan Scheib, Armel Rosselet,\n  Stefan Thieme-Marti", "title": "2D/3D Megavoltage Image Registration Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We presented a 2D/3D MV image registration method based on a Convolutional\nNeural Network. Most of the traditional image registration method\nintensity-based, which use optimization algorithms to maximize the similarity\nbetween to images. Although these methods can achieve good results for\nkilovoltage images, the same does not occur for megavoltage images due to the\nlower image quality. Also, these methods most of the times do not present a\ngood capture range. To deal with this problem, we propose the use of\nConvolutional Neural Network. The experiments were performed using a dataset of\n50 brain images. The results showed to be promising compared to traditional\nimage registration methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:30:27 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Pinheiro", "Hector N. B.", ""], ["Ren", "Tsang Ing", ""], ["Scheib", "Stefan", ""], ["Rosselet", "Armel", ""], ["Thieme-Marti", "Stefan", ""]]}, {"id": "1811.11819", "submitter": "Siavash Khodadadeh", "authors": "Siavash Khodadadeh, Ladislau B\\\"ol\\\"oni and Mubarak Shah", "title": "Unsupervised Meta-Learning For Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot or one-shot learning of classifiers requires a significant inductive\nbias towards the type of task to be learned. One way to acquire this is by\nmeta-learning on tasks similar to the target task. In this paper, we propose\nUMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning\nfor classification tasks. The meta-learning step of UMTRA is performed on a\nflat collection of unlabeled images. While we assume that these images can be\ngrouped into a diverse set of classes and are relevant to the target task, no\nexplicit information about the classes or any labels are needed. UMTRA uses\nrandom sampling and augmentation to create synthetic training tasks for\nmeta-learning phase. Labels are only needed at the final target task learning\nstep, and they can be as little as one sample per class. On the Omniglot and\nMini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested\napproach based on unsupervised learning of representations, while alternating\nfor the best performance with the recent CACTUs algorithm. Compared to\nsupervised model-agnostic meta-learning approaches, UMTRA trades off some\nclassification accuracy for a reduction in the required labels of several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:38:59 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 00:01:57 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Khodadadeh", "Siavash", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""], ["Shah", "Mubarak", ""]]}, {"id": "1811.11823", "submitter": "Yutong Bai", "authors": "Yutong Bai, Qing Liu, Lingxi Xie, Weichao Qiu, Yan Zheng, Alan Yuille", "title": "Semantic Part Detection via Matching: Learning to Generalize to Novel\n  Viewpoints from Limited Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting semantic parts of an object is a challenging task in computer\nvision, particularly because it is hard to construct large annotated datasets\ndue to the difficulty of annotating semantic parts. In this paper we present an\napproach which learns from a small training dataset of annotated semantic\nparts, where the object is seen from a limited range of viewpoints, but\ngeneralizes to detect semantic parts from a much larger range of viewpoints.\nOur approach is based on a matching algorithm for finding accurate spatial\ncorrespondence between two images, which enables semantic parts annotated on\none image to be transplanted to another. In particular, this enables images in\nthe training dataset to be matched to a virtual 3D model of the object (for\nsimplicity, we assume that the object viewpoint can be estimated by standard\ntechniques). Then a clustering algorithm is used to annotate the semantic parts\nof the 3D virtual model. This virtual 3D model can be used to synthesize\nannotated images from a large range of viewpoint. These can be matched to\nimages in the test set, using the same matching algorithm, to detect semantic\nparts in novel viewpoints of the object. Our algorithm is very simple,\nintuitive, and contains very few parameters. We evaluate our approach in the\ncar subclass of the VehicleSemanticPart dataset. We show it outperforms\nstandard deep network approaches and, in particular, performs much better on\nnovel viewpoints. For facilitating the future research, code is available:\nhttps://github.com/ytongbai/SemanticPartDetection\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:48:18 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 02:00:47 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 17:10:21 GMT"}, {"version": "v4", "created": "Fri, 13 Sep 2019 03:54:50 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Bai", "Yutong", ""], ["Liu", "Qing", ""], ["Xie", "Lingxi", ""], ["Qiu", "Weichao", ""], ["Zheng", "Yan", ""], ["Yuille", "Alan", ""]]}, {"id": "1811.11833", "submitter": "Sebastin Santy", "authors": "Sebastin Santy, Wazeer Zulfikar, Rishabh Mehrotra, Emine Yilmaz", "title": "Towards Task Understanding in Visual Settings", "comments": "Accepted as Student Abstract at 33rd AAAI Conference on Artificial\n  Intelligence, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of understanding real world tasks depicted in visual\nimages. While most existing image captioning methods excel in producing natural\nlanguage descriptions of visual scenes involving human tasks, there is often\nthe need for an understanding of the exact task being undertaken rather than a\nliteral description of the scene. We leverage insights from real world task\nunderstanding systems, and propose a framework composed of convolutional neural\nnetworks, and an external hierarchical task ontology to produce task\ndescriptions from input images. Detailed experiments highlight the efficacy of\nthe extracted descriptions, which could potentially find their way in many\napplications, including image alt text generation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:06:27 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Santy", "Sebastin", ""], ["Zulfikar", "Wazeer", ""], ["Mehrotra", "Rishabh", ""], ["Yilmaz", "Emine", ""]]}, {"id": "1811.11843", "submitter": "Guoxin Fan", "authors": "Guoxin Fan, Huaqing Liu, Zhenhua Wu, Yufeng Li, Chaobo Feng, Dongdong\n  Wang, Jie Luo, Xiaofei Guan, William M. Wells III, Shisheng He", "title": "Deep learning based automatic segmentation of lumbosacral nerves on\n  non-contrast CT for radiographic evaluation: a pilot study", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and objective: Combined evaluation of lumbosacral structures (e.g.\nnerves, bone) on multimodal radiographic images is routinely conducted prior to\nspinal surgery and interventional procedures. Generally, magnetic resonance\nimaging is conducted to differentiate nerves, while computed tomography (CT) is\nused to observe bony structures. The aim of this study is to investigate the\nfeasibility of automatically segmenting lumbosacral structures (e.g. nerves &\nbone) on non-contrast CT with deep learning. Methods: a total of 50 cases with\nspinal CT were manually labeled for lumbosacral nerves and bone with Slicer\n4.8. The ratio of training: validation: testing is 32:8:10. A 3D-Unet is\nadopted to build the model SPINECT for automatically segmenting lumbosacral\nstructures. Pixel accuracy, IoU, and Dice score are used to assess the\nsegmentation performance of lumbosacral structures. Results: the testing\nresults reveals successful segmentation of lumbosacral bone and nerve on CT.\nThe average pixel accuracy is 0.940 for bone and 0.918 for nerve. The average\nIoU is 0.897 for bone and 0.827 for nerve. The dice score is 0.945 for bone and\n0.905 for nerve. Conclusions: this pilot study indicated that automatic\nsegmenting lumbosacral structures (nerves and bone) on non-contrast CT is\nfeasible and may have utility for planning and navigating spinal interventions\nand surgery.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:24:56 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Fan", "Guoxin", ""], ["Liu", "Huaqing", ""], ["Wu", "Zhenhua", ""], ["Li", "Yufeng", ""], ["Feng", "Chaobo", ""], ["Wang", "Dongdong", ""], ["Luo", "Jie", ""], ["Guan", "Xiaofei", ""], ["Wells", "William M.", "III"], ["He", "Shisheng", ""]]}, {"id": "1811.11849", "submitter": "Kha Gia Quach", "authors": "Kha Gia Quach, Ngan Le, Chi Nhan Duong, Ibsa Jalata, Kaushik Roy, Khoa\n  Luu", "title": "Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on\n  Crowd Videos", "comments": "Under review at Patter Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-level emotion recognition (ER) is a growing research area as the\ndemands for assessing crowds of all sizes are becoming an interest in both the\nsecurity arena as well as social media. This work extends the earlier ER\ninvestigations, which focused on either group-level ER on single images or\nwithin a video, by fully investigating group-level expression recognition on\ncrowd videos. In this paper, we propose an effective deep feature level fusion\nmechanism to model the spatial-temporal information in the crowd videos. In our\napproach, the fusing process is performed on the deep feature domain by a\ngenerative probabilistic model, Non-Volume Preserving Fusion (NVPF), that\nmodels spatial information relationships. Furthermore, we extend our proposed\nspatial NVPF approach to the spatial-temporal NVPF approach to learn the\ntemporal information between frames. To demonstrate the robustness and\neffectiveness of each component in the proposed approach, three experiments\nwere conducted: (i) evaluation on AffectNet database to benchmark the proposed\nEmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to\nbenchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)\nexamine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos\n(GECV) dataset composed of 627 videos collected from publicly available\nsources. GECV dataset is a collection of videos containing crowds of people.\nEach video is labeled with emotion categories at three levels: individual\nfaces, group of people, and the entire video frame.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:35:23 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 03:02:03 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 06:14:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Quach", "Kha Gia", ""], ["Le", "Ngan", ""], ["Duong", "Chi Nhan", ""], ["Jalata", "Ibsa", ""], ["Roy", "Kaushik", ""], ["Luu", "Khoa", ""]]}, {"id": "1811.11852", "submitter": "Jaewon Yang", "authors": "Jaewon Yang, Dookun Park, Jae Ho Sohn, Zhen Jane Wang, Grant T.\n  Gullberg, and Youngho Seo", "title": "Joint Correction of Attenuation and Scatter Using Deep Convolutional\n  Neural Networks (DCNN) for Time-of-Flight PET", "comments": "4 pages, 7 figures, IEEE MIC 2018 conference", "journal-ref": null, "doi": "10.1088/1361-6560/ab0606", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) have demonstrated its capability to\nconvert MR image to pseudo CT for PET attenuation correction in PET/MRI.\nConventionally, attenuated events are corrected in sinogram space using\nattenuation maps derived from CT or MR-derived pseudo CT. Separately, scattered\nevents are iteratively estimated by a 3D model-based simulation using\ndown-sampled attenuation and emission sinograms. However, no studies have\ninvestigated joint correction of attenuation and scatter using DCNN in image\nspace. Therefore, we aim to develop and optimize a DCNN model for attenuation\nand scatter correction (ASC) simultaneously in PET image space without\nadditional anatomical imaging or time-consuming iterative scatter simulation.\nFor the first time, we demonstrated the feasibility of directly producing PET\nimages corrected for attenuation and scatter using DCNN (PET-DCNN) from\nnoncorrected PET (PET-NC) images.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:44:59 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Yang", "Jaewon", ""], ["Park", "Dookun", ""], ["Sohn", "Jae Ho", ""], ["Wang", "Zhen Jane", ""], ["Gullberg", "Grant T.", ""], ["Seo", "Youngho", ""]]}, {"id": "1811.11872", "submitter": "Giovanni Poggi", "authors": "Sergio Vitale, Davide Cozzolino, Giuseppe Scarpa, Luisa Verdoliva,\n  Giovanni Poggi", "title": "Guided patch-wise nonlocal SAR despeckling", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2906412", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for SAR image despeckling which leverages information\ndrawn from co-registered optical imagery. Filtering is performed by plain\npatch-wise nonlocal means, operating exclusively on SAR data. However, the\nfiltering weights are computed by taking into account also the optical guide,\nwhich is much cleaner than the SAR data, and hence more discriminative. To\navoid injecting optical-domain information into the filtered image, a\nSAR-domain statistical test is preliminarily performed to reject right away any\nrisky predictor. Experiments on two SAR-optical datasets prove the proposed\nmethod to suppress very effectively the speckle, preserving structural details,\nand without introducing visible filtering artifacts. Overall, the proposed\nmethod compares favourably with all state-of-the-art despeckling filters, and\nalso with our own previous optical-guided filter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:05:35 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Vitale", "Sergio", ""], ["Cozzolino", "Davide", ""], ["Scarpa", "Giuseppe", ""], ["Verdoliva", "Luisa", ""], ["Poggi", "Giovanni", ""]]}, {"id": "1811.11874", "submitter": "Chen Gong", "authors": "Chen Gong, N. Benjamin Erichson, John P. Kelly, Laura Trutoiu, Brian\n  T. Schowengerdt, Steven L. Brunton, Eric J. Seibel", "title": "RetinaMatch: Efficient Template Matching of Retina Images for\n  Teleophthalmology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal template matching and registration is an important challenge in\nteleophthalmology with low-cost imaging devices. However, the images from such\ndevices generally have a small field of view (FOV) and image quality\ndegradations, making matching difficult. In this work, we develop an efficient\nand accurate retinal matching technique that combines dimension reduction and\nmutual information (MI), called RetinaMatch. The dimension reduction\ninitializes the MI optimization as a coarse localization process, which narrows\nthe optimization domain and avoids local optima. The effectiveness of\nRetinaMatch is demonstrated on the open fundus image database STARE with\nsimulated reduced FOV and anticipated degradations, and on retinal images\nacquired by adapter-based optics attached to a smartphone. RetinaMatch achieves\na success rate over 94\\% on human retinal images with the matched target\nregistration errors below 2 pixels on average, excluding the observer\nvariability. It outperforms the standard template matching solutions. In the\napplication of measuring vessel diameter repeatedly, single pixel errors are\nexpected. In addition, our method can be used in the process of image\nmosaicking with area-based registration, providing a robust approach when the\nfeature based methods fail. To the best of our knowledge, this is the first\ntemplate matching algorithm for retina images with small template images from\nunconstrained retinal areas. In the context of the emerging mixed reality\nmarket, we envision automated retinal image matching and registration methods\nas transformative for advanced teleophthalmology and long-term retinal\nmonitoring.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:06:54 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Gong", "Chen", ""], ["Erichson", "N. Benjamin", ""], ["Kelly", "John P.", ""], ["Trutoiu", "Laura", ""], ["Schowengerdt", "Brian T.", ""], ["Brunton", "Steven L.", ""], ["Seibel", "Eric J.", ""]]}, {"id": "1811.11875", "submitter": "Nathan Inkawhich", "authors": "Nathan Inkawhich, Matthew Inkawhich, Yiran Chen, Hai Li", "title": "Adversarial Attacks for Optical Flow-Based Action Recognition\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning research has catapulted deep models into\nproduction systems that our society is becoming increasingly dependent on,\nespecially in the image and video domains. However, recent work has shown that\nthese largely uninterpretable models exhibit glaring security vulnerabilities\nin the presence of an adversary. In this work, we develop a powerful untargeted\nadversarial attack for action recognition systems in both white-box and\nblack-box settings. Action recognition models differ from image-classification\nmodels in that their inputs contain a temporal dimension, which we explicitly\ntarget in the attack. Drawing inspiration from image classifier attacks, we\ncreate new attacks which achieve state-of-the-art success rates on a two-stream\nclassifier trained on the UCF-101 dataset. We find that our attacks can\nsignificantly degrade a model's performance with sparsely and imperceptibly\nperturbed examples. We also demonstrate the transferability of our attacks to\nblack-box action recognition systems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:10:47 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Inkawhich", "Nathan", ""], ["Inkawhich", "Matthew", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1811.11896", "submitter": "Pai Liu", "authors": "Pai Liu, Jingwei Gan, and Rajan K. Chakrabarty", "title": "Variational Autoencoding the Lagrangian Trajectories of Particles in a\n  Combustion System", "comments": "2nd version: typo corrected, corresponding author changed 19 pages, 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning method to simulate the motion of particles\ntrapped in a chaotic recirculating flame. The Lagrangian trajectories of\nparticles, captured using a high-speed camera and subsequently reconstructed in\n3-dimensional space, were used to train a variational autoencoder (VAE) which\ncomprises multiple layers of convolutional neural networks. We show that the\ntrajectories, which are statistically representative of those determined in\nexperiments, can be generated using the VAE network. The performance of our\nmodel is evaluated with respect to the accuracy and generalization of the\noutputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 00:44:58 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 03:18:25 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Liu", "Pai", ""], ["Gan", "Jingwei", ""], ["Chakrabarty", "Rajan K.", ""]]}, {"id": "1811.11903", "submitter": "Chunhua Shen", "authors": "Hui Li, Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "Visual Question Answering as Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) demands simultaneous comprehension of both\nthe image visual content and natural language questions. In some cases, the\nreasoning needs the help of common sense or general knowledge which usually\nappear in the form of text. Current methods jointly embed both the visual\ninformation and the textual feature into the same space. However, how to model\nthe complex interactions between the two different modalities is not an easy\ntask. In contrast to struggling on multimodal feature fusion, in this paper, we\npropose to unify all the input information by natural language so as to convert\nVQA into a machine reading comprehension problem. With this transformation, our\nmethod not only can tackle VQA datasets that focus on observation based\nquestions, but can also be naturally extended to handle knowledge-based VQA\nwhich requires to explore large-scale external knowledge base. It is a step\ntowards being able to exploit large volumes of text and natural language\nprocessing techniques to address VQA problem. Two types of models are proposed\nto deal with open-ended VQA and multiple-choice VQA respectively. We evaluate\nour models on three VQA benchmarks. The comparable performance with the\nstate-of-the-art demonstrates the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 01:11:16 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Hui", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1811.11921", "submitter": "Kejie Li", "authors": "Kejie Li, Ravi Garg, Ming Cai, Ian Reid", "title": "Single-view Object Shape Reconstruction Using Deep Shape Prior and\n  Silhouette", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape reconstruction from a single image is a highly ill-posed problem.\nModern deep learning based systems try to solve this problem by learning an\nend-to-end mapping from image to shape via a deep network. In this paper, we\naim to solve this problem via an online optimization framework inspired by\ntraditional methods. Our framework employs a deep autoencoder to learn a set of\nlatent codes of 3D object shapes, which are fitted by a probabilistic shape\nprior using Gaussian Mixture Model (GMM). At inference, the shape and pose are\njointly optimized guided by both image cues and deep shape prior without\nrelying on an initialization from any trained deep nets. Surprisingly, our\nmethod achieves comparable performance to state-of-the-art methods even without\ntraining an end-to-end network, which shows a promising step in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:03:52 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 00:44:27 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Li", "Kejie", ""], ["Garg", "Ravi", ""], ["Cai", "Ming", ""], ["Reid", "Ian", ""]]}, {"id": "1811.11938", "submitter": "Chiranjoy Chattopadhyay", "authors": "Mahak Jain, Anurag Sanyal, Shreya Goyal, Chiranjoy Chattopadhyay,\n  Gaurav Bhatnagar", "title": "Automatic Rendering of Building Floor Plan Images from Textual\n  Descriptions in English", "comments": "8 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings understand natural language description and could able to\nimagine a corresponding visual for the same. For example, given a description\nof the interior of a house, we could imagine its structure and arrangements of\nfurniture. Automatic synthesis of real-world images from text descriptions has\nbeen explored in the computer vision community. However, there is no such\nattempt in the area of document images, like floor plans. Floor plan synthesis\nfrom sketches, as well as data-driven models, were proposed earlier. Ours is\nthe first attempt to render building floor plan images from textual description\nautomatically. Here, the input is a natural language description of the\ninternal structure and furniture arrangements within a house, and the output is\nthe 2D floor plan image of the same. We have experimented on publicly available\nbenchmark floor plan datasets. We were able to render realistic synthesized\nfloor plan images from the description written in English.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:55:25 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jain", "Mahak", ""], ["Sanyal", "Anurag", ""], ["Goyal", "Shreya", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Bhatnagar", "Gaurav", ""]]}, {"id": "1811.11946", "submitter": "Pranav Ganti", "authors": "Pranav Ganti, Steven L. Waslander", "title": "Network Uncertainty Informed Semantic Feature Selection for Visual SLAM", "comments": "Published in: 2019 16th Conference on Computer and Robot Vision (CRV)", "journal-ref": null, "doi": "10.1109/CRV.2019.00024", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to facilitate long-term localization using a visual simultaneous\nlocalization and mapping (SLAM) algorithm, careful feature selection can help\nensure that reference points persist over long durations and the runtime and\nstorage complexity of the algorithm remain consistent. We present SIVO\n(Semantically Informed Visual Odometry and Mapping), a novel\ninformation-theoretic feature selection method for visual SLAM which\nincorporates semantic segmentation and neural network uncertainty into the\nfeature selection pipeline. Our algorithm selects points which provide the\nhighest reduction in Shannon entropy between the entropy of the current state\nand the joint entropy of the state, given the addition of the new feature with\nthe classification entropy of the feature from a Bayesian neural network. Each\nselected feature significantly reduces the uncertainty of the vehicle state and\nhas been detected to be a static object (building, traffic sign, etc.)\nrepeatedly with a high confidence. This selection strategy generates a sparse\nmap which can facilitate long-term localization. The KITTI odometry dataset is\nused to evaluate our method, and we also compare our results against ORB_SLAM2.\nOverall, SIVO performs comparably to the baseline method while reducing the map\nsize by almost 70%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:53:17 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 15:00:41 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ganti", "Pranav", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1811.11968", "submitter": "Yongchao Long", "authors": "Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, Hefeng Wu", "title": "ADCrowdNet: An Attention-injective Deformable Convolutional Network for\n  Crowd Understanding", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention-injective deformable convolutional network called\nADCrowdNet for crowd understanding that can address the accuracy degradation\nproblem of highly congested noisy scenes. ADCrowdNet contains two concatenated\nnetworks. An attention-aware network called Attention Map Generator (AMG) first\ndetects crowd regions in images and computes the congestion degree of these\nregions. Based on detected crowd regions and congestion priors, a multi-scale\ndeformable network called Density Map Estimator (DME) then generates\nhigh-quality density maps. With the attention-aware training scheme and\nmulti-scale deformable convolutional scheme, the proposed ADCrowdNet achieves\nthe capability of being more effective to capture the crowd features and more\nresistant to various noises. We have evaluated our method on four popular crowd\ncounting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an\nextra vehicle counting dataset TRANCOS, and our approach beats existing\nstate-of-the-art approaches on all of these datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:10:03 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 14:53:44 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 03:27:40 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 07:18:29 GMT"}, {"version": "v5", "created": "Thu, 11 Apr 2019 06:27:08 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liu", "Ning", ""], ["Long", "Yongchao", ""], ["Zou", "Changqing", ""], ["Niu", "Qun", ""], ["Pan", "Li", ""], ["Wu", "Hefeng", ""]]}, {"id": "1811.11969", "submitter": "Lijun Yu", "authors": "Lijun Yu, Dawei Zhang, Xiangqun Chen, Alexander Hauptmann", "title": "Traffic Danger Recognition With Surveillance Cameras Without Training\n  Data", "comments": "To be published in proceedings of Advanced Video and Signal-based\n  Surveillance (AVSS), 2018 15th IEEE International Conference on, pp. 378-383,\n  IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a traffic danger recognition model that works with arbitrary\ntraffic surveillance cameras to identify and predict car crashes. There are too\nmany cameras to monitor manually. Therefore, we developed a model to predict\nand identify car crashes from surveillance cameras based on a 3D reconstruction\nof the road plane and prediction of trajectories. For normal traffic, it\nsupports real-time proactive safety checks of speeds and distances between\nvehicles to provide insights about possible high-risk areas. We achieve good\nprediction and recognition of car crashes without using any labeled training\ndata of crashes. Experiments on the BrnoCompSpeed dataset show that our model\ncan accurately monitor the road, with mean errors of 1.80% for distance\nmeasurement, 2.77 km/h for speed measurement, 0.24 m for car position\nprediction, and 2.53 km/h for speed prediction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:16:40 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yu", "Lijun", ""], ["Zhang", "Dawei", ""], ["Chen", "Xiangqun", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1811.11971", "submitter": "Shujian Yu", "authors": "Shujian Yu and Jose C. Principe", "title": "Simple stopping criteria for information theoretic feature selection", "comments": "Paper published in the journal of Entropy", "journal-ref": "Entropy 2019, 21(1), 99", "doi": "10.3390/e21010099", "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection aims to select the smallest feature subset that yields the\nminimum generalization error. In the rich literature in feature selection,\ninformation theory-based approaches seek a subset of features such that the\nmutual information between the selected features and the class labels is\nmaximized. Despite the simplicity of this objective, there still remain several\nopen problems in optimization. These include, for example, the automatic\ndetermination of the optimal subset size (i.e., the number of features) or a\nstopping criterion if the greedy searching strategy is adopted. In this paper,\nwe suggest two stopping criteria by just monitoring the conditional mutual\ninformation (CMI) among groups of variables. Using the recently developed\nmultivariate matrix-based Renyi's \\alpha-entropy functional, which can be\ndirectly estimated from data samples, we showed that the CMI among groups of\nvariables can be easily computed without any decomposition or approximation,\nhence making our criteria easy to implement and seamlessly integrated into any\nexisting information theoretic feature selection methods with a greedy search\nstrategy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:24:28 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 10:38:02 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Yu", "Shujian", ""], ["Principe", "Jose C.", ""]]}, {"id": "1811.11975", "submitter": "Yaadhav Raaj", "authors": "Yaadhav Raaj and Haroon Idrees and Gines Hidalgo and Yaser Sheikh", "title": "Efficient Online Multi-Person 2D Pose Tracking with Recurrent\n  Spatio-Temporal Affinity Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online approach to efficiently and simultaneously detect and\ntrack the 2D pose of multiple people in a video sequence. We build upon Part\nAffinity Field (PAF) representation designed for static images, and propose an\narchitecture that can encode and predict Spatio-Temporal Affinity Fields (STAF)\nacross a video sequence. In particular, we propose a novel temporal topology\ncross-linked across limbs which can consistently handle body motions of a wide\nrange of magnitudes. Additionally, we make the overall approach recurrent in\nnature, where the network ingests STAF heatmaps from previous frames and\nestimates those for the current frame. Our approach uses only online inference\nand tracking, and is currently the fastest and the most accurate bottom-up\napproach that is runtime invariant to the number of people in the scene and\naccuracy invariant to input frame rate of camera. Running at $\\sim$30 fps on a\nsingle GPU at single scale, it achieves highly competitive results on the\nPoseTrack benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:57:53 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 20:34:41 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 18:43:26 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Raaj", "Yaadhav", ""], ["Idrees", "Haroon", ""], ["Hidalgo", "Gines", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1811.11977", "submitter": "Shang-Ta Yang", "authors": "Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka, Min Sun,\n  Hung-Kuo Chu", "title": "DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a\n  Single RGB Panorama", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework, called DuLa-Net, to predict\nManhattan-world 3D room layouts from a single RGB panorama. To achieve better\nprediction accuracy, our method leverages two projections of the panorama at\nonce, namely the equirectangular panorama-view and the perspective\nceiling-view, that each contains different clues about the room layouts. Our\nnetwork architecture consists of two encoder-decoder branches for analyzing\neach of the two views. In addition, a novel feature fusion structure is\nproposed to connect the two branches, which are then jointly trained to predict\nthe 2D floor plans and layout heights. To learn more complex room layouts, we\nintroduce the Realtor360 dataset that contains panoramas of Manhattan-world\nroom layouts with different numbers of corners. Experimental results show that\nour work outperforms recent state-of-the-art in prediction accuracy and\nperformance, especially in the rooms with non-cuboid layouts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 06:06:52 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 15:37:59 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yang", "Shang-Ta", ""], ["Wang", "Fu-En", ""], ["Peng", "Chi-Han", ""], ["Wonka", "Peter", ""], ["Sun", "Min", ""], ["Chu", "Hung-Kuo", ""]]}, {"id": "1811.11979", "submitter": "Hadi Kazemi", "authors": "Hadi Kazemi, Sobhan Soleymani, Fariborz Taherkhani, Seyed Mehdi\n  Iranmanesh, Nasser M. Nasrabadi", "title": "Unsupervised Image-to-Image Translation Using Domain-Specific\n  Variational Information Bound", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is a class of computer vision\nproblems which aims at modeling conditional distribution of images in the\ntarget domain, given a set of unpaired images in the source and target domains.\nAn image in the source domain might have multiple representations in the target\ndomain. Therefore, ambiguity in modeling of the conditional distribution\narises, specially when the images in the source and target domains come from\ndifferent modalities. Current approaches mostly rely on simplifying assumptions\nto map both domains into a shared-latent space. Consequently, they are only\nable to model the domain-invariant information between the two modalities.\nThese approaches usually fail to model domain-specific information which has no\nrepresentation in the target domain. In this work, we propose an unsupervised\nimage-to-image translation framework which maximizes a domain-specific\nvariational information bound and learns the target domain-invariant\nrepresentation of the two domain. The proposed framework makes it possible to\nmap a single source image into multiple images in the target domain, utilizing\nseveral target domain-specific codes sampled randomly from the prior\ndistribution, or extracted from reference images.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 06:25:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kazemi", "Hadi", ""], ["Soleymani", "Sobhan", ""], ["Taherkhani", "Fariborz", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1811.11985", "submitter": "Ken Sakurada", "authors": "Ken Sakurada, Mikiya Shibuya, Weimin Wang", "title": "Weakly Supervised Silhouette-based Semantic Scene Change Detection", "comments": "Accepted at the 2020 IEEE International Conference on Robotics and\n  Automation (ICRA). Code and dataset are available at\n  https://github.com/xdspacelab/sscdnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel semantic scene change detection scheme with only\nweak supervision. A straightforward approach for this task is to train a\nsemantic change detection network directly from a large-scale dataset in an\nend-to-end manner. However, a specific dataset for this task, which is usually\nlabor-intensive and time-consuming, becomes indispensable. To avoid this\nproblem, we propose to train this kind of network from existing datasets by\ndividing this task into change detection and semantic extraction. On the other\nhand, the difference in camera viewpoints, for example, images of the same\nscene captured from a vehicle-mounted camera at different time points, usually\nbrings a challenge to the change detection task. To address this challenge, we\npropose a new siamese network structure with the introduction of correlation\nlayer. In addition, we create a publicly available dataset for semantic change\ndetection to evaluate the proposed method. The experimental results verified\nboth the robustness to viewpoint difference in change detection task and the\neffectiveness for semantic change detection of the proposed networks. Our code\nand dataset are available at https://github.com/xdspacelab/sscdnet.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 06:54:46 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 01:36:03 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sakurada", "Ken", ""], ["Shibuya", "Mikiya", ""], ["Wang", "Weimin", ""]]}, {"id": "1811.11987", "submitter": "Laurent Bou\\'e", "authors": "Laurent Bou\\'e", "title": "Deep learning for pedestrians: backpropagation in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this document is to provide a pedagogical introduction to the\nmain concepts underpinning the training of deep neural networks using gradient\ndescent; a process known as backpropagation. Although we focus on a very\ninfluential class of architectures called \"convolutional neural networks\"\n(CNNs) the approach is generic and useful to the machine learning community as\na whole. Motivated by the observation that derivations of backpropagation are\noften obscured by clumsy index-heavy narratives that appear somewhat\nmathemagical, we aim to offer a conceptually clear, vectorized description that\narticulates well the higher level logic. Following the principle of \"writing is\nnature's way of letting you know how sloppy your thinking is\", we try to make\nthe calculations meticulous, self-contained and yet as intuitive as possible.\nTaking nothing for granted, ample illustrations serve as visual guides and an\nextensive bibliography is provided for further explorations.\n  (For the sake of clarity, long mathematical derivations and visualizations\nhave been broken up into short \"summarized views\" and longer \"detailed views\"\nencoded into the PDF as optional content groups. Some figures contain\nanimations designed to illustrate important concepts in a more engaging style.\nFor these reasons, we advise to download the document locally and open it using\nAdobe Acrobat Reader. Other viewers were not tested and may not render the\ndetailed views, animations correctly.)\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:00:09 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bou\u00e9", "Laurent", ""]]}, {"id": "1811.11991", "submitter": "Yutaro Miyauchi", "authors": "Yutaro Miyauchi, Yusuke Sugano, Yasuyuki Matsushita", "title": "Shape-conditioned Image Generation by Learning Latent Appearance\n  Representation from Unpaired Data", "comments": "Accepted at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image generation is effective for diverse tasks including\ntraining data synthesis for learning-based computer vision. However, despite\nthe recent advances in generative adversarial networks (GANs), it is still a\nchallenging task to generate images with detailed conditioning on object\nshapes. Existing methods for conditional image generation use category labels\nand/or keypoints and are only give limited control over object categories. In\nthis work, we present SCGAN, an architecture to generate images with a desired\nshape specified by an input normal map. The shape-conditioned image generation\ntask is achieved by explicitly modeling the image appearance via a latent\nappearance vector. The network is trained using unpaired training samples of\nreal images and rendered normal maps. This approach enables us to generate\nimages of arbitrary object categories with the target shape and diverse image\nappearances. We show the effectiveness of our method through both qualitative\nand quantitative evaluation on training data generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:20:29 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Miyauchi", "Yutaro", ""], ["Sugano", "Yusuke", ""], ["Matsushita", "Yasuyuki", ""]]}, {"id": "1811.11996", "submitter": "Luna Zhang", "authors": "Luna M. Zhang", "title": "Effective, Fast, and Memory-Efficient Compressed Multi-function\n  Convolutional Neural Networks for More Accurate Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) usually use the same activation\nfunction, such as RELU, for all convolutional layers. There are performance\nlimitations of just using RELU. In order to achieve better classification\nperformance, reduce training and testing times, and reduce power consumption\nand memory usage, a new \"Compressed Multi-function CNN\" is developed. Google's\nInception-V4, for example, is a very deep CNN that consists of 4 Inception-A\nblocks, 7 Inception-B blocks, and 3 Inception-C blocks. RELU is used for all\nconvolutional layers. A new \"Compressed Multi-function Inception-V4\" (CMI) that\ncan use different activation functions is created with k Inception-A blocks, m\nInception-B blocks, and n Inception-C blocks where k in {1, 2, 3, 4}, m in {1,\n2, 3, 4, 5, 6, 7}, n in {1, 2, 3}, and (k+m+n)<14. For performance analysis, a\ndataset for classifying brain MRI images into one of the four stages of\nAlzheimer's disease is used to compare three CMI architectures with\nInception-V4 in terms of F1-score, training and testing times (related to power\nconsumption), and memory usage (model size). Overall, simulations show that the\nnew CMI models can outperform both the commonly used Inception-V4 and\nInception-V4 using different activation functions. In the future, other\n\"Compressed Multi-function CNNs\", such as \"Compressed Multi-function ResNets\nand DenseNets\" that have a reduced number of convolutional blocks using\ndifferent activation functions, will be developed to further increase\nclassification accuracy, reduce training and testing times, reduce\ncomputational power, and reduce memory usage (model size) for building more\neffective healthcare systems, such as implementing accurate and convenient\ndisease diagnosis systems on mobile devices that have limited battery power and\nmemory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:33:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Zhang", "Luna M.", ""]]}, {"id": "1811.11997", "submitter": "Aneek Barman Roy", "authors": "K. Manikandan, Ayush Patidar, Pallav Walia, Aneek Barman Roy", "title": "Hand Gesture Detection and Conversion to Speech and Text", "comments": "5 pages, 5 figures, International Conference on Innovations and\n  Discoveries in Science, Engineering and Technology(ICIDSET) 2018", "journal-ref": "International Journal of Pure and Applied Mathematics, Volume 120\n  No. 6 2018, 1347-1362, ISSN: 1314-3395 (on-line version)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hand gestures are one of the typical methods used in sign language. It is\nvery difficult for the hearing-impaired people to communicate with the world.\nThis project presents a solution that will not only automatically recognize the\nhand gestures but will also convert it into speech and text output so that\nimpaired person can easily communicate with normal people. A camera attached to\ncomputer will capture images of hand and the contour feature extraction is used\nto recognize the hand gestures of the person. Based on the recognized gestures,\nthe recorded soundtrack will be played.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:37:07 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Manikandan", "K.", ""], ["Patidar", "Ayush", ""], ["Walia", "Pallav", ""], ["Roy", "Aneek Barman", ""]]}, {"id": "1811.12004", "submitter": "Daniil Osokin", "authors": "Daniil Osokin", "title": "Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we adapt multi-person pose estimation architecture to use it on\nedge devices. We follow the bottom-up approach from OpenPose, the winner of\nCOCO 2016 Keypoints Challenge, because of its decent quality and robustness to\nnumber of people inside the frame. With proposed network design and optimized\npost-processing code the full solution runs at 28 frames per second (fps) on\nIntel$\\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU.\nThe network model has 4.1M parameters and 9 billions floating-point operations\n(GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with\nalmost the same quality. The code and model are available as a part of\nIntel$\\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:05:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Osokin", "Daniil", ""]]}, {"id": "1811.12006", "submitter": "Peihua Li", "authors": "Zilin Gao and Jiangtao Xie and Qilong Wang and Peihua Li", "title": "Global Second-order Pooling Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Networks (ConvNets) are fundamental to, besides\nlarge-scale visual recognition, a lot of vision tasks. As the primary goal of\nthe ConvNets is to characterize complex boundaries of thousands of classes in a\nhigh-dimensional space, it is critical to learn higher-order representations\nfor enhancing non-linear modeling capability. Recently, Global Second-order\nPooling (GSoP), plugged at the end of networks, has attracted increasing\nattentions, achieving much better performance than classical, first-order\nnetworks in a variety of vision tasks. However, how to effectively introduce\nhigher-order representation in earlier layers for improving non-linear\ncapability of ConvNets is still an open problem. In this paper, we propose a\nnovel network model introducing GSoP across from lower to higher layers for\nexploiting holistic image information throughout a network. Given an input 3D\ntensor outputted by some previous convolutional layer, we perform GSoP to\nobtain a covariance matrix which, after nonlinear transformation, is used for\ntensor scaling along channel dimension. Similarly, we can perform GSoP along\nspatial dimension for tensor scaling as well. In this way, we can make full use\nof the second-order statistics of the holistic image throughout a network. The\nproposed networks are thoroughly evaluated on large-scale ImageNet-1K, and\nexperiments have shown that they outperformed non-trivially the counterparts\nwhile achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:15:39 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:55:13 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Gao", "Zilin", ""], ["Xie", "Jiangtao", ""], ["Wang", "Qilong", ""], ["Li", "Peihua", ""]]}, {"id": "1811.12008", "submitter": "Stefan Milz", "authors": "Timo S\\\"amann, Karl Amende, Stefan Milz, Christian Witt, Martin Simon\n  and Johannes Petzold", "title": "Efficient Semantic Segmentation for Visual Bird's-eye View\n  Interpretation", "comments": null, "journal-ref": "Advances in Intelligent Systems and Computing 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform semantic segmentation in real-time capable\napplications with limited hardware is of great importance. One such application\nis the interpretation of the visual bird's-eye view, which requires the\nsemantic segmentation of the four omnidirectional camera images. In this paper,\nwe present an efficient semantic segmentation that sets new standards in terms\nof runtime and hardware requirements. Our two main contributions are the\ndecrease of the runtime by parallelizing the ArgMax layer and the reduction of\nhardware requirements by applying the channel pruning method to the ENet model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:21:18 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["S\u00e4mann", "Timo", ""], ["Amende", "Karl", ""], ["Milz", "Stefan", ""], ["Witt", "Christian", ""], ["Simon", "Martin", ""], ["Petzold", "Johannes", ""]]}, {"id": "1811.12013", "submitter": "Xiang Gao", "authors": "Xiang Gao, Wei Hu, Jiaxiang Tang, Jiaying Liu and Zongming Guo", "title": "Optimized Skeleton-based Action Recognition via Sparsified Graph\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of accessible depth sensors, dynamic human body skeletons\nhave attracted much attention as a robust modality for action recognition.\nPrevious methods model skeletons based on RNN or CNN, which has limited\nexpressive power for irregular skeleton joints. While graph convolutional\nnetworks (GCN) have been proposed to address irregular graph-structured data,\nthe fundamental graph construction remains challenging. In this paper, we\nrepresent skeletons naturally on graphs, and propose a graph regression based\nGCN (GR-GCN) for skeleton-based action recognition, aiming to capture the\nspatio-temporal variation in the data. As the graph representation is crucial\nto graph convolution, we first propose graph regression to statistically learn\nthe underlying graph from multiple observations. In particular, we provide\nspatio-temporal modeling of skeletons and pose an optimization problem on the\ngraph structure over consecutive frames, which enforces the sparsity of the\nunderlying graph for efficient representation. The optimized graph not only\nconnects each joint to its neighboring joints in the same frame strongly or\nweakly, but also links with relevant joints in the previous and subsequent\nframes. We then feed the optimized graph into the GCN along with the\ncoordinates of the skeleton sequence for feature learning, where we deploy\nhigh-order and fast Chebyshev approximation of spectral graph convolution.\nFurther, we provide analysis of the variation characterization by the Chebyshev\napproximation. Experimental results validate the effectiveness of the proposed\ngraph regression and show that the proposed GR-GCN achieves the\nstate-of-the-art performance on the widely used NTU RGB+D, UT-Kinect and SYSU\n3D datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:36:18 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 05:33:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Tang", "Jiaxiang", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""]]}, {"id": "1811.12016", "submitter": "Yao-Cheng Yang", "authors": "Yi-Lun Liao, Yao-Cheng Yang, Yu-Chiang Frank Wang", "title": "3D Shape Reconstruction from a Single 2D Image via 2D-3D\n  Self-Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at inferring 3D shapes from 2D images, 3D shape reconstruction has\ndrawn huge attention from researchers in computer vision and deep learning\ncommunities. However, it is not practical to assume that 2D input images and\ntheir associated ground truth 3D shapes are always available during training.\nIn this paper, we propose a framework for semi-supervised 3D reconstruction.\nThis is realized by our introduced 2D-3D self-consistency, which aligns the\npredicted 3D models and the projected 2D foreground segmentation masks.\nMoreover, our model not only enables recovering 3D shapes with the\ncorresponding 2D masks, camera pose information can be jointly disentangled and\npredicted, even such supervision is never available during training. In the\nexperiments, we qualitatively and quantitatively demonstrate the effectiveness\nof our model, which performs favorably against state-of-the-art approaches in\neither supervised or semi-supervised settings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:47:35 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Liao", "Yi-Lun", ""], ["Yang", "Yao-Cheng", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1811.12019", "submitter": "Kazuki Osawa", "authors": "Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota,\n  and Satoshi Matsuoka", "title": "Large-Scale Distributed Second-Order Optimization Using\n  Kronecker-Factored Approximate Curvature for Deep Convolutional Neural\n  Networks", "comments": "10 pages, 7 figures. Accepted at CVPR 2019, Long Beach, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training of deep neural networks suffer from the\ngeneralization gap caused by the increase in the effective mini-batch size.\nPrevious approaches try to solve this problem by varying the learning rate and\nbatch size over epochs and layers, or some ad hoc modification of the batch\nnormalization. We propose an alternative approach using a second-order\noptimization method that shows similar generalization capability to first-order\nmethods, but converges faster and can handle larger mini-batches. To test our\nmethod on a benchmark where highly optimized first-order methods are available\nas references, we train ResNet-50 on ImageNet. We converged to 75% Top-1\nvalidation accuracy in 35 epochs for mini-batch sizes under 16,384, and\nachieved 75% even with a mini-batch size of 131,072, which took only 978\niterations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:52:04 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 12:46:17 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 08:45:32 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 08:20:19 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 04:24:57 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Osawa", "Kazuki", ""], ["Tsuji", "Yohei", ""], ["Ueno", "Yuichiro", ""], ["Naruse", "Akira", ""], ["Yokota", "Rio", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1811.12026", "submitter": "Yingqi Wu", "authors": "Qing Song, Yingqi Wu and Lu Yang", "title": "Attacks on State-of-the-Art Face Recognition using Attentional\n  Adversarial Attack Generative Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the broad use of face recognition, its weakness gradually emerges that\nit is able to be attacked. So, it is important to study how face recognition\nnetworks are subject to attacks. In this paper, we focus on a novel way to do\nattacks against face recognition network that misleads the network to identify\nsomeone as the target person not misclassify inconspicuously. Simultaneously,\nfor this purpose, we introduce a specific attentional adversarial attack\ngenerative network to generate fake face images. For capturing the semantic\ninformation of the target person, this work adds a conditional variational\nautoencoder and attention modules to learn the instance-level correspondences\nbetween faces. Unlike traditional two-player GAN, this work introduces face\nrecognition networks as the third player to participate in the competition\nbetween generator and discriminator which allows the attacker to impersonate\nthe target person better. The generated faces which are hard to arouse the\nnotice of onlookers can evade recognition by state-of-the-art networks and most\nof them are recognized as the target person.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:14:56 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:03:16 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Song", "Qing", ""], ["Wu", "Yingqi", ""], ["Yang", "Lu", ""]]}, {"id": "1811.12030", "submitter": "Xin Lu", "authors": "Xin Lu, Buyu Li, Yuxin Yue, Quanquan Li, Junjie Yan", "title": "Grid R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel object detection framework named Grid R-CNN,\nwhich adopts a grid guided localization mechanism for accurate object\ndetection. Different from the traditional regression based methods, the Grid\nR-CNN captures the spatial information explicitly and enjoys the position\nsensitive property of fully convolutional architecture. Instead of using only\ntwo independent points, we design a multi-point supervision formulation to\nencode more clues in order to reduce the impact of inaccurate prediction of\nspecific points. To take the full advantage of the correlation of points in a\ngrid, we propose a two-stage information fusion strategy to fuse feature maps\nof neighbor grid points. The grid guided localization approach is easy to be\nextended to different state-of-the-art detection frameworks. Grid R-CNN leads\nto high quality object localization, and experiments demonstrate that it\nachieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO\nbenchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:20:43 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Lu", "Xin", ""], ["Li", "Buyu", ""], ["Yue", "Yuxin", ""], ["Li", "Quanquan", ""], ["Yan", "Junjie", ""]]}, {"id": "1811.12035", "submitter": "Siwen Jiang", "authors": "Siwen Jiang, Wenxuan Wei, Shihao Guo, Hongguang Fu, Lei Huang", "title": "Utilizing Complex-valued Network for Learning to Compare Image Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the great achievements of convolutional neural network(CNN) in\nfeature and metric learning have attracted many researchers. However, the vast\nmajority of deep network architectures have been used to represent based on\nreal values. The research of complex-valued networks is seldom concerned due to\nthe absence of effective models and suitable distance of complex-valued vector.\nMotived by recent works, complex vectors have been shown to have a richer\nrepresentational capacity and efficient complex blocks have been reported, we\npropose a new approach for learning image descriptors with complex numbers to\ncompare image patches. We also propose a new architecture to learn image\nsimilarity function directly based on complex-valued network. We show that our\nmodels can perform competitive results on benchmark datasets. We make the\nsource code of our models publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:31:09 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 02:13:46 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Jiang", "Siwen", ""], ["Wei", "Wenxuan", ""], ["Guo", "Shihao", ""], ["Fu", "Hongguang", ""], ["Huang", "Lei", ""]]}, {"id": "1811.12039", "submitter": "I\\~nigo Alonso", "authors": "I\\~nigo Alonso, Ana C. Murillo", "title": "EV-SegNet: Semantic Segmentation for Event-based Cameras", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event cameras, or Dynamic Vision Sensor (DVS), are very promising sensors\nwhich have shown several advantages over frame based cameras. However, most\nrecent work on real applications of these cameras is focused on 3D\nreconstruction and 6-DOF camera tracking. Deep learning based approaches, which\nare leading the state-of-the-art in visual recognition tasks, could potentially\ntake advantage of the benefits of DVS, but some adaptations are needed still\nneeded in order to effectively work on these cameras. This work introduces a\nfirst baseline for semantic segmentation with this kind of data. We build a\nsemantic segmentation CNN based on state-of-the-art techniques which takes\nevent information as the only input. Besides, we propose a novel representation\nfor DVS data that outperforms previously used event representations for related\ntasks. Since there is no existing labeled dataset for this task, we propose how\nto automatically generate approximated semantic segmentation labels for some\nsequences of the DDD17 dataset, which we publish together with the model, and\ndemonstrate they are valid to train a model for DVS data only. We compare our\nresults on semantic segmentation from DVS data with results using corresponding\ngrayscale images, demonstrating how they are complementary and worth combining.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:48:48 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Alonso", "I\u00f1igo", ""], ["Murillo", "Ana C.", ""]]}, {"id": "1811.12043", "submitter": "Jun-Hyuk Kim", "authors": "Jun-Hyuk Kim, Jun-Ho Choi, Manri Cheon and Jong-Seok Lee", "title": "MAMNet: Multi-path Adaptive Modulation Network for Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, single image super-resolution (SR) methods based on deep\nconvolutional neural networks (CNNs) have made significant progress. However,\ndue to the non-adaptive nature of the convolution operation, they cannot adapt\nto various characteristics of images, which limits their representational\ncapability and, consequently, results in unnecessarily large model sizes. To\naddress this issue, we propose a novel multi-path adaptive modulation network\n(MAMNet). Specifically, we propose a multi-path adaptive modulation block\n(MAMB), which is a lightweight yet effective residual block that adaptively\nmodulates residual feature responses by fully exploiting their information via\nthree paths. The three paths model three types of information suitable for SR:\n1) channel-specific information (CSI) using global variance pooling, 2)\ninter-channel dependencies (ICD) based on the CSI, 3) and channel-specific\nspatial dependencies (CSD) via depth-wise convolution. We demonstrate that the\nproposed MAMB is effective and parameter-efficient for image SR than other\nfeature modulation methods. In addition, experimental results show that our\nMAMNet outperforms most of the state-of-the-art methods with a relatively small\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:59:31 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 09:11:34 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Kim", "Jun-Hyuk", ""], ["Choi", "Jun-Ho", ""], ["Cheon", "Manri", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1811.12047", "submitter": "Xutong Ren", "authors": "Xutong Ren, Lingxi Xie, Chen Wei, Siyuan Qiao, Chi Su, Jiaying Liu, Qi\n  Tian, Elliot K. Fishman, Alan L. Yuille", "title": "Generalized Coarse-to-Fine Visual Recognition with Progressive Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is difficult, partly because the desired mathematical\nfunction connecting input and output data is often complex, fuzzy and thus hard\nto learn. Coarse-to-fine (C2F) learning is a promising direction, but it\nremains unclear how it is applied to a wide range of vision problems.\n  This paper presents a generalized C2F framework by making two technical\ncontributions. First, we provide a unified way of C2F propagation, in which the\ncoarse prediction (a class vector, a detected box, a segmentation mask, etc.)\nis encoded into a dense (pixel-level) matrix and concatenated to the original\ninput, so that the fine model takes the same design of the coarse model but\nsees additional information. Second, we present a progressive training strategy\nwhich starts with feeding the ground-truth instead of the coarse output into\nthe fine model, and gradually increases the fraction of coarse output, so that\nat the end of training the fine model is ready for testing. We also relate our\napproach to curriculum learning by showing that data difficulty keeps\nincreasing during the training process. We apply our framework to three vision\ntasks including image classification, object localization and semantic\nsegmentation, and demonstrate consistent accuracy gain compared to the baseline\ntraining strategy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 10:16:32 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 03:43:16 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Ren", "Xutong", ""], ["Xie", "Lingxi", ""], ["Wei", "Chen", ""], ["Qiao", "Siyuan", ""], ["Su", "Chi", ""], ["Liu", "Jiaying", ""], ["Tian", "Qi", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1811.12084", "submitter": "Andreas Selmar Hauptmann", "authors": "Simon Arridge and Andreas Hauptmann", "title": "Networks for Nonlinear Diffusion Problems in Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multitude of imaging and vision tasks have seen recently a major\ntransformation by deep learning methods and in particular by the application of\nconvolutional neural networks. These methods achieve impressive results, even\nfor applications where it is not apparent that convolutions are suited to\ncapture the underlying physics.\n  In this work we develop a network architecture based on nonlinear diffusion\nprocesses, named DiffNet. By design, we obtain a nonlinear network architecture\nthat is well suited for diffusion related problems in imaging. Furthermore, the\nperformed updates are explicit, by which we obtain better interpretability and\ngeneralisability compared to classical convolutional neural network\narchitectures. The performance of DiffNet tested on the inverse problem of\nnonlinear diffusion with the Perona-Malik filter on the STL-10 image dataset.\nWe obtain competitive results to the established U-Net architecture, with a\nfraction of parameters and necessary training data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:54:54 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Arridge", "Simon", ""], ["Hauptmann", "Andreas", ""]]}, {"id": "1811.12104", "submitter": "Mikihiro Tanaka", "authors": "Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato,\n  Yoshitaka Ushiku, Tatsuya Harada", "title": "Generating Easy-to-Understand Referring Expressions for Target\n  Identifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the generation of referring expressions that not only\nrefer to objects correctly but also let humans find them quickly. As a target\nbecomes relatively less salient, identifying referred objects itself becomes\nmore difficult. However, the existing studies regarded all sentences that refer\nto objects correctly as equally good, ignoring whether they are easily\nunderstood by humans. If the target is not salient, humans utilize\nrelationships with the salient contexts around it to help listeners to\ncomprehend it better. To derive this information from human annotations, our\nmodel is designed to extract information from the target and from the\nenvironment. Moreover, we regard that sentences that are easily understood are\nthose that are comprehended correctly and quickly by humans. We optimized this\nby using the time required to locate the referred objects by humans and their\naccuracies. To evaluate our system, we created a new referring expression\ndataset whose images were acquired from Grand Theft Auto V (GTA V), limiting\ntargets to persons. Experimental results show the effectiveness of our\napproach. Our code and dataset are available at\nhttps://github.com/mikittt/easy-to-understand-REG.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 12:46:54 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 10:30:34 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 12:49:02 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 04:23:10 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Tanaka", "Mikihiro", ""], ["Itamochi", "Takayuki", ""], ["Narioka", "Kenichi", ""], ["Sato", "Ikuro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.12108", "submitter": "Kilho Son", "authors": "Kilho Son and Jesse Hostetler and Sek Chai", "title": "Bootstrapping Deep Neural Networks from Approximate Image Processing\n  Pipelines", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex image processing and computer vision systems often consist of a\nprocessing pipeline of functional modules. We intend to replace parts or all of\na target pipeline with deep neural networks to achieve benefits such as\nincreased accuracy or reduced computational requirement. To acquire a large\namount of labeled data necessary to train the deep neural network, we propose a\nworkflow that leverages the target pipeline to create a significantly larger\nlabeled training set automatically, without prior domain knowledge of the\ntarget pipeline. We show experimentally that despite the noise introduced by\nautomated labeling and only using a very small initially labeled data set, the\ntrained deep neural networks can achieve similar or even better performance\nthan the components they replace, while in some cases also reducing\ncomputational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 12:54:51 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 21:22:18 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Son", "Kilho", ""], ["Hostetler", "Jesse", ""], ["Chai", "Sek", ""]]}, {"id": "1811.12139", "submitter": "Muzi Peng", "authors": "Xiaohua Wang, Muzi Peng, Lijuan Pan, Min Hu, Chunhua Jin, Fuji Ren", "title": "Two-level Attention with Two-stage Multi-task Learning for Facial\n  Emotion Recognition", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with facial emotion recognition on categorical model, the\ndimensional emotion recognition can describe numerous emotions of the real\nworld more accurately. Most prior works of dimensional emotion estimation only\nconsidered laboratory data and used video, speech or other multi-modal\nfeatures. The effect of these methods applied on static images in the real\nworld is unknown. In this paper, a two-level attention with two-stage\nmulti-task learning (2Att-2Mt) framework is proposed for facial emotion\nestimation on only static images. Firstly, the features of corresponding\nregion(position-level features) are extracted and enhanced automatically by\nfirst-level attention mechanism. In the following, we utilize Bi-directional\nRecurrent Neural Network(Bi-RNN) with self-attention(second-level attention) to\nmake full use of the relationship features of different layers(layer-level\nfeatures) adaptively. Owing to the inherent complexity of dimensional emotion\nrecognition, we propose a two-stage multi-task learning structure to exploited\ncategorical representations to ameliorate the dimensional representations and\nestimate valence and arousal simultaneously in view of the correlation of the\ntwo targets. The quantitative results conducted on AffectNet dataset show\nsignificant advancement on Concordance Correlation Coefficient(CCC) and Root\nMean Square Error(RMSE), illustrating the superiority of the proposed\nframework. Besides, extensive comparative experiments have also fully\ndemonstrated the effectiveness of different components.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:47:01 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Wang", "Xiaohua", ""], ["Peng", "Muzi", ""], ["Pan", "Lijuan", ""], ["Hu", "Min", ""], ["Jin", "Chunhua", ""], ["Ren", "Fuji", ""]]}, {"id": "1811.12150", "submitter": "Haoran Wang", "authors": "Haoran Wang, Yue Fan, Zexin Wang, Licheng Jiao, Bernt Schiele", "title": "Parameter-Free Spatial Attention Network for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global average pooling (GAP) allows to localize discriminative information\nfor recognition [40]. While GAP helps the convolution neural network to attend\nto the most discriminative features of an object, it may suffer if that\ninformation is missing e.g. due to camera viewpoint changes. To circumvent this\nissue, we argue that it is advantageous to attend to the global configuration\nof the object by modeling spatial relations among high-level features. We\npropose a novel architecture for Person Re-Identification, based on a novel\nparameter-free spatial attention layer introducing spatial relations among the\nfeature map activations back to the model. Our spatial attention layer\nconsistently improves the performance over the model without it. Results on\nfour benchmarks demonstrate a superiority of our model over the\nstate-of-the-art achieving rank-1 accuracy of 94.7% on Market-1501, 89.0% on\nDukeMTMC-ReID, 74.9% on CUHK03-labeled and 69.7% on CUHK03-detected.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:05:04 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Wang", "Haoran", ""], ["Fan", "Yue", ""], ["Wang", "Zexin", ""], ["Jiao", "Licheng", ""], ["Schiele", "Bernt", ""]]}, {"id": "1811.12152", "submitter": "Hila Levi", "authors": "Hila Levi, Shimon Ullman", "title": "Efficient Coarse-to-Fine Non-Local Module for the Detection of Small\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image is not just a collection of objects, but rather a graph where each\nobject is related to other objects through spatial and semantic relations.\nUsing relational reasoning modules, such as the non-local module\n\\cite{wang2017non}, can therefore improve object detection. Current schemes\napply such dedicated modules either to a specific layer of the bottom-up\nstream, or between already-detected objects. We show that the relational\nprocess can be better modeled in a coarse-to-fine manner and present a novel\nframework, applying a non-local module sequentially to increasing resolution\nfeature maps along the top-down stream. In this way, information can naturally\npassed from larger objects to smaller related ones. Applying the module to fine\nfeature maps further allows the information to pass between the small objects\nthemselves, exploiting repetitions of instances of the same class. In practice,\ndue to the expensive memory utilization of the non-local module, it is\ninfeasible to apply the module as currently used to high-resolution feature\nmaps. We redesigned the non local module, improved it in terms of memory and\nnumber of operations, allowing it to be placed anywhere along the network. We\nfurther incorporated relative spatial information into the module, in a manner\nthat can be incorporated into our efficient implementation. We show the\neffectiveness of our scheme by improving the results of detecting small objects\non COCO by 1-2 AP points over Faster and Mask RCNN and by 1 AP over using\nnon-local module on the bottom-up stream.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:09:45 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 19:38:48 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Levi", "Hila", ""], ["Ullman", "Shimon", ""]]}, {"id": "1811.12197", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Stamatios Lefkimmiatis", "title": "Iterative Residual CNNs for Burst Photography Applications", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern inexpensive imaging sensors suffer from inherent hardware constraints\nwhich often result in captured images of poor quality. Among the most common\nways to deal with such limitations is to rely on burst photography, which\nnowadays acts as the backbone of all modern smartphone imaging applications. In\nthis work, we focus on the fact that every frame of a burst sequence can be\naccurately described by a forward (physical) model. This in turn allows us to\nrestore a single image of higher quality from a sequence of low quality images\nas the solution of an optimization problem. Inspired by an extension of the\ngradient descent method that can handle non-smooth functions, namely the\nproximal gradient descent, and modern deep learning techniques, we propose a\nconvolutional iterative network with a transparent architecture. Our network,\nuses a burst of low quality image frames and is able to produce an output of\nhigher image quality recovering fine details which are not distinguishable in\nany of the original burst frames. We focus both on the burst photography\npipeline as a whole, i.e. burst demosaicking and denoising, as well as on the\ntraditional Gaussian denoising task. The developed method demonstrates\nconsistent state-of-the art performance across the two tasks and as opposed to\nother recent deep learning approaches does not have any inherent restrictions\neither to the number of frames or their ordering. Code can be found at\nhttps://fkokkinos.github.io/deep_burst/\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:30:52 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 12:53:16 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1811.12222", "submitter": "Xibin Song", "authors": "Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai,\n  Hao Su, Hongdong Li and Ruigang Yang", "title": "ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving has attracted remarkable attention from both industry and\nacademia. An important task is to estimate 3D properties(e.g.translation,\nrotation and shape) of a moving or parked vehicle on the road. This task, while\ncritical, is still under-researched in the computer vision community -\npartially owing to the lack of large scale and fully-annotated 3D car database\nsuitable for autonomous driving research. In this paper, we contribute the\nfirst large-scale database suitable for 3D car instance understanding -\nApolloCar3D. The dataset contains 5,277 driving images and over 60K car\ninstances, where each car is fitted with an industry-grade 3D CAD model with\nabsolute model size and semantically labelled keypoints. This dataset is above\n20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. To\nenable efficient labelling in 3D, we build a pipeline by considering 2D-3D\nkeypoint correspondences for a single instance and 3D relationship among\nmultiple instances. Equipped with such dataset, we build various baseline\nalgorithms with the state-of-the-art deep convolutional neural networks.\nSpecifically, we first segment each car with a pre-trained Mask R-CNN, and then\nregress towards its 3D pose and shape based on a deformable 3D car model with\nor without using semantic keypoints. We show that using keypoints significantly\nimproves fitting performance. Finally, we develop a new 3D metric jointly\nconsidering 3D pose and 3D shape, allowing for comprehensive evaluation and\nablation study. By comparing with human performance we suggest several future\ndirections for further improvements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:56:58 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:43:54 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Song", "Xibin", ""], ["Wang", "Peng", ""], ["Zhou", "Dingfu", ""], ["Zhu", "Rui", ""], ["Guan", "Chenye", ""], ["Dai", "Yuchao", ""], ["Su", "Hao", ""], ["Li", "Hongdong", ""], ["Yang", "Ruigang", ""]]}, {"id": "1811.12231", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge,\n  Felix A. Wichmann, Wieland Brendel", "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias\n  improves accuracy and robustness", "comments": "Accepted at ICLR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are commonly thought to recognise\nobjects by learning increasingly complex representations of object shapes. Some\nrecent studies suggest a more important role of image textures. We here put\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\nhuman observers on images with a texture-shape cue conflict. We show that\nImageNet-trained CNNs are strongly biased towards recognising textures rather\nthan shapes, which is in stark contrast to human behavioural evidence and\nreveals fundamentally different classification strategies. We then demonstrate\nthat the same standard architecture (ResNet-50) that learns a texture-based\nrepresentation on ImageNet is able to learn a shape-based representation\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\nThis provides a much better fit for human behavioural performance in our\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\npsychophysical trials across 97 observers) and comes with a number of\nunexpected emergent benefits such as improved object detection performance and\npreviously unseen robustness towards a wide range of image distortions,\nhighlighting advantages of a shape-based representation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:04:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 13:59:09 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Geirhos", "Robert", ""], ["Rubisch", "Patricia", ""], ["Michaelis", "Claudio", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "1811.12238", "submitter": "Siyu Huang", "authors": "Siyu Huang, Zhi-Qi Cheng, Xi Li, Xiao Wu, Zhongfei Zhang, Alexander\n  Hauptmann", "title": "Perceiving Physical Equation by Observing Visual Scenarios", "comments": "NIPS 2018 Workshop on Modeling the Physical World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring universal laws of the environment is an important ability of human\nintelligence as well as a symbol of general AI. In this paper, we take a step\ntoward this goal such that we introduce a new challenging problem of inferring\ninvariant physical equation from visual scenarios. For instance, teaching a\nmachine to automatically derive the gravitational acceleration formula by\nwatching a free-falling object. To tackle this challenge, we present a novel\npipeline comprised of an Observer Engine and a Physicist Engine by respectively\nimitating the actions of an observer and a physicist in the real world.\nGenerally, the Observer Engine watches the visual scenarios and then extracting\nthe physical properties of objects. The Physicist Engine analyses these data\nand then summarizing the inherent laws of object dynamics. Specifically, the\nlearned laws are expressed by mathematical equations such that they are more\ninterpretable than the results given by common probabilistic models.\nExperiments on synthetic videos have shown that our pipeline is able to\ndiscover physical equations on various physical worlds with different visual\nappearances.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:13:26 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Huang", "Siyu", ""], ["Cheng", "Zhi-Qi", ""], ["Li", "Xi", ""], ["Wu", "Xiao", ""], ["Zhang", "Zhongfei", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1811.12248", "submitter": "Yingli Tian", "authors": "Yuancheng Ye, Xiaodong Yang, and Yingli Tian", "title": "Discovering Spatio-Temporal Action Tubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging problem of spatial and temporal\naction detection in videos. We first develop an effective approach to localize\nframe-level action regions through integrating static and kinematic information\nby the early- and late-fusion detection scheme. With the intention of exploring\nimportant temporal connections among the detected action regions, we propose a\ntracking-by-point-matching algorithm to stitch the discrete action regions into\na continuous spatio-temporal action tube. Recurrent 3D convolutional neural\nnetwork is used to predict action categories and determine temporal boundaries\nof the generated tubes. We then introduce an action footprint map to refine the\ncandidate tubes based on the action-specific spatial characteristics preserved\nin the convolutional layers of R3DCNN. In the extensive experiments, our method\nachieves superior detection results on the three public benchmark datasets:\nUCFSports, J-HMDB and UCF101.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:29:43 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ye", "Yuancheng", ""], ["Yang", "Xiaodong", ""], ["Tian", "Yingli", ""]]}, {"id": "1811.12296", "submitter": "Vinkle Kumar Srivastav", "authors": "Thibaut Issenhuth, Vinkle Srivastav, Afshin Gangi, Nicolas Padoy", "title": "Face Detection in the Operating Room: Comparison of State-of-the-art\n  Methods and a Self-supervised Approach", "comments": "13 pages", "journal-ref": null, "doi": "10.1007/s11548-019-01944-y", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Face detection is a needed component for the automatic analysis and\nassistance of human activities during surgical procedures. Efficient face\ndetection algorithms can indeed help to detect and identify the persons present\nin the room, and also be used to automatically anonymize the data. However,\ncurrent algorithms trained on natural images do not generalize well to the\noperating room (OR) images. In this work, we provide a comparison of\nstate-of-the-art face detectors on OR data and also present an approach to\ntrain a face detector for the OR by exploiting non-annotated OR images.\nMethods: We propose a comparison of 6 state-of-the-art face detectors on\nclinical data using Multi-View Operating Room Faces (MVOR-Faces), a dataset of\noperating room images capturing real surgical activities. We then propose to\nuse self-supervision, a domain adaptation method, for the task of face\ndetection in the OR. The approach makes use of non-annotated images to\nfine-tune a state-of-the-art detector for the OR without using any human\nsupervision. Results: The results show that the best model, namely the tiny\nface detector, yields an average precision of 0.536 at Intersection over Union\n(IoU) of 0.5. Our self-supervised model using non-annotated clinical data\noutperforms this result by 9.2%. Conclusion: We present the first comparison of\nstate-of-the-art face detectors on operating room images and show that results\ncan be significantly improved by using self-supervision on non-annotated data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:38:16 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 11:08:53 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Issenhuth", "Thibaut", ""], ["Srivastav", "Vinkle", ""], ["Gangi", "Afshin", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1811.12297", "submitter": "Benjamin Planche", "authors": "Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna Karanam, Harald\n  Kosch, YingLi Tian, Jan Ernst, Andreas Hutter", "title": "Incremental Scene Synthesis", "comments": null, "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to incrementally generate complete 2D or 3D scenes with\nthe following properties: (a) it is globally consistent at each step according\nto a learned scene prior, (b) real observations of a scene can be incorporated\nwhile observing global consistency, (c) unobserved regions can be hallucinated\nlocally in consistence with previous observations, hallucinations and global\npriors, and (d) hallucinations are statistical in nature, i.e., different\nscenes can be generated from the same observations. To achieve this, we model\nthe virtual scene, where an active agent at each step can either perceive an\nobserved part of the scene or generate a local hallucination. The latter can be\ninterpreted as the agent's expectation at this step through the scene and can\nbe applied to autonomous navigation. In the limit of observing real data at\neach point, our method converges to solving the SLAM problem. It can otherwise\nsample entirely imagined scenes from prior distributions. Besides autonomous\nagents, applications include problems where large data is required for building\nrobust real-world applications, but few samples are available. We demonstrate\nefficacy on various 2D as well as 3D data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:41:44 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 08:57:48 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 20:41:46 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 19:50:54 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Planche", "Benjamin", ""], ["Rong", "Xuejian", ""], ["Wu", "Ziyan", ""], ["Karanam", "Srikrishna", ""], ["Kosch", "Harald", ""], ["Tian", "YingLi", ""], ["Ernst", "Jan", ""], ["Hutter", "Andreas", ""]]}, {"id": "1811.12326", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi, Alireza Zaeemzadeh, Nazanin Rahnavard, and Mubarak\n  Shah", "title": "Iterative Projection and Matching: Finding Structure-preserving\n  Representatives and Its Application to Computer Vision", "comments": "11 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of data selection is to capture the most structural information from\na set of data. This paper presents a fast and accurate data selection method,\nin which the selected samples are optimized to span the subspace of all data.\nWe propose a new selection algorithm, referred to as iterative projection and\nmatching (IPM), with linear complexity w.r.t. the number of data, and without\nany parameter to be tuned. In our algorithm, at each iteration, the maximum\ninformation from the structure of the data is captured by one selected sample,\nand the captured information is neglected in the next iterations by projection\non the null-space of previously selected samples. The computational efficiency\nand the selection accuracy of our proposed algorithm outperform those of the\nconventional methods. Furthermore, the superiority of the proposed algorithm is\nshown on active learning for video action recognition dataset on UCF-101;\nlearning using representatives on ImageNet; training a generative adversarial\nnetwork (GAN) to generate multi-view images from a single-view input on CMU\nMulti-PIE dataset; and video summarization on UTE Egocentric dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:24:13 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Joneidi", "Mohsen", ""], ["Zaeemzadeh", "Alireza", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""]]}, {"id": "1811.12328", "submitter": "William Smith", "authors": "Ye Yu and William A. P. Smith", "title": "InverseRenderNet: Learning single image inverse rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train a fully convolutional neural network to perform inverse\nrendering from a single, uncontrolled image. The network takes an RGB image as\ninput, regresses albedo and normal maps from which we compute lighting\ncoefficients. Our network is trained using large uncontrolled image collections\nwithout ground truth. By incorporating a differentiable renderer, our network\ncan learn from self-supervision. Since the problem is ill-posed we introduce\nadditional supervision: 1. We learn a statistical natural illumination prior,\n2. Our key insight is to perform offline multiview stereo (MVS) on images\ncontaining rich illumination variation. From the MVS pose and depth maps, we\ncan cross project between overlapping views such that Siamese training can be\nused to ensure consistent estimation of photometric invariants. MVS depth also\nprovides direct coarse supervision for normal map estimation. We believe this\nis the first attempt to use MVS supervision for learning inverse rendering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:27:03 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yu", "Ye", ""], ["Smith", "William A. P.", ""]]}, {"id": "1811.12354", "submitter": "Yoav Artzi", "authors": "Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi", "title": "Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments", "comments": "arXiv admin note: text overlap with arXiv:1809.00786", "journal-ref": "Published in CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:06:22 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:17:42 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 17:27:38 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 17:35:24 GMT"}, {"version": "v5", "created": "Wed, 10 Apr 2019 18:43:40 GMT"}, {"version": "v6", "created": "Mon, 3 Jun 2019 16:24:49 GMT"}, {"version": "v7", "created": "Sat, 16 May 2020 23:36:36 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Howard", ""], ["Suhr", "Alane", ""], ["Misra", "Dipendra", ""], ["Snavely", "Noah", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.12362", "submitter": "Simyung Chang", "authors": "Simyung Chang, SeongUk Park, John Yang, Nojun Kwak", "title": "Sym-parameterized Dynamic Inference for Mixed-Domain Image Translation", "comments": "16pages, This paper is accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image-to-image translation have led to some ways to\ngenerate multiple domain images through a single network. However, there is\nstill a limit in creating an image of a target domain without a dataset on it.\nWe propose a method that expands the concept of `multi-domain' from data to the\nloss area and learns the combined characteristics of each domain to dynamically\ninfer translations of images in mixed domains. First, we introduce\nSym-parameter and its learning method for variously mixed losses while\nsynchronizing them with input conditions. Then, we propose Sym-parameterized\nGenerative Network (SGN) which is empirically confirmed of learning mixed\ncharacteristics of various data and losses, and translating images to any\nmixed-domain without ground truths, such as 30% Van Gogh and 20% Monet and 40%\nsnowy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:14:16 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 15:41:01 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 10:24:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chang", "Simyung", ""], ["Park", "SeongUk", ""], ["Yang", "John", ""], ["Kwak", "Nojun", ""]]}, {"id": "1811.12373", "submitter": "Ke Li", "authors": "Ke Li, Tianhao Zhang, Jitendra Malik", "title": "Diverse Image Synthesis from Semantic Layouts via Conditional IMLE", "comments": "18 pages, 16 figures; IEEE International Conference on Computer\n  Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods for conditional image synthesis are only able to\ngenerate a single plausible image for any given input, or at best a fixed\nnumber of plausible images. In this paper, we focus on the problem of\ngenerating images from semantic segmentation maps and present a simple new\nmethod that can generate an arbitrary number of images with diverse appearance\nfor the same semantic layout. Unlike most existing approaches which adopt the\nGAN framework, our method is based on the recently introduced Implicit Maximum\nLikelihood Estimation (IMLE) framework. Compared to the leading approach, our\nmethod is able to generate more diverse images while producing fewer artifacts\ndespite using the same architecture. The learned latent space also has sensible\nstructure despite the lack of supervision that encourages such behaviour.\nVideos and code are available at\nhttps://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:36:00 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 17:54:53 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Ke", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12402", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "On the Implicit Assumptions of GANs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) have generated a lot of excitement.\nDespite their popularity, they exhibit a number of well-documented issues in\npractice, which apparently contradict theoretical guarantees. A number of\nenlightening papers have pointed out that these issues arise from unjustified\nassumptions that are commonly made, but the message seems to have been lost\namid the optimism of recent years. We believe the identified problems deserve\nmore attention, and highlight the implications on both the properties of GANs\nand the trajectory of research on probabilistic models. We recently proposed an\nalternative method that sidesteps these problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:59:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12432", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, Larry S. Davis", "title": "AdaFrame: Adaptive Frame Selection for Fast Video Recognition", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AdaFrame, a framework that adaptively selects relevant frames on a\nper-input basis for fast video recognition. AdaFrame contains a Long Short-Term\nMemory network augmented with a global memory that provides context information\nfor searching which frames to use over time. Trained with policy gradient\nmethods, AdaFrame generates a prediction, determines which frame to observe\nnext, and computes the utility, i.e., expected future rewards, of seeing more\nframes at each time step. At testing time, AdaFrame exploits predicted\nutilities to achieve adaptive lookahead inference such that the overall\ncomputational costs are reduced without incurring a decrease in accuracy.\nExtensive experiments are conducted on two large-scale video benchmarks, FCVID\nand ActivityNet. AdaFrame matches the performance of using all frames with only\n8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We further\nqualitatively demonstrate learned frame usage can indicate the difficulty of\nmaking classification decisions; easier samples need fewer frames while harder\nones require more, both at instance-level within the same class and at\nclass-level among different categories.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:08:52 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 14:54:13 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Wu", "Zuxuan", ""], ["Xiong", "Caiming", ""], ["Ma", "Chih-Yao", ""], ["Socher", "Richard", ""], ["Davis", "Larry S.", ""]]}, {"id": "1811.12463", "submitter": "Kai Wang", "authors": "Daniel Ritchie, Kai Wang, Yu-an Lin", "title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, fast and flexible pipeline for indoor scene synthesis that\nis based on deep convolutional generative models. Our method operates on a\ntop-down image-based representation, and inserts objects iteratively into the\nscene by predicting their category, location, orientation and size with\nseparate neural network modules. Our pipeline naturally supports automatic\ncompletion of partial scenes, as well as synthesis of complete scenes. Our\nmethod is significantly faster than the previous image-based method and\ngenerates result that outperforms it and other state-of-the-art deep generative\nscene models in terms of faithfulness to training data and perceived visual\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:03:28 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ritchie", "Daniel", ""], ["Wang", "Kai", ""], ["Lin", "Yu-an", ""]]}, {"id": "1811.12481", "submitter": "Zhuo Hui", "authors": "Zhuo Hui, Ayan Chakrabarti, Kalyan Sunkavalli, and Aswin C.\n  Sankaranarayanan", "title": "Learning to Separate Multiple Illuminants in a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to separate a single image captured under two\nilluminants, with different spectra, into the two images corresponding to the\nappearance of the scene under each individual illuminant. We do this by\ntraining a deep neural network to predict the per-pixel reflectance\nchromaticity of the scene, which we use in conjunction with a previous\nflash/no-flash image-based separation algorithm to produce the final two output\nimages. We design our reflectance chromaticity network and loss functions by\nincorporating intuitions from the physics of image formation. We show that this\nleads to significantly better performance than other single image techniques\nand even approaches the quality of the two image separation method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:56:25 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 22:50:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hui", "Zhuo", ""], ["Chakrabarti", "Ayan", ""], ["Sunkavalli", "Kalyan", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1811.12488", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Muhammad Awais, Muhammad Asim, Zain ul Aabidin Lodhi,\n  Muhammad Umair, Ali Ahmed", "title": "Leveraging Deep Stein's Unbiased Risk Estimator for Unsupervised X-ray\n  Denoising", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/223", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the plethora of techniques devised to curb the prevalence of noise in\nmedical images, deep learning based approaches have shown the most promise.\nHowever, one critical limitation of these deep learning based denoisers is the\nrequirement of high-quality noiseless ground truth images that are difficult to\nobtain in many medical imaging applications such as X-rays. To circumvent this\nissue, we leverage recently proposed approach of [7] that incorporates Stein's\nUnbiased Risk Estimator (SURE) to train a deep convolutional neural network\nwithout requiring denoised ground truth X-ray data. Our experimental results\ndemonstrate the effectiveness of SURE based approach for denoising X-ray\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:04:38 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Shamshad", "Fahad", ""], ["Awais", "Muhammad", ""], ["Asim", "Muhammad", ""], ["Lodhi", "Zain ul Aabidin", ""], ["Umair", "Muhammad", ""], ["Ahmed", "Ali", ""]]}, {"id": "1811.12493", "submitter": "Francisco Leiva", "authors": "Francisco Leiva, Nicol\\'as Cruz, Ignacio Bugue\\~no and Javier\n  Ruiz-del-Solar", "title": "Playing Soccer without Colors in the SPL: A Convolutional Neural Network\n  Approach", "comments": "12 pages, 6 figures. Presented in RoboCup Symposium 2018. Final\n  version will appear in Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to propose a vision system for humanoid robotic\nsoccer that does not use any color information. The main features of this\nsystem are: (i) real-time operation in the NAO robot, and (ii) the ability to\ndetect the ball, the robots, their orientations, the lines and key field\nfeatures robustly. Our ball detector, robot detector, and robot's orientation\ndetector obtain the highest reported detection rates. The proposed vision\nsystem is tested in a SPL field with several NAO robots under realistic and\nhighly demanding conditions. The obtained results are: robot detection rate of\n94.90%, ball detection rate of 97.10%, and a completely perceived orientation\nrate of 99.88% when the observed robot is static, and 95.52% when the observed\nrobot is moving.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:26:01 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Leiva", "Francisco", ""], ["Cruz", "Nicol\u00e1s", ""], ["Bugue\u00f1o", "Ignacio", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "1811.12495", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Kwang In Kim, Christian Theobalt", "title": "On Implicit Filter Level Sparsity in Convolutional Neural Networks", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate filter level sparsity that emerges in convolutional neural\nnetworks (CNNs) which employ Batch Normalization and ReLU activation, and are\ntrained with adaptive gradient descent techniques and L2 regularization or\nweight decay. We conduct an extensive experimental study casting our initial\nfindings into hypotheses and conclusions about the mechanisms underlying the\nemergent filter level sparsity. This study allows new insight into the\nperformance gap obeserved between adapative and non-adaptive gradient descent\nmethods in practice. Further, analysis of the effect of training strategies and\nhyperparameters on the sparsity leads to practical suggestions in designing CNN\ntraining strategies enabling us to explore the tradeoffs between feature\nselectivity, network capacity, and generalization performance. Lastly, we show\nthat the implicit sparsity can be harnessed for neural network speedup at par\nor better than explicit sparsification / pruning approaches, with no\nmodifications to the typical training pipeline required.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:29:31 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 15:40:40 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mehta", "Dushyant", ""], ["Kim", "Kwang In", ""], ["Theobalt", "Christian", ""]]}, {"id": "1811.12506", "submitter": "Yingda Xia", "authors": "Yingda Xia, Fengze Liu, Dong Yang, Jinzheng Cai, Lequan Yu, Zhuotun\n  Zhu, Daguang Xu, Alan Yuille, Holger Roth", "title": "3D Semi-Supervised Learning with Uncertainty-Aware Multi-View\n  Co-Training", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While making a tremendous impact in various fields, deep neural networks\nusually require large amounts of labeled data for training which are expensive\nto collect in many applications, especially in the medical domain. Unlabeled\ndata, on the other hand, is much more abundant. Semi-supervised learning\ntechniques, such as co-training, could provide a powerful tool to leverage\nunlabeled data. In this paper, we propose a novel framework, uncertainty-aware\nmulti-view co-training (UMCT), to address semi-supervised learning on 3D data,\nsuch as volumetric data from medical imaging. In our work, co-training is\nachieved by exploiting multi-viewpoint consistency of 3D data. We generate\ndifferent views by rotating or permuting the 3D data and utilize asymmetrical\n3D kernels to encourage diversified features in different sub-networks. In\naddition, we propose an uncertainty-weighted label fusion mechanism to estimate\nthe reliability of each view's prediction with Bayesian deep learning. As one\nview requires the supervision from other views in co-training, our\nself-adaptive approach computes a confidence score for the prediction of each\nunlabeled sample in order to assign a reliable pseudo label. Thus, our approach\ncan take advantage of unlabeled data during training. We show the effectiveness\nof our proposed semi-supervised method on several public datasets from medical\nimage segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,\na fully-supervised method based on our approach achieved state-of-the-art\nperformances on both the LiTS liver tumor segmentation and the Medical\nSegmentation Decathlon (MSD) challenge, demonstrating the robustness and value\nof our framework, even when fully supervised training is feasible.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:58:53 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 01:50:28 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Xia", "Yingda", ""], ["Liu", "Fengze", ""], ["Yang", "Dong", ""], ["Cai", "Jinzheng", ""], ["Yu", "Lequan", ""], ["Zhu", "Zhuotun", ""], ["Xu", "Daguang", ""], ["Yuille", "Alan", ""], ["Roth", "Holger", ""]]}, {"id": "1811.12546", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Jun-Hyuk Kim, Manri Cheon, Jong-Seok Lee", "title": "Lightweight and Efficient Image Super-Resolution with Block State-based\n  Recursive Network", "comments": "The code is available at https://github.com/idearibosome/tf-bsrn-sr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several deep learning-based image super-resolution methods have\nbeen developed by stacking massive numbers of layers. However, this leads too\nlarge model sizes and high computational complexities, thus some recursive\nparameter-sharing methods have been also proposed. Nevertheless, their designs\ndo not properly utilize the potential of the recursive operation. In this\npaper, we propose a novel, lightweight, and efficient super-resolution method\nto maximize the usefulness of the recursive architecture, by introducing block\nstate-based recursive network. By taking advantage of utilizing the block\nstate, the recursive part of our model can easily track the status of the\ncurrent image features. We show the benefits of the proposed method in terms of\nmodel size, speed, and efficiency. In addition, we show that our method\noutperforms the other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 00:01:37 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Kim", "Jun-Hyuk", ""], ["Cheon", "Manri", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1811.12563", "submitter": "Tessie Chao", "authors": "Tianqi Zhao", "title": "Deep Multimodal Learning: An Effective Method for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos have become ubiquitous on the Internet. And video analysis can provide\nlots of information for detecting and recognizing objects as well as help\npeople understand human actions and interactions with the real world. However,\nfacing data as huge as TB level, effective methods should be applied. Recurrent\nneural network (RNN) architecture has wildly been used on many sequential\nlearning problems such as Language Model, Time-Series Analysis, etc. In this\npaper, we propose some variations of RNN such as stacked bidirectional LSTM/GRU\nnetwork with attention mechanism to categorize large-scale video data. We also\nexplore different multimodal fusion methods. Our model combines both visual and\naudio information on both video and frame level and received great result.\nEnsemble methods are also applied. Because of its multimodal characteristics,\nwe decide to call this method Deep Multimodal Learning(DML). Our DML-based\nmodel was trained on Google Cloud and our own server and was tested in a\nwell-known video classification competition on Kaggle held by Google.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:05:41 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhao", "Tianqi", ""]]}, {"id": "1811.12569", "submitter": "Kailas Vodrahalli", "authors": "Kailas Vodrahalli, Ke Li, Jitendra Malik", "title": "Are All Training Examples Created Equal? An Empirical Study", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision algorithms often rely on very large training datasets.\nHowever, it is conceivable that a carefully selected subsample of the dataset\nis sufficient for training. In this paper, we propose a gradient-based\nimportance measure that we use to empirically analyze relative importance of\ntraining images in four datasets of varying complexity. We find that in some\ncases, a small subsample is indeed sufficient for training. For other datasets,\nhowever, the relative differences in importance are negligible. These results\nhave important implications for active learning on deep networks. Additionally,\nour analysis method can be used as a general tool to better understand\ndiversity of training examples in datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:16:42 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Vodrahalli", "Kailas", ""], ["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12596", "submitter": "Lu Yang", "authors": "Lu Yang, Qing Song, Zhihui Wang, Ming Jiang", "title": "Parsing R-CNN for Instance-Level Human Analysis", "comments": "COCO 2018 DensePose Challenge Winner", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-level human analysis is common in real-life scenarios and has\nmultiple manifestations, such as human part segmentation, dense pose\nestimation, human-object interactions, etc. Models need to distinguish\ndifferent human instances in the image panel and learn rich features to\nrepresent the details of each instance. In this paper, we present an end-to-end\npipeline for solving the instance-level human analysis, named Parsing R-CNN. It\nprocesses a set of human instances simultaneously through comprehensive\nconsidering the characteristics of region-based approach and the appearance of\na human, thus allowing representing the details of instances. Parsing R-CNN is\nvery flexible and efficient, which is applicable to many issues in human\ninstance analysis. Our approach outperforms all state-of-the-art methods on\nCIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and\nDensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st\nplace in the COCO 2018 Challenge DensePose Estimation task. Code and models are\npublic available.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:21:11 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yang", "Lu", ""], ["Song", "Qing", ""], ["Wang", "Zhihui", ""], ["Jiang", "Ming", ""]]}, {"id": "1811.12607", "submitter": "Savinay Nagendra", "authors": "Christopher Funk, Savinay Nagendra, Jesse Scott, Bharadwaj\n  Ravichandran, John H. Challis, Robert T. Collins, Yanxi Liu", "title": "Learning Dynamics from Kinematics: Estimating 2D Foot Pressure Maps from\n  Video Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose stability analysis is the key to understanding locomotion and control of\nbody equilibrium, with applications in numerous fields such as kinesiology,\nmedicine, and robotics. In biomechanics, Center of Pressure (CoP) is used in\nstudies of human postural control and gait. We propose and validate a novel\napproach to learn CoP from pose of a human body to aid stability analysis. More\nspecifically, we propose an end-to-end deep learning architecture to regress\nfoot pressure heatmaps, and hence the CoP locations, from 2D human pose derived\nfrom video. We have collected a set of long (5min +) choreographed Taiji (Tai\nChi) sequences of multiple subjects with synchronized foot pressure and video\ndata. The derived human pose data and corresponding foot pressure maps are used\njointly in training a convolutional neural network with residual architecture,\nnamed PressNET. Cross-subject validation results show promising performance of\nPressNET, significantly outperforming the baseline method of K-Nearest\nNeighbors. Furthermore, we demonstrate that our computation of center of\npressure (CoP) from PressNET is not only significantly more accurate than those\nobtained from the baseline approach but also meets the expectations of\ncorresponding lab-based measurements of stability studies in kinesiology.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 04:16:24 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 22:25:51 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 03:00:38 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 13:33:23 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Funk", "Christopher", ""], ["Nagendra", "Savinay", ""], ["Scott", "Jesse", ""], ["Ravichandran", "Bharadwaj", ""], ["Challis", "John H.", ""], ["Collins", "Robert T.", ""], ["Liu", "Yanxi", ""]]}, {"id": "1811.12608", "submitter": "Yongchao Xu", "authors": "Yukang Wang, Yongchao Xu, Stavros Tsogkas, Xiang Bai, Sven Dickinson,\n  Kaleem Siddiqi", "title": "DeepFlux for Skeletons in the Wild", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing object skeletons in natural images is challenging, owing to large\nvariations in object appearance and scale, and the complexity of handling\nbackground clutter. Many recent methods frame object skeleton detection as a\nbinary pixel classification problem, which is similar in spirit to\nlearning-based edge detection, as well as to semantic segmentation methods. In\nthe present article, we depart from this strategy by training a CNN to predict\na two-dimensional vector field, which maps each scene point to a candidate\nskeleton pixel, in the spirit of flux-based skeletonization algorithms. This\n\"image context flux\" representation has two major advantages over previous\napproaches. First, it explicitly encodes the relative position of skeletal\npixels to semantically meaningful entities, such as the image points in their\nspatial context, and hence also the implied object boundaries. Second, since\nthe skeleton detection context is a region-based vector field, it is better\nable to cope with object parts of large width. We evaluate the proposed method\non three benchmark datasets for skeleton detection and two for symmetry\ndetection, achieving consistently superior performance over state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 04:21:47 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Wang", "Yukang", ""], ["Xu", "Yongchao", ""], ["Tsogkas", "Stavros", ""], ["Bai", "Xiang", ""], ["Dickinson", "Sven", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "1811.12611", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng, Haifeng Shen", "title": "Virtual Class Enhanced Discriminative Embedding Learning", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning discriminative features to improve the recognition\nperformances gradually becomes the primary goal of deep learning, and numerous\nremarkable works have emerged. In this paper, we propose a novel yet extremely\nsimple method \\textbf{Virtual Softmax} to enhance the discriminative property\nof learned features by injecting a dynamic virtual negative class into the\noriginal softmax. Injecting virtual class aims to enlarge inter-class margin\nand compress intra-class distribution by strengthening the decision boundary\nconstraint. Although it seems weird to optimize with this additional virtual\nclass, we show that our method derives from an intuitive and clear motivation,\nand it indeed encourages the features to be more compact and separable. This\npaper empirically and experimentally demonstrates the superiority of Virtual\nSoftmax, improving the performances on a variety of object classification and\nface verification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 04:43:20 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""], ["Shen", "Haifeng", ""]]}, {"id": "1811.12638", "submitter": "Jyoti Islam", "authors": "Jyoti Islam, Yanqing Zhang", "title": "Towards Robust Lung Segmentation in Chest Radiographs with Deep Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/234", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of Lungs plays a crucial role in the computer-aided\ndiagnosis of chest X-Ray (CXR) images. Developing an efficient Lung\nsegmentation model is challenging because of difficulties such as the presence\nof several edges at the rib cage and clavicle, inconsistent lung shape among\ndifferent individuals, and the appearance of the lung apex. In this paper, we\npropose a robust model for Lung segmentation in Chest Radiographs. Our model\nlearns to ignore the irrelevant regions in an input Chest Radiograph while\nhighlighting regions useful for lung segmentation. The proposed model is\nevaluated on two public chest X-Ray datasets (Montgomery County, MD, USA, and\nShenzhen No. 3 People's Hospital in China). The experimental result with a DICE\nscore of 98.6% demonstrates the robustness of our proposed lung segmentation\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:38:07 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Islam", "Jyoti", ""], ["Zhang", "Yanqing", ""]]}, {"id": "1811.12641", "submitter": "Xingxing Wei", "authors": "Xingxing Wei, Siyuan Liang, Ning Chen, Xiaochun Cao", "title": "Transferable Adversarial Attacks for Image and Video Object Detection", "comments": "IJCAI2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have been demonstrated to threaten many computer vision\ntasks including object detection. However, the existing attacking methods for\nobject detection have two limitations: poor transferability, which denotes that\nthe generated adversarial examples have low success rate to attack other kinds\nof detection methods, and high computation cost, which means that they need\nmore time to generate an adversarial image, and therefore are difficult to deal\nwith the video data. To address these issues, we utilize a generative mechanism\nto obtain the adversarial image and video. In this way, the processing time is\nreduced. To enhance the transferability, we destroy the feature maps extracted\nfrom the feature network, which usually constitutes the basis of object\ndetectors. The proposed method is based on the Generative Adversarial Network\n(GAN) framework, where we combine the high-level class loss and low-level\nfeature loss to jointly train the adversarial example generator. A series of\nexperiments conducted on PASCAL VOC and ImageNet VID datasets show that our\nmethod can efficiently generate image and video adversarial examples, and more\nimportantly, these adversarial examples have better transferability, and thus,\nare able to simultaneously attack two kinds of representative object detection\nmodels: proposal based models like Faster-RCNN, and regression based models\nlike SSD.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:55:22 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 06:46:33 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 02:50:26 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2019 03:13:17 GMT"}, {"version": "v5", "created": "Mon, 13 May 2019 07:22:16 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wei", "Xingxing", ""], ["Liang", "Siyuan", ""], ["Chen", "Ning", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1811.12649", "submitter": "Andrew Zhai", "authors": "Andrew Zhai, Hao-Yu Wu", "title": "Classification is a Strong Baseline for Deep Metric Learning", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims to learn a function mapping image pixels to\nembedding feature vectors that model the similarity between images. Two major\napplications of metric learning are content-based image retrieval and face\nverification. For the retrieval tasks, the majority of current state-of-the-art\n(SOTA) approaches are triplet-based non-parametric training. For the face\nverification tasks, however, recent SOTA approaches have adopted\nclassification-based parametric training. In this paper, we look into the\neffectiveness of classification based approaches on image retrieval datasets.\nWe evaluate on several standard retrieval datasets such as CAR-196,\nCUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval\nand clustering, and establish that our classification-based approach is\ncompetitive across different feature dimensions and base feature networks. We\nfurther provide insights into the performance effects of subsampling classes\nfor scalable classification-based training, and the effects of binarization,\nenabling efficient storage and computation for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 07:21:25 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 18:53:50 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhai", "Andrew", ""], ["Wu", "Hao-Yu", ""]]}, {"id": "1811.12666", "submitter": "Ryota Natsume", "authors": "Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima", "title": "FSNet: An Identity-Aware Generative Model for Image-based Face Swapping", "comments": "20pages, Asian Conference of Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents FSNet, a deep generative model for image-based face\nswapping. Traditionally, face-swapping methods are based on three-dimensional\nmorphable models (3DMMs), and facial textures are replaced between the\nestimated three-dimensional (3D) geometries in two images of different\nindividuals. However, the estimation of 3D geometries along with different\nlighting conditions using 3DMMs is still a difficult task. We herein represent\nthe face region with a latent variable that is assigned with the proposed deep\nneural network (DNN) instead of facial textures. The proposed DNN synthesizes a\nface-swapped image using the latent variable of the face region and another\nimage of the non-face region. The proposed method is not required to fit to the\n3DMM; additionally, it performs face swapping only by feeding two face images\nto the proposed network. Consequently, our DNN-based face swapping performs\nbetter than previous approaches for challenging inputs with different face\norientations and lighting conditions. Through several experiments, we\ndemonstrated that the proposed method performs face swapping in a more stable\nmanner than the state-of-the-art method, and that its results are compatible\nwith the method thereof.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:16:57 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Natsume", "Ryota", ""], ["Yatagawa", "Tatsuya", ""], ["Morishima", "Shigeo", ""]]}, {"id": "1811.12670", "submitter": "Ziwei Liu", "authors": "Weidong Yin, Ziwei Liu, Chen Change Loy", "title": "Instance-level Facial Attributes Transfer with Geometry-Aware Flow", "comments": "To appear in AAAI 2019. Code and models are available at:\n  https://github.com/wdyin/GeoGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of instance-level facial attribute transfer without\npaired training data, e.g. faithfully transferring the exact mustache from a\nsource face to a target face. This is a more challenging task than the\nconventional semantic-level attribute transfer, which only preserves the\ngeneric attribute style instead of instance-level traits. We propose the use of\ngeometry-aware flow, which serves as a well-suited representation for modeling\nthe transformation between instance-level facial attributes. Specifically, we\nleverage the facial landmarks as the geometric guidance to learn the\ndifferentiable flows automatically, despite of the large pose gap existed.\nGeometry-aware flow is able to warp the source face attribute into the target\nface context and generate a warp-and-blend result. To compensate for the\npotential appearance gap between source and target faces, we propose a\nhallucination sub-network that produces an appearance residual to further\nrefine the warp-and-blend result. Finally, a cycle-consistency framework\nconsisting of both attribute transfer module and attribute removal module is\ndesigned, so that abundant unpaired face images can be used as training data.\nExtensive evaluations validate the capability of our approach in transferring\ninstance-level facial attributes faithfully across large pose and appearance\ngaps. Thanks to the flow representation, our approach can readily be applied to\ngenerate realistic details on high-resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:43:00 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yin", "Weidong", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "1811.12673", "submitter": "Xiaojun Jia", "authors": "Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Hassan Foroosh", "title": "ComDefend: An Efficient Image Compression Model to Defend Adversarial\n  Examples", "comments": null, "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been demonstrated to be vulnerable to\nadversarial examples. Specifically, adding imperceptible perturbations to clean\nimages can fool the well trained deep neural networks. In this paper, we\npropose an end-to-end image compression model to defend adversarial examples:\n\\textbf{ComDefend}. The proposed model consists of a compression convolutional\nneural network (ComCNN) and a reconstruction convolutional neural network\n(ResCNN). The ComCNN is used to maintain the structure information of the\noriginal image and purify adversarial perturbations. And the ResCNN is used to\nreconstruct the original image with high quality. In other words, ComDefend can\ntransform the adversarial image to its clean version, which is then fed to the\ntrained classifier. Our method is a pre-processing module, and does not modify\nthe classifier's structure during the whole process. Therefore, it can be\ncombined with other model-specific defense models to jointly improve the\nclassifier's robustness. A series of experiments conducted on MNIST, CIFAR10\nand ImageNet show that the proposed method outperforms the state-of-the-art\ndefense methods, and is consistently effective to protect classifiers against\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:56:30 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 07:06:56 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 13:04:25 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Jia", "Xiaojun", ""], ["Wei", "Xingxing", ""], ["Cao", "Xiaochun", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1811.12693", "submitter": "Oliver Joseph David Barrowclough", "authors": "Konstantinos Gavriil, Georg Muntingh and Oliver J. D. Barrowclough", "title": "Void Filling of Digital Elevation Models with Deep Generative Models", "comments": "5 pages; 4 figures; corrected names in references; clarifications\n  regarding the two generators in the paper; added reference (Borji 2018) on\n  GAN evaluation measures; extended future work discussion; changed (Fig. 4.f)\n  to show a failure case", "journal-ref": null, "doi": "10.1109/LGRS.2019.2902222", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in machine learning algorithms, cheap computational\nresources, and the availability of big data have spurred the deep learning\nrevolution in various application domains. In particular, supervised learning\ntechniques in image analysis have led to superhuman performance in various\ntasks, such as classification, localization, and segmentation, while\nunsupervised learning techniques based on increasingly advanced generative\nmodels have been applied to generate high-resolution synthetic images\nindistinguishable from real images.\n  In this paper we consider a state-of-the-art machine learning model for image\ninpainting, namely a Wasserstein Generative Adversarial Network based on a\nfully convolutional architecture with a contextual attention mechanism. We show\nthat this model can successfully be transferred to the setting of digital\nelevation models (DEMs) for the purpose of generating semantically plausible\ndata for filling voids. Training, testing and experimentation is done on\nGeoTIFF data from various regions in Norway, made openly available by the\nNorwegian Mapping Authority.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:04:30 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 13:24:22 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gavriil", "Konstantinos", ""], ["Muntingh", "Georg", ""], ["Barrowclough", "Oliver J. D.", ""]]}, {"id": "1811.12695", "submitter": "Kashif Nazir", "authors": "Atif Nazir, Kashif Nazir", "title": "An Efficient Image Retrieval Based on Fusion of Low-Level Visual\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to an increase in the number of image achieves, Content-Based Image\nRetrieval (CBIR) has gained attention for research community of computer\nvision. The image visual contents are represented in a feature space in the\nform of numerical values that is considered as a feature vector of image.\nImages belonging to different classes may contain the common visuals and shapes\nthat can result in the closeness of computed feature space of two different\nimages belonging to separate classes. Due to this reason, feature extraction\nand image representation is selected with appropriate features as it directly\naffects the performance of image retrieval system. The commonly used visual\nfeatures are image spatial layout, color, texture and shape. Image feature\nspace is combined to achieve the discriminating ability that is not possible to\nachieve when the features are used separately. Due to this reason, in this\npaper, we aim to explore the low-level feature combination that are based on\ncolor and shape features. We selected color moments and color histogram to\nrepresent color while shape is represented by using invariant moments. We\nselected this combination, as these features are reported intuitive, compact\nand robust for image representation. We evaluated the performance of our\nproposed research by using the Corel, Coil and Ground Truth (GT) image\ndatasets. We evaluated the proposed low-level feature fusion by calculating the\nprecision, recall and time required for feature extraction. The precision,\nrecall and feature extraction values obtained from the proposed low-level\nfeature fusion outperforms the existing research of CBIR.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:11:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Nazir", "Atif", ""], ["Nazir", "Kashif", ""]]}, {"id": "1811.12704", "submitter": "Nikolaos Passalis", "authors": "Paraskevas Pegios, Nikolaos Passalis and Anastasios Tefas", "title": "Style Decomposition for Improved Neural Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Neural Style Transfer (NST) methods are capable of performing style\ntransfer of arbitrary styles in a style-agnostic manner via feature transforms\nin (almost) real-time. Even though their unimodal parametric style modeling\napproach has been proven adequate to transfer a single style from relatively\nsimple images, they are usually not capable of effectively handling more\ncomplex styles, producing significant artifacts, as well as reducing the\nquality of the synthesized textures in the stylized image. To overcome these\nlimitations, in this paper we propose a novel universal NST approach that\nseparately models each sub-style that exists in a given style image (or a\ncollection of style images). This allows for better modeling the subtle style\ndifferences within the same style image and then using the most appropriate\nsub-style (or mixtures of different sub-styles) to stylize the content image.\nThe ability of the proposed approach to a) perform a wide range of different\nstylizations using the sub-styles that exist in one style image, while giving\nthe ability to the user to appropriate mix the different sub-styles, b)\nautomatically match the most appropriate sub-style to different semantic\nregions of the content image, improving existing state-of-the-art universal NST\napproaches, and c) detecting and transferring the sub-styles from collections\nof images are demonstrated through extensive experiments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:33:34 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Pegios", "Paraskevas", ""], ["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1811.12709", "submitter": "Jishnu Mukhoti", "authors": "Jishnu Mukhoti, Yarin Gal", "title": "Evaluating Bayesian Deep Learning Methods for Semantic Segmentation", "comments": "Updated baselines and numbers on concrete dropout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been revolutionary for computer vision and semantic\nsegmentation in particular, with Bayesian Deep Learning (BDL) used to obtain\nuncertainty maps from deep models when predicting semantic classes. This\ninformation is critical when using semantic segmentation for autonomous driving\nfor example. Standard semantic segmentation systems have well-established\nevaluation metrics. However, with BDL's rising popularity in computer vision we\nrequire new metrics to evaluate whether a BDL method produces better\nuncertainty estimates than another method. In this work we propose three such\nmetrics to evaluate BDL models designed specifically for the task of semantic\nsegmentation. We modify DeepLab-v3+, one of the state-of-the-art deep neural\nnetworks, and create its Bayesian counterpart using MC dropout and Concrete\ndropout as inference techniques. We then compare and test these two inference\ntechniques on the well-known Cityscapes dataset using our suggested metrics.\nOur results provide new benchmarks for researchers to compare and evaluate\ntheir improved uncertainty quantification in pursuit of safer semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:41:31 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 17:00:28 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mukhoti", "Jishnu", ""], ["Gal", "Yarin", ""]]}, {"id": "1811.12748", "submitter": "Akash Kumar", "authors": "Akash Kumar, Sagnik Bhowmick, N. Jayanthi and S. Indu", "title": "Improving Landmark Recognition using Saliency detection and Feature\n  classification", "comments": "Pre-print of the paper to be published in Springer, accepted in the\n  proceedings of the in 2nd Workshop on Digital Heritage at the 11th Indian\n  Conference on Computer Vision, Graphics and Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Landmark Recognition has been one of the most sought-after\nclassification challenges in the field of vision and perception. After so many\nyears of generic classification of buildings and monuments from images, people\nare now focussing upon fine-grained problems - recognizing the category of each\nbuilding or monument. We proposed an ensemble network for the purpose of\nclassification of Indian Landmark Images. To this end, our method gives robust\nclassification by ensembling the predictions from Graph-Based Visual Saliency\n(GBVS) network alongwith supervised feature-based classification algorithms\nsuch as kNN and Random Forest. The final architecture is an adaptive learning\nof all the mentioned networks. The proposed network produces a reliable score\nto eliminate false category cases. Evaluation of our model was done on a new\ndataset, which involves challenges such as landmark clutter, variable scaling,\npartial occlusion, etc.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:53:40 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kumar", "Akash", ""], ["Bhowmick", "Sagnik", ""], ["Jayanthi", "N.", ""], ["Indu", "S.", ""]]}, {"id": "1811.12751", "submitter": "Yexun Zhang", "authors": "Yexun Zhang, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Domain-Invariant Adversarial Learning for Unsupervised Domain Adaption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaption aims to learn a powerful classifier for the\ntarget domain given a labeled source data set and an unlabeled target data set.\nTo alleviate the effect of `domain shift', the major challenge in domain\nadaptation, studies have attempted to align the distributions of the two\ndomains. Recent research has suggested that generative adversarial network\n(GAN) has the capability of implicitly capturing data distribution. In this\npaper, we thus propose a simple but effective model for unsupervised domain\nadaption leveraging adversarial learning. The same encoder is shared between\nthe source and target domains which is expected to extract domain-invariant\nrepresentations with the help of an adversarial discriminator. With the labeled\nsource data, we introduce the center loss to increase the discriminative power\nof feature learned. We further align the conditional distribution of the two\ndomains to enforce the discrimination of the features in the target domain.\nUnlike previous studies where the source features are extracted with a fixed\npre-trained encoder, our method jointly learns feature representations of two\ndomains. Moreover, by sharing the encoder, the model does not need to know the\nsource of images during testing and hence is more widely applicable. We\nevaluate the proposed method on several unsupervised domain adaption benchmarks\nand achieve superior or comparable performance to state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:02:45 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhang", "Yexun", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1811.12755", "submitter": "Jiaxin Gu", "authors": "Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang\n  Liu, David Doermann", "title": "Projection Convolutional Neural Networks for 1-bit CNNs via Discrete\n  Back Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of deep convolutional neural networks (DCNNs) has driven\nsignificant improvement in the accuracy of recognition systems for many\ncomputer vision tasks. However, their practical applications are often\nrestricted in resource-constrained environments. In this paper, we introduce\nprojection convolutional neural networks (PCNNs) with a discrete back\npropagation via projection (DBPP) to improve the performance of binarized\nneural networks (BNNs). The contributions of our paper include: 1) for the\nfirst time, the projection function is exploited to efficiently solve the\ndiscrete back propagation problem, which leads to a new highly compressed CNNs\n(termed PCNNs); 2) by exploiting multiple projections, we learn a set of\ndiverse quantized kernels that compress the full-precision kernels in a more\nefficient way than those proposed previously; 3) PCNNs achieve the best\nclassification performance compared to other state-of-the-art BNNs on the\nImageNet and CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:10:21 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 04:46:32 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Gu", "Jiaxin", ""], ["Li", "Ce", ""], ["Zhang", "Baochang", ""], ["Han", "Jungong", ""], ["Cao", "Xianbin", ""], ["Liu", "Jianzhuang", ""], ["Doermann", "David", ""]]}, {"id": "1811.12758", "submitter": "Axel Davy", "authors": "Axel Davy, Thibaud Ehret, Jean-Michel Morel, Pablo Arias, Gabriele\n  Facciolo", "title": "Non-Local Video Denoising by CNN", "comments": "A shorter version of this work has been accepted at ICIP 2019 (A\n  NON-LOCAL CNN FOR VIDEO DENOISING). The results of v2 were improved compared\n  to v1 and the code was updated accordingly. Code is available at:\n  https://github.com/axeldavy/vnlnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local patch based methods were until recently state-of-the-art for image\ndenoising but are now outperformed by CNNs. Yet they are still the\nstate-of-the-art for video denoising, as video redundancy is a key factor to\nattain high denoising performance. The problem is that CNN architectures are\nhardly compatible with the search for self-similarities. In this work we\npropose a new and efficient way to feed video self-similarities to a CNN. The\nnon-locality is incorporated into the network via a first non-trainable layer\nwhich finds for each patch in the input image its most similar patches in a\nsearch region. The central values of these patches are then gathered in a\nfeature vector which is assigned to each image pixel. This information is\npresented to a CNN which is trained to predict the clean image. We apply the\nproposed architecture to image and video denoising. For the latter patches are\nsearched for in a 3D spatio-temporal volume. The proposed architecture achieves\nstate-of-the-art results. To the best of our knowledge, this is the first\nsuccessful application of a CNN to video denoising.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:24:27 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 14:28:56 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Davy", "Axel", ""], ["Ehret", "Thibaud", ""], ["Morel", "Jean-Michel", ""], ["Arias", "Pablo", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "1811.12766", "submitter": "Thibaud Ehret", "authors": "Thibaud Ehret, Axel Davy, Jean-Michel Morel, Gabriele Facciolo, Pablo\n  Arias", "title": "Model-blind Video Denoising Via Frame-to-frame Training", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the processing chain that has produced a video is a difficult\nreverse engineering task, even when the camera is available. This makes model\nbased video processing a still more complex task. In this paper we propose a\nfully blind video denoising method, with two versions off-line and on-line.\nThis is achieved by fine-tuning a pre-trained AWGN denoising network to the\nvideo with a novel frame-to-frame training strategy. Our denoiser can be used\nwithout knowledge of the origin of the video or burst and the post processing\nsteps applied from the camera sensor. The on-line process only requires a\ncouple of frames before achieving visually-pleasing results for a wide range of\nperturbations. It nonetheless reaches state of the art performance for standard\nGaussian noise, and can be used off-line with still better performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:44:50 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 08:44:47 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 15:56:46 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ehret", "Thibaud", ""], ["Davy", "Axel", ""], ["Morel", "Jean-Michel", ""], ["Facciolo", "Gabriele", ""], ["Arias", "Pablo", ""]]}, {"id": "1811.12772", "submitter": "Moshiur R Farazi", "authors": "Moshiur R Farazi, Salman H Khan, Nick Barnes", "title": "From Known to the Unknown: Transferring Knowledge to Answer Questions\n  about Novel Visual and Semantic Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Visual Question Answering (VQA) systems can answer intelligent\nquestions about `Known' visual content. However, their performance drops\nsignificantly when questions about visually and linguistically `Unknown'\nconcepts are presented during inference (`Open-world' scenario). A practical\nVQA system should be able to deal with novel concepts in real world settings.\nTo address this problem, we propose an exemplar-based approach that transfers\nlearning (i.e., knowledge) from previously `Known' concepts to answer questions\nabout the `Unknown'. We learn a highly discriminative joint embedding space,\nwhere visual and semantic features are fused to give a unified representation.\nOnce novel concepts are presented to the model, it looks for the closest match\nfrom an exemplar set in the joint embedding space. This auxiliary information\nis used alongside the given Image-Question pair to refine visual attention in a\nhierarchical fashion. Since handling the high dimensional exemplars on large\ndatasets can be a significant challenge, we introduce an efficient matching\nscheme that uses a compact feature description for search and retrieval. To\nevaluate our model, we propose a new split for VQA, separating Unknown visual\nand semantic concepts from the training set. Our approach shows significant\nimprovements over state-of-the-art VQA models on the proposed Open-World VQA\ndataset and standard VQA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:00:37 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Farazi", "Moshiur R", ""], ["Khan", "Salman H", ""], ["Barnes", "Nick", ""]]}, {"id": "1811.12774", "submitter": "Keyu Yan", "authors": "Keyu Yan (1 and 2) Wenming Zheng (1 and 2), Tong Zhang (1 and 2), Yuan\n  Zong (1) and Zhen Cui (3) ((1) the Key Laboratory of Child Development and\n  Learning Science of Ministry of Education, and the Department of Information\n  Science and Engineering, Southeast University, China. (2) the Key Laboratory\n  of Child Development and Learning Science of Ministry of Education, Research\n  Center for Learning Science, Southeast University, Nanjing, Jiangsu, China.\n  (3) School of Computer Science and Engineering, Nanjing University of Science\n  and Technology, Nanjing, Jiangsu, China)", "title": "Cross-database non-frontal facial expression recognition based on\n  transductive deep transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-database non-frontal expression recognition is a very meaningful but\nrather difficult subject in the fields of computer vision and affect computing.\nIn this paper, we proposed a novel transductive deep transfer learning\narchitecture based on widely used VGGface16-Net for this problem. In this\nframework, the VGGface16-Net is used to jointly learn an common optimal\nnonlinear discriminative features from the non-frontal facial expression\nsamples between the source and target databases and then we design a novel\ntransductive transfer layer to deal with the cross-database non-frontal facial\nexpression classification task. In order to validate the performance of the\nproposed transductive deep transfer learning networks, we present extensive\ncrossdatabase experiments on two famous available facial expression databases,\nnamely the BU-3DEF and the Multi-PIE database. The final experimental results\nshow that our transductive deep transfer network outperforms the\nstate-of-the-art cross-database facial expression recognition methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:09:36 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yan", "Keyu", "", "1 and 2"], ["Zheng", "Wenming", "", "1 and 2"], ["Zhang", "Tong", "", "1 and 2"], ["Zong", "Yuan", ""], ["Cui", "Zhen", ""]]}, {"id": "1811.12781", "submitter": "Hyeji Kim", "authors": "Hyeji Kim, Muhammad Umar Karim Khan, Chong-Min Kyung", "title": "Efficient Neural Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network compression reduces the computational complexity and memory\nconsumption of deep neural networks by reducing the number of parameters. In\nSVD-based network compression, the right rank needs to be decided for every\nlayer of the network. In this paper, we propose an efficient method for\nobtaining the rank configuration of the whole network. Unlike previous methods\nwhich consider each layer separately, our method considers the whole network to\nchoose the right rank configuration. We propose novel accuracy metrics to\nrepresent the accuracy and complexity relationship for a given neural network.\nWe use these metrics in a non-iterative fashion to obtain the right rank\nconfiguration which satisfies the constraints on FLOPs and memory while\nmaintaining sufficient accuracy. Experiments show that our method provides\nbetter compromise between accuracy and computational complexity/memory\nconsumption while performing compression at much higher speed. For VGG-16 our\nnetwork can reduce the FLOPs by 25% and improve accuracy by 0.7% compared to\nthe baseline, while requiring only 3 minutes on a CPU to search for the right\nrank configuration. Previously, similar results were achieved in 4 hours with 8\nGPUs. The proposed method can be used for lossless compression of a neural\nnetwork as well. The better accuracy and complexity compromise, as well as the\nextremely fast speed of our method makes it suitable for neural network\ncompression.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:17:12 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 06:59:15 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 11:53:58 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Kim", "Hyeji", ""], ["Khan", "Muhammad Umar Karim", ""], ["Kyung", "Chong-Min", ""]]}, {"id": "1811.12784", "submitter": "Garoe Dorta", "authors": "Garoe Dorta, Sara Vicente, Neill D.F. Campbell, Ivor J.A. Simpson", "title": "The GAN that Warped: Semantic Attribute Editing with Unpaired Data", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently been used to edit images with great\nsuccess, in particular for faces. However, they are often limited to only being\nable to work at a restricted range of resolutions. Many methods are so flexible\nthat face edits can often result in an unwanted loss of identity. This work\nproposes to learn how to perform semantic image edits through the application\nof smooth warp fields. Previous approaches that attempted to use warping for\nsemantic edits required paired data, i.e. example images of the same subject\nwith different semantic attributes. In contrast, we employ recent advances in\nGenerative Adversarial Networks that allow our model to be trained with\nunpaired data. We demonstrate face editing at very high resolutions (4k images)\nwith a single forward pass of a deep network at a lower resolution. We also\nshow that our edits are substantially better at preserving the subject's\nidentity. The robustness of our approach is demonstrated by showing plausible\nimage editing results on the Cub200 birds dataset. To our knowledge this has\nnot been previously accomplished, due the challenging nature of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:26:38 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 17:40:39 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 17:56:31 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 16:01:39 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Dorta", "Garoe", ""], ["Vicente", "Sara", ""], ["Campbell", "Neill D. F.", ""], ["Simpson", "Ivor J. A.", ""]]}, {"id": "1811.12786", "submitter": "Yixing Zhu", "authors": "Yixing Zhu, Jun Du", "title": "TextMountain: Accurate Scene Text Detection via Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel scene text detection method named\nTextMountain. The key idea of TextMountain is making full use of border-center\ninformation. Different from previous works that treat center-border as a binary\nclassification problem, we predict text center-border probability (TCBP) and\ntext center-direction (TCD). The TCBP is just like a mountain whose top is text\ncenter and foot is text border. The mountaintop can separate text instances\nwhich cannot be easily achieved using semantic segmentation map and its rising\ndirection can plan a road to top for each pixel on mountain foot at the group\nstage. The TCD helps TCBP learning better. Our label rules will not lead to the\nambiguous problem with the transformation of angle, so the proposed method is\nrobust to multi-oriented text and can also handle well with curved text. In\ninference stage, each pixel at the mountain foot needs to search the path to\nthe mountaintop and this process can be efficiently completed in parallel,\nyielding the efficiency of our method compared with others. The experiments on\nMLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 databases demonstrate that the\nproposed method achieves better or comparable performance in terms of both\naccuracy and efficiency. It is worth mentioning our method achieves an\nF-measure of 76.85% on MLT which outperforms the previous methods by a large\nmargin. Code will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:28:41 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhu", "Yixing", ""], ["Du", "Jun", ""]]}, {"id": "1811.12789", "submitter": "Guilherme Aresta", "authors": "Guilherme Aresta, Colin Jacobs, Teresa Ara\\'ujo, Ant\\'onio Cunha,\n  Isabel Ramos, Bram van Ginneken and Aur\\'elio Campilho", "title": "iW-Net: an automatic and minimalistic interactive lung nodule\n  segmentation deep network", "comments": "Pre-print submitted to IEEE Transactions on Biomedical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose iW-Net, a deep learning model that allows for both automatic and\ninteractive segmentation of lung nodules in computed tomography images. iW-Net\nis composed of two blocks: the first one provides an automatic segmentation and\nthe second one allows to correct it by analyzing 2 points introduced by the\nuser in the nodule's boundary. For this purpose, a physics inspired weight map\nthat takes the user input into account is proposed, which is used both as a\nfeature map and in the system's loss function. Our approach is extensively\nevaluated on the public LIDC-IDRI dataset, where we achieve a state-of-the-art\nperformance of 0.55 intersection over union vs the 0.59 inter-observer\nagreement. Also, we show that iW-Net allows to correct the segmentation of\nsmall nodules, essential for proper patient referral decision, as well as\nimprove the segmentation of the challenging non-solid nodules and thus may be\nan important tool for increasing the early diagnosis of lung cancer.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:43:03 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Aresta", "Guilherme", ""], ["Jacobs", "Colin", ""], ["Ara\u00fajo", "Teresa", ""], ["Cunha", "Ant\u00f3nio", ""], ["Ramos", "Isabel", ""], ["van Ginneken", "Bram", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1811.12797", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Structure and Motion from Multiframes", "comments": "7 figures, 20 pages", "journal-ref": "M.A. K{\\l}opotek: Structure and Motion from Multiframes. Machine\n  Graphics and Vision , Vol. 7, nos 1/2, 1998,pp. 383-396", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper gives an overview of the problems and methods of recovery of\nstructure and motion parameters of rigid bodies from multiframes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:05:37 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1811.12813", "submitter": "Mohammad Imrul Jubair", "authors": "Oishee Bintey Hoque, Mohammad Imrul Jubair, Md. Saiful Islam,\n  Al-Farabi Akash, Alvin Sachie Paulson", "title": "Real Time Bangladeshi Sign Language Detection using Faster R-CNN", "comments": "6 pages, Accepted in International Conference on Innovation in\n  Engineering and Technology (ICIET) 27-29 December, 2018, Dhaka, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bangladeshi Sign Language (BdSL) is a commonly used medium of communication\nfor the hearing-impaired people in Bangladesh. Developing a real time system to\ndetect these signs from images is a great challenge. In this paper, we present\na technique to detect BdSL from images that performs in real time. Our method\nuses Convolutional Neural Network based object detection technique to detect\nthe presence of signs in the image region and to recognize its class. For this\npurpose, we adopted Faster Region-based Convolutional Network approach and\ndeveloped a dataset $-$ BdSLImset $-$ to train our system. Previous research\nworks in detecting BdSL generally depend on external devices while most of the\nother vision-based techniques do not perform efficiently in real time. Our\napproach, however, is free from such limitations and the experimental results\ndemonstrate that the proposed method successfully identifies and recognizes\nBangladeshi signs in real time.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:25:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hoque", "Oishee Bintey", ""], ["Jubair", "Mohammad Imrul", ""], ["Islam", "Md. Saiful", ""], ["Akash", "Al-Farabi", ""], ["Paulson", "Alvin Sachie", ""]]}, {"id": "1811.12814", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, Jiashi\n  Feng, Yannis Kalantidis", "title": "Graph-Based Global Reasoning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally modeling and reasoning over relations between regions can be\nbeneficial for many computer vision tasks on both images and videos.\nConvolutional Neural Networks (CNNs) excel at modeling local relations by\nconvolution operations, but they are typically inefficient at capturing global\nrelations between distant regions and require stacking multiple convolution\nlayers. In this work, we propose a new approach for reasoning globally in which\na set of features are globally aggregated over the coordinate space and then\nprojected to an interaction space where relational reasoning can be efficiently\ncomputed. After reasoning, relation-aware features are distributed back to the\noriginal coordinate space for down-stream tasks. We further present a highly\nefficient instantiation of the proposed approach and introduce the Global\nReasoning unit (GloRe unit) that implements the coordinate-interaction space\nmapping by weighted global pooling and weighted broadcasting, and the relation\nreasoning via graph convolution on a small graph in interaction space. The\nproposed GloRe unit is lightweight, end-to-end trainable and can be easily\nplugged into existing CNNs for a wide range of tasks. Extensive experiments\nshow our GloRe unit can consistently boost the performance of state-of-the-art\nbackbone architectures, including ResNet, ResNeXt, SE-Net and DPN, for both 2D\nand 3D CNNs, on image classification, semantic segmentation and video action\nrecognition task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:28:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Yunpeng", ""], ["Rohrbach", "Marcus", ""], ["Yan", "Zhicheng", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""], ["Kalantidis", "Yannis", ""]]}, {"id": "1811.12817", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte,\n  Luc Van Gool", "title": "Practical Full Resolution Learned Lossless Image Compression", "comments": "Updated preprocessing and Table 1, see A.1 in supplementary. Code and\n  models: https://github.com/fab-jul/L3C-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first practical learned lossless image compression system,\nL3C, and show that it outperforms the popular engineered codecs, PNG, WebP and\nJPEG 2000. At the core of our method is a fully parallelizable hierarchical\nprobabilistic model for adaptive entropy coding which is optimized end-to-end\nfor the compression task. In contrast to recent autoregressive discrete\nprobabilistic models such as PixelCNN, our method i) models the image\ndistribution jointly with learned auxiliary representations instead of\nexclusively modeling the image distribution in RGB space, and ii) only requires\nthree forward-passes to predict all pixel probabilities instead of one for each\npixel. As a result, L3C obtains over two orders of magnitude speedups when\nsampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN).\nFurthermore, we find that learning the auxiliary representation is crucial and\noutperforms predefined auxiliary representations such as an RGB pyramid\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:32:47 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 13:24:44 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:57:56 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mentzer", "Fabian", ""], ["Agustsson", "Eirikur", ""], ["Tschannen", "Michael", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1811.12833", "submitter": "Tuan-Hung Vu", "authors": "Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick\n  P\\'erez", "title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in\n  Semantic Segmentation", "comments": "Accepted in CVPR'19. Code is available at\n  https://github.com/valeoai/ADVENT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a key problem for many computer vision tasks. While\napproaches based on convolutional neural networks constantly break new records\non different benchmarks, generalizing well to diverse testing environments\nremains a major challenge. In numerous real world applications, there is indeed\na large gap between data distributions in train and test domains, which results\nin severe performance loss at run-time. In this work, we address the task of\nunsupervised domain adaptation in semantic segmentation with losses based on\nthe entropy of the pixel-wise predictions. To this end, we propose two novel,\ncomplementary methods using (i) entropy loss and (ii) adversarial loss\nrespectively. We demonstrate state-of-the-art performance in semantic\nsegmentation on two challenging \"synthetic-2-real\" set-ups and show that the\napproach can also be used for detection.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 15:00:29 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 19:16:22 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Vu", "Tuan-Hung", ""], ["Jain", "Himalaya", ""], ["Bucher", "Maxime", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1811.12866", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "Super-Resolution via Image-Adapted Denoising CNNs: Incorporating\n  External and Internal Learning", "comments": "Accepted to IEEE Signal Processing Letters (extended version)", "journal-ref": null, "doi": "10.1109/LSP.2019.2920250", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks exhibit state-of-the-art results in the task of\nimage super-resolution (SR) with a fixed known acquisition process (e.g., a\nbicubic downscaling kernel), they experience a huge performance loss when the\nreal observation model mismatches the one used in training. Recently, two\ndifferent techniques suggested to mitigate this deficiency, i.e., enjoy the\nadvantages of deep learning without being restricted by the training phase. The\nfirst one follows the plug-and-play (P&P) approach that solves general inverse\nproblems (e.g., SR) by using Gaussian denoisers for handling the prior term in\nmodel-based optimization schemes. The second builds on internal recurrence of\ninformation inside a single image, and trains a super-resolver network at test\ntime on examples synthesized from the low-resolution image. Our work\nincorporates these two independent strategies, enjoying the impressive\ngeneralization capabilities of deep learning, captured by the first, and\nfurther improving it through internal learning at test time. First, we apply a\nrecent P&P strategy to SR. Then, we show how it may become image-adaptive in\ntest time. This technique outperforms the above two strategies on popular\ndatasets and gives better results than other state-of-the-art methods in\npractical cases where the observation model is inexact or unknown in advance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 16:15:19 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 23:27:53 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 10:50:58 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}]