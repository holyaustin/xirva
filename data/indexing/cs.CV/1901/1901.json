[{"id": "1901.00001", "submitter": "Yuri G. Gordienko", "authors": "Vlad Taran, Yuri Gordienko, Alexandr Rokovyi, Oleg Alienin, Sergii\n  Stirenko", "title": "Impact of Ground Truth Annotation Quality on Performance of Semantic\n  Image Segmentation of Traffic Conditions", "comments": "10 pages, 6 figures, 2 tables, The Second International Conference on\n  Computer Science, Engineering and Education Applications (ICCSEEA2019) 26-27\n  January 2019, Kiev, Ukraine", "journal-ref": "Hu Z., Petoukhov S., Dychka I., He M. (eds) Advances in Computer\n  Science for Engineering and Education II. ICCSEEA 2019. Advances in\n  Intelligent Systems and Computing, vol 938 (pp.183-193). Springer, Cham", "doi": "10.1007/978-3-030-16621-2_17", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preparation of high-quality datasets for the urban scene understanding is a\nlabor-intensive task, especially, for datasets designed for the autonomous\ndriving applications. The application of the coarse ground truth (GT)\nannotations of these datasets without detriment to the accuracy of semantic\nimage segmentation (by the mean intersection over union - mIoU) could simplify\nand speedup the dataset preparation and model fine tuning before its practical\napplication. Here the results of the comparative analysis for semantic\nsegmentation accuracy obtained by PSPNet deep learning architecture are\npresented for fine and coarse annotated images from Cityscapes dataset. Two\nscenarios were investigated: scenario 1 - the fine GT images for training and\nprediction, and scenario 2 - the fine GT images for training and the coarse GT\nimages for prediction. The obtained results demonstrated that for the most\nimportant classes the mean accuracy values of semantic image segmentation for\ncoarse GT annotations are higher than for the fine GT ones, and the standard\ndeviation values are vice versa. It means that for some applications some\nunimportant classes can be excluded and the model can be tuned further for some\nclasses and specific regions on the coarse GT dataset without loss of the\naccuracy even. Moreover, this opens the perspectives to use deep neural\nnetworks for the preparation of such coarse GT datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 09:19:55 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Taran", "Vlad", ""], ["Gordienko", "Yuri", ""], ["Rokovyi", "Alexandr", ""], ["Alienin", "Oleg", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1901.00003", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung, Ricson Cheng, Katerina Fragkiadaki", "title": "Learning Spatial Common Sense with Geometry-Aware Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We integrate two powerful ideas, geometry and deep visual representation\nlearning, into recurrent network architectures for mobile visual scene\nunderstanding. The proposed networks learn to \"lift\" and integrate 2D visual\nfeatures over time into latent 3D feature maps of the scene. They are equipped\nwith differentiable geometric operations, such as projection, unprojection,\negomotion estimation and stabilization, in order to compute a\ngeometrically-consistent mapping between the world scene and their 3D latent\nfeature state. We train the proposed architectures to predict novel camera\nviews given short frame sequences as input. Their predictions strongly\ngeneralize to scenes with a novel number of objects, appearances and\nconfigurations; they greatly outperform previous works that do not consider\negomotion stabilization or a space-aware latent feature state. We train the\nproposed architectures to detect and segment objects in 3D using the latent 3D\nfeature map as input--as opposed to per frame features. The resulting object\ndetections persist over time: they continue to exist even when an object gets\noccluded or leaves the field of view. Our experiments suggest the proposed\nspace-aware latent feature memory and egomotion-stabilized convolutions are\nessential architectural choices for spatial common sense to emerge in\nartificial embodied visual agents.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:37:18 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 16:46:52 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 00:39:11 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Cheng", "Ricson", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1901.00027", "submitter": "Zhenwei Miao", "authors": "Zhenwei Miao, Kim-Hui Yap, Xudong Jiang, Subbhuraam Sinduja, Zhenhua\n  Wang", "title": "DCI: Discriminative and Contrast Invertible Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature descriptors have been widely used in fine-grained visual object\nsearch thanks to their robustness in scale and rotation variation and cluttered\nbackground. However, the performance of such descriptors drops under severe\nillumination changes. In this paper, we proposed a Discriminative and Contrast\nInvertible (DCI) local feature descriptor. In order to increase the\ndiscriminative ability of the descriptor under illumination changes, a Laplace\ngradient based histogram is proposed. A robust contrast flipping estimate is\nproposed based on the divergence of a local region. Experiments on fine-grained\nobject recognition and retrieval applications demonstrate the superior\nperformance of DCI descriptor to others.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 19:32:18 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Miao", "Zhenwei", ""], ["Yap", "Kim-Hui", ""], ["Jiang", "Xudong", ""], ["Sinduja", "Subbhuraam", ""], ["Wang", "Zhenhua", ""]]}, {"id": "1901.00031", "submitter": "Zhenwei Miao", "authors": "Zhenwei Miao, Kim-Hui Yap, Xudong Jiang", "title": "Interest Point Detection based on Adaptive Ternary Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an adaptive pixel ternary coding mechanism is proposed and a\ncontrast invariant and noise resistant interest point detector is developed on\nthe basis of this mechanism. Every pixel in a local region is adaptively\nencoded into one of the three statuses: bright, uncertain and dark. The blob\nsignificance of the local region is measured by the spatial distribution of the\nbright and dark pixels. Interest points are extracted from this blob\nsignificance measurement. By labeling the statuses of ternary bright,\nuncertain, and dark, the proposed detector shows more robustness to image noise\nand quantization errors. Moreover, the adaptive strategy for the ternary\ncording, which relies on two thresholds that automatically converge to the\nmedian of the local region in measurement, enables this coding to be\ninsensitive to the image local contrast. As a result, the proposed detector is\ninvariant to illumination changes. The state-of-the-art results are achieved on\nthe standard datasets, and also in the face recognition application.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 20:00:00 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Miao", "Zhenwei", ""], ["Yap", "Kim-Hui", ""], ["Jiang", "Xudong", ""]]}, {"id": "1901.00039", "submitter": "Shengqin Jiang", "authors": "Shengqin Jiang, Xiaobo Lu, Yinjie Lei, Lingqiao Liu", "title": "Mask-aware networks for crowd counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting problem aims to count the number of objects within an image or\na frame in the videos and is usually solved by estimating the density map\ngenerated from the object location annotations. The values in the density map,\nby nature, take two possible states: zero indicating no object around, a\nnon-zero value indicating the existence of objects and the value denoting the\nlocal object density. In contrast to traditional methods which do not\ndifferentiate the density prediction of these two states, we propose to use a\ndedicated network branch to predict the object/non-object mask and then combine\nits prediction with the input image to produce the density map. Our rationale\nis that the mask prediction could be better modeled as a binary segmentation\nproblem and the difficulty of estimating the density could be reduced if the\nmask is known. A key to the proposed scheme is the strategy of incorporating\nthe mask prediction into the density map estimator. To this end, we study five\npossible solutions, and via analysis and experimental validation we identify\nthe most effective one. Through extensive experiments on five public datasets,\nwe demonstrate the superior performance of the proposed approach over the\nbaselines and show that our network could achieve the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 03:32:42 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 02:20:04 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Jiang", "Shengqin", ""], ["Lu", "Xiaobo", ""], ["Lei", "Yinjie", ""], ["Liu", "Lingqiao", ""]]}, {"id": "1901.00040", "submitter": "Alireza Sedghi", "authors": "Alireza Sedghi, Jie Luo, Alireza Mehrtash, Steve Pieper, Clare M.\n  Tempany, Tina Kapur, Parvin Mousavi, William M. Wells III", "title": "Deep Information Theoretic Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes an information theoretic framework for deep metric\nbased image registration techniques. We show an exact equivalence between\nmaximum profile likelihood and minimization of joint entropy, an important\nearly information theoretic registration method. We further derive deep\nclassifier-based metrics that can be used with iterated maximum likelihood to\nachieve Deep Information Theoretic Registration on patches rather than pixels.\nThis alleviates a major shortcoming of previous information theoretic\nregistration approaches, namely the implicit pixel-wise independence\nassumptions. Our proposed approach does not require well-registered training\ndata; this brings previous fully supervised deep metric registration approaches\nto the realm of weak supervision. We evaluate our approach on several image\nregistration tasks and show significantly better performance compared to mutual\ninformation, specifically when images have substantially different contrasts.\nThis work enables general-purpose registration in applications where current\nmethods are not successful.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 20:45:45 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Sedghi", "Alireza", ""], ["Luo", "Jie", ""], ["Mehrtash", "Alireza", ""], ["Pieper", "Steve", ""], ["Tempany", "Clare M.", ""], ["Kapur", "Tina", ""], ["Mousavi", "Parvin", ""], ["Wells", "William M.", "III"]]}, {"id": "1901.00049", "submitter": "Ryota Natsume", "authors": "Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma,\n  Hao Li, Shigeo Morishima", "title": "SiCloPe: Silhouette-Based Clothed People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 21:14:44 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 11:30:39 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Natsume", "Ryota", ""], ["Saito", "Shunsuke", ""], ["Huang", "Zeng", ""], ["Chen", "Weikai", ""], ["Ma", "Chongyang", ""], ["Li", "Hao", ""], ["Morishima", "Shigeo", ""]]}, {"id": "1901.00054", "submitter": "Long Zhang", "authors": "Long Zhang, Xuechao Sun, Yong Li and Zhenyu Zhang", "title": "A Noise-Sensitivity-Analysis-Based Test Prioritization Technique for\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been widely used in the fields such as\nnatural language processing, computer vision and image recognition. But several\nstudies have been shown that deep neural networks can be easily fooled by\nartificial examples with some perturbations, which are widely known as\nadversarial examples. Adversarial examples can be used to attack deep neural\nnetworks or to improve the robustness of deep neural networks. A common way of\ngenerating adversarial examples is to first generate some noises and then add\nthem into original examples. In practice, different examples have different\nnoise-sensitive. To generate an effective adversarial example, it may be\nnecessary to add a lot of noise to low noise-sensitive example, which may make\nthe adversarial example meaningless. In this paper, we propose a\nnoise-sensitivity-analysis-based test prioritization technique to pick out\nexamples by their noise sensitivity. We construct an experiment to validate our\napproach on four image sets and two DNN models, which shows that examples are\nsensitive to noise and our method can effectively pick out examples by their\nnoise sensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 06:42:59 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 08:07:57 GMT"}, {"version": "v3", "created": "Sun, 20 Jan 2019 02:12:25 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zhang", "Long", ""], ["Sun", "Xuechao", ""], ["Li", "Yong", ""], ["Zhang", "Zhenyu", ""]]}, {"id": "1901.00055", "submitter": "Hao Xiong", "authors": "Hao Xiong, Chaoyue Wang, Dacheng Tao, Michael Barnett, Chenyu Wang", "title": "Multiple Sclerosis Lesion Inpainting Using Non-Local Partial\n  Convolutions", "comments": "We make significant changes to the paper and do not plan to submit\n  current version to arXiv until it is accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sclerosis (MS) is an inflammatory demyelinating disease of the\ncentral nervous system (CNS) that results in focal injury to the grey and white\nmatter. The presence of white matter lesions biases morphometric analyses such\nas registration, individual longitudinal measurements and tissue segmentation\nfor brain volume measurements. Lesion-inpainting with intensities derived from\nsurrounding healthy tissue represents one approach to alleviate such problems.\nHowever, existing methods inpaint lesions based on texture information derived\nfrom local surrounding tissue, often leading to inconsistent inpainting and the\ngeneration of artifacts such as intensity discrepancy and blurriness. Based on\nthese observations, we propose non-local partial convolutions (NLPC) that\nintegrates a Unet-like network with the non-local module. The non-local module\nis exploited to capture long range dependencies between the lesion area and\nremaining normal-appearing brain regions. Then, the lesion area is filled by\nreferring to normal-appearing regions with more similar features. This method\ngenerates inpainted regions that appear more realistic and natural. Our\nquantitative experimental results also demonstrate superiority of this\ntechnique of existing state-of-the-art inpainting methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 12:56:56 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 10:53:38 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 07:24:41 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Xiong", "Hao", ""], ["Wang", "Chaoyue", ""], ["Tao", "Dacheng", ""], ["Barnett", "Michael", ""], ["Wang", "Chenyu", ""]]}, {"id": "1901.00062", "submitter": "Hyomin Choi", "authors": "Hyomin Choi and Ivan V. Bajic", "title": "Deep Frame Prediction for Video Coding", "comments": "This paper is accepted by IEEE Transactions on Circuits and Systems\n  for Video Technology in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel frame prediction method using a deep neural network (DNN),\nwith the goal of improving video coding efficiency. The proposed DNN makes use\nof decoded frames, at both encoder and decoder, to predict textures of the\ncurrent coding block. Unlike conventional inter-prediction, the proposed method\ndoes not require any motion information to be transferred between the encoder\nand the decoder. Still, both uni-directional and bi-directional prediction are\npossible using the proposed DNN, which is enabled by the use of the temporal\nindex channel, in addition to color channels. In this study, we developed a\njointly trained DNN for both uni- and bi- directional prediction, as well as\nseparate networks for uni- and bi-directional prediction, and compared the\nefficacy of both approaches. The proposed DNNs were compared with the\nconventional motion-compensated prediction in the latest video coding standard,\nHEVC, in terms of BD-Bitrate. The experiments show that the proposed joint DNN\n(for both uni- and bi-directional prediction) reduces the luminance bitrate by\nabout 4.4%, 2.4%, and 2.3% in the Low delay P, Low delay, and Random access\nconfigurations, respectively. In addition, using the separately trained DNNs\nbrings further bit savings of about 0.3%-0.5%.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 23:41:50 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 17:19:47 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 22:21:51 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1901.00063", "submitter": "Zhenpei Yang", "authors": "Zhenpei Yang, Jeffrey Z.Pan, Linjie Luo, Xiaowei Zhou, Kristen\n  Grauman, Qixing Huang", "title": "Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion", "comments": "fixed issues with bibtex file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the relative rigid pose between two RGB-D scans of the same\nunderlying environment is a fundamental problem in computer vision, robotics,\nand computer graphics. Most existing approaches allow only limited maximum\nrelative pose changes since they require considerable overlap between the input\nscans. We introduce a novel deep neural network that extends the scope to\nextreme relative poses, with little or even no overlap between the input scans.\nThe key idea is to infer more complete scene information about the underlying\nenvironment and match on the completed scans. In particular, instead of only\nperforming scene completion from each individual scan, our approach alternates\nbetween relative pose estimation and scene completion. This allows us to\nperform scene completion by utilizing information from both input scans at late\niterations, resulting in better results for both scene completion and relative\npose estimation. Experimental results on benchmark datasets show that our\napproach leads to considerable improvements over state-of-the-art approaches\nfor relative pose estimation. In particular, our approach provides encouraging\nrelative pose estimates even between non-overlapping scans.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 23:43:16 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 22:33:42 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Yang", "Zhenpei", ""], ["Pan", "Jeffrey Z.", ""], ["Luo", "Linjie", ""], ["Zhou", "Xiaowei", ""], ["Grauman", "Kristen", ""], ["Huang", "Qixing", ""]]}, {"id": "1901.00097", "submitter": "Jiarong Dong", "authors": "Jiarong Dong and Ke Gao and Xiaokai Chen and Junbo Guo and Juan Cao\n  and Yongdong Zhang", "title": "Not All Words are Equal: Video-specific Information Loss for Video\n  Captioning", "comments": "BMVC2018 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ideal description for a given video should fix its gaze on salient and\nrepresentative content, which is capable of distinguishing this video from\nothers. However, the distribution of different words is unbalanced in video\ncaptioning datasets, where distinctive words for describing video-specific\nsalient objects are far less than common words such as 'a' 'the' and 'person'.\nThe dataset bias often results in recognition error or detail deficiency of\nsalient but unusual objects. To address this issue, we propose a novel learning\nstrategy called Information Loss, which focuses on the relationship between the\nvideo-specific visual content and corresponding representative words. Moreover,\na framework with hierarchical visual representations and an optimized\nhierarchical attention mechanism is established to capture the most salient\nspatial-temporal visual information, which fully exploits the potential\nstrength of the proposed learning strategy. Extensive experiments demonstrate\nthat the ingenious guidance strategy together with the optimized architecture\noutperforms state-of-the-art video captioning methods on MSVD with CIDEr score\n87.5, and achieves superior CIDEr score 47.7 on MSR-VTT. We also show that our\nInformation Loss is generic which improves various models by significant\nmargins.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 05:19:02 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Dong", "Jiarong", ""], ["Gao", "Ke", ""], ["Chen", "Xiaokai", ""], ["Guo", "Junbo", ""], ["Cao", "Juan", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1901.00098", "submitter": "Jonghyun Choi", "authors": "Tae-hoon Kim, Dongmin Kang, Kari Pulli, Jonghyun Choi", "title": "Training with the Invisibles: Obfuscating Images to Share Safely for\n  Learning Visual Recognition Models", "comments": "The logical flow and the experimental validations have to be\n  significantly revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-performance visual recognition systems generally require a large\ncollection of labeled images to train. The expensive data curation can be an\nobstacle for improving recognition performance. Sharing more data allows\ntraining for better models. But personal and private information in the data\nprevent such sharing. To promote sharing visual data for learning a recognition\nmodel, we propose to obfuscate the images so that humans are not able to\nrecognize their detailed contents, while machines can still utilize them to\ntrain new models. We validate our approach by comprehensive experiments on\nthree challenging visual recognition tasks; image classification, attribute\nclassification, and facial landmark detection on several datasets including\nSVHN, CIFAR10, Pascal VOC 2012, CelebA, and MTFL. Our method successfully\nobfuscates the images from humans recognition, but a machine model trained with\nthem performs within about 1% margin (up to 0.48%) of the performance of a\nmodel trained with the original, non-obfuscated data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 05:39:05 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 15:12:43 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Kim", "Tae-hoon", ""], ["Kang", "Dongmin", ""], ["Pulli", "Kari", ""], ["Choi", "Jonghyun", ""]]}, {"id": "1901.00109", "submitter": "Ranjan Mondal", "authors": "Ranjan Mondal, Soumendu Sundar Mukherjee, Sanchayan Santra and\n  Bhabatosh Chanda", "title": "Morphological Network: How Far Can We Go with Morphological Neurons?", "comments": "35 pages, 19 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the idea of using morphological operations as networks has\nreceived much attention. Mathematical morphology provides very efficient and\nuseful image processing and image analysis tools based on basic operators like\ndilation and erosion, defined in terms of kernels. Many other morphological\noperations are built up using the dilation and erosion operations. Although the\nlearning of structuring elements such as dilation or erosion using the\nbackpropagation algorithm is not new, the order and the way these morphological\noperations are used is not standard. In this paper, we have theoretically\nanalyzed the use of morphological operations for processing 1D feature vectors\nand shown that this gets extended to the 2D case in a simple manner. Our\ntheoretical results show that a morphological block represents a sum of hinge\nfunctions. Hinge functions are used in many places for classification and\nregression tasks (Breiman (1993)). We have also proved a universal\napproximation theorem -- a stack of two morphological blocks can approximate\nany continuous function over arbitrary compact sets. To experimentally validate\nthe efficacy of this network in real-life applications, we have evaluated its\nperformance on satellite image classification datasets since morphological\noperations are very sensitive to geometrical shapes and structures. We have\nalso shown results on a few tasks like segmentation of blood vessels from\nfundus images, segmentation of lungs from chest x-ray and image dehazing. The\nresults are encouraging and further establishes the potential of morphological\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 07:52:24 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 19:19:40 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 22:21:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mondal", "Ranjan", ""], ["Mukherjee", "Soumendu Sundar", ""], ["Santra", "Sanchayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "1901.00120", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Hwee Kuan Lee, Maxine Tan", "title": "Gated-Dilated Networks for Lung Nodule Classification in CT scans", "comments": "Published in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2958663", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different types of Convolutional Neural Networks (CNNs) have been applied to\ndetect cancerous lung nodules from computed tomography (CT) scans. However, the\nsize of a nodule is very diverse and can range anywhere between 3 and 30\nmillimeters. The high variation of nodule sizes makes classifying them a\ndifficult and challenging task. In this study, we propose a novel CNN\narchitecture called Gated-Dilated (GD) networks to classify nodules as\nmalignant or benign. Unlike previous studies, the GD network uses multiple\ndilated convolutions instead of max-poolings to capture the scale variations.\nMoreover, the GD network has a Context-Aware sub-network that analyzes the\ninput features and guides the features to a suitable dilated convolution. We\nevaluated the proposed network on more than 1,000 CT scans from the LIDC-LDRI\ndataset. Our proposed network outperforms state-of-the-art baseline models\nincluding Multi-Crop, Resnet, and Densenet, with an AUC of >0.95. Compared to\nthe baseline models, the GD network improves the classification accuracies of\nmid-range sized nodules. Furthermore, we observe a relationship between the\nsize of the nodule and the attention signal generated by the Context-Aware\nsub-network, which validates our new network architecture.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 09:13:17 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 10:47:42 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Lee", "Hwee Kuan", ""], ["Tan", "Maxine", ""]]}, {"id": "1901.00121", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Sadiq M. Sait, and Aiman El-Maleh", "title": "FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review", "comments": "This article has been accepted for publication in IEEE Access\n  (December, 2018)", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2890150", "report-no": null, "categories": "cs.NE cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 09:17:51 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Sait", "Sadiq M.", ""], ["El-Maleh", "Aiman", ""]]}, {"id": "1901.00148", "submitter": "Wenbo Li", "authors": "Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du, Tianzi\n  Xiao, Gang Yu, Hongtao Lu, Yichen Wei, and Jian Sun", "title": "Rethinking on Multi-Stage Networks for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing pose estimation approaches fall into two categories: single-stage\nand multi-stage methods. While multi-stage methods are seemingly more suited\nfor the task, their performance in current practice is not as good as\nsingle-stage methods. This work studies this issue. We argue that the current\nmulti-stage methods' unsatisfactory performance comes from the insufficiency in\nvarious design choices. We propose several improvements, including the\nsingle-stage module design, cross stage feature aggregation, and coarse-to-fine\nsupervision. The resulting method establishes the new state-of-the-art on both\nMS COCO and MPII Human Pose dataset, justifying the effectiveness of a\nmulti-stage architecture. The source code is publicly available for further\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 12:52:37 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 02:58:39 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 13:31:00 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 01:30:32 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Li", "Wenbo", ""], ["Wang", "Zhicheng", ""], ["Yin", "Binyi", ""], ["Peng", "Qixiang", ""], ["Du", "Yuming", ""], ["Xiao", "Tianzi", ""], ["Yu", "Gang", ""], ["Lu", "Hongtao", ""], ["Wei", "Yichen", ""], ["Sun", "Jian", ""]]}, {"id": "1901.00166", "submitter": "Nibaran Das", "authors": "Bodhisatwa Mandal, Suvam Dubey, Swarnendu Ghosh, Ritesh Sarkhel,\n  Nibaran Das", "title": "Handwritten Indic Character Recognition using Capsule Networks", "comments": "Accepted in IEEE Applied Signal Processing Conference 2018(ASPCON\n  2018 ) held on December 7-9, 2018 at Jadavpur University, Kolkata, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks(CNNs) has become one of the primary algorithms\nfor various computer vision tasks. Handwritten character recognition is a\ntypical example of such task that has also attracted attention. CNN\narchitectures such as LeNet and AlexNet have become very prominent over the\nlast two decades however the spatial invariance of the different kernels has\nbeen a prominent issue till now. With the introduction of capsule networks,\nkernels can work together in consensus with one another with the help of\ndynamic routing, that combines individual opinions of multiple groups of\nkernels called capsules to employ equivariance among kernels. In the current\nwork, we have implemented capsule network on handwritten Indic digits and\ncharacter datasets to show its superiority over networks like LeNet.\nFurthermore, it has also been shown that they can boost the performance of\nother networks like LeNet and AlexNet.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 15:33:13 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Mandal", "Bodhisatwa", ""], ["Dubey", "Suvam", ""], ["Ghosh", "Swarnendu", ""], ["Sarkhel", "Ritesh", ""], ["Das", "Nibaran", ""]]}, {"id": "1901.00206", "submitter": "Mehryar Emambakhsh", "authors": "Mehryar Emambakhsh and Adrian Evans", "title": "Nasal Patches and Curves for Expression-robust 3D Face Recognition", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (PAMI), vol. 39, no. 5, pp. 995-1007, 2017", "doi": "10.1109/TPAMI.2016.2565473", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of the nasal region for expression robust 3D face recognition\nis thoroughly investigated by a novel five-step algorithm. First, the nose tip\nlocation is coarsely detected and the face is segmented, aligned and the nasal\nregion cropped. Then, a very accurate and consistent nasal landmarking\nalgorithm detects seven keypoints on the nasal region. In the third step, a\nfeature extraction algorithm based on the surface normals of Gabor-wavelet\nfiltered depth maps is utilised and, then, a set of spherical patches and\ncurves are localised over the nasal region to provide the feature descriptors.\nThe last step applies a genetic algorithm-based feature selector to detect the\nmost stable patches and curves over different facial expressions. The algorithm\nprovides the highest reported nasal region-based recognition ranks on the FRGC,\nBosphorus and BU-3DFE datasets. The results are comparable with, and in many\ncases better than, many state-of-the-art 3D face recognition algorithms, which\nuse the whole facial domain. The proposed method does not rely on sophisticated\nalignment or denoising steps, is very robust when only one sample per subject\nis used in the gallery, and does not require a training step for the\nlandmarking algorithm. https://github.com/mehryaragha/NoseBiometrics\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 20:05:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Emambakhsh", "Mehryar", ""], ["Evans", "Adrian", ""]]}, {"id": "1901.00211", "submitter": "Bashar Alhafni", "authors": "Bashar Alhafni, Saulo Fernando Guedes, Lays Cavalcante Ribeiro, Juhyun\n  Park, Jeongkyu Lee", "title": "Mapping Areas using Computer Vision Algorithms and Drones", "comments": "7 pages, 12 figures. This work was presented at the American Society\n  for Engineering Education (ASEE) Northeast Conference in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to implement a system, titled as Drone Map Creator\n(DMC) using Computer Vision techniques. DMC can process visual information from\nan HD camera in a drone and automatically create a map by stitching together\nvisual information captured by a drone. The proposed approach employs the\nSpeeded up robust features (SURF) method to detect the key points for each\nimage frame; then the corresponding points between the frames are identified by\nmaximizing the determinant of a Hessian matrix. Finally, two images are\nstitched together by using the identified points. Our results show that despite\nsome limitations from the external environment, we could have successfully\nstitched images together along video sequences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 21:24:07 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Alhafni", "Bashar", ""], ["Guedes", "Saulo Fernando", ""], ["Ribeiro", "Lays Cavalcante", ""], ["Park", "Juhyun", ""], ["Lee", "Jeongkyu", ""]]}, {"id": "1901.00212", "submitter": "Kamyar Nazeri", "authors": "Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran\n  Ebrahimi", "title": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning", "comments": "Code and data: https://github.com/knazeri/edge-connect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the last few years, deep learning techniques have yielded significant\nimprovements in image inpainting. However, many of these techniques fail to\nreconstruct reasonable structures as they are commonly over-smoothed and/or\nblurry. This paper develops a new approach for image inpainting that does a\nbetter job of reproducing filled regions exhibiting fine details. We propose a\ntwo-stage adversarial model EdgeConnect that comprises of an edge generator\nfollowed by an image completion network. The edge generator hallucinates edges\nof the missing region (both regular and irregular) of the image, and the image\ncompletion network fills in the missing regions using hallucinated edges as a\npriori. We evaluate our model end-to-end over the publicly available datasets\nCelebA, Places2, and Paris StreetView, and show that it outperforms current\nstate-of-the-art techniques quantitatively and qualitatively. Code and models\navailable at: https://github.com/knazeri/edge-connect\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 21:38:40 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 16:39:19 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 18:31:31 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Nazeri", "Kamyar", ""], ["Ng", "Eric", ""], ["Joseph", "Tony", ""], ["Qureshi", "Faisal Z.", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "1901.00224", "submitter": "Tingting Qiao", "authors": "Tingting Qiao, Weijing Zhang, Miao Zhang, Zixuan Ma, Duanqing Xu", "title": "Ancient Painting to Natural Image: A New Solution for Painting\n  Processing", "comments": "10 pages, 6 figures, published in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting a large-scale and well-annotated dataset for image processing has\nbecome a common practice in computer vision. However, in the ancient painting\narea, this task is not practical as the number of paintings is limited and\ntheir style is greatly diverse. We, therefore, propose a novel solution for the\nproblems that come with ancient painting processing. This is to use domain\ntransfer to convert ancient paintings to photo-realistic natural images. By\ndoing so, the ancient painting processing problems become natural image\nprocessing problems and models trained on natural images can be directly\napplied to the transferred paintings. Specifically, we focus on Chinese ancient\nflower, bird and landscape paintings in this work. A novel Domain Style\nTransfer Network (DSTN) is proposed to transfer ancient paintings to natural\nimages which employ a compound loss to ensure that the transferred paintings\nstill maintain the color composition and content of the input paintings. The\nexperiment results show that the transferred paintings generated by the DSTN\nhave a better performance in both the human perceptual test and other image\nprocessing tasks than other state-of-art methods, indicating the authenticity\nof the transferred paintings and the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 00:35:19 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 02:59:47 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Qiao", "Tingting", ""], ["Zhang", "Weijing", ""], ["Zhang", "Miao", ""], ["Ma", "Zixuan", ""], ["Xu", "Duanqing", ""]]}, {"id": "1901.00248", "submitter": "Donna Xu", "authors": "Donna Xu, Yaxin Shi, Ivor W. Tsang, Yew-Soon Ong, Chen Gong and Xiaobo\n  Shen", "title": "A Survey on Multi-output Learning", "comments": "Paper accepted by IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 03:10:24 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 10:59:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xu", "Donna", ""], ["Shi", "Yaxin", ""], ["Tsang", "Ivor W.", ""], ["Ong", "Yew-Soon", ""], ["Gong", "Chen", ""], ["Shen", "Xiaobo", ""]]}, {"id": "1901.00275", "submitter": "Yuan-Fang Li", "authors": "Wei Chen, Jincai Chen, Fuhao Zou, Yuan-Fang Li, Ping Lu, Qiang Wang,\n  Wei Zhao", "title": "Vector and Line Quantization for Billion-scale Similarity Search on GPUs", "comments": "Accepted by Future Generation Computer Systems (FGCS)", "journal-ref": null, "doi": "10.1016/j.future.2019.04.033", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 06:09:12 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 11:06:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Chen", "Wei", ""], ["Chen", "Jincai", ""], ["Zou", "Fuhao", ""], ["Li", "Yuan-Fang", ""], ["Lu", "Ping", ""], ["Wang", "Qiang", ""], ["Zhao", "Wei", ""]]}, {"id": "1901.00282", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha\n  Sridharan", "title": "On Minimum Discrepancy Estimation for Deep Domain Adaptation", "comments": "Accepted in Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of large sets of labeled data, Deep Learning (DL) has\naccomplished extraordinary triumphs in the avenue of computer vision,\nparticularly in object classification and recognition tasks. However, DL cannot\nalways perform well when the training and testing images come from different\ndistributions or in the presence of domain shift between training and testing\nimages. They also suffer in the absence of labeled input data. Domain\nadaptation (DA) methods have been proposed to make up the poor performance due\nto domain shift. In this paper, we present a new unsupervised deep domain\nadaptation method based on the alignment of second order statistics\n(covariances) as well as maximum mean discrepancy of the source and target data\nwith a two stream Convolutional Neural Network (CNN). We demonstrate the\nability of the proposed approach to achieve state-of the-art performance for\nimage classification on three benchmark domain adaptation datasets: Office-31\n[27], Office-Home [37] and Office-Caltech [8].\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 07:17:58 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Baktashmotlagh", "Mahsa", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1901.00303", "submitter": "Caijing Miao", "authors": "Caijing Miao, Lingxi Xie, Fang Wan, Chi Su, Hongye Liu, Jianbin Jiao,\n  Qixiang Ye", "title": "SIXray : A Large-scale Security Inspection X-ray Benchmark for\n  Prohibited Item Discovery in Overlapping Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a large-scale dataset and establish a baseline for\nprohibited item discovery in Security Inspection X-ray images. Our dataset,\nnamed SIXray, consists of 1,059,231 X-ray images, in which 6 classes of 8,929\nprohibited items are manually annotated. It raises a brand new challenge of\noverlapping image data, meanwhile shares the same properties with existing\ndatasets, including complex yet meaningless contexts and class imbalance. We\npropose an approach named class-balanced hierarchical refinement (CHR) to deal\nwith these difficulties. CHR assumes that each input image is sampled from a\nmixture distribution, and that deep networks require an iterative process to\ninfer image contents accurately. To accelerate, we insert reversed connections\nto different network backbones, delivering high-level visual cues to assist\nmid-level features. In addition, a class-balanced loss function is designed to\nmaximally alleviate the noise introduced by easy negative samples. We evaluate\nCHR on SIXray with different ratios of positive/negative samples. Compared to\nthe baselines, CHR enjoys a better ability of discriminating objects especially\nusing mid-level features, which offers the possibility of using a\nweakly-supervised approach towards accurate object localization. In particular,\nthe advantage of CHR is more significant in the scenarios with fewer positive\ntraining samples, which demonstrates its potential application in real-world\nsecurity inspection.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 09:23:42 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Miao", "Caijing", ""], ["Xie", "Lingxi", ""], ["Wan", "Fang", ""], ["Su", "Chi", ""], ["Liu", "Hongye", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "1901.00326", "submitter": "Michal Koperski", "authors": "Michal Koperski, Tomasz Konopczynski, Rafa{\\l} Nowak, Piotr\n  Semberecki, Tomasz Trzcinski", "title": "Plugin Networks for Inference under Partial Evidence", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to incorporate partial evidence in\nthe inference of deep convolutional neural networks. Contrary to the existing,\ntop performing methods, which either iteratively modify the input of the\nnetwork or exploit external label taxonomy to take the partial evidence into\naccount, we add separate network modules (\"Plugin Networks\") to the\nintermediate layers of a pre-trained convolutional network. The goal of these\nmodules is to incorporate additional signal, ie information about known labels,\ninto the inference procedure and adjust the predicted output accordingly. Since\nthe attached plugins have a simple structure, consisting of only fully\nconnected layers, we drastically reduced the computational cost of training and\ninference. At the same time, the proposed architecture allows to propagate\ninformation about known labels directly to the intermediate layers to improve\nthe final representation. Extensive evaluation of the proposed method confirms\nthat our Plugin Networks outperform the state-of-the-art in a variety of tasks,\nincluding scene categorization, multi-label image annotation, and semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 11:30:19 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 12:02:32 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 05:51:02 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Koperski", "Michal", ""], ["Konopczynski", "Tomasz", ""], ["Nowak", "Rafa\u0142", ""], ["Semberecki", "Piotr", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1901.00361", "submitter": "Bowen Lin", "authors": "Bowen Lin, Shujun Fu, Caiming Zhang, Fengling Wang, Yuliang Li", "title": "Optical Fringe Patterns Filtering Based on Multi-Stage Convolution\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.optlaseng.2019.105853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical fringe patterns are often contaminated by speckle noise, making it\ndifficult to accurately and robustly extract their phase fields. To deal with\nthis problem, we propose a filtering method based on deep learning, called\noptical fringe patterns denoising convolutional neural network (FPD-CNN), for\ndirectly removing speckle from the input noisy fringe patterns. Regularization\ntechnology is integrated into the design of deep architecture. Specifically,\nthe FPD-CNN method is divided into multiple stages, each stage consists of a\nset of convolutional layers along with batch normalization and leaky rectified\nlinear unit (Leaky ReLU) activation function. The end-to-end joint training is\ncarried out using the Euclidean loss. Extensive experiments on simulated and\nexperimental optical fringe patterns,especially finer ones with high-density\nregions, show that the proposed method is competitive with some\nstate-of-the-art denoising techniques in spatial or transform domains,\nefficiently preserving main features of fringe at a fairly fast speed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 13:55:16 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 14:39:23 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 05:44:47 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Lin", "Bowen", ""], ["Fu", "Shujun", ""], ["Zhang", "Caiming", ""], ["Wang", "Fengling", ""], ["Li", "Yuliang", ""]]}, {"id": "1901.00363", "submitter": "Yipeng Sun", "authors": "Jiaming Liu, Chengquan Zhang, Yipeng Sun, Junyu Han and Errui Ding", "title": "Detecting Text in the Wild with Deep Character Embedding Network", "comments": "Asian Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most text detection methods hypothesize texts are horizontal or\nmulti-oriented and thus define quadrangles as the basic detection unit.\nHowever, text in the wild is usually perspectively distorted or curved, which\ncan not be easily tackled by existing approaches. In this paper, we propose a\ndeep character embedding network (CENet) which simultaneously predicts the\nbounding boxes of characters and their embedding vectors, thus making text\ndetection a simple clustering task in the character embedding space. The\nproposed method does not require strong assumptions of forming a straight line\non general text detection, which provides flexibility on arbitrarily curved or\nperspectively distorted text. For character detection task, a dense prediction\nsubnetwork is designed to obtain the confidence score and bounding boxes of\ncharacters. For character embedding task, a subnet is trained with contrastive\nloss to project detected characters into embedding space. The two tasks share a\nbackbone CNN from which the multi-scale feature maps are extracted. The final\ntext regions can be easily achieved by a thresholding process on character\nconfidence and embedding distance of character pairs. We evaluated our method\non ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves\nstate-of-the-art or comparable performance on all these datasets, and shows\nsubstantial improvement in the irregular-text datasets, i.e. Total-Text.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:00:33 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Liu", "Jiaming", ""], ["Zhang", "Chengquan", ""], ["Sun", "Yipeng", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "1901.00366", "submitter": "Yimin Chen", "authors": "Shitao Tang, Litong Feng, Wenqi Shao, Zhanghui Kuang, Wei Zhang, Yimin\n  Chen", "title": "Learning Efficient Detector with Semi-supervised Adaptive Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) has been used in image classification for model\ncompression. However, rare studies apply this technology on single-stage object\ndetectors. Focal loss shows that the accumulated errors of easily-classified\nsamples dominate the overall loss in the training process. This problem is also\nencountered when applying KD in the detection task. For KD, the teacher-defined\nhard samples are far more important than any others. We propose ADL to address\nthis issue by adaptively mimicking the teacher's logits, with more attention\npaid on two types of hard samples: hard-to-learn samples predicted by teacher\nwith low certainty and hard-to-mimic samples with a large gap between the\nteacher's and the student's prediction. ADL enlarges the distillation loss for\nhard-to-learn and hard-to-mimic samples and reduces distillation loss for the\ndominant easy samples, enabling distillation to work on the single-stage\ndetector first time, even if the student and the teacher are identical.\nBesides, ADL is effective in both the supervised setting and the\nsemi-supervised setting, even when the labeled data and unlabeled data are from\ndifferent distributions. For distillation on unlabeled data, ADL achieves\nbetter performance than existing data distillation which simply utilizes hard\ntargets, making the student detector surpass its teacher. On the COCO database,\nsemi-supervised adaptive distillation (SAD) makes a student detector with a\nbackbone of ResNet-50 surpasses its teacher with a backbone of ResNet-101,\nwhile the student has half of the teacher's computation complexity. The code is\navaiable at https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:03:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 05:50:37 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Tang", "Shitao", ""], ["Feng", "Litong", ""], ["Shao", "Wenqi", ""], ["Kuang", "Zhanghui", ""], ["Zhang", "Wei", ""], ["Chen", "Yimin", ""]]}, {"id": "1901.00392", "submitter": "Kai Han", "authors": "Kai Han, Jianyuan Guo, Chao Zhang, Mingjian Zhu", "title": "Attribute-Aware Attention Model for Fine-grained Representation Learning", "comments": "Accepted by ACM Multimedia 2018 (Oral). Code is available at\n  https://github.com/iamhankai/attribute-aware-attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to learn a discriminative fine-grained representation is a key point in\nmany computer vision applications, such as person re-identification,\nfine-grained classification, fine-grained image retrieval, etc. Most of the\nprevious methods focus on learning metrics or ensemble to derive better global\nrepresentation, which are usually lack of local information. Based on the\nconsiderations above, we propose a novel Attribute-Aware Attention Model\n($A^3M$), which can learn local attribute representation and global category\nrepresentation simultaneously in an end-to-end manner. The proposed model\ncontains two attention models: attribute-guided attention module uses attribute\ninformation to help select category features in different regions, at the same\ntime, category-guided attention module selects local features of different\nattributes with the help of category cues. Through this attribute-category\nreciprocal process, local and global features benefit from each other. Finally,\nthe resulting feature contains more intrinsic information for image recognition\ninstead of the noisy and irrelevant features. Extensive experiments conducted\non Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the\neffectiveness of our $A^3M$. Code is available at\nhttps://github.com/iamhankai/attribute-aware-attention.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:22:59 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 12:29:29 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Han", "Kai", ""], ["Guo", "Jianyuan", ""], ["Zhang", "Chao", ""], ["Zhu", "Mingjian", ""]]}, {"id": "1901.00413", "submitter": "Ramakrishnan Angarai Ganesan", "authors": "Shiva Kumar H R and Ramakrishnan A G", "title": "Lipi Gnani - A Versatile OCR for Documents in any Language Printed in\n  Kannada Script", "comments": "21 pages, 16 figures, 12 tables, submitted to ACM Transactions on\n  Asian and Low-Resource Language Information Processing", "journal-ref": null, "doi": null, "report-no": "mile_shr_agr_01_2019", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Kannada OCR, named Lipi Gnani, has been designed and developed from\nscratch, with the motivation of it being able to convert printed text or poetry\nin Kannada script, without any restriction on vocabulary. The training and test\nsets have been collected from over 35 books published between the period 1970\nto 2002, and this includes books written in Halegannada and pages containing\nSanskrit slokas written in Kannada script. The coverage of the OCR is nearly\ncomplete in the sense that it recognizes all the punctuation marks, special\nsymbols, Indo-Arabic and Kannada numerals and also the interspersed English\nwords. Several minor and major original contributions have been done in\ndeveloping this OCR at the different processing stages such as binarization,\nline and character segmentation, recognition and Unicode mapping. This has\ncreated a Kannada OCR that performs as good as, and in some cases, better than\nthe Google's Tesseract OCR, as shown by the results. To the knowledge of the\nauthors, this is the maiden report of a complete Kannada OCR, handling all the\nissues involved. Currently, there is no dictionary based postprocessing, and\nthe obtained results are due solely to the recognition process. Four benchmark\ntest databases containing scanned pages from books in Kannada, Sanskrit,\nKonkani and Tulu languages, but all of them printed in Kannada script, have\nbeen created. The word level recognition accuracy of Lipi Gnani is 4% higher on\nthe Kannada dataset than that of Google's Tesseract OCR, 8% higher on the\ndatasets of Tulu and Sanskrit, and 25% higher on the Konkani dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 15:22:41 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["R", "Shiva Kumar H", ""], ["G", "Ramakrishnan A", ""]]}, {"id": "1901.00418", "submitter": "Gustav M{\\aa}rtensson", "authors": "Gustav M{\\aa}rtensson, Daniel Ferreira, Lena Cavallin, J-Sebastian\n  Muehlboeck, Lars-Olof Wahlund, Chunliang Wang, Eric Westman", "title": "AVRA: Automatic Visual Ratings of Atrophy from MRI images using\n  Recurrent Convolutional Neural Networks", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.nicl.2019.101872", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the degree of atrophy is done clinically by neuroradiologists\nfollowing established visual rating scales. For these assessments to be\nreliable the rater requires substantial training and experience, and even then\nthe rating agreement between two radiologists is not perfect. We have developed\na model we call AVRA (Automatic Visual Ratings of Atrophy) based on machine\nlearning methods and trained on 2350 visual ratings made by an experienced\nneuroradiologist. It provides fast and automatic ratings for Scheltens' scale\nof medial temporal atrophy (MTA), the frontal subscale of Pasquier's Global\nCortical Atrophy (GCA-F) scale, and Koedam's scale of Posterior Atrophy (PA).\nWe demonstrate substantial inter-rater agreement between AVRA's and a\nneuroradiologist ratings with Cohen's weighted kappa values of $\\kappa_w$ =\n0.74/0.72 (MTA left/right), $\\kappa_w$ = 0.62 (GCA-F) and $\\kappa_w$ = 0.74\n(PA), with an inherent intra-rater agreement of $\\kappa_w$ = 1. We conclude\nthat automatic visual ratings of atrophy can potentially have great clinical\nand scientific value, and aim to present AVRA as a freely available toolbox.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 10:23:50 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["M\u00e5rtensson", "Gustav", ""], ["Ferreira", "Daniel", ""], ["Cavallin", "Lena", ""], ["Muehlboeck", "J-Sebastian", ""], ["Wahlund", "Lars-Olof", ""], ["Wang", "Chunliang", ""], ["Westman", "Eric", ""]]}, {"id": "1901.00449", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Simon J. Julier, Nadia Bianchi-Berthouze", "title": "Instant Automated Inference of Perceived Mental Stress through\n  Smartphone PPG and Thermal Imaging", "comments": "Accepted by Journal of Medical Internet Research (JMIR) Mental Health\n  - Special Issue on Computing and Mental Health (2018)", "journal-ref": null, "doi": "10.2196/10140", "report-no": null, "categories": "physics.med-ph cs.CV cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: A smartphone is a promising tool for daily cardiovascular\nmeasurement and mental stress monitoring. A smartphone camera-based\nPhotoPlethysmoGraphy (PPG) and a low-cost thermal camera can be used to create\ncheap, convenient and mobile monitoring systems. However, to ensure reliable\nmonitoring results, a person has to remain still for several minutes while a\nmeasurement is being taken. This is very cumbersome and makes its use in\nreal-life mobile situations quite impractical.\n  Objective: We propose a system which combines PPG and thermography with the\naim of improving cardiovascular signal quality and capturing stress responses\nquickly.\n  Methods: Using a smartphone camera with a low cost thermal camera added on,\nwe built a novel system which continuously and reliably measures two different\ntypes of cardiovascular events: i) blood volume pulse and ii)\nvasoconstriction/dilation-induced temperature changes of the nose tip. 17\nhealthy participants, involved in a series of stress-inducing mental workload\ntasks, measured their physiological responses to stressors over a short window\nof time (20 seconds) immediately after each task. Participants reported their\nlevel of perceived mental stress using a 10-cm Visual Analogue Scale (VAS). We\nused normalized K-means clustering to reduce interpersonal differences in the\nself-reported ratings. For the instant stress inference task, we built novel\nlow-level feature sets representing variability of cardiovascular patterns. We\nthen used the automatic feature learning capability of artificial Neural\nNetworks (NN) to improve the mapping between the extracted set of features and\nthe self-reported ratings. We compared our proposed method with existing\nhand-engineered features-based machine learning methods.\n  Results, Conclusions: ... due to limited space here, we refer to our\nmanuscript.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 00:49:11 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Cho", "Youngjun", ""], ["Julier", "Simon J.", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1901.00463", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira Bermudez", "title": "Improved Hyperspectral Unmixing With Endmember Variability Parametrized\n  Using an Interpolated Scaling Tensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endmember (EM) variability has an important impact on the performance of\nhyperspectral image (HI) analysis algorithms. Recently, extended linear mixing\nmodels have been proposed to account for EM variability in the spectral\nunmixing (SU) problem. The direct use of these models has led to severely\nill-posed optimization problems. Different regularization strategies have been\nconsidered to deal with this issue, but none so far has consistently exploited\nthe information provided by the existence of multiple pure pixels often present\nin HIs. In this work, we propose to break the SU problem into a sequence of two\nproblems. First, we use pure pixel information to estimate an interpolated\ntensor of scaling factors representing spectral variability. This is done by\nconsidering the spectral variability to be a smooth function over the HI and\nconfining the energy of the scaling tensor to a low-rank structure. Afterwards,\nwe solve a matrix-factorization problem to estimate the fractional abundances\nusing the variability scaling factors estimated in the previous step, what\nleads to a significantly more well-posed problem. Simulation swith synthetic\nand real data attest the effectiveness of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 17:51:12 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1901.00466", "submitter": "Davis Rempe", "authors": "Davis Rempe, Srinath Sridhar, He Wang, Leonidas J. Guibas", "title": "Learning Generalizable Physical Dynamics of 3D Rigid Objects", "comments": "13 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a remarkable ability to predict the effect of physical\ninteractions on the dynamics of objects. Endowing machines with this ability\nwould allow important applications in areas like robotics and autonomous\nvehicles. In this work, we focus on predicting the dynamics of 3D rigid\nobjects, in particular an object's final resting position and total rotation\nwhen subjected to an impulsive force. Different from previous work, our\napproach is capable of generalizing to unseen object shapes - an important\nrequirement for real-world applications. To achieve this, we represent object\nshape as a 3D point cloud that is used as input to a neural network, making our\napproach agnostic to appearance variation. The design of our network is\ninformed by an understanding of physical laws. We train our model with data\nfrom a physics engine that simulates the dynamics of a large number of shapes.\nExperiments show that we can accurately predict the resting position and total\nrotation for unseen object geometries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 17:57:50 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Rempe", "Davis", ""], ["Sridhar", "Srinath", ""], ["Wang", "He", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1901.00484", "submitter": "Meera Hahn", "authors": "Meera Hahn, Andrew Silva and James M. Rehg", "title": "Action2Vec: A Crossmodal Embedding Approach to Action Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel cross-modal embedding space for actions, named\nAction2Vec, which combines linguistic cues from class labels with\nspatio-temporal features derived from video clips. Our approach uses a\nhierarchical recurrent network to capture the temporal structure of video\nfeatures. We train our embedding using a joint loss that combines\nclassification accuracy with similarity to Word2Vec semantics. We evaluate\nAction2Vec by performing zero shot action recognition and obtain state of the\nart results on three standard datasets. In addition, we present two novel\nanalogy tests which quantify the extent to which our joint embedding captures\ndistributional semantics. This is the first joint embedding space to combine\nverbs and action videos, and the first to be thoroughly evaluated with respect\nto its distributional semantics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:35:32 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Hahn", "Meera", ""], ["Silva", "Andrew", ""], ["Rehg", "James M.", ""]]}, {"id": "1901.00488", "submitter": "Jianzhu Guo", "authors": "Jianzhu Guo, Xiangyu Zhu, Jinchuan Xiao, Zhen Lei, Genxun Wan, Stan Z.\n  Li", "title": "Improving Face Anti-Spoofing by 3D Virtual Synthesis", "comments": "Accepted to ICB 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is crucial for the security of face recognition systems.\nLearning based methods especially deep learning based methods need large-scale\ntraining samples to reduce overfitting. However, acquiring spoof data is very\nexpensive since the live faces should be re-printed and re-captured in many\nviews. In this paper, we present a method to synthesize virtual spoof data in\n3D space to alleviate this problem. Specifically, we consider a printed photo\nas a flat surface and mesh it into a 3D object, which is then randomly bent and\nrotated in 3D space. Afterward, the transformed 3D photo is rendered through\nperspective projection as a virtual sample. The synthetic virtual samples can\nsignificantly boost the anti-spoofing performance when combined with a proposed\ndata balancing strategy. Our promising results open up new possibilities for\nadvancing face anti-spoofing using cheap and large-scale synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:40:33 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 09:13:03 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 15:37:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Guo", "Jianzhu", ""], ["Zhu", "Xiangyu", ""], ["Xiao", "Jinchuan", ""], ["Lei", "Zhen", ""], ["Wan", "Genxun", ""], ["Li", "Stan Z.", ""]]}, {"id": "1901.00516", "submitter": "Peter He", "authors": "Peter He, Alexis Gkantiragas, Gerard Glowacki", "title": "Honey Authentication with Machine Learning Augmented Bright-Field\n  Microscopy", "comments": "Accepted at the 'AI for Social Good' workshop at the 32nd Conference\n  on Neural Information Processing Systems (NeurIPS2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey has been collected and used by humankind as both a food and medicine\nfor thousands of years. However, in the modern economy, honey has become\nsubject to mislabelling and adulteration making it the third most faked food\nproduct in the world. The international scale of fraudulent honey has had both\neconomic and environmental ramifications. In this paper, we propose a novel\nmethod of identifying fraudulent honey using machine learning augmented\nmicroscopy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 13:01:25 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["He", "Peter", ""], ["Gkantiragas", "Alexis", ""], ["Glowacki", "Gerard", ""]]}, {"id": "1901.00520", "submitter": "Bin Ma", "authors": "Bin Ma, Shubao Liu, Yingxuan Zhi, Qi Song", "title": "Flow Based Self-supervised Pixel Embedding for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new self-supervised approach to image feature learning from\nmotion cue. This new approach leverages recent advances in deep learning in two\ndirections: 1) the success of training deep neural network in estimating\noptical flow in real data using synthetic flow data; and 2) emerging work in\nlearning image features from motion cues, such as optical flow. Building on\nthese, we demonstrate that image features can be learned in self-supervision by\nfirst training an optical flow estimator with synthetic flow data, and then\nlearning image features from the estimated flows in real motion data. We\ndemonstrate and evaluate this approach on an image segmentation task. Using the\nlearned image feature representation, the network performs significantly better\nthan the ones trained from scratch in few-shot segmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 20:24:41 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 20:01:48 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Ma", "Bin", ""], ["Liu", "Shubao", ""], ["Zhi", "Yingxuan", ""], ["Song", "Qi", ""]]}, {"id": "1901.00534", "submitter": "Anna Smagina Mrs", "authors": "Anna Smagina, Valentina Bozhkova, Sergey Gladilin, Dmitry Nikolaev", "title": "Linear colour segmentation revisited", "comments": null, "journal-ref": "Proc. SPIE 11041, Eleventh International Conference on Machine\n  Vision (ICMV 2018)", "doi": "10.1117/12.2523007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the known algorithms for linear colour segmentation\nbased on a physical approach and propose a new modification of segmentation\nalgorithm. This algorithm is based on a region adjacency graph framework\nwithout a pre-segmentation stage. Proposed edge weight functions are defined\nfrom linear image model with normal noise. The colour space projective\ntransform is introduced as a novel pre-processing technique for better handling\nof shadow and highlight areas. The resulting algorithm is tested on a benchmark\ndataset consisting of the images of 19 natural scenes selected from the\nBarnard's DXC-930 SFU dataset and 12 natural scene images newly published for\ncommon use. The dataset is provided with pixel-by-pixel ground truth colour\nsegmentation for every image. Using this dataset, we show that the proposed\nalgorithm modifications lead to qualitative advantages over other model-based\nsegmentation algorithms, and also show the positive effect of each proposed\nmodification. The source code and datasets for this work are available for free\naccess at http://github.com/visillect/segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 21:06:55 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Smagina", "Anna", ""], ["Bozhkova", "Valentina", ""], ["Gladilin", "Sergey", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "1901.00536", "submitter": "Abby Stylianou", "authors": "Abby Stylianou, Richard Souvenir, Robert Pless", "title": "Visualizing Deep Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 21:24:11 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Stylianou", "Abby", ""], ["Souvenir", "Richard", ""], ["Pless", "Robert", ""]]}, {"id": "1901.00542", "submitter": "Mengtian Li", "authors": "Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer and Deva Ramanan", "title": "Photo-Sketching: Inferring Contour Drawings from Images", "comments": "WACV 2019. For code and dataset, see\n  http://www.cs.cmu.edu/~mengtial/proj/sketch/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edges, boundaries and contours are important subjects of study in both\ncomputer graphics and computer vision. On one hand, they are the 2D elements\nthat convey 3D shapes, on the other hand, they are indicative of occlusion\nevents and thus separation of objects or semantic concepts. In this paper, we\naim to generate contour drawings, boundary-like drawings that capture the\noutline of the visual scene. Prior art often cast this problem as boundary\ndetection. However, the set of visual cues presented in the boundary detection\noutput are different from the ones in contour drawings, and also the artistic\nstyle is ignored. We address these issues by collecting a new dataset of\ncontour drawings and proposing a learning-based method that resolves diversity\nin the annotation and, unlike boundary detectors, can work with imperfect\nalignment of the annotation and the actual ground truth. Our method surpasses\nprevious methods quantitatively and qualitatively. Surprisingly, when our model\nfine-tunes on BSDS500, we achieve the state-of-the-art performance in salient\nboundary detection, suggesting contour drawing might be a scalable alternative\nto boundary annotation, which at the same time is easier and more interesting\nfor annotators to draw.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 21:59:44 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Li", "Mengtian", ""], ["Lin", "Zhe", ""], ["Mech", "Radomir", ""], ["Yumer", "Ersin", ""], ["Ramanan", "Deva", ""]]}, {"id": "1901.00544", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, Zsolt Kira", "title": "Multi-class Classification without Multi-class Labels", "comments": "International Conference on Learning Representations (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new strategy for multi-class classification that\nrequires no class-specific labels, but instead leverages pairwise similarity\nbetween examples, which is a weaker form of annotation. The proposed method,\nmeta classification learning, optimizes a binary classifier for pairwise\nsimilarity prediction and through this process learns a multi-class classifier\nas a submodule. We formulate this approach, present a probabilistic graphical\nmodel for it, and derive a surprisingly simple loss function that can be used\nto learn neural network-based models. We then demonstrate that this same\nframework generalizes to the supervised, unsupervised cross-task, and\nsemi-supervised settings. Our method is evaluated against state of the art in\nall three learning paradigms and shows a superior or comparable accuracy,\nproviding evidence that learning multi-class classification without multi-class\nlabels is a viable learning option.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 22:09:12 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Schlosser", "Joel", ""], ["Odom", "Phillip", ""], ["Kira", "Zsolt", ""]]}, {"id": "1901.00563", "submitter": "Zheng Zhang", "authors": "Jie Wen, Zuofeng Zhong, Zheng Zhang, Lunke Fei, Zhihui Lai, Runze Chen", "title": "Adaptive Locality Preserving Regression", "comments": "The paper has been accepted by IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT), and the code can be available at\n  https://drive.google.com/file/d/1iNzONkRByIaUhXwdEhOkkh_0d2AAXNE8/view", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2018", "doi": "10.1109/TCSVT.2018.2889727", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel discriminative regression method, called adaptive\nlocality preserving regression (ALPR) for classification. In particular, ALPR\naims to learn a more flexible and discriminative projection that not only\npreserves the intrinsic structure of data, but also possesses the properties of\nfeature selection and interpretability. To this end, we introduce a target\nlearning technique to adaptively learn a more discriminative and flexible\ntarget matrix rather than the pre-defined strict zero-one label matrix for\nregression. Then a locality preserving constraint regularized by the adaptive\nlearned weights is further introduced to guide the projection learning, which\nis beneficial to learn a more discriminative projection and avoid overfitting.\nMoreover, we replace the conventional `Frobenius norm' with the special l21\nnorm to constrain the projection, which enables the method to adaptively select\nthe most important features from the original high-dimensional data for feature\nextraction. In this way, the negative influence of the redundant features and\nnoises residing in the original data can be greatly eliminated. Besides, the\nproposed method has good interpretability for features owing to the\nrow-sparsity property of the l21 norm. Extensive experiments conducted on the\nsynthetic database with manifold structure and many real-world databases prove\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 00:36:23 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Wen", "Jie", ""], ["Zhong", "Zuofeng", ""], ["Zhang", "Zheng", ""], ["Fei", "Lunke", ""], ["Lai", "Zhihui", ""], ["Chen", "Runze", ""]]}, {"id": "1901.00600", "submitter": "DaoYu Lin", "authors": "Daoyu Lin, Guangluan Xu, Xiaoke Wang, Yang Wang, Xian Sun, Kun Fu", "title": "A Remote Sensing Image Dataset for Cloud Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based overlays are often present in optical remote sensing images, thus\nlimiting the application of acquired data. Removing clouds is an indispensable\npre-processing step in remote sensing image analysis. Deep learning has\nachieved great success in the field of remote sensing in recent years,\nincluding scene classification and change detection. However, deep learning is\nrarely applied in remote sensing image removal clouds. The reason is the lack\nof data sets for training neural networks. In order to solve this problem, this\npaper first proposed the Remote sensing Image Cloud rEmoving dataset (RICE).\nThe proposed dataset consists of two parts: RICE1 contains 500 pairs of images,\neach pair has images with cloud and cloudless size of 512*512; RICE2 contains\n450 sets of images, each set contains three 512*512 size images. ,\nrespectively, the reference picture without clouds, the picture of the cloud\nand the mask of its cloud. The dataset is freely available at\n\\url{https://github.com/BUPTLdy/RICE_DATASET}.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 03:43:38 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Lin", "Daoyu", ""], ["Xu", "Guangluan", ""], ["Wang", "Xiaoke", ""], ["Wang", "Yang", ""], ["Sun", "Xian", ""], ["Fu", "Kun", ""]]}, {"id": "1901.00616", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, Salman Khan, Nick Barnes", "title": "Volumetric Convolution: Automatic Representation Learning in Unit Ball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is an efficient technique to obtain abstract feature\nrepresentations using hierarchical layers in deep networks. Although performing\nconvolution in Euclidean geometries is fairly straightforward, its extension to\nother topological spaces---such as a sphere ($\\mathbb{S}^2$) or a unit ball\n($\\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel\n`\\emph{volumetric convolution}' operation that can effectively convolve\narbitrary functions in $\\mathbb{B}^3$. We develop a theoretical framework for\n\\emph{volumetric convolution} based on Zernike polynomials and efficiently\nimplement it as a differentiable and an easily pluggable layer for deep\nnetworks. Furthermore, our formulation leads to derivation of a novel formula\nto measure the symmetry of a function in $\\mathbb{B}^3$ around an arbitrary\naxis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of\nproposed volumetric convolution operation on a possible use-case i.e., 3D\nobject recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 05:53:25 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""]]}, {"id": "1901.00621", "submitter": "Wei Zhang", "authors": "Weidong Zhang, Wei Zhang, Jason Gu", "title": "Edge-Semantic Learning Strategy for Layout Estimation in Indoor\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual cognition of the indoor environment can benefit from the spatial\nlayout estimation, which is to represent an indoor scene with a 2D box on a\nmonocular image. In this paper, we propose to fully exploit the edge and\nsemantic information of a room image for layout estimation. More specifically,\nwe present an encoder-decoder network with shared encoder and two separate\ndecoders, which are composed of multiple deconvolution (transposed convolution)\nlayers, to jointly learn the edge maps and semantic labels of a room image. We\ncombine these two network predictions in a scoring function to evaluate the\nquality of the layouts, which are generated by ray sampling and from a\npredefined layout pool. Guided by the scoring function, we apply a novel\nrefinement strategy to further optimize the layout hypotheses. Experimental\nresults show that the proposed network can yield accurate estimates of edge\nmaps and semantic labels. By fully utilizing the two different types of labels,\nthe proposed method achieves state-of-the-art layout estimation performance on\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:07:56 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhang", "Weidong", ""], ["Zhang", "Wei", ""], ["Gu", "Jason", ""]]}, {"id": "1901.00643", "submitter": "Bingbing Zhuang", "authors": "Bingbing Zhuang, Loong-Fah Cheong, Gim Hee Lee", "title": "Baseline Desensitizing In Translation Averaging", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing translation averaging algorithms are either sensitive to\ndisparate camera baselines and have to rely on extensive preprocessing to\nimprove the observed Epipolar Geometry graph, or if they are robust against\ndisparate camera baselines, require complicated optimization to minimize the\nhighly nonlinear angular error objective. In this paper, we carefully design a\nsimple yet effective bilinear objective function, introducing a variable to\nperform the requisite normalization. The objective function enjoys the\nbaseline-insensitive property of the angular error and yet is amenable to\nsimple and efficient optimization by block coordinate descent, with good\nempirical performance. A rotation-assisted Iterative Reweighted Least Squares\nscheme is further put forth to help deal with outliers. We also contribute\ntowards a better understanding of the behavior of two recent convex algorithms,\nLUD and Shapefit/kick, clarifying the underlying subtle difference that leads\nto the performance gap. Finally, we demonstrate that our algorithm achieves\noverall superior accuracies in benchmark dataset compared to state-of-theart\nmethods, and is also several times faster.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 08:05:24 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhuang", "Bingbing", ""], ["Cheong", "Loong-Fah", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1901.00675", "submitter": "Francois Luus Dr.", "authors": "Francois Luus, Naweed Khan, Ismail Akhalwaya", "title": "Active Learning with TensorBoard Projector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ML-based system for interactive labeling of image datasets is contributed\nin TensorBoard Projector to speed up image annotation performed by humans. The\ntool visualizes feature spaces and makes it directly editable by online\nintegration of applied labels, and it is a system for verifying and managing\nmachine learning data pertaining to labels. We propose realistic annotation\nemulation to evaluate the system design of interactive active learning, based\non our improved semi-supervised extension of t-SNE dimensionality reduction.\nOur active learning tool can significantly increase labeling efficiency\ncompared to uncertainty sampling, and we show that less than 100 labeling\nactions are typically sufficient for good classification on a variety of\nspecialized image datasets. Our contribution is unique given that it needs to\nperform dimensionality reduction, feature space visualization and editing,\ninteractive label propagation, low-complexity active learning, human perceptual\nmodeling, annotation emulation and unsupervised feature extraction for\nspecialized datasets in a production-quality implementation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 10:42:36 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Luus", "Francois", ""], ["Khan", "Naweed", ""], ["Akhalwaya", "Ismail", ""]]}, {"id": "1901.00680", "submitter": "Tong He", "authors": "Tong He, Haibin Huang, Li Yi, Yuqian Zhou, Chihao Wu, Jue Wang,\n  Stefano Soatto", "title": "GeoNet: Deep Geodesic Networks for Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface-based geodesic topology provides strong cues for object semantic\nanalysis and geometric modeling. However, such connectivity information is lost\nin point clouds. Thus we introduce GeoNet, the first deep learning architecture\ntrained to model the intrinsic structure of surfaces represented as point\nclouds. To demonstrate the applicability of learned geodesic-aware\nrepresentations, we propose fusion schemes which use GeoNet in conjunction with\nother baseline or backbone networks, such as PU-Net and PointNet++, for\ndown-stream point cloud analysis. Our method improves the state-of-the-art on\nmultiple representative tasks that can benefit from understandings of the\nunderlying surface topology, including point upsampling, normal estimation,\nmesh reconstruction and non-rigid shape classification.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 11:02:14 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["He", "Tong", ""], ["Huang", "Haibin", ""], ["Yi", "Li", ""], ["Zhou", "Yuqian", ""], ["Wu", "Chihao", ""], ["Wang", "Jue", ""], ["Soatto", "Stefano", ""]]}, {"id": "1901.00686", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Stefan Heinrich, Stefan Wermter", "title": "Generating Multiple Objects at Spatially Distinct Locations", "comments": "Published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent improvements to Generative Adversarial Networks (GANs) have made it\npossible to generate realistic images in high resolution based on natural\nlanguage descriptions such as image captions. Furthermore, conditional GANs\nallow us to control the image generation process through labels or even natural\nlanguage descriptions. However, fine-grained control of the image layout, i.e.\nwhere in the image specific objects should be located, is still difficult to\nachieve. This is especially true for images that should contain multiple\ndistinct objects at different spatial locations. We introduce a new approach\nwhich allows us to control the location of arbitrarily many objects within an\nimage by adding an object pathway to both the generator and the discriminator.\nOur approach does not need a detailed semantic layout but only bounding boxes\nand the respective labels of the desired objects are needed. The object pathway\nfocuses solely on the individual objects and is iteratively applied at the\nlocations specified by the bounding boxes. The global pathway focuses on the\nimage background and the general image layout. We perform experiments on the\nMulti-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show\nthat through the use of the object pathway we can control object locations\nwithin images and can model complex scenes with multiple objects at various\nlocations. We further show that the object pathway focuses on the individual\nobjects and learns features relevant for these, while the global pathway\nfocuses on global image characteristics and the image background.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 11:18:52 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Hinz", "Tobias", ""], ["Heinrich", "Stefan", ""], ["Wermter", "Stefan", ""]]}, {"id": "1901.00711", "submitter": "Marcus Klasson", "authors": "Marcus Klasson, Cheng Zhang, Hedvig Kjellstr\\\"om", "title": "A Hierarchical Grocery Store Image Dataset with Visual and Semantic\n  Labels", "comments": "To appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models built into visual support systems and other\nassistive devices need to provide accurate predictions about their environment.\nWe focus on an application of assistive technology for people with visual\nimpairments, for daily activities such as shopping or cooking. In this paper,\nwe provide a new benchmark dataset for a challenging task in this application -\nclassification of fruits, vegetables, and refrigerated products, e.g. milk\npackages and juice cartons, in grocery stores. To enable the learning process\nto utilize multiple sources of structured information, this dataset not only\ncontains a large volume of natural images but also includes the corresponding\ninformation of the product from an online shopping website. Such information\nencompasses the hierarchical structure of the object classes, as well as an\niconic image of each type of object. This dataset can be used to train and\nevaluate image classification models for helping visually impaired people in\nnatural environments. Additionally, we provide benchmark results evaluated on\npretrained convolutional neural networks often used for image understanding\npurposes, and also a multi-view variational autoencoder, which is capable of\nutilizing the rich product information in the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 13:28:08 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Klasson", "Marcus", ""], ["Zhang", "Cheng", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1901.00713", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Fernando Pereira, Paulo Lobato Correia", "title": "Face Recognition: A Novel Multi-Level Taxonomy based Survey", "comments": "This paper is a preprint of a paper submitted to IET Biometrics. If\n  accepted, the copy of record will be available at the IET Digital Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world where security issues have been gaining growing importance, face\nrecognition systems have attracted increasing attention in multiple application\nareas, ranging from forensics and surveillance to commerce and entertainment.\nTo help understanding the landscape and abstraction levels relevant for face\nrecognition systems, face recognition taxonomies allow a deeper dissection and\ncomparison of the existing solutions. This paper proposes a new, more\nencompassing and richer multi-level face recognition taxonomy, facilitating the\norganization and categorization of available and emerging face recognition\nsolutions; this taxonomy may also guide researchers in the development of more\nefficient face recognition solutions. The proposed multi-level taxonomy\nconsiders levels related to the face structure, feature support and feature\nextraction approach. Following the proposed taxonomy, a comprehensive survey of\nrepresentative face recognition solutions is presented. The paper concludes\nwith a discussion on current algorithmic and application related challenges\nwhich may define future research directions for face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 13:47:53 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Pereira", "Fernando", ""], ["Correia", "Paulo Lobato", ""]]}, {"id": "1901.00726", "submitter": "Polina Lemenkova", "authors": "Polina Lemenkova", "title": "Topology, homogeneity and scale factors for object detection:\n  application of eCognition software for urban mapping using multispectral\n  satellite image", "comments": "6 pages, 12 figures, INSO2015, Ed. by A. Girgvliani et al. Akaki\n  Tsereteli State University, Kutaisi (Imereti), Georgia", "journal-ref": "Proceedings of 7th International Conference 'Internet and Society.\n  Modelling' INSO2015, 2015 (80-85)", "doi": "10.6084/m9.figshare.7211588", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The research scope of this paper is to apply spatial object based image\nanalysis (OBIA) method for processing panchromatic multispectral image covering\nstudy area of Brussels for urban mapping. The aim is to map different land\ncover types and more specifically, built-up areas from the very high resolution\n(VHR) satellite image using OBIA approach. A case study covers urban landscapes\nin the eastern areas of the city of Brussels, Belgium. Technically, this\nresearch was performed in eCognition raster processing software demonstrating\nexcellent results of image segmentation and classification. The tools embedded\nin eCognition enabled to perform image segmentation and objects classification\nprocesses in a semi-automated regime, which is useful for the city planning,\nspatial analysis and urban growth analysis. The combination of the OBIA method\ntogether with technical tools of the eCognition demonstrated applicability of\nthis method for urban mapping in densely populated areas, e.g. in megapolis and\ncapital cities. The methodology included multiresolution segmentation and\nclassification of the created objects.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 02:24:48 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Lemenkova", "Polina", ""]]}, {"id": "1901.00738", "submitter": "Mohammad Motamedi", "authors": "Mohammad Motamedi, Felix Portillo, Mahya Saffarpour, Daniel Fong, and\n  Soheil Ghiasi", "title": "Resource-Scalable CNN Synthesis for IoT Applications", "comments": "7 Pages, 3 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image recognition systems use sophisticated Convolutional\nNeural Networks (CNNs) that are designed and trained to identify numerous\nobject classes. Such networks are fairly resource intensive to compute,\nprohibiting their deployment on resource-constrained embedded platforms. On one\nhand, the ability to classify an exhaustive list of categories is excessive for\nthe demands of most IoT applications. On the other hand, designing a new\ncustom-designed CNN for each new IoT application is impractical, due to the\ninherent difficulty in developing competitive models and time-to-market\npressure. To address this problem, we investigate the question of: \"Can one\nutilize an existing optimized CNN model to automatically build a competitive\nCNN for an IoT application whose objects of interest are a fraction of\ncategories that the original CNN was designed to classify, such that the\nresource requirement is proportionally scaled down?\" We use the term resource\nscalability to refer to this concept, and develop a methodology for automated\nsynthesis of resource scalable CNNs from an existing optimized baseline CNN.\nThe synthesized CNN has sufficient learning capacity for handling the given IoT\napplication requirements, and yields competitive accuracy. The proposed\napproach is fast, and unlike the presently common practice of CNN design, does\nnot require iterative rounds of training trial and error.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 01:21:57 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Motamedi", "Mohammad", ""], ["Portillo", "Felix", ""], ["Saffarpour", "Mahya", ""], ["Fong", "Daniel", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1901.00751", "submitter": "Neil Deshmukh", "authors": "Neil Deshmukh", "title": "Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n  Learning Methods", "comments": "Best Machine Learning Paper at the 9th IEEE Columbia Ubiquitous\n  Computing, Electronics & Mobile Communication Conference (UEMCON) and\n  presented at the IEEE MIT Undergraduate Research Technology Conference (URTC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel low-cost device prototype for the automatic\ndiagnosis of diseases, utilizing inputted symptoms and personal background. The\nengineering goal is to solve the problem of limited healthcare access with a\nsingle device. Diagnosing diseases automatically is an immense challenge, owing\nto their variable properties and symptoms. On the other hand, Neural Networks\nhave developed into a powerful tool in the field of machine learning, one that\nis showing to be extremely promising at computing diagnosis even with\ninconsistent variables.\n  In this research, a cheap device was created to allow for straightforward\ndiagnosis and treatment of human diseases. By utilizing Deep Neural Networks\n(DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi\nZero processor ($5), the device is able to detect up to 1537 different diseases\nand conditions and utilize a CNN for on-device visual diagnostics. The user can\ninput the symptoms using the buttons on the device and can take pictures using\nthe same mechanism. The algorithm processes inputted symptoms, providing\ndiagnosis and possible treatment options for common conditions. The purpose of\nthis work was to be able to diagnose diseases through an affordable processor\nwith high accuracy, as it is currently achieving an accuracy of 90% for Top-5\nsymptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve\nperformance far above any other tested system, and its efficiency and ease of\nuse will prove it to be a helpful tool for people around the world. This device\ncould potentially provide low-cost universal access to vital diagnostics and\ntreatment options.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 03:27:56 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 01:59:35 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Deshmukh", "Neil", ""]]}, {"id": "1901.00850", "submitter": "Chenxi Liu", "authors": "Runtao Liu, Chenxi Liu, Yutong Bai, Alan Yuille", "title": "CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions", "comments": "To appear in CVPR 2019. All data and code concerning CLEVR-Ref+ and\n  IEP-Ref have been released at https://cs.jhu.edu/~cxliu/2019/clevr-ref+", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 18:58:06 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 19:59:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liu", "Runtao", ""], ["Liu", "Chenxi", ""], ["Bai", "Yutong", ""], ["Yuille", "Alan", ""]]}, {"id": "1901.00861", "submitter": "Bradley Gram-Hansen", "authors": "Bradley Gram-Hansen, Patrick Helber, Indhu Varatharajan, Faiza Azam,\n  Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski", "title": "Mapping Informal Settlements in Developing Countries using Machine\n  Learning and Low Resolution Multi-spectral Data", "comments": "Published at the AAAI/ACM Conference on AI, ethics and society.\n  Extended results from our previous workshop: arXiv:1812.00812", "journal-ref": "AAAI/ACM Conference on AI, Ethics, and Society (AIES 2019)", "doi": "10.1145/3306618.3314253", "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informal settlements are home to the most socially and economically\nvulnerable people on the planet. In order to deliver effective economic and\nsocial aid, non-government organizations (NGOs), such as the United Nations\nChildren's Fund (UNICEF), require detailed maps of the locations of informal\nsettlements. However, data regarding informal and formal settlements is\nprimarily unavailable and if available is often incomplete. This is due, in\npart, to the cost and complexity of gathering data on a large scale. To address\nthese challenges, we, in this work, provide three contributions. 1) A brand new\nmachine learning data-set, purposely developed for informal settlement\ndetection. 2) We show that it is possible to detect informal settlements using\nfreely available low-resolution (LR) data, in contrast to previous studies that\nuse very-high resolution (VHR) satellite and aerial imagery, something that is\ncost-prohibitive for NGOs. 3) We demonstrate two effective classification\nschemes on our curated data set, one that is cost-efficient for NGOs and\nanother that is cost-prohibitive for NGOs, but has additional utility. We\nintegrate these schemes into a semi-automated pipeline that converts either a\nLR or VHR satellite image into a binary map that encodes the locations of\ninformal settlements.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 16:51:40 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 23:18:26 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 11:11:39 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gram-Hansen", "Bradley", ""], ["Helber", "Patrick", ""], ["Varatharajan", "Indhu", ""], ["Azam", "Faiza", ""], ["Coca-Castro", "Alejandro", ""], ["Kopackova", "Veronika", ""], ["Bilinski", "Piotr", ""]]}, {"id": "1901.00889", "submitter": "Xing Di", "authors": "Xing Di, He Zhang, Vishal M. Patel", "title": "Polarimetric Thermal to Visible Face Verification via Attribute\n  Preserved Synthesis", "comments": "This work has been accepted by the 9th IEEE International Conference\n  on Biometrics: Theory, Applications, and Systems (BTAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal to visible face verification is a challenging problem due to the\nlarge domain discrepancy between the modalities. Existing approaches either\nattempt to synthesize visible faces from thermal faces or extract robust\nfeatures from these modalities for cross-modal matching. In this paper, we take\na different approach in which we make use of the attributes extracted from the\nvisible image to synthesize the attribute-preserved visible image from the\ninput thermal image for cross-modal matching. A pre-trained VGG-Face network is\nused to extract the attributes from the visible image. Then, a novel Attribute\nPreserved Generative Adversarial Network (AP-GAN) is proposed to synthesize the\nvisible image from the thermal image guided by the extracted attributes.\nFinally, a deep network is used to extract features from the synthesized image\nand the input visible image for verification. Extensive experiments on the ARL\nPolarimetric face dataset show that the proposed method achieves significant\nimprovements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:38:33 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Di", "Xing", ""], ["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1901.00893", "submitter": "Horia Porav", "authors": "Horia Porav, Tom Bruls and Paul Newman", "title": "I Can See Clearly Now : Image Restoration via De-Raining", "comments": "Submitted to ICRA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for improving segmentation tasks on images affected by\nadherent rain drops and streaks. We introduce a novel stereo dataset recorded\nusing a system that allows one lens to be affected by real water droplets while\nkeeping the other lens clear. We train a denoising generator using this dataset\nand show that it is effective at removing the effect of real water droplets, in\nthe context of image reconstruction and road marking segmentation. To further\ntest our de-noising approach, we describe a method of adding computer-generated\nadherent water droplets and streaks to any images, and use this technique as a\nproxy to demonstrate the effectiveness of our model in the context of general\nsemantic segmentation. We benchmark our results using the CamVid road marking\nsegmentation dataset, Cityscapes semantic segmentation datasets and our own\nreal-rain dataset, and show significant improvement on all tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:45:39 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Porav", "Horia", ""], ["Bruls", "Tom", ""], ["Newman", "Paul", ""]]}, {"id": "1901.00898", "submitter": "Horia Porav", "authors": "Horia Porav and Paul Newman", "title": "Imminent Collision Mitigation with Reinforcement Learning and Vision", "comments": "Presented at ITSC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the role of reinforcement learning in reducing the\nseverity of on-road collisions by controlling velocity and steering in\nsituations in which contact is imminent. We construct a model, given camera\nimages as input, that is capable of learning and predicting the dynamics of\nobstacles, cars and pedestrians, and train our policy using this model. Two\npolicies that control both braking and steering are compared against a baseline\nwhere the only action taken is (conventional) braking in a straight line. The\ntwo policies are trained using two distinct reward structures, one where any\nand all collisions incur a fixed penalty, and a second one where the penalty is\ncalculated based on already established delta-v models of injury severity. The\nresults show that both policies exceed the performance of the baseline, with\nthe policy trained using injury models having the highest performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:09:40 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Porav", "Horia", ""], ["Newman", "Paul", ""]]}, {"id": "1901.00927", "submitter": "Seungchul Ryu Dr.", "authors": "Seungchul Ryu", "title": "Local Area Transform for Cross-Modality Correspondence Matching and Deep\n  Scene Recognition", "comments": "Doctoral Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing correspondences is a fundamental task in variety of image\nprocessing and computer vision applications. In particular, finding the\ncorrespondences between a non-linearly deformed image pair induced by different\nmodality conditions is a challenging problem. This paper describes a efficient\nbut powerful image transform called local area transform (LAT) for\nmodality-robust correspondence estimation. Specifically, LAT transforms an\nimage from the intensity domain to the local area domain, which is invariant\nunder nonlinear intensity deformations, especially radiometric, photometric,\nand spectral deformations. In addition, robust feature descriptors are\nreformulated with LAT for several practical applications. Furthermore,\nLAT-convolution layer and Aception block are proposed and, with these novel\ncomponents, deep neural network called LAT-Net is proposed especially for scene\nrecognition task. Experimental results show that LATransformed images provide a\nconsistency for nonlinearly deformed images, even under random intensity\ndeformations. LAT reduces the mean absolute difference as compared to\nconventional methods. Furthermore, the reformulation of descriptors with LAT\nshows superiority to conventional methods, which is a promising result for the\ntasks of cross-spectral and modality correspondence matching. the local area\ncan be considered as an alternative domain to the intensity domain to achieve\nrobust correspondence matching, image recognition, and a lot of applications:\nsuch as feature matching, stereo matching, dense correspondence matching, image\nrecognition, and image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 22:12:20 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Ryu", "Seungchul", ""]]}, {"id": "1901.00976", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Lu Jiang, Yi Yang, Alexander G Hauptmann", "title": "Contrastive Adaptation Network for Unsupervised Domain Adaptation", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) makes predictions for the target domain\ndata while manual annotations are only available in the source domain. Previous\nmethods minimize the domain discrepancy neglecting the class information, which\nmay lead to misalignment and poor generalization performance. To address this\nissue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a\nnew metric which explicitly models the intra-class domain discrepancy and the\ninter-class domain discrepancy. We design an alternating update strategy for\ntraining CAN in an end-to-end manner. Experiments on two real-world benchmarks\nOffice-31 and VisDA-2017 demonstrate that CAN performs favorably against the\nstate-of-the-art methods and produces more discriminative features.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 03:58:10 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 21:19:04 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kang", "Guoliang", ""], ["Jiang", "Lu", ""], ["Yang", "Yi", ""], ["Hauptmann", "Alexander G", ""]]}, {"id": "1901.00979", "submitter": "Alisha Sharma", "authors": "Alisha Sharma and Jonathan Ventura", "title": "Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic\n  Video", "comments": "Accepted to IEEE AIVR 2019", "journal-ref": null, "doi": "10.1109/aivr46125.2019.00018", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a convolutional neural network model for unsupervised learning\nof depth and ego-motion from cylindrical panoramic video. Panoramic depth\nestimation is an important technology for applications such as virtual reality,\n3D modeling, and autonomous robotic navigation. In contrast to previous\napproaches for applying convolutional neural networks to panoramic imagery, we\nuse the cylindrical panoramic projection which allows for the use of the\ntraditional CNN layers such as convolutional filters and max pooling without\nmodification. Our evaluation of synthetic and real data shows that unsupervised\nlearning of depth and ego-motion on cylindrical panoramic images can produce\nhigh-quality depth maps and that an increased field-of-view improves ego-motion\nestimation accuracy. We also introduce Headcam, a novel dataset of panoramic\nvideo collected from a helmet-mounted camera while biking in an urban setting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 04:09:28 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:18:07 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sharma", "Alisha", ""], ["Ventura", "Jonathan", ""]]}, {"id": "1901.01015", "submitter": "Ratnesh Kumar", "authors": "Ratnesh Kumar, Edwin Weill, Farzin Aghdasi, Parthsarathy Sriram", "title": "Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding", "comments": "Accepted at IJCNN 2019. This arxiv version adds result on newer\n  datasets post conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of vehicle re-identification in a camera\nnetwork utilizing triplet embeddings. Re-identification is the problem of\nmatching appearances of objects across different cameras. With the\nproliferation of surveillance cameras enabling smart and safer cities, there is\nan ever-increasing need to re-identify vehicles across cameras. Typical\nchallenges arising in smart city scenarios include variations of viewpoints,\nillumination and self occlusions. Most successful approaches for\nre-identification involve (deep) learning an embedding space such that the\nvehicles of same identities are projected closer to one another, compared to\nthe vehicles representing different identities. Popular loss functions for\nlearning an embedding (space) include contrastive or triplet loss. In this\npaper we provide an extensive evaluation of these losses applied to vehicle\nre-identification and demonstrate that using the best practices for learning\nembeddings outperform most of the previous approaches proposed in the vehicle\nre-identification literature. Compared to most existing state-of-the-art\napproaches, our approach is simpler and more straightforward for training\nutilizing only identity-level annotations, along with one of the smallest\npublished embedding dimensions for efficient inference. Furthermore in this\nwork we introduce a formal evaluation of a triplet sampling variant (batch\nsample) into the re-identification literature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 08:13:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 01:44:17 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 18:43:01 GMT"}, {"version": "v4", "created": "Thu, 8 Aug 2019 20:03:45 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kumar", "Ratnesh", ""], ["Weill", "Edwin", ""], ["Aghdasi", "Farzin", ""], ["Sriram", "Parthsarathy", ""]]}, {"id": "1901.01021", "submitter": "Rongrong Ma", "authors": "Rongrong Ma, Jianyu Miao, Lingfeng Niu, Peng Zhang", "title": "Transformed $\\ell_1$ Regularization for Learning Sparse Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved extraordinary success in numerous\nareas. However, to attain this success, DNNs often carry a large number of\nweight parameters, leading to heavy costs of memory and computation resources.\nOverfitting is also likely to happen in such network when the training data are\ninsufficient. These shortcomings severely hinder the application of DNNs in\nresource-constrained platforms. In fact, many network weights are known to be\nredundant and can be removed from the network without much loss of performance.\nTo this end, we introduce a new non-convex integrated transformed $\\ell_1$\nregularizer to promote sparsity for DNNs, which removes both redundant\nconnections and unnecessary neurons simultaneously. To be specific, we apply\nthe transformed $\\ell_1$ to the matrix space of network weights and utilize it\nto remove redundant connections. Besides, group sparsity is also employed as an\nauxiliary to remove unnecessary neurons. An efficient stochastic proximal\ngradient algorithm is presented to solve the new model at the same time. To the\nbest of our knowledge, this is the first work to utilize a non-convex\nregularizer in sparse optimization based method to promote sparsity for DNNs.\nExperiments on several public datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 08:46:16 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Ma", "Rongrong", ""], ["Miao", "Jianyu", ""], ["Niu", "Lingfeng", ""], ["Zhang", "Peng", ""]]}, {"id": "1901.01028", "submitter": "Mateusz Trokielewicz", "authors": "Daniel Kerrigan, Mateusz Trokielewicz, Adam Czajka, Kevin Bowyer", "title": "Iris Recognition with Image Segmentation Employing Retrained\n  Off-the-Shelf Deep Neural Networks", "comments": "Paper submitted for the IEEE International Conference on Biometrics\n  (ICB2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers three new, open-source, deep learning-based iris\nsegmentation methods, and the methodology how to use irregular segmentation\nmasks in a conventional Gabor-wavelet-based iris recognition. To train and\nvalidate the methods, we used a wide spectrum of iris images acquired by\ndifferent teams and different sensors and offered publicly, including data\ntaken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS,\nWarsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), and\nND-TWINS-2009-2010 (iris images acquired from identical twins). This varied\ntraining data should increase the generalization capabilities of the proposed\nsegmentation techniques. In database-disjoint training and testing, we show\nthat deep learning-based segmentation outperforms the conventional (OSIRIS)\nsegmentation in terms of Intersection over Union calculated between the\nobtained results and manually annotated ground-truth. Interestingly, the\nGabor-based iris matching is not always better when deep learning-based\nsegmentation is used, and is on par with the method employing Daugman's based\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 09:20:13 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Kerrigan", "Daniel", ""], ["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin", ""]]}, {"id": "1901.01034", "submitter": "Tomasz Konopczy\\'nski", "authors": "Tomasz Konopczy\\'nski, Thorben Kr\\\"oger, Lei Zheng, J\\\"urgen Hesser", "title": "Instance Segmentation of Fibers from Low Resolution CT Scans via 3D Deep\n  Embedding Learning", "comments": "Accepted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for automatic extraction (instance segmentation)\nof fibers from low resolution 3D X-ray computed tomography scans of short glass\nfiber reinforced polymers. We have designed a 3D instance segmentation\narchitecture built upon a deep fully convolutional network for semantic\nsegmentation with an extra output for embedding learning. We show that the\nembedding learning is capable of learning a mapping of voxels to an embedded\nspace in which a standard clustering algorithm can be used to distinguish\nbetween different instances of an object in a volume. In addition, we discuss a\nmerging post-processing method which makes it possible to process volumes of\nany size. The proposed 3D instance segmentation network together with our\nmerging algorithm is the first known to authors knowledge procedure that\nproduces results good enough, that they can be used for further analysis of low\nresolution fiber composites CT scans.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 09:53:07 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Konopczy\u0144ski", "Tomasz", ""], ["Kr\u00f6ger", "Thorben", ""], ["Zheng", "Lei", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1901.01049", "submitter": "Qing Li", "authors": "Qing Li, Jiasong Zhu, Rui Cao, Ke Sun, Jonathan M. Garibaldi, Qingquan\n  Li, Bozhi Liu and Guoping Qiu", "title": "Relative Geometry-Aware Siamese Neural Network for 6DOF Camera\n  Relocalization", "comments": null, "journal-ref": "Neurocomputing 2020", "doi": "10.1016/j.neucom.2020.09.071", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6DOF camera relocalization is an important component of autonomous driving\nand navigation. Deep learning has recently emerged as a promising technique to\ntackle this problem. In this paper, we present a novel relative geometry-aware\nSiamese neural network to enhance the performance of deep learning-based\nmethods through explicitly exploiting the relative geometry constraints between\nimages. We perform multi-task learning and predict the absolute and relative\nposes simultaneously. We regularize the shared-weight twin networks in both the\npose and feature domains to ensure that the estimated poses are globally as\nwell as locally correct. We employ metric learning and design a novel adaptive\nmetric distance loss to learn a feature that is capable of distinguishing poses\nof visually similar images from different locations. We evaluate the proposed\nmethod on public indoor and outdoor benchmarks and the experimental results\ndemonstrate that our method can significantly improve localization performance.\nFurthermore, extensive ablation evaluations are conducted to demonstrate the\neffectiveness of different terms of the loss function.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 10:54:55 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 02:37:04 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Qing", ""], ["Zhu", "Jiasong", ""], ["Cao", "Rui", ""], ["Sun", "Ke", ""], ["Garibaldi", "Jonathan M.", ""], ["Li", "Qingquan", ""], ["Liu", "Bozhi", ""], ["Qiu", "Guoping", ""]]}, {"id": "1901.01060", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J.\n  Mitra, Maks Ovsjanikov", "title": "PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point\n  Clouds", "comments": null, "journal-ref": "Computer Graphics Forum, 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds obtained with 3D scanners or by image-based reconstruction\ntechniques are often corrupted with significant amount of noise and outliers.\nTraditional methods for point cloud denoising largely rely on local surface\nfitting (e.g., jets or MLS surfaces), local or non-local averaging, or on\nstatistical assumptions about the underlying noise model. In contrast, we\ndevelop a simple data-driven method for removing outliers and reducing noise in\nunordered point clouds. We base our approach on a deep learning architecture\nadapted from PCPNet, which was recently proposed for estimating local 3D shape\nproperties in point clouds. Our method first classifies and discards outlier\nsamples, and then estimates correction vectors that project noisy points onto\nthe original clean surfaces. The approach is efficient and robust to varying\namounts of noise and outliers, while being able to handle large densely-sampled\npoint clouds. In our extensive evaluation, both on synthesic and real data, we\nshow an increased robustness to strong noise levels compared to various\nstate-of-the-art methods, enabling accurate surface reconstruction from\nextremely noisy real data obtained by range scans. Finally, the simplicity and\nuniversality of our approach makes it very easy to integrate in any existing\ngeometry processing pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 11:28:26 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 16:29:25 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 15:22:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["La Barbera", "Vittorio", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1901.01091", "submitter": "Thomas Lucas", "authors": "Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Schmid,\n  Jakob Verbeek", "title": "Adaptive Density Estimation for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of generative models has seen tremendous progress over\nrecent years, in particular due to generative adversarial networks (GANs),\nvariational autoencoders, and flow-based models. GANs have dramatically\nimproved sample quality, but suffer from two drawbacks: (i) they mode-drop,\ni.e., do not cover the full support of the train data, and (ii) they do not\nallow for likelihood evaluations on held-out data. In contrast,\nlikelihood-based training encourages models to cover the full support of the\ntrain data, but yields poorer samples. These mutual shortcomings can in\nprinciple be addressed by training generative latent variable models in a\nhybrid adversarial-likelihood manner. However, we show that commonly made\nparametric assumptions create a conflict between them, making successful hybrid\nmodels non trivial. As a solution, we propose to use deep invertible\ntransformations in the latent variable decoder. This approach allows for\nlikelihood computations in image space, is more efficient than fully invertible\nmodels, and can take full advantage of adversarial training. We show that our\nmodel significantly improves over existing hybrid models: offering GAN-like\nsamples, IS and FID scores that are competitive with fully adversarial models,\nand improved likelihood scores.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 13:43:18 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:47:20 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2020 15:03:37 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Lucas", "Thomas", ""], ["Shmelkov", "Konstantin", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1901.01138", "submitter": "Xiaohui Huang", "authors": "Xiaohui Huang, Pan He, Anand Rangarajan and Sanjay Ranka", "title": "Intelligent Intersection: Two-Stream Convolutional Networks for\n  Real-time Near Accident Detection in Traffic Video", "comments": "Submitted to ACM Transactions on Spatial Algorithms and Systems\n  (TSAS); Special issue on Urban Mobility: Algorithms and Systems. arXiv admin\n  note: text overlap with arXiv:1703.07402 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Intelligent Transportation System, real-time systems that monitor and\nanalyze road users become increasingly critical as we march toward the smart\ncity era. Vision-based frameworks for Object Detection, Multiple Object\nTracking, and Traffic Near Accident Detection are important applications of\nIntelligent Transportation System, particularly in video surveillance and etc.\nAlthough deep neural networks have recently achieved great success in many\ncomputer vision tasks, a uniformed framework for all the three tasks is still\nchallenging where the challenges multiply from demand for real-time\nperformance, complex urban setting, highly dynamic traffic event, and many\ntraffic movements. In this paper, we propose a two-stream Convolutional Network\narchitecture that performs real-time detection, tracking, and near accident\ndetection of road users in traffic video data. The two-stream model consists of\na spatial stream network for Object Detection and a temporal stream network to\nleverage motion features for Multiple Object Tracking. We detect near accidents\nby incorporating appearance features and motion features from two-stream\nnetworks. Using aerial videos, we propose a Traffic Near Accident Dataset\n(TNAD) covering various types of traffic interactions that is suitable for\nvision-based traffic analysis tasks. Our experiments demonstrate the advantage\nof our framework with an overall competitive qualitative and quantitative\nperformance at high frame rates on the TNAD dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 15:05:32 GMT"}], "update_date": "2019-06-02", "authors_parsed": [["Huang", "Xiaohui", ""], ["He", "Pan", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1901.01151", "submitter": "Suraj Kothawade", "authors": "Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav\n  Doctor, Ganesh Ramakrishnan", "title": "Learning From Less Data: A Unified Data Subset Selection and Active\n  Learning Framework for Computer Vision", "comments": "Accepted to WACV 2019. arXiv admin note: substantial text overlap\n  with arXiv:1805.11191", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning based state-of-the-art computer vision techniques\nare in general data hungry. Their data curation poses the challenges of\nexpensive human labeling, inadequate computing resources and larger experiment\nturn around times. Training data subset selection and active learning\ntechniques have been proposed as possible solutions to these challenges. A\nspecial class of subset selection functions naturally model notions of\ndiversity, coverage and representation and can be used to eliminate redundancy\nthus lending themselves well for training data subset selection. They can also\nhelp improve the efficiency of active learning in further reducing human\nlabeling efforts by selecting a subset of the examples obtained using the\nconventional uncertainty sampling based techniques. In this work, we\nempirically demonstrate the effectiveness of two diversity models, namely the\nFacility-Location and Dispersion models for training-data subset selection and\nreducing labeling effort. We demonstrate this across the board for a variety of\ncomputer vision tasks including Gender Recognition, Face Recognition, Scene\nRecognition, Object Detection and Object Recognition. Our results show that\ndiversity based subset selection done in the right way can increase the\naccuracy by upto 5 - 10% over existing baselines, particularly in settings in\nwhich less training data is available. This allows the training of complex\nmachine learning models like Convolutional Neural Networks with much less\ntraining data and labeling costs while incurring minimal performance loss.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 14:07:08 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Kaushal", "Vishal", ""], ["Iyer", "Rishabh", ""], ["Kothawade", "Suraj", ""], ["Mahadev", "Rohan", ""], ["Doctor", "Khoshrav", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1901.01153", "submitter": "Suraj Kothawade", "authors": "Vishal Kaushal, Rishabh Iyer, Khoshrav Doctor, Anurag Sahoo, Pratik\n  Dubal, Suraj Kothawade, Rohan Mahadev, Kunal Dargan, Ganesh Ramakrishnan", "title": "Demystifying Multi-Faceted Video Summarization: Tradeoff Between\n  Diversity,Representation, Coverage and Importance", "comments": "Accepted to WACV 2019. arXiv admin note: substantial text overlap\n  with arXiv:1704.01466, arXiv:1809.08846", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses automatic summarization of videos in a unified manner.\nIn particular, we propose a framework for multi-faceted summarization for\nextractive, query base and entity summarization (summarization at the level of\nentities like objects, scenes, humans and faces in the video). We investigate\nseveral summarization models which capture notions of diversity, coverage,\nrepresentation and importance, and argue the utility of these different models\ndepending on the application. While most of the prior work on submodular\nsummarization approaches has focused oncombining several models and learning\nweighted mixtures, we focus on the explainability of different models and\nfeaturizations, and how they apply to different domains. We also provide\nimplementation details on summarization systems and the different modalities\ninvolved. We hope that the study from this paper will give insights into\npractitioners to appropriately choose the right summarization models for the\nproblems at hand.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 14:03:37 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Kaushal", "Vishal", ""], ["Iyer", "Rishabh", ""], ["Doctor", "Khoshrav", ""], ["Sahoo", "Anurag", ""], ["Dubal", "Pratik", ""], ["Kothawade", "Suraj", ""], ["Mahadev", "Rohan", ""], ["Dargan", "Kunal", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1901.01210", "submitter": "Tomasz Konopczy\\'nski", "authors": "Tomasz Konopczy\\'nski, Jitendra Rathore, Thorben Kr\\\"oger, Lei Zheng,\n  Christoph S. Garbe, Simone Carmignato, J\\\"urgen Hesser", "title": "Reference Setup for Quantitative Comparison of Segmentation Techniques\n  for Short Glass Fiber CT Data", "comments": "Accepted to 7th Conference on Industrial Computed Tomography, Leuven,\n  Belgium (iCT 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing different algorithms for segmenting glass fibers in industrial\ncomputed tomography (CT) scans is difficult due to the absence of a standard\nreference dataset. In this work, we introduce a set of annotated scans of\nshort-fiber reinforced polymers (SFRP) as well as synthetically created CT\nvolume data together with the evaluation metrics. We suggest both the metrics\nand this data set as a reference for studying the performance of different\nalgorithms. The real scans were acquired by a Nikon MCT225 X-ray CT system. The\nsimulated scans were created by the use of an in-house computational model and\nthird-party commercial software. For both types of data, corresponding ground\ntruth annotations have been prepared, including hand annotations for the real\nscans and STL models for the synthetic scans. Additionally, a Hessian-based\nFrangi vesselness filter for fiber segmentation has been implemented and\nopen-sourced to serve as a reference for comparisons.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 16:57:47 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Konopczy\u0144ski", "Tomasz", ""], ["Rathore", "Jitendra", ""], ["Kr\u00f6ger", "Thorben", ""], ["Zheng", "Lei", ""], ["Garbe", "Christoph S.", ""], ["Carmignato", "Simone", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1901.01211", "submitter": "Tomasz Konopczy\\'nski", "authors": "Tomasz Konopczy\\'nski, Danish Rathore, Jitendra Rathore, Thorben\n  Kr\\\"oger, Lei Zheng, Christoph S. Garbe, Simone Carmignato, J\\\"urgen Hesser", "title": "Fully Convolutional Deep Network Architectures for Automatic Short Glass\n  Fiber Semantic Segmentation from CT scans", "comments": "Accepted to 8th Conference on Industrial Computed Tomography, Wels,\n  Austria (iCT 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first attempt to perform short glass fiber semantic\nsegmentation from X-ray computed tomography volumetric datasets at medium (3.9\n{\\mu}m isotropic) and low (8.3 {\\mu}m isotropic) resolution using deep learning\narchitectures. We performed experiments on both synthetic and real CT scans and\nevaluated deep fully convolutional architectures with both 2D and 3D kernels.\nOur artificial neural networks outperform existing methods at both medium and\nlow resolution scans.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 17:00:54 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Konopczy\u0144ski", "Tomasz", ""], ["Rathore", "Danish", ""], ["Rathore", "Jitendra", ""], ["Kr\u00f6ger", "Thorben", ""], ["Zheng", "Lei", ""], ["Garbe", "Christoph S.", ""], ["Carmignato", "Simone", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1901.01223", "submitter": "Xurong Li", "authors": "Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu,\n  and Chunming Wu", "title": "Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been broadly leveraged by major cloud providers, such as\nGoogle, AWS and Baidu, to offer various computer vision related services\nincluding image classification, object identification, illegal image detection,\netc. While recent works extensively demonstrated that deep learning\nclassification models are vulnerable to adversarial examples, cloud-based image\ndetection models, which are more complicated than classifiers, may also have\nsimilar security concern but not get enough attention yet. In this paper, we\nmainly focus on the security issues of real-world cloud-based image detectors.\nSpecifically, (1) based on effective semantic segmentation, we propose four\nattacks to generate semantics-aware adversarial examples via only interacting\nwith black-box APIs; and (2) we make the first attempt to conduct an extensive\nempirical study of black-box attacks against real-world cloud-based image\ndetectors. Through the comprehensive evaluations on five major cloud platforms:\nAWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that\nour image processing based attacks can reach a success rate of approximately\n100%, and the semantic segmentation based attacks have a success rate over 90%\namong different detection services, such as violence, politician, and\npornography detection. We also proposed several possible defense strategies for\nthese security challenges in the real-life situation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 17:34:13 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 07:17:44 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 13:43:33 GMT"}, {"version": "v4", "created": "Sat, 14 Sep 2019 14:30:25 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Li", "Xurong", ""], ["Ji", "Shouling", ""], ["Han", "Meng", ""], ["Ji", "Juntao", ""], ["Ren", "Zhenyu", ""], ["Liu", "Yushan", ""], ["Wu", "Chunming", ""]]}, {"id": "1901.01238", "submitter": "Shusil Dangi", "authors": "Shusil Dangi, Cristian Linte, and Ziv Yaniv", "title": "A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation", "comments": "11 pages manuscript, 5 pages supplementary materials", "journal-ref": null, "doi": "10.1002/mp.13853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac image segmentation is a critical process for generating personalized\nmodels of the heart and for quantifying cardiac performance parameters. Several\nconvolutional neural network (CNN) architectures have been proposed to segment\nthe heart chambers from cardiac cine MR images. Here we propose a multi-task\nlearning (MTL)-based regularization framework for cardiac MR image\nsegmentation. The network is trained to perform the main task of semantic\nsegmentation, along with a simultaneous, auxiliary task of pixel-wise distance\nmap regression. The proposed distance map regularizer is a decoder network\nadded to the bottleneck layer of an existing CNN architecture, facilitating the\nnetwork to learn robust global features. The regularizer block is removed after\ntraining, so that the original number of network parameters does not change. We\nshow that the proposed regularization method improves both binary and\nmulti-class segmentation performance over the corresponding state-of-the-art\nCNN architectures on two publicly available cardiac cine MRI datasets,\nobtaining average dice coefficient of 0.84$\\pm$0.03 and 0.91$\\pm$0.04,\nrespectively. Furthermore, we also demonstrate improved generalization\nperformance of the distance map regularized network on cross-dataset\nsegmentation, showing as much as 42% improvement in myocardium Dice coefficient\nfrom 0.56$\\pm$0.28 to 0.80$\\pm$0.14.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 18:24:52 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 20:02:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Dangi", "Shusil", ""], ["Linte", "Cristian", ""], ["Yaniv", "Ziv", ""]]}, {"id": "1901.01255", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Benjamin Busam and Nassir Navab and Slobodan Ilic and\n  Peter Sturm", "title": "Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric\n  Fits", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). arXiv admin note: substantial text overlap with\n  arXiv:1803.07191", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and effective method for detecting 3D primitives in\ncluttered, unorganized point clouds, without axillary segmentation or type\nspecification. We consider the quadric surfaces for encapsulating the basic\nbuilding blocks of our environments - planes, spheres, ellipsoids, cones or\ncylinders, in a unified fashion. Moreover, quadrics allow us to model higher\ndegree of freedom shapes, such as hyperboloids or paraboloids that could be\nused in non-rigid settings.\n  We begin by contributing two novel quadric fits targeting 3D point sets that\nare endowed with tangent space information. Based upon the idea of aligning the\nquadric gradients with the surface normals, our first formulation is exact and\nrequires as low as four oriented points. The second fit approximates the first,\nand reduces the computational effort. We theoretically analyze these fits with\nrigor, and give algebraic and geometric arguments. Next, by re-parameterizing\nthe solution, we devise a new local Hough voting scheme on the null-space\ncoefficients that is combined with RANSAC, reducing the complexity from\n$O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the\nfirst method capable of performing a generic cross-type multi-object primitive\ndetection in difficult scenes without segmentation. Our extensive qualitative\nand quantitative results show that our method is efficient and flexible, as\nwell as being accurate.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:09:50 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Birdal", "Tolga", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Sturm", "Peter", ""]]}, {"id": "1901.01336", "submitter": "Max Robinson", "authors": "Max Robinson", "title": "Projective Decomposition and Matrix Equivalence up to Scale", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data matrix may be seen simply as a means of organizing observations into\nrows ( e.g., by measured object) and into columns ( e.g., by measured variable)\nso that the observations can be analyzed with mathematical tools. As a\nmathematical object, a matrix defines a linear mapping between points\nrepresenting weighted combinations of its rows (the row vector space) and\npoints representing weighted combinations of its columns (the column vector\nspace). From this perspective, a data matrix defines a relationship between the\ninformation that labels its rows and the information that labels its columns,\nand numerical methods are used to analyze this relationship. A first step is to\nnormalize the data, transforming each observation from scales convenient for\nmeasurement to a common scale, on which addition and multiplication can\nmeaningfully combine the different observations. For example, z-transformation\nrescales every variable to the same scale, standardized variation from an\nexpected value, but ignores scale differences between measured objects. Here we\ndevelop the concepts and properties of projective decomposition, which applies\nthe same normalization strategy to both rows and columns by separating the\nmatrix into row- and column-scaling factors and a scale-normalized matrix. We\nshow that different scalings of the same scale-normalized matrix form an\nequivalence class, and call the scale-normalized, canonical member of the class\nits scale-invariant form that preserves all pairwise relative ratios.\nProjective decomposition therefore provides a means of normalizing the broad\nclass of ratio-scale data, in which relative ratios are of primary interest,\nonto a common scale without altering the ratios of interest, and simultaneously\naccounting for scale effects for both organizations of the matrix values. Both\nof these properties distinguish it from z-transformation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:47:42 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Robinson", "Max", ""]]}, {"id": "1901.01342", "submitter": "Sourish Chaudhuri", "authors": "Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew\n  Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia\n  Schmid, Zhonghua Xi, Caroline Pantofaru", "title": "AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active speaker detection is an important component in video analysis\nalgorithms for applications such as speaker diarization, video re-targeting for\nmeetings, speech enhancement, and human-robot interaction. The absence of a\nlarge, carefully labeled audio-visual dataset for this task has constrained\nalgorithm evaluations with respect to data diversity, environments, and\naccuracy. This has made comparisons and improvements difficult. In this paper,\nwe present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) that\nwill be released publicly to facilitate algorithm development and enable\ncomparisons. The dataset contains temporally labeled face tracks in video,\nwhere each face instance is labeled as speaking or not, and whether the speech\nis audible. This dataset contains about 3.65 million human labeled frames or\nabout 38.5 hours of face tracks, and the corresponding audio. We also present a\nnew audio-visual approach for active speaker detection, and analyze its\nperformance, demonstrating both its strength and the contributions of the\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 00:01:06 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 01:28:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Roth", "Joseph", ""], ["Chaudhuri", "Sourish", ""], ["Klejch", "Ondrej", ""], ["Marvin", "Radhika", ""], ["Gallagher", "Andrew", ""], ["Kaver", "Liat", ""], ["Ramaswamy", "Sharadh", ""], ["Stopczynski", "Arkadiusz", ""], ["Schmid", "Cordelia", ""], ["Xi", "Zhonghua", ""], ["Pantofaru", "Caroline", ""]]}, {"id": "1901.01369", "submitter": "Ningning Wang", "authors": "Ningning Wang, Xiaojin Gong", "title": "Adaptive Fusion for RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D salient object detection aims to identify the most visually distinctive\nobjects in a pair of color and depth images. Based upon an observation that\nmost of the salient objects may stand out at least in one modality, this paper\nproposes an adaptive fusion scheme to fuse saliency predictions generated from\ntwo modalities. Specifically, we design a two-streamed convolutional neural\nnetwork (CNN), each of which extracts features and predicts a saliency map from\neither RGB or depth modality. Then, a saliency fusion module learns a switch\nmap that is used to adaptively fuse the predicted saliency maps. A loss\nfunction composed of saliency supervision, switch map supervision, and\nedge-preserving constraints is designed to make full supervision, and the\nentire network is trained in an end-to-end manner. Benefited from the adaptive\nfusion strategy and the edge-preserving constraint, our approach outperforms\nstate-of-the-art methods on three publicly available datasets.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 05:16:46 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 09:05:58 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Wang", "Ningning", ""], ["Gong", "Xiaojin", ""]]}, {"id": "1901.01370", "submitter": "Jian Wang", "authors": "Jian Wang, Tianfan Xue, Jonathan T. Barron, Jiawen Chen", "title": "Stereoscopic Dark Flash for Low-light Photography", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a camera configuration for acquiring \"stereoscopic\ndark flash\" images: a simultaneous stereo pair in which one camera is a\nconventional RGB sensor, but the other camera is sensitive to near-infrared and\nnear-ultraviolet instead of R and B. When paired with a \"dark\" flash (i.e., one\nhaving near-infrared and near-ultraviolet light, but no visible light) this\ncamera allows us to capture the two images in a flash/no-flash image pair at\nthe same time, all while not disturbing any human subjects or onlookers with a\ndazzling visible flash. We present a hardware prototype of this camera that\napproximates an idealized camera, and we present an imaging procedure that let\nus acquire dark flash stereo pairs that closely resemble those we would get\nfrom that idealized camera. We then present a technique for fusing these stereo\npairs, first by performing registration and warping, and then by using recent\nadvances in hyperspectral image fusion and deep learning to produce a final\nimage. Because our camera configuration and our data acquisition process allow\nus to capture true low-noise long exposure RGB images alongside our dark flash\nstereo pairs, our learned model can be trained end-to-end to produce a fused\nimage that retains the color and tone of a real RGB image while having the\nlow-noise properties of a flash image.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 05:26:27 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 04:25:37 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Wang", "Jian", ""], ["Xue", "Tianfan", ""], ["Barron", "Jonathan T.", ""], ["Chen", "Jiawen", ""]]}, {"id": "1901.01381", "submitter": "Jiong Wu", "authors": "Jiong Wu and Xiaoying Tang", "title": "Brain segmentation based on multi-atlas guided 3D fully convolutional\n  network ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we proposed and validated a multi-atlas guided 3D fully\nconvolutional network (FCN) ensemble model (M-FCN) for segmenting brain regions\nof interest (ROIs) from structural magnetic resonance images (MRIs). One major\nlimitation of existing state-of-the-art 3D FCN segmentation models is that they\noften apply image patches of fixed size throughout training and testing, which\nmay miss some complex tissue appearance patterns of different brain ROIs. To\naddress this limitation, we trained a 3D FCN model for each ROI using patches\nof adaptive size and embedded outputs of the convolutional layers in the\ndeconvolutional layers to further capture the local and global context\npatterns. In addition, with an introduction of multi-atlas based guidance in\nM-FCN, our segmentation was generated by combining the information of images\nand labels, which is highly robust. To reduce over-fitting of the FCN model on\nthe training data, we adopted an ensemble strategy in the learning procedure.\nEvaluation was performed on two brain MRI datasets, aiming respectively at\nsegmenting 14 subcortical and ventricular structures and 54 brain ROIs. The\nsegmentation results of the proposed method were compared with those of a\nstate-of-the-art multi-atlas based segmentation method and an existing 3D FCN\nsegmentation model. Our results suggested that the proposed method had a\nsuperior segmentation performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 08:23:48 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wu", "Jiong", ""], ["Tang", "Xiaoying", ""]]}, {"id": "1901.01415", "submitter": "Dengxin Dai", "authors": "Dengxin Dai, Christos Sakaridis, Simon Hecker, Luc Van Gool", "title": "Curriculum Model Adaptation with Synthetic and Real Data for Semantic\n  Foggy Scene Understanding", "comments": "accepted by IJCV (22 pages), an extension of arXiv:1808.01265 our\n  eccv18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of semantic scene understanding under fog.\nAlthough marked progress has been made in semantic scene understanding, it is\nmainly concentrated on clear-weather scenes. Extending semantic segmentation\nmethods to adverse weather conditions such as fog is crucial for outdoor\napplications. In this paper, we propose a novel method, named Curriculum Model\nAdaptation (CMAda), which gradually adapts a semantic segmentation model from\nlight synthetic fog to dense real fog in multiple steps, using both labeled\nsynthetic foggy data and unlabeled real foggy data. The method is based on the\nfact that the results of semantic segmentation in moderately adverse conditions\n(light fog) can be bootstrapped to solve the same problem in highly adverse\nconditions (dense fog). CMAda is extensible to other adverse conditions and\nprovides a new paradigm for learning with synthetic data and unlabeled real\ndata. In addition, we present three other main stand-alone contributions: 1) a\nnovel method to add synthetic fog to real, clear-weather scenes using semantic\ninput; 2) a new fog density estimator; 3) a novel fog densification method to\ndensify the fog in real foggy scenes without using depth; and 4) the Foggy\nZurich dataset comprising 3808 real foggy images, with pixel-level semantic\nannotations for 40 images under dense fog. Our experiments show that 1) our fog\nsimulation and fog density estimator outperform their state-of-the-art\ncounterparts with respect to the task of semantic foggy scene understanding\n(SFSU); 2) CMAda improves the performance of state-of-the-art models for SFSU\nsignificantly, benefiting both from our synthetic and real foggy data. The\ndatasets and code are available at the project website.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 13:38:53 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 12:17:57 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Dai", "Dengxin", ""], ["Sakaridis", "Christos", ""], ["Hecker", "Simon", ""], ["Van Gool", "Luc", ""]]}, {"id": "1901.01431", "submitter": "Imad Rida", "authors": "Imad Rida, Lunke Fei, Hugo Proen\\c{c}a, Amine Nait-Ali, and Abdenour\n  Hadid", "title": "Forensic shoe-print identification: a brief survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an advanced research topic in forensics science, automatic shoe-print\nidentification has been extensively studied in the last two decades, since shoe\nmarks are the clues most frequently left in a crime scene. Hence, these\nimpressions provide a pertinent evidence for the proper progress of\ninvestigations in order to identify the potential criminals. The main goal of\nthis survey is to provide a cohesive overview of the research carried out in\nforensic shoe-print identification and its basic background. Apart defining the\nproblem and describing the phases that typically compose the processing chain\nof shoe-print identification, we provide a summary/comparison of the\nstate-of-the-art approaches, in order to guide the neophyte and help to advance\nthe research topic. This is done through introducing simple and basic\ntaxonomies as well as summaries of the state-of-the-art performance. Lastly, we\ndiscuss the current open problems and challenges in this research topic, point\nout for promising directions in this field.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 15:45:00 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 22:04:44 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 15:38:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rida", "Imad", ""], ["Fei", "Lunke", ""], ["Proen\u00e7a", "Hugo", ""], ["Nait-Ali", "Amine", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1901.01449", "submitter": "Hongming Li", "authors": "Hongming Li, Pamela Boimel, James Janopaul-Naylor, Haoyu Zhong, Ying\n  Xiao, Edgar Ben-Josef, Yong Fan", "title": "Deep Convolutional Neural Networks for Imaging Data Based Survival\n  Analysis of Rectal Cancer", "comments": "Accepted by ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent radiomic studies have witnessed promising performance of deep learning\ntechniques in learning radiomic features and fusing multimodal imaging data.\nMost existing deep learning based radiomic studies build predictive models in a\nsetting of pattern classification, not appropriate for survival analysis\nstudies where some data samples have incomplete observations. To improve\nexisting survival analysis techniques whose performance is hinged on imaging\nfeatures, we propose a deep learning method to build survival regression models\nby optimizing imaging features with deep convolutional neural networks (CNNs)\nin a proportional hazards model. To make the CNNs applicable to tumors with\nvaried sizes, a spatial pyramid pooling strategy is adopted. Our method has\nbeen validated based on a simulated imaging dataset and a FDG-PET/CT dataset of\nrectal cancer patients treated for locally advanced rectal cancer. Compared\nwith survival prediction models built upon hand-crafted radiomic features using\nCox proportional hazards model and random survival forests, our method achieved\ncompetitive prediction performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 18:09:38 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Li", "Hongming", ""], ["Boimel", "Pamela", ""], ["Janopaul-Naylor", "James", ""], ["Zhong", "Haoyu", ""], ["Xiao", "Ying", ""], ["Ben-Josef", "Edgar", ""], ["Fan", "Yong", ""]]}, {"id": "1901.01451", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Early Prediction of Alzheimer's Disease Dementia Based on Baseline\n  Hippocampal MRI and 1-Year Follow-Up Cognitive Measures Using Deep Recurrent\n  Neural Networks", "comments": "Accepted by ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal biological, imaging, and neuropsychological markers have\ndemonstrated promising performance for distinguishing Alzheimer's disease (AD)\npatients from cognitively normal elders. However, it remains difficult to early\npredict when and which mild cognitive impairment (MCI) individuals will convert\nto AD dementia. Informed by pattern classification studies which have\ndemonstrated that pattern classifiers built on longitudinal data could achieve\nbetter classification performance than those built on cross-sectional data, we\ndevelop a deep learning model based on recurrent neural networks (RNNs) to\nlearn informative representation and temporal dynamics of longitudinal\ncognitive measures of individual subjects and combine them with baseline\nhippocampal MRI for building a prognostic model of AD dementia progression.\nExperimental results on a large cohort of MCI subjects have demonstrated that\nthe deep learning model could learn informative measures from longitudinal data\nfor characterizing the progression of MCI subjects to AD dementia, and the\nprognostic model could early predict AD progression with high accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 18:19:45 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1901.01474", "submitter": "Zheng Zhang", "authors": "Yujuan Ding, Wai Kueng Wong, Zhihui Lai, Zheng Zhang", "title": "Bilinear Supervised Hashing Based on 2D Image Features", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 23:39:27 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ding", "Yujuan", ""], ["Wong", "Wai Kueng", ""], ["Lai", "Zhihui", ""], ["Zhang", "Zheng", ""]]}, {"id": "1901.01492", "submitter": "Daniel Gordon", "authors": "Daniel Gordon and Dieter Fox and Ali Farhadi", "title": "What Should I Do Now? Marrying Reinforcement Learning and Symbolic\n  Planning", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term planning poses a major difficulty to many reinforcement learning\nalgorithms. This problem becomes even more pronounced in dynamic visual\nenvironments. In this work we propose Hierarchical Planning and Reinforcement\nLearning (HIP-RL), a method for merging the benefits and capabilities of\nSymbolic Planning with the learning abilities of Deep Reinforcement Learning.\nWe apply HIPRL to the complex visual tasks of interactive question answering\nand visual semantic planning and achieve state-of-the-art results on three\nchallenging datasets all while taking fewer steps at test time and training in\nfewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 03:15:15 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Gordon", "Daniel", ""], ["Fox", "Dieter", ""], ["Farhadi", "Ali", ""]]}, {"id": "1901.01493", "submitter": "Huayu Li", "authors": "Huayu Li", "title": "Channel Locality Block: A Variant of Squeeze-and-Excitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism is a hot spot in deep learning field. Using channel\nattention model is an effective method for improving the performance of the\nconvolutional neural network. Squeeze-and-Excitation block takes advantage of\nthe channel dependence, selectively emphasizing the important channels and\ncompressing the relatively useless channel. In this paper, we proposed a\nvariant of SE block based on channel locality. Instead of using full connection\nlayers to explore the global channel dependence, we adopt convolutional layers\nto learn the correlation between the nearby channels. We term this new\nalgorithm Channel Locality(C-Local) block. We evaluate SE block and C-Local\nblock by applying them to different CNNs architectures on cifar-10 dataset. We\nobserved that our C-Local block got higher accuracy than SE block did.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 03:22:26 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Li", "Huayu", ""]]}, {"id": "1901.01498", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma and Chunting Zhou and Eduard Hovy", "title": "MAE: Mutual Posterior-Divergence Regularization for Variational\n  AutoEncoders", "comments": "Published at ICLR-2019. 12 pages contents + 4 pages appendix, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoder (VAE), a simple and effective deep generative model,\nhas led to a number of impressive empirical successes and spawned many advanced\nvariants and theoretical investigations. However, recent studies demonstrate\nthat, when equipped with expressive generative distributions (aka. decoders),\nVAE suffers from learning uninformative latent representations with the\nobservation called KL Varnishing, in which case VAE collapses into an\nunconditional generative model. In this work, we introduce mutual\nposterior-divergence regularization, a novel regularization that is able to\ncontrol the geometry of the latent space to accomplish meaningful\nrepresentation learning, while achieving comparable or superior capability of\ndensity estimation. Experiments on three image benchmark datasets demonstrate\nthat, when equipped with powerful decoders, our model performs well both on\ndensity estimation and representation learning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 04:01:47 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ma", "Xuezhe", ""], ["Zhou", "Chunting", ""], ["Hovy", "Eduard", ""]]}, {"id": "1901.01499", "submitter": "Ryen Krusinga", "authors": "Ryen Krusinga, Sohil Shah, Matthias Zwicker, Tom Goldstein, David\n  Jacobs", "title": "Understanding the (un)interpretability of natural image distributions\n  using generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability density estimation is a classical and well studied problem, but\nstandard density estimation methods have historically lacked the power to model\ncomplex and high-dimensional image distributions. More recent generative models\nleverage the power of neural networks to implicitly learn and represent\nprobability models over complex images. We describe methods to extract explicit\nprobability density estimates from GANs, and explore the properties of these\nimage density functions. We perform sanity check experiments to provide\nevidence that these probabilities are reasonable. However, we also show that\ndensity functions of natural images are difficult to interpret and thus limited\nin use. We study reasons for this lack of interpretability, and show that we\ncan get interpretability back by doing density estimation on latent\nrepresentations of images.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 04:11:29 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 22:11:57 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Krusinga", "Ryen", ""], ["Shah", "Sohil", ""], ["Zwicker", "Matthias", ""], ["Goldstein", "Tom", ""], ["Jacobs", "David", ""]]}, {"id": "1901.01535", "submitter": "Despoina Paschalidou", "authors": "Despoina Paschalidou, Ali Osman Ulusoy, Carolin Schmitt, Luc van Gool,\n  Andreas Geiger", "title": "RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials", "comments": "Accepted to CVPR 2018 as spotlight. Project url with code:\n  http://raynet-mvs.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of reconstructing a dense 3D model\nusing images captured from different views. Recent methods based on\nconvolutional neural networks (CNN) allow learning the entire task from data.\nHowever, they do not incorporate the physics of image formation such as\nperspective geometry and occlusion. Instead, classical approaches based on\nMarkov Random Fields (MRF) with ray-potentials explicitly model these physical\nprocesses, but they cannot cope with large surface appearance variations across\ndifferent viewpoints. In this paper, we propose RayNet, which combines the\nstrengths of both frameworks. RayNet integrates a CNN that learns\nview-invariant feature representations with an MRF that explicitly encodes the\nphysics of perspective projection and occlusion. We train RayNet end-to-end\nusing empirical risk minimization. We thoroughly evaluate our approach on\nchallenging real-world datasets and demonstrate its benefits over a piece-wise\ntrained baseline, hand-crafted models as well as other learning-based\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 12:53:18 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Paschalidou", "Despoina", ""], ["Ulusoy", "Ali Osman", ""], ["Schmitt", "Carolin", ""], ["van Gool", "Luc", ""], ["Geiger", "Andreas", ""]]}, {"id": "1901.01550", "submitter": "Zhiling Long", "authors": "Tariq Alshawi, Zhiling Long, and Ghassan AlRegib", "title": "Unsupervised uncertainty estimation using spatiotemporal cues in video\n  saliency detection", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 27, no. 6, pp.\n  2818-2827, Jun. 2018", "doi": "10.1109/TIP.2018.2813159", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of quantifying reliability of\ncomputational saliency for videos, which can be used to improve saliency-based\nvideo processing and enable more reliable performance and risk assessment of\nsuch processing. Our approach is twofold. First, we explore spatial\ncorrelations in both saliency map and eye-fixation map. Then, we learn\nspatiotemporal correlations that define a reliable saliency map. We first study\nspatiotemporal eye-fixation data from a public dataset and investigate a common\nfeature in human visual attention, which dictates correlation in saliency\nbetween a pixel and its direct neighbors. Based on the study, we then develop\nan algorithm that estimates a pixel-wise uncertainty map that reflects our\nconfidence in the associated computational saliency map by relating a pixel's\nsaliency to the saliency of its neighbors. To estimate such uncertainties, we\nmeasure the divergence of a pixel, in a saliency map, from its local\nneighborhood. Additionally, we propose a systematic procedure to evaluate the\nestimation performance by explicitly computing uncertainty ground truth as a\nfunction of a given saliency map and eye fixations of human subjects. In our\nexperiments, we explore multiple definitions of locality and neighborhoods in\nspatiotemporal video signals. In addition, we examine the relationship between\nthe parameters of our proposed algorithm and the content of the videos. The\nproposed algorithm is unsupervised, making it more suitable for generalization\nto most natural videos. Also, it is computationally efficient and flexible for\ncustomization to specific video content. Experiments using three publicly\navailable video datasets show that the proposed algorithm outperforms\nstate-of-the-art uncertainty estimation methods with improvement in accuracy up\nto 63% and offers efficiency and flexibility that make it more useful in\npractical situations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 15:17:59 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Alshawi", "Tariq", ""], ["Long", "Zhiling", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.01562", "submitter": "Tomasz Konopczy\\'nski", "authors": "Tomasz Konopczy\\'nski, Thorben Kr\\\"oger, Lei Zheng, Christoph S.\n  Garbe, J\\\"urgen Hesser", "title": "Automated Multiscale 3D Feature Learning for Vessels Segmentation in\n  Thorax CT Images", "comments": "Published in: 2016 IEEE Nuclear Science Symposium, Medical Imaging\n  Conference and Room-Temperature Semiconductor Detector Workshop\n  (NSS/MIC/RTSD)", "journal-ref": null, "doi": "10.1109/NSSMIC.2016.8069570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the vessel segmentation problem by building upon the multiscale\nfeature learning method of Kiros et al., which achieves the current top score\nin the VESSEL12 MICCAI challenge. Following their idea of feature learning\ninstead of hand-crafted filters, we have extended the method to learn 3D\nfeatures. The features are learned in an unsupervised manner in a multi-scale\nscheme using dictionary learning via least angle regression. The 3D feature\nkernels are further convolved with the input volumes in order to create feature\nmaps. Those maps are used to train a supervised classifier with the annotated\nvoxels. In order to process the 3D data with a large number of filters a\nparallel implementation has been developed. The algorithm has been applied on\nthe example scans and annotations provided by the VESSEL12 challenge. We have\ncompared our setup with Kiros et al. by running their implementation. Our\ncurrent results show an improvement in accuracy over the slice wise method from\n96.66$\\pm$1.10% to 97.24$\\pm$0.90%.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 16:06:32 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Konopczy\u0144ski", "Tomasz", ""], ["Kr\u00f6ger", "Thorben", ""], ["Zheng", "Lei", ""], ["Garbe", "Christoph S.", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1901.01569", "submitter": "Songyao Jiang", "authors": "Songyao Jiang, Zhiqiang Tao, Yun Fu", "title": "Segmentation Guided Image-to-Image Translation with Adversarial Networks", "comments": "Accepted for publication in 2019 14th IEEE International Conference\n  on Automatic Face & Gesture Recognition (FG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently image-to-image translation has received increasing attention, which\naims to map images in one domain to another specific one. Existing methods\nmainly solve this task via a deep generative model, and focus on exploring the\nrelationship between different domains. However, these methods neglect to\nutilize higher-level and instance-specific information to guide the training\nprocess, leading to a great deal of unrealistic generated images of low\nquality. Existing methods also lack of spatial controllability during\ntranslation. To address these challenge, we propose a novel Segmentation Guided\nGenerative Adversarial Networks (SGGAN), which leverages semantic segmentation\nto further boost the generation performance and provide spatial mapping. In\nparticular, a segmentor network is designed to impose semantic information on\nthe generated images. Experimental results on multi-domain face image\ntranslation task empirically demonstrate our ability of the spatial\nmodification and our superiority in image quality over several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 16:36:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 18:58:28 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Jiang", "Songyao", ""], ["Tao", "Zhiqiang", ""], ["Fu", "Yun", ""]]}, {"id": "1901.01570", "submitter": "Ziyu Wan", "authors": "Ziyu Wan, Dongdong Chen, Yan Li, Xingguang Yan, Junge Zhang, Yizhou\n  Yu, Jing Liao", "title": "Transductive Zero-Shot Learning with Visual Structure Constraint", "comments": "NeurIPS 2019, code available at https://github.com/raywzy/VSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To recognize objects of the unseen classes, most existing Zero-Shot\nLearning(ZSL) methods first learn a compatible projection function between the\ncommon semantic space and the visual space based on the data of source seen\nclasses, then directly apply it to the target unseen classes. However, in real\nscenarios, the data distribution between the source and target domain might not\nmatch well, thus causing the well-known \\textbf{domain shift} problem. Based on\nthe observation that visual features of test instances can be separated into\ndifferent clusters, we propose a new visual structure constraint on class\ncenters for transductive ZSL, to improve the generality of the projection\nfunction (i.e. alleviate the above domain shift problem). Specifically, three\ndifferent strategies (symmetric Chamfer-distance, Bipartite matching distance,\nand Wasserstein distance) are adopted to align the projected unseen semantic\ncenters and visual cluster centers of test instances. We also propose a new\ntraining strategy to handle the real cases where many unrelated images exist in\nthe test dataset, which is not considered in previous methods. Experiments on\nmany widely used datasets demonstrate that the proposed visual structure\nconstraint can bring substantial performance gain consistently and achieve\nstate-of-the-art results. The source code is available at\n\\url{https://github.com/raywzy/VSC}.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 16:43:07 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 08:23:51 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wan", "Ziyu", ""], ["Chen", "Dongdong", ""], ["Li", "Yan", ""], ["Yan", "Xingguang", ""], ["Zhang", "Junge", ""], ["Yu", "Yizhou", ""], ["Liao", "Jing", ""]]}, {"id": "1901.01575", "submitter": "Jeffery Kinnison", "authors": "Jeffery Kinnison, Mateusz Trokielewicz, Camila Carballo, Adam Czajka,\n  Walter Scheirer", "title": "Learning-Free Iris Segmentation Revisited: A First Step Toward Fast\n  Volumetric Operation Over Video Samples", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subject matching performance in iris biometrics is contingent upon fast,\nhigh-quality iris segmentation. In many cases, iris biometrics acquisition\nequipment takes a number of images in sequence and combines the segmentation\nand matching results for each image to strengthen the result. To date,\nsegmentation has occurred in 2D, operating on each image individually. But such\nmethodologies, while powerful, do not take advantage of potential gains in\nperformance afforded by treating sequential images as volumetric data. As a\nfirst step in this direction, we apply the Flexible Learning-Free\nReconstructoin of Neural Volumes (FLoRIN) framework, an open source\nsegmentation and reconstruction framework originally designed for neural\nmicroscopy volumes, to volumetric segmentation of iris videos. Further, we\nintroduce a novel dataset of near-infrared iris videos, in which each subject's\npupil rapidly changes size due to visible-light stimuli, as a test bed for\nFLoRIN. We compare the matching performance for iris masks generated by FLoRIN,\ndeep-learning-based (SegNet), and Daugman's (OSIRIS) iris segmentation\napproaches. We show that by incorporating volumetric information, FLoRIN\nachieves a factor of 3.6 to an order of magnitude increase in throughput with\nonly a minor drop in subject matching performance. We also demonstrate that\nFLoRIN-based iris segmentation maintains this speedup on low-resource hardware,\nmaking it suitable for embedded biometrics systems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 17:22:08 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Kinnison", "Jeffery", ""], ["Trokielewicz", "Mateusz", ""], ["Carballo", "Camila", ""], ["Czajka", "Adam", ""], ["Scheirer", "Walter", ""]]}, {"id": "1901.01578", "submitter": "Suraj Mishra", "authors": "Suraj Mishra, Peixian Liang, Adam Czajka, Danny Z. Chen, X. Sharon Hu", "title": "CC-Net: Image Complexity Guided Network Compression for Biomedical Image\n  Segmentation", "comments": "Updated FM energy dist. figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) for biomedical image analysis are often\nof very large size, resulting in high memory requirement and high latency of\noperations. Searching for an acceptable compressed representation of the base\nCNN for a specific imaging application typically involves a series of\ntime-consuming training/validation experiments to achieve a good compromise\nbetween network size and accuracy. To address this challenge, we propose\nCC-Net, a new image complexity-guided CNN compression scheme for biomedical\nimage segmentation. Given a CNN model, CC-Net predicts the final accuracy of\nnetworks of different sizes based on the average image complexity computed from\nthe training data. It then selects a multiplicative factor for producing a\ndesired network with acceptable network accuracy and size. Experiments show\nthat CC-Net is effective for generating compressed segmentation networks,\nretaining up to 95% of the base network segmentation accuracy and utilizing\nonly 0.1% of trainable parameters of the full-sized networks in the best case.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 17:30:50 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 13:04:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mishra", "Suraj", ""], ["Liang", "Peixian", ""], ["Czajka", "Adam", ""], ["Chen", "Danny Z.", ""], ["Hu", "X. Sharon", ""]]}, {"id": "1901.01620", "submitter": "Pierre-Henri Conze", "authors": "Pierre-Henri Conze, Sylvain Brochard, Val\\'erie Burdin, Frances T.\n  Sheehan and Christelle Pons", "title": "Healthy versus pathological learning transferability in shoulder muscle\n  MRI segmentation using deep convolutional encoder-decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of pathological shoulder muscles in patients with\nmusculo-skeletal diseases is a challenging task due to the huge variability in\nmuscle shape, size, location, texture and injury. A reliable fully-automated\nsegmentation method from magnetic resonance images could greatly help\nclinicians to plan therapeutic interventions and predict interventional\noutcomes while eliminating time consuming manual segmentation efforts. The\npurpose of this work is three-fold. First, we investigate the feasibility of\npathological shoulder muscle segmentation using deep learning techniques, given\na very limited amount of available annotated pediatric data. Second, we address\nthe learning transferability from healthy to pathological data by comparing\ndifferent learning schemes in terms of model generalizability. Third, extended\nversions of deep convolutional encoder-decoder architectures using encoders\npre-trained on non-medical data are proposed to improve the segmentation\naccuracy. Methodological aspects are evaluated in a leave-one-out fashion on a\ndataset of 24 shoulder examinations from patients with obstetrical brachial\nplexus palsy and focus on 4 different muscles including deltoid as well as\ninfraspinatus, supraspinatus and subscapularis from the rotator cuff. The most\nrelevant segmentation model is partially pre-trained on ImageNet and jointly\nexploits inter-patient healthy and pathological annotated data. Its performance\nreaches Dice scores of 82.4%, 82.0%, 71.0% and 82.8% for deltoid,\ninfraspinatus, supraspinatus and subscapularis muscles. Absolute surface\nestimation errors are all below 83mm$^2$ except for supraspinatus with\n134.6mm$^2$. These contributions offer new perspectives for force inference in\nthe context of musculo-skeletal disorder management.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 22:51:01 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 00:10:58 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 14:46:55 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Conze", "Pierre-Henri", ""], ["Brochard", "Sylvain", ""], ["Burdin", "Val\u00e9rie", ""], ["Sheehan", "Frances T.", ""], ["Pons", "Christelle", ""]]}, {"id": "1901.01641", "submitter": "Quan Yuan", "authors": "Quan Yuan, Junxia Li, Lingwei Zhang, Zhefu Wu, Guangyu Liu", "title": "Blind Motion Deblurring with Cycle Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind motion deblurring is one of the most basic and challenging problems in\nimage processing and computer vision. It aims to recover a sharp image from its\nblurred version knowing nothing about the blur process. Many existing methods\nuse Maximum A Posteriori (MAP) or Expectation Maximization (EM) frameworks to\ndeal with this kind of problems, but they cannot handle well the figh frequency\nfeatures of natural images. Most recently, deep neural networks have been\nemerging as a powerful tool for image deblurring. In this paper, we prove that\nencoder-decoder architecture gives better results for image deblurring tasks.\nIn addition, we propose a novel end-to-end learning model which refines\ngenerative adversarial network by many novel training strategies so as to\ntackle the problem of deblurring. Experimental results show that our model can\ncapture high frequency features well, and the results on benchmark dataset show\nthat proposed model achieves the competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 01:52:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 08:24:40 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Yuan", "Quan", ""], ["Li", "Junxia", ""], ["Zhang", "Lingwei", ""], ["Wu", "Zhefu", ""], ["Liu", "Guangyu", ""]]}, {"id": "1901.01649", "submitter": "Yingtian Zou", "authors": "Guohao Ying, Yingtian Zou, Lin Wan, Yiming Hu, Jiashi Feng", "title": "Better Guider Predicts Future Better: Difference Guided Generative\n  Adversarial Networks", "comments": "To appear in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future is a fantasy but practicality work. It is the key\ncomponent to intelligent agents, such as self-driving vehicles, medical\nmonitoring devices and robotics. In this work, we consider generating unseen\nfuture frames from previous obeservations, which is notoriously hard due to the\nuncertainty in frame dynamics. While recent works based on generative\nadversarial networks (GANs) made remarkable progress, there is still an\nobstacle for making accurate and realistic predictions. In this paper, we\npropose a novel GAN based on inter-frame difference to circumvent the\ndifficulties. More specifically, our model is a multi-stage generative network,\nwhich is named the Difference Guided Generative Adversarial Netwok (DGGAN). The\nDGGAN learns to explicitly enforce future-frame predictions that is guided by\nsynthetic inter-frame difference. Given a sequence of frames, DGGAN first uses\ndual paths to generate meta information. One path, called Coarse Frame\nGenerator, predicts the coarse details about future frames, and the other path,\ncalled Difference Guide Generator, generates the difference image which include\ncomplementary fine details. Then our coarse details will then be refined via\nguidance of difference image under the support of GANs. With this model and\nnovel architecture, we achieve state-of-the-art performance for future video\nprediction on UCF-101, KITTI.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 02:42:57 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ying", "Guohao", ""], ["Zou", "Yingtian", ""], ["Wan", "Lin", ""], ["Hu", "Yiming", ""], ["Feng", "Jiashi", ""]]}, {"id": "1901.01651", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Hei Long Chan, Robin Yong, Sarbin Ranjitkar, Alan\n  Brook, Grant Townsend, Ke Chen, Lok Ming Lui", "title": "Tooth morphometry using quasi-conformal theory", "comments": null, "journal-ref": "Pattern Recognition 99, 107064 (2020)", "doi": "10.1016/j.patcog.2019.107064", "report-no": null, "categories": "cs.CV cs.CG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape analysis is important in anthropology, bioarchaeology and forensic\nscience for interpreting useful information from human remains. In particular,\nteeth are morphologically stable and hence well-suited for shape analysis. In\nthis work, we propose a framework for tooth morphometry using quasi-conformal\ntheory. Landmark-matching Teichm\\\"uller maps are used for establishing a 1-1\ncorrespondence between tooth surfaces with prescribed anatomical landmarks.\nThen, a quasi-conformal statistical shape analysis model based on the\nTeichm\\\"uller mapping results is proposed for building a tooth classification\nscheme. We deploy our framework on a dataset of human premolars to analyze the\ntooth shape variation among genders and ancestries. Experimental results show\nthat our method achieves much higher classification accuracy with respect to\nboth gender and ancestry when compared to the existing methods. Furthermore,\nour model reveals the underlying tooth shape difference between different\ngenders and ancestries in terms of the local geometric distortion and\ncurvatures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 03:00:12 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Chan", "Hei Long", ""], ["Yong", "Robin", ""], ["Ranjitkar", "Sarbin", ""], ["Brook", "Alan", ""], ["Townsend", "Grant", ""], ["Chen", "Ke", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1901.01660", "submitter": "Zhipeng Zhang", "authors": "Zhipeng Zhang, Houwen Peng", "title": "Deeper and Wider Siamese Networks for Real-Time Visual Tracking", "comments": "have been accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese networks have drawn great attention in visual tracking because of\ntheir balanced accuracy and speed. However, the backbone networks used in\nSiamese trackers are relatively shallow, such as AlexNet [18], which does not\nfully take advantage of the capability of modern deep neural networks. In this\npaper, we investigate how to leverage deeper and wider convolutional neural\nnetworks to enhance tracking robustness and accuracy. We observe that direct\nreplacement of backbones with existing powerful architectures, such as ResNet\n[14] and Inception [33], does not bring improvements. The main reasons are that\n1)large increases in the receptive field of neurons lead to reduced feature\ndiscriminability and localization precision; and 2) the network padding for\nconvolutions induces a positional bias in learning. To address these issues, we\npropose new residual modules to eliminate the negative impact of padding, and\nfurther design new architectures using these modules with controlled receptive\nfield size and network stride. The designed architectures are lightweight and\nguarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20].\nExperiments show that solely due to the proposed network architectures, our\nSiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and\n24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on\nthe OTB-15, VOT-16 and VOT-17 datasets, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 04:17:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 06:43:32 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 13:07:52 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Zhang", "Zhipeng", ""], ["Peng", "Houwen", ""]]}, {"id": "1901.01677", "submitter": "Aamir Mustafa", "authors": "Aamir Mustafa, Salman H. Khan, Munawar Hayat, Jianbing Shen and Ling\n  Shao", "title": "Image Super-Resolution as a Defense Against Adversarial Attacks", "comments": "Published in IEEE Transactions in Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2940533", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved significant success across\nmultiple computer vision tasks. However, they are vulnerable to carefully\ncrafted, human-imperceptible adversarial noise patterns which constrain their\ndeployment in critical security-sensitive systems. This paper proposes a\ncomputationally efficient image enhancement approach that provides a strong\ndefense mechanism to effectively mitigate the effect of such adversarial\nperturbations. We show that deep image restoration networks learn mapping\nfunctions that can bring off-the-manifold adversarial samples onto the natural\nimage manifold, thus restoring classification towards correct classes. A\ndistinguishing feature of our approach is that, in addition to providing\nrobustness against attacks, it simultaneously enhances image quality and\nretains models performance on clean images. Furthermore, the proposed method\ndoes not modify the classifier or requires a separate mechanism to detect\nadversarial images. The effectiveness of the scheme has been demonstrated\nthrough extensive experiments, where it has proven a strong defense in gray-box\nsettings. The proposed scheme is simple and has the following advantages: (1)\nit does not require any model training or parameter optimization, (2) it\ncomplements other existing defense mechanisms, (3) it is agnostic to the\nattacked model and attack type and (4) it provides superior performance across\nall popular attack algorithms. Our codes are publicly available at\nhttps://github.com/aamir-mustafa/super-resolution-adversarial-defense.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 06:43:23 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 05:46:23 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Mustafa", "Aamir", ""], ["Khan", "Salman H.", ""], ["Hayat", "Munawar", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1901.01703", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang, Jinlong Hou, Jie Liu,\n  Tong Zhang", "title": "Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual\n  Representation Learning", "comments": "This work is accepted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2956775", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing visual representation learning tasks, deep convolutional neural\nnetworks (CNNs) are often trained on images annotated with single tags, such as\nImageNet. However, a single tag cannot describe all important contents of one\nimage, and some useful visual information may be wasted during training. In\nthis work, we propose to train CNNs from images annotated with multiple tags,\nto enhance the quality of visual representation of the trained CNN model. To\nthis end, we build a large-scale multi-label image database with 18M images and\n11K categories, dubbed Tencent ML-Images. We efficiently train the ResNet-101\nmodel with multi-label outputs on Tencent ML-Images, taking 90 hours for 60\nepochs, based on a large-scale distributed deep learning framework,i.e.,TFplus.\nThe good quality of the visual representation of the Tencent ML-Images\ncheckpoint is verified through three transfer learning tasks, including\nsingle-label image classification on ImageNet and Caltech-256, object detection\non PASCAL VOC 2007, and semantic segmentation on PASCAL VOC 2012. The Tencent\nML-Images database, the checkpoints of ResNet-101, and all the training\ncodehave been released at https://github.com/Tencent/tencent-ml-images. It is\nexpected to promote other vision tasks in the research and industry community.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:35:15 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 11:34:02 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 03:14:15 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 09:33:20 GMT"}, {"version": "v5", "created": "Sat, 12 Oct 2019 04:14:24 GMT"}, {"version": "v6", "created": "Thu, 7 Nov 2019 02:21:40 GMT"}, {"version": "v7", "created": "Sun, 9 Feb 2020 14:06:39 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wu", "Baoyuan", ""], ["Chen", "Weidong", ""], ["Fan", "Yanbo", ""], ["Zhang", "Yong", ""], ["Hou", "Jinlong", ""], ["Liu", "Jie", ""], ["Zhang", "Tong", ""]]}, {"id": "1901.01706", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Universal Deep Beamformer for Variable Rate Ultrasound Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) imaging is based on the time-reversal principle, in which\nindividual channel RF measurements are back-propagated and accumulated to form\nan image after applying specific delays. While this time reversal is usually\nimplemented as a delay-and-sum (DAS) beamformer, the image quality quickly\ndegrades as the number of measurement channels decreases. To address this\nproblem, various types of adaptive beamforming techniques have been proposed\nusing predefined models of the signals. However, the performance of these\nadaptive beamforming approaches degrade when the underlying model is not\nsufficiently accurate. Here, we demonstrate for the first time that a single\nuniversal deep beamformer trained using a purely data-driven way can generate\nsignificantly improved images over widely varying aperture and channel\nsubsampling patterns. In particular, we design an end-to-end deep learning\nframework that can directly process sub-sampled RF data acquired at different\nsubsampling rate and detector configuration to generate high quality ultrasound\nimages using a single beamformer. Experimental results using B-mode focused\nultrasound confirm the efficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:52:02 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1901.01708", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz", "title": "Post-mortem Iris Recognition with Deep-Learning-based Image Segmentation", "comments": "Paper submitted for the Elsevier Image and Vision Computing Journal\n  on Jan 5th, 2019, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first known to us iris recognition methodology\ndesigned specifically for post-mortem samples. We propose to use deep\nlearning-based iris segmentation models to extract highly irregular iris\ntexture areas in post-mortem iris images. We show how to use segmentation masks\npredicted by neural networks in conventional, Gabor-based iris recognition\nmethod, which employs circular approximations of the pupillary and limbic iris\nboundaries. As a whole, this method allows for a significant improvement in\npost-mortem iris recognition accuracy over the methods designed only for\nante-mortem irises, including the academic OSIRIS and commercial IriCore\nimplementations. The proposed method reaches the EER less than 1% for samples\ncollected up to 10 hours after death, when compared to 16.89% and 5.37% of EER\nobserved for OSIRIS and IriCore, respectively. For samples collected up to 369\nhours post-mortem, the proposed method achieves the EER 21.45%, while 33.59%\nand 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the\nmethod is tested on a database of iris images collected from ophthalmology\nclinic patients, for which it also offers an advantage over the two other\nalgorithms. This work is the first step towards post-mortem-specific iris\nrecognition, which increases the chances of identification of deceased subjects\nin forensic investigations. The new database of post-mortem iris images\nacquired from 42 subjects, as well as the deep learning-based segmentation\nmodels are made available along with the paper, to ensure all the results\npresented in this manuscript are reproducible.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:57:35 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:42:08 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1901.01711", "submitter": "Shengke Xue", "authors": "Shengke Xue, Wenyuan Qiu, Fan Liu, and Xinyu Jin", "title": "Double Weighted Truncated Nuclear Norm Regularization for Low-Rank\n  Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion focuses on recovering a matrix from a small subset of its\nobserved elements, and has already gained cumulative attention in computer\nvision. Many previous approaches formulate this issue as a low-rank matrix\napproximation problem. Recently, a truncated nuclear norm has been presented as\na surrogate of traditional nuclear norm, for better estimation to the rank of a\nmatrix. The truncated nuclear norm regularization (TNNR) method is applicable\nin real-world scenarios. However, it is sensitive to the selection of the\nnumber of truncated singular values and requires numerous iterations to\nconverge. Hereby, this paper proposes a revised approach called the double\nweighted truncated nuclear norm regularization (DW-TNNR), which assigns\ndifferent weights to the rows and columns of a matrix separately, to accelerate\nthe convergence with acceptable performance. The DW-TNNR is more robust to the\nnumber of truncated singular values than the TNNR. Instead of the iterative\nupdating scheme in the second step of TNNR, this paper devises an efficient\nstrategy that uses a gradient descent manner in a concise form, with a\ntheoretical guarantee in optimization. Sufficient experiments conducted on real\nvisual data prove that DW-TNNR has promising performance and holds the\nsuperiority in both speed and accuracy for matrix completion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 09:04:37 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Xue", "Shengke", ""], ["Qiu", "Wenyuan", ""], ["Liu", "Fan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1901.01760", "submitter": "Shu Liu", "authors": "Hong Zhang, Hao Ouyang, Shu Liu, Xiaojuan Qi, Xiaoyong Shen, Ruigang\n  Yang, Jiaya Jia", "title": "Human Pose Estimation with Spatial Contextual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the importance of spatial contextual information in human pose\nestimation. Most state-of-the-art pose networks are trained in a multi-stage\nmanner and produce several auxiliary predictions for deep supervision. With\nthis principle, we present two conceptually simple and yet computational\nefficient modules, namely Cascade Prediction Fusion (CPF) and Pose Graph Neural\nNetwork (PGNN), to exploit underlying contextual information. Cascade\nprediction fusion accumulates prediction maps from previous stages to extract\ninformative signals. The resulting maps also function as a prior to guide\nprediction at following stages. To promote spatial correlation among joints,\nour PGNN learns a structured representation of human pose as a graph. Direct\nmessage passing between different joints is enabled and spatial relation is\ncaptured. These two modules require very limited computational complexity.\nExperimental results demonstrate that our method consistently outperforms\nprevious methods on MPII and LSP benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 11:58:10 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Zhang", "Hong", ""], ["Ouyang", "Hao", ""], ["Liu", "Shu", ""], ["Qi", "Xiaojuan", ""], ["Shen", "Xiaoyong", ""], ["Yang", "Ruigang", ""], ["Jia", "Jiaya", ""]]}, {"id": "1901.01805", "submitter": "Panagiotis Filntisis", "authors": "Panagiotis P. Filntisis, Niki Efthymiou, Petros Koutras, Gerasimos\n  Potamianos, Petros Maragos", "title": "Fusing Body Posture with Facial Expressions for Joint Recognition of\n  Affect in Child-Robot Interaction", "comments": "To be presented in IROS 2019", "journal-ref": "IEEE Robotics and Automation Letters, 4(4), 4011-4018, 2019", "doi": "10.1109/LRA.2019.2930434", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of multi-cue affect recognition in\nchallenging scenarios such as child-robot interaction. Towards this goal we\npropose a method for automatic recognition of affect that leverages body\nexpressions alongside facial ones, as opposed to traditional methods that\ntypically focus only on the latter. Our deep-learning based method uses\nhierarchical multi-label annotations and multi-stage losses, can be trained\nboth jointly and separately, and offers us computational models for both\nindividual modalities, as well as for the whole body emotion. We evaluate our\nmethod on a challenging child-robot interaction database of emotional\nexpressions collected by us, as well as on the GEMEP public database of acted\nemotions by adults, and show that the proposed method achieves significantly\nbetter results than facial-only expression baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 13:50:49 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 15:21:55 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 14:53:15 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Filntisis", "Panagiotis P.", ""], ["Efthymiou", "Niki", ""], ["Koutras", "Petros", ""], ["Potamianos", "Gerasimos", ""], ["Maragos", "Petros", ""]]}, {"id": "1901.01868", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Mihai Puscas, Jannik Wolff, Tassilo Klein, Nicu Sebe,\n  Moin Nabi", "title": "Low-Shot Learning from Imaginary 3D Model", "comments": "To appear at WACV 2019. arXiv admin note: text overlap with\n  arXiv:1811.09192", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of deep learning, neural networks have demonstrated\nremarkable results in many visual recognition tasks, constantly pushing the\nlimits. However, the state-of-the-art approaches are largely unsuitable in\nscarce data regimes. To address this shortcoming, this paper proposes employing\na 3D model, which is derived from training images. Such a model can then be\nused to hallucinate novel viewpoints and poses for the scarce samples of the\nfew-shot learning scenario. A self-paced learning approach allows for the\nselection of a diverse set of high-quality images, which facilitates the\ntraining of a classifier. The performance of the proposed approach is showcased\non the fine-grained CUB-200-2011 dataset in a few-shot setting and\nsignificantly improves our baseline accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:19:58 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Pahde", "Frederik", ""], ["Puscas", "Mihai", ""], ["Wolff", "Jannik", ""], ["Klein", "Tassilo", ""], ["Sebe", "Nicu", ""], ["Nabi", "Moin", ""]]}, {"id": "1901.01874", "submitter": "Yifei Huang", "authors": "Yifei Huang, Zhenqiang Li, Minjie Cai, Yoichi Sato", "title": "Mutual Context Network for Jointly Estimating Egocentric Gaze and\n  Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address two coupled tasks of gaze prediction and action\nrecognition in egocentric videos by exploring their mutual context. Our\nassumption is that in the procedure of performing a manipulation task, what a\nperson is doing determines where the person is looking at, and the gaze point\nreveals gaze and non-gaze regions which contain important and complementary\ninformation about the undergoing action. We propose a novel mutual context\nnetwork (MCN) that jointly learns action-dependent gaze prediction and\ngaze-guided action recognition in an end-to-end manner. Experiments on public\negocentric video datasets demonstrate that our MCN achieves state-of-the-art\nperformance of both gaze prediction and action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:10:07 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 07:41:51 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 07:31:49 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 02:08:22 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Huang", "Yifei", ""], ["Li", "Zhenqiang", ""], ["Cai", "Minjie", ""], ["Sato", "Yoichi", ""]]}, {"id": "1901.01880", "submitter": "Xu Chen", "authors": "Xu Chen, Jie Song, Otmar Hilliges", "title": "Monocular Neural Image Based Rendering with Continuous View Control", "comments": "The first two authors contributed equally to this paper. ICCV\n  camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach that learns to synthesize high-quality, novel views of\n3D objects or scenes, while providing fine-grained and precise control over the\n6-DOF viewpoint. The approach is self-supervised and only requires 2D images\nand associated view transforms for training. Our main contribution is a network\narchitecture that leverages a transforming auto-encoder in combination with a\ndepth-guided warping procedure to predict geometrically accurate unseen views.\nLeveraging geometric constraints renders direct supervision via depth or flow\nmaps unnecessary. If large parts of the object are occluded in the source view,\na purely learning based prior is used to predict the values for dis-occluded\npixels. Our network furthermore predicts a per-pixel mask, used to fuse\ndepth-guided and pixel-based predictions. The resulting images reflect the\ndesired 6-DOF transformation and details are preserved. We thoroughly evaluate\nour architecture on synthetic and real scenes and under fine-grained and\nfixed-view settings. Finally, we demonstrate that the approach generalizes to\nentirely unseen images such as product images downloaded from the internet.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:24:25 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 08:23:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chen", "Xu", ""], ["Song", "Jie", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1901.01892", "submitter": "Yuntao Chen", "authors": "Yanghao Li, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "Scale-Aware Trident Networks for Object Detection", "comments": "ICCV 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale variation is one of the key challenges in object detection. In this\nwork, we first present a controlled experiment to investigate the effect of\nreceptive fields for scale variation in object detection. Based on the findings\nfrom the exploration experiments, we propose a novel Trident Network\n(TridentNet) aiming to generate scale-specific feature maps with a uniform\nrepresentational power. We construct a parallel multi-branch architecture in\nwhich each branch shares the same transformation parameters but with different\nreceptive fields. Then, we adopt a scale-aware training scheme to specialize\neach branch by sampling object instances of proper scales for training. As a\nbonus, a fast approximation version of TridentNet could achieve significant\nimprovements without any additional parameters and computational cost compared\nwith the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101\nbackbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are\navailable at https://git.io/fj5vR.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 16:08:37 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 03:17:44 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Li", "Yanghao", ""], ["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1901.01913", "submitter": "Yuqian Zhang", "authors": "Yuqian Zhang, Yenson Lau, Han-Wen Kuo, Sky Cheung, Abhay Pasupathy,\n  John Wright", "title": "On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution is the problem of recovering a convolutional kernel\n$\\boldsymbol a_0$ and an activation signal $\\boldsymbol x_0$ from their\nconvolution $\\boldsymbol y = \\boldsymbol a_0 \\circledast \\boldsymbol x_0$. This\nproblem is ill-posed without further constraints or priors. This paper studies\nthe situation where the nonzero entries in the activation signal are sparsely\nand randomly populated. We normalize the convolution kernel to have unit\nFrobenius norm and cast the sparse blind deconvolution problem as a nonconvex\noptimization problem over the sphere. With this spherical constraint, every\nspurious local minimum turns out to be close to some signed shift truncation of\nthe ground truth, under certain hypotheses. This benign property motivates an\neffective two stage algorithm that recovers the ground truth from the partial\ninformation offered by a suboptimal local minimum. This geometry-inspired\nalgorithm recovers the ground truth for certain microscopy problems, also\nexhibits promising performance in the more challenging image deblurring\nproblem. Our insights into the global geometry and the two stage algorithm\nextend to the convolutional dictionary learning problem, where a superposition\nof multiple convolution signals is observed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 16:42:46 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Zhang", "Yuqian", ""], ["Lau", "Yenson", ""], ["Kuo", "Han-Wen", ""], ["Cheung", "Sky", ""], ["Pasupathy", "Abhay", ""], ["Wright", "John", ""]]}, {"id": "1901.01928", "submitter": "Marcelo Gennari do Nascimento", "authors": "Marcelo Gennari, Roger Fawcett, Victor Adrian Prisacariu", "title": "DSConv: Efficient Convolution Operator", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 5148-5157", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a popular way of increasing the speed and lowering the memory\nusage of Convolution Neural Networks (CNNs). When labelled training data is\navailable, network weights and activations have successfully been quantized\ndown to 1-bit. The same cannot be said about the scenario when labelled\ntraining data is not available, e.g. when quantizing a pre-trained model, where\ncurrent approaches show, at best, no loss of accuracy at 8-bit quantizations.\nWe introduce DSConv, a flexible quantized convolution operator that replaces\nsingle-precision operations with their far less expensive integer counterparts,\nwhile maintaining the probability distributions over both the kernel weights\nand the outputs. We test our model as a plug-and-play replacement for standard\nconvolution on most popular neural network architectures, ResNet, DenseNet,\nGoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with\nless than 1% loss of accuracy, without retraining, using only 4-bit\nquantization. We also show how a distillation-based adaptation stage with\nunlabelled data can improve results even further.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:18:16 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:03:41 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Gennari", "Marcelo", ""], ["Fawcett", "Roger", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "1901.01939", "submitter": "Amirsina Torfi", "authors": "Amirsina Torfi, Rouzbeh A. Shirvani, Sobhan Soleymani, Naser M.\n  Nasrabadi", "title": "GASL: Guided Attention for Sparsity Learning in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of network pruning is imposing sparsity on the neural network\nby increasing the number of parameters with zero value in order to reduce the\narchitecture size and the computational speedup. In most of the previous\nresearch works, sparsity is imposed stochastically without considering any\nprior knowledge of the weights distribution or other internal network\ncharacteristics. Enforcing too much sparsity may induce accuracy drop due to\nthe fact that a lot of important elements might have been eliminated. In this\npaper, we propose Guided Attention for Sparsity Learning (GASL) to achieve (1)\nmodel compression by having less number of elements and speed-up; (2) prevent\nthe accuracy drop by supervising the sparsity operation via a guided attention\nmechanism and (3) introduce a generic mechanism that can be adapted for any\ntype of architecture; Our work is aimed at providing a framework based on\ninterpretable attention mechanisms for imposing structured and non-structured\nsparsity in deep neural networks. For Cifar-100 experiments, we achieved the\nstate-of-the-art sparsity level and 2.91x speedup with competitive accuracy\ncompared to the best method. For MNIST and LeNet architecture we also achieved\nthe highest sparsity and speedup level.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:43:08 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:15:36 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Torfi", "Amirsina", ""], ["Shirvani", "Rouzbeh A.", ""], ["Soleymani", "Sobhan", ""], ["Nasrabadi", "Naser M.", ""]]}, {"id": "1901.01969", "submitter": "Iain Styles", "authors": "Wenqi Lu, Jinming Duan, David Orive-Miguel, Lionel Herve, Iain B\n  Styles", "title": "Graph- and finite element-based total variation models for the inverse\n  problem in diffuse optical tomography", "comments": "24 pages, 11 figures. Reviced version includes revised figures and\n  improved clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Total variation (TV) is a powerful regularization method that has been widely\napplied in different imaging applications, but is difficult to apply to diffuse\noptical tomography (DOT) image reconstruction (inverse problem) due to complex\nand unstructured geometries, non-linearity of the data fitting and\nregularization terms, and non-differentiability of the regularization term. We\ndevelop several approaches to overcome these difficulties by: i) defining\ndiscrete differential operators for unstructured geometries using both finite\nelement and graph representations; ii) developing an optimization algorithm\nbased on the alternating direction method of multipliers (ADMM) for the\nnon-differentiable and non-linear minimization problem; iii) investigating\nisotropic and anisotropic variants of TV regularization, and comparing their\nfinite element- and graph-based implementations. These approaches are evaluated\non experiments on simulated data and real data acquired from a tissue phantom.\nOur results show that both FEM and graph-based TV regularization is able to\naccurately reconstruct both sparse and non-sparse distributions without the\nover-smoothing effect of Tikhonov regularization and the over-sparsifying\neffect of L$_1$ regularization. The graph representation was found to\nout-perform the FEM method for low-resolution meshes, and the FEM method was\nfound to be more accurate for high-resolution meshes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 18:52:32 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:51:46 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Lu", "Wenqi", ""], ["Duan", "Jinming", ""], ["Orive-Miguel", "David", ""], ["Herve", "Lionel", ""], ["Styles", "Iain B", ""]]}, {"id": "1901.01971", "submitter": "Zhe Cao", "authors": "Zhe Cao, Abhishek Kar, Christian Haene, Jitendra Malik", "title": "Learning Independent Object Motion from Unlabelled Stereoscopic Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for learning motion of independently moving objects from\nstereo videos. The only human annotation used in our system are 2D object\nbounding boxes which introduce the notion of objects to our system. Unlike\nprior learning based work which has focused on predicting dense pixel-wise\noptical flow field and/or a depth map for each image, we propose to predict\nobject instance specific 3D scene flow maps and instance masks from which we\nare able to derive the motion direction and speed for each object instance. Our\nnetwork takes the 3D geometry of the problem into account which allows it to\ncorrelate the input images. We present experiments evaluating the accuracy of\nour 3D flow vectors, as well as depth maps and projected 2D optical flow where\nour jointly learned system outperforms earlier approaches trained for each task\nindependently.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 18:58:11 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 10:27:40 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Cao", "Zhe", ""], ["Kar", "Abhishek", ""], ["Haene", "Christian", ""], ["Malik", "Jitendra", ""]]}, {"id": "1901.01982", "submitter": "Shi Yin", "authors": "Shi Yin, Zhengqiang Zhang, Hongming Li, Qinmu Peng, Xinge You, Susan\n  L. Furth, Gregory E. Tasian, Yong Fan", "title": "Fully-automatic segmentation of kidneys in clinical ultrasound images\n  using a boundary distance regression network", "comments": "4 pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.04815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains challenging to automatically segment kidneys in clinical\nultrasound images due to the kidneys' varied shapes and image intensity\ndistributions, although semi-automatic methods have achieved promising\nperformance. In this study, we developed a novel boundary distance regression\ndeep neural network to segment the kidneys, informed by the fact that the\nkidney boundaries are relatively consistent across images in terms of their\nappearance. Particularly, we first use deep neural networks pre-trained for\nclassification of natural images to extract high-level image features from\nultrasound images, then these feature maps are used as input to learn kidney\nboundary distance maps using a boundary distance regression network, and\nfinally the predicted boundary distance maps are classified as kidney pixels or\nnon-kidney pixels using a pixel classification network in an end-to-end\nlearning fashion. Experimental results have demonstrated that our method could\neffectively improve the performance of automatic kidney segmentation,\nsignificantly better than deep learning based pixel classification networks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 19:44:23 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Yin", "Shi", ""], ["Zhang", "Zhengqiang", ""], ["Li", "Hongming", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""], ["Furth", "Susan L.", ""], ["Tasian", "Gregory E.", ""], ["Fan", "Yong", ""]]}, {"id": "1901.01997", "submitter": "Shengke Xue", "authors": "Shengke Xue, Wenyuan Qiu, Fan Liu, and Xinyu Jin", "title": "Truncated nuclear norm regularization for low-rank tensor completion", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.00704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, low-rank tensor completion has become increasingly attractive in\nrecovering incomplete visual data. Considering a color image or video as a\nthree-dimensional (3D) tensor, existing studies have put forward several\ndefinitions of tensor nuclear norm. However, they are limited and may not\naccurately approximate the real rank of a tensor, and they do not explicitly\nuse the low-rank property in optimization. It is proved that the recently\nproposed truncated nuclear norm (TNN) can replace the traditional nuclear norm,\nas an improved approximation to the rank of a matrix. In this paper, we propose\na new method called the tensor truncated nuclear norm (T-TNN), which suggests a\nnew definition of tensor nuclear norm. The truncated nuclear norm is\ngeneralized from the matrix case to the tensor case. With the help of the low\nrankness of TNN, our approach improves the efficacy of tensor completion. We\nadopt the definition of the previously proposed tensor singular value\ndecomposition, the alternating direction method of multipliers, and the\naccelerated proximal gradient line search method in our algorithm. Substantial\nexperiments on real-world videos and images illustrate that the performance of\nour approach is better than those of previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:21:47 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Xue", "Shengke", ""], ["Qiu", "Wenyuan", ""], ["Liu", "Fan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1901.02000", "submitter": "Irtiza Hasan", "authors": "Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Vasileios\n  Belagiannis, Sikandar Amin, Alessio Del Bue, Marco Cristani, and Fabio\n  Galasso", "title": "Forecasting People Trajectories and Head Poses by Jointly Reasoning on\n  Tracklets and Vislets", "comments": "Accepted at IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE\n  INTELLIGENCE 2019. arXiv admin note: text overlap with arXiv:1805.00652", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the correlation between people trajectories and\ntheir head orientations. We argue that people trajectory and head pose\nforecasting can be modelled as a joint problem. Recent approaches on trajectory\nforecasting leverage short-term trajectories (aka tracklets) of pedestrians to\npredict their future paths. In addition, sociological cues, such as expected\ndestination or pedestrian interaction, are often combined with tracklets. In\nthis paper, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between\npositions and head orientations (vislets) thanks to a joint unconstrained\noptimization of full covariance matrices during the LSTM backpropagation. We\nadditionally exploit the head orientations as a proxy for the visual attention,\nwhen modeling social interactions. MX-LSTM predicts future pedestrians location\nand head pose, increasing the standard capabilities of the current approaches\non long-term trajectory forecasting. Compared to the state-of-the-art, our\napproach shows better performances on an extensive set of public benchmarks.\nMX-LSTM is particularly effective when people move slowly, i.e. the most\nchallenging scenario for all other models. The proposed approach also allows\nfor accurate predictions on a longer time horizon.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 10:15:17 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 09:46:17 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Hasan", "Irtiza", ""], ["Setti", "Francesco", ""], ["Tsesmelis", "Theodore", ""], ["Belagiannis", "Vasileios", ""], ["Amin", "Sikandar", ""], ["Del Bue", "Alessio", ""], ["Cristani", "Marco", ""], ["Galasso", "Fabio", ""]]}, {"id": "1901.02004", "submitter": "Raul Gomez", "authors": "Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas", "title": "Self-Supervised Learning from Web Data for Multimodal Retrieval", "comments": "Submitted to Multi-Modal Scene Understanding. arXiv admin note:\n  substantial text overlap with arXiv:1808.06368", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-Supervised learning from multimodal image and text data allows deep\nneural networks to learn powerful features with no need of human annotated\ndata. Web and Social Media platforms provide a virtually unlimited amount of\nthis multimodal data. In this work we propose to exploit this free available\ndata to learn a multimodal image and text embedding, aiming to leverage the\nsemantic knowledge learnt in the text domain and transfer it to a visual model\nfor semantic image retrieval. We demonstrate that the proposed pipeline can\nlearn from images with associated textwithout supervision and analyze the\nsemantic structure of the learnt joint image and text embedding space. We\nperform a thorough analysis and performance comparison of five different state\nof the art text embeddings in three different benchmarks. We show that the\nembeddings learnt with Web and Social Media data have competitive performances\nover supervised methods in the text based image retrieval task, and we clearly\noutperform state of the art in the MIRFlickr dataset when training in the\ntarget data. Further, we demonstrate how semantic multimodal image retrieval\ncan be performed using the learnt embeddings, going beyond classical\ninstance-level retrieval problems. Finally, we present a new dataset,\nInstaCities1M, composed by Instagram images and their associated texts that can\nbe used for fair comparison of image-text embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 14:34:49 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Gomez", "Raul", ""], ["Gomez", "Lluis", ""], ["Gibert", "Jaume", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1901.02039", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip\n  Marcus, Matthias Niessner", "title": "Spherical CNNs on Unstructured Grids", "comments": "Accepted as a conference paper at ICLR 2019. Codes available at\n  https://github.com/maxjiang93/ugscnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient convolution kernel for Convolutional Neural Networks\n(CNNs) on unstructured grids using parameterized differential operators while\nfocusing on spherical signals such as panorama images or planetary signals. To\nthis end, we replace conventional convolution kernels with linear combinations\nof differential operators that are weighted by learnable parameters.\nDifferential operators can be efficiently estimated on unstructured grids using\none-ring neighbors, and learnable parameters can be optimized through standard\nback-propagation. As a result, we obtain extremely efficient neural networks\nthat match or outperform state-of-the-art network architectures in terms of\nperformance but with a significantly lower number of network parameters. We\nevaluate our algorithm in an extensive series of experiments on a variety of\ncomputer vision and climate science tasks, including shape classification,\nclimate pattern segmentation, and omnidirectional image semantic segmentation.\nOverall, we present (1) a novel CNN approach on unstructured grids using\nparameterized differential operators for spherical signals, and (2) we show\nthat our unique kernel parameterization allows our model to achieve the same or\nhigher accuracy with significantly fewer network parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 19:56:19 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Huang", "Jingwei", ""], ["Kashinath", "Karthik", ""], ["Prabhat", "", ""], ["Marcus", "Philip", ""], ["Niessner", "Matthias", ""]]}, {"id": "1901.02040", "submitter": "Yuankai Huo", "authors": "Yunxi Xiong, Yuankai Huo, Jiachen Wang, L. Taylor Davis, Maureen\n  McHugo, Bennett A. Landman", "title": "Reproducibility Evaluation of SLANT Whole Brain Segmentation Across\n  Clinical Magnetic Resonance Imaging Protocols", "comments": "To appear in SPIE Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation on structural magnetic resonance imaging (MRI) is\nessential for understanding neuroanatomical-functional relationships.\nTraditionally, multi-atlas segmentation has been regarded as the standard\nmethod for whole brain segmentation. In past few years, deep convolutional\nneural network (DCNN) segmentation methods have demonstrated their advantages\nin both accuracy and computational efficiency. Recently, we proposed the\nspatially localized atlas network tiles (SLANT) method, which is able to\nsegment a 3D MRI brain scan into 132 anatomical regions. Commonly, DCNN\nsegmentation methods yield inferior performance under external validations,\nespecially when the testing patterns were not presented in the training\ncohorts. Recently, we obtained a clinically acquired, multi-sequence MRI brain\ncohort with 1480 clinically acquired, de-identified brain MRI scans on 395\npatients using seven different MRI protocols. Moreover, each subject has at\nleast two scans from different MRI protocols. Herein, we assess the SLANT\nmethod's intra- and inter-protocol reproducibility. SLANT achieved less than\n0.05 coefficient of variation (CV) for intra-protocol experiments and less than\n0.15 CV for inter-protocol experiments. The results show that the SLANT method\nachieved high intra- and inter- protocol reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 19:57:07 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Xiong", "Yunxi", ""], ["Huo", "Yuankai", ""], ["Wang", "Jiachen", ""], ["Davis", "L. Taylor", ""], ["McHugo", "Maureen", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1901.02070", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Dequan Wang, Jingwei Huang, Philip Marcus, Matthias\n  Nie{\\ss}ner", "title": "Convolutional Neural Networks on non-uniform geometrical signals using\n  Euclidean spectral transformation", "comments": "Accepted as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been successful in processing data\nsignals that are uniformly sampled in the spatial domain (e.g., images).\nHowever, most data signals do not natively exist on a grid, and in the process\nof being sampled onto a uniform physical grid suffer significant aliasing error\nand information loss. Moreover, signals can exist in different topological\nstructures as, for example, points, lines, surfaces and volumes. It has been\nchallenging to analyze signals with mixed topologies (for example, point cloud\nwith surface mesh). To this end, we develop mathematical formulations for\nNon-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample\nnonuniform data signals of different topologies defined on a simplex mesh into\nthe spectral domain with no spatial sampling error. The spectral transform is\nperformed in the Euclidean space, which removes the translation ambiguity from\nworks on the graph spectrum. Our representation has four distinct advantages:\n(1) the process causes no spatial sampling error during the initial sampling,\n(2) the generality of this approach provides a unified framework for using CNNs\nto analyze signals of mixed topologies, (3) it allows us to leverage\nstate-of-the-art backbone CNN architectures for effective learning without\nhaving to design a particular architecture for a particular data structure in\nan ad-hoc fashion, and (4) the representation allows weighted meshes where each\nelement has a different weight (i.e., texture) indicating local properties. We\nachieve results on par with the state-of-the-art for the 3D shape retrieval\ntask, and a new state-of-the-art for the point cloud to surface reconstruction\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 21:23:33 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Wang", "Dequan", ""], ["Huang", "Jingwei", ""], ["Marcus", "Philip", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1901.02078", "submitter": "Stephen Phillips", "authors": "Stephen Phillips, Kostas Daniilidis", "title": "All Graphs Lead to Rome: Learning Geometric and Cycle-Consistent\n  Representations with Graph Convolutional Networks", "comments": "9 pages, 7 figures, 2 tables, 2 supplemental figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature matching is a fundamental part of many geometric computer\nvision applications, and using multiple images can improve performance. In this\nwork, we formulate multi-image matching as a graph embedding problem then use a\nGraph Convolutional Network to learn an appropriate embedding function for\naligning image features. We use cycle consistency to train our network in an\nunsupervised fashion, since ground truth correspondence is difficult or\nexpensive to aquire. In addition, geometric consistency losses can be added at\ntraining time, even if the information is not available in the test set, unlike\nprevious approaches that optimize cycle consistency directly. To the best of\nour knowledge, no other works have used learning for multi-image feature\nmatching. Our experiments show that our method is competitive with other\noptimization based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 21:35:20 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Phillips", "Stephen", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1901.02103", "submitter": "Maxim Naumov", "authors": "Maxim Naumov", "title": "On the Dimensionality of Embeddings for Sparse Features and Data", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we discuss a common misconception, namely that embeddings are\nalways used to reduce the dimensionality of the item space. We show that when\nwe measure dimensionality in terms of information entropy then the embedding of\nsparse probability distributions, that can be used to represent sparse features\nor data, may or not reduce the dimensionality of the item space. However, the\nembeddings do provide a different and often more meaningful representation of\nthe items for a particular task at hand. Also, we give upper bounds and more\nprecise guidelines for choosing the embedding dimension.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 23:30:14 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Naumov", "Maxim", ""]]}, {"id": "1901.02106", "submitter": "Sofia Broom\\'e", "authors": "Sofia Broom\\'e, Karina Bech Gleerup, Pia Haubro Andersen, Hedvig\n  Kjellstr\\\"om", "title": "Dynamics are Important for the Recognition of Equine Pain in Video", "comments": "CVPR 2019: IEEE Conference on Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prerequisite to successfully alleviate pain in animals is to recognize it,\nwhich is a great challenge in non-verbal species. Furthermore, prey animals\nsuch as horses tend to hide their pain. In this study, we propose a deep\nrecurrent two-stream architecture for the task of distinguishing pain from\nnon-pain in videos of horses. Different models are evaluated on a unique\ndataset showing horses under controlled trials with moderate pain induction,\nwhich has been presented in earlier work. Sequential models are experimentally\ncompared to single-frame models, showing the importance of the temporal\ndimension of the data, and are benchmarked against a veterinary expert\nclassification of the data. We additionally perform baseline comparisons with\ngeneralized versions of state-of-the-art human pain recognition methods. While\nequine pain detection in machine learning is a novel field, our results surpass\nveterinary expert performance and outperform pain detection results reported\nfor other larger non-human species.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 23:47:11 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 14:01:59 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Broom\u00e9", "Sofia", ""], ["Gleerup", "Karina Bech", ""], ["Andersen", "Pia Haubro", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1901.02132", "submitter": "Jiecao Yu", "authors": "Jiecao Yu, Jongsoo Park, Maxim Naumov", "title": "Spatial-Winograd Pruning Enabling Sparse Winograd Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are deployed in various\napplications but demand immense computational requirements. Pruning techniques\nand Winograd convolution are two typical methods to reduce the CNN computation.\nHowever, they cannot be directly combined because Winograd transformation fills\nin the sparsity resulting from pruning. Li et al. (2017) propose sparse\nWinograd convolution in which weights are directly pruned in the Winograd\ndomain, but this technique is not very practical because Winograd-domain\nretraining requires low learning rates and hence significantly longer training\ntime. Besides, Liu et al. (2018) move the ReLU function into the Winograd\ndomain, which can help increase the weight sparsity but requires changes in the\nnetwork structure. To achieve a high Winograd-domain weight sparsity without\nchanging network structures, we propose a new pruning method, spatial-Winograd\npruning. As the first step, spatial-domain weights are pruned in a structured\nway, which efficiently transfers the spatial-domain sparsity into the Winograd\ndomain and avoids Winograd-domain retraining. For the next step, we also\nperform pruning and retraining directly in the Winograd domain but propose to\nuse an importance factor matrix to adjust weight importance and weight\ngradients. This adjustment makes it possible to effectively retrain the pruned\nWinograd-domain network without changing the network structure. For the three\nmodels on the datasets of CIFAR10, CIFAR-100, and ImageNet, our proposed method\ncan achieve the Winograd domain sparsities of 63%, 50%, and 74%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 02:17:44 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Yu", "Jiecao", ""], ["Park", "Jongsoo", ""], ["Naumov", "Maxim", ""]]}, {"id": "1901.02154", "submitter": "Yueru Chen", "authors": "Yueru Chen, Yijing Yang, Wei Wang and C.-C. Jay Kuo", "title": "Ensembles of feedforward-designed convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ensemble method that fuses the output decision vectors of multiple\nfeedforward-designed convolutional neural networks (FF-CNNs) to solve the image\nclassification problem is proposed in this work. To enhance the performance of\nthe ensemble system, it is critical to increasing the diversity of FF-CNN\nmodels. To achieve this objective, we introduce diversities by adopting three\nstrategies: 1) different parameter settings in convolutional layers, 2)\nflexible feature subsets fed into the Fully-connected (FC) layers, and 3)\nmultiple image embeddings of the same input source. Furthermore, we partition\ninput samples into easy and hard ones based on their decision confidence\nscores. As a result, we can develop a new ensemble system tailored to hard\nsamples to further boost classification accuracy. Experiments are conducted on\nthe MNIST and CIFAR-10 datasets to demonstrate the effectiveness of the\nensemble method.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 04:48:22 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Chen", "Yueru", ""], ["Yang", "Yijing", ""], ["Wang", "Wei", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1901.02184", "submitter": "Quanshi Zhang", "authors": "Zenan Ling, Haotian Ma, Yu Yang, Robert C. Qiu, Song-Chun Zhu, and\n  Quanshi Zhang", "title": "Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to disentangle and interpret contextual effects\nthat are encoded in a pre-trained deep neural network. We use our method to\nexplain the gaming strategy of the alphaGo Zero model. Unlike previous studies\nthat visualized image appearances corresponding to the network output or a\nneural activation only from a global perspective, our research aims to clarify\nhow a certain input unit (dimension) collaborates with other units (dimensions)\nto constitute inference patterns of the neural network and thus contribute to\nthe network output. The analysis of local contextual effects w.r.t. certain\ninput units is of special values in real applications. Explaining the logic of\nthe alphaGo Zero model is a typical application. In experiments, our method\nsuccessfully disentangled the rationale of each move during the Go game.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 07:23:05 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ling", "Zenan", ""], ["Ma", "Haotian", ""], ["Yang", "Yu", ""], ["Qiu", "Robert C.", ""], ["Zhu", "Song-Chun", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1901.02199", "submitter": "Louis Clouatre", "authors": "Louis Clou\\^atre, Marc Demers", "title": "FIGR: Few-shot Image Generation with Reptile", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) boast impressive capacity to generate\nrealistic images. However, like much of the field of deep learning, they\nrequire an inordinate amount of data to produce results, thereby limiting their\nusefulness in generating novelty. In the same vein, recent advances in\nmeta-learning have opened the door to many few-shot learning applications. In\nthe present work, we propose Few-shot Image Generation using Reptile (FIGR), a\nGAN meta-trained with Reptile. Our model successfully generates novel images on\nboth MNIST and Omniglot with as little as 4 images from an unseen class. We\nfurther contribute FIGR-8, a new dataset for few-shot image generation, which\ncontains 1,548,944 icons categorized in over 18,409 classes. Trained on FIGR-8,\ninitial results show that our model can generalize to more advanced concepts\n(such as \"bird\" and \"knife\") from as few as 8 samples from a previously unseen\nclass of images and as little as 10 training steps through those 8 images. This\nwork demonstrates the potential of training a GAN for few-shot image generation\nand aims to set a new benchmark for future work in the domain.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 08:15:08 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Clou\u00e2tre", "Louis", ""], ["Demers", "Marc", ""]]}, {"id": "1901.02212", "submitter": "Ilke Demir", "authors": "Umur Aybars Ciftci, Ilke Demir", "title": "FakeCatcher: Detection of Synthetic Portrait Videos using Biological\n  Signals", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI), accepted July 2020. Dataset: http://bit.ly/FakeCatcher", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3009287", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of fake portrait videos poses direct threats on\nsociety, law, and privacy. Believing the fake video of a politician,\ndistributing fake pornographic content of celebrities, fabricating impersonated\nfake videos as evidence in courts are just a few real world consequences of\ndeep fakes. We present a novel approach to detect synthetic content in portrait\nvideos, as a preventive solution for the emerging threat of deep fakes. In\nother words, we introduce a deep fake detector. We observe that detectors\nblindly utilizing deep learning are not effective in catching fake content, as\ngenerative models produce formidably realistic results. Our key assertion\nfollows that biological signals hidden in portrait videos can be used as an\nimplicit descriptor of authenticity, because they are neither spatially nor\ntemporally preserved in fake content. To prove and exploit this assertion, we\nfirst engage several signal transformations for the pairwise separation\nproblem, achieving 99.39% accuracy. Second, we utilize those findings to\nformulate a generalized classifier for fake content, by analyzing proposed\nsignal transformations and corresponding feature sets. Third, we generate novel\nsignal maps and employ a CNN to improve our traditional classifier for\ndetecting synthetic content. Lastly, we release an \"in the wild\" dataset of\nfake portrait videos that we collected as a part of our evaluation process. We\nevaluate FakeCatcher on several datasets, resulting with 96%, 94.65%, 91.50%,\nand 91.07% accuracies, on Face Forensics, Face Forensics++, CelebDF, and on our\nnew Deep Fakes Dataset respectively. We also analyze signals from various\nfacial regions, under image distortions, with varying segment durations, from\ndifferent generators, against unseen datasets, and under several dimensionality\nreduction techniques.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 09:20:36 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 03:06:13 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 03:02:54 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ciftci", "Umur Aybars", ""], ["Demir", "Ilke", ""]]}, {"id": "1901.02229", "submitter": "Krishna Kanth Nakka", "authors": "Krishna Kanth Nakka, Mathieu Salzmann", "title": "Interpretable BoW Networks for Adversarial Example Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to providing interpretability to deep convolutional\nneural networks (CNNs) consists of visualizing either their feature maps, or\nthe image regions that contribute the most to the prediction. In this paper, we\nintroduce an alternative strategy to interpret the results of a CNN. To this\nend, we leverage a Bag of visual Word representation within the network and\nassociate a visual and semantic meaning to the corresponding codebook elements\nvia the use of a generative adversarial network. The reason behind the\nprediction for a new sample can then be interpreted by looking at the visual\nrepresentation of the most highly activated codeword. We then propose to\nexploit our interpretable BoW networks for adversarial example detection. To\nthis end, we build upon the intuition that, while adversarial samples look very\nsimilar to real images, to produce incorrect predictions, they should activate\ncodewords with a significantly different visual representation. We therefore\ncast the adversarial example detection problem as that of comparing the input\nimage with the most highly activated visual codeword. As evidenced by our\nexperiments, this allows us to outperform the state-of-the-art adversarial\nexample detection methods on standard benchmarks, independently of the attack\nstrategy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 10:04:33 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Nakka", "Krishna Kanth", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1901.02237", "submitter": "Xin Zhao", "authors": "Xin Zhao, Zhe Liu, Ruolan Hu, Kaiqi Huang", "title": "3D Object Detection Using Scale Invariant and Feature Reweighting\n  Networks", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": "10.1609/aaai.v33i01.33019267", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection plays an important role in a large number of real-world\napplications. It requires us to estimate the localizations and the orientations\nof 3D objects in real scenes. In this paper, we present a new network\narchitecture which focuses on utilizing the front view images and frustum point\nclouds to generate 3D detection results. On the one hand, a PointSIFT module is\nutilized to improve the performance of 3D segmentation. It can capture the\ninformation from different orientations in space and the robustness to\ndifferent scale shapes. On the other hand, our network obtains the useful\nfeatures and suppresses the features with less information by a SENet module.\nThis module reweights channel features and estimates the 3D bounding boxes more\neffectively. Our method is evaluated on both KITTI dataset for outdoor scenes\nand SUN-RGBD dataset for indoor scenes. The experimental results illustrate\nthat our method achieves better performance than the state-of-the-art methods\nespecially when point clouds are highly sparse.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 10:31:38 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhao", "Xin", ""], ["Liu", "Zhe", ""], ["Hu", "Ruolan", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1901.02284", "submitter": "Xu Chen", "authors": "Xu Chen, Jie Song, Otmar Hilliges", "title": "Unpaired Pose Guided Human Image Generation", "comments": "camera ready for CVPR 2019 VUHCS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the task of full generative modelling of realistic images\nof humans, guided only by coarse sketch of the pose, while providing control\nover the specific instance or type of outfit worn by the user. This is a\ndifficult problem because input and output domain are very different and direct\nimage-to-image translation becomes infeasible. We propose an end-to-end\ntrainable network under the generative adversarial framework, that provides\ndetailed control over the final appearance while not requiring paired training\ndata and hence allows us to forgo the challenging problem of fitting 3D poses\nto 2D images. The model allows to generate novel samples conditioned on either\nan image taken from the target domain or a class label indicating the style of\nclothing (e.g., t-shirt). We thoroughly evaluate the architecture and the\ncontributions of the individual components experimentally. Finally, we show in\na large scale perceptual study that our approach can generate realistic looking\nimages and that participants struggle in detecting fake images versus real\nsamples, especially if faces are blurred.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 12:39:21 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 19:58:44 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Xu", ""], ["Song", "Jie", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1901.02338", "submitter": "Maryam Jaberi", "authors": "Maryam Jaberi, Marianna Pensky, Hassan Foroosh", "title": "Sparse One-Time Grab Sampling of Inliers", "comments": "WiML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating structures in \"big data\" and clustering them are among the most\nfundamental problems in computer vision, pattern recognition, data mining, and\nmany other other research fields. Over the past few decades, many studies have\nbeen conducted focusing on different aspects of these problems. One of the main\napproaches that is explored in the literature to tackle the problems of size\nand dimensionality is sampling subsets of the data in order to estimate the\ncharacteristics of the whole population, e.g. estimating the underlying\nclusters or structures in the data. In this paper, we propose a `one-time-grab'\nsampling algorithm\\cite{jaberi2015swift,jaberi2018sparse}. This method can be\nused as the front end to any supervised or unsupervised clustering method.\nRather than focusing on the strategy of maximizing the probability of sampling\ninliers, our goal is to minimize the number of samples needed to instantiate\nall underlying model instances. More specifically, our goal is to answer the\nfollowing question: {\\em `Given a very large population of points with $C$\nembedded structures and gross outliers, what is the minimum number of points\n$r$ to be selected randomly in one grab in order to make sure with probability\n$P$ that at least $\\varepsilon$ points are selected on each structure, where\n$\\varepsilon$ is the number of degrees of freedom of each structure.'}\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:25:33 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jaberi", "Maryam", ""], ["Pensky", "Marianna", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1901.02350", "submitter": "Xiaotao Liu", "authors": "Yundong Zhang, Xiang Xu, Xiaotao Liu", "title": "Robust and High Performance Face Detector", "comments": "arXiv admin note: text overlap with arXiv:1708.05237 and substantial\n  text overlap with arXiv:1809.02693 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, face detection has experienced significant performance\nimprovement with the boost of deep convolutional neural networks. In this\nreport, we reimplement the state-of-the-art detector SRN and apply some tricks\nproposed in the recent literatures to obtain an extremely strong face detector,\nnamed VIM-FD. In specific, we exploit more powerful backbone network like\nDenseNet-121, revisit the data augmentation based on data-anchor-sampling\nproposed in PyramidBox, and use the max-in-out label and anchor matching\nstrategy in SFD. In addition, we also introduce the attention mechanism to\nprovide additional supervision. Over the most popular and challenging face\ndetection benchmark, i.e., WIDER FACE, the proposed VIM-FD achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 09:06:57 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Zhang", "Yundong", ""], ["Xu", "Xiang", ""], ["Liu", "Xiaotao", ""]]}, {"id": "1901.02404", "submitter": "Ori Bar El", "authors": "Ori Bar El, Ori Licht, Netanel Yosephian", "title": "GILT: Generating Images from Long Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating an image reflecting the content of a long text is a complex process\nthat requires a sense of creativity. For example, creating a book cover or a\nmovie poster based on their summary or a food image based on its recipe. In\nthis paper we present the new task of generating images from long text that\ndoes not describe the visual content of the image directly. For this, we build\na system for generating high-resolution 256 $\\times$ 256 images of food\nconditioned on their recipes. The relation between the recipe text (without its\ntitle) to the visual content of the image is vague, and the textual structure\nof recipes is complex, consisting of two sections (ingredients and\ninstructions) both containing multiple sentences.\n  We used the recipe1M dataset to train and evaluate our model that is based on\na the StackGAN-v2 architecture.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 16:59:46 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["El", "Ori Bar", ""], ["Licht", "Ori", ""], ["Yosephian", "Netanel", ""]]}, {"id": "1901.02411", "submitter": "Ranjan Mondal", "authors": "Ranjan Mondal, Pulak Purkait, Sanchayan Santra and Bhabatosh Chanda", "title": "Morphological Networks for Image De-raining", "comments": "Mathematical Morphology \\and Optimization \\and Morphological Network\n  \\and Image Filtering", "journal-ref": "https://dgci2019.sciencesconf.org/", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical morphological methods have successfully been applied to filter\nout (emphasize or remove) different structures of an image. However, it is\nargued that these methods could be suitable for the task only if the type and\norder of the filter(s) as well as the shape and size of operator kernel are\ndesigned properly. Thus the existing filtering operators are problem (instance)\nspecific and are designed by the domain experts. In this work we propose a\nmorphological network that emulates classical morphological filtering\nconsisting of a series of erosion and dilation operators with trainable\nstructuring elements. We evaluate the proposed network for image de-raining\ntask where the SSIM and mean absolute error (MAE) loss corresponding to\npredicted and ground-truth clean image is back-propagated through the network\nto train the structuring elements. We observe that a single morphological\nnetwork can de-rain an image with any arbitrary shaped rain-droplets and\nachieves similar performance with the contemporary CNNs for this task with a\nfraction of trainable parameters (network size). The proposed morphological\nnetwork(MorphoN) is not designed specifically for de-raining and can readily be\napplied to similar filtering / noise cleaning tasks. The source code can be\nfound here https://github.com/ranjanZ/2D-Morphological-Network\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:13:47 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Mondal", "Ranjan", ""], ["Purkait", "Pulak", ""], ["Santra", "Sanchayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "1901.02413", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Xin Wang, Ying Nian Wu, Huilin Zhou, Song-Chun Zhu", "title": "Interpretable CNNs for Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generic method to learn interpretable convolutional\nfilters in a deep convolutional neural network (CNN) for object classification,\nwhere each interpretable filter encodes features of a specific object part. Our\nmethod does not require additional annotations of object parts or textures for\nsupervision. Instead, we use the same training data as traditional CNNs. Our\nmethod automatically assigns each interpretable filter in a high conv-layer\nwith an object part of a certain category during the learning process. Such\nexplicit knowledge representations in conv-layers of CNN help people clarify\nthe logic encoded in the CNN, i.e., answering what patterns the CNN extracts\nfrom an input image and uses for prediction. We have tested our method using\ndifferent benchmark CNNs with various structures to demonstrate the broad\napplicability of our method. Experiments have shown that our interpretable\nfilters are much more semantically meaningful than traditional filters.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:15:19 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 07:04:28 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhang", "Quanshi", ""], ["Wang", "Xin", ""], ["Wu", "Ying Nian", ""], ["Zhou", "Huilin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1901.02425", "submitter": "Sen Jia", "authors": "Sen Jia, Neil D. B. Bruce", "title": "Richer and Deeper Supervision Network for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Salient Object Detection (SOD) systems are mostly based on\nConvolutional Neural Networks (CNNs). Specifically, Deeply Supervised Saliency\n(DSS) system has shown it is very useful to add short connections to the\nnetwork and supervising on the side output. In this work, we propose a new SOD\nsystem which aims at designing a more efficient and effective way to pass back\nglobal information. Richer and Deeper Supervision (RDS) is applied to better\ncombine features from each side output without demanding much extra\ncomputational space. Meanwhile, the backbone network used for SOD is normally\npre-trained on the object classification dataset, ImageNet. But the pre-trained\nmodel has been trained on cropped images in order to only focus on\ndistinguishing features within the region of the object. But the ignored\nbackground information is also significant in the task of SOD. We try to solve\nthis problem by introducing the training data designed for object detection. A\ncoarse global information is learned based on an entire image with its bounding\nbox before training on the SOD dataset. The large-scale of object images can\nslightly improve the performance of SOD. Our experiment shows the proposed RDS\nnetwork achieves the state-of-the-art results on five public SOD datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:02:05 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jia", "Sen", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1901.02433", "submitter": "Zehua Cheng", "authors": "Liyao Gao, Zehua Cheng", "title": "Learning with Collaborative Neural Network Group by Reflection", "comments": "6 Pages. Ubicomp Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the present engineering of neural systems, the preparing of extensive\nscale learning undertakings generally not just requires a huge neural system\nwith a mind boggling preparing process yet additionally troublesome discover a\nclarification for genuine applications. In this paper, we might want to present\nthe Collaborative Neural Network Group (CNNG). CNNG is a progression of neural\nsystems that work cooperatively to deal with various errands independently in a\nsimilar learning framework. It is advanced from a solitary neural system by\nreflection. Along these lines, in light of various circumstances removed by the\ncalculation, the CNNG can perform diverse techniques when handling the\ninformation. The examples of chose methodology can be seen by human to make\nprofound adapting more reasonable. In our execution, the CNNG is joined by a\nfew moderately little neural systems. We give a progression of examinations to\nassess the execution of CNNG contrasted with other learning strategies. The\nCNNG is able to get a higher accuracy with a much lower training cost. We can\nreduce the error rate by 74.5% and reached the accuracy of 99.45% in MNIST with\nthree feedforward networks (4 layers) in one training epoch.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:20:44 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 13:29:20 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Gao", "Liyao", ""], ["Cheng", "Zehua", ""]]}, {"id": "1901.02442", "submitter": "Joseph Betthauser", "authors": "Joseph L. Betthauser, John T. Krall, Rahul R. Kaliki, Matthew S.\n  Fifer, and Nitish V. Thakor", "title": "Stable Electromyographic Sequence Prediction During Movement Transitions\n  using Temporal Convolutional Networks", "comments": "4 pages, 5 figures, accepted for Neural Engineering (NER) 2019\n  Conference", "journal-ref": null, "doi": "10.1109/NER.2019.8717169", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient muscle movements influence the temporal structure of myoelectric\nsignal patterns, often leading to unstable prediction behavior from\nmovement-pattern classification methods. We show that temporal convolutional\nnetwork sequential models leverage the myoelectric signal's history to discover\ncontextual temporal features that aid in correctly predicting movement\nintentions, especially during interclass transitions. We demonstrate\nmyoelectric classification using temporal convolutional networks to effect 3\nsimultaneous hand and wrist degrees-of-freedom in an experiment involving nine\nhuman-subjects. Temporal convolutional networks yield significant $(p<0.001)$\nperformance improvements over other state-of-the-art methods in terms of both\nclassification accuracy and stability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:48:50 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Betthauser", "Joseph L.", ""], ["Krall", "John T.", ""], ["Kaliki", "Rahul R.", ""], ["Fifer", "Matthew S.", ""], ["Thakor", "Nitish V.", ""]]}, {"id": "1901.02444", "submitter": "Yi-Hsuan Tsai", "authors": "Yi-Wen Chen, Yi-Hsuan Tsai, Chu-Ya Yang, Yen-Yu Lin, Ming-Hsuan Yang", "title": "Unseen Object Segmentation in Videos via Transferable Representations", "comments": "Accepted in ACCV'18 (oral). Code is available at\n  https://github.com/wenz116/TransferSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to learn object segmentation models in videos, conventional methods\nrequire a large amount of pixel-wise ground truth annotations. However,\ncollecting such supervised data is time-consuming and labor-intensive. In this\npaper, we exploit existing annotations in source images and transfer such\nvisual information to segment videos with unseen object categories. Without\nusing any annotations in the target video, we propose a method to jointly mine\nuseful segments and learn feature representations that better adapt to the\ntarget frames. The entire process is decomposed into two tasks: 1) solving a\nsubmodular function for selecting object-like segments, and 2) learning a CNN\nmodel with a transferable module for adapting seen categories in the source\ndomain to the unseen target video. We present an iterative update scheme\nbetween two tasks to self-learn the final solution for object segmentation.\nExperimental results on numerous benchmark datasets show that the proposed\nmethod performs favorably against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:49:51 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Chen", "Yi-Wen", ""], ["Tsai", "Yi-Hsuan", ""], ["Yang", "Chu-Ya", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1901.02446", "submitter": "Alexander Kirillov", "authors": "Alexander Kirillov, Ross Girshick, Kaiming He and Piotr Doll\\'ar", "title": "Panoptic Feature Pyramid Networks", "comments": "accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced panoptic segmentation task has renewed our\ncommunity's interest in unifying the tasks of instance segmentation (for thing\nclasses) and semantic segmentation (for stuff classes). However, current\nstate-of-the-art methods for this joint task use separate and dissimilar\nnetworks for instance and semantic segmentation, without performing any shared\ncomputation. In this work, we aim to unify these methods at the architectural\nlevel, designing a single network for both tasks. Our approach is to endow Mask\nR-CNN, a popular instance segmentation method, with a semantic segmentation\nbranch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly,\nthis simple baseline not only remains effective for instance segmentation, but\nalso yields a lightweight, top-performing method for semantic segmentation. In\nthis work, we perform a detailed study of this minimally extended version of\nMask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust\nand accurate baseline for both tasks. Given its effectiveness and conceptual\nsimplicity, we hope our method can serve as a strong baseline and aid future\nresearch in panoptic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:55:31 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 18:09:43 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kirillov", "Alexander", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1901.02452", "submitter": "Yang Li", "authors": "Yang Li, Sangwhan Cha", "title": "Face Recognition System", "comments": "Deep neural network, face recognition, server-client model, business\n  model, deep multi-model fusion, convolutional neural network. arXiv admin\n  note: substantial text overlap with arXiv:1811.07339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is one of the new and important branches in machine learning.\nDeep learning refers to a set of algorithms that solve various problems such as\nimages and texts by using various machine learning algorithms in multi-layer\nneural networks. Deep learning can be classified as a neural network from the\ngeneral category, but there are many changes in the concrete realization. At\nthe core of deep learning is feature learning, which is designed to obtain\nhierarchical information through hierarchical networks, so as to solve the\nimportant problems that previously required artificial design features. Deep\nLearning is a framework that contains several important algorithms. For\ndifferent applications (images, voice, text), you need to use different network\nmodels to achieve better results. With the development of deep learning and the\nintroduction of deep convolutional neural networks, the accuracy and speed of\nface recognition have made great strides. However, as we said above, the\nresults from different networks and models are very different. In this paper,\nfacial features are extracted by merging and comparing multiple models, and\nthen a deep neural network is constructed to train and construct the combined\nfeatures. In this way, the advantages of multiple models can be combined to\nmention the recognition accuracy. After getting a model with high accuracy, we\nbuild a product model. This article compares the pure-client model with the\nserver-client model, analyzes the pros and cons of the two models, and analyzes\nthe various commercial products that are required for the server-client model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:07:25 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Li", "Yang", ""], ["Cha", "Sangwhan", ""]]}, {"id": "1901.02453", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W.\n  Jacobs, and Jan Kautz", "title": "Neural Inverse Rendering of an Indoor Scene from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse rendering aims to estimate physical attributes of a scene, e.g.,\nreflectance, geometry, and lighting, from image(s). Inverse rendering has been\nstudied primarily for single objects or with methods that solve for only one of\nthe scene attributes. We propose the first learning-based approach that jointly\nestimates albedo, normals, and lighting of an indoor scene from a single image.\nOur key contribution is the Residual Appearance Renderer (RAR), which can be\ntrained to synthesize complex appearance effects (e.g., inter-reflection, cast\nshadows, near-field illumination, and realistic shading), which would be\nneglected otherwise. This enables us to perform self-supervised learning on\nreal data using a reconstruction loss, based on re-synthesizing the input image\nfrom the estimated components. We finetune with real data after pretraining\nwith synthetic data. To this end, we use physically-based rendering to create a\nlarge-scale synthetic dataset, which is a significant improvement over prior\ndatasets. Experimental results show that our approach outperforms\nstate-of-the-art methods that estimate one or more scene attributes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:12:42 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 01:12:40 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 07:24:52 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Gu", "Jinwei", ""], ["Kim", "Kihwan", ""], ["Liu", "Guilin", ""], ["Jacobs", "David W.", ""], ["Kautz", "Jan", ""]]}, {"id": "1901.02499", "submitter": "Da Ma", "authors": "Da Ma, Manuel J. Cardoso, Maria A. Zuluaga, Marc Modat, Nick. Powell,\n  Frances Wiseman, Victor Tybulewicz, Elizabeth Fisher, Mark. F. Lythgoe,\n  Sebastien Ourselin", "title": "Grey matter sublayer thickness estimation in themouse cerebellum", "comments": "8 pages, 7 figures, International Conference on Medical Image\n  Computing and Computer-Assisted Intervention 2015", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention. Springer, Cham, 2015", "doi": "10.1007/978-3-319-24574-4_77", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cerebellar grey matter morphology is an important feature to study\nneurodegenerative diseases such as Alzheimer's disease or Down's syndrome. Its\nvolume or thickness is commonly used as a surrogate imaging biomarker for such\ndiseases. Most studies about grey matter thickness estimation focused on the\ncortex, and little attention has been drawn on the morphology of the\ncerebellum. Using ex vivo high-resolution MRI, it is now possible to visualise\nthe different cell layers in the mouse cerebellum. In this work, we introduce a\nframework to extract the Purkinje layer within the grey matter, enabling the\nestimation of the thickness of the cerebellar grey matter, the granular layer\nand molecular layer from gadolinium-enhanced ex vivo mouse brain MRI.\nApplication to mouse model of Down's syndrome found reduced cortical and layer\nthicknesses in the transchromosomic group.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:23:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ma", "Da", ""], ["Cardoso", "Manuel J.", ""], ["Zuluaga", "Maria A.", ""], ["Modat", "Marc", ""], ["Powell", "Nick.", ""], ["Wiseman", "Frances", ""], ["Tybulewicz", "Victor", ""], ["Fisher", "Elizabeth", ""], ["Lythgoe", "Mark. F.", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1901.02508", "submitter": "Fereshteh Sadat Bashiri", "authors": "Fereshteh S. Bashiri, Reihaneh Rostami, Peggy Peissig, Roshan M.\n  D'Souza, Zeyun Yu", "title": "An Application of Manifold Learning in Global Shape Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid expansion of applied 3D computational vision, shape\ndescriptors have become increasingly important for a wide variety of\napplications and objects from molecules to planets. Appropriate shape\ndescriptors are critical for accurate (and efficient) shape retrieval and 3D\nmodel classification. Several spectral-based shape descriptors have been\nintroduced by solving various physical equations over a 3D surface model. In\nthis paper, for the first time, we incorporate a specific group of techniques\nin statistics and machine learning, known as manifold learning, to develop a\nglobal shape descriptor in the computer graphics domain. The proposed\ndescriptor utilizes the Laplacian Eigenmap technique in which the Laplacian\neigenvalue problem is discretized using an exponential weighting scheme. As a\nresult, our descriptor eliminates the limitations tied to the existing spectral\ndescriptors, namely dependency on triangular mesh representation and high\nintra-class quality of 3D models. We also present a straightforward\nnormalization method to obtain a scale-invariant descriptor. The extensive\nexperiments performed in this study show that the present contribution provides\na highly discriminative and robust shape descriptor under the presence of a\nhigh level of noise, random scale variations, and low sampling rate, in\naddition to the known isometric-invariance property of the Laplace-Beltrami\noperator. The proposed method significantly outperforms state-of-the-art\nalgorithms on several non-rigid shape retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:41:49 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bashiri", "Fereshteh S.", ""], ["Rostami", "Reihaneh", ""], ["Peissig", "Peggy", ""], ["D'Souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1901.02511", "submitter": "Sumanth Chennupati", "authors": "Ganesh Sistu, Sumanth Chennupati and Senthil Yogamani", "title": "Multi-stream CNN based Video Semantic Segmentation for Automated Driving", "comments": "Accepted for Oral Presentation at VISAPP 2019", "journal-ref": null, "doi": "10.5220/0007248401730180", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of semantic segmentation algorithms operate on a single frame even\nin the case of videos. In this work, the goal is to exploit temporal\ninformation within the algorithm model for leveraging motion cues and temporal\nconsistency. We propose two simple high-level architectures based on Recurrent\nFCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent\nnetwork namely LSTM is inserted between the encoder and decoder. MSFCN combines\nthe encoders of different frames into a fused encoder via 1x1 channel-wise\nconvolution. We use a ResNet50 network as the baseline encoder and construct\nthree networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3\nproduces the best results with an accuracy improvement of 9% and 15% for\nHighway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using\nmean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS\ndatasets over the baseline FCN network. We also designed an efficient version\nof MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The\nefficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA\nwith negligible increase in computational complexity compared to the baseline\nversion.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:45:49 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Sistu", "Ganesh", ""], ["Chennupati", "Sumanth", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1901.02513", "submitter": "Ertunc Erdil", "authors": "Ertunc Erdil, Ali Ozgur Argunsah, Tolga Tasdizen, Devrim Unay, Mujdat\n  Cetin", "title": "Combining nonparametric spatial context priors with nonparametric shape\n  priors for dendritic spine segmentation in 2-photon microscopy images", "comments": "IEEE International Symposium on Biomedical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven segmentation is an important initial step of shape prior-based\nsegmentation methods since it is assumed that the data term brings a curve to a\nplausible level so that shape and data terms can then work together to produce\nbetter segmentations. When purely data driven segmentation produces poor\nresults, the final segmentation is generally affected adversely. One challenge\nfaced by many existing data terms is due to the fact that they consider only\npixel intensities to decide whether to assign a pixel to the foreground or to\nthe background region. When the distributions of the foreground and background\npixel intensities have significant overlap, such data terms become ineffective,\nas they produce uncertain results for many pixels in a test image. In such\ncases, using prior information about the spatial context of the object to be\nsegmented together with the data term can bring a curve to a plausible stage,\nwhich would then serve as a good initial point to launch shape-based\nsegmentation. In this paper, we propose a new segmentation approach that\ncombines nonparametric context priors with a learned-intensity-based data term\nand nonparametric shape priors. We perform experiments for dendritic spine\nsegmentation in both 2D and 3D 2-photon microscopy images. The experimental\nresults demonstrate that using spatial context priors leads to significant\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:47:46 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 12:21:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Erdil", "Ertunc", ""], ["Argunsah", "Ali Ozgur", ""], ["Tasdizen", "Tolga", ""], ["Unay", "Devrim", ""], ["Cetin", "Mujdat", ""]]}, {"id": "1901.02520", "submitter": "Yuchen He", "authors": "Yuchen He and Sung Ha Kang", "title": "Lattice Identification and Separation: Theory and Algorithm", "comments": "30 Pages plus 4 pages of Appendix. 4 Pages of References. 24 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.MG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by lattice mixture identification and grain boundary detection, we\npresent a framework for lattice pattern representation and comparison, and\npropose an efficient algorithm for lattice separation. We define new scale and\nshape descriptors, which helps to considerably reduce the size of equivalence\nclasses of lattice bases. These finitely many equivalence relations are fully\ncharacterized by modular group theory. We construct the lattice space\n$\\mathscr{L}$ based on the equivalent descriptors and define a metric\n$d_{\\mathscr{L}}$ to accurately quantify the visual similarities and\ndifferences between lattices. Furthermore, we introduce the Lattice\nIdentification and Separation Algorithm (LISA), which identifies each lattice\npatterns from superposed lattices. LISA finds lattice candidates from the high\nresponses in the image spectrum, then sequentially extracts different layers of\nlattice patterns one by one. Analyzing the frequency components, we reveal the\nintricate dependency of LISA's performances on particle radius, lattice\ndensity, and relative translations. Various numerical experiments are designed\nto show LISA's robustness against a large number of lattice layers, moir\\'{e}\npatterns and missing particles.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:25:32 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["He", "Yuchen", ""], ["Kang", "Sung Ha", ""]]}, {"id": "1901.02527", "submitter": "Dong Huk Park", "authors": "Dong Huk Park, Trevor Darrell, Anna Rohrbach", "title": "Robust Change Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing what has changed in a scene can be useful to a user, but only if\ngenerated text focuses on what is semantically relevant. It is thus important\nto distinguish distractors (e.g. a viewpoint change) from relevant changes\n(e.g. an object has moved). We present a novel Dual Dynamic Attention Model\n(DUDA) to perform robust Change Captioning. Our model learns to distinguish\ndistractors from semantic changes, localize the changes via Dual Attention over\n\"before\" and \"after\" images, and accurately describe them in natural language\nvia Dynamic Speaker, by adaptively focusing on the necessary visual inputs\n(e.g. \"before\" or \"after\" image). To study the problem in depth, we collect a\nCLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene\nchanges. We benchmark a number of baselines on our dataset, and systematically\nstudy different change types and robustness to distractors. We show the\nsuperiority of our DUDA model in terms of both change captioning and\nlocalization. We also show that our approach is general, obtaining\nstate-of-the-art results on the recent realistic Spot-the-Diff dataset which\nhas no distractors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 21:29:42 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 00:18:00 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Park", "Dong Huk", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "1901.02529", "submitter": "Xuan Thanh Nguyen", "authors": "X. T. Nguyen, T. D. Ngo and T. H. Le", "title": "A Spatial-temporal 3D Human Pose Reconstruction Framework", "comments": "10 pages. JIPS Journal 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose reconstruction from single-view camera is a difficult and\nchallenging topic. Many approaches have been proposed, but almost focusing on\nframe-by-frame independently while inter-frames are highly correlated in a pose\nsequence. In contrast, we introduce a novel spatial-temporal 3D reconstruction\nframework that leverages both intra and inter frame relationships in\nconsecutive 2D pose sequences. Orthogonal Matching Pursuit (OMP) algorithm,\npre-trained Pose-angle Limits and Temporal Models have been implemented. We\nquantitatively compare our framework versus recent works on CMU motion capture\ndataset and Vietnamese traditional dance sequences. Our method outperforms\nothers with 10 percent lower of Euclidean reconstruction error and robustness\nagainst Gaussian noise. Additionally, it is also important to mention that our\nreconstructed 3D pose sequences are smoother and more natural than others.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 21:46:52 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 09:20:23 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Nguyen", "X. T.", ""], ["Ngo", "T. D.", ""], ["Le", "T. H.", ""]]}, {"id": "1901.02532", "submitter": "Yahui Liu", "authors": "Xiaohu Lu, Yahui Liu, Kai Li", "title": "Fast 3D Line Segment Detection From Unorganized Point Cloud", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a very simple but efficient algorithm for 3D line segment\ndetection from large scale unorganized point cloud. Unlike traditional methods\nwhich usually extract 3D edge points first and then link them to fit for 3D\nline segments, we propose a very simple 3D line segment detection algorithm\nbased on point cloud segmentation and 2D line detection. Given the input\nunorganized point cloud, three steps are performed to detect 3D line segments.\nFirstly, the point cloud is segmented into 3D planes via region growing and\nregion merging. Secondly, for each 3D plane, all the points belonging to it are\nprojected onto the plane itself to form a 2D image, which is followed by 2D\ncontour extraction and Least Square Fitting to get the 2D line segments. Those\n2D line segments are then re-projected onto the 3D plane to get the\ncorresponding 3D line segments. Finally, a post-processing procedure is\nproposed to eliminate outliers and merge adjacent 3D line segments. Experiments\non several public datasets demonstrate the efficiency and robustness of our\nmethod. More results and the C++ source code of the proposed algorithm are\npublicly available at https://github.com/xiaohulugo/3DLineDetection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 21:51:51 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Lu", "Xiaohu", ""], ["Liu", "Yahui", ""], ["Li", "Kai", ""]]}, {"id": "1901.02537", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Jiashen Cao, Micheal S. Ryoo, Hyesoon Kim", "title": "Collaborative Execution of Deep Neural Networks on Internet of Things\n  Devices", "comments": "Updated version after sysML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advancements in deep neural networks (DNNs), we are able to solve\ntraditionally challenging problems. Since DNNs are compute intensive,\nconsumers, to deploy a service, need to rely on expensive and scarce compute\nresources in the cloud. This approach, in addition to its dependability on\nhigh-quality network infrastructure and data centers, raises new privacy\nconcerns. These challenges may limit DNN-based applications, so many\nresearchers have tried optimize DNNs for local and in-edge execution. However,\ninadequate power and computing resources of edge devices along with small\nnumber of requests limits current optimizations applicability, such as batch\nprocessing. In this paper, we propose an approach that utilizes aggregated\nexisting computing power of Internet of Things (IoT) devices surrounding an\nenvironment by creating a collaborative network. In this approach, IoT devices\ncooperate to conduct single-batch inferencing in real time. While exploiting\nseveral new model-parallelism methods and their distribution characteristics,\nour approach enhances the collaborative network by creating a balanced and\ndistributed processing pipeline. We have illustrated our work using many\nRaspberry Pis with studying DNN models such as AlexNet, VGG16, Xception, and\nC3D.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:05:16 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Cao", "Jiashen", ""], ["Ryoo", "Micheal S.", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1901.02542", "submitter": "Ovidiu Vaduvescu", "authors": "D. Copandean, C. Nandra, D. Gorgan, O. Vaduvescu", "title": "Asteroids Detection Technique: Classic \"Blink\" An Automated Approch", "comments": "Conference: 2018 IEEE International Conference on Automation, Quality\n  and Testing, Robotics (AQTR), 24-26 May 2018, Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/AQTR.2018.8402768", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asteroids detection is a very important research field that received\nincreased attention in the last couple of decades. Some major surveys have\ntheir own dedicated people, equipment and detection applications, so they are\ndiscovering Near Earth Asteroids (NEAs) daily. The interest in asteroids is not\nlimited to those major surveys, it is shared by amateurs and mini-surveys too.\nA couple of them are using the few existent software solutions, most of which\nare developed by amateurs. The rest obtain their results in a visual manner:\nthey \"blink\" a sequence of reduced images of the same field, taken at a\nspecific time interval, and they try to detect a real moving object in the\nresulting animation. Such a technique becomes harder with the increase in size\nof the CCD cameras. Aiming to replace manual detection, we propose an automated\n\"blink\" technique for asteroids detection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:34:18 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Copandean", "D.", ""], ["Nandra", "C.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.02545", "submitter": "Ovidiu Vaduvescu", "authors": "T. Stefanut, V. Bacu, C. Nandra, D. Balasz, D. Gorgan and O. Vaduvescu", "title": "NEARBY Platform: Algorithm for Automated Asteroids Detection in\n  Astronomical Images", "comments": "IEEE 14th International Conference on Intelligent Computer\n  Communication and Processing (ICCP), Sep 6-8, 2018, Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/ICCP.2018.8516594", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past two decades an increasing interest in discovering Near Earth\nObjects has been noted in the astronomical community. Dedicated surveys have\nbeen operated for data acquisition and processing, resulting in the present\ndiscovery of over 18.000 objects that are closer than 30 million miles of\nEarth. Nevertheless, recent events have shown that there still are many\nundiscovered asteroids that can be on collision course to Earth. This article\npresents an original NEO detection algorithm developed in the NEARBY research\nobject, that has been integrated into an automated MOPS processing pipeline\naimed at identifying moving space objects based on the blink method. Proposed\nsolution can be considered an approach of Big Data processing and analysis,\nimplementing visual analytics techniques for rapid human data validation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:45:28 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Stefanut", "T.", ""], ["Bacu", "V.", ""], ["Nandra", "C.", ""], ["Balasz", "D.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.02551", "submitter": "Aron Yu", "authors": "Aron Yu, Kristen Grauman", "title": "Thinking Outside the Pool: Active Training Image Creation for Relative\n  Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current wisdom suggests more labeled image data is always better, and\nobtaining labels is the bottleneck. Yet curating a pool of sufficiently diverse\nand informative images is itself a challenge. In particular, training image\ncuration is problematic for fine-grained attributes, where the subtle visual\ndifferences of interest may be rare within traditional image sources. We\npropose an active image generation approach to address this issue. The main\nidea is to jointly learn the attribute ranking task while also learning to\ngenerate novel realistic image samples that will benefit that task. We\nintroduce an end-to-end framework that dynamically \"imagines\" image pairs that\nwould confuse the current model, presents them to human annotators for\nlabeling, then improves the predictive model with the new examples. With\nresults on two datasets, we show that by thinking outside the pool of real\nimages, our approach gains generalization accuracy for challenging fine-grained\nattribute comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 23:10:03 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Yu", "Aron", ""], ["Grauman", "Kristen", ""]]}, {"id": "1901.02571", "submitter": "Chao Liu", "authors": "Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa Narasimhan and Jan Kautz", "title": "Neural RGB->D Sensing: Depth and Uncertainty from a Video Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth sensing is crucial for 3D reconstruction and scene understanding.\nActive depth sensors provide dense metric measurements, but often suffer from\nlimitations such as restricted operating ranges, low spatial resolution, sensor\ninterference, and high power consumption. In this paper, we propose a deep\nlearning (DL) method to estimate per-pixel depth and its uncertainty\ncontinuously from a monocular video stream, with the goal of effectively\nturning an RGB camera into an RGB-D camera. Unlike prior DL-based methods, we\nestimate a depth probability distribution for each pixel rather than a single\ndepth value, leading to an estimate of a 3D depth probability volume for each\ninput frame. These depth probability volumes are accumulated over time under a\nBayesian filtering framework as more incoming frames are processed\nsequentially, which effectively reduces depth uncertainty and improves\naccuracy, robustness, and temporal stability. Compared to prior work, the\nproposed approach achieves more accurate and stable results, and generalizes\nbetter to new datasets. Experimental results also show the output of our\napproach can be directly fed into classical RGB-D based 3D scanning methods for\n3D scene reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 01:14:46 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Liu", "Chao", ""], ["Gu", "Jinwei", ""], ["Kim", "Kihwan", ""], ["Narasimhan", "Srinivasa", ""], ["Kautz", "Jan", ""]]}, {"id": "1901.02573", "submitter": "Fabricio Breve", "authors": "Fabricio Aparecido Breve", "title": "Interactive Image Segmentation using Label Propagation through Complex\n  Networks", "comments": "Paper accepted for publication in Expert Systems With Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive image segmentation is a topic of many studies in image\nprocessing. In a conventional approach, a user marks some pixels of the\nobject(s) of interest and background, and an algorithm propagates these labels\nto the rest of the image. This paper presents a new graph-based method for\ninteractive segmentation with two stages. In the first stage, nodes\nrepresenting pixels are connected to their $k$-nearest neighbors to build a\ncomplex network with the small-world property to propagate the labels quickly.\nIn the second stage, a regular network in a grid format is used to refine the\nsegmentation on the object borders. Despite its simplicity, the proposed method\ncan perform the task with high accuracy. Computer simulations are performed\nusing some real-world images to show its effectiveness in both two-classes and\nmulti-classes problems. It is also applied to all the images from the Microsoft\nGrabCut dataset for comparison, and the segmentation accuracy is comparable to\nthose achieved by some state-of-the-art methods, while it is faster than them.\nIn particular, it outperforms some recent approaches when the user input is\ncomposed only by a few \"scribbles\" draw over the objects. Its computational\ncomplexity is only linear on the image size at the best-case scenario and\nlinearithmic in the worst case.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 01:22:23 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Breve", "Fabricio Aparecido", ""]]}, {"id": "1901.02579", "submitter": "Zhenqiang Li", "authors": "Zhenqiang Li, Yifei Huang, Minjie Cai, Yoichi Sato", "title": "Manipulation-skill Assessment from Videos with Spatial Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in computer vision have made it possible to automatically\nassess from videos the manipulation skills of humans in performing a task,\nwhich breeds many important applications in domains such as health\nrehabilitation and manufacturing. Previous methods of video-based skill\nassessment did not consider the attention mechanism humans use in assessing\nvideos, limiting their performance as only a small part of video regions is\ninformative for skill assessment. Our motivation here is to estimate attention\nin videos that helps to focus on critically important video regions for better\nskill assessment. In particular, we propose a novel RNN-based spatial attention\nmodel that considers accumulated attention state from previous frames as well\nas high-level knowledge about the progress of an undergoing task. We evaluate\nour approach on a newly collected dataset of infant grasping task and four\nexisting datasets of hand manipulation tasks. Experiment results demonstrate\nthat state-of-the-art performance can be achieved by considering attention in\nautomatic skill assessment.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 01:45:06 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 05:03:51 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Zhenqiang", ""], ["Huang", "Yifei", ""], ["Cai", "Minjie", ""], ["Sato", "Yoichi", ""]]}, {"id": "1901.02596", "submitter": "Chuhui Xue", "authors": "Chuhui Xue, Shijian Lu and Wei Zhang", "title": "MSR: Multi-Scale Shape Regression for Scene Text Detection", "comments": "Accepted by IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art scene text detection techniques predict quadrilateral boxes\nthat are prone to localization errors while dealing with straight or curved\ntext lines of different orientations and lengths in scenes. This paper presents\na novel multi-scale shape regression network (MSR) that is capable of locating\ntext lines of different lengths, shapes and curvatures in scenes. The proposed\nMSR detects scene texts by predicting dense text boundary points that\ninherently capture the location and shape of text lines accurately and are also\nmore tolerant to the variation of text line length as compared with the state\nof the arts using proposals or segmentation. Additionally, the multi-scale\nnetwork extracts and fuses features at different scales which demonstrates\nsuperb tolerance to the text scale variation. Extensive experiments over\nseveral public datasets show that the proposed MSR obtains superior detection\nperformance for both curved and straight text lines of different lengths and\norientations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 04:00:03 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 08:17:03 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Xue", "Chuhui", ""], ["Lu", "Shijian", ""], ["Zhang", "Wei", ""]]}, {"id": "1901.02598", "submitter": "Chien-Yi Chang", "authors": "Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, Juan Carlos\n  Niebles", "title": "D3TW: Discriminative Differentiable Dynamic Time Warping for Weakly\n  Supervised Action Alignment and Segmentation", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address weakly supervised action alignment and segmentation in videos,\nwhere only the order of occurring actions is available during training. We\npropose Discriminative Differentiable Dynamic Time Warping (D3TW), the first\ndiscriminative model using weak ordering supervision. The key technical\nchallenge for discriminative modeling with weak supervision is that the loss\nfunction of the ordering supervision is usually formulated using dynamic\nprogramming and is thus not differentiable. We address this challenge with a\ncontinuous relaxation of the min-operator in dynamic programming and extend the\nalignment loss to be differentiable. The proposed D3TW innovatively solves\nsequence alignment with discriminative modeling and end-to-end training, which\nsubstantially improves the performance in weakly supervised action alignment\nand segmentation tasks. We show that our model is able to bypass the\ndegenerated sequence problem usually encountered in previous work and\noutperform the current state-of-the-art across three evaluation metrics in two\nchallenging datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 04:12:01 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 23:48:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Chang", "Chien-Yi", ""], ["Huang", "De-An", ""], ["Sui", "Yanan", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1901.02602", "submitter": "Asanka G. Perera", "authors": "Asanka G Perera, Yee Wei Law, and Javaan Chahl", "title": "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition", "comments": "12 pages, 4 figures, UAVision workshop, ECCV, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current UAV-recorded datasets are mostly limited to action recognition and\nobject tracking, whereas the gesture signals datasets were mostly recorded in\nindoor spaces. Currently, there is no outdoor recorded public video dataset for\nUAV commanding signals. Gesture signals can be effectively used with UAVs by\nleveraging the UAVs visual sensors and operational simplicity. To fill this gap\nand enable research in wider application areas, we present a UAV gesture\nsignals dataset recorded in an outdoor setting. We selected 13 gestures\nsuitable for basic UAV navigation and command from general aircraft handling\nand helicopter handling signals. We provide 119 high-definition video clips\nconsisting of 37151 frames. The overall baseline gesture recognition\nperformance computed using Pose-based Convolutional Neural Network (P-CNN) is\n91.9 %. All the frames are annotated with body joints and gesture classes in\norder to extend the dataset's applicability to a wider research area including\ngesture recognition, action recognition, human pose recognition and situation\nawareness.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 04:35:18 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Perera", "Asanka G", ""], ["Law", "Yee Wei", ""], ["Chahl", "Javaan", ""]]}, {"id": "1901.02620", "submitter": "Al-Hussein A. El-Shafie", "authors": "Al-Hussein A. El-Shafie, Mohamed Zaki, Serag El-Din Habib", "title": "Fast CNN-Based Object Tracking Using Localization Layers and Deep\n  Features Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object trackers based on Convolution Neural Network (CNN) have achieved\nstate-of-the-art performance on recent tracking benchmarks, while they suffer\nfrom slow computational speed. The high computational load arises from the\nextraction of the feature maps of the candidate and training patches in every\nvideo frame. The candidate and training patches are typically placed randomly\naround the previous target location and the estimated target location\nrespectively. In this paper, we propose novel schemes to speed-up the\nprocessing of the CNN-based trackers. We input the whole region-of-interest\nonce to the CNN to eliminate the redundant computations of the random candidate\npatches. In addition to classifying each candidate patch as an object or\nbackground, we adapt the CNN to classify the target location inside the object\npatches as a coarse localization step, and we employ bilinear interpolation for\nthe CNN feature maps as a fine localization step. Moreover, bilinear\ninterpolation is exploited to generate CNN feature maps of the training patches\nwithout actually forwarding the training patches through the network which\nachieves a significant reduction of the required computations. Our tracker does\nnot rely on offline video training. It achieves competitive performance results\non the OTB benchmark with 8x speed improvements compared to the equivalent\ntracker.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 06:46:38 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["El-Shafie", "Al-Hussein A.", ""], ["Zaki", "Mohamed", ""], ["Habib", "Serag El-Din", ""]]}, {"id": "1901.02626", "submitter": "Zheng Tang", "authors": "Zheng Tang and Jenq-Neng Hwang", "title": "MOANA: An Online Learned Adaptive Appearance Model for Robust Multiple\n  Object Tracking in 3D", "comments": "Accepted to be published at IEEE Access (Special Section: AI-Driven\n  Big Data Processing: Theory, Methodology, and Applications)", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2903121", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking has been a challenging field, mainly due to noisy\ndetection sets and identity switch caused by occlusion and similar appearance\namong nearby targets. Previous works rely on appearance models built on\nindividual or several selected frames for the comparison of features, but they\ncannot encode long-term appearance changes caused by pose, viewing angle and\nlighting conditions. In this work, we propose an adaptive model that learns\nonline a relatively long-term appearance change of each target. The proposed\nmodel is compatible with any feature of fixed dimension or their combination,\nwhose learning rates are dynamically controlled by adaptive update and spatial\nweighting schemes. To handle occlusion and nearby objects sharing similar\nappearance, we also design cross-matching and re-identification schemes based\non the application of the proposed adaptive appearance models. Additionally,\nthe 3D geometry information is effectively incorporated in our formulation for\ndata association. The proposed method outperforms all the state-of-the-art on\nthe MOTChallenge 3D benchmark and achieves real-time computation with only a\nstandard desktop CPU. It has also shown superior performance over the\nstate-of-the-art on the 2D benchmark of MOTChallenge.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 07:58:30 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 00:52:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Tang", "Zheng", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1901.02645", "submitter": "Lu Zhang", "authors": "Lu Zhang, Xiangyu Zhu, Xiangyu Chen, Xu Yang, Zhen Lei, Zhiyong Liu", "title": "Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian\n  Detection", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pedestrian detection has shown great advantages under poor\nillumination conditions, since the thermal modality provides complementary\ninformation for the color image. However, real multispectral data suffers from\nthe position shift problem, i.e. the color-thermal image pairs are not strictly\naligned, making one object has different positions in different modalities. In\ndeep learning based methods, this problem makes it difficult to fuse the\nfeature maps from both modalities and puzzles the CNN training. In this paper,\nwe propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned\nmultispectral data in an end-to-end way. Firstly, we design a Region Feature\nAlignment (RFA) module to capture the position shift and adaptively align the\nregion features of the two modalities. Secondly, we present a new multimodal\nfusion method, which performs feature re-weighting to select more reliable\nfeatures and suppress the useless ones. Besides, we propose a novel RoI jitter\nstrategy to improve the robustness to unexpected shift patterns of different\ndevices and system settings. Finally, since our method depends on a new kind of\nlabelling: bounding boxes that match each modality, we manually relabel the\nKAIST dataset by locating bounding boxes in both modalities and building their\nrelationships, providing a new KAIST-Paired Annotation. Extensive experimental\nvalidations on existing datasets are performed, demonstrating the effectiveness\nand robustness of the proposed method. Code and data are available at\nhttps://github.com/luzhang16/AR-CNN.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 09:16:36 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 11:57:50 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Lu", ""], ["Zhu", "Xiangyu", ""], ["Chen", "Xiangyu", ""], ["Yang", "Xu", ""], ["Lei", "Zhen", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1901.02662", "submitter": "Lu Jin", "authors": "Zechao Li, Lu Jin, Jinhui Tang", "title": "Deep Semantic Multimodal Hashing Network for Scalable Multimedia\n  Retrieval", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely applied to multimodal retrieval on large-scale\nmultimedia data due to its efficiency in computation and storage. Particularly,\ndeep hashing has received unprecedented research attention in recent years,\nowing to its perfect retrieval performance. However, most of existing deep\nhashing methods learn binary hash codes by preserving the similarity\nrelationship while without exploiting the semantic labels of data points, which\nresult in suboptimal binary codes. In this work, we propose a novel Deep\nSemantic Multimodal Hashing Network for scalable multimodal retrieval. In\nDSMHN, two sets of modality-specific hash functions are jointly learned by\nexplicitly preserving both the inter-modality similarities and the\nintra-modality semantic labels. Specifically, with the assumption that the\nlearned hash codes should be optimal for task-specific classification, two\nstream networks are jointly trained to learn the hash functions by embedding\nthe semantic labels on the resultant hash codes. Different from previous deep\nhashing methods, which are tied to some particular forms of loss functions, the\nproposed deep hashing framework can be flexibly integrated with different types\nof loss functions. In addition, the bit balance property is investigated to\ngenerate binary codes with each bit having 50% probability to be 1 or -1.\nMoreover, a unified deep multimodal hashing framework is proposed to learn\ncompact and high-quality hash codes by exploiting the feature representation\nlearning, inter-modality similarity preserving learning, semantic label\npreserving learning and hash functions learning with bit balanced constraint\nsimultaneously. We conduct extensive experiments for both unimodal and\ncross-modal retrieval tasks on three widely-used multimodal retrieval datasets.\nThe experimental result demonstrates that DSMHN significantly outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 10:27:57 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 03:04:54 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Li", "Zechao", ""], ["Jin", "Lu", ""], ["Tang", "Jinhui", ""]]}, {"id": "1901.02675", "submitter": "Thrupthi Ann John", "authors": "Thrupthi Ann John, Isha Dua, Vineeth N Balasubramanian, C. V. Jawahar", "title": "Low-Cost Transfer Learning of Face Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do we know what the different filters of a face network represent? Can we use\nthis filter information to train other tasks without transfer learning? For\ninstance, can age, head pose, emotion and other face related tasks be learned\nfrom face recognition network without transfer learning? Understanding the role\nof these filters allows us to transfer knowledge across tasks and take\nadvantage of large data sets in related tasks. Given a pretrained network, we\ncan infer which tasks the network generalizes for and the best way to transfer\nthe information to a new task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 11:09:09 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["John", "Thrupthi Ann", ""], ["Dua", "Isha", ""], ["Balasubramanian", "Vineeth N", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1901.02694", "submitter": "Zhihao Cao", "authors": "Xiaoxiao Sun, Shaomin Mu, Yongyu Xu, Zhihao Cao, Tingting Su", "title": "Image Recognition of Tea Leaf Diseases Based on Convolutional Neural\n  Network", "comments": "2018 International Conference on Security, Pattern Analysis, and\n  Cybernetics(SPAC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify and prevent tea leaf diseases effectively, convolution\nneural network (CNN) was used to realize the image recognition of tea disease\nleaves. Firstly, image segmentation and data enhancement are used to preprocess\nthe images, and then these images were input into the network for training.\nSecondly, to reach a higher recognition accuracy of CNN, the learning rate and\niteration numbers were adjusted frequently and the dropout was added properly\nin the case of over-fitting. Finally, the experimental results show that the\nrecognition accuracy of CNN is 93.75%, while the accuracy of SVM and BP neural\nnetwork is 89.36% and 87.69% respectively. Therefore, the recognition algorithm\nbased on CNN is better in classification and can improve the recognition\nefficiency of tea leaf diseases effectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 12:30:00 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Sun", "Xiaoxiao", ""], ["Mu", "Shaomin", ""], ["Xu", "Yongyu", ""], ["Cao", "Zhihao", ""], ["Su", "Tingting", ""]]}, {"id": "1901.02701", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Dolzodmaa Davaasuren, Nilam Ram, Prasenjit Mitra,\n  Byron Reeves, Thomas Robinson", "title": "Guess What's on my Screen? Clustering Smartphone Screenshots with Active\n  Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant proportion of individuals' daily activities is experienced\nthrough digital devices. Smartphones in particular have become one of the\npreferred interfaces for content consumption and social interaction.\nIdentifying the content embedded in frequently-captured smartphone screenshots\nis thus a crucial prerequisite to studies of media behavior and health\nintervention planning that analyze activity interplay and content switching\nover time. Screenshot images can depict heterogeneous contents and\napplications, making the a priori definition of adequate taxonomies a\ncumbersome task, even for humans. Privacy protection of the sensitive data\ncaptured on screens means the costs associated with manual annotation are\nlarge, as the effort cannot be crowd-sourced. Thus, there is need to examine\nutility of unsupervised and semi-supervised methods for digital screenshot\nclassification. This work introduces the implications of applying clustering on\nlarge screenshot sets when only a limited amount of labels is available. In\nthis paper we develop a framework for combining K-Means clustering with Active\nLearning for efficient leveraging of labeled and unlabeled samples, with the\ngoal of discovering latent classes and describing a large collection of\nscreenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions\nfor class probability propagation provide for more well-formed cluster\nconfigurations. Visual and textual vector representations of the screenshot\nimages are derived and combined to assess the relative contribution of\nmulti-modal features to the overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 12:51:36 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 11:22:33 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Chiatti", "Agnese", ""], ["Davaasuren", "Dolzodmaa", ""], ["Ram", "Nilam", ""], ["Mitra", "Prasenjit", ""], ["Reeves", "Byron", ""], ["Robinson", "Thomas", ""]]}, {"id": "1901.02783", "submitter": "Chelsea Weaver", "authors": "Chelsea Weaver and Naoki Saito", "title": "The Use of Mutual Coherence to Prove $\\ell^1/\\ell^0$-Equivalence in\n  Classification Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the decomposition of a signal over an overcomplete set of\nvectors. Minimization of the $\\ell^1$-norm of the coefficient vector can often\nretrieve the sparsest solution (so-called \"$\\ell^1/\\ell^0$-equivalence\"), a\ngenerally NP-hard task, and this fact has powered the field of compressed\nsensing. Wright et al.'s sparse representation-based classification (SRC)\napplies this relationship to machine learning, wherein the signal to be\ndecomposed represents the test sample and columns of the dictionary are\ntraining samples. We investigate the relationships between\n$\\ell^1$-minimization, sparsity, and classification accuracy in SRC. After\nproving that the tractable, deterministic approach to verifying\n$\\ell^1/\\ell^0$-equivalence fundamentally conflicts with the high coherence\nbetween same-class training samples, we demonstrate that $\\ell^1$-minimization\ncan still recover the sparsest solution when the classes are well-separated.\nFurther, using a nonlinear transform so that sparse recovery conditions may be\nsatisfied, we demonstrate that approximate (not strict) equivalence is key to\nthe success of SRC.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 15:38:32 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Weaver", "Chelsea", ""], ["Saito", "Naoki", ""]]}, {"id": "1901.02826", "submitter": "Andreas Bock", "authors": "Andreas Bock, Alexis Arnaudon, Colin Cotter", "title": "Selective metamorphosis for growth modelling with applications to\n  landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a framework for shape matching in computational anatomy allowing\nusers control of the degree to which the matching is diffeomorphic. This\ncontrol is given as a function defined over the image and parameterises the\ntemplate deformation. By modelling localised template deformation we have a\nmathematical description of growth only in specified parts of an image. The\nlocation can either be specified from prior knowledge of the growth location or\nlearned from data. For simplicity, we consider landmark matching and infer the\ndistribution of a finite dimensional parameterisation of the control via Markov\nchain Monte Carlo. Preliminary numerical results are shown and future paths of\ninvestigation are laid out. Well-posedness of this new problem is studied\ntogether with an analysis of the associated geodesic equations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 14:09:35 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 14:47:30 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Bock", "Andreas", ""], ["Arnaudon", "Alexis", ""], ["Cotter", "Colin", ""]]}, {"id": "1901.02838", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Naoto Yokoya, Nan Ge, Jocelyn Chanussot, Xiao Xiang Zhu", "title": "Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality\n  Learning Framework for Land Cover and Land Use Classification", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote\n  Sensing,2019,147:193-205", "doi": "10.1016/j.isprsjprs.2018.10.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at tackling a general but interesting cross-modality\nfeature learning question in remote sensing community --- can a limited amount\nof highly-discrimin-ative (e.g., hyperspectral) training data improve the\nperformance of a classification task using a large amount of\npoorly-discriminative (e.g., multispectral) data? Traditional semi-supervised\nmanifold alignment methods do not perform sufficiently well for such problems,\nsince the hyperspectral data is very expensive to be largely collected in a\ntrade-off between time and efficiency, compared to the multispectral data. To\nthis end, we propose a novel semi-supervised cross-modality learning framework,\ncalled learnable manifold alignment (LeMA). LeMA learns a joint graph structure\ndirectly from the data instead of using a given fixed graph defined by a\nGaussian kernel function. With the learned graph, we can further capture the\ndata distribution by graph-based label propagation, which enables finding a\nmore accurate decision boundary. Additionally, an optimization strategy based\non the alternating direction method of multipliers (ADMM) is designed to solve\nthe proposed model. Extensive experiments on two hyperspectral-multispectral\ndatasets demonstrate the superiority and effectiveness of the proposed method\nin comparison with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 17:22:36 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Ge", "Nan", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1901.02839", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian\n  Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn\n  Schuller, Kam Star, Elnar Hajiyev and Maja Pantic", "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research\n  in the Wild", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2019", "doi": "10.1109/TPAMI.2019.2944808", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural human-computer interaction and audio-visual human behaviour sensing\nsystems, which would achieve robust performance in-the-wild are more needed\nthan ever as digital devices are increasingly becoming an indispensable part of\nour life. Accurately annotated real-world data are the crux in devising such\nsystems. However, existing databases usually consider controlled settings, low\ndemographic variability, and a single task. In this paper, we introduce the\nSEWA database of more than 2000 minutes of audio-visual data of 398 people\ncoming from six cultures, 50% female, and uniformly spanning the age range of\n18 to 65 years old. Subjects were recorded in two different contexts: while\nwatching adverts and while discussing adverts in a video chat. The database\nincludes rich annotations of the recordings in terms of facial landmarks,\nfacial action units (FAU), various vocalisations, mirroring, and continuously\nvalued valence, arousal, liking, agreement, and prototypic examples of\n(dis)liking. This database aims to be an extremely valuable resource for\nresearchers in affective computing and automatic human sensing and is expected\nto push forward the research in human behaviour analysis, including cultural\nstudies. Along with the database, we provide extensive baseline experiments for\nautomatic FAU detection and automatic valence, arousal and (dis)liking\nintensity estimation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 17:28:57 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 22:52:44 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Kossaifi", "Jean", ""], ["Walecki", "Robert", ""], ["Panagakis", "Yannis", ""], ["Shen", "Jie", ""], ["Schmitt", "Maximilian", ""], ["Ringeval", "Fabien", ""], ["Han", "Jing", ""], ["Pandit", "Vedhas", ""], ["Toisoul", "Antoine", ""], ["Schuller", "Bjorn", ""], ["Star", "Kam", ""], ["Hajiyev", "Elnar", ""], ["Pantic", "Maja", ""]]}, {"id": "1901.02840", "submitter": "Yang Wang", "authors": "Yang Wang, Haibin Huang, Chuan Wang, Tong He, Jue Wang, Minh Hoai", "title": "GIF2Video: Color Dequantization and Temporal Interpolation of GIF images", "comments": "to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Interchange Format (GIF) is a highly portable graphics format that\nis ubiquitous on the Internet. Despite their small sizes, GIF images often\ncontain undesirable visual artifacts such as flat color regions, false\ncontours, color shift, and dotted patterns. In this paper, we propose\nGIF2Video, the first learning-based method for enhancing the visual quality of\nGIFs in the wild. We focus on the challenging task of GIF restoration by\nrecovering information lost in the three steps of GIF creation: frame sampling,\ncolor quantization, and color dithering. We first propose a novel CNN\narchitecture for color dequantization. It is built upon a compositional\narchitecture for multi-step color correction, with a comprehensive loss\nfunction designed to handle large quantization errors. We then adapt the\nSuperSlomo network for temporal interpolation of GIF frames. We introduce two\nlarge datasets, namely GIF-Faces and GIF-Moments, for both training and\nevaluation. Experimental results show that our method can significantly improve\nthe visual quality of GIFs, and outperforms direct baseline and\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 17:31:11 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 19:30:06 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Yang", ""], ["Huang", "Haibin", ""], ["Wang", "Chuan", ""], ["He", "Tong", ""], ["Wang", "Jue", ""], ["Hoai", "Minh", ""]]}, {"id": "1901.02858", "submitter": "Varuna De Silva D", "authors": "Mirco Moencks, Varuna De Silva, Jamie Roche, and Ahmet Kondoz", "title": "Adaptive Feature Processing for Robust Human Activity Recognition on a\n  Novel Multi-Modal Dataset", "comments": "Working Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) is a key building block of many emerging\napplications such as intelligent mobility, sports analytics, ambient-assisted\nliving and human-robot interaction. With robust HAR, systems will become more\nhuman-aware, leading towards much safer and empathetic autonomous systems.\nWhile human pose detection has made significant progress with the dawn of deep\nconvolutional neural networks (CNNs), the state-of-the-art research has almost\nexclusively focused on a single sensing modality, especially video. However, in\nsafety critical applications it is imperative to utilize multiple sensor\nmodalities for robust operation. To exploit the benefits of state-of-the-art\nmachine learning techniques for HAR, it is extremely important to have\nmultimodal datasets. In this paper, we present a novel, multi-modal sensor\ndataset that encompasses nine indoor activities, performed by 16 participants,\nand captured by four types of sensors that are commonly used in indoor\napplications and autonomous vehicles. This multimodal dataset is the first of\nits kind to be made openly available and can be exploited for many applications\nthat require HAR, including sports analytics, healthcare assistance and indoor\nintelligent mobility. We propose a novel data preprocessing algorithm to enable\nadaptive feature extraction from the dataset to be utilized by different\nmachine learning algorithms. Through rigorous experimental evaluations, this\npaper reviews the performance of machine learning approaches to posture\nrecognition, and analyses the robustness of the algorithms. When performing HAR\nwith the RGB-Depth data from our new dataset, machine learning algorithms such\nas a deep neural network reached a mean accuracy of up to 96.8% for\nclassification across all stationary and dynamic activities\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 18:25:14 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Moencks", "Mirco", ""], ["De Silva", "Varuna", ""], ["Roche", "Jamie", ""], ["Kondoz", "Ahmet", ""]]}, {"id": "1901.02875", "submitter": "Jiajun Wu", "authors": "Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T.\n  Freeman, Joshua B. Tenenbaum, Jiajun Wu", "title": "Learning to Infer and Execute 3D Shape Programs", "comments": "ICLR 2019. Project page: http://shape2prog.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perception of 3D shapes goes beyond reconstructing them as a set of\npoints or a composition of geometric primitives: we also effortlessly\nunderstand higher-level shape structure such as the repetition and reflective\nsymmetry of object parts. In contrast, recent advances in 3D shape sensing\nfocus more on low-level geometry but less on these higher-level relationships.\nIn this paper, we propose 3D shape programs, integrating bottom-up recognition\nsystems with top-down, symbolic program structure to capture both low-level\ngeometry and high-level structural priors for 3D shapes. Because there are no\nannotations of shape programs for real shapes, we develop neural modules that\nnot only learn to infer 3D shape programs from raw, unannotated shapes, but\nalso to execute these programs for shape reconstruction. After initial\nbootstrapping, our end-to-end differentiable model learns 3D shape programs by\nreconstructing shapes in a self-supervised manner. Experiments demonstrate that\nour model accurately infers and executes 3D shape programs for highly complex\nshapes from various categories. It can also be integrated with an\nimage-to-shape module to infer 3D shape programs directly from an RGB image,\nleading to 3D shape reconstructions that are both more accurate and more\nphysically plausible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 18:55:03 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 19:37:26 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 23:07:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Tian", "Yonglong", ""], ["Luo", "Andrew", ""], ["Sun", "Xingyuan", ""], ["Ellis", "Kevin", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1901.02884", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast, Marc T. P. Adam, and Raymond Chiong", "title": "Deep Learning for Human Affect Recognition: Insights and New\n  Developments", "comments": "To be published in IEEE Transactions on Affective Computing. 20\n  pages, 7 figures, 6 tables", "journal-ref": null, "doi": "10.1109/TAFFC.2018.2890471", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic human affect recognition is a key step towards more natural\nhuman-computer interaction. Recent trends include recognition in the wild using\na fusion of audiovisual and physiological sensors, a challenging setting for\nconventional machine learning algorithms. Since 2010, novel deep learning\nalgorithms have been applied increasingly in this field. In this paper, we\nreview the literature on human affect recognition between 2010 and 2017, with a\nspecial focus on approaches using deep neural networks. By classifying a total\nof 950 studies according to their usage of shallow or deep architectures, we\nare able to show a trend towards deep learning. Reviewing a subset of 233\nstudies that employ deep neural networks, we comprehensively quantify their\napplications in this field. We find that deep learning is used for learning of\n(i) spatial feature representations, (ii) temporal feature representations, and\n(iii) joint feature representations for multimodal sensor data. Exemplary\nstate-of-the-art architectures illustrate the progress. Our findings show the\nrole deep architectures will play in human affect recognition, and can serve as\na reference point for researchers working on related applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 23:33:47 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Adam", "Marc T. P.", ""], ["Chiong", "Raymond", ""]]}, {"id": "1901.02911", "submitter": "Ezequiel de la Rosa", "authors": "Ezequiel de la Rosa, D\\'esir\\'e Sidib\\'e, Thomas Decourselle, Thibault\n  Leclercq, Alexandre Cochet, Alain Lalande", "title": "Myocardial Infarction Quantification From Late Gadolinium Enhancement\n  MRI Using Top-hat Transforms and Neural Networks", "comments": "Submitted to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significance: Late gadolinium enhanced magnetic resonance imaging (LGE-MRI)\nis the gold standard technique for myocardial viability assessment. Although\nthe technique accurately reflects the damaged tissue, there is no clinical\nstandard for quantifying myocardial infarction (MI), demanding most algorithms\nto be expert dependent. Objectives and Methods: In this work a new automatic\nmethod for MI quantification from LGE-MRI is proposed. Our novel segmentation\napproach is devised for accurately detecting not only hyper-enhanced lesions,\nbut also microvascular-obstructed areas. Moreover, it includes a myocardial\ndisease detection step which extends the algorithm for working under healthy\nscans. The method is based on a cascade approach where firstly, diseased slices\nare identified by a convolutional neural network (CNN). Secondly, by means of\nmorphological operations a fast coarse scar segmentation is obtained. Thirdly,\nthe segmentation is refined by a boundary-voxel reclassification strategy using\nan ensemble of CNNs. For its validation, reproducibility and further comparison\nagainst other methods, we tested the method on a big multi-field expert\nannotated LGE-MRI database including healthy and diseased cases. Results and\nConclusion: In an exhaustive comparison against nine reference algorithms, the\nproposal achieved state-of-the-art segmentation performances and showed to be\nthe only method agreeing in volumetric scar quantification with the expert\ndelineations. Moreover, the method was able to reproduce the intra- and\ninter-observer variability ranges. It is concluded that the method could\nsuitably be transferred to clinical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 19:32:31 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["de la Rosa", "Ezequiel", ""], ["Sidib\u00e9", "D\u00e9sir\u00e9", ""], ["Decourselle", "Thomas", ""], ["Leclercq", "Thibault", ""], ["Cochet", "Alexandre", ""], ["Lalande", "Alain", ""]]}, {"id": "1901.02915", "submitter": "Charles Zheng", "authors": "Charles Y. Zheng, Francisco Pereira, Chris I. Baker, Martin N. Hebart", "title": "Revealing interpretable object representations from human behavior", "comments": "Accepted in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study how mental object representations are related to behavior, we\nestimated sparse, non-negative representations of objects using human\nbehavioral judgments on images representative of 1,854 object categories. These\nrepresentations predicted a latent similarity structure between objects, which\ncaptured most of the explainable variance in human behavioral judgments.\nIndividual dimensions in the low-dimensional embedding were found to be highly\nreproducible and interpretable as conveying degrees of taxonomic membership,\nfunctionality, and perceptual attributes. We further demonstrated the\npredictive power of the embeddings for explaining other forms of human\nbehavior, including categorization, typicality judgments, and feature ratings,\nsuggesting that the dimensions reflect human conceptual representations of\nobjects beyond the specific task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 20:04:42 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Zheng", "Charles Y.", ""], ["Pereira", "Francisco", ""], ["Baker", "Chris I.", ""], ["Hebart", "Martin N.", ""]]}, {"id": "1901.02920", "submitter": "Tao Sun", "authors": "Tao Sun, Zhewei Wang, C. D. Smith, Jundong Liu", "title": "TraceCaps: A Capsule-based Neural Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a capsule-based neural network model to solve the\nsemantic segmentation problem. By taking advantage of the extractable\npart-whole dependencies available in capsule layers, we derive the\nprobabilities of the class labels for individual capsules through a recursive,\nlayer-by-layer procedure. We model this procedure as a traceback pipeline and\ntake it as a central piece to build an end-to-end segmentation network. Under\nthe proposed framework, image-level class labels and object boundaries are\njointly sought in an explicit manner, which poses a significant advantage over\nthe state-of-the-art fully convolutional network (FCN) solutions. With the\ncapability to extracted part-whole information, our traceback pipeline can\npotentially be utilized as the building blocks to design interpretable neural\nnetworks. Experiments conducted on modified MNIST and neuroimages demonstrate\nthat our model considerably enhance the segmentation performance compared to\nthe leading FCN variants.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 20:23:13 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 19:22:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Sun", "Tao", ""], ["Wang", "Zhewei", ""], ["Smith", "C. D.", ""], ["Liu", "Jundong", ""]]}, {"id": "1901.02937", "submitter": "Zhiling Long", "authors": "Muhammad Amir Shafiq, Tariq Alshawi, Zhiling Long and Ghassan AlRegib", "title": "SalSi: A new seismic attribute for salt dome detection", "comments": "Proceedings of IEEE Intl. Conf. on Acoustics, Speech and Signal\n  Processing (ICASSP), Shanghai, China, Mar. 2016. arXiv admin note: text\n  overlap with arXiv:1812.11960", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a saliency-based attribute, SalSi, to detect salt\ndome bodies within seismic volumes. SalSi is based on the saliency theory and\nmodeling of the human vision system (HVS). In this work, we aim to highlight\nthe parts of the seismic volume that receive highest attention from the human\ninterpreter, and based on the salient features of a seismic image, we detect\nthe salt domes. Experimental results show the effectiveness of SalSi on the\nreal seismic dataset acquired from the North Sea, F3 block. Subjectively, we\nhave used the ground truth and the output of different salt dome delineation\nalgorithms to validate the results of SalSi. For the objective evaluation of\nresults, we have used the receiver operating characteristics (ROC) curves and\narea under the curves (AUC) to demonstrate SalSi is a promising and an\neffective attribute for seismic interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 21:33:16 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Shafiq", "Muhammad Amir", ""], ["Alshawi", "Tariq", ""], ["Long", "Zhiling", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.02942", "submitter": "Asma Baghdadi", "authors": "Asma Baghdadi, Yassine Aribi, Rahma Fourati, Najla Halouani, Patrick\n  Siarry and Adel M. Alimi", "title": "DASPS: A Database for Anxious States based on a Psychological\n  Stimulation", "comments": "11 pages, IEEE transactions on SMC:systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anxiety affects human capabilities and behavior as much as it affects\nproductivity and quality of life. It can be considered as the main cause of\ndepression and suicide. Anxious states are easily detectable by humans due to\ntheir acquired cognition, humans interpret the interlocutor's tone of speech,\ngesture, facial expressions and recognize their mental state. There is a need\nfor non-invasive reliable techniques that performs the complex task of anxiety\ndetection. In this paper, we present DASPS database containing recorded\nElectroencephalogram (EEG) signals of 23 participants during anxiety\nelicitation by means of face-to-face psychological stimuli. EEG signals were\ncaptured with Emotiv Epoc headset as it's a wireless wearable low-cost\nequipment. In our study, we investigate the impact of different parameters,\nnotably: trial duration, feature type, feature combination and anxiety levels\nnumber. Our findings showed that anxiety is well elicited in 1 second. For\ninstance, stacked sparse autoencoder with different type of features achieves\n83.50% and 74.60% for 2 and 4 anxiety levels detection, respectively. The\npresented results prove the benefits of the use of a low-cost EEG headset\ninstead of medical non-wireless devices and create a starting point for new\nresearches in the field of anxiety detection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 21:54:35 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 23:46:48 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Baghdadi", "Asma", ""], ["Aribi", "Yassine", ""], ["Fourati", "Rahma", ""], ["Halouani", "Najla", ""], ["Siarry", "Patrick", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1901.02968", "submitter": "Anastasia Dubrovina", "authors": "Anastasia Dubrovina and Fei Xia and Panos Achlioptas and Mira Shalah\n  and Raphael Groscot and Leonidas Guibas", "title": "Composite Shape Modeling via Latent Space Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network architecture, termed Decomposer-Composer,\nfor semantic structure-aware 3D shape modeling. Our method utilizes an\nauto-encoder-based pipeline, and produces a novel factorized shape embedding\nspace, where the semantic structure of the shape collection translates into a\ndata-dependent sub-space factorization, and where shape composition and\ndecomposition become simple linear operations on the embedding coordinates. We\nfurther propose to model shape assembly using an explicit learned part\ndeformation module, which utilizes a 3D spatial transformer network to perform\nan in-network volumetric grid deformation, and which allows us to train the\nwhole system end-to-end. The resulting network allows us to perform part-level\nshape manipulation, unattainable by existing approaches. Our extensive ablation\nstudy, comparison to baseline methods and qualitative analysis demonstrate the\nimproved performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 23:25:38 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 13:53:56 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dubrovina", "Anastasia", ""], ["Xia", "Fei", ""], ["Achlioptas", "Panos", ""], ["Shalah", "Mira", ""], ["Groscot", "Raphael", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1901.02970", "submitter": "He Wang", "authors": "He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song,\n  Leonidas J. Guibas", "title": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and\n  Size Estimation", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to estimate the 6D pose and dimensions of unseen\nobject instances in an RGB-D image. Contrary to \"instance-level\" 6D pose\nestimation tasks, our problem assumes that no exact object CAD models are\navailable during either training or testing time. To handle different and\nunseen object instances in a given category, we introduce a Normalized Object\nCoordinate Space (NOCS)---a shared canonical representation for all possible\nobject instances within a category. Our region-based neural network is then\ntrained to directly infer the correspondence from observed pixels to this\nshared object representation (NOCS) along with other object information such as\nclass label and instance mask. These predictions can be combined with the depth\nmap to jointly estimate the metric 6D pose and dimensions of multiple objects\nin a cluttered scene. To train our network, we present a new context-aware\ntechnique to generate large amounts of fully annotated mixed reality data. To\nfurther improve our model and evaluate its performance on real data, we also\nprovide a fully annotated real-world dataset with large environment and\ninstance variation. Extensive experiments demonstrate that the proposed method\nis able to robustly estimate the pose and size of unseen object instances in\nreal environments while also achieving state-of-the-art performance on standard\n6D pose estimation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 23:31:40 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 08:24:32 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "He", ""], ["Sridhar", "Srinath", ""], ["Huang", "Jingwei", ""], ["Valentin", "Julien", ""], ["Song", "Shuran", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1901.02985", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua,\n  Alan Yuille, Li Fei-Fei", "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image\n  Segmentation", "comments": "To appear in CVPR 2019 as oral. Code for Auto-DeepLab released at\n  https://github.com/tensorflow/models/tree/master/research/deeplab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural Architecture Search (NAS) has successfully identified neural\nnetwork architectures that exceed human designed ones on large-scale image\nclassification. In this paper, we study NAS for semantic image segmentation.\nExisting works often focus on searching the repeatable cell structure, while\nhand-designing the outer network structure that controls the spatial resolution\nchanges. This choice simplifies the search space, but becomes increasingly\nproblematic for dense image prediction which exhibits a lot more network level\narchitectural variations. Therefore, we propose to search the network level\nstructure in addition to the cell level structure, which forms a hierarchical\narchitecture search space. We present a network level search space that\nincludes many popular designs, and develop a formulation that allows efficient\ngradient-based architecture search (3 P100 GPU days on Cityscapes images). We\ndemonstrate the effectiveness of the proposed method on the challenging\nCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our\narchitecture searched specifically for semantic image segmentation, attains\nstate-of-the-art performance without any ImageNet pretraining.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 01:05:15 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 19:40:44 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liu", "Chenxi", ""], ["Chen", "Liang-Chieh", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Hua", "Wei", ""], ["Yuille", "Alan", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1901.03003", "submitter": "Lianwen Jin", "authors": "Canjie Luo, Lianwen Jin, Zenghui Sun", "title": "A Multi-Object Rectified Attention Network for Scene Text Recognition", "comments": "9 Tables, 9 Figures. Accepted to appear in Pattern Recognition, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular text is widely used. However, it is considerably difficult to\nrecognize because of its various shapes and distorted patterns. In this paper,\nwe thus propose a multi-object rectified attention network (MORAN) for general\nscene text recognition. The MORAN consists of a multi-object rectification\nnetwork and an attention-based sequence recognition network. The multi-object\nrectification network is designed for rectifying images that contain irregular\ntext. It decreases the difficulty of recognition and enables the\nattention-based sequence recognition network to more easily read irregular\ntext. It is trained in a weak supervision way, thus requiring only images and\ncorresponding text labels. The attention-based sequence recognition network\nfocuses on target characters and sequentially outputs the predictions.\nMoreover, to improve the sensitivity of the attention-based sequence\nrecognition network, a fractional pickup method is proposed for an\nattention-based decoder in the training phase. With the rectification\nmechanism, the MORAN can read both regular and irregular scene text. Extensive\nexperiments on various benchmarks are conducted, which show that the MORAN\nachieves state-of-the-art performance. The source code is available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 02:55:52 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Luo", "Canjie", ""], ["Jin", "Lianwen", ""], ["Sun", "Zenghui", ""]]}, {"id": "1901.03006", "submitter": "Daniel Liu", "authors": "Daniel Liu, Ronald Yu, Hao Su", "title": "Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud\n  Classifiers", "comments": "Abridged version accepted at the 2019 IEEE International Conference\n  on Image Processing (ICIP). Source code:\n  https://github.com/Daniel-Liu-c0deb0t/3D-Neural-Network-Adversarial-Attacks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object classification and segmentation using deep neural networks has been\nextremely successful. As the problem of identifying 3D objects has many\nsafety-critical applications, the neural networks have to be robust against\nadversarial changes to the input data set. There is a growing body of research\non generating human-imperceptible adversarial attacks and defenses against them\nin the 2D image classification domain. However, 3D objects have various\ndifferences with 2D images, and this specific domain has not been rigorously\nstudied so far.\n  We present a preliminary evaluation of adversarial attacks on deep 3D point\ncloud classifiers, namely PointNet and PointNet++, by evaluating both white-box\nand black-box adversarial attacks that were proposed for 2D images and\nextending those attacks to reduce the perceptibility of the perturbations in 3D\nspace. We also show the high effectiveness of simple defenses against those\nattacks by proposing new defenses that exploit the unique structure of 3D point\nclouds. Finally, we attempt to explain the effectiveness of the defenses\nthrough the intrinsic structures of both the point clouds and the neural\nnetwork architectures. Overall, we find that networks that process 3D point\ncloud data are weak to adversarial attacks, but they are also more easily\ndefensible compared to 2D image classifiers. Our investigation will provide the\ngroundwork for future studies on improving the robustness of deep neural\nnetworks that handle 3D data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 03:12:07 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 04:50:59 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 03:29:12 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 17:55:46 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Liu", "Daniel", ""], ["Yu", "Ronald", ""], ["Su", "Hao", ""]]}, {"id": "1901.03031", "submitter": "Huibing Wang", "authors": "Huibing Wang, Haohao Li, Xianping Fu", "title": "Multi-feature Distance Metric Learning for Non-rigid 3D Shape Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, feature-learning-based 3D shape retrieval approaches\nhave been received widespread attention in the computer graphic community.\nThese approaches usually explored the hand-crafted distance metric or\nconventional distance metric learning methods to compute the similarity of the\nsingle feature. The single feature always contains onefold geometric\ninformation, which cannot characterize the 3D shapes well. Therefore, the\nmultiple features should be used for the retrieval task to overcome the\nlimitation of single feature and further improve the performance. However, most\nconventional distance metric learning methods fail to integrate the\ncomplementary information from multiple features to construct the distance\nmetric. To address these issue, a novel multi-feature distance metric learning\nmethod for non-rigid 3D shape retrieval is presented in this study, which can\nmake full use of the complimentary geometric information from multiple shape\nfeatures by utilizing the KL-divergences. Minimizing KL-divergence between\ndifferent metric of features and a common metric is a consistency constraints,\nwhich can lead the consistency shared latent feature space of the multiple\nfeatures. We apply the proposed method to 3D model retrieval, and test our\nmethod on well known benchmark database. The results show that our method\nsubstantially outperforms the state-of-the-art non-rigid 3D shape retrieval\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 06:28:16 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Wang", "Huibing", ""], ["Li", "Haohao", ""], ["Fu", "Xianping", ""]]}, {"id": "1901.03035", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira,\n  Richard Socher, Caiming Xiong", "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "comments": "ICLR 2019, code is available at\n  https://github.com/chihyaoma/selfmonitoring-agent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vision-and-Language Navigation (VLN) task entails an agent following\nnavigational instruction in photo-realistic unknown environments. This\nchallenging task demands that the agent be aware of which instruction was\ncompleted, which instruction is needed next, which way to go, and its\nnavigation progress towards the goal. In this paper, we introduce a\nself-monitoring agent with two complementary components: (1) visual-textual\nco-grounding module to locate the instruction completed in the past, the\ninstruction required for the next action, and the next moving direction from\nsurrounding images and (2) progress monitor to ensure the grounded instruction\ncorrectly reflects the navigation progress. We test our self-monitoring agent\non a standard benchmark and analyze our proposed approach through a series of\nablation studies that elucidate the contributions of the primary components.\nUsing our proposed method, we set the new state of the art by a significant\nmargin (8% absolute increase in success rate on the unseen test set). Code is\navailable at https://github.com/chihyaoma/selfmonitoring-agent .\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 06:46:50 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Lu", "Jiasen", ""], ["Wu", "Zuxuan", ""], ["AlRegib", "Ghassan", ""], ["Kira", "Zsolt", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1901.03037", "submitter": "Thang Dang Duy", "authors": "Dang Duy Thang and Toshihiro Matsui", "title": "Image Transformation can make Neural Networks more robust against\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are being applied in many tasks related to IoT with\nencouraging results. For example, neural networks can precisely detect human,\nobjects and animal via surveillance camera for security purpose. However,\nneural networks have been recently found vulnerable to well-designed input\nsamples that called adversarial examples. Such issue causes neural networks to\nmisclassify adversarial examples that are imperceptible to humans. We found\ngiving a rotation to an adversarial example image can defeat the effect of\nadversarial examples. Using MNIST number images as the original images, we\nfirst generated adversarial examples to neural network recognizer, which was\ncompletely fooled by the forged examples. Then we rotated the adversarial image\nand gave them to the recognizer to find the recognizer to regain the correct\nrecognition. Thus, we empirically confirmed rotation to images can protect\npattern recognizer based on neural networks from adversarial example attacks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 06:53:48 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Thang", "Dang Duy", ""], ["Matsui", "Toshihiro", ""]]}, {"id": "1901.03060", "submitter": "Qingbo Wu", "authors": "Lei Ma, Hongliang Li, Qingbo Wu, Fanman Meng and King Ngi Ngan", "title": "Hierarchy Neighborhood Discriminative Hashing for An Unified View of\n  Single-Label and Multi-Label Image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep supervised hashing methods have become popular for large-scale\nimage retrieval task. To preserve the semantic similarity notion between\nexamples, they typically utilize the pairwise supervision or the triplet\nsupervised information for hash learning. However, these methods usually ignore\nthe semantic class information which can help the improvement of the semantic\ndiscriminative ability of hash codes. In this paper, we propose a novel\nhierarchy neighborhood discriminative hashing method. Specifically, we\nconstruct a bipartite graph to build coarse semantic neighbourhood relationship\nbetween the sub-class feature centers and the embeddings features. Moreover, we\nutilize the pairwise supervised information to construct the fined semantic\nneighbourhood relationship between embeddings features. Finally, we propose a\nhierarchy neighborhood discriminative hashing loss to unify the single-label\nand multilabel image retrieval problem with a one-stream deep neural network\narchitecture. Experimental results on two largescale datasets demonstrate that\nthe proposed method can outperform the state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 08:53:19 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 09:17:22 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Ma", "Lei", ""], ["Li", "Hongliang", ""], ["Wu", "Qingbo", ""], ["Meng", "Fanman", ""], ["Ngan", "King Ngi", ""]]}, {"id": "1901.03062", "submitter": "Wu Liu", "authors": "Xinchen Liu, Wu Liu, Huadong Ma, Shuangqun Li", "title": "PVSS: A Progressive Vehicle Search System for Video Surveillance\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on the task of searching for a specific vehicle that\nappeared in the surveillance networks. Existing methods usually assume the\nvehicle images are well cropped from the surveillance videos, then use visual\nattributes, like colors and types, or license plate numbers to match the target\nvehicle in the image set. However, a complete vehicle search system should\nconsider the problems of vehicle detection, representation, indexing, storage,\nmatching, and so on. Besides, attribute-based search cannot accurately find the\nsame vehicle due to intra-instance changes in different cameras and the\nextremely uncertain environment. Moreover, the license plates may be\nmisrecognized in surveillance scenes due to the low resolution and noise. In\nthis paper, a Progressive Vehicle Search System, named as PVSS, is designed to\nsolve the above problems. PVSS is constituted of three modules: the crawler,\nthe indexer, and the searcher. The vehicle crawler aims to detect and track\nvehicles in surveillance videos and transfer the captured vehicle images,\nmetadata and contextual information to the server or cloud. Then multi-grained\nattributes, such as the visual features and license plate fingerprints, are\nextracted and indexed by the vehicle indexer. At last, a query triplet with an\ninput vehicle image, the time range, and the spatial scope is taken as the\ninput by the vehicle searcher. The target vehicle will be searched in the\ndatabase by a progressive process. Extensive experiments on the public dataset\nfrom a real surveillance network validate the effectiveness of the PVSS.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 09:02:08 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Liu", "Xinchen", ""], ["Liu", "Wu", ""], ["Ma", "Huadong", ""], ["Li", "Shuangqun", ""]]}, {"id": "1901.03067", "submitter": "Wu Liu", "authors": "Meng Zhang, Xinchen Liu, Wu Liu, Anfu Zhou, Huadong Ma, Tao Mei", "title": "Multi-Granularity Reasoning for Social Relation Recognition from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering social relations in images can make machines better interpret the\nbehavior of human beings. However, automatically recognizing social relations\nin images is a challenging task due to the significant gap between the domains\nof visual content and social relation. Existing studies separately process\nvarious features such as faces expressions, body appearance, and contextual\nobjects, thus they cannot comprehensively capture the multi-granularity\nsemantics, such as scenes, regional cues of persons, and interactions among\npersons and objects. To bridge the domain gap, we propose a Multi-Granularity\nReasoning framework for social relation recognition from images. The global\nknowledge and mid-level details are learned from the whole scene and the\nregions of persons and objects, respectively. Most importantly, we explore the\nfine-granularity pose keypoints of persons to discover the interactions among\npersons and objects. Specifically, the pose-guided Person-Object Graph and\nPerson-Pose Graph are proposed to model the actions from persons to object and\nthe interactions between paired persons, respectively. Based on the graphs,\nsocial relation reasoning is performed by graph convolutional networks.\nFinally, the global features and reasoned knowledge are integrated as a\ncomprehensive representation for social relation recognition. Extensive\nexperiments on two public datasets show the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 09:09:44 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Zhang", "Meng", ""], ["Liu", "Xinchen", ""], ["Liu", "Wu", ""], ["Zhou", "Anfu", ""], ["Ma", "Huadong", ""], ["Mei", "Tao", ""]]}, {"id": "1901.03068", "submitter": "Rustam Latypov", "authors": "Rustam Latypov and Evgeni Stolov", "title": "New Radon Transform Based Texture Features of Handwritten Document", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present some new features describing the handwritten\ndocument as a texture. These features are based on the Radon transform. All\nvalues can be obtained easily and suit for the coarse classification of\ndocuments.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 09:13:16 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Latypov", "Rustam", ""], ["Stolov", "Evgeni", ""]]}, {"id": "1901.03088", "submitter": "Deepak Anand", "authors": "Goutham Ramakrishnan, Deepak Anand and Amit Sethi", "title": "Fast GPU-Enabled Color Normalization for Digital Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Normalizing unwanted color variations due to differences in staining\nprocesses and scanner responses has been shown to aid machine learning in\ncomputational pathology. Of the several popular techniques for color\nnormalization, structure preserving color normalization (SPCN) is\nwell-motivated, convincingly tested, and published with its code base. However,\nSPCN makes occasional errors in color basis estimation leading to artifacts\nsuch as swapping the color basis vectors between stains or giving a colored\ntinge to the background with no tissue. We made several algorithmic\nimprovements to remove these artifacts. Additionally, the original SPCN code is\nnot readily usable on gigapixel whole slide images (WSIs) due to long run\ntimes, use of proprietary software platform and libraries, and its inability to\nautomatically handle WSIs. We completely rewrote the software such that it can\nautomatically handle images of any size in popular WSI formats. Our software\nutilizes GPU-acceleration and open-source libraries that are becoming\nubiquitous with the advent of deep learning. We also made several other small\nimprovements and achieved a multifold overall speedup on gigapixel images. Our\nalgorithm and software is usable right out-of-the-box by the computational\npathology community.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 10:42:26 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Ramakrishnan", "Goutham", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "1901.03107", "submitter": "Arpan Gupta", "authors": "Arpan Gupta and Sakthi Balan M", "title": "Cricket stroke extraction: Towards creation of a large-scale cricket\n  actions dataset", "comments": "14 pages (excluding references), 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of temporal action localization for a\nlarge-scale untrimmed cricket videos dataset. Our action of interest for\ncricket videos is a cricket stroke played by a batsman, which is, usually,\ncovered by cameras placed at the stands of the cricket ground at both ends of\nthe cricket pitch. After applying a sequence of preprocessing steps, we have\n~73 million frames for 1110 videos in the dataset at constant frame rate and\nresolution. The method of localization is a generalized one which applies a\ntrained random forest model for CUTs detection(using summed up grayscale\nhistogram difference features) and two linear SVM camera models(CAM1 and CAM2)\nfor first frame detection, trained on HOG features of CAM1 and CAM2 video\nshots. CAM1 and CAM2 are assumed to be part of the cricket stroke. At the\npredicted boundary positions, the HOG features of the first frames are computed\nand a simple algorithm was used to combine the positively predicted camera\nshots. In order to make the process as generic as possible, we did not consider\nany domain specific knowledge, such as tracking or specific shape and motion\nfeatures.\n  The detailed analysis of our methodology is provided along with the metrics\nused for evaluation of individual models, and the final predicted segments. We\nachieved a weighted mean TIoU of 0.5097 over a small sample of the test set.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 11:35:28 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Gupta", "Arpan", ""], ["M", "Sakthi Balan", ""]]}, {"id": "1901.03162", "submitter": "Artemij Amiranashvili", "authors": "Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, Thomas Brox", "title": "Motion Perception in Reinforcement Learning with Dynamic Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic environments, learned controllers are supposed to take motion into\naccount when selecting the action to be taken. However, in existing\nreinforcement learning works motion is rarely treated explicitly; it is rather\nassumed that the controller learns the necessary motion representation from\ntemporal stacks of frames implicitly. In this paper, we show that for\ncontinuous control tasks learning an explicit representation of motion improves\nthe quality of the learned controller in dynamic scenarios. We demonstrate this\non common benchmark tasks (Walker, Swimmer, Hopper), on target reaching and\nball catching tasks with simulated robotic arms, and on a dynamic single ball\njuggling task. Moreover, we find that when equipped with an appropriate network\narchitecture, the agent can, on some tasks, learn motion features also with\npure reinforcement learning, without additional supervision. Further we find\nthat using an image difference between the current and the previous frame as an\nadditional input leads to better results than a temporal stack of frames.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:59:19 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 15:30:32 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Amiranashvili", "Artemij", ""], ["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""], ["Brox", "Thomas", ""]]}, {"id": "1901.03198", "submitter": "Yanlin Qian", "authors": "Yanlin Qian, Joni-Kristian K\\\"am\\\"ar\\\"ainen, Jarno Nikkanen, Jiri\n  Matas", "title": "On Finding Gray Pixels", "comments": "appear in IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2019. 9 pages, 7 figures. this article is an\n  extension of arXiv:1803.08326", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel grayness index for finding gray pixels and demonstrate its\neffectiveness and efficiency in illumination estimation. The grayness index, GI\nin short, is derived using the Dichromatic Reflection Model and is\nlearning-free. GI allows to estimate one or multiple illumination sources in\ncolor-biased images. On standard single-illumination and multiple-illumination\nestimation benchmarks, GI outperforms state-of-the-art statistical methods and\nmany recent deep methods. GI is simple and fast, written in a few dozen lines\nof code, processing a 1080p image in ~0.4 seconds with a non-optimized Matlab\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 13:44:13 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 13:32:01 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 12:49:32 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Qian", "Yanlin", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Nikkanen", "Jarno", ""], ["Matas", "Jiri", ""]]}, {"id": "1901.03201", "submitter": "Paria Mehrani", "authors": "Paria Mehrani and John K. Tsotsos", "title": "Early recurrence enables figure border ownership", "comments": "31 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The face-vase illusion introduced by Rubin demonstrates how one can switch\nback and forth between two different interpretations depending on how the\nfigure outlines are assigned [1]. This border ownership assignment is an\nimportant step in the perception of forms. Zhou et al. [2] found neurons in the\nvisual cortex whose responses not only depend on the local features present in\ntheir classical receptive fields, but also on their contextual information.\nVarious models proposed that feedback from higher ventral areas or lateral\nconnections could provide the required contextual information. However, some\nstudies [3, 4, 5] ruled out the plausibility of models exclusively based on\nlateral connections. In addition, further evidence [6] suggests that ventral\nfeedback even from V4 is not fast enough to provide context to border ownership\nneurons in either V1 or V2. As a result, the border ownership assignment\nmechanism in the brain is a mystery yet to be solved. Here, we test with\ncomputational simulations the hypothesis that the dorsal stream provides the\nglobal information to border ownership cells in the ventral stream. Our\nproposed model incorporates early recurrence from the dorsal pathway as well as\nlateral modulations within the ventral stream. Our simulation experiments show\nthat our model border ownership neurons, similar to their biological\ncounterparts, exhibit different responses to figures on either side of the\nborder.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 14:45:13 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 01:48:59 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Mehrani", "Paria", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1901.03278", "submitter": "Kai Chen", "authors": "Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, Dahua Lin", "title": "Region Proposal by Guided Anchoring", "comments": "CVPR 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region anchors are the cornerstone of modern object detection techniques.\nState-of-the-art detectors mostly rely on a dense anchoring scheme, where\nanchors are sampled uniformly over the spatial domain with a predefined set of\nscales and aspect ratios. In this paper, we revisit this foundational stage.\nOur study shows that it can be done much more effectively and efficiently.\nSpecifically, we present an alternative scheme, named Guided Anchoring, which\nleverages semantic features to guide the anchoring. The proposed method jointly\npredicts the locations where the center of objects of interest are likely to\nexist as well as the scales and aspect ratios at different locations. On top of\npredicted anchor shapes, we mitigate the feature inconsistency with a feature\nadaption module. We also study the use of high-quality proposals to improve\ndetection performance. The anchoring scheme can be seamlessly integrated into\nproposal methods and detectors. With Guided Anchoring, we achieve 9.1% higher\nrecall on MS COCO with 90% fewer anchors than the RPN baseline. We also adopt\nGuided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively\nimproving the detection mAP by 2.2%, 2.7% and 1.2%. Code will be available at\nhttps://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:13:13 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 06:25:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Jiaqi", ""], ["Chen", "Kai", ""], ["Yang", "Shuo", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1901.03281", "submitter": "Qi Xie", "authors": "Qi Xie, Minghao Zhou, Qian Zhao, Deyu Meng, Wangmeng Zuo, Zongben Xu", "title": "Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging can help better understand the characteristics of\ndifferent materials, compared with traditional image systems. However, only\nhigh-resolution multispectral (HrMS) and low-resolution hyperspectral (LrHS)\nimages can generally be captured at video rate in practice. In this paper, we\npropose a model-based deep learning approach for merging an HrMS and LrHS\nimages to generate a high-resolution hyperspectral (HrHS) image. In specific,\nwe construct a novel MS/HS fusion model which takes the observation models of\nlow-resolution images and the low-rankness knowledge along the spectral mode of\nHrHS image into consideration. Then we design an iterative algorithm to solve\nthe model by exploiting the proximal gradient method. And then, by unfolding\nthe designed algorithm, we construct a deep network, called MS/HS Fusion Net,\nwith learning the proximal operators and model parameters by convolutional\nneural networks. Experimental results on simulated and real data substantiate\nthe superiority of our method both visually and quantitatively as compared with\nstate-of-the-art methods along this line of research.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:16:59 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Xie", "Qi", ""], ["Zhou", "Minghao", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Zuo", "Wangmeng", ""], ["Xu", "Zongben", ""]]}, {"id": "1901.03353", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Mykhailo Shvets, Alexander C. Berg", "title": "RetinaMask: Learning to predict masks improves state-of-the-art\n  single-shot detection for free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently two-stage detectors have surged ahead of single-shot detectors in\nthe accuracy-vs-speed trade-off. Nevertheless single-shot detectors are\nimmensely popular in embedded vision applications. This paper brings\nsingle-shot detectors up to the same level as current two-stage techniques. We\ndo this by improving training for the state-of-the-art single-shot detector,\nRetinaNet, in three ways: integrating instance mask prediction for the first\ntime, making the loss function adaptive and more stable, and including\nadditional hard examples in training. We call the resulting augmented network\nRetinaMask. The detection component of RetinaMask has the same computational\ncost as the original RetinaNet, but is more accurate. COCO test-dev results are\nup to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the\nruntime is the same during evaluation. Adding Group Normalization increases the\nperformance of RetinaMask-101 to 41.7 mAP. Code is\nat:https://github.com/chengyangfu/retinamask\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 19:35:28 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Shvets", "Mykhailo", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1901.03360", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Antonio Loquercio, Davide Scaramuzza and Stefano Soatto", "title": "Unsupervised Moving Object Detection via Contextual Information\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adversarial contextual model for detecting moving objects in\nimages. A deep neural network is trained to predict the optical flow in a\nregion using information from everywhere else but that region (context), while\nanother network attempts to make such context as uninformative as possible. The\nresult is a model where hypotheses naturally compete with no need for explicit\nregularization or hyper-parameter tuning. Although our method requires no\nsupervision whatsoever, it outperforms several methods that are pre-trained on\nlarge annotated datasets. Our model can be thought of as a generalization of\nclassical variational generative region-based segmentation, but in a way that\navoids explicit regularization or solution of partial differential equations at\nrun-time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 19:58:16 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 07:21:07 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yang", "Yanchao", ""], ["Loquercio", "Antonio", ""], ["Scaramuzza", "Davide", ""], ["Soatto", "Stefano", ""]]}, {"id": "1901.03398", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Characterizing and evaluating adversarial examples for Offline\n  Handwritten Signature Verification", "comments": "Accepted for the IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": "10.1109/TIFS.2019.2894031", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of Adversarial Examples is attracting increasing interest from\nthe Machine Learning community, due to its significant impact to the security\nof Machine Learning systems. Adversarial examples are similar (from a\nperceptual notion of similarity) to samples from the data distribution, that\n\"fool\" a machine learning classifier. For computer vision applications, these\nare images with carefully crafted but almost imperceptible changes, that are\nmisclassified. In this work, we characterize this phenomenon under an existing\ntaxonomy of threats to biometric systems, in particular identifying new attacks\nfor Offline Handwritten Signature Verification systems. We conducted an\nextensive set of experiments on four widely used datasets: MCYT-75, CEDAR,\nGPDS-160 and the Brazilian PUC-PR, considering both a CNN-based system and a\nsystem using a handcrafted feature extractor (CLBP). We found that attacks that\naim to get a genuine signature rejected are easy to generate, even in a limited\nknowledge scenario, where the attacker does not have access to the trained\nclassifier nor the signatures used for training. Attacks that get a forgery to\nbe accepted are harder to produce, and often require a higher level of noise -\nin most cases, no longer \"imperceptible\" as previous findings in object\nrecognition. We also evaluated the impact of two countermeasures on the success\nrate of the attacks and the amount of noise required for generating successful\nattacks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 21:14:11 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1901.03419", "submitter": "Jin Zhu", "authors": "Jin Zhu, Guang Yang and Pietro Lio", "title": "How Can We Make GAN Perform Better in Single Medical Image\n  Super-Resolution? A Lesion Focused Multi-Scale Approach", "comments": "5 pages, 4 figure, 1 table. Accepted at 2019 IEEE International\n  Symposium on Biomedical Imaging (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is of great importance as a low-level\ncomputer vision task. The fast development of Generative Adversarial Network\n(GAN) based deep learning architectures realises an efficient and effective\nSISR to boost the spatial resolution of natural images captured by digital\ncameras. However, the SISR for medical images is still a very challenging\nproblem. This is due to (1) compared to natural images, in general, medical\nimages have lower signal to noise ratios, (2) GAN based models pre-trained on\nnatural images may synthesise unrealistic patterns in medical images which\ncould affect the clinical interpretation and diagnosis, and (3) the vanilla GAN\narchitecture may suffer from unstable training and collapse mode that can also\naffect the SISR results. In this paper, we propose a novel lesion focused SR\n(LFSR) method, which incorporates GAN to achieve perceptually realistic SISR\nresults for brain tumour MRI images. More importantly, we test and make\ncomparison using recently developed GAN variations, e.g., Wasserstein GAN\n(WGAN) and WGAN with Gradient Penalty (WGAN-GP), and propose a novel\nmulti-scale GAN (MS-GAN), to achieve a more stabilised and efficient training\nand improved perceptual quality of the super-resolved results. Based on both\nquantitative evaluations and our designed mean opinion score, the proposed LFSR\ncoupled with MS-GAN has performed better in terms of both perceptual quality\nand efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 22:24:46 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Zhu", "Jin", ""], ["Yang", "Guang", ""], ["Lio", "Pietro", ""]]}, {"id": "1901.03446", "submitter": "Tong He", "authors": "Tong He, Stefano Soatto", "title": "Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses\n  and Task Priors", "comments": "Proc. of the AAAI, September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to infer 3D pose and shape of vehicles from a single\nimage. To tackle this ill-posed problem, we optimize two-scale projection\nconsistency between the generated 3D hypotheses and their 2D\npseudo-measurements. Specifically, we use a morphable wireframe model to\ngenerate a fine-scaled representation of vehicle shape and pose. To reduce its\nsensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse\nrepresentation which improves robustness. We also integrate three task priors,\nincluding unsupervised monocular depth, a ground plane constraint as well as\nvehicle shape priors, with forward projection errors into an overall energy\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 01:21:10 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["He", "Tong", ""], ["Soatto", "Stefano", ""]]}, {"id": "1901.03447", "submitter": "Ning Yu", "authors": "Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Michal\n  Lukac", "title": "Texture Mixer: A Network for Controllable Synthesis and Interpolation of\n  Texture", "comments": "Accepted to CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of interpolating visual textures. We\nformulate this problem by requiring (1) by-example controllability and (2)\nrealistic and smooth interpolation among an arbitrary number of texture\nsamples. To solve it we propose a neural network trained simultaneously on a\nreconstruction task and a generation task, which can project texture examples\nonto a latent space where they can be linearly interpolated and projected back\nonto the image domain, thus ensuring both intuitive control and realistic\nresults. We show our method outperforms a number of baselines according to a\ncomprehensive suite of metrics as well as a user study. We further show several\napplications based on our technique, which include texture brush, texture\ndissolve, and animal hybridization.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 01:21:12 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 09:22:11 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yu", "Ning", ""], ["Barnes", "Connelly", ""], ["Shechtman", "Eli", ""], ["Amirghodsi", "Sohrab", ""], ["Lukac", "Michal", ""]]}, {"id": "1901.03460", "submitter": "Zheng Shou", "authors": "Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus\n  Rohrbach, Shih-Fu Chang, Zhicheng Yan", "title": "DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video\n  Action Recognition", "comments": "Accepted by CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion has shown to be useful for video understanding, where motion is\ntypically represented by optical flow. However, computing flow from video\nframes is very time-consuming. Recent works directly leverage the motion\nvectors and residuals readily available in the compressed video to represent\nmotion at no cost. While this avoids flow computation, it also hurts accuracy\nsince the motion vector is noisy and has substantially reduced resolution,\nwhich makes it a less discriminative motion representation. To remedy these\nissues, we propose a lightweight generator network, which reduces noises in\nmotion vectors and captures fine motion details, achieving a more\nDiscriminative Motion Cue (DMC) representation. Since optical flow is a more\naccurate motion representation, we train the DMC generator to approximate flow\nusing a reconstruction loss and a generative adversarial loss, jointly with the\ndownstream action classification task. Extensive evaluations on three action\nrecognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the\neffectiveness of our method. Our full system, consisting of the generator and\nthe classifier, is coined as DMC-Net which obtains high accuracy close to that\nof using flow and runs two orders of magnitude faster than using optical flow\nat inference time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 02:39:41 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 19:56:57 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 20:51:27 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Shou", "Zheng", ""], ["Lin", "Xudong", ""], ["Kalantidis", "Yannis", ""], ["Sevilla-Lara", "Laura", ""], ["Rohrbach", "Marcus", ""], ["Chang", "Shih-Fu", ""], ["Yan", "Zhicheng", ""]]}, {"id": "1901.03462", "submitter": "Xiaoyan Gu", "authors": "Yizhi Liu, Xiaoyan Gu, Lei Huang, Junlin Ouyang, Miao Liao, Liangran\n  Wu", "title": "Analyzing Periodicity and Saliency for Adult Video Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based adult video detection plays an important role in preventing\npornography. However, existing methods usually rely on single modality and\nseldom focus on multi-modality semantics representation. Addressing at this\nproblem, we put forward an approach of analyzing periodicity and saliency for\nadult video detection. At first, periodic patterns and salient regions are\nrespective-ly analyzed in audio-frames and visual-frames. Next, the multi-modal\nco-occurrence semantics is described by combining audio periodicity with visual\nsaliency. Moreover, the performance of our approach is evaluated step by step.\nExperimental results show that our approach obviously outper-forms some\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 02:50:29 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Liu", "Yizhi", ""], ["Gu", "Xiaoyan", ""], ["Huang", "Lei", ""], ["Ouyang", "Junlin", ""], ["Liao", "Miao", ""], ["Wu", "Liangran", ""]]}, {"id": "1901.03465", "submitter": "Duong H. Nguyen", "authors": "Duong Hai Nguyen, Tai Nhu Do, In-Seop Na, Soo-Hyung Kim", "title": "Hand Segmentation and Fingertip Tracking from Depth Camera Images Using\n  Deep Convolutional Neural Network and Multi-task SegNet", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand segmentation and fingertip detection play an indispensable role in hand\ngesture-based human-machine interaction systems. In this study, we propose a\nmethod to discriminate hand components and to locate fingertips in RGB-D\nimages. The system consists of three main steps: hand detection using RGB\nimages providing regions which are considered as promising areas for further\nprocessing, hand segmentation, and fingertip detection using depth image and\nour modified SegNet, a single lightweight architecture that can process two\nindependent tasks at the same time. The experimental results show that our\nsystem is a promising method for hand segmentation and fingertip detection\nwhich achieves a comparable performance while model complexity is suitable for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 03:06:19 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 13:19:12 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 15:17:14 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Nguyen", "Duong Hai", ""], ["Do", "Tai Nhu", ""], ["Na", "In-Seop", ""], ["Kim", "Soo-Hyung", ""]]}, {"id": "1901.03470", "submitter": "Dong Jiang", "authors": "Shenglan Liu and Dong Jiang and Lin Feng and Feilong Wang and Zhanbo\n  Feng and Xiang Liu and Shuai Guo and Bingjun Li and Yuchen Cong", "title": "Color Recognition for Rubik's Cube Robot", "comments": "6 pages, 6 figures, uses spconf.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed three methods to solve color recognition of\nRubik's cube, which includes one offline method and two online methods. Scatter\nbalance \\& extreme learning machine (SB-ELM), a offline method, is proposed to\nillustrate the efficiency of training based method. We also point out the\nconception of color drifting which indicates offline methods are always\nineffectiveness and can not work well in continuous change circumstance. By\ncontrast, dynamic weight label propagation is proposed for labeling blocks\ncolor by known center blocks color of Rubik's cube. Furthermore, weak label\nhierarchic propagation, another online method, is also proposed for unknown all\ncolor information but only utilizes weak label of center block in color\nrecognition. We finally design a Rubik's cube robot and construct a dataset to\nillustrate the efficiency and effectiveness of our online methods and to\nindicate the ineffectiveness of offline method by color drifting in our\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 04:11:45 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Liu", "Shenglan", ""], ["Jiang", "Dong", ""], ["Feng", "Lin", ""], ["Wang", "Feilong", ""], ["Feng", "Zhanbo", ""], ["Liu", "Xiang", ""], ["Guo", "Shuai", ""], ["Li", "Bingjun", ""], ["Cong", "Yuchen", ""]]}, {"id": "1901.03472", "submitter": "Xu Li", "authors": "Xinling Zhang, Xu Li, Ying Chen, Yixin Gan, Dexing Kong, Rongqin Zheng", "title": "Segmentation of Levator Hiatus Using Multi-Scale Local Region Active\n  contours and Boundary Shape Similarity Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multi-scale framework with local region based active contour\nand boundary shape similarity constraint is proposed for the segmentation of\nlevator hiatus in ultrasound images. In this paper, we proposed a multiscale\nactive contour framework to segment levator hiatus ultrasound images by\ncombining the local region information and boundary shape similarity\nconstraint. In order to get more precisely initializations and reduce the\ncomputational cost, Gaussian pyramid method is used to decompose the image into\ncoarse-to-fine scales. A localized region active contour model is firstly\nperformed on the coarsest scale image to get a rough contour of the levator\nhiatus, then the segmentation result on the coarse scale is interpolated into\nthe finer scale image as the initialization. The boundary shape similarity\nbetween different scales is incorporate into the local region based active\ncontour model so that the result from coarse scale can guide the contour\nevolution at finer scale. By incorporating the multi-scale and boundary shape\nsimilarity, the proposed method can precisely locate the levator hiatus\nboundaries despite various ultrasound image artifacts. With a data set of 90\nlevator hiatus ultrasound images, the efficiency and accuracy of the proposed\nmethod are validated by quantitative and qualitative evaluations (TP, FP, Js)\nand comparison with other two state-of-art active contour segmentation methods\n(C-V model, DRLSE model).\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 04:15:09 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Zhang", "Xinling", ""], ["Li", "Xu", ""], ["Chen", "Ying", ""], ["Gan", "Yixin", ""], ["Kong", "Dexing", ""], ["Zheng", "Rongqin", ""]]}, {"id": "1901.03473", "submitter": "Longlong Jing", "authors": "Jiaxing Tan, Longlong Jing, Yumei Huo, Yingli Tian, Oguz Akin", "title": "LGAN: Lung Segmentation in CT Scans Using Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung segmentation in computerized tomography (CT) images is an important\nprocedure in various lung disease diagnosis. Most of the current lung\nsegmentation approaches are performed through a series of procedures with\nmanually empirical parameter adjustments in each step. Pursuing an automatic\nsegmentation method with fewer steps, in this paper, we propose a novel deep\nlearning Generative Adversarial Network (GAN) based lung segmentation schema,\nwhich we denote as LGAN. Our proposed schema can be generalized to different\nkinds of neural networks for lung segmentation in CT images and is evaluated on\na dataset containing 220 individual CT scans with two metrics: segmentation\nquality and shape similarity. Also, we compared our work with current state of\nthe art methods. The results obtained with this study demonstrate that the\nproposed LGAN schema can be used as a promising tool for automatic lung\nsegmentation due to its simplified procedure as well as its good performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 04:26:44 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Tan", "Jiaxing", ""], ["Jing", "Longlong", ""], ["Huo", "Yumei", ""], ["Tian", "Yingli", ""], ["Akin", "Oguz", ""]]}, {"id": "1901.03495", "submitter": "Shuyang Sun", "authors": "Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang", "title": "FishNet: A Versatile Backbone for Image, Region, and Pixel Level\n  Prediction", "comments": "NeurIPS 2018. Code available at https://github.com/kevin-ssy/FishNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic principles in designing convolutional neural network (CNN)\nstructures for predicting objects on different levels, e.g., image-level,\nregion-level, and pixel-level are diverging. Generally, network structures\ndesigned specifically for image classification are directly used as default\nbackbone structure for other tasks including detection and segmentation, but\nthere is seldom backbone structure designed under the consideration of unifying\nthe advantages of networks designed for pixel-level or region-level predicting\ntasks, which may require very deep features with high resolution. Towards this\ngoal, we design a fish-like network, called FishNet. In FishNet, the\ninformation of all resolutions is preserved and refined for the final task.\nBesides, we observe that existing works still cannot \\emph{directly} propagate\nthe gradient information from deep layers to shallow layers. Our design can\nbetter handle this problem. Extensive experiments have been conducted to\ndemonstrate the remarkable performance of the FishNet. In particular, on\nImageNet-1k, the accuracy of FishNet is able to surpass the performance of\nDenseNet and ResNet with fewer parameters. FishNet was applied as one of the\nmodules in the winning entry of the COCO Detection 2018 challenge. The code is\navailable at https://github.com/kevin-ssy/FishNet.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 06:43:56 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Sun", "Shuyang", ""], ["Pang", "Jiangmiao", ""], ["Shi", "Jianping", ""], ["Yi", "Shuai", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1901.03517", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Marco Lorenzi, Stefano B. Blumberg, Alexandra L.\n  Young, Pere P. Morell, Neil P. Oxtoby, Arman Eshaghi, Keir X. Yong, Sebastian\n  J. Crutch, Polina Golland, Daniel C. Alexander (for the Alzheimer's Disease\n  Neuroimaging Initiative)", "title": "Disease Knowledge Transfer across Neurodegenerative Diseases", "comments": "accepted at MICCAI 2019, 13 pages, 5 figures, 2 tables", "journal-ref": "Medical Image Computing and Computer Assisted Intervention 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Disease Knowledge Transfer (DKT), a novel technique for\ntransferring biomarker information between related neurodegenerative diseases.\nDKT infers robust multimodal biomarker trajectories in rare neurodegenerative\ndiseases even when only limited, unimodal data is available, by transferring\ninformation from larger multimodal datasets from common neurodegenerative\ndiseases. DKT is a joint-disease generative model of biomarker progressions,\nwhich exploits biomarker relationships that are shared across diseases. Our\nproposed method allows, for the first time, the estimation of plausible,\nmultimodal biomarker trajectories in Posterior Cortical Atrophy (PCA), a rare\nneurodegenerative disease where only unimodal MRI data is available. For this\nwe train DKT on a combined dataset containing subjects with two distinct\ndiseases and sizes of data available: 1) a larger, multimodal typical AD (tAD)\ndataset from the TADPOLE Challenge, and 2) a smaller unimodal Posterior\nCortical Atrophy (PCA) dataset from the Dementia Research Centre (DRC), for\nwhich only a limited number of Magnetic Resonance Imaging (MRI) scans are\navailable. Although validation is challenging due to lack of data in PCA, we\nvalidate DKT on synthetic data and two patient datasets (TADPOLE and PCA\ncohorts), showing it can estimate the ground truth parameters in the simulation\nand predict unseen biomarkers on the two patient datasets. While we\ndemonstrated DKT on Alzheimer's variants, we note DKT is generalisable to other\nforms of related neurodegenerative diseases. Source code for DKT is available\nonline: https://github.com/mrazvan22/dkt.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 09:11:27 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 14:25:07 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Marinescu", "Razvan V.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Lorenzi", "Marco", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Blumberg", "Stefano B.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Young", "Alexandra L.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Morell", "Pere P.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Oxtoby", "Neil P.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Eshaghi", "Arman", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Yong", "Keir X.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Crutch", "Sebastian J.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Golland", "Polina", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Alexander", "Daniel C.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"]]}, {"id": "1901.03546", "submitter": "Rishab Sharma", "authors": "Rishab Sharma and Anirudha Vishvakarma", "title": "Retrieving Similar E-Commerce Images Using Deep Learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity.\nWe present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion. We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:48:48 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Sharma", "Rishab", ""], ["Vishvakarma", "Anirudha", ""]]}, {"id": "1901.03547", "submitter": "Andrea Migliorati", "authors": "Andrea Migliorati, Attilio Fiandrotti, Gianluca Francini, Skjalg\n  Lepsoy, Riccardo Leonardi", "title": "Feature Fusion for Robust Patch Matching With Compact Binary Descriptors", "comments": "MMSP 2018 - IEEE 20th International Workshop on Multimedia Signal\n  Processing - August 29-31 2018, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of learning compact yet discriminative patch\ndescriptors within a deep learning framework. We observe that features\nextracted by convolutional layers in the pixel domain are largely complementary\nto features extracted in a transformed domain. We propose a convolutional\nnetwork framework for learning binary patch descriptors where pixel domain\nfeatures are fused with features extracted from the transformed domain. In our\nframework, while convolutional and transformed features are distinctly\nextracted, they are fused and provided to a single classifier which thus\njointly operates on convolutional and transformed features. We experiment at\nmatching patches from three different datasets, showing that our feature fusion\napproach outperforms multiple state-of-the-art approaches in terms of accuracy,\nrate, and complexity.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:52:54 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Migliorati", "Andrea", ""], ["Fiandrotti", "Attilio", ""], ["Francini", "Gianluca", ""], ["Lepsoy", "Skjalg", ""], ["Leonardi", "Riccardo", ""]]}, {"id": "1901.03553", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Arman Eshaghi, Marco Lorenzi, Alexandra L. Young,\n  Neil P. Oxtoby, Sara Garbarino, Sebastian J. Crutch, Daniel C. Alexander (for\n  the Alzheimer's Disease Neuroimaging Initiative)", "title": "DIVE: A spatiotemporal progression model of brain pathology in\n  neurodegenerative disorders", "comments": "24 pages, 5 figures, 2 tables, 1 algorithm", "journal-ref": "NeuroImage, Volume 192, 15 May 2019, Pages 166-177", "doi": "10.1016/j.neuroimage.2019.02.053", "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we present DIVE: Data-driven Inference of Vertexwise Evolution. DIVE is\nan image-based disease progression model with single-vertex resolution,\ndesigned to reconstruct long-term patterns of brain pathology from short-term\nlongitudinal data sets. DIVE clusters vertex-wise biomarker measurements on the\ncortical surface that have similar temporal dynamics across a patient\npopulation, and concurrently estimates an average trajectory of vertex\nmeasurements in each cluster. DIVE uniquely outputs a parcellation of the\ncortex into areas with common progression patterns, leading to a new signature\nfor individual diseases. DIVE further estimates the disease stage and\nprogression speed for every visit of every subject, potentially enhancing\nstratification for clinical trials or management. On simulated data, DIVE can\nrecover ground truth clusters and their underlying trajectory, provided the\naverage trajectories are sufficiently different between clusters. We\ndemonstrate DIVE on data from two cohorts: the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) and the Dementia Research Centre (DRC), UK, containing\npatients with Posterior Cortical Atrophy (PCA) as well as typical Alzheimer's\ndisease (tAD). DIVE finds similar spatial patterns of atrophy for tAD subjects\nin the two independent datasets (ADNI and DRC), and further reveals distinct\npatterns of pathology in different diseases (tAD vs PCA) and for distinct types\nof biomarker data: cortical thickness from Magnetic Resonance Imaging (MRI) vs\namyloid load from Positron Emission Tomography (PET). Finally, DIVE can be used\nto estimate a fine-grained spatial distribution of pathology in the brain using\nany kind of voxelwise or vertexwise measures including Jacobian compression\nmaps, fractional anisotropy (FA) maps from diffusion imaging or other PET\nmeasures. DIVE source code is available online:\nhttps://github.com/mrazvan22/dive\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 11:13:44 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Marinescu", "Razvan V.", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Eshaghi", "Arman", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Lorenzi", "Marco", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Young", "Alexandra L.", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Oxtoby", "Neil P.", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Garbarino", "Sara", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Crutch", "Sebastian J.", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"], ["Alexander", "Daniel C.", "", "for\n  the Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1901.03554", "submitter": "Kancharagunta Kishan BABU", "authors": "Kishan Babu Kancharagunta and Shiv Ram Dubey", "title": "CSGAN: Cyclic-Synthesized Generative Adversarial Networks for\n  Image-to-Image Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary motivation of Image-to-Image Transformation is to convert an\nimage of one domain to another domain. Most of the research has been focused on\nthe task of image transformation for a set of pre-defined domains. Very few\nworks are reported that actually developed a common framework for\nimage-to-image transformation for different domains. With the introduction of\nGenerative Adversarial Networks (GANs) as a general framework for the image\ngeneration problem, there is a tremendous growth in the area of image-to-image\ntransformation. Most of the research focuses over the suitable objective\nfunction for image-to-image transformation. In this paper, we propose a new\nCyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image\ntransformation. The proposed CSGAN uses a new objective function (loss) called\nCyclic-Synthesized Loss (CS) between the synthesized image of one domain and\ncycled image of another domain. The performance of the proposed CSGAN is\nevaluated on two benchmark image-to-image transformation datasets, including\nCUHK Face dataset and CMP Facades dataset. The results are computed using the\nwidely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The\nexperimental results of the proposed CSGAN approach are compared with the\nlatest state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and\nPS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK\ndataset and exhibits the promising and comparable performance over Facades\ndataset in terms of both qualitative and quantitative measures. The code is\navailable at https://github.com/KishanKancharagunta/CSGAN.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 11:17:34 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Kancharagunta", "Kishan Babu", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1901.03577", "submitter": "Thierry Bouwmans", "authors": "T. Bouwmans, B. Garcia-Garcia", "title": "Background Subtraction in Real Applications: Challenges, Current Models\n  and Future Directions", "comments": "Submitted to Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision applications based on videos often require the detection of\nmoving objects in their first step. Background subtraction is then applied in\norder to separate the background and the foreground. In literature, background\nsubtraction is surely among the most investigated field in computer vision\nproviding a big amount of publications. Most of them concern the application of\nmathematical and machine learning models to be more robust to the challenges\nmet in videos. However, the ultimate goal is that the background subtraction\nmethods developed in research could be employed in real applications like\ntraffic surveillance. But looking at the literature, we can remark that there\nis often a gap between the current methods used in real applications and the\ncurrent methods in fundamental research. In addition, the videos evaluated in\nlarge-scale datasets are not exhaustive in the way that they only covered a\npart of the complete spectrum of the challenges met in real applications. In\nthis context, we attempt to provide the most exhaustive survey as possible on\nreal applications that used background subtraction in order to identify the\nreal challenges met in practice, the current used background models and to\nprovide future directions. Thus, challenges are investigated in terms of\ncamera, foreground objects and environments. In addition, we identify the\nbackground models that are effectively used in these applications in order to\nfind potential usable recent background models in terms of robustness, time and\nmemory requirements.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 12:55:48 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Bouwmans", "T.", ""], ["Garcia-Garcia", "B.", ""]]}, {"id": "1901.03597", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky, Tom Mahler, Ilan Shelef, Yuval Elovici", "title": "CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning", "comments": "This paper is included in the Proceedings of the 28th USENIX Security\n  Symposium (USENIX Security 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2018, clinics and hospitals were hit with numerous attacks leading to\nsignificant data breaches and interruptions in medical services. An attacker\nwith access to medical records can do much more than hold the data for ransom\nor sell it on the black market.\n  In this paper, we show how an attacker can use deep-learning to add or remove\nevidence of medical conditions from volumetric (3D) medical scans. An attacker\nmay perform this act in order to stop a political candidate, sabotage research,\ncommit insurance fraud, perform an act of terrorism, or even commit murder. We\nimplement the attack using a 3D conditional GAN and show how the framework\n(CT-GAN) can be automated. Although the body is complex and 3D medical scans\nare very large, CT-GAN achieves realistic results which can be executed in\nmilliseconds.\n  To evaluate the attack, we focused on injecting and removing lung cancer from\nCT scans. We show how three expert radiologists and a state-of-the-art deep\nlearning AI are highly susceptible to the attack. We also explore the attack\nsurface of a modern radiology network and demonstrate one attack vector: we\nintercepted and manipulated CT scans in an active hospital network with a\ncovert penetration test.\n  Demo video: https://youtu.be/_mkRAArj-x0\n  Source code: https://github.com/ymirsky/CT-GAN\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:41:31 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:40:04 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 10:15:36 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Mirsky", "Yisroel", ""], ["Mahler", "Tom", ""], ["Shelef", "Ilan", ""], ["Elovici", "Yuval", ""]]}, {"id": "1901.03628", "submitter": "Adam Harley", "authors": "Adam W. Harley, Shih-En Wei, Jason Saragih, Katerina Fragkiadaki", "title": "Image Disentanglement and Uncooperative Re-Entanglement for\n  High-Fidelity Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain image-to-image translation should satisfy two requirements: (1)\npreserve the information that is common to both domains, and (2) generate\nconvincing images covering variations that appear in the target domain. This is\nchallenging, especially when there are no example translations available as\nsupervision. Adversarial cycle consistency was recently proposed as a solution,\nwith beautiful and creative results, yielding much follow-up work. However,\naugmented reality applications cannot readily use such techniques to provide\nusers with compelling translations of real scenes, because the translations do\nnot have high-fidelity constraints. In other words, current models are liable\nto change details that should be preserved: while re-texturing a face, they may\nalter the face's expression in an unpredictable way. In this paper, we\nintroduce the problem of high-fidelity image-to-image translation, and present\na method for solving it. Our main insight is that low-fidelity translations\ntypically escape a cycle-consistency penalty, because the back-translator\nlearns to compensate for the forward-translator's errors. We therefore\nintroduce an optimization technique that prevents the networks from\ncooperating: simply train each network only when its input data is real. Prior\nworks, in comparison, train each network with a mix of real and generated data.\nExperimental results show that our method accurately disentangles the factors\nthat separate the domains, and converges to semantics-preserving translations\nthat prior methods miss.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 16:08:21 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 02:07:51 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Harley", "Adam W.", ""], ["Wei", "Shih-En", ""], ["Saragih", "Jason", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1901.03638", "submitter": "Tong Qin", "authors": "Tong Qin, Jie Pan, Shaozu Cao, and Shaojie Shen", "title": "A General Optimization-based Framework for Local Odometry Estimation\n  with Multiple Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, more and more sensors are equipped on robots to increase robustness\nand autonomous ability. We have seen various sensor suites equipped on\ndifferent platforms, such as stereo cameras on ground vehicles, a monocular\ncamera with an IMU (Inertial Measurement Unit) on mobile phones, and stereo\ncameras with an IMU on aerial robots. Although many algorithms for state\nestimation have been proposed in the past, they are usually applied to a single\nsensor or a specific sensor suite. Few of them can be employed with multiple\nsensor choices. In this paper, we proposed a general optimization-based\nframework for odometry estimation, which supports multiple sensor sets. Every\nsensor is treated as a general factor in our framework. Factors which share\ncommon state variables are summed together to build the optimization problem.\nWe further demonstrate the generality with visual and inertial sensors, which\nform three sensor suites (stereo cameras, a monocular camera with an IMU, and\nstereo cameras with an IMU). We validate the performance of our system on\npublic datasets and through real-world experiments with multiple sensors.\nResults are compared against other state-of-the-art algorithms. We highlight\nthat our system is a general framework, which can easily fuse various sensors\nin a pose graph optimization. Our implementations are open\nsource\\footnote{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 16:31:28 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Qin", "Tong", ""], ["Pan", "Jie", ""], ["Cao", "Shaozu", ""], ["Shen", "Shaojie", ""]]}, {"id": "1901.03642", "submitter": "Tong Qin", "authors": "Tong Qin, Shaozu Cao, Jie Pan, and Shaojie Shen", "title": "A General Optimization-based Framework for Global Pose Estimation with\n  Multiple Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate state estimation is a fundamental problem for autonomous robots. To\nachieve locally accurate and globally drift-free state estimation, multiple\nsensors with complementary properties are usually fused together. Local sensors\n(camera, IMU, LiDAR, etc) provide precise pose within a small region, while\nglobal sensors (GPS, magnetometer, barometer, etc) supply noisy but globally\ndrift-free localization in a large-scale environment. In this paper, we propose\na sensor fusion framework to fuse local states with global sensors, which\nachieves locally accurate and globally drift-free pose estimation. Local\nestimations, produced by existing VO/VIO approaches, are fused with global\nsensors in a pose graph optimization. Within the graph optimization, local\nestimations are aligned into a global coordinate. Meanwhile, the accumulated\ndrifts are eliminated. We evaluate the performance of our system on public\ndatasets and with real-world experiments. Results are compared against other\nstate-of-the-art algorithms. We highlight that our system is a general\nframework, which can easily fuse various global sensors in a unified pose graph\noptimization. Our implementations are open\nsource\\footnote{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 16:41:12 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Qin", "Tong", ""], ["Cao", "Shaozu", ""], ["Pan", "Jie", ""], ["Shen", "Shaojie", ""]]}, {"id": "1901.03660", "submitter": "Suayder Costa", "authors": "Suayder Milhomem, Tiago da Silva Almeida, Warley Gramacho da Silva,\n  Edeilson Milhomem da Silva and Rafael Lima de Carvalho", "title": "Weightless Neural Network with Transfer Learning to Detect Distress in\n  Asphalt", "comments": "6 pages, 5 figures, published on IJAERS", "journal-ref": "International Journal of Advanced Engineering Research and\n  Science, Page No: 294-299, vol.5,no. 12, date:2018", "doi": "10.22161/ijaers.5.12.40", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present paper shows a solution to the problem of automatic distress\ndetection, more precisely the detection of holes in paved roads. To do so, the\nproposed solution uses a weightless neural network known as Wisard to decide\nwhether an image of a road has any kind of cracks. In addition, the proposed\narchitecture also shows how the use of transfer learning was able to improve\nthe overall accuracy of the decision system. As a verification step of the\nresearch, an experiment was carried out using images from the streets at the\nFederal University of Tocantins, Brazil. The architecture of the developed\nsolution presents a result of 85.71% accuracy in the dataset, proving to be\nsuperior to approaches of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 09:33:22 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Milhomem", "Suayder", ""], ["Almeida", "Tiago da Silva", ""], ["da Silva", "Warley Gramacho", ""], ["da Silva", "Edeilson Milhomem", ""], ["de Carvalho", "Rafael Lima", ""]]}, {"id": "1901.03661", "submitter": "Arka Majumdar", "authors": "Shane Colburn, Yi Chu, Eli Shlizerman, Arka Majumdar", "title": "An Optical Frontend for a Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1364/AO.58.003179", "report-no": null, "categories": "cs.CV cs.ET cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parallelism of optics and the miniaturization of optical components using\nnanophotonic structures, such as metasurfaces present a compelling alternative\nto electronic implementations of convolutional neural networks. The lack of a\nlow-power optical nonlinearity, however, requires slow and energy-inefficient\nconversions between the electronic and optical domains. Here, we design an\narchitecture which utilizes a single electrical to optical conversion by\ndesigning a free-space optical frontend unit that implements the linear\noperations of the first layer with the subsequent layers realized\nelectronically. Speed and power analysis of the architecture indicates that the\nhybrid photonic-electronic architecture outperforms sole electronic\narchitecture for large image sizes and kernels. Benchmarking of the\nphotonic-electronic architecture on a modified version of AlexNet achieves a\nclassification accuracy of 87% on images from the Kaggle Cats and Dogs\nchallenge database.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 06:46:19 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 02:16:45 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Colburn", "Shane", ""], ["Chu", "Yi", ""], ["Shlizerman", "Eli", ""], ["Majumdar", "Arka", ""]]}, {"id": "1901.03662", "submitter": "Andrew Gilman", "authors": "Soren Bouma, Matthew D. M. Pawley, Krista Hupman and Andrew Gilman", "title": "Individual common dolphin identification via metric embedding learning", "comments": "Published in IVCNZ 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-identification (photo-id) of dolphin individuals is a commonly used\ntechnique in ecological sciences to monitor state and health of individuals, as\nwell as to study the social structure and distribution of a population.\nTraditional photo-id involves a laborious manual process of matching each\ndolphin fin photograph captured in the field to a catalogue of known\nindividuals.\n  We examine this problem in the context of open-set recognition and utilise a\ntriplet loss function to learn a compact representation of fin images in a\nEuclidean embedding, where the Euclidean distance metric represents fin\nsimilarity. We show that this compact representation can be successfully learnt\nfrom a fairly small (in deep learning context) training set and still\ngeneralise well to out-of-sample identities (completely new dolphin\nindividuals), with top-1 and top-5 test set (37 individuals) accuracy of\n$90.5\\pm2$ and $93.6\\pm1$ percent. In the presence of 1200 distractors, top-1\naccuracy dropped by $12\\%$; however, top-5 accuracy saw only a $2.8\\%$ drop\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 02:29:20 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Bouma", "Soren", ""], ["Pawley", "Matthew D. M.", ""], ["Hupman", "Krista", ""], ["Gilman", "Andrew", ""]]}, {"id": "1901.03665", "submitter": "Ethan Harris", "authors": "Ethan Harris, Mahesan Niranjan, Jonathon Hare", "title": "A Biologically Inspired Visual Working Memory for Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to look multiple times through a series of pose-adjusted glimpses\nis fundamental to human vision. This critical faculty allows us to understand\nhighly complex visual scenes. Short term memory plays an integral role in\naggregating the information obtained from these glimpses and informing our\ninterpretation of the scene. Computational models have attempted to address\nglimpsing and visual attention but have failed to incorporate the notion of\nmemory. We introduce a novel, biologically inspired visual working memory\narchitecture that we term the Hebb-Rosenblatt memory. We subsequently introduce\na fully differentiable Short Term Attentive Working Memory model (STAWM) which\nuses transformational attention to learn a memory over each image it sees. The\nstate of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space\nof a layer. By projecting different queries through this layer we can obtain\ngoal-oriented latent representations for tasks including classification and\nvisual reconstruction. Our model obtains highly competitive classification\nperformance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset,\nto perform reconstruction the model learns to make a sequence of updates to a\ncanvas which constitute a parts-based representation. Classification with the\nself supervised representation obtained from MNIST is shown to be in line with\nthe state of the art models (none of which use a visual attention mechanism).\nFinally, we show that STAWM can be trained under the dual constraints of\nclassification and reconstruction to provide an interpretable visual sketchpad\nwhich helps open the 'black-box' of deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 09:12:56 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Harris", "Ethan", ""], ["Niranjan", "Mahesan", ""], ["Hare", "Jonathon", ""]]}, {"id": "1901.03706", "submitter": "Yatie Xiao", "authors": "Yatie Xiao, Chi-Man Pun, Jizhe Zhou", "title": "Generating Adversarial Perturbation with Root Mean Square Gradient", "comments": "The formula in Algorithm 1 lacks important representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus our attention on the problem of generating adversarial perturbations\nbased on the gradient in image classification domain\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 12:42:42 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 01:36:02 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 01:40:02 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 02:08:21 GMT"}, {"version": "v5", "created": "Fri, 17 May 2019 06:39:19 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Xiao", "Yatie", ""], ["Pun", "Chi-Man", ""], ["Zhou", "Jizhe", ""]]}, {"id": "1901.03707", "submitter": "Davis Gilton", "authors": "Davis Gilton, Greg Ongie, Rebecca Willett", "title": "Neumann Networks for Inverse Problems in Imaging", "comments": "Added further experiments, reorganized proof section, added further\n  references and supporting figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many challenging image processing tasks can be described by an ill-posed\nlinear inverse problem: deblurring, deconvolution, inpainting, compressed\nsensing, and superresolution all lie in this framework. Traditional inverse\nproblem solvers minimize a cost function consisting of a data-fit term, which\nmeasures how well an image matches the observations, and a regularizer, which\nreflects prior knowledge and promotes images with desirable properties like\nsmoothness. Recent advances in machine learning and image processing have\nillustrated that it is often possible to learn a regularizer from training data\nthat can outperform more traditional regularizers. We present an end-to-end,\ndata-driven method of solving inverse problems inspired by the Neumann series,\nwhich we call a Neumann network. Rather than unroll an iterative optimization\nalgorithm, we truncate a Neumann series which directly solves the linear\ninverse problem with a data-driven nonlinear regularizer. The Neumann network\narchitecture outperforms traditional inverse problem solution methods,\nmodel-free deep learning approaches, and state-of-the-art unrolled iterative\nmethods on standard datasets. Finally, when the images belong to a union of\nsubspaces and under appropriate assumptions on the forward model, we prove\nthere exists a Neumann network configuration that well-approximates the optimal\noracle estimator for the inverse problem and demonstrate empirically that the\ntrained Neumann network has the form predicted by theory.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 17:44:22 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 00:25:31 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Gilton", "Davis", ""], ["Ongie", "Greg", ""], ["Willett", "Rebecca", ""]]}, {"id": "1901.03728", "submitter": "Valsamis Ntouskos", "authors": "Fiora Pirri, Lorenzo Mauro, Edoardo Alati, Valsamis Ntouskos, Mahdieh\n  Izadpanahkakhk, Elham Omrani", "title": "Anticipation and next action forecasting in video: an end-to-end model\n  with memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action anticipation and forecasting in videos do not require a hat-trick, as\nfar as there are signs in the context to foresee how actions are going to be\ndeployed. Capturing these signs is hard because the context includes the past.\nWe propose an end-to-end network for action anticipation and forecasting with\nmemory, to both anticipate the current action and foresee the next one.\nExperiments on action sequence datasets show excellent results indicating that\ntraining on histories with a dynamic memory can significantly improve\nforecasting performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 19:47:53 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Pirri", "Fiora", ""], ["Mauro", "Lorenzo", ""], ["Alati", "Edoardo", ""], ["Ntouskos", "Valsamis", ""], ["Izadpanahkakhk", "Mahdieh", ""], ["Omrani", "Elham", ""]]}, {"id": "1901.03749", "submitter": "Shilei Fu", "authors": "Shilei Fu, Feng Xu, Ya-Qiu Jin", "title": "Translating SAR to Optical Images for Assisted Interpretation", "comments": "4 pages, 5 figures, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advantages of all-weather and all-day high-resolution imaging,\nSAR remote sensing images are much less viewed and used by general people\nbecause human vision is not adapted to microwave scattering phenomenon.\nHowever, expert interpreters can be trained by compare side-by-side SAR and\noptical images to learn the translation rules from SAR to optical. This paper\nattempts to develop machine intelligence that are trainable with large-volume\nco-registered SAR and optical images to translate SAR image to optical version\nfor assisted SAR interpretation. A novel reciprocal GAN scheme is proposed for\nthis translation task. It is trained and tested on both spaceborne GF-3 and\nairborne UAVSAR images. Comparisons and analyses are presented for datasets of\ndifferent resolutions and polarizations. Results show that the proposed\ntranslation network works well under many scenarios and it could potentially be\nused for assisted SAR interpretation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 08:48:47 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Fu", "Shilei", ""], ["Xu", "Feng", ""], ["Jin", "Ya-Qiu", ""]]}, {"id": "1901.03756", "submitter": "Esube Bekele", "authors": "Esube Bekele and Wallace Lawson", "title": "The Deeper, the Better: Analysis of Person Attributes Recognition", "comments": "8 pages, 34 png figures and 1 pdf figure, uses FG2019.sty, submitted\n  to FG2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person attributes recognition, we describe a person in terms of their\nappearance. Typically, this includes a wide range of traits including age,\ngender, clothing, and footwear. Although this could be used in a wide variety\nof scenarios, it generally is applied to video surveillance, where attribute\nrecognition is impacted by low resolution, and other issues such as variable\npose, occlusion and shadow. Recent approaches have used deep convolutional\nneural networks (CNNs) to improve the accuracy in person attribute recognition.\nHowever, many of these networks are relatively shallow and it is unclear to\nwhat extent they use contextual cues to improve classification accuracy. In\nthis paper, we propose deeper methods for person attribute recognition.\nInterpreting the reasons behind the classification is highly important, as it\ncan provide insight into how the classifier is making decisions. Interpretation\nsuggests that deeper networks generally take more contextual information into\nconsideration, which helps improve classification accuracy and\ngeneralizability. We present experimental analysis and results for whole body\nattributes using the PA-100K and PETA datasets and facial attributes using the\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 21:52:57 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bekele", "Esube", ""], ["Lawson", "Wallace", ""]]}, {"id": "1901.03760", "submitter": "Weizhen Cai", "authors": "Zhewei Wang, Weizhen Cai, Charles D. Smith, Noriko Kantake, Thomas J.\n  Rosol, Jundong Liu", "title": "Residual Pyramid FCN for Robust Follicle Segmentation", "comments": "5 pages; accepted to ISBI'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pyramid network structure to improve the\nFCN-based segmentation solutions and apply it to label thyroid follicles in\nhistology images. Our design is based on the notion that a hierarchical\nupdating scheme, if properly implemented, can help FCNs capture the major\nobjects, as well as structure details in an image. To this end, we devise a\nresidual module to be mounted on consecutive network layers, through which\npixel labels would be propagated from the coarsest layer towards the finest\nlayer in a bottom-up fashion. We add five residual units along the decoding\npath of a modified U-Net to make our segmentation network, Res-Seg-Net.\nExperiments demonstrate that the multi-resolution set-up in our model is\neffective in producing segmentations with improved accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 22:06:10 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Wang", "Zhewei", ""], ["Cai", "Weizhen", ""], ["Smith", "Charles D.", ""], ["Kantake", "Noriko", ""], ["Rosol", "Thomas J.", ""], ["Liu", "Jundong", ""]]}, {"id": "1901.03762", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Anahita Bhiwandiwalla and Alexei Bastidas and\n  Hanlin Tang", "title": "Using Scene Graph Context to Improve Image Generation", "comments": "arXiv admin note: text overlap with arXiv:1804.01622 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic images from scene graphs asks neural networks to be able\nto reason about object relationships and compositionality. As a relatively new\ntask, how to properly ensure the generated images comply with scene graphs or\nhow to measure task performance remains an open question. In this paper, we\npropose to harness scene graph context to improve image generation from scene\ngraphs. We introduce a scene graph context network that pools features\ngenerated by a graph convolutional neural network that are then provided to\nboth the image generation network and the adversarial loss. With the context\nnetwork, our model is trained to not only generate realistic looking images,\nbut also to better preserve non-spatial object relationships. We also define\ntwo novel evaluation metrics, the relation score and the mean opinion relation\nscore, for this task that directly evaluate scene graph compliance. We use both\nquantitative and qualitative studies to demonstrate that our pro-posed model\noutperforms the state-of-the-art on this challenging task.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 22:17:46 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:21:33 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Tripathi", "Subarna", ""], ["Bhiwandiwalla", "Anahita", ""], ["Bastidas", "Alexei", ""], ["Tang", "Hanlin", ""]]}, {"id": "1901.03781", "submitter": "Jun Gao", "authors": "Jun Gao, Chengcheng Tang, Vignesh Ganapathi-Subramanian, Jiahui Huang,\n  Hao Su, Leonidas J. Guibas", "title": "DeepSpline: Data-Driven Reconstruction of Parametric Curves and Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reconstruction of geometry based on different input modes, such as images or\npoint clouds, has been instrumental in the development of computer aided design\nand computer graphics. Optimal implementations of these applications have\ntraditionally involved the use of spline-based representations at their core.\nMost such methods attempt to solve optimization problems that minimize an\noutput-target mismatch. However, these optimization techniques require an\ninitialization that is close enough, as they are local methods by nature. We\npropose a deep learning architecture that adapts to perform spline fitting\ntasks accordingly, providing complementary results to the aforementioned\ntraditional methods. We showcase the performance of our approach, by\nreconstructing spline curves and surfaces based on input images or point\nclouds.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 02:20:09 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Gao", "Jun", ""], ["Tang", "Chengcheng", ""], ["Ganapathi-Subramanian", "Vignesh", ""], ["Huang", "Jiahui", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1901.03784", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin\n  Yumer, Raquel Urtasun", "title": "UPSNet: A Unified Panoptic Segmentation Network", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified panoptic segmentation network (UPSNet)\nfor tackling the newly proposed panoptic segmentation task. On top of a single\nbackbone residual network, we first design a deformable convolution based\nsemantic segmentation head and a Mask R-CNN style instance segmentation head\nwhich solve these two subtasks simultaneously. More importantly, we introduce a\nparameter-free panoptic head which solves the panoptic segmentation via\npixel-wise classification. It first leverages the logits from the previous two\nheads and then innovatively expands the representation for enabling prediction\nof an extra unknown class which helps better resolve the conflicts between\nsemantic and instance segmentation. Additionally, it handles the challenge\ncaused by the varying number of instances and permits back propagation to the\nbottom modules in an end-to-end manner. Extensive experimental results on\nCityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves\nstate-of-the-art performance with much faster inference. Code has been made\navailable at: https://github.com/uber-research/UPSNet\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 02:39:03 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 22:49:57 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Xiong", "Yuwen", ""], ["Liao", "Renjie", ""], ["Zhao", "Hengshuang", ""], ["Hu", "Rui", ""], ["Bai", "Min", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1901.03786", "submitter": "Bas Peters", "authors": "Bas Peters, Justin Granek, Eldad Haber", "title": "Automatic classification of geologic units in seismic images using\n  partially interpreted examples", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geologic interpretation of large seismic stacked or migrated seismic images\ncan be a time-consuming task for seismic interpreters. Neural network based\nsemantic segmentation provides fast and automatic interpretations, provided a\nsufficient number of example interpretations are available. Networks that map\nfrom image-to-image emerged recently as powerful tools for automatic\nsegmentation, but standard implementations require fully interpreted examples.\nGenerating training labels for large images manually is time consuming. We\nintroduce a partial loss-function and labeling strategies such that networks\ncan learn from partially interpreted seismic images. This strategy requires\nonly a small number of annotated pixels per seismic image. Tests on seismic\nimages and interpretation information from the Sea of Ireland show that we\nobtain high-quality predicted interpretations from a small number of large\nseismic images. The combination of a partial-loss function, a multi-resolution\nnetwork that explicitly takes small and large-scale geological features into\naccount, and new labeling strategies make neural networks a more practical tool\nfor automatic seismic interpretation.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 02:45:32 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Peters", "Bas", ""], ["Granek", "Justin", ""], ["Haber", "Eldad", ""]]}, {"id": "1901.03796", "submitter": "Yu Liu", "authors": "Yu Liu, Lingqiao Liu, Hamid Rezatofighi, Thanh-Toan Do, Qinfeng Shi\n  and Ian Reid", "title": "Learning Pairwise Relationship for Multi-object Detection in Crowded\n  Scenes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the post-processing step for object detection, non-maximum suppression\n(GreedyNMS) is widely used in most of the detectors for many years. It is\nefficient and accurate for sparse scenes, but suffers an inevitable trade-off\nbetween precision and recall in crowded scenes. To overcome this drawback, we\npropose a Pairwise-NMS to cure GreedyNMS. Specifically, a pairwise-relationship\nnetwork that is based on deep learning is learned to predict if two overlapping\nproposal boxes contain two objects or zero/one object, which can handle\nmultiple overlapping objects effectively. Through neatly coupling with\nGreedyNMS without losing efficiency, consistent improvements have been achieved\nin heavily occluded datasets including MOT15, TUD-Crossing and PETS. In\naddition, Pairwise-NMS can be integrated into any learning based detectors\n(Both of Faster-RCNN and DPM detectors are tested in this paper), thus building\na bridge between GreedyNMS and end-to-end learning detectors.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 04:43:17 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Liu", "Yu", ""], ["Liu", "Lingqiao", ""], ["Rezatofighi", "Hamid", ""], ["Do", "Thanh-Toan", ""], ["Shi", "Qinfeng", ""], ["Reid", "Ian", ""]]}, {"id": "1901.03798", "submitter": "Liang Lin", "authors": "Keze Wang and Liang Lin and Chenhan Jiang and Chen Qian and Pengxu Wei", "title": "3D Human Pose Machines with Self-supervised Learning", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI), 2019. Our simple yet effective self-supervised\n  correction mechanism to incorporate 3D pose geometric structural information\n  is innovative in the literature, and may also inspire other 3D vision tasks.\n  Please find the code of this project at: http://www.sysu-hcp.net/3d_pose_ssl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by recent computer vision and robotic applications, recovering 3D\nhuman poses has become increasingly important and attracted growing interests.\nIn fact, completing this task is quite challenging due to the diverse\nappearances, viewpoints, occlusions and inherently geometric ambiguities inside\nmonocular images. Most of the existing methods focus on designing some\nelaborate priors /constraints to directly regress 3D human poses based on the\ncorresponding 2D human pose-aware features or 2D pose predictions. However, due\nto the insufficient 3D pose data for training and the domain gap between 2D\nspace and 3D space, these methods have limited scalabilities for all practical\nscenarios (e.g., outdoor scene). Attempt to address this issue, this paper\nproposes a simple yet effective self-supervised correction mechanism to learn\nall intrinsic structures of human poses from abundant images. Specifically, the\nproposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose\ntransformation and 3D-to-2D pose projection, to serve as a bridge between 3D\nand 2D human poses in a type of \"free\" self-supervision for accurate 3D human\npose estimation. The 2D-to-3D pose implies to sequentially regress intermediate\n3D poses by transforming the pose representation from the 2D domain to the 3D\ndomain under the sequence-dependent temporal context, while the 3D-to-2D pose\nprojection contributes to refining the intermediate 3D poses by maintaining\ngeometric consistency between the 2D projections of 3D poses and the estimated\n2D poses. We further apply our self-supervised correction mechanism to develop\na 3D human pose machine, which jointly integrates the 2D spatial relationship,\ntemporal smoothness of predictions and 3D geometric knowledge. Extensive\nevaluations demonstrate the superior performance and efficiency of our\nframework over all the compared competing methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 04:55:26 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 01:21:59 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Wang", "Keze", ""], ["Lin", "Liang", ""], ["Jiang", "Chenhan", ""], ["Qian", "Chen", ""], ["Wei", "Pengxu", ""]]}, {"id": "1901.03814", "submitter": "Xi Chen", "authors": "Xi Chen, Donglian Qi, and Jianxin Shen", "title": "Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with other semantic segmentation tasks, portrait segmentation\nrequires both higher precision and faster inference speed. However, this\nproblem has not been well studied in previous works. In this paper, we propose\na lightweight network architecture, called Boundary-Aware Network (BANet) which\nselectively extracts detail information in boundary area to make high-quality\nsegmentation output with real-time( >25FPS) speed. In addition, we design a new\nloss function called refine loss which supervises the network with image level\ngradient information. Our model is able to produce finer segmentation results\nwhich has richer details than annotations.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 07:19:27 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Chen", "Xi", ""], ["Qi", "Donglian", ""], ["Shen", "Jianxin", ""]]}, {"id": "1901.03842", "submitter": "Kumar Abhishek", "authors": "Kumar Abhishek, Ashok Yogi", "title": "Summarization and Visualization of Large Volumes of Broadcast Video Data", "comments": "Undergraduate Thesis from April 2015, 55 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, there has been an astounding growth in the number of\nnews channels as well as the amount of broadcast news video data. As a result,\nit is imperative that automated methods need to be developed in order to\neffectively summarize and store this voluminous data. Format detection of news\nvideos plays an important role in news video analysis. Our problem involves\nbuilding a robust and versatile news format detector, which identifies the\ndifferent band elements in a news frame. Probabilistic progressive Hough\ntransform has been used for the detection of band edges. The detected bands are\nclassified as natural images, computer generated graphics (non-text) and text\nbands. A contrast based text detector has been used to identify the text\nregions from news frames. Two classifers have been trained and evaluated for\nthe labeling of the detected bands as natural or artificial - Support Vector\nMachine (SVM) Classifer with RBF kernel, and Extreme Learning Machine (ELM)\nclassifier. The classifiers have been trained on a dataset of 6000 images (3000\nimages of each class). The ELM classifier reports a balanced accuracy of\n77.38%, while the SVM classifier outperforms it with a balanced accuracy of\n96.5% using 10-fold cross-validation. The detected bands which have been\nfragmented due to the presence of gradients in the image have been merged using\na three-tier hierarchical reasoning model. The bands were detected with a\nJaccard Index of 0.8138, when compared to manually marked ground truth data. We\nhave also presented an extensive literature review of previous work done\ntowards news videos format detection, element band classification, and\nassociative reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 10:43:48 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Abhishek", "Kumar", ""], ["Yogi", "Ashok", ""]]}, {"id": "1901.03852", "submitter": "Mikhail Mozerov Dr", "authors": "Mikhail G. Mozerov, and Joost van de Weijer", "title": "One-view occlusion detection for stereo matching with a fully connected\n  CRF model", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2892668", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the standard belief propagation (BP) sequential\ntechnique proposed in the tree-reweighted sequential method to the fully\nconnected CRF models with the geodesic distance affinity. The proposed method\nhas been applied to the stereo matching problem. Also a new approach to the BP\nmarginal solution is proposed that we call one-view occlusion detection (OVOD).\nIn contrast to the standard winner takes all (WTA) estimation, the proposed\nOVOD solution allows to find occluded regions in the disparity map and\nsimultaneously improve the matching result. As a result we can perform only one\nenergy minimization process and avoid the cost calculation for the second view\nand the left-right check procedure. We show that the OVOD approach considerably\nimproves results for cost augmentation and energy minimization techniques in\ncomparison with the standard one-view affinity space implementation. We apply\nour method to the Middlebury data set and reach state-of-the-art especially for\nmedian, average and mean squared error metrics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 11:24:36 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Mozerov", "Mikhail G.", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1901.03857", "submitter": "Kei Sakamoto", "authors": "Kei Sakamoto, Kei-ichi Morita, Tohru Ikeda, Kou Kayamori", "title": "Deep-learning-based identification of odontogenic keratocysts in\n  hematoxylin- and eosin-stained jaw cyst specimens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study was to develop a digital histopathology system for\nidentifying odontogenic keratocysts in hematoxylin- and eosin-stained tissue\nspecimens of jaw cysts. Approximately 5000 microscopy images with 400$\\times$\nmagnification were obtained from 199 odontogenic keratocysts, 208 dentigerous\ncysts, and 55 radicular cysts. A proportion of these images were used to make\ntraining patches, which were annotated as belonging to one of the following\nthree classes: keratocysts, non-keratocysts, and stroma. The patches for the\ncysts contained the complete lining epithelium, with the cyst cavity being\npresent on the upper side. The convolutional neural network (CNN) VGG16 was\nfinetuned to this dataset. The trained CNN could recognize the basal cell\npalisading pattern, which is the definitive criterion for diagnosing\nkeratocysts. Some of the remaining images were scanned and analyzed by the\ntrained CNN, whose output was then used to train another CNN for binary\nclassification (keratocyst or not). The area under the receiver operating\ncharacteristics curve for the entire algorithm was 0.997 for the test dataset.\nThus, the proposed patch classification strategy is usable for automated\nkeratocyst diagnosis. However, further optimization must be performed to make\nit suitable for practical use.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 12:10:52 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sakamoto", "Kei", ""], ["Morita", "Kei-ichi", ""], ["Ikeda", "Tohru", ""], ["Kayamori", "Kou", ""]]}, {"id": "1901.03861", "submitter": "Cheng Sun", "authors": "Cheng Sun, Chi-Wei Hsiao, Min Sun, Hwann-Tzong Chen", "title": "HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch\n  Data Augmentation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the problem of estimating the 3D room layout\nfrom a single panoramic image. We represent room layout as three 1D vectors\nthat encode, at each image column, the boundary positions of floor-wall and\nceiling-wall, and the existence of wall-wall boundary. The proposed network,\nHorizonNet, trained for predicting 1D layout, outperforms previous\nstate-of-the-art approaches. The designed post-processing procedure for\nrecovering 3D room layouts from 1D predictions can automatically infer the room\nshape with low computation cost - it takes less than 20ms for a panorama image\nwhile prior works might need dozens of seconds. We also propose Pano Stretch\nData Augmentation, which can diversify panorama data and be applied to other\npanorama-related learning tasks. Due to the limited data available for\nnon-cuboid layout, we relabel 65 general layout from the current dataset for\nfinetuning. Our approach shows good performance on general layouts by\nqualitative results and cross-validation.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 13:57:29 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 07:16:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sun", "Cheng", ""], ["Hsiao", "Chi-Wei", ""], ["Sun", "Min", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1901.03892", "submitter": "Kevin Zhang", "authors": "Kevin Alex Zhang, Alfredo Cuesta-Infante, Lei Xu, Kalyan\n  Veeramachaneni", "title": "SteganoGAN: High Capacity Image Steganography with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image steganography is a procedure for hiding messages inside pictures. While\nother techniques such as cryptography aim to prevent adversaries from reading\nthe secret message, steganography aims to hide the presence of the message\nitself. In this paper, we propose a novel technique for hiding arbitrary binary\ndata in images using generative adversarial networks which allow us to optimize\nthe perceptual quality of the images produced by our model. We show that our\napproach achieves state-of-the-art payloads of 4.4 bits per pixel, evades\ndetection by steganalysis tools, and is effective on images from multiple\ndatasets. To enable fair comparisons, we have released an open source library\nthat is available online at https://github.com/DAI-Lab/SteganoGAN.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 18:47:38 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:39:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Kevin Alex", ""], ["Cuesta-Infante", "Alfredo", ""], ["Xu", "Lei", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1901.03910", "submitter": "Matthias Innmann", "authors": "Matthias Innmann, Kihwan Kim, Jinwei Gu, Matthias Niessner, Charles\n  Loop, Marc Stamminger, Jan Kautz", "title": "NRMVS: Non-Rigid Multi-View Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene reconstruction from unorganized RGB images is an important task in many\ncomputer vision applications. Multi-view Stereo (MVS) is a common solution in\nphotogrammetry applications for the dense reconstruction of a static scene. The\nstatic scene assumption, however, limits the general applicability of MVS\nalgorithms, as many day-to-day scenes undergo non-rigid motion, e.g., clothes,\nfaces, or human bodies. In this paper, we open up a new challenging direction:\ndense 3D reconstruction of scenes with non-rigid changes observed from\narbitrary, sparse, and wide-baseline views. We formulate the problem as a joint\noptimization of deformation and depth estimation, using deformation graphs as\nthe underlying representation. We propose a new sparse 3D to 2D matching\ntechnique, together with a dense patch-match evaluation scheme to estimate\ndeformation and depth with photometric consistency. We show that creating a\ndense 4D structure from a few RGB images with non-rigid changes is possible,\nand demonstrate that our method can be used to interpolate novel deformed\nscenes from various combinations of these deformation estimates derived from\nthe sparse views.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 22:19:26 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Innmann", "Matthias", ""], ["Kim", "Kihwan", ""], ["Gu", "Jinwei", ""], ["Niessner", "Matthias", ""], ["Loop", "Charles", ""], ["Stamminger", "Marc", ""], ["Kautz", "Jan", ""]]}, {"id": "1901.03912", "submitter": "Senthil Yogamani", "authors": "Ganesh Sistu, Isabelle Leang and Senthil Yogamani", "title": "Real-time Joint Object Detection and Semantic Segmentation Network for\n  Automated Driving", "comments": "Presented at NeurIPS 2018 Workshop on Machine Learning on the Phone\n  and other Consumer Devices (MLPCD 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) are successfully used for various visual\nperception tasks including bounding box object detection, semantic\nsegmentation, optical flow, depth estimation and visual SLAM. Generally these\ntasks are independently explored and modeled. In this paper, we present a joint\nmulti-task network design for learning object detection and semantic\nsegmentation simultaneously. The main motivation is to achieve real-time\nperformance on a low power embedded SOC by sharing of encoder for both the\ntasks. We construct an efficient architecture using a small ResNet10 like\nencoder which is shared for both decoders. Object detection uses YOLO v2 like\ndecoder and semantic segmentation uses FCN8 like decoder. We evaluate the\nproposed network in two public datasets (KITTI, Cityscapes) and in our private\nfisheye camera dataset, and demonstrate that joint network provides the same\naccuracy as that of separate networks. We further optimize the network to\nachieve 30 fps for 1280x384 resolution image.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 22:25:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sistu", "Ganesh", ""], ["Leang", "Isabelle", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1901.03915", "submitter": "Sebastian Penhou\\\"et", "authors": "Sebastian Penhou\\\"et and Paul Sanzenbacher", "title": "Automated Deep Photo Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealism is a complex concept that cannot easily be formulated\nmathematically. Deep Photo Style Transfer is an attempt to transfer the style\nof a reference image to a content image while preserving its photorealism. This\nis achieved by introducing a constraint that prevents distortions in the\ncontent image and by applying the style transfer independently for semantically\ndifferent parts of the images. In addition, an automated segmentation process\nis presented that consists of a neural network based segmentation method\nfollowed by a semantic grouping step. To further improve the results a measure\nfor image aesthetics is used and elaborated. If the content and the style image\nare sufficiently similar, the result images look very realistic. With the\nautomation of the image segmentation the pipeline becomes completely\nindependent from any user interaction, which allows for new applications.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 23:41:28 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Penhou\u00ebt", "Sebastian", ""], ["Sanzenbacher", "Paul", ""]]}, {"id": "1901.03916", "submitter": "Donald Dansereau", "authors": "Donald G. Dansereau, Bernd Girod, and Gordon Wetzstein", "title": "LiFF: Light Field Features in Scale and Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature detectors and descriptors are key low-level vision tools that many\nhigher-level tasks build on. Unfortunately these fail in the presence of\nchallenging light transport effects including partial occlusion, low contrast,\nand reflective or refractive surfaces. Building on spatio-angular imaging\nmodalities offered by emerging light field cameras, we introduce a new and\ncomputationally efficient 4D light field feature detector and descriptor: LiFF.\nLiFF is scale invariant and utilizes the full 4D light field to detect features\nthat are robust to changes in perspective. This is particularly useful for\nstructure from motion (SfM) and other tasks that match features across\nviewpoints of a scene. We demonstrate significantly improved 3D reconstructions\nvia SfM when using LiFF instead of the leading 2D or 4D features, and show that\nLiFF runs an order of magnitude faster than the leading 4D approach. Finally,\nLiFF inherently estimates depth for each feature, opening a path for future\nresearch in light field-based SfM.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 00:03:56 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Dansereau", "Donald G.", ""], ["Girod", "Bernd", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1901.03918", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma and Anil K. Jain", "title": "Generalizing Fingerprint Spoof Detector: Learning a One-Class Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevailing fingerprint recognition systems are vulnerable to spoof attacks.\nTo mitigate these attacks, automated spoof detectors are trained to distinguish\na set of live or bona fide fingerprints from a set of known spoof fingerprints.\nDespite their success, spoof detectors remain vulnerable when exposed to\nattacks from spoofs made with materials not seen during training of the\ndetector. To alleviate this shortcoming, we approach spoof detection as a\none-class classification problem. The goal is to train a spoof detector on only\nthe live fingerprints such that once the concept of \"live\" has been learned,\nspoofs of any material can be rejected. We accomplish this through training\nmultiple generative adversarial networks (GANS) on live fingerprint images\nacquired with the open source, dual-camera, 1900 ppi RaspiReader fingerprint\nreader. Our experimental results, conducted on 5.5K spoof images (from 12\nmaterials) and 11.8K live images show that the proposed approach improves the\ncross-material spoof detection performance over state-of-the-art one-class and\nbinary class spoof detectors on 11 of 12 testing materials and 7 of 12 testing\nmaterials, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 00:29:06 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 19:30:03 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Jain", "Anil K.", ""]]}, {"id": "1901.03924", "submitter": "Zhihao Cao", "authors": "Zhihao Cao, Shaomin Mu, Yongyu Xu, Mengping Dong", "title": "Image retrieval method based on CNN and dimension reduction", "comments": "2018 International Conference on Security, Pattern Analysis, and\n  Cybernetics(SPAC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image retrieval method based on convolution neural network and dimension\nreduction is proposed in this paper. Convolution neural network is used to\nextract high-level features of images, and to solve the problem that the\nextracted feature dimensions are too high and have strong correlation,\nmultilinear principal component analysis is used to reduce the dimension of\nfeatures. The features after dimension reduction are binary hash coded for fast\nimage retrieval. Experiments show that the method proposed in this paper has\nbetter retrieval effect than the retrieval method based on principal component\nanalysis on the e-commerce image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 02:29:27 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Cao", "Zhihao", ""], ["Mu", "Shaomin", ""], ["Xu", "Yongyu", ""], ["Dong", "Mengping", ""]]}, {"id": "1901.03953", "submitter": "Manikanta Kotaru", "authors": "Manikanta Kotaru, Guy Satat, Ramesh Raskar and Sachin Katti", "title": "Light-Field for RF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computer vision systems and computational photography systems are\nvisible light based which is a small fraction of the electromagnetic (EM)\nspectrum. In recent years radio frequency (RF) hardware has become more widely\navailable, for example, many cars are equipped with a RADAR, and almost every\nhome has a WiFi device. In the context of imaging, RF spectrum holds many\nadvantages compared to visible light systems. In particular, in this regime, EM\nenergy effectively interacts in different ways with matter. This property\nallows for many novel applications such as privacy preserving computer vision\nand imaging through absorbing and scattering materials in visible light such as\nwalls. Here, we expand many of the concepts in computational photography in\nvisible light to RF cameras. The main limitation of imaging with RF is the\nlarge wavelength that limits the imaging resolution when compared to visible\nlight. However, the output of RF cameras is usually processed by computer\nvision and perception algorithms which would benefit from multi-modal sensing\nof the environment, and from sensing in situations in which visible light\nsystems fail. To bridge the gap between computational photography and RF\nimaging, we expand the concept of light-field to RF. This work paves the way to\nnovel computational sensing systems with RF.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 09:28:54 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kotaru", "Manikanta", ""], ["Satat", "Guy", ""], ["Raskar", "Ramesh", ""], ["Katti", "Sachin", ""]]}, {"id": "1901.03954", "submitter": "Yunxuan Xiao", "authors": "Yunxuan Xiao, Yikai Li, Yuwei Wu, Lizhen Zhu", "title": "Auto-Retoucher(ART) - A framework for Background Replacement and Image\n  Editing", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replacing the background and simultaneously adjusting foreground objects is a\nchallenging task in image editing. Current techniques for generating such\nimages relies heavily on user interactions with image editing softwares, which\nis a tedious job for professional retouchers. To reduce their workload, some\nexciting progress has been made on generating images with a given background.\nHowever, these models can neither adjust the position and scale of the\nforeground objects, nor guarantee the semantic consistency between foreground\nand background. To overcome these limitations, we propose a framework --\nART(Auto-Retoucher), to generate images with sufficient semantic and spatial\nconsistency. Images are first processed by semantic matting and scene parsing\nmodules, then a multi-task verifier model will give two confidence scores for\nthe current background and position setting. We demonstrate that our jointly\noptimized verifier model successfully improves the visual consistency, and our\nART framework performs well on images with the human body as foregrounds.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 09:51:08 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Xiao", "Yunxuan", ""], ["Li", "Yikai", ""], ["Wu", "Yuwei", ""], ["Zhu", "Lizhen", ""]]}, {"id": "1901.03960", "submitter": "Pai Liu", "authors": "Jingwei Gan, Pai Liu, Rajan K. Chakrabarty", "title": "Introducing a Generative Adversarial Network Model for Lagrangian\n  Trajectory Simulation", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generative adversarial network (GAN) model to simulate the\n3-dimensional Lagrangian motion of particles trapped in the recirculation zone\nof a buoyancy-opposed flame. The GAN model comprises a stochastic recurrent\nneural network, serving as a generator, and a convoluted neural network,\nserving as a discriminator. Adversarial training was performed to the point\nwhere the best-trained discriminator failed to distinguish the ground truth\nfrom the trajectory produced by the best-trained generator. The model\nperformance was then benchmarked against a statistical analysis performed on\nboth the simulated trajectories and the ground truth, with regard to the\naccuracy and generalization criteria.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 11:06:34 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Gan", "Jingwei", ""], ["Liu", "Pai", ""], ["Chakrabarty", "Rajan K.", ""]]}, {"id": "1901.03991", "submitter": "Gaurav Chaurasia", "authors": "Andrin Jenal, Nikolay Savinov, Torsten Sattler, Gaurav Chaurasia", "title": "RNN-based Generative Model for Fine-Grained Sketching", "comments": "Includes supplemental material. Link to datasets to be added shortly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have shown great promise when it comes to synthesising\nnovel images. While they can generate images that look convincing on a\nhigher-level, generating fine-grained details is still a challenge. In order to\nfoster research on more powerful generative approaches, this paper proposes a\nnovel task: generative modelling of 2D tree skeletons. Trees are an interesting\nshape class because they exhibit complexity and variations that are well-suited\nto measure the ability of a generative model to generated detailed structures.\nWe propose a new dataset for this task and demonstrate that state-of-the-art\ngenerative models fail to synthesise realistic images on our benchmark, even\nthough they perform well on current datasets like MNIST digits. Motivated by\nthese results, we propose a novel network architecture based on combining a\nvariational autoencoder using Recurrent Neural Networks and a convolutional\ndiscriminator. The network, error metrics and training procedure are adapted to\nthe task of fine-grained sketching. Through quantitative and perceptual\nexperiments, we show that our model outperforms previous work and that our\ndataset is a valuable benchmark for generative models. We will make our dataset\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 14:23:44 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Jenal", "Andrin", ""], ["Savinov", "Nikolay", ""], ["Sattler", "Torsten", ""], ["Chaurasia", "Gaurav", ""]]}, {"id": "1901.04017", "submitter": "Anna Kuznetsova", "authors": "Yuri Monakhov, Oleg Nikitin, Anna Kuznetsova, Alexey Kharlamov,\n  Alexandr Amochkin", "title": "A Machine-Synesthetic Approach To DDoS Network Attack Detection", "comments": "12 pages, 2 figures, 5 tables. Accepted to the Intelligent Systems\n  Conference (IntelliSys) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the authors' opinion, anomaly detection systems, or ADS, seem to be the\nmost perspective direction in the subject of attack detection, because these\nsystems can detect, among others, the unknown (zero-day) attacks. To detect\nanomalies, the authors propose to use machine synesthesia. In this case,\nmachine synesthesia is understood as an interface that allows using image\nclassification algorithms in the problem of detecting network anomalies, making\nit possible to use non-specialized image detection methods that have recently\nbeen widely and actively developed. The proposed approach is that the network\ntraffic data is \"projected\" into the image. It can be seen from the\nexperimental results that the proposed method for detecting anomalies shows\nhigh results in the detection of attacks. On a large sample, the value of the\ncomplex efficiency indicator reaches 97%.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 17:01:46 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 06:38:48 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Monakhov", "Yuri", ""], ["Nikitin", "Oleg", ""], ["Kuznetsova", "Anna", ""], ["Kharlamov", "Alexey", ""], ["Amochkin", "Alexandr", ""]]}, {"id": "1901.04056", "submitter": "Patrick Bilic", "authors": "Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz\n  Chlebus, Hao Chen, Qi Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, J\\\"urgen\n  Hesser, Samuel Kadoury, Tomasz Konopczy\\`nski, Miao Le, Chunming Li, Xiaomeng\n  Li, Jana Lipkov\\`a, John Lowengrub, Hans Meine, Jan Hendrik Moltz, Chris Pal,\n  Marie Piraud, Xiaojuan Qi, Jin Qi, Markus Rempfler, Karsten Roth, Andrea\n  Schenk, Anjany Sekuboyina, Eugene Vorontsov, Ping Zhou, Christian\n  H\\\"ulsemeyer, Marcel Beetz, Florian Ettlinger, Felix Gruen, Georgios Kaissis,\n  Fabian Loh\\\"ofer, Rickmer Braren, Julian Holch, Felix Hofmann, Wieland\n  Sommer, Volker Heinemann, Colin Jacobs, Gabriel Efrain Humpire Mamani, Bram\n  van Ginneken, Gabriel Chartrand, An Tang, Michal Drozdzal, Avi Ben-Cohen,\n  Eyal Klang, Marianne M. Amitai, Eli Konen, Hayit Greenspan, Johan Moreau,\n  Alexandre Hostettler, Luc Soler, Refael Vivanti, Adi Szeskin, Naama\n  Lev-Cohain, Jacob Sosna, Leo Joskowicz, Bjoern H. Menze", "title": "The Liver Tumor Segmentation Benchmark (LiTS)", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we report the set-up and results of the Liver Tumor\nSegmentation Benchmark (LITS) organized in conjunction with the IEEE\nInternational Symposium on Biomedical Imaging (ISBI) 2016 and International\nConference On Medical Image Computing Computer Assisted Intervention (MICCAI)\n2017. Twenty four valid state-of-the-art liver and liver tumor segmentation\nalgorithms were applied to a set of 131 computed tomography (CT) volumes with\ndifferent types of tumor contrast levels (hyper-/hypo-intense), abnormalities\nin tissues (metastasectomie) size and varying amount of lesions. The submitted\nalgorithms have been tested on 70 undisclosed volumes. The dataset is created\nin collaboration with seven hospitals and research institutions and manually\nreviewed by independent three radiologists. We found that not a single\nalgorithm performed best for liver and tumors. The best liver segmentation\nalgorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation\nthe best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image\ndata and manual annotations continue to be publicly available through an online\nevaluation system as an ongoing benchmarking resource.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 20:38:16 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bilic", "Patrick", ""], ["Christ", "Patrick Ferdinand", ""], ["Vorontsov", "Eugene", ""], ["Chlebus", "Grzegorz", ""], ["Chen", "Hao", ""], ["Dou", "Qi", ""], ["Fu", "Chi-Wing", ""], ["Han", "Xiao", ""], ["Heng", "Pheng-Ann", ""], ["Hesser", "J\u00fcrgen", ""], ["Kadoury", "Samuel", ""], ["Konopczynski", "Tomasz", ""], ["Le", "Miao", ""], ["Li", "Chunming", ""], ["Li", "Xiaomeng", ""], ["Lipkov\u00e0", "Jana", ""], ["Lowengrub", "John", ""], ["Meine", "Hans", ""], ["Moltz", "Jan Hendrik", ""], ["Pal", "Chris", ""], ["Piraud", "Marie", ""], ["Qi", "Xiaojuan", ""], ["Qi", "Jin", ""], ["Rempfler", "Markus", ""], ["Roth", "Karsten", ""], ["Schenk", "Andrea", ""], ["Sekuboyina", "Anjany", ""], ["Vorontsov", "Eugene", ""], ["Zhou", "Ping", ""], ["H\u00fclsemeyer", "Christian", ""], ["Beetz", "Marcel", ""], ["Ettlinger", "Florian", ""], ["Gruen", "Felix", ""], ["Kaissis", "Georgios", ""], ["Loh\u00f6fer", "Fabian", ""], ["Braren", "Rickmer", ""], ["Holch", "Julian", ""], ["Hofmann", "Felix", ""], ["Sommer", "Wieland", ""], ["Heinemann", "Volker", ""], ["Jacobs", "Colin", ""], ["Mamani", "Gabriel Efrain Humpire", ""], ["van Ginneken", "Bram", ""], ["Chartrand", "Gabriel", ""], ["Tang", "An", ""], ["Drozdzal", "Michal", ""], ["Ben-Cohen", "Avi", ""], ["Klang", "Eyal", ""], ["Amitai", "Marianne M.", ""], ["Konen", "Eli", ""], ["Greenspan", "Hayit", ""], ["Moreau", "Johan", ""], ["Hostettler", "Alexandre", ""], ["Soler", "Luc", ""], ["Vivanti", "Refael", ""], ["Szeskin", "Adi", ""], ["Lev-Cohain", "Naama", ""], ["Sosna", "Jacob", ""], ["Joskowicz", "Leo", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1901.04059", "submitter": "Zhaoyang Xu", "authors": "Zhaoyang Xu, Carlos Fern\\'andez Moro, B\\'ela Boz\\'oky, Qianni Zhang", "title": "GAN-based Virtual Re-Staining: A Promising Solution for Whole Slide\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological cancer diagnosis is based on visual examination of stained\ntissue slides. Hematoxylin and eosin (H\\&E) is a standard stain routinely\nemployed worldwide. It is easy to acquire and cost effective, but cells and\ntissue components show low-contrast with varying tones of dark blue and pink,\nwhich makes difficult visual assessments, digital image analysis, and\nquantifications. These limitations can be overcome by IHC staining of target\nproteins of the tissue slide. IHC provides a selective, high-contrast imaging\nof cells and tissue components, but their use is largely limited by a\nsignificantly more complex laboratory processing and high cost. We proposed a\nconditional CycleGAN (cCGAN) network to transform the H\\&E stained images into\nIHC stained images, facilitating virtual IHC staining on the same slide. This\ndata-driven method requires only a limited amount of labelled data but will\ngenerate pixel level segmentation results. The proposed cCGAN model improves\nthe original network \\cite{zhu_unpaired_2017} by adding category conditions and\nintroducing two structural loss functions, which realize a multi-subdomain\ntranslation and improve the translation accuracy as well. % need to give\nreasons here. Experiments demonstrate that the proposed model outperforms the\noriginal method in unpaired image translation with multi-subdomains. We also\nexplore the potential of unpaired images to image translation method applied on\nother histology images related tasks with different staining techniques.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 20:54:39 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Xu", "Zhaoyang", ""], ["Moro", "Carlos Fern\u00e1ndez", ""], ["Boz\u00f3ky", "B\u00e9la", ""], ["Zhang", "Qianni", ""]]}, {"id": "1901.04077", "submitter": "Mohamed Shehata", "authors": "Mohamed Shehata, Reda Abo-Al-Ez, Farid Zaghlool and Mohamed Taha\n  Abou-Kreisha", "title": "Vehicles Detection Based on Background Modeling", "comments": "4 pages, 4 figures", "journal-ref": "International Journal of Engineering Trends and Technology 66.2\n  (2018): 92-95", "doi": "10.14445/22315381/IJETT-V66P216", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background image subtraction algorithm is a common approach which detects\nmoving objects in a video sequence by finding the significant difference\nbetween the video frames and the static background model. This paper presents a\ndeveloped system which achieves vehicle detection by using background image\nsubtraction algorithm based on blocks followed by deep learning data validation\nalgorithm. The main idea is to segment the image into equal size blocks, to\nmodel the static reference background image (SRBI), by calculating the variance\nbetween each block pixels and each counterpart block pixels in the adjacent\nframe, the system implemented into four different methods: Absolute Difference,\nImage Entropy, Exclusive OR (XOR) and Discrete Cosine Transform (DCT). The\nexperimental results showed that the DCT method has the highest vehicle\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 22:41:18 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shehata", "Mohamed", ""], ["Abo-Al-Ez", "Reda", ""], ["Zaghlool", "Farid", ""], ["Abou-Kreisha", "Mohamed Taha", ""]]}, {"id": "1901.04111", "submitter": "Junting Dong", "authors": "Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, Xiaowei Zhou", "title": "Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views", "comments": "Project page: https://zju-3dv.github.io/mvpose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D pose estimation for multiple people in\na few calibrated camera views. The main challenge of this problem is to find\nthe cross-view correspondences among noisy and incomplete 2D pose predictions.\nMost previous methods address this challenge by directly reasoning in 3D using\na pictorial structure model, which is inefficient due to the huge state space.\nWe propose a fast and robust approach to solve this problem. Our key idea is to\nuse a multi-way matching algorithm to cluster the detected 2D poses in all\nviews. Each resulting cluster encodes 2D poses of the same person across\ndifferent views and consistent correspondences across the keypoints, from which\nthe 3D pose of each person can be effectively inferred. The proposed convex\noptimization based multi-way matching algorithm is efficient and robust against\nmissing and false detections, without knowing the number of people in the\nscene. Moreover, we propose to combine geometric and appearance cues for\ncross-view matching. The proposed approach achieves significant performance\ngains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the\nCampus and Shelf datasets, respectively), while being efficient for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 03:27:05 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Dong", "Junting", ""], ["Jiang", "Wen", ""], ["Huang", "Qixing", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "1901.04140", "submitter": "Zihan Zhou", "authors": "Xuehui Sun, Zihan Zhou, Yuda Fan", "title": "Image Based Review Text Generation with Emotional Guidance", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current field of computer vision, automatically generating texts from\ngiven images has been a fully worked technique. Up till now, most works of this\narea focus on image content describing, namely image-captioning. However, rare\nresearches focus on generating product review texts, which is ubiquitous in the\nonline shopping malls and is crucial for online shopping selection and\nevaluation. Different from content describing, review texts include more\nsubjective information of customers, which may bring difference to the results.\nTherefore, we aimed at a new field concerning generating review text from\ncustomers based on images together with the ratings of online shopping\nproducts, which appear as non-image attributes. We made several adjustments to\nthe existing image-captioning model to fit our task, in which we should also\ntake non-image features into consideration. We also did experiments based on\nour model and get effective primary results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 05:42:51 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sun", "Xuehui", ""], ["Zhou", "Zihan", ""], ["Fan", "Yuda", ""]]}, {"id": "1901.04206", "submitter": "Y.C. Su", "authors": "Yanchi Su and Zhanshan Li and Haihong Yu and Zeyu Wang", "title": "Multi-band Weighted $l_p$ Norm Minimization for Image Denoising", "comments": "accepted by Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2020.05.049", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank matrix approximation (LRMA) has drawn increasing attention in recent\nyears, due to its wide range of applications in computer vision and machine\nlearning. However, LRMA, achieved by nuclear norm minimization (NNM), tends to\nover-shrink the rank components with the same threshold and ignore the\ndifferences between rank components. To address this problem, we propose a\nflexible and precise model named multi-band weighted $l_p$ norm minimization\n(MBWPNM). The proposed MBWPNM not only gives more accurate approximation with a\nSchatten $p$-norm, but also considers the prior knowledge where different rank\ncomponents have different importance. We analyze the solution of MBWPNM and\nprove that MBWPNM is equivalent to a non-convex $l_p$ norm subproblems under\ncertain weight condition, whose global optimum can be solved by a generalized\nsoft-thresholding algorithm. We then adopt the MBWPNM algorithm to color and\nmultispectral image denoising. Extensive experiments on additive white Gaussian\nnoise removal and realistic noise removal demonstrate that the proposed MBWPNM\nachieves a better performance than several state-of-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 09:32:09 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 10:12:05 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 13:11:01 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 02:36:51 GMT"}, {"version": "v5", "created": "Tue, 23 Jun 2020 05:16:00 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Su", "Yanchi", ""], ["Li", "Zhanshan", ""], ["Yu", "Haihong", ""], ["Wang", "Zeyu", ""]]}, {"id": "1901.04210", "submitter": "Brojeshwar Bhowmick", "authors": "Soumyadip Maity, Arindam Saha and Brojeshwar Bhowmick", "title": "Edge SLAM: Edge Points Based Monocular Visual SLAM", "comments": "ICCV Workshops 2017, Venice, Italy, October 22-29, 2017", "journal-ref": "International Conference on Computer Vision Workshops, 2017,\n  2408--2417", "doi": "10.1109/ICCVW.2017.284", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual SLAM shows significant progress in recent years due to high attention\nfrom vision community but still, challenges remain for low-textured\nenvironments. Feature based visual SLAMs do not produce reliable camera and\nstructure estimates due to insufficient features in a low-textured environment.\nMoreover, existing visual SLAMs produce partial reconstruction when the number\nof 3D-2D correspondences is insufficient for incremental camera estimation\nusing bundle adjustment. This paper presents Edge SLAM, a feature based\nmonocular visual SLAM which mitigates the above mentioned problems. Our\nproposed Edge SLAM pipeline detects edge points from images and tracks those\nusing optical flow for point correspondence. We further refine these point\ncorrespondences using geometrical relationship among three views. Owing to our\nedge-point tracking, we use a robust method for two-view initialization for\nbundle adjustment. Our proposed SLAM also identifies the potential situations\nwhere estimating a new camera into the existing reconstruction is becoming\nunreliable and we adopt a novel method to estimate the new camera reliably\nusing a local optimization technique. We present an extensive evaluation of our\nproposed SLAM pipeline with most popular open datasets and compare with the\nstate-of-the art. Experimental result indicates that our Edge SLAM is robust\nand works reliably well for both textured and less-textured environment in\ncomparison to existing state-of-the-art SLAMs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 09:40:45 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Maity", "Soumyadip", ""], ["Saha", "Arindam", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1901.04240", "submitter": "Philip Sellars", "authors": "Philip Sellars and Angelica Aviles-Rivero and Nicolas Papadakis and\n  David Coomes and Anita Faul and Carola-Bibane Sch\\\"onlieb", "title": "Semi-supervised Learning with Graphs: Covariance Based Superpixels For\n  Hyperspectral Image Classification", "comments": "Four pages with two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a graph-based semi-supervised framework for\nhyperspectral image classification. We first introduce a novel superpixel\nalgorithm based on the spectral covariance matrix representation of pixels to\nprovide a better representation of our data. We then construct a superpixel\ngraph, based on carefully considered feature vectors, before performing\nclassification. We demonstrate, through a set of experimental results using two\nbenchmarking datasets, that our approach outperforms three state-of-the-art\nclassification frameworks, especially when an extremely small amount of\nlabelled data is used.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 11:18:27 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 08:43:06 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 16:22:41 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 18:23:41 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Sellars", "Philip", ""], ["Aviles-Rivero", "Angelica", ""], ["Papadakis", "Nicolas", ""], ["Coomes", "David", ""], ["Faul", "Anita", ""], ["Sch\u00f6nlieb", "Carola-Bibane", ""]]}, {"id": "1901.04248", "submitter": "Ovidiu Vaduvescu", "authors": "V. Bacu, A. Sabou, T. Stefanut, D. Gorgan and O. Vaduvescu", "title": "NEARBY Platform for Detecting Asteroids in Astronomical Images Using\n  Cloud-based Containerized Applications", "comments": "IEEE 14th International Conference on Intelligent Computer\n  Communication and Processing (ICCP), Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/ICCP.2018.8516578", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuing monitoring and surveying of the nearby space to detect Near\nEarth Objects (NEOs) and Near Earth Asteroids (NEAs) are essential because of\nthe threats that this kind of objects impose on the future of our planet. We\nneed more computational resources and advanced algorithms to deal with the\nexponential growth of the digital cameras' performances and to be able to\nprocess (in near real-time) data coming from large surveys. This paper presents\na software platform called NEARBY that supports automated detection of moving\nsources (asteroids) among stars from astronomical images. The detection\nprocedure is based on the classic \"blink\" detection and, after that, the system\nsupports visual analysis techniques to validate the moving sources, assisted by\nstatic and dynamical presentations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 11:49:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bacu", "V.", ""], ["Sabou", "A.", ""], ["Stefanut", "T.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.04252", "submitter": "Nicola Capece", "authors": "Nicola Capece, Francesco Banterle, Paolo Cignoni, Fabio Ganovelli,\n  Roberto Scopigno, Ugo Erra", "title": "DeepFlash: Turning a Flash Selfie into a Studio Portrait", "comments": null, "journal-ref": "Elsevier Image Communication, Volume 77, September 2019, Pages\n  28-39", "doi": "10.1016/j.image.2019.05.013", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for turning a flash selfie taken with a smartphone into a\nphotograph as if it was taken in a studio setting with uniform lighting. Our\nmethod uses a convolutional neural network trained on a set of pairs of\nphotographs acquired in an ad-hoc acquisition campaign. Each pair consists of\none photograph of a subject's face taken with the camera flash enabled and\nanother one of the same subject in the same pose illuminated using a\nphotographic studio-lighting setup. We show how our method can amend defects\nintroduced by a close-up camera flash, such as specular highlights, shadows,\nskin shine, and flattened images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 11:57:05 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 16:04:51 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Capece", "Nicola", ""], ["Banterle", "Francesco", ""], ["Cignoni", "Paolo", ""], ["Ganovelli", "Fabio", ""], ["Scopigno", "Roberto", ""], ["Erra", "Ugo", ""]]}, {"id": "1901.04268", "submitter": "Zehang Lin", "authors": "Zhenguo Yang, Zehang Lin, Peipei Kang, Jianming Lv, Qing Li and Wenyin\n  Liu", "title": "Learning Shared Semantic Space with Correlation Alignment for\n  Cross-modal Event Retrieval", "comments": "22 pages, submitted to ACM Transactions on Multimedia Computing\n  Communications and Applications(ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn shared semantic space with correlation\nalignment (${S}^{3}CA$) for multimodal data representations, which aligns\nnonlinear correlations of multimodal data distributions in deep neural networks\ndesigned for heterogeneous data. In the context of cross-modal (event)\nretrieval, we design a neural network with convolutional layers and\nfully-connected layers to extract features for images, including images on\nFlickr-like social media. Simultaneously, we exploit a fully-connected neural\nnetwork to extract semantic features for texts, including news articles from\nnews media. In particular, nonlinear correlations of layer activations in the\ntwo neural networks are aligned with correlation alignment during the joint\ntraining of the networks. Furthermore, we project the multimodal data into a\nshared semantic space for cross-modal (event) retrieval, where the distances\nbetween heterogeneous data samples can be measured directly. In addition, we\ncontribute a Wiki-Flickr Event dataset, where the multimodal data samples are\nnot describing each other in pairs like the existing paired datasets, but all\nof them are describing semantic events. Extensive experiments conducted on both\npaired and unpaired datasets manifest the effectiveness of ${S}^{3}CA$,\noutperforming the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 12:48:53 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 13:19:55 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 00:31:51 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Yang", "Zhenguo", ""], ["Lin", "Zehang", ""], ["Kang", "Peipei", ""], ["Lv", "Jianming", ""], ["Li", "Qing", ""], ["Liu", "Wenyin", ""]]}, {"id": "1901.04355", "submitter": "Saeed Alahmari", "authors": "Saeed S. Alahmari, Dmitry Goldgof, Lawrence O. Hall, Palak Dave, Hady\n  Ahmady Phoulady, and Peter R. Mouton", "title": "Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop", "comments": "Accepted by ICMLA18 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lack of enough labeled data is a major problem in building machine learning\nbased models when the manual annotation (labeling) is error-prone, expensive,\ntedious, and time-consuming. In this paper, we introduce an iterative deep\nlearning based method to improve segmentation and counting of cells based on\nunbiased stereology applied to regions of interest of extended depth of field\n(EDF) images. This method uses an existing machine learning algorithm called\nthe adaptive segmentation algorithm (ASA) to generate masks (verified by a\nuser) for EDF images to train deep learning models. Then an iterative deep\nlearning approach is used to feed newly predicted and accepted deep learning\nmasks/images (verified by a user) to the training set of the deep learning\nmodel. The error rate in unbiased stereology count of cells on an unseen test\nset reduced from about 3 % to less than 1 % after 5 iterations of the iterative\ndeep learning based unbiased stereology process.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 15:01:49 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Alahmari", "Saeed S.", ""], ["Goldgof", "Dmitry", ""], ["Hall", "Lawrence O.", ""], ["Dave", "Palak", ""], ["Phoulady", "Hady Ahmady", ""], ["Mouton", "Peter R.", ""]]}, {"id": "1901.04366", "submitter": "Daniel McDuff", "authors": "Daniel McDuff and Ethan Blackford", "title": "iPhys: An Open Non-Contact Imaging-Based Physiological Measurement\n  Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging-based, non-contact measurement of physiology (including imaging\nphotoplethysmography and imaging ballistocardiography) is a growing field of\nresearch. There are several strengths of imaging methods that make them\nattractive. They remove the need for uncomfortable contact sensors and can\nenable spatial and concomitant measurement from a single sensor. Furthermore,\ncameras are ubiquitous and often low-cost solutions for sensing. Open source\ntoolboxes help accelerate the progress of research by providing a means to\ncompare new approaches against standard implementations of the\nstate-of-the-art. We present an open source imaging-based physiological\nmeasurement toolbox with implementations of many of the most frequently\nemployed computational methods. We hope that this toolbox will contribute to\nthe advancement of non-contact physiological sensing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 15:48:31 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["McDuff", "Daniel", ""], ["Blackford", "Ethan", ""]]}, {"id": "1901.04392", "submitter": "Pierre Tirilly", "authors": "Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne,\n  Pierre Boulet", "title": "Unsupervised Visual Feature Learning with Spike-timing-dependent\n  Plasticity: How Far are we from Traditional Feature Learning Approaches?", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2019.04.016", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) equipped with latency coding and spike-timing\ndependent plasticity rules offer an alternative to solve the data and energy\nbottlenecks of standard computer vision approaches: they can learn visual\nfeatures without supervision and can be implemented by ultra-low power hardware\narchitectures. However, their performance in image classification has never\nbeen evaluated on recent image datasets. In this paper, we compare SNNs to\nauto-encoders on three visual recognition datasets, and extend the use of SNNs\nto color images. The analysis of the results helps us identify some bottlenecks\nof SNNs: the limits of on-center/off-center coding, especially for color\nimages, and the ineffectiveness of current inhibition mechanisms. These issues\nshould be addressed to build effective SNNs for image recognition.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:42:30 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:04:24 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Falez", "Pierre", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""], ["Devienne", "Philippe", ""], ["Boulet", "Pierre", ""]]}, {"id": "1901.04405", "submitter": "Nikesh Dattani", "authors": "Nike Dattani", "title": "Quadratization in discrete optimization and quantum mechanics", "comments": "Contributors (in order of lines contributed on GitHub): Nike Dattani,\n  Richard Tanburn, Andreas Soteriou, Nicholas Chancellor, Szilard Szalay,\n  Elisabeth Rodriguez-Heck, Hou Tin Chau, Ka Wa Yip, Yudong Cao", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.DM math.OC physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A book about turning high-degree optimization problems into quadratic\noptimization problems that maintain the same global minimum (ground state).\nThis book explores quadratizations for pseudo-Boolean optimization,\nperturbative gadgets used in QMA completeness theorems, and also\nnon-perturbative k-local to 2-local transformations used for quantum mechanics,\nquantum annealing and universal adiabatic quantum computing. The book contains\n~70 different Hamiltonian transformations, each of them on a separate page,\nwhere the cost (in number of auxiliary binary variables or auxiliary qubits, or\nnumber of sub-modular terms, or in graph connectivity, etc.), pros, cons,\nexamples, and references are given. One can therefore look up a quadratization\nappropriate for the specific term(s) that need to be quadratized, much like\nusing an integral table to look up the integral that needs to be done. This\nbook is therefore useful for writing compilers to transform general\noptimization problems, into a form that quantum annealing or universal\nadiabatic quantum computing hardware requires; or for transforming quantum\nchemistry problems written in the Jordan-Wigner or Bravyi-Kitaev form, into a\nform where all multi-qubit interactions become 2-qubit pairwise interactions,\nwithout changing the desired ground state. Applications cited include computer\nvision problems (e.g. image de-noising, un-blurring, etc.), number theory (e.g.\ninteger factoring), graph theory (e.g. Ramsey number determination), and\nquantum chemistry. The book is open source, and anyone can make modifications\nhere: https://github.com/HPQC-LABS/Book_About_Quadratization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 17:06:40 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 04:47:42 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Dattani", "Nike", ""]]}, {"id": "1901.04412", "submitter": "Abhineet Singh", "authors": "Abhineet Singh, Hayden Kalke, Mark Loewen and Nilanjan Ray", "title": "River Ice Segmentation with Deep Learning", "comments": "supplementary:\n  http://webdocs.cs.ualberta.ca/~vis/asingh1/docs/river_ice_segm_supp.pdf", "journal-ref": null, "doi": "10.1109/TGRS.2020.2981082", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of computing surface ice concentration for\ntwo different types of ice from digital images of river surface. It presents\nthe results of attempting to solve this problem using several state of the art\nsemantic segmentation methods based on deep convolutional neural networks\n(CNNs). This task presents two main challenges - very limited availability of\nlabeled training data and presence of noisy labels due to the great difficulty\nof visually distinguishing between the two types of ice, even for human\nexperts. The results are used to analyze the extent to which some of the best\ndeep learning methods currently in existence can handle these challenges. The\ncode and data used in the experiments are made publicly available to facilitate\nfurther work in this domain.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 17:16:14 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 19:19:33 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Singh", "Abhineet", ""], ["Kalke", "Hayden", ""], ["Loewen", "Mark", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1901.04530", "submitter": "Omry Sendik", "authors": "Omry Sendik, Dani Lischinski, Daniel Cohen-Or", "title": "CrossNet: Latent Cross-Consistency for Unpaired Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent GAN-based architectures have been able to deliver impressive\nperformance on the general task of image-to-image translation. In particular,\nit was shown that a wide variety of image translation operators may be learned\nfrom two image sets, containing images from two different domains, without\nestablishing an explicit pairing between the images. This was made possible by\nintroducing clever regularizers to overcome the under-constrained nature of the\nunpaired translation problem. In this work, we introduce a novel architecture\nfor unpaired image translation, and explore several new regularizers enabled by\nit. Specifically, our architecture comprises a pair of GANs, as well as a pair\nof translators between their respective latent spaces. These cross-translators\nenable us to impose several regularizing constraints on the learnt image\ntranslation operator, collectively referred to as latent cross-consistency. Our\nresults show that our proposed architecture and latent cross-consistency\nconstraints are able to outperform the existing state-of-the-art on a variety\nof image translation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 19:31:58 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 19:47:34 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sendik", "Omry", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1901.04540", "submitter": "Jiantao Pu", "authors": "Yi Zhen, Hang Chen, Xu Zhang, Meng Liu, Xin Meng, Jian Zhang, Jiantao\n  Pu", "title": "Assessment of central serous chorioretinopathy (CSC) depicted on color\n  fundus photographs using deep Learning", "comments": "4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate whether and to what extent central serous chorioretinopathy\n(CSC) depicted on color fundus photographs can be assessed using deep learning\ntechnology. We collected a total of 2,504 fundus images acquired on different\nsubjects. We verified the CSC status of these images using their corresponding\noptical coherence tomography (OCT) images. A total of 1,329 images depicted\nCSC. These images were preprocessed and normalized. This resulting dataset was\nrandomly split into three parts in the ratio of 8:1:1 respectively for\ntraining, validation, and testing purposes. We used the deep learning\narchitecture termed InceptionV3 to train the classifier. We performed\nnonparametric receiver operating characteristic (ROC) analyses to assess the\ncapability of the developed algorithm to identify CSC. The Kappa coefficient\nbetween the two raters was 0.48 (p < 0.001), while the Kappa coefficients\nbetween the computer and the two raters were 0.59 (p < 0.001) and 0.33 (p <\n0.05).Our experiments showed that the computer algorithm based on deep learning\ncan assess CSC depicted on color fundus photographs in a relatively reliable\nand consistent way.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 19:56:22 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhen", "Yi", ""], ["Chen", "Hang", ""], ["Zhang", "Xu", ""], ["Liu", "Meng", ""], ["Meng", "Xin", ""], ["Zhang", "Jian", ""], ["Pu", "Jiantao", ""]]}, {"id": "1901.04547", "submitter": "Kwang Moo Yi", "authors": "Kyong Hwan Jin, Michael Unser, Kwang Moo Yi", "title": "Self-Supervised Deep Active Accelerated MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to simultaneously learn to sample and reconstruct magnetic\nresonance images (MRI) to maximize the reconstruction quality given a limited\nsample budget, in a self-supervised setup. Unlike existing deep methods that\nfocus only on reconstructing given data, thus being passive, we go beyond the\ncurrent state of the art by considering both the data acquisition and the\nreconstruction process within a single deep-learning framework. As our network\nlearns to acquire data, the network is active in nature. In order to do so, we\nsimultaneously train two neural networks, one dedicated to reconstruction and\nthe other to progressive sampling, each with an automatically generated\nsupervision signal that links them together. The two supervision signals are\ncreated through Monte Carlo tree search (MCTS). MCTS returns a better sampling\npattern than what the current sampling network can give and, thus, a better\nfinal reconstruction. The sampling network is trained to mimic the MCTS results\nusing the previous sampling network, thus being enhanced. The reconstruction\nnetwork is trained to give the highest reconstruction quality, given the MCTS\nsampling pattern. Through this framework, we are able to train the two networks\nwithout providing any direct supervision on sampling.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:13:00 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Jin", "Kyong Hwan", ""], ["Unser", "Michael", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1901.04584", "submitter": "Jianning Li", "authors": "Jianning Li, Long Cao, Yangyang Ge, Bowen Meng, Cheng Wang, Wei Guo", "title": "Towards Personalized Management of Type B Aortic Dissection Using STENT:\n  a STandard cta database with annotation of the ENtire aorta and True-false\n  lumen", "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the rights to agree to the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type B Aortic Dissection(TBAD) is a rare aortic disease with a high 5-year\nmortality.Personalized and precise management of TBAD has been increasingly\ndesired in clinic which requires the geometric parameters of TBAD specific to\nthe patient be measured accurately.This remains to be a challenging task for\nvascular surgeons as manual measurement is highly subjective and imprecise. To\nsolve this problem,we introduce STENT-a STandard cta database with annotation\nof the ENtire aorta and True-false lumen. The database contains 274 CT\nangiography (CTA) scans from 274 unique TBAD patients and is split into a\ntraining set(254 cases including 210 preoperative and 44 postoperative scans )\nand a test set(20 cases).Based on STENT,we develop a series of methods\nincluding automated TBAD segmentation and automated measurement of TBAD\nparameters that facilitate personalized and precise management of the disease.\nIn this work, the database and the proposed methods are thoroughly introduced\nand evaluated and the results of our study shows the feasibility and\neffectiveness of our approach to easing the decision-making process for\nvascular surgeons during personalized TBAD management.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 02:10:35 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 02:03:13 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Jianning", ""], ["Cao", "Long", ""], ["Ge", "Yangyang", ""], ["Meng", "Bowen", ""], ["Wang", "Cheng", ""], ["Guo", "Wei", ""]]}, {"id": "1901.04596", "submitter": "Guo-Jun Qi", "authors": "Liheng Zhang, Guo-Jun Qi, Liqiang Wang, Jiebo Luo", "title": "AET vs. AED: Unsupervised Representation Learning by Auto-Encoding\n  Transformations rather than Data", "comments": "in Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019), Long Beach, CA, June 16th - June 20th, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks often relies on a large amount of labeled\nexamples, which can be difficult to obtain in many real scenarios. To address\nthis challenge, unsupervised methods are strongly preferred for training neural\nnetworks without using any labeled data. In this paper, we present a novel\nparadigm of unsupervised representation learning by Auto-Encoding\nTransformation (AET) in contrast to the conventional Auto-Encoding Data (AED)\napproach. Given a randomly sampled transformation, AET seeks to predict it\nmerely from the encoded features as accurately as possible at the output end.\nThe idea is the following: as long as the unsupervised features successfully\nencode the essential information about the visual structures of original and\ntransformed images, the transformation can be well predicted. We will show that\nthis AET paradigm allows us to instantiate a large variety of transformations,\nfrom parameterized, to non-parameterized and GAN-induced ones. Our experiments\nshow that AET greatly improves over existing unsupervised approaches, setting\nnew state-of-the-art performances being greatly closer to the upper bounds by\ntheir fully supervised counterparts on CIFAR-10, ImageNet and Places datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:45:21 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 22:10:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Liheng", ""], ["Qi", "Guo-Jun", ""], ["Wang", "Liqiang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1901.04604", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Wei Wang, Yan Yan, Nicu Sebe", "title": "Dual Generator Generative Adversarial Networks for Multi-Domain\n  Image-to-Image Translation", "comments": "16 pages, 7 figures, accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for image-to-image translation with Generative\nAdversarial Networks (GANs) can learn a mapping from one domain to another\ndomain using unpaired image data. However, these methods require the training\nof one specific model for every pair of image domains, which limits the\nscalability in dealing with more than two image domains. In addition, the\ntraining stage of these methods has the common problem of model collapse that\ndegrades the quality of the generated images. To tackle these issues, we\npropose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a\nrobust and scalable approach allowing to perform unpaired image-to-image\ntranslation for multiple domains using only dual generators within a single\nmodel. Moreover, we explore different optimization losses for better training\nof G$^2$GAN, and thus make unpaired image-to-image translation with higher\nconsistency and better stability. Extensive experiments on six publicly\navailable datasets with different scenarios, i.e., architectural buildings,\nseasons, landscape and human faces, demonstrate that the proposed G$^2$GAN\nachieves superior model capacity and better generation performance comparing\nwith existing image-to-image translation GAN models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 23:25:41 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Wang", "Wei", ""], ["Yan", "Yan", ""], ["Sebe", "Nicu", ""]]}, {"id": "1901.04618", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang and Graham Healy and Alan F. Smeaton and Tomas E. Ward", "title": "Spatial Filtering Pipeline Evaluation of Cortically Coupled Computer\n  Vision System for Rapid Serial Visual Presentation", "comments": null, "journal-ref": null, "doi": "10.1080/2326263X.2019.1568821", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid Serial Visual Presentation (RSVP) is a paradigm that supports the\napplication of cortically coupled computer vision to rapid image search. In\nRSVP, images are presented to participants in a rapid serial sequence which can\nevoke Event-related Potentials (ERPs) detectable in their Electroencephalogram\n(EEG). The contemporary approach to this problem involves supervised spatial\nfiltering techniques which are applied for the purposes of enhancing the\ndiscriminative information in the EEG data. In this paper we make two primary\ncontributions to that field: 1) We propose a novel spatial filtering method\nwhich we call the Multiple Time Window LDA Beamformer (MTWLB) method; 2) we\nprovide a comprehensive comparison of nine spatial filtering pipelines using\nthree spatial filtering schemes namely, MTWLB, xDAWN, Common Spatial Pattern\n(CSP) and three linear classification methods Linear Discriminant Analysis\n(LDA), Bayesian Linear Regression (BLR) and Logistic Regression (LR). Three\npipelines without spatial filtering are used as baseline comparison. The Area\nUnder Curve (AUC) is used as an evaluation metric in this paper. The results\nreveal that MTWLB and xDAWN spatial filtering techniques enhance the\nclassification performance of the pipeline but CSP does not. The results also\nsupport the conclusion that LR can be effective for RSVP based BCI if\ndiscriminative features are available.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:27:40 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Wang", "Zhengwei", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""]]}, {"id": "1901.04619", "submitter": "Timo Kohlberger", "authors": "Timo Kohlberger, Yun Liu, Melissa Moran, Po-Hsuan (Cameron) Chen,\n  Trissia Brown, Craig H. Mermel, Jason D. Hipp, Martin C. Stumpe", "title": "Whole-Slide Image Focus Quality: Automatic Assessment and Impact on AI\n  Cancer Detection", "comments": null, "journal-ref": "Pathology Informatics (2019)", "doi": "10.4103/jpi.jpi_11_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology enables remote access or consults and powerful image\nanalysis algorithms. However, the slide digitization process can create\nartifacts such as out-of-focus (OOF). OOF is often only detected upon careful\nreview, potentially causing rescanning and workflow delays. Although scan-time\noperator screening for whole-slide OOF is feasible, manual screening for OOF\naffecting only parts of a slide is impractical. We developed a convolutional\nneural network (ConvFocus) to exhaustively localize and quantify the severity\nof OOF regions on digitized slides. ConvFocus was developed using our refined\nsemi-synthetic OOF data generation process, and evaluated using real\nwhole-slide images spanning 3 different tissue types and 3 different stain\ntypes that were digitized by two different scanners. ConvFocus's predictions\nwere compared with pathologist-annotated focus quality grades across 514\ndistinct regions representing 37,700 35x35 $\\mu$m image patches, and 21\ndigitized \"z-stack\" whole-slide images that contain known OOF patterns. When\ncompared to pathologist-graded focus quality, ConvFocus achieved Spearman rank\ncoefficients of 0.81 and 0.94 on two scanners, and reproduced the expected OOF\npatterns from z-stack scanning. We also evaluated the impact of OOF on the\naccuracy of a state-of-the-art metastatic breast cancer detector and saw a\nconsistent decrease in performance with increasing OOF. Comprehensive\nwhole-slide OOF categorization could enable rescans prior to pathologist\nreview, potentially reducing the impact of digitization focus issues on the\nclinical workflow. We show that the algorithm trained on our semi-synthetic OOF\ndata generalizes well to real OOF regions across tissue types, stains, and\nscanners. Finally, quantitative OOF maps can flag regions that might otherwise\nbe misclassified by image analysis algorithms, preventing OOF-induced errors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:31:35 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 20:32:06 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kohlberger", "Timo", "", "Cameron"], ["Liu", "Yun", "", "Cameron"], ["Moran", "Melissa", "", "Cameron"], ["Po-Hsuan", "", "", "Cameron"], ["Chen", "", ""], ["Brown", "Trissia", ""], ["Mermel", "Craig H.", ""], ["Hipp", "Jason D.", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1901.04622", "submitter": "Hao Tang", "authors": "Hao Tang, Hong Liu, Wei Xiao, Nicu Sebe", "title": "Fast and Robust Dynamic Hand Gesture Recognition via Key Frames\n  Extraction and Feature Fusion", "comments": "11 pages, 3 figures, accepted to NeuroComputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is a hot topic in computer vision and pattern\nrecognition, which plays a vitally important role in natural human-computer\ninterface. Although great progress has been made recently, fast and robust hand\ngesture recognition remains an open problem, since the existing methods have\nnot well balanced the performance and the efficiency simultaneously. To bridge\nit, this work combines image entropy and density clustering to exploit the key\nframes from hand gesture video for further feature extraction, which can\nimprove the efficiency of recognition. Moreover, a feature fusion strategy is\nalso proposed to further improve feature representation, which elevates the\nperformance of recognition. To validate our approach in a \"wild\" environment,\nwe also introduce two new datasets called HandGesture and Action3D datasets.\nExperiments consistently demonstrate that our strategy achieves competitive\nresults on Northwestern University, Cambridge, HandGesture and Action3D hand\ngesture datasets. Our code and datasets will release at\nhttps://github.com/Ha0Tang/HandGestureRecognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:47:59 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Hong", ""], ["Xiao", "Wei", ""], ["Sebe", "Nicu", ""]]}, {"id": "1901.04623", "submitter": "Rafael Felix", "authors": "Rafael Felix and Michele Sasdelli and Ian Reid and Gustavo Carneiro", "title": "Multi-modal Ensemble Classification for Generalized Zero Shot Learning", "comments": "10 pages, 3 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero shot learning (GZSL) is defined by a training process\ncontaining a set of visual samples from seen classes and a set of semantic\nsamples from seen and unseen classes, while the testing process consists of the\nclassification of visual samples from seen and unseen classes. Current\napproaches are based on testing processes that focus on only one of the\nmodalities (visual or semantic), even when the training uses both modalities\n(mostly for regularizing the training process). This under-utilization of\nmodalities, particularly during testing, can hinder the classification accuracy\nof the method. In addition, we note a scarce attention to the development of\nlearning methods that explicitly optimize a balanced performance of seen and\nunseen classes. Such issue is one of the reasons behind the vastly superior\nclassification accuracy of seen classes in GZSL methods. In this paper, we\nmitigate these issues by proposing a new GZSL method based on multi-modal\ntraining and testing processes, where the optimization explicitly promotes a\nbalanced classification accuracy between seen and unseen classes. Furthermore,\nwe explore Bayesian inference for the visual and semantic classifiers, which is\nanother novelty of our work in the GZSL framework. Experiments show that our\nmethod holds the state of the art (SOTA) results in terms of harmonic mean\n(H-mean) classification between seen and unseen classes and area under the seen\nand unseen curve (AUSUC) on several public GZSL benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:50:58 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 03:55:33 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Felix", "Rafael", ""], ["Sasdelli", "Michele", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1901.04637", "submitter": "Antao Zhou", "authors": "Gang Cao, Antao Zhou, Xianglin Huang, Gege Song, Lifang Yang, Yonggui\n  Zhu", "title": "Resampling detection of recompressed images via dual-stream\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resampling detection plays an important role in identifying image tampering,\nsuch as image splicing. Currently, the resampling detection is still difficult\nin recompressed images, which are yielded by applying resampling followed by\npost-JPEG compression to primary JPEG images. Except for the scenario of low\nquality primary compression, it remains rather challenging due to the\nwidespread use of middle/high quality compression in imaging devices. In this\npaper, we propose a new convolution neural network (CNN) method to learn the\nresampling trace features directly from the recompressed images. To this end, a\nnoise extraction layer based on low-order high pass filters is deployed to\nyield the image residual domain, which is more beneficial to extract\nmanipulation trace features. A dual-stream CNN is presented to capture the\nresampling trails along different directions, where the horizontal and vertical\nstreams are interleaved and concatenated. Lastly, the learned features are fed\ninto Sigmoid/Softmax layer, which acts as a binary/multiple classifier for\nachieving the blind detection and parameter estimation of resampling,\nrespectively. Extensive experimental results demonstrate that our proposed\nmethod could detect resampling effectively in recompressed images and\noutperform the state-of-the-art detectors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 02:15:26 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 02:36:11 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 08:10:33 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Cao", "Gang", ""], ["Zhou", "Antao", ""], ["Huang", "Xianglin", ""], ["Song", "Gege", ""], ["Yang", "Lifang", ""], ["Zhu", "Yonggui", ""]]}, {"id": "1901.04641", "submitter": "Devinder Kumar", "authors": "Vignesh Sankar, Devinder Kumar, David A. Clausi, Graham W. Taylor and\n  Alexander Wong", "title": "SISC: End-to-end Interpretable Discovery Radiomics-Driven Lung Cancer\n  Prediction via Stacked Interpretable Sequencing Cells", "comments": "First two authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Lung cancer is the leading cause of cancer-related death\nworldwide. Computer-aided diagnosis (CAD) systems have shown significant\npromise in recent years for facilitating the effective detection and\nclassification of abnormal lung nodules in computed tomography (CT) scans.\nWhile hand-engineered radiomic features have been traditionally used for lung\ncancer prediction, there have been significant recent successes achieving\nstate-of-the-art results in the area of discovery radiomics. Here, radiomic\nsequencers comprising of highly discriminative radiomic features are discovered\ndirectly from archival medical data. However, the interpretation of predictions\nmade using such radiomic sequencers remains a challenge. Method: A novel\nend-to-end interpretable discovery radiomics-driven lung cancer prediction\npipeline has been designed, build, and tested. The radiomic sequencer being\ndiscovered possesses a deep architecture comprised of stacked interpretable\nsequencing cells (SISC). Results: The SISC architecture is shown to outperform\nprevious approaches while providing more insight in to its decision making\nprocess. Conclusion: The SISC radiomic sequencer is able to achieve\nstate-of-the-art results in lung cancer prediction, and also offers prediction\ninterpretability in the form of critical response maps. Significance: The\ncritical response maps are useful for not only validating the predictions of\nthe proposed SISC radiomic sequencer, but also provide improved\nradiologist-machine collaboration for effective diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 02:55:04 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Sankar", "Vignesh", ""], ["Kumar", "Devinder", ""], ["Clausi", "David A.", ""], ["Taylor", "Graham W.", ""], ["Wong", "Alexander", ""]]}, {"id": "1901.04656", "submitter": "Zhaoqiang Xia", "authors": "Zhaoqiang Xia, Xiaopeng Hong, Xingyu Gao, Xiaoyi Feng, Guoying Zhao", "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing\n  Spontaneous Micro-expressions", "comments": "Submitted to IEEE TMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the recognition task of spontaneous facial micro-expressions has\nattracted much attention with its various real-world applications. Plenty of\nhandcrafted or learned features have been employed for a variety of classifiers\nand achieved promising performances for recognizing micro-expressions. However,\nthe micro-expression recognition is still challenging due to the subtle\nspatiotemporal changes of micro-expressions. To exploit the merits of deep\nlearning, we propose a novel deep recurrent convolutional networks based\nmicro-expression recognition approach, capturing the spatial-temporal\ndeformations of micro-expression sequence. Specifically, the proposed deep\nmodel is constituted of several recurrent convolutional layers for extracting\nvisual features and a classificatory layer for recognition. It is optimized by\nan end-to-end manner and obviates manual feature design. To handle sequential\ndata, we exploit two types of extending the connectivity of convolutional\nnetworks across temporal domain, in which the spatiotemporal deformations are\nmodeled in views of facial appearance and geometry separately. Besides, to\novercome the shortcomings of limited and imbalanced training samples, temporal\ndata augmentation strategies as well as a balanced loss are jointly used for\nour deep network. By performing the experiments on three spontaneous\nmicro-expression datasets, we verify the effectiveness of our proposed\nmicro-expression recognition approach compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 04:33:07 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Xia", "Zhaoqiang", ""], ["Hong", "Xiaopeng", ""], ["Gao", "Xingyu", ""], ["Feng", "Xiaoyi", ""], ["Zhao", "Guoying", ""]]}, {"id": "1901.04684", "submitter": "Huan Zhang", "authors": "Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S. Dhillon,\n  Cho-Jui Hsieh", "title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "comments": "Accepted by International Conference on Learning Representations\n  (ICLR) 2019. Huan Zhang and Hongge Chen contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adversarial training procedure proposed by Madry et al. (2018) is one of\nthe most effective methods to defend against adversarial examples in deep\nneural networks (DNNs). In our paper, we shed some lights on the practicality\nand the hardness of adversarial training by showing that the effectiveness\n(robustness on test set) of adversarial training has a strong correlation with\nthe distance between a test point and the manifold of training data embedded by\nthe network. Test examples that are relatively far away from this manifold are\nmore likely to be vulnerable to adversarial attacks. Consequentially, an\nadversarial training based defense is susceptible to a new class of attacks,\nthe \"blind-spot attack\", where the input images reside in \"blind-spots\" (low\ndensity regions) of the empirical distribution of training data but is still on\nthe ground-truth data manifold. For MNIST, we found that these blind-spots can\nbe easily found by simply scaling and shifting image pixel values. Most\nimportantly, for large datasets with high dimensional and complex data manifold\n(CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training\nmakes defending on any valid test examples difficult due to the curse of\ndimensionality and the scarcity of training data. Additionally, we find that\nblind-spots also exist on provable defenses including (Wong & Kolter, 2018) and\n(Sinha et al., 2018) because these trainable robustness certificates can only\nbe practically optimized on a limited set of training data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:21:44 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Zhang", "Huan", ""], ["Chen", "Hongge", ""], ["Song", "Zhao", ""], ["Boning", "Duane", ""], ["Dhillon", "Inderjit S.", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1901.04687", "submitter": "Sang Ho Lee", "authors": "Sang-ho Lee, Simyung Chang, Nojun Kwak", "title": "URNet : User-Resizable Residual Networks with Conditional Gating Module", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are widely used to process spatial scenes, but\ntheir computational cost is fixed and depends on the structure of the network\nused. There are methods to reduce the cost by compressing networks or varying\nits computational path dynamically according to the input image. However, since\na user can not control the size of the learned model, it is difficult to\nrespond dynamically if the amount of service requests suddenly increases. We\npropose User-Resizable Residual Networks (URNet), which allows users to adjust\nthe scale of the network as needed during evaluation. URNet includes\nConditional Gating Module (CGM) that determines the use of each residual block\naccording to the input image and the desired scale. CGM is trained in a\nsupervised manner using the newly proposed scale loss and its corresponding\ntraining methods. URNet can control the amount of computation according to\nuser's demand without degrading the accuracy significantly. It can also be used\nas a general compression method by fixing the scale size during training. In\nthe experiments on ImageNet, URNet based on ResNet-101 maintains the accuracy\nof the baseline even when resizing it to approximately 80% of the original\nnetwork, and demonstrates only about 1% accuracy degradation when using about\n65% of the computation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:26:42 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 08:03:15 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lee", "Sang-ho", ""], ["Chang", "Simyung", ""], ["Kwak", "Nojun", ""]]}, {"id": "1901.04780", "submitter": "Chen Wang", "authors": "Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart\\'in-Mart\\'in, Cewu Lu, Li\n  Fei-Fei, Silvio Savarese", "title": "DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key technical challenge in performing 6D object pose estimation from RGB-D\nimage is to fully leverage the two complementary data sources. Prior works\neither extract information from the RGB image and depth separately or use\ncostly post-processing steps, limiting their performances in highly cluttered\nscenes and real-time applications. In this work, we present DenseFusion, a\ngeneric framework for estimating 6D pose of a set of known objects from RGB-D\nimages. DenseFusion is a heterogeneous architecture that processes the two data\nsources individually and uses a novel dense fusion network to extract\npixel-wise dense feature embedding, from which the pose is estimated.\nFurthermore, we integrate an end-to-end iterative pose refinement procedure\nthat further improves the pose estimation while achieving near real-time\ninference. Our experiments show that our method outperforms state-of-the-art\napproaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed\nmethod to a real robot to grasp and manipulate objects based on the estimated\npose.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 11:58:04 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Wang", "Chen", ""], ["Xu", "Danfei", ""], ["Zhu", "Yuke", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Lu", "Cewu", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1901.04846", "submitter": "Felix M. Riese", "authors": "Felix M. Riese, Sina Keller", "title": "Soil Texture Classification with 1D Convolutional Neural Networks based\n  on Hyperspectral Data", "comments": "Accepted to the ISPRS Geospatial Week 2019 in Enschede (NL)", "journal-ref": null, "doi": "10.5194/isprs-annals-IV-2-W5-615-2019", "report-no": null, "categories": "cs.CV cs.LG physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil texture is important for many environmental processes. In this paper, we\nstudy the classification of soil texture based on hyperspectral data. We\ndevelop and implement three 1-dimensional (1D) convolutional neural networks\n(CNN): the LucasCNN, the LucasResNet which contains an identity block as\nresidual network, and the LucasCoordConv with an additional coordinates layer.\nFurthermore, we modify two existing 1D CNN approaches for the presented\nclassification task. The code of all five CNN approaches is available on GitHub\n(Riese, 2019). We evaluate the performance of the CNN approaches and compare\nthem to a random forest classifier. Thereby, we rely on the freely available\nLUCAS topsoil dataset. The CNN approach with the least depth turns out to be\nthe best performing classifier. The LucasCoordConv achieves the best\nperformance regarding the average accuracy. In future work, we can further\nenhance the introduced LucasCNN, LucasResNet and LucasCoordConv and include\nadditional variables of the rich LUCAS dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:29:04 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 16:14:04 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2019 13:57:12 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Riese", "Felix M.", ""], ["Keller", "Sina", ""]]}, {"id": "1901.04870", "submitter": "Pongsate Tangseng", "authors": "Pongsate Tangseng, Takayuki Okatani", "title": "Toward Explainable Fashion Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been conducted so far to build systems for recommending\nfashion items and outfits. Although they achieve good performances in their\nrespective tasks, most of them cannot explain their judgments to the users,\nwhich compromises their usefulness. Toward explainable fashion recommendation,\nthis study proposes a system that is able not only to provide a goodness score\nfor an outfit but also to explain the score by providing reason behind it. For\nthis purpose, we propose a method for quantifying how influential each feature\nof each item is to the score. Using this influence value, we can identify which\nitem and what feature make the outfit good or bad. We represent the image of\neach item with a combination of human-interpretable features, and thereby the\nidentification of the most influential item-feature pair gives useful\nexplanation of the output score. To evaluate the performance of this approach,\nwe design an experiment that can be performed without human annotation; we\nreplace a single item-feature pair in an outfit so that the score will\ndecrease, and then we test if the proposed method can detect the replaced item\ncorrectly using the above influence values. The experimental results show that\nthe proposed method can accurately detect bad items in outfits lowering their\nscores.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:02:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 16:54:40 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 06:32:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Tangseng", "Pongsate", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1901.04872", "submitter": "Lawrence  M Zhou", "authors": "Mingyong Zhou", "title": "Electrical Impedance Tomography based on Genetic Algorithm", "comments": "Full paper was accepted into proceedings of 4th International\n  Conference on Innovation in Computing System & Engineering Technology (\n  ICICSET 2018) in Zurich, Switzerland on August 14- 15, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we applies GA algorithm into Electrical Impedance Tomography\n(EIT) application. We first outline the EIT problem as an optimization problem\nand define a target optimization function. Then we show how the GA algorithm as\nan alternative searching algorithm can be used for solving EIT inverse problem.\nIn this paper, we explore evolutionary methods such as GA algorithms combined\nwith various regularization operators to solve EIT inverse computing problem.\n  Key words: Electrical Impedance Tomography (EIT), GA, Tikhonov operator ,\nMumford-Shah operator, Particle Swarm Optimization(PSO), Back Propagation(BP).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 07:01:36 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhou", "Mingyong", ""]]}, {"id": "1901.04877", "submitter": "Jun Liu", "authors": "Jun Liu, Henghui Ding, Amir Shahroudy, Ling-Yu Duan, Xudong Jiang,\n  Gang Wang, Alex C. Kot", "title": "Feature Boosting Network For 3D Pose Estimation", "comments": "Accepted to T-PAMI. DOI: 10.1109/TPAMI.2019.2894422", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a feature boosting network is proposed for estimating 3D hand\npose and 3D body pose from a single RGB image. In this method, the features\nlearned by the convolutional layers are boosted with a new long short-term\ndependence-aware (LSTD) module, which enables the intermediate convolutional\nfeature maps to perceive the graphical long short-term dependency among\ndifferent hand (or body) parts using the designed Graphical ConvLSTM. Learning\na set of features that are reliable and discriminatively representative of the\npose of a hand (or body) part is difficult due to the ambiguities, texture and\nillumination variation, and self-occlusion in the real application of 3D pose\nestimation. To improve the reliability of the features for representing each\nbody part and enhance the LSTD module, we further introduce a context\nconsistency gate (CCG) in this paper, with which the convolutional feature maps\nare modulated according to their consistency with the context representations.\nWe evaluate the proposed method on challenging benchmark datasets for 3D hand\npose estimation and 3D full body pose estimation. Experimental results show the\neffectiveness of our method that achieves state-of-the-art performance on both\nof the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:20:05 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 07:05:58 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Liu", "Jun", ""], ["Ding", "Henghui", ""], ["Shahroudy", "Amir", ""], ["Duan", "Ling-Yu", ""], ["Jiang", "Xudong", ""], ["Wang", "Gang", ""], ["Kot", "Alex C.", ""]]}, {"id": "1901.04881", "submitter": "Talha Siddiqui", "authors": "Talha A. Siddiqui, Samarth Bharadwaj, Shivkumar Kalyanaraman", "title": "A deep learning approach to solar-irradiance forecasting in sky-videos", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ahead-of-time forecasting of incident solar-irradiance on a panel is\nindicative of expected energy yield and is essential for efficient grid\ndistribution and planning. Traditionally, these forecasts are based on\nmeteorological physics models whose parameters are tuned by coarse-grained\nradiometric tiles sensed from geo-satellites. This research presents a novel\napplication of deep neural network approach to observe and estimate short-term\nweather effects from videos. Specifically, we use time-lapsed videos\n(sky-videos) obtained from upward facing wide-lensed cameras (sky-cameras) to\ndirectly estimate and forecast solar irradiance. We introduce and present\nresults on two large publicly available datasets obtained from weather stations\nin two regions of North America using relatively inexpensive optical hardware.\nThese datasets contain over a million images that span for 1 and 12 years\nrespectively, the largest such collection to our knowledge. Compared to\nsatellite based approaches, the proposed deep learning approach significantly\nreduces the normalized mean-absolute-percentage error for both nowcasting, i.e.\nprediction of the solar irradiance at the instance the frame is captured, as\nwell as forecasting, ahead-of-time irradiance prediction for a duration for\nupto 4 hours.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:24:36 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Siddiqui", "Talha A.", ""], ["Bharadwaj", "Samarth", ""], ["Kalyanaraman", "Shivkumar", ""]]}, {"id": "1901.04889", "submitter": "Yuanyuan Zhang", "authors": "Yuanyuan Zhang, Zi-Rui Wang, Jun Du", "title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for\n  Audio-video Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition (AER) is a challenging task due to the abstract\nconcept and multiple expressions of emotion. Although there is no consensus on\na definition, human emotional states usually can be apperceived by auditory and\nvisual systems. Inspired by this cognitive process in human beings, it's\nnatural to simultaneously utilize audio and visual information in AER. However,\nmost traditional fusion approaches only build a linear paradigm, such as\nfeature concatenation and multi-system fusion, which hardly captures complex\nassociation between audio and video. In this paper, we introduce factorized\nbilinear pooling (FBP) to deeply integrate the features of audio and video.\nSpecifically, the features are selected through the embedded attention\nmechanism from respective modalities to obtain the emotion-related regions. The\nwhole pipeline can be completed in a neural network. Validated on the AFEW\ndatabase of the audio-video sub-challenge in EmotiW2018, the proposed approach\nachieves an accuracy of 62.48%, outperforming the state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:51:39 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhang", "Yuanyuan", ""], ["Wang", "Zi-Rui", ""], ["Du", "Jun", ""]]}, {"id": "1901.04908", "submitter": "Iuliia Kotseruba", "authors": "John K. Tsotsos, Iuliia Kotseruba, Calden Wloka", "title": "Rapid Visual Categorization is not Guided by Early Salience-Based\n  Selection", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0224306", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current dominant visual processing paradigm in both human and machine\nresearch is the feedforward, layered hierarchy of neural-like processing\nelements. Within this paradigm, visual saliency is seen by many to have a\nspecific role, namely that of early selection. Early selection is thought to\nenable very fast visual performance by limiting processing to only the most\nsalient candidate portions of an image. This strategy has led to a plethora of\nsaliency algorithms that have indeed improved processing time efficiency in\nmachine algorithms, which in turn have strengthened the suggestion that human\nvision also employs a similar early selection strategy. However, at least one\nset of critical tests of this idea has never been performed with respect to the\nrole of early selection in human vision. How would the best of the current\nsaliency models perform on the stimuli used by experimentalists who first\nprovided evidence for this visual processing paradigm? Would the algorithms\nreally provide correct candidate sub-images to enable fast categorization on\nthose same images? Do humans really need this early selection for their\nimpressive performance? Here, we report on a new series of tests of these\nquestions whose results suggest that it is quite unlikely that such an early\nselection process has any role in human rapid visual categorization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 16:22:24 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 14:43:38 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 20:58:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Tsotsos", "John K.", ""], ["Kotseruba", "Iuliia", ""], ["Wloka", "Calden", ""]]}, {"id": "1901.04947", "submitter": "Sze Teng Liong", "authors": "Y.S. Gan, Sze-Teng Liong and Yen-Chang Huang", "title": "Automatic Surface Area and Volume Prediction on Ellipsoidal Ham using\n  Deep Learning", "comments": "This paper contains 19 pages, 12 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel methods to predict the surface and volume of the\nham through a camera. This implies that the conventional weight measurement to\nobtain in the object's volume can be neglected and hence it is economically\neffective. Both of the measurements are obtained in the following two ways:\nmanually and automatically. The former is assume as the true or exact\nmeasurement and the latter is through a computer vision technique with some\ngeometrical analysis that includes mathematical derived functions. For the\nautomatic implementation, most of the existing approaches extract the features\nof the food material based on handcrafted features and to the best of our\nknowledge this is the first attempt to estimate the surface area and volume on\nham with deep learning features. We address the estimation task with a Mask\nRegion-based CNN (Mask R-CNN) approach, which well performs the ham detection\nand semantic segmentation from a video. The experimental results demonstrate\nthat the algorithm proposed is robust as promising surface area and volume\nestimation are obtained for two angles of the ellipsoidal ham (i.e., horizontal\nand vertical positions). Specifically, in the vertical ham point of view, it\nachieves an overall accuracy up to 95% whereas the horizontal ham reaches 80%\nof accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 17:26:43 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Gan", "Y. S.", ""], ["Liong", "Sze-Teng", ""], ["Huang", "Yen-Chang", ""]]}, {"id": "1901.04949", "submitter": "Peixian Liang", "authors": "Peixian Liang, Jianxu Chen, Hao Zheng, Lin Yang, Yizhe Zhang, Danny Z.\n  Chen", "title": "Cascade Decoder: A Universal Decoding Method for Biomedical Image\n  Segmentation", "comments": "Accepted at ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Encoder-Decoder architecture is a main stream deep learning model for\nbiomedical image segmentation. The encoder fully compresses the input and\ngenerates encoded features, and the decoder then produces dense predictions\nusing encoded features. However, decoders are still under-explored in such\narchitectures. In this paper, we comprehensively study the state-of-the-art\nEncoder-Decoder architectures, and propose a new universal decoder, called\ncascade decoder, to improve semantic segmentation accuracy. Our cascade decoder\ncan be embedded into existing networks and trained altogether in an end-to-end\nfashion. The cascade decoder structure aims to conduct more effective decoding\nof hierarchically encoded features and is more compatible with common encoders\nthan the known decoders. We replace the decoders of state-of-the-art models\nwith our cascade decoder for several challenging biomedical image segmentation\ntasks, and the considerable improvements achieved demonstrate the efficacy of\nour new decoding method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 17:35:35 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Liang", "Peixian", ""], ["Chen", "Jianxu", ""], ["Zheng", "Hao", ""], ["Yang", "Lin", ""], ["Zhang", "Yizhe", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1901.04988", "submitter": "Teng Wang", "authors": "Teng Wang, Chao Wang, Xuehai Zhou, Huaping Chen", "title": "A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities", "comments": "Some part in the section of introduction dont have the labeling\n  reference. And there are some wrong of data in figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 04:43:25 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 01:34:12 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Teng", ""], ["Wang", "Chao", ""], ["Zhou", "Xuehai", ""], ["Chen", "Huaping", ""]]}, {"id": "1901.05002", "submitter": "Shanghua Xiao", "authors": "Shanghua Xiao", "title": "Light-weighted Saliency Detection with Distinctively Lower Memory Cost\n  and Model Size", "comments": "7 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1809.00644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) based saliency detection approaches have succeed\nin recent years, and improved the performance by a great margin via\nincreasingly sophisticated network architecture. Despite the performance\nimprovement, the computational cost is excessively high for such low level\nvisual task. In this work, we propose a light-weighted saliency detection\napproach with distinctively lower runtime memory cost and model size. We\nevaluated the performance of our approach on multiple benchmark datasets, and\nachieved competitive results comparing with state-of-the-art methods on\nmultiple metrics. We also evaluated the computational cost of our approach with\nmultiple measurements. The runtime memory cost of our approach is 42 to 99\ntimes fewer comparing with the previous DNNs based methods. The model size of\nour approach is 63 to 129 times smaller, and takes less than 1 Megabytes\nstorage space with out any deep compression technique.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 16:15:02 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Xiao", "Shanghua", ""]]}, {"id": "1901.05031", "submitter": "Jeff Calder", "authors": "Mauricio Flores Rios, Jeff Calder, Gilad Lerman", "title": "Algorithms for $\\ell_p$-based semi-supervised learning on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.LG cs.NA math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast algorithms for solving the variational and game-theoretic\n$p$-Laplace equations on weighted graphs for $p>2$. The graph $p$-Laplacian for\n$p>2$ has been proposed recently as a replacement for the standard ($p=2$)\ngraph Laplacian in semi-supervised learning problems with very few labels,\nwhere the minimizer of the graph Laplacian becomes degenerate. We present\nseveral efficient and scalable algorithms for both the variational and\ngame-theoretic formulations, and present numerical results on synthetic data\nand real data that illustrate the effectiveness of the $p$-Laplacian\nformulation for semi-supervised learning with few labels.\n  We also prove new discrete to continuum convergence results for $p$-Laplace\nproblems on $k$-nearest neighbor ($k$-NN) graphs, which are more commonly used\nin practice than random geometric graphs. Our analysis shows that, on $k$-NN\ngraphs, the $p$-Laplacian retains information about the data distribution as\n$p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is sensitive to the data\ndistribution. This situation can be contrasted with random geometric graphs,\nwhere the $p$-Laplacian \\emph{forgets} the data distribution as $p\\to \\infty$.\nFinally, we give a general framework for proving discrete to continuum\nconvergence results in graph-based learning that only requires pointwise\nconsistency and a type of monotonicity.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 20:03:12 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:02:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Rios", "Mauricio Flores", ""], ["Calder", "Jeff", ""], ["Lerman", "Gilad", ""]]}, {"id": "1901.05043", "submitter": "Mohammad Mahdi Dehshibi Dr.", "authors": "Mohammad Mahdi Dehshibi, Jitka Cejkova, Dominik Svara, Andrew\n  Adamatzky", "title": "On complexity of branching droplets in electrical field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CC cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Decanol droplets in a thin layer of sodium decanoate with sodium chloride\nexhibit bifurcation branching growth due to interplay between osmotic pressure,\ndiffusion and surface tension. We aimed to evaluate if morphology of the\nbranching droplets changes when the droplets are subject to electrical\npotential difference. We analysed graph-theoretic structure of the droplets and\napplied several complexity measures. We found that, in overall, the current\nincreases complexity of the branching droplets in terms of number of connected\ncomponents and nodes in their graph presentations, morphological complexity and\ncompressibility.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 20:53:40 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Dehshibi", "Mohammad Mahdi", ""], ["Cejkova", "Jitka", ""], ["Svara", "Dominik", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "1901.05064", "submitter": "Wang Guangjun", "authors": "Guangjun Wang", "title": "A novel 3D display based on micro-volumetric scanning and real time\n  reconstruction of holograms principle", "comments": "arXiv admin note: substantial text overlap with arXiv:1706.03231", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes a novel 3D display contains a micro-volumetric\nscanning system (MVS) and a real time reconstruction hologram system (RTRH).\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 13:06:49 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Wang", "Guangjun", ""]]}, {"id": "1901.05103", "submitter": "Jeong Joon Park", "authors": "Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe,\n  Steven Lovegrove", "title": "DeepSDF: Learning Continuous Signed Distance Functions for Shape\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer graphics, 3D computer vision and robotics communities have produced\nmultiple approaches to representing 3D geometry for rendering and\nreconstruction. These provide trade-offs across fidelity, efficiency and\ncompression capabilities. In this work, we introduce DeepSDF, a learned\ncontinuous Signed Distance Function (SDF) representation of a class of shapes\nthat enables high quality shape representation, interpolation and completion\nfrom partial and noisy 3D input data. DeepSDF, like its classical counterpart,\nrepresents a shape's surface by a continuous volumetric field: the magnitude of\na point in the field represents the distance to the surface boundary and the\nsign indicates whether the region is inside (-) or outside (+) of the shape,\nhence our representation implicitly encodes a shape's boundary as the\nzero-level-set of the learned function while explicitly representing the\nclassification of space as being part of the shapes interior or not. While\nclassical SDF's both in analytical or discretized voxel form typically\nrepresent the surface of a single shape, DeepSDF can represent an entire class\nof shapes. Furthermore, we show state-of-the-art performance for learned 3D\nshape representation and completion while reducing the model size by an order\nof magnitude compared with previous work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 01:21:27 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Park", "Jeong Joon", ""], ["Florence", "Peter", ""], ["Straub", "Julian", ""], ["Newcombe", "Richard", ""], ["Lovegrove", "Steven", ""]]}, {"id": "1901.05104", "submitter": "Bao Zhao", "authors": "Bao Zhao, Xiaobo Chen, Xinyi Le and Juntong Xi", "title": "A Comprehensive Performance Evaluation for 3D Transformation Estimation\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D local feature extraction and matching is the basis for solving many tasks\nin the area of computer vision, such as 3D registration, modeling, recognition\nand retrieval. However, this process commonly draws into false correspondences,\ndue to noise, limited features, occlusion, incomplete surface and etc. In order\nto estimate accurate transformation based on these corrupted correspondences,\nnumerous transformation estimation techniques have been proposed. However, the\nmerits, demerits and appropriate application for these methods are unclear\nowing to that no comprehensive evaluation for the performance of these methods\nhas been conducted. This paper evaluates eleven state-of-the-art transformation\nestimation proposals on both descriptor based and synthetic correspondences. On\ndescriptor based correspondences, several evaluation items (including the\nperformance on different datasets, robustness to different overlap ratios and\nthe performance of these technique combined with Iterative Closest Point (ICP),\ndifferent local features and LRF/A techniques) of these methods are tested on\nfour popular datasets acquired with different devices. On synthetic\ncorrespondences, the robustness of these methods to varying percentages of\ncorrect correspondences (PCC) is evaluated. In addition, we also evaluate the\nefficiencies of these methods. Finally, the merits, demerits and application\nguidance of these tested transformation estimation methods are summarized.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 01:46:46 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Zhao", "Bao", ""], ["Chen", "Xiaobo", ""], ["Le", "Xinyi", ""], ["Xi", "Juntong", ""]]}, {"id": "1901.05105", "submitter": "Xin Huang", "authors": "Xin Huang, Stephen McGill, Brian C. Williams, Luke Fletcher, Guy\n  Rosman", "title": "Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections", "comments": "Accepted at ICRA'19. 8 pages, 9 figures, 1 table. Video at\n  https://youtu.be/clR08hRdtlM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the motion of a driver's vehicle is crucial for advanced driving\nsystems, enabling detection of potential risks towards shared control between\nthe driver and automation systems. In this paper, we propose a variational\nneural network approach that predicts future driver trajectory distributions\nfor the vehicle based on multiple sensors. Our predictor generates both a\nconditional variational distribution of future trajectories, as well as a\nconfidence estimate for different time horizons. Our approach allows us to\nhandle inherently uncertain situations, and reason about information gain from\neach input, as well as combine our model with additional predictors, creating a\nmixture of experts. We show how to augment the variational predictor with a\nphysics-based predictor, and based on their confidence estimations, improve\noverall system performance. The resulting combined model is aware of the\nuncertainty associated with its predictions, which can help the vehicle\nautonomy to make decisions with more confidence. The model is validated on\nreal-world urban driving data collected in multiple locations. This validation\ndemonstrates that our approach improves the prediction error of a physics-based\nmodel by 25% while successfully identifying the uncertain cases with 82%\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 01:46:57 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 03:31:12 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Huang", "Xin", ""], ["McGill", "Stephen", ""], ["Williams", "Brian C.", ""], ["Fletcher", "Luke", ""], ["Rosman", "Guy", ""]]}, {"id": "1901.05107", "submitter": "Debayan Deb", "authors": "Debayan Deb, Arun Ross, Anil K. Jain, Kwaku Prakah-Asante, K.\n  Venkatesh Prasad", "title": "Actions Speak Louder Than (Pass)words: Passive Authentication of\n  Smartphone Users via Deep Temporal Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevailing user authentication schemes on smartphones rely on explicit user\ninteraction, where a user types in a passcode or presents a biometric cue such\nas face, fingerprint, or iris. In addition to being cumbersome and obtrusive to\nthe users, such authentication mechanisms pose security and privacy concerns.\nPassive authentication systems can tackle these challenges by frequently and\nunobtrusively monitoring the user's interaction with the device. In this paper,\nwe propose a Siamese Long Short-Term Memory network architecture for passive\nauthentication, where users can be verified without requiring any explicit\nauthentication step. We acquired a dataset comprising of measurements from 30\nsmartphone sensor modalities for 37 users. We evaluate our approach on 8\ndominant modalities, namely, keystroke dynamics, GPS location, accelerometer,\ngyroscope, magnetometer, linear accelerometer, gravity, and rotation sensors.\nExperimental results find that, within 3 seconds, a genuine user can be\ncorrectly verified 97.15% of the time at a false accept rate of 0.1%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 01:53:48 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Deb", "Debayan", ""], ["Ross", "Arun", ""], ["Jain", "Anil K.", ""], ["Prakah-Asante", "Kwaku", ""], ["Prasad", "K. Venkatesh", ""]]}, {"id": "1901.05127", "submitter": "Yuan Yao", "authors": "Yuan Yao, Jianqiang Ren, Xuansong Xie, Weidong Liu, Yong-Jin Liu, Jun\n  Wang", "title": "Attention-aware Multi-stroke Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer has drawn considerable attention from both academic and\nindustrial field. Although visual effect and efficiency have been significantly\nimproved, existing methods are unable to coordinate spatial distribution of\nvisual attention between the content image and stylized image, or render\ndiverse level of detail via different brush strokes. In this paper, we tackle\nthese limitations by developing an attention-aware multi-stroke style transfer\nmodel. We first propose to assemble self-attention mechanism into a\nstyle-agnostic reconstruction autoencoder framework, from which the attention\nmap of a content image can be derived. By performing multi-scale style swap on\ncontent features and style features, we produce multiple feature maps\nreflecting different stroke patterns. A flexible fusion strategy is further\npresented to incorporate the salient characteristics from the attention map,\nwhich allows integrating multiple stroke patterns into different spatial\nregions of the output image harmoniously. We demonstrate the effectiveness of\nour method, as well as generate comparable stylized images with multiple stroke\npatterns against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 03:40:27 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Yao", "Yuan", ""], ["Ren", "Jianqiang", ""], ["Xie", "Xuansong", ""], ["Liu", "Weidong", ""], ["Liu", "Yong-Jin", ""], ["Wang", "Jun", ""]]}, {"id": "1901.05135", "submitter": "Nikolaos Passalis", "authors": "Nikolaos Passalis and Anastasios Tefas", "title": "Deep Supervised Hashing leveraging Quadratic Spherical Mutual\n  Information for Content-based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep supervised hashing techniques have been proposed to allow for\nefficiently querying large image databases. However, deep supervised image\nhashing techniques are developed, to a great extent, heuristically often\nleading to suboptimal results. Contrary to this, we propose an efficient deep\nsupervised hashing algorithm that optimizes the learned codes using an\ninformation-theoretic measure, the Quadratic Mutual Information (QMI). The\nproposed method is adapted to the needs of large-scale hashing and information\nretrieval leading to a novel information-theoretic measure, the Quadratic\nSpherical Mutual Information (QSMI). Apart from demonstrating the effectiveness\nof the proposed method under different scenarios and outperforming existing\nstate-of-the-art image hashing techniques, this paper provides a structured way\nto model the process of information retrieval and develop novel methods adapted\nto the needs of each application.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 05:03:54 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1901.05179", "submitter": "Gui-Song Xia", "authors": "Fu-Dong Wang and Gui-Song Xia and Nan Xue and Yipeng Zhang and\n  Marcello Pelillo", "title": "A Functional Representation for Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching is an important and persistent problem in computer vision and\npattern recognition for finding node-to-node correspondence between\ngraph-structured data. However, as widely used, graph matching that\nincorporates pairwise constraints can be formulated as a quadratic assignment\nproblem (QAP), which is NP-complete and results in intrinsic computational\ndifficulties. In this paper, we present a functional representation for graph\nmatching (FRGM) that aims to provide more geometric insights on the problem and\nreduce the space and time complexities of corresponding algorithms. To achieve\nthese goals, we represent a graph endowed with edge attributes by a linear\nfunction space equipped with a functional such as inner product or metric, that\nhas an explicit geometric meaning. Consequently, the correspondence between\ngraphs can be represented as a linear representation map of that functional.\nSpecifically, we reformulate the linear functional representation map as a new\nparameterization for Euclidean graph matching, which is associative with\ngeometric parameters for graphs under rigid or nonrigid deformations. This\nallows us to estimate the correspondence and geometric deformations\nsimultaneously. The use of the representation of edge attributes rather than\nthe affinity matrix enables us to reduce the space complexity by two orders of\nmagnitudes. Furthermore, we propose an efficient optimization strategy with low\ntime complexity to optimize the objective function. The experimental results on\nboth synthetic and real-world datasets demonstrate that the proposed FRGM can\nachieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 08:46:21 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Wang", "Fu-Dong", ""], ["Xia", "Gui-Song", ""], ["Xue", "Nan", ""], ["Zhang", "Yipeng", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1901.05203", "submitter": "Sorin Grigorescu", "authors": "Liviu Marina, Bogdan Trasnea, Cocias Tiberiu, Andrei Vasilcoi, Florin\n  Moldoveanu and Sorin Grigorescu", "title": "Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving\n  Context Understanding", "comments": null, "journal-ref": "Int. Conf. on Robotic Computing IRC 2019, Naples, Italy, February\n  25-27, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid maps obtained from fused sensory information are nowadays among the most\npopular approaches for motion planning for autonomous driving cars. In this\npaper, we introduce Deep Grid Net (DGN), a deep learning (DL) system designed\nfor understanding the context in which an autonomous car is driving. DGN\nincorporates a learned driving environment representation based on Occupancy\nGrids (OG) obtained from raw Lidar data and constructed on top of the\nDempster-Shafer (DS) theory. The predicted driving context is further used for\nswitching between different driving strategies implemented within EB robinos,\nElektrobit's Autonomous Driving (AD) software platform. Based on genetic\nalgorithms (GAs), we also propose a neuroevolutionary approach for learning the\ntuning hyperparameters of DGN. The performance of the proposed deep network has\nbeen evaluated against similar competing driving context estimation\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 10:03:08 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Marina", "Liviu", ""], ["Trasnea", "Bogdan", ""], ["Tiberiu", "Cocias", ""], ["Vasilcoi", "Andrei", ""], ["Moldoveanu", "Florin", ""], ["Grigorescu", "Sorin", ""]]}, {"id": "1901.05259", "submitter": "Bodo Kaiser", "authors": "Bodo Kaiser and Shadi Albarqouni", "title": "MRI to CT Translation with GANs", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detailed description and reference implementation of\npreprocessing steps necessary to prepare the public Retrospective Image\nRegistration Evaluation (RIRE) dataset for the task of magnetic resonance\nimaging (MRI) to X-ray computed tomography (CT) translation. Furthermore we\ndescribe and implement three state of the art convolutional neural network\n(CNN) and generative adversarial network (GAN) models where we report\nstatistics and visual results of two of them.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 12:31:08 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Kaiser", "Bodo", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1901.05305", "submitter": "Mustafa Talha Avcu", "authors": "Mustafa Talha Avcu, Zhuo Zhang, Derrick Wei Shih Chan", "title": "Seizure Detection using Least EEG Channels by Deep Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683229", "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work aims to develop an end-to-end solution for seizure onset detection.\nWe design the SeizNet, a Convolutional Neural Network for seizure detection. To\ncompare SeizNet with traditional machine learning approach, a baseline\nclassifier is implemented using spectrum band power features with Support\nVector Machines (BPsvm). We explore the possibility to use the least number of\nchannels for accurate seizure detection by evaluating SeizNet and BPsvm\napproaches using all channels and two channels settings respectively. EEG Data\nis acquired from 29 pediatric patients admitted to KK Woman's and Children's\nHospital who were diagnosed as typical absence seizures. We conduct\nleave-one-out cross validation for all subjects. Using full channel data, BPsvm\nyields a sensitivity of 86.6\\% and 0.84 false alarm (per hour) while SeizNet\nyields overall sensitivity of 95.8 \\% with 0.17 false alarm. More\ninterestingly, two channels seizNet outperforms full channel BPsvm with a\nsensitivity of 93.3\\% and 0.58 false alarm. We further investigate\ninterpretability of SeizNet by decoding the filters learned along convolutional\nlayers. Seizure-like characteristics can be clearly observed in the filters\nfrom third and forth convolutional layers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:31:52 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Avcu", "Mustafa Talha", ""], ["Zhang", "Zhuo", ""], ["Chan", "Derrick Wei Shih", ""]]}, {"id": "1901.05320", "submitter": "Risheng Liu", "authors": "Risheng Liu, Xin Fan, Ming Zhu, Minjun Hou, Zhongxuan Luo", "title": "Real-world Underwater Enhancement: Challenges, Benchmarks, and Solutions", "comments": "arXiv admin note: text overlap with arXiv:1712.04143 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement is such an important low-level vision task with\nmany applications that numerous algorithms have been proposed in recent years.\nThese algorithms developed upon various assumptions demonstrate successes from\nvarious aspects using different data sets and different metrics. In this work,\nwe setup an undersea image capturing system, and construct a large-scale\nReal-world Underwater Image Enhancement (RUIE) data set divided into three\nsubsets. The three subsets target at three challenging aspects for enhancement,\ni.e., image visibility quality, color casts, and higher-level\ndetection/classification, respectively. We conduct extensive and systematic\nexperiments on RUIE to evaluate the effectiveness and limitations of various\nalgorithms to enhance visibility and correct color casts on images with\nhierarchical categories of degradation. Moreover, underwater image enhancement\nin practice usually serves as a preprocessing step for mid-level and high-level\nvision tasks. We thus exploit the object detection performance on enhanced\nimages as a brand new task-specific evaluation criterion. The findings from\nthese evaluations not only confirm what is commonly believed, but also suggest\npromising solutions and new directions for visibility enhancement, color\ncorrection, and object detection on real-world underwater images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 10:36:16 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 06:49:36 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Liu", "Risheng", ""], ["Fan", "Xin", ""], ["Zhu", "Ming", ""], ["Hou", "Minjun", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1901.05355", "submitter": "Shridhar Ravikumar", "authors": "Shridhar Ravikumar", "title": "Lightweight Markerless Monocular Face Capture with 3D Spatial Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple lightweight markerless facial performance capture\nframework using just a monocular video input that combines Active Appearance\nModels for feature tracking and prior constraints on 3D shapes into an\nintegrated objective function. 2D monocular inputs inherently lack information\nalong the depth axis and can lead to physically implausible solutions. In order\nto address this loss of information, we enforce a constraint on our objective\nfunction within a probabilistic framework that uses preexisting animations\nobtained from accurate 3D tracking systems, thus achieving more plausible\nresults. Our system fits a Blendshape model to tracked 2D features while also\nhandling noise in estimation of features and camera parameters. We learn\nseparate constraints for the upper and lower regions of the face thus\nmaintaining flexibility. We show that using this approach, we can obtain\nsignificant improvement in tracking especially along the depth dimension. Our\nmethod uses easily obtainable prior animation data. We show that our method can\ngenerate convincing animations using only a monocular video input. We\nquantitatively evaluate our results comparing it with an approach using a\nmonocular input without our spatial constraints and show that our results are\ncloser to the ground-truth geometry. Finally, we also evaluate the effect that\nthe choice of the Blendshape set has on the results of the solver by solving\nfor a different set of Blendshapes and quantitatively comparing it with our\nprevious results and to the ground truth. We show that while the choice of\nBlendshapes does make a difference, the use of our spatial constraints\ngenerates results that are closer to the ground truth.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 15:51:25 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ravikumar", "Shridhar", ""]]}, {"id": "1901.05362", "submitter": "Hui Men", "authors": "Hui Men, Hanhe Lin, Vlad Hosu, Daniel Maurer, Andres Bruhn, Dietmar\n  Saupe", "title": "Technical Report on Visual Quality Assessment for Frame Interpolation", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current benchmarks for optical flow algorithms evaluate the estimation\nquality by comparing their predicted flow field with the ground truth, and\nadditionally may compare interpolated frames, based on these predictions, with\nthe correct frames from the actual image sequences. For the latter comparisons,\nobjective measures such as mean square errors are applied. However, for\napplications like image interpolation, the expected user's quality of\nexperience cannot be fully deduced from such simple quality measures.\nTherefore, we conducted a subjective quality assessment study by crowdsourcing\nfor the interpolated images provided in one of the optical flow benchmarks, the\nMiddlebury benchmark. We used paired comparisons with forced choice and\nreconstructed absolute quality scale values according to Thurstone's model\nusing the classical least squares method. The results give rise to a re-ranking\nof 141 participating algorithms w.r.t. visual quality of interpolated frames\nmostly based on optical flow estimation. Our re-ranking result shows the\nnecessity of visual quality assessment as another evaluation metric for optical\nflow and frame interpolation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:11:39 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 09:58:37 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Men", "Hui", ""], ["Lin", "Hanhe", ""], ["Hosu", "Vlad", ""], ["Maurer", "Daniel", ""], ["Bruhn", "Andres", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1901.05375", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "DAFE-FD: Density Aware Feature Enrichment for Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on face detection, which is focused primarily on improving\naccuracy of detecting smaller faces, attempt to develop new anchor design\nstrategies to facilitate increased overlap between anchor boxes and ground\ntruth faces of smaller sizes. In this work, we approach the problem of small\nface detection with the motivation of enriching the feature maps using a\ndensity map estimation module. This module, inspired by recent crowd\ncounting/density estimation techniques, performs the task of estimating the per\npixel density of people/faces present in the image. Output of this module is\nemployed to accentuate the feature maps from the backbone network using a\nfeature enrichment module before being used for detecting smaller faces. The\nproposed approach can be used to complement recent anchor-design based novel\nmethods to further improve their results. Experiments conducted on different\ndatasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:32:00 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1901.05376", "submitter": "Faisal Qureshi", "authors": "Tony Joseph, Konstantinos G. Derpanis, Faisal Z. Qureshi", "title": "Joint Spatial and Layer Attention for Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach that learns to sequentially attend\nto different Convolutional Neural Networks (CNN) layers (i.e., ``what'' feature\nabstraction to attend to) and different spatial locations of the selected\nfeature map (i.e., ``where'') to perform the task at hand. Specifically, at\neach Recurrent Neural Network (RNN) step, both a CNN layer and localized\nspatial region within it are selected for further processing. We demonstrate\nthe effectiveness of this approach on two computer vision tasks: (i)\nimage-based six degree of freedom camera pose regression and (ii) indoor scene\nclassification. Empirically, we show that combining the ``what'' and ``where''\naspects of attention improves network performance on both tasks. We evaluate\nour method on standard benchmarks for camera localization (Cambridge, 7-Scenes,\nand TUM-LSI) and for scene classification (MIT-67 Indoor Scenes). For camera\nlocalization our approach reduces the median error by 18.8\\% for position and\n8.2\\% for orientation (averaged over all scenes), and for scene classification\nit improves the mean accuracy by 3.4\\% over previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:32:31 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 11:38:07 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Joseph", "Tony", ""], ["Derpanis", "Konstantinos G.", ""], ["Qureshi", "Faisal Z.", ""]]}, {"id": "1901.05377", "submitter": "Rene Lacher", "authors": "Rene Lacher, Francisco Vasconcelos, Norman Williams, Gerrit\n  Rindermann, John Hipwell, David Hawkes, Danail Stoyanov", "title": "Nonrigid reconstruction of 3D breast surfaces with a low-cost RGBD\n  camera for surgical planning and aesthetic evaluation", "comments": null, "journal-ref": "Medical Image Analysis, Volume 53, April 2019, pp. 11-25", "doi": "10.1016/j.media.2019.01.003", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accounting for 26% of all new cancer cases worldwide, breast cancer remains\nthe most common form of cancer in women. Although early breast cancer has a\nfavourable long-term prognosis, roughly a third of patients suffer from a\nsuboptimal aesthetic outcome despite breast conserving cancer treatment.\nClinical-quality 3D modelling of the breast surface therefore assumes an\nincreasingly important role in advancing treatment planning, prediction and\nevaluation of breast cosmesis. Yet, existing 3D torso scanners are expensive\nand either infrastructure-heavy or subject to motion artefacts. In this paper\nwe employ a single consumer-grade RGBD camera with an ICP-based registration\napproach to jointly align all points from a sequence of depth images\nnon-rigidly. Subtle body deformation due to postural sway and respiration is\nsuccessfully mitigated leading to a higher geometric accuracy through\nregularised locally affine transformations. We present results from 6 clinical\ncases where our method compares well with the gold standard and outperforms a\nprevious approach. We show that our method produces better reconstructions\nqualitatively by visual assessment and quantitatively by consistently obtaining\nlower landmark error scores and yielding more accurate breast volume estimates.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:34:44 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Lacher", "Rene", ""], ["Vasconcelos", "Francisco", ""], ["Williams", "Norman", ""], ["Rindermann", "Gerrit", ""], ["Hipwell", "John", ""], ["Hawkes", "David", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1901.05427", "submitter": "Yi-Hsuan Tsai", "authors": "Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker", "title": "Domain Adaptation for Structured Output via Discriminative Patch\n  Representations", "comments": "Accepted in ICCV'19 (Oral). Project page at\n  http://www.nec-labs.com/~mas/adapt-seg/adapt-seg.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting structured outputs such as semantic segmentation relies on\nexpensive per-pixel annotations to learn supervised models like convolutional\nneural networks. However, models trained on one data domain may not generalize\nwell to other domains without annotations for model finetuning. To avoid the\nlabor-intensive process of annotation, we develop a domain adaptation method to\nadapt the source data to the unlabeled target domain. We propose to learn\ndiscriminative feature representations of patches in the source domain by\ndiscovering multiple modes of patch-wise output distribution through the\nconstruction of a clustered space. With such representations as guidance, we\nuse an adversarial learning scheme to push the feature representations of\ntarget patches in the clustered space closer to the distributions of source\npatches. In addition, we show that our framework is complementary to existing\ndomain adaptation techniques and achieves consistent improvements on semantic\nsegmentation. Extensive ablations and results are demonstrated on numerous\nbenchmark datasets with various settings, such as synthetic-to-real and\ncross-city scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 18:29:54 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 03:06:58 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 03:00:13 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 19:58:56 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Tsai", "Yi-Hsuan", ""], ["Sohn", "Kihyuk", ""], ["Schulter", "Samuel", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1901.05495", "submitter": "Chongyi Li", "authors": "Chongyi Li and Chunle Guo and Wenqi Ren and Runmin Cong and Junhui Hou\n  and Sam Kwong and Dacheng Tao", "title": "An Underwater Image Enhancement Benchmark Dataset and Beyond", "comments": "14 pages", "journal-ref": "IEEE TRANSACTIONS ON IMAGE PROCESSING 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement has been attracting much attention due to its\nsignificance in marine engineering and aquatic robotics. Numerous underwater\nimage enhancement algorithms have been proposed in the last few years. However,\nthese algorithms are mainly evaluated using either synthetic datasets or few\nselected real-world images. It is thus unclear how these algorithms would\nperform on images acquired in the wild and how we could gauge the progress in\nthe field. To bridge this gap, we present the first comprehensive perceptual\nstudy and analysis of underwater image enhancement using large-scale real-world\nimages. In this paper, we construct an Underwater Image Enhancement Benchmark\n(UIEB) including 950 real-world underwater images, 890 of which have the\ncorresponding reference images. We treat the rest 60 underwater images which\ncannot obtain satisfactory reference images as challenging data. Using this\ndataset, we conduct a comprehensive study of the state-of-the-art underwater\nimage enhancement algorithms qualitatively and quantitatively. In addition, we\npropose an underwater image enhancement network (called Water-Net) trained on\nthis benchmark as a baseline, which indicates the generalization of the\nproposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark\nevaluations and the proposed Water-Net demonstrate the performance and\nlimitations of state-of-the-art algorithms, which shed light on future research\nin underwater image enhancement. The dataset and code are available at\nhttps://li-chongyi.github.io/proj_benchmark.html.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 12:19:25 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 10:51:11 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Chunle", ""], ["Ren", "Wenqi", ""], ["Cong", "Runmin", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""], ["Tao", "Dacheng", ""]]}, {"id": "1901.05531", "submitter": "Abhishek Das", "authors": "Abhishek Das, Devi Parikh, Dhruv Batra", "title": "Response to \"Visual Dialogue without Vision or Dialogue\" (Massiceti et\n  al., 2018)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent workshop paper, Massiceti et al. presented a baseline model and\nsubsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what\nwe believe to be unfounded concerns about the dataset and evaluation. This\narticle intends to rebut the critique and clarify potential confusions for\npractitioners and future participants in the Visual Dialog challenge.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 21:27:57 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Das", "Abhishek", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1901.05553", "submitter": "Hugo Oliveira", "authors": "Hugo Oliveira, Edemir Ferreira and Jefersson A. dos Santos", "title": "Truly Generalizable Radiograph Segmentation with Conditional Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization techniques for biomedical images yield different visual patterns\nin radiological exams. These differences may hamper the use of data-driven\napproaches for inference over these images, such as Deep Neural Networks.\nAnother noticeable difficulty in this field is the lack of labeled data, even\nthough in many cases there is an abundance of unlabeled data available.\nTherefore an important step in improving the generalization capabilities of\nthese methods is to perform Unsupervised and Semi-Supervised Domain Adaptation\nbetween different datasets of biomedical images. In order to tackle this\nproblem, in this work we propose an Unsupervised and Semi-Supervised Domain\nAdaptation method for segmentation of biomedical images using Generative\nAdversarial Networks for Unsupervised Image Translation. We merge these\nunsupervised networks with supervised deep semantic segmentation architectures\nin order to create a semi-supervised method capable of learning from both\nunlabeled and labeled data, whenever labeling is available. We compare our\nmethod using several domains, datasets, segmentation tasks and traditional\nbaselines, such as unsupervised distance-based methods and reusing pretrained\nmodels both with and without Fine-tuning. We perform both quantitative and\nqualitative analysis of the proposed method and baselines in the distinct\nscenarios considered in our experimental evaluation. The proposed method shows\nconsistently better results than the baselines in scarce labeled data\nscenarios, achieving Jaccard values greater than 0.9 and good segmentation\nquality in most tasks. Unsupervised Domain Adaptation results were observed to\nbe close to the Fully Supervised Domain Adaptation used in the traditional\nprocedure of Fine-tuning pretrained networks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 22:45:21 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 02:00:42 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 00:42:04 GMT"}, {"version": "v4", "created": "Sat, 7 Dec 2019 01:16:50 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Oliveira", "Hugo", ""], ["Ferreira", "Edemir", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1901.05554", "submitter": "Yen-Liang Lin", "authors": "Xia Li, Yen-Liang Lin, James Miller, Alex Cheon, Walt Dixon", "title": "Primitive-based 3D Building Modeling, Sensor Simulation, and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we begin to consider modeling large, realistic 3D building scenes, it\nbecomes necessary to consider a more compact representation over the polygonal\nmesh model. Due to the large amounts of annotated training data, which is\ncostly to obtain, we leverage synthetic data to train our system for the\nsatellite image domain. By utilizing the synthetic data, we formulate the\nbuilding decomposition as an application of instance segmentation and primitive\nfitting to decompose a building into a set of primitive shapes. Experimental\nresults on WorldView-3 satellite image dataset demonstrate the effectiveness of\nour 3D building modeling approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 22:52:37 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Li", "Xia", ""], ["Lin", "Yen-Liang", ""], ["Miller", "James", ""], ["Cheon", "Alex", ""], ["Dixon", "Walt", ""]]}, {"id": "1901.05555", "submitter": "Yin Cui", "authors": "Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie", "title": "Class-Balanced Loss Based on Effective Number of Samples", "comments": "Code is available at:\n  https://github.com/richardaecn/class-balanced-loss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase of large-scale, real-world datasets, it becomes\ncritical to address the problem of long-tailed data distribution (i.e., a few\nclasses account for most of the data, while most classes are\nunder-represented). Existing solutions typically adopt class re-balancing\nstrategies such as re-sampling and re-weighting based on the number of\nobservations for each class. In this work, we argue that as the number of\nsamples increases, the additional benefit of a newly added data point will\ndiminish. We introduce a novel theoretical framework to measure data overlap by\nassociating with each sample a small neighboring region rather than a single\npoint. The effective number of samples is defined as the volume of samples and\ncan be calculated by a simple formula $(1-\\beta^{n})/(1-\\beta)$, where $n$ is\nthe number of samples and $\\beta \\in [0,1)$ is a hyperparameter. We design a\nre-weighting scheme that uses the effective number of samples for each class to\nre-balance the loss, thereby yielding a class-balanced loss. Comprehensive\nexperiments are conducted on artificially induced long-tailed CIFAR datasets\nand large-scale datasets including ImageNet and iNaturalist. Our results show\nthat when trained with the proposed class-balanced loss, the network is able to\nachieve significant performance gains on long-tailed datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 23:03:45 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Cui", "Yin", ""], ["Jia", "Menglin", ""], ["Lin", "Tsung-Yi", ""], ["Song", "Yang", ""], ["Belongie", "Serge", ""]]}, {"id": "1901.05556", "submitter": "Gladys Hilasaca", "authors": "Gladys Hilasaca and Fernando Paulovich", "title": "Visual Feature Fusion and its Application to Support Unsupervised\n  Clustering Tasks", "comments": "15 pages, 21 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On visual analytics applications, the concept of putting the user on the loop\nrefers to the ability to replace heuristics by user knowledge on machine\nlearning and data mining tasks. On supervised tasks, the user engagement occurs\nvia the manipulation of the training data. However, on unsupervised tasks, the\nuser involvement is limited to changes in the algorithm parametrization or the\ninput data representation, also known as features. Depending on the application\ndomain, different types of features can be extracted from the raw data.\nTherefore, the result of unsupervised algorithms heavily depends on the type of\nemployed feature. Since there is no perfect feature extractor, combining\ndifferent features have been explored in a process called feature fusion. The\nfeature fusion is straightforward when the machine learning or data mining task\nhas a cost function. However, when such a function does not exist, user support\nfor combination needs to be provided otherwise the process is impractical. In\nthis paper, we present a novel feature fusion approach that uses small data\nsamples to allows users not only to effortless control the combination of\ndifferent feature sets but also to interpret the attained results. The\neffectiveness of our approach is confirmed by a comprehensive set of\nqualitative and quantitative tests, opening up different possibilities of\nuser-guided analytical scenarios not covered yet. The ability of our approach\nto providing real-time feedback for the feature fusion is exploited on the\ncontext of unsupervised clustering techniques, where the composed groups\nreflect the semantics of the feature combination.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 23:08:36 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Hilasaca", "Gladys", ""], ["Paulovich", "Fernando", ""]]}, {"id": "1901.05567", "submitter": "Weikai Chen", "authors": "Shichen Liu, Weikai Chen, Tianye Li, Hao Li", "title": "Soft Rasterizer: Differentiable Rendering for Unsupervised Single-View\n  Mesh Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering is the process of generating 2D images from 3D assets, simulated in\na virtual environment, typically with a graphics pipeline. By inverting such\nrenderer, one can think of a learning approach to predict a 3D shape from an\ninput image. However, standard rendering pipelines involve a fundamental\ndiscretization step called rasterization, which prevents the rendering process\nto be differentiable, hence suitable for learning. We present the first\nnon-parametric and truly differentiable rasterizer based on silhouettes. Our\nmethod enables unsupervised learning for high-quality 3D mesh reconstruction\nfrom a single image. We call our framework `soft rasterizer' as it provides an\naccurate soft approximation of the standard rasterizer. The key idea is to fuse\nthe probabilistic contributions of all mesh triangles with respect to the\nrendered pixels. When combined with a mesh generator in a deep neural network,\nour soft rasterizer is able to generate an approximated silhouette of the\ngenerated polygon mesh in the forward pass. The rendering loss is\nback-propagated to supervise the mesh generation without the need of 3D\ntraining data. Experimental results demonstrate that our approach significantly\noutperforms the state-of-the-art unsupervised techniques, both quantitatively\nand qualitatively. We also show that our soft rasterizer can achieve comparable\nresults to the cutting-edge supervised learning method and in various cases\neven better ones, especially for real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 00:00:58 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 00:32:24 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Liu", "Shichen", ""], ["Chen", "Weikai", ""], ["Li", "Tianye", ""], ["Li", "Hao", ""]]}, {"id": "1901.05590", "submitter": "William Whitney", "authors": "William F. Whitney and Rob Fergus", "title": "Disentangling Video with Independent Prediction", "comments": "Presented at the Learning Disentangled Representations: from\n  Perception to Control workshop at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised variational model for disentangling video into\nindependent factors, i.e. each factor's future can be predicted from its past\nwithout considering the others. We show that our approach often learns factors\nwhich are interpretable as objects in a scene.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 02:21:24 GMT"}], "update_date": "2019-01-27", "authors_parsed": [["Whitney", "William F.", ""], ["Fergus", "Rob", ""]]}, {"id": "1901.05599", "submitter": "Michael Iuzzolino", "authors": "Michael L. Iuzzolino, Michael E. Walker, Daniel Szafir", "title": "Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails", "comments": "iROS 2018", "journal-ref": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS) (pp. 576-582)", "doi": "10.1109/IROS.2018.8593883", "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots hold promise in many scenarios involving outdoor use, such as\nsearch-and-rescue, wildlife management, and collecting data to improve\nenvironment, climate, and weather forecasting. However, autonomous navigation\nof outdoor trails remains a challenging problem. Recent work has sought to\naddress this issue using deep learning. Although this approach has achieved\nstate-of-the-art results, the deep learning paradigm may be limited due to a\nreliance on large amounts of annotated training data. Collecting and curating\ntraining datasets may not be feasible or practical in many situations,\nespecially as trail conditions may change due to seasonal weather variations,\nstorms, and natural erosion. In this paper, we explore an approach to address\nthis issue through virtual-to-real-world transfer learning using a variety of\ndeep learning models trained to classify the direction of a trail in an image.\nOur approach utilizes synthetic data gathered from virtual environments for\nmodel training, bypassing the need to collect a large amount of real images of\nthe outdoors. We validate our approach in three main ways. First, we\ndemonstrate that our models achieve classification accuracies upwards of 95% on\nour synthetic data set. Next, we utilize our classification models in the\ncontrol system of a simulated robot to demonstrate feasibility. Finally, we\nevaluate our models on real-world trail data and demonstrate the potential of\nvirtual-to-real-world transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 03:11:58 GMT"}], "update_date": "2019-01-27", "authors_parsed": [["Iuzzolino", "Michael L.", ""], ["Walker", "Michael E.", ""], ["Szafir", "Daniel", ""]]}, {"id": "1901.05602", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Jian Zhao, Mei Xie, Guodong Du, Hengsheng Zhang, Jianshu\n  Li, Zheng Ma, and Jiashi Feng", "title": "Learning Generalizable and Identity-Discriminative Representations for\n  Face Anti-Spoofing", "comments": "8 pages; 8 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (a.k.a presentation attack detection) has drawn growing\nattention due to the high-security demand in face authentication systems.\nExisting CNN-based approaches usually well recognize the spoofing faces when\ntraining and testing spoofing samples display similar patterns, but their\nperformance would drop drastically on testing spoofing faces of unseen scenes.\nIn this paper, we try to boost the generalizability and applicability of these\nmethods by designing a CNN model with two major novelties. First, we propose a\nsimple yet effective Total Pairwise Confusion (TPC) loss for CNN training,\nwhich enhances the generalizability of the learned Presentation Attack (PA)\nrepresentations. Secondly, we incorporate a Fast Domain Adaptation (FDA)\ncomponent into the CNN model to alleviate negative effects brought by domain\nchanges. Besides, our proposed model, which is named Generalizable Face\nAuthentication CNN (GFA-CNN), works in a multi-task manner, performing face\nanti-spoofing and face recognition simultaneously. Experimental results show\nthat GFA-CNN outperforms previous face anti-spoofing approaches and also well\npreserves the identity information of input face images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 03:24:52 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zhao", "Jian", ""], ["Xie", "Mei", ""], ["Du", "Guodong", ""], ["Zhang", "Hengsheng", ""], ["Li", "Jianshu", ""], ["Ma", "Zheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1901.05613", "submitter": "Sanjay Saha", "authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed,\n  Bilkis Jamal Ferdosi, Sanjay Saha and Md. Shopon", "title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 04:27:34 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Ahmed", "Shahjalal", ""], ["Islam", "Md. Rafiqul", ""], ["Hassan", "Jahid", ""], ["Ahmed", "Minhaz Uddin", ""], ["Ferdosi", "Bilkis Jamal", ""], ["Saha", "Sanjay", ""], ["Shopon", "Md.", ""]]}, {"id": "1901.05633", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Hengsheng Zhang, Mei Xie, Yao Luo, Yuefei Zhang, Zheng\n  Ma", "title": "Deep Transfer Across Domains for Face Anti-spoofing", "comments": "8 pages; 3 figures; 2 tables", "journal-ref": null, "doi": "10.1117/1.JEI.28.4.043001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical face recognition system demands not only high recognition\nperformance, but also the capability of detecting spoofing attacks. While\nemerging approaches of face anti-spoofing have been proposed in recent years,\nmost of them do not generalize well to new database. The generalization ability\nof face anti-spoofing needs to be significantly improved before they can be\nadopted by practical application systems. The main reason for the poor\ngeneralization of current approaches is the variety of materials among the\nspoofing devices. As the attacks are produced by putting a spoofing display\n(e.t., paper, electronic screen, forged mask) in front of a camera, the variety\nof spoofing materials can make the spoofing attacks quite different.\nFurthermore, the background/lighting condition of a new environment can make\nboth the real accesses and spoofing attacks different. Another reason for the\npoor generalization is that limited labeled data is available for training in\nface anti-spoofing. In this paper, we focus on improving the generalization\nability across different kinds of datasets. We propose a CNN framework using\nsparsely labeled data from the target domain to learn features that are\ninvariant across domains for face anti-spoofing. Experiments on public-domain\nface spoofing databases show that the proposed method significantly improve the\ncross-dataset testing performance only with a small number of labeled samples\nfrom the target domain.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 05:49:07 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 06:18:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zhang", "Hengsheng", ""], ["Xie", "Mei", ""], ["Luo", "Yao", ""], ["Zhang", "Yuefei", ""], ["Ma", "Zheng", ""]]}, {"id": "1901.05634", "submitter": "Lakmal Meegahapola", "authors": "Madhawa Vidanapathirana, Lakmal Meegahapola, Indika Perera", "title": "Cognitive Analysis of 360 degree Surround Photos", "comments": "IEEE Future Technologies Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360 degrees surround photography or photospheres have taken the world by\nstorm as the new media for content creation providing viewers rich, immersive\nexperience compared to conventional photography. With the emergence of Virtual\nReality as a mainstream trend, the 360 degrees photography is increasingly\nimportant to offer a practical approach to the general public to capture\nvirtual reality ready content from their mobile phones without explicit tool\nsupport or knowledge. Even though the amount of 360-degree surround content\nbeing uploaded to the Internet continues to grow, there is no proper way to\nindex them or to process them for further information. This is because of the\ndifficulty in image processing the photospheres due to the distorted nature of\nobjects embedded. This challenge lies in the way 360-degree panoramic\nphotospheres are saved. This paper presents a unique, and innovative technique\nnamed Photosphere to Cognition Engine (P2CE), which allows cognitive analysis\non 360-degree surround photos using existing image cognitive analysis\nalgorithms and APIs designed for conventional photos. We have optimized the\nsystem using a wide variety of indoor and outdoor samples and extensive\nevaluation approaches. On average, P2CE provides up-to 100% growth in accuracy\non image cognitive analysis of Photospheres over direct use of conventional\nnon-photosphere based Image Cognition Systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 05:50:00 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Vidanapathirana", "Madhawa", ""], ["Meegahapola", "Lakmal", ""], ["Perera", "Indika", ""]]}, {"id": "1901.05635", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Hengsheng Zhang, Mei Xie, Yao Luo, Yuefei Zhang, Zheng\n  Ma", "title": "Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM\n  Architecture", "comments": "20 pages; 3 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal information is very important to capture the discriminative\ncues between genuine and fake faces from video sequences. To explore such a\ntemporal feature, the fine-grained motions (e.g., eye blinking, mouth movements\nand head swing) across video frames are very critical. In this paper, we\npropose a joint CNN-LSTM network for face anti-spoofing, focusing on the motion\ncues across video frames. We first extract the high discriminative features of\nvideo frames using the conventional Convolutional Neural Network (CNN). Then we\nleverage Long Short-Term Memory (LSTM) with the extracted features as inputs to\ncapture the temporal dynamics in videos. To ensure the fine-grained motions\nmore easily to be perceived in the training process, the eulerian motion\nmagnification is used as the preprocessing to enhance the facial expressions\nexhibited by individuals, and the attention mechanism is embedded in LSTM to\nensure the model learn to focus selectively on the dynamic frames across the\nvideo clips. Experiments on Replay Attack and MSU-MFSD databases show that the\nproposed method yields state-of-the-art performance with better generalization\nability compared with several other popular algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 05:58:22 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zhang", "Hengsheng", ""], ["Xie", "Mei", ""], ["Luo", "Yao", ""], ["Zhang", "Yuefei", ""], ["Ma", "Zheng", ""]]}, {"id": "1901.05657", "submitter": "Lu Liu", "authors": "Lu Liu, Robby T. Tan", "title": "Certainty Driven Consistency Loss on Multi-Teacher Networks for\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the successful approaches in semi-supervised learning is based on the\nconsistency regularization. Typically, a student model is trained to be\nconsistent with teacher prediction for the inputs under different\nperturbations. To be successful, the prediction targets given by teacher should\nhave good quality, otherwise the student can be misled by teacher.\nUnfortunately, existing methods do not assess the quality of the teacher\ntargets. In this paper, we propose a novel Certainty-driven Consistency Loss\n(CCL) that exploits the predictive uncertainty in the consistency loss to let\nthe student dynamically learn from reliable targets. Specifically, we propose\ntwo approaches, i.e. Filtering CCL and Temperature CCL to either filter out\nuncertain predictions or pay less attention on them in the consistency\nregularization. We further introduce a novel decoupled framework to encourage\nmodel difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100\ndemonstrate the advantages of our method over a few existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 07:30:44 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 11:09:06 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 08:46:19 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 03:54:27 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 10:16:46 GMT"}, {"version": "v6", "created": "Fri, 31 Jul 2020 02:02:56 GMT"}, {"version": "v7", "created": "Fri, 7 May 2021 07:29:46 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Liu", "Lu", ""], ["Tan", "Robby T.", ""]]}, {"id": "1901.05676", "submitter": "Xueying Wang", "authors": "Xueying Wang, Lei Liu, Guangli Li, Xiao Dong, Peng Zhao, Xiaobing Feng", "title": "Background subtraction on depth videos with convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction is a significant component of computer vision systems.\nIt is widely used in video surveillance, object tracking, anomaly detection,\netc. A new data source for background subtraction appeared as the emergence of\nlow-cost depth sensors like Microsof t Kinect, Asus Xtion PRO, etc. In this\npaper, we propose a background subtraction approach on depth videos, which is\nbased on convolutional neural networks (CNNs), called BGSNet-D (BackGround\nSubtraction neural Networks for Depth videos). The method can be used in color\nunavailable scenarios like poor lighting situations, and can also be applied to\ncombine with existing RGB background subtraction methods. A preprocessing\nstrategy is designed to reduce the influences incurred by noise from depth\nsensors. The experimental results on the SBM-RGBD dataset show that the\nproposed method outperforms existing methods on depth data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 08:17:35 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Xueying", ""], ["Liu", "Lei", ""], ["Li", "Guangli", ""], ["Dong", "Xiao", ""], ["Zhao", "Peng", ""], ["Feng", "Xiaobing", ""]]}, {"id": "1901.05686", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Image Enhancement Network Trained by Using HDR images", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel image enhancement network is proposed, where HDR\nimages are used for generating training data for our network. Most of\nconventional image enhancement methods, including Retinex based methods, do not\ntake into account restoring lost pixel values caused by clipping and\nquantizing. In addition, recently proposed CNN based methods still have a\nlimited scope of application or a limited performance, due to network\narchitectures. In contrast, the proposed method have a higher performance and a\nsimpler network architecture than existing CNN based methods. Moreover, the\nproposed method enables us to restore lost pixel values. Experimental results\nshow that the proposed method can provides higher-quality images than\nconventional image enhancement methods including a CNN based method, in terms\nof TMQI and NIQE.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 09:04:40 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 00:57:51 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1901.05733", "submitter": "Mostafa Salem Dr", "authors": "Mostafa Salem, Sergi Valverde, Mariano Cabezas, Deborah Pareto, Arnau\n  Oliver, Joaquim Salvi, \\`Alex Rovira, Xavier Llad\\'o", "title": "Multiple Sclerosis Lesion Synthesis in MRI using an encoder-decoder\n  U-NET", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose generating synthetic multiple sclerosis (MS)\nlesions on MRI images with the final aim to improve the performance of\nsupervised machine learning algorithms, therefore avoiding the problem of the\nlack of available ground truth. We propose a two-input two-output fully\nconvolutional neural network model for MS lesion synthesis in MRI images. The\nlesion information is encoded as discrete binary intensity level masks passed\nto the model and stacked with the input images. The model is trained end-to-end\nwithout the need for manually annotating the lesions in the training set. We\nthen perform the generation of synthetic lesions on healthy images via\nregistration of patient images, which are subsequently used for data\naugmentation to increase the performance for supervised MS lesion detection\nalgorithms. Our pipeline is evaluated on MS patient data from an in-house\nclinical dataset and the public ISBI2015 challenge dataset. The evaluation is\nbased on measuring the similarities between the real and the synthetic images\nas well as in terms of lesion detection performance by segmenting both the\noriginal and synthetic images individually using a state-of-the-art\nsegmentation framework. We also demonstrate the usage of synthetic MS lesions\ngenerated on healthy images as data augmentation. We analyze a scenario of\nlimited training data (one-image training) to demonstrate the effect of the\ndata augmentation on both datasets. Our results significantly show the\neffectiveness of the usage of synthetic MS lesion images. For the ISBI2015\nchallenge, our one-image model trained using only a single image plus the\nsynthetic data augmentation strategy showed a performance similar to that of\nother CNN methods that were fully trained using the entire training set,\nyielding a comparable human expert rater performance\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:25:50 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Salem", "Mostafa", ""], ["Valverde", "Sergi", ""], ["Cabezas", "Mariano", ""], ["Pareto", "Deborah", ""], ["Oliver", "Arnau", ""], ["Salvi", "Joaquim", ""], ["Rovira", "\u00c0lex", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1901.05742", "submitter": "Zhiyuan Chen", "authors": "Zhiyuan Chen, Annan Li, Yunhong Wang", "title": "A Temporal Attentive Approach for Video-Based Pedestrian Attribute\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first tackle the problem of pedestrian attribute\nrecognition by video-based approach. The challenge mainly lies in spatial and\ntemporal modeling and how to integrating them for effective and dynamic\npedestrian representation. To solve this problem, a novel multi-task model\nbased on the conventional neural network and temporal attention strategy is\nproposed. Since publicly available dataset is rare, two new large-scale video\ndatasets with expanded attribute definition are presented, on which the\neffectiveness of both video-based pedestrian attribute recognition methods and\nthe proposed new network architecture is well demonstrated. The two datasets\nare published on http://irip.buaa.edu.cn/mars_duke_attributes/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:52:39 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 05:40:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Zhiyuan", ""], ["Li", "Annan", ""], ["Wang", "Yunhong", ""]]}, {"id": "1901.05743", "submitter": "Icaro Cavalcante Dourado", "authors": "Icaro Cavalcante Dourado, Daniel Carlos Guimar\\~aes Pedronette,\n  Ricardo da Silva Torres", "title": "Unsupervised Graph-based Rank Aggregation for Improved Retrieval", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipm.2019.03.008", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust and comprehensive graph-based rank aggregation\napproach, used to combine results of isolated ranker models in retrieval tasks.\nThe method follows an unsupervised scheme, which is independent of how the\nisolated ranks are formulated. Our approach is able to combine arbitrary\nmodels, defined in terms of different ranking criteria, such as those based on\ntextual, image or hybrid content representations.\n  We reformulate the ad-hoc retrieval problem as a document retrieval based on\nfusion graphs, which we propose as a new unified representation model capable\nof merging multiple ranks and expressing inter-relationships of retrieval\nresults automatically. By doing so, we claim that the retrieval system can\nbenefit from learning the manifold structure of datasets, thus leading to more\neffective results. Another contribution is that our graph-based aggregation\nformulation, unlike existing approaches, allows for encapsulating contextual\ninformation encoded from multiple ranks, which can be directly used for\nranking, without further computations and post-processing steps over the\ngraphs. Based on the graphs, a novel similarity retrieval score is formulated\nusing an efficient computation of minimum common subgraphs. Finally, another\nbenefit over existing approaches is the absence of hyperparameters.\n  A comprehensive experimental evaluation was conducted considering diverse\nwell-known public datasets, composed of textual, image, and multimodal\ndocuments. Performed experiments demonstrate that our method reaches top\nperformance, yielding better effectiveness scores than state-of-the-art\nbaseline methods and promoting large gains over the rankers being fused, thus\ndemonstrating the successful capability of the proposal in representing queries\nbased on a unified graph-based model of rank fusions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:55:04 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 23:09:28 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Dourado", "Icaro Cavalcante", ""], ["Pedronette", "Daniel Carlos Guimar\u00e3es", ""], ["Torres", "Ricardo da Silva", ""]]}, {"id": "1901.05770", "submitter": "Wei Liu", "authors": "Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong", "title": "SAFE: Scale Aware Feature Encoder for Scene Text Recognition", "comments": "ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we address the problem of having characters with different\nscales in scene text recognition. We propose a novel scale aware feature\nencoder (SAFE) that is designed specifically for encoding characters with\ndifferent scales. SAFE is composed of a multi-scale convolutional encoder and a\nscale attention network. The multi-scale convolutional encoder targets at\nextracting character features under multiple scales, and the scale attention\nnetwork is responsible for selecting features from the most relevant scale(s).\nSAFE has two main advantages over the traditional single-CNN encoder used in\ncurrent state-of-the-art text recognizers. First, it explicitly tackles the\nscale problem by extracting scale-invariant features from the characters. This\nallows the recognizer to put more effort in handling other challenges in scene\ntext recognition, like those caused by view distortion and poor image quality.\nSecond, it can transfer the learning of feature encoding across different\ncharacter scales. This is particularly important when the training set has a\nvery unbalanced distribution of character scales, as training with such a\ndataset will make the encoder biased towards extracting features from the\npredominant scale. To evaluate the effectiveness of SAFE, we design a simple\ntext recognizer named scale-spatial attention network (S-SAN) that employs SAFE\nas its feature encoder, and carry out experiments on six public benchmarks.\nExperimental results demonstrate that S-SAN can achieve state-of-the-art (or,\nin some cases, extremely competitive) performance without any post-processing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 12:58:19 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Chaofeng", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1901.05773", "submitter": "Shizuo Kaji", "authors": "S. Kida, S. Kaji, K. Nawa, T. Imae, T. Nakamoto, S. Ozaki, T. Ohta, Y.\n  Nozawa, K. Nakagawa", "title": "Visual enhancement of Cone-beam CT by use of CycleGAN", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13963", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cone-beam computed tomography (CBCT) offers advantages over conventional\nfan-beam CT in that it requires a shorter time and less exposure to obtain\nimages. CBCT has found a wide variety of applications in patient positioning\nfor image-guided radiation therapy, extracting radiomic information for\ndesigning patient-specific treatment, and computing fractional dose\ndistributions for adaptive radiation therapy. However, CBCT images suffer from\nlow soft-tissue contrast, noise, and artifacts compared to conventional\nfan-beam CT images. Therefore, it is essential to improve the image quality of\nCBCT. In this paper, we propose a synthetic approach to translate CBCT images\nwith deep neural networks. Our method requires only unpaired and unaligned CBCT\nimages and planning fan-beam CT (PlanCT) images for training. Once trained, 3D\nreconstructed CBCT images can be directly translated to high-quality\nPlanCT-like images. We demonstrate the effectiveness of our method with images\nobtained from 24 prostate patients, and we provide a statistical and visual\ncomparison. The image quality of the translated images shows substantial\nimprovement in voxel values, spatial uniformity, and artifact suppression\ncompared to those of the original CBCT. The anatomical structures of the\noriginal CBCT images were also well preserved in the translated images. Our\nmethod enables more accurate adaptive radiation therapy, and opens up new\napplications for CBCT that hinge on high-quality images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 13:02:59 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 03:43:02 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 04:46:55 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kida", "S.", ""], ["Kaji", "S.", ""], ["Nawa", "K.", ""], ["Imae", "T.", ""], ["Nakamoto", "T.", ""], ["Ozaki", "S.", ""], ["Ohta", "T.", ""], ["Nozawa", "Y.", ""], ["Nakagawa", "K.", ""]]}, {"id": "1901.05798", "submitter": "Jiabao Wang", "authors": "Jiabao Wang, Yang Li, Zhuang Miao", "title": "Ensemble Feature for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person re-identification (re-ID), the key task is feature representation,\nwhich is used to compute distance or similarity in prediction. Person re-ID\nachieves great improvement when deep learning methods are introduced to tackle\nthis problem. The features extracted by convolutional neural networks (CNN) are\nmore effective and discriminative than the hand-crafted features. However, deep\nfeature extracted by a single CNN network is not robust enough in testing\nstage. To improve the ability of feature representation, we propose a new\nensemble network (EnsembleNet) by dividing a single network into multiple\nend-to-end branches. The ensemble feature is obtained by concatenating each of\nthe branch features to represent a person. EnsembleNet is designed based on\nResNet-50 and its backbone shares most of the parameters for saving computation\nand memory cost. Experimental results show that our EnsembleNet achieves the\nstate-of-the-art performance on the public Market1501, DukeMTMC-reID and CUHK03\nperson re-ID benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:17:03 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Jiabao", ""], ["Li", "Yang", ""], ["Miao", "Zhuang", ""]]}, {"id": "1901.05807", "submitter": "Lei Fan", "authors": "Yucai Bai, Lei Fan, Ziyu Pan and Long Chen", "title": "Monocular Outdoor Semantic Mapping with a Multi-task Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotic applications, especially for the autonomous driving,\nunderstanding the semantic information and the geometric structure of\nsurroundings are both essential. Semantic 3D maps, as a carrier of the\nenvironmental knowledge, are then intensively studied for their abilities and\napplications. However, it is still challenging to produce a dense outdoor\nsemantic map from a monocular image stream. Motivated by this target, in this\npaper, we propose a method for large-scale 3D reconstruction from consecutive\nmonocular images. First, with the correlation of underlying information between\ndepth and semantic prediction, a novel multi-task Convolutional Neural Network\n(CNN) is designed for joint prediction. Given a single image, the network\nlearns low-level information with a shared encoder and separately predicts with\ndecoders containing additional Atrous Spatial Pyramid Pooling (ASPP) layers and\nthe residual connection which merits disparities and semantic mutually. To\novercome the inconsistency of monocular depth prediction for reconstruction,\npost-processing steps with the superpixelization and the effective 3D\nrepresentation approach are obtained to give the final semantic map.\nExperiments are compared with other methods on both semantic labeling and depth\nprediction. We also qualitatively demonstrate the map reconstructed from\nlarge-scale, difficult monocular image sequences to prove the effectiveness and\nsuperiority.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:31:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 05:45:32 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 01:35:15 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bai", "Yucai", ""], ["Fan", "Lei", ""], ["Pan", "Ziyu", ""], ["Chen", "Long", ""]]}, {"id": "1901.05808", "submitter": "Sumanth Chennupati", "authors": "Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani and Samir Rawashdeh", "title": "AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated\n  Driving", "comments": "Accepted as a Short Paper for a poster presentation at VISAPP 2019", "journal-ref": null, "doi": "10.5220/0007684106450652", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in automated driving is highly specific to the environment\nand thus semantic segmentation plays a key role in recognizing the objects in\nthe environment around the car. Pixel level classification once considered a\nchallenging task which is now becoming mature to be productized in a car.\nHowever, semantic annotation is time consuming and quite expensive. Synthetic\ndatasets with domain adaptation techniques have been used to alleviate the lack\nof large annotated datasets. In this work, we explore an alternate approach of\nleveraging the annotations of other tasks to improve semantic segmentation.\nRecently, multi-task learning became a popular paradigm in automated driving\nwhich demonstrates joint learning of multiple tasks improves overall\nperformance of each tasks. Motivated by this, we use auxiliary tasks like depth\nestimation to improve the performance of semantic segmentation task. We propose\nadaptive task loss weighting techniques to address scale issues in multi-task\nloss functions which become more crucial in auxiliary tasks. We experimented on\nautomotive datasets including SYNTHIA and KITTI and obtained 3% and 5%\nimprovement in accuracy respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:32:06 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Chennupati", "Sumanth", ""], ["Sistu", "Ganesh", ""], ["Yogamani", "Senthil", ""], ["Rawashdeh", "Samir", ""]]}, {"id": "1901.05811", "submitter": "Vinay Kumar", "authors": "Vinay Kumar and Vivek Singh Bawa", "title": "No reference image quality assessment metric based on regional mutual\n  information among images", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the inclusion of camera in daily life, an automatic no reference image\nquality evaluation index is required for automatic classification of images.\nThe present manuscripts proposes a new No Reference Regional Mutual Information\nbased technique for evaluating the quality of an image. We use regional mutual\ninformation on subsets of the complete image. Proposed technique is tested on\nfour benchmark natural image databases, and one benchmark synthetic database. A\ncomparative analysis with classical and state-of-art methods indicate\nsuperiority of the present technique for high quality images and comparable for\nother images of the respective databases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:36:30 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Kumar", "Vinay", ""], ["Bawa", "Vivek Singh", ""]]}, {"id": "1901.05876", "submitter": "Bin Kong", "authors": "Eric Wu, Bin Kong, Xin Wang, Junjie Bai, Yi Lu, Feng Gao, Shaoting\n  Zhang, Kunlin Cao, Qi Song, Siwei Lyu, Youbing Yin", "title": "Residual Attention based Network for Hand Bone Age Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized automatic methods have been employed to boost the productivity\nas well as objectiveness of hand bone age assessment. These approaches make\npredictions according to the whole X-ray images, which include other objects\nthat may introduce distractions. Instead, our framework is inspired by the\nclinical workflow (Tanner-Whitehouse) of hand bone age assessment, which\nfocuses on the key components of the hand. The proposed framework is composed\nof two components: a Mask R-CNN subnet of pixelwise hand segmentation and a\nresidual attention network for hand bone age assessment. The Mask R-CNN subnet\nsegments the hands from X-ray images to avoid the distractions of other objects\n(e.g., X-ray tags). The hierarchical attention components of the residual\nattention subnet force our network to focus on the key components of the X-ray\nimages and generate the final predictions as well as the associated visual\nsupports, which is similar to the assessment procedure of clinicians. We\nevaluate the performance of the proposed pipeline on the RSNA pediatric bone\nage dataset and the results demonstrate its superiority over the previous\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 23:09:32 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wu", "Eric", ""], ["Kong", "Bin", ""], ["Wang", "Xin", ""], ["Bai", "Junjie", ""], ["Lu", "Yi", ""], ["Gao", "Feng", ""], ["Zhang", "Shaoting", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Lyu", "Siwei", ""], ["Yin", "Youbing", ""]]}, {"id": "1901.05880", "submitter": "Francis Tom", "authors": "Debarghya China, Francis Tom, Sumanth Nandamuri, Aupendu Kar,\n  Mukundhan Srinivasan, Pabitra Mitra, Debdoot Sheet", "title": "UltraCompression: Framework for High Density Compression of Ultrasound\n  Volumes using Physics Modeling Deep Neural Networks", "comments": "To appear in the Proceedings of the 2019 IEEE International Symposium\n  on Biomedical Imaging (ISBI 2019); First three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound image compression by preserving speckle-based key information is a\nchallenging task. In this paper, we introduce an ultrasound image compression\nframework with the ability to retain realism of speckle appearance despite\nachieving very high-density compression factors. The compressor employs a\ntissue segmentation method, transmitting segments along with transducer\nfrequency, number of samples and image size as essential information required\nfor decompression. The decompressor is based on a convolutional network trained\nto generate patho-realistic ultrasound images which convey essential\ninformation pertinent to tissue pathology visible in the images. We demonstrate\ngeneralizability of the building blocks using two variants to build the\ncompressor. We have evaluated the quality of decompressed images using\ndistortion losses as well as perception loss and compared it with other off the\nshelf solutions. The proposed method achieves a compression ratio of $725:1$\nwhile preserving the statistical distribution of speckles. This enables image\nsegmentation on decompressed images to achieve dice score of $0.89 \\pm 0.11$,\nwhich evidently is not so accurately achievable when images are compressed with\ncurrent standards like JPEG, JPEG 2000, WebP and BPG. We envision this frame\nwork to serve as a roadmap for speckle image compression standards.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 16:42:34 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["China", "Debarghya", ""], ["Tom", "Francis", ""], ["Nandamuri", "Sumanth", ""], ["Kar", "Aupendu", ""], ["Srinivasan", "Mukundhan", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1901.05884", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Yukang Chen, Xinbang Zhang, Qian Zhang, Chang Huang,\n  Gaofeng Meng, Wenyu Liu, Xinggang Wang", "title": "EAT-NAS: Elastic Architecture Transfer for Accelerating Large-scale\n  Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) methods have been proposed to release human\nexperts from tedious architecture engineering. However, most current methods\nare constrained in small-scale search due to the issue of computational\nresources. Meanwhile, directly applying architectures searched on small\ndatasets to large datasets often bears no performance guarantee. This\nlimitation impedes the wide use of NAS on large-scale tasks. To overcome this\nobstacle, we propose an elastic architecture transfer mechanism for\naccelerating large-scale neural architecture search (EAT-NAS). In our\nimplementations, architectures are first searched on a small dataset, e.g.,\nCIFAR-10. The best one is chosen as the basic architecture. The search process\non the large dataset, e.g., ImageNet, is initialized with the basic\narchitecture as the seed. The large-scale search process is accelerated with\nthe help of the basic architecture. What we propose is not only a NAS method\nbut a mechanism for architecture-level transfer.\n  In our experiments, we obtain two final models EATNet-A and EATNet-B that\nachieve competitive accuracies, 74.7% and 74.2% on ImageNet, respectively,\nwhich also surpass the models searched from scratch on ImageNet under the same\nsettings. For the computational cost, EAT-NAS takes only less than 5 days on 8\nTITAN X GPUs, which is significantly less than the computational consumption of\nthe state-of-the-art large-scale NAS methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 16:48:12 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:10:45 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2019 08:26:53 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Fang", "Jiemin", ""], ["Chen", "Yukang", ""], ["Zhang", "Xinbang", ""], ["Zhang", "Qian", ""], ["Huang", "Chang", ""], ["Meng", "Gaofeng", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]]}, {"id": "1901.05894", "submitter": "Shiv Ram Dubey", "authors": "Swalpa Kumar Roy, Suvojit Manna, Shiv Ram Dubey, Bidyut Baran\n  Chaudhuri", "title": "LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation\n  Function for Neural Networks", "comments": "Submitted to IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activation function in neural network is one of the important aspects\nwhich facilitates the deep training by introducing the non-linearity into the\nlearning process. However, because of zero-hard rectification, some of the\nexisting activation functions such as ReLU and Swish miss to utilize the large\nnegative input values and may suffer from the dying gradient problem. Thus, it\nis important to look for a better activation function which is free from such\nproblems. As a remedy, this paper proposes a new non-parametric function,\ncalled Linearly Scaled Hyperbolic Tangent (LiSHT) for Neural Networks (NNs).\nThe proposed LiSHT activation function is an attempt to scale the non-linear\nHyperbolic Tangent (Tanh) function by a linear function and tackle the dying\ngradient problem. The training and classification experiments are performed\nover benchmark Iris, MNIST, CIFAR10, CIFAR100 and twitter140 datasets to show\nthat the proposed activation achieves faster convergence and higher\nperformance. A very promising performance improvement is observed on three\ndifferent type of neural networks including Multi-layer Perceptron (MLP),\nConvolutional Neural Network (CNN) and Recurrent neural network like Long-short\nterm memory (LSTM). The advantages of proposed activation function are also\nvisualized in terms of the feature activation maps, weight distribution and\nloss landscape. The code is available at https://github.com/swalpa/lisht.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 02:24:06 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 10:51:23 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Roy", "Swalpa Kumar", ""], ["Manna", "Suvojit", ""], ["Dubey", "Shiv Ram", ""], ["Chaudhuri", "Bidyut Baran", ""]]}, {"id": "1901.05903", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey", "title": "A Performance Comparison of Loss Functions for Deep Face Recognition", "comments": "Accepted in NCVPRIPG 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is one of the most widely publicized feature in the devices\ntoday and hence represents an important problem that should be studied with the\nutmost priority. As per the recent trends, the Convolutional Neural Network\n(CNN) based approaches are highly successful in many tasks of Computer Vision\nincluding face recognition. The loss function is used on the top of CNN to\njudge the goodness of any network. In this paper, we present a performance\ncomparison of different loss functions such as Cross-Entropy, Angular Softmax,\nAdditive-Margin Softmax, ArcFace and Marginal Loss for face recognition. The\nexperiments are conducted with two CNN architectures namely, ResNet and\nMobileNet. Two widely used face datasets namely, CASIA-Webface and MS-Celeb-1M\nare used for the training and benchmark Labeled Faces in the Wild (LFW) face\ndataset is used for the testing.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 03:00:10 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 08:49:18 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1901.05945", "submitter": "Wei Xiong", "authors": "Wei Xiong, Jiahui Yu, Zhe Lin, Jimei Yang, Xin Lu, Connelly Barnes and\n  Jiebo Luo", "title": "Foreground-aware Image Inpainting", "comments": "Camera Ready version of CVPR 2019 with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image inpainting methods typically fill holes by borrowing\ninformation from surrounding pixels. They often produce unsatisfactory results\nwhen the holes overlap with or touch foreground objects due to lack of\ninformation about the actual extent of foreground and background regions within\nthe holes. These scenarios, however, are very important in practice, especially\nfor applications such as the removal of distracting objects. To address the\nproblem, we propose a foreground-aware image inpainting system that explicitly\ndisentangles structure inference and content completion. Specifically, our\nmodel learns to predict the foreground contour first, and then inpaints the\nmissing region using the predicted contour as guidance. We show that by such\ndisentanglement, the contour completion model predicts reasonable contours of\nobjects, and further substantially improves the performance of image\ninpainting. Experiments show that our method significantly outperforms existing\nmethods and achieves superior inpainting results on challenging cases with\ncomplex compositions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 18:39:10 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 04:03:40 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 05:59:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Xiong", "Wei", ""], ["Yu", "Jiahui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Lu", "Xin", ""], ["Barnes", "Connelly", ""], ["Luo", "Jiebo", ""]]}, {"id": "1901.05946", "submitter": "Christos Sakaridis", "authors": "Christos Sakaridis, Dengxin Dai, Luc Van Gool", "title": "Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for\n  Semantic Nighttime Image Segmentation", "comments": "ICCV 2019 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most progress in semantic segmentation reports on daytime images taken under\nfavorable illumination conditions. We instead address the problem of semantic\nsegmentation of nighttime images and improve the state-of-the-art, by adapting\ndaytime models to nighttime without using nighttime annotations. Moreover, we\ndesign a new evaluation framework to address the substantial uncertainty of\nsemantics in nighttime images. Our central contributions are: 1) a curriculum\nframework to gradually adapt semantic segmentation models from day to night via\nlabeled synthetic images and unlabeled real images, both for progressively\ndarker times of day, which exploits cross-time-of-day correspondences for the\nreal images to guide the inference of their labels; 2) a novel\nuncertainty-aware annotation and evaluation framework and metric for semantic\nsegmentation, designed for adverse conditions and including image regions\nbeyond human recognition capability in the evaluation in a principled fashion;\n3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920\nunlabeled twilight images with correspondences to their daytime counterparts\nplus a set of 151 nighttime images with fine pixel-level annotations created\nwith our protocol, which serves as a first benchmark to perform our novel\nevaluation. Experiments show that our guided curriculum adaptation\nsignificantly outperforms state-of-the-art methods on real nighttime sets both\nfor standard metrics and our uncertainty-aware metric. Furthermore, our\nuncertainty-aware evaluation reveals that selective invalidation of predictions\ncan lead to better results on data with ambiguous content such as our nighttime\nbenchmark and profit safety-oriented applications which involve invalid inputs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 18:40:39 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 14:29:46 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1901.05992", "submitter": "Amod Jog", "authors": "Amod Jog, Andrew Hoopes, Douglas N. Greve, Koen Van Leemput, Bruce\n  Fischl", "title": "PSACNN: Pulse Sequence Adaptive Fast Whole Brain Segmentation", "comments": "Typo in author name corrected. Greves -> Greve", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of convolutional neural networks~(CNN), supervised learning\nmethods are increasingly being used for whole brain segmentation. However, a\nlarge, manually annotated training dataset of labeled brain images required to\ntrain such supervised methods is frequently difficult to obtain or create. In\naddition, existing training datasets are generally acquired with a homogeneous\nmagnetic resonance imaging~(MRI) acquisition protocol. CNNs trained on such\ndatasets are unable to generalize on test data with different acquisition\nprotocols. Modern neuroimaging studies and clinical trials are necessarily\nmulti-center initiatives with a wide variety of acquisition protocols. Despite\nstringent protocol harmonization practices, it is very difficult to standardize\nthe gamut of MRI imaging parameters across scanners, field strengths, receive\ncoils etc., that affect image contrast. In this paper we propose a CNN-based\nsegmentation algorithm that, in addition to being highly accurate and fast, is\nalso resilient to variation in the input acquisition. Our approach relies on\nbuilding approximate forward models of pulse sequences that produce a typical\ntest image. For a given pulse sequence, we use its forward model to generate\nplausible, synthetic training examples that appear as if they were acquired in\na scanner with that pulse sequence. Sampling over a wide variety of pulse\nsequences results in a wide variety of augmented training examples that help\nbuild an image contrast invariant model. Our method trains a single CNN that\ncan segment input MRI images with acquisition parameters as disparate as\n$T_1$-weighted and $T_2$-weighted contrasts with only $T_1$-weighted training\ndata. The segmentations generated are highly accurate with state-of-the-art\nresults~(overall Dice overlap$=0.94$), with a fast run time~($\\approx$ 45\nseconds), and consistent across a wide range of acquisition protocols.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 19:58:39 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 05:27:22 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 22:40:49 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jog", "Amod", ""], ["Hoopes", "Andrew", ""], ["Greve", "Douglas N.", ""], ["Van Leemput", "Koen", ""], ["Fischl", "Bruce", ""]]}, {"id": "1901.06006", "submitter": "Samira Masoudi", "authors": "Samira Masoudi, Afsaneh Razi, Cameron H.G. Wright, Jay C. Gatlin, Ulas\n  Bagci", "title": "Instance-Level Microtubule Tracking", "comments": "13 pages, 12 figures, 9 tables", "journal-ref": null, "doi": "10.1109/TMI.2019.2963865", "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a new method of instance-level microtubule (MT) tracking in\ntime-lapse image series using recurrent attention. Our novel deep learning\nalgorithm segments individual MTs at each frame. Segmentation results from\nsuccessive frames are used to assign correspondences among MTs. This ultimately\ngenerates a distinct path trajectory for each MT through the frames. Based on\nthese trajectories, we estimate MT velocities. To validate our proposed\ntechnique, we conduct experiments using real and simulated data. We use\nstatistics derived from real time-lapse series of MT gliding assays to simulate\nrealistic MT time-lapse image series in our simulated data. This dataset is\nemployed as pre-training and hyperparameter optimization for our network before\ntraining on the real data. Our experimental results show that the proposed\nsupervised learning algorithm improves the precision for MT instance velocity\nestimation drastically to 71.3% from the baseline result (29.3%). We also\ndemonstrate how the inclusion of temporal information into our deep network can\nreduce the false negative rates from 67.8% (baseline) down to 28.7% (proposed).\nOur findings in this work are expected to help biologists characterize the\nspatial arrangement of MTs, specifically the effects of MT-MT interactions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 21:00:54 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 18:21:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Masoudi", "Samira", ""], ["Razi", "Afsaneh", ""], ["Wright", "Cameron H. G.", ""], ["Gatlin", "Jay C.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1901.06013", "submitter": "Weilian Song", "authors": "Weilian Song, Scott Workman, Armin Hadzic, Xu Zhang, Eric Green, Mei\n  Chen, Reginald Souleyrette, Nathan Jacobs", "title": "FARSA: Fully Automated Roadway Safety Assessment", "comments": "9 pages, 8 figures, WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of road safety assessment. An emerging approach\nfor conducting such assessments in the United States is through the US Road\nAssessment Program (usRAP), which rates roads from highest risk (1 star) to\nlowest (5 stars). Obtaining these ratings requires manual, fine-grained\nlabeling of roadway features in street-level panoramas, a slow and costly\nprocess. We propose to automate this process using a deep convolutional neural\nnetwork that directly estimates the star rating from a street-level panorama,\nrequiring milliseconds per image at test time. Our network also estimates many\nother road-level attributes, including curvature, roadside hazards, and the\ntype of median. To support this, we incorporate task-specific attention layers\nso the network can focus on the panorama regions that are most useful for a\nparticular task. We evaluated our approach on a large dataset of real-world\nimages from two US states. We found that incorporating additional tasks, and\nusing a semi-supervised training approach, significantly reduced overfitting\nproblems, allowed us to optimize more layers of the network, and resulted in\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 21:48:05 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Song", "Weilian", ""], ["Workman", "Scott", ""], ["Hadzic", "Armin", ""], ["Zhang", "Xu", ""], ["Green", "Eric", ""], ["Chen", "Mei", ""], ["Souleyrette", "Reginald", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1901.06026", "submitter": "Davide Modolo", "authors": "Rahul Rama Varior, Bing Shuai, Joseph Tighe, Davide Modolo", "title": "Multi-Scale Attention Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd counting datasets, people appear at different scales, depending on\ntheir distance from the camera. To address this issue, we propose a novel\nmulti-branch scale-aware attention network that exploits the hierarchical\nstructure of convolutional neural networks and generates, in a single forward\npass, multi-scale density predictions from different layers of the\narchitecture. To aggregate these maps into our final prediction, we present a\nnew soft attention mechanism that learns a set of gating masks. Furthermore, we\nintroduce a scale-aware loss function to regularize the training of different\nbranches and guide them to specialize on a particular scale. As this new\ntraining requires annotations for the size of each head, we also propose a\nsimple, yet effective technique to estimate them automatically. Finally, we\npresent an ablation study on each of these components and compare our approach\nagainst the literature on 4 crowd counting datasets: UCF-QNRF, ShanghaiTech A &\nB and UCF_CC_50. Our approach achieves state-of-the-art on all them with a\nremarkable improvement on UCF-QNRF (+25% reduction in error).\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 22:50:56 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 02:11:40 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 00:45:01 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Varior", "Rahul Rama", ""], ["Shuai", "Bing", ""], ["Tighe", "Joseph", ""], ["Modolo", "Davide", ""]]}, {"id": "1901.06032", "submitter": "Asifullah Khan", "authors": "Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi", "title": "A Survey of the Recent Architectures of Deep Convolutional Neural\n  Networks", "comments": "Number of Pages: 70, Number of Figures: 11, Number of Tables: 11.\n  Artif Intell Rev (2020)", "journal-ref": null, "doi": "10.1007/s10462-020-09825-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Network (CNN) is a special type of Neural Networks,\nwhich has shown exemplary performance on several competitions related to\nComputer Vision and Image Processing. Some of the exciting application areas of\nCNN include Image Classification and Segmentation, Object Detection, Video\nProcessing, Natural Language Processing, and Speech Recognition. The powerful\nlearning ability of deep CNN is primarily due to the use of multiple feature\nextraction stages that can automatically learn representations from the data.\nThe availability of a large amount of data and improvement in the hardware\ntechnology has accelerated the research in CNNs, and recently interesting deep\nCNN architectures have been reported. Several inspiring ideas to bring\nadvancements in CNNs have been explored, such as the use of different\nactivation and loss functions, parameter optimization, regularization, and\narchitectural innovations. However, the significant improvement in the\nrepresentational capacity of the deep CNN is achieved through architectural\ninnovations. Notably, the ideas of exploiting spatial and channel information,\ndepth and width of architecture, and multi-path information processing have\ngained substantial attention. Similarly, the idea of using a block of layers as\na structural unit is also gaining popularity. This survey thus focuses on the\nintrinsic taxonomy present in the recently reported deep CNN architectures and,\nconsequently, classifies the recent innovations in CNN architectures into seven\ndifferent categories. These seven categories are based on spatial exploitation,\ndepth, multi-path, width, feature-map exploitation, channel boosting, and\nattention. Additionally, the elementary understanding of CNN components,\ncurrent challenges, and applications of CNN are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 23:20:23 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 03:10:29 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 04:49:30 GMT"}, {"version": "v4", "created": "Thu, 2 Jan 2020 07:52:04 GMT"}, {"version": "v5", "created": "Wed, 12 Feb 2020 05:27:04 GMT"}, {"version": "v6", "created": "Fri, 20 Mar 2020 14:47:20 GMT"}, {"version": "v7", "created": "Sun, 10 May 2020 12:43:40 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Khan", "Asifullah", ""], ["Sohail", "Anabia", ""], ["Zahoora", "Umme", ""], ["Qureshi", "Aqsa Saeed", ""]]}, {"id": "1901.06034", "submitter": "Si Lu", "authors": "Si Lu", "title": "High-speed Video from Asynchronous Camera Array", "comments": "10 pages, 82 figures, Published at IEEE WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for capturing high-speed video using an\nasynchronous camera array. Our method sequentially fires each sensor in a\ncamera array with a small time offset and assembles captured frames into a\nhigh-speed video according to the time stamps. The resulting video, however,\nsuffers from parallax jittering caused by the viewpoint difference among\nsensors in the camera array. To address this problem, we develop a dedicated\nnovel view synthesis algorithm that transforms the video frames as if they were\ncaptured by a single reference sensor. Specifically, for any frame from a\nnon-reference sensor, we find the two temporally neighboring frames captured by\nthe reference sensor. Using these three frames, we render a new frame with the\nsame time stamp as the non-reference frame but from the viewpoint of the\nreference sensor. Specifically, we segment these frames into super-pixels and\nthen apply local content-preserving warping to warp them to form the new frame.\nWe employ a multi-label Markov Random Field method to blend these warped\nframes. Our experiments show that our method can produce high-quality and\nhigh-speed video of a wide variety of scenes with large parallax, scene\ndynamics, and camera motion and outperforms several baseline and\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 23:26:55 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Lu", "Si", ""]]}, {"id": "1901.06046", "submitter": "Si Lu", "authors": "Si Lu", "title": "Good Similar Patches for Image Denoising", "comments": "10 pages, 13 figures, 6 tables, IEEE WACV 2019", "journal-ref": null, "doi": "10.1109/WACV.2019.00205", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based denoising algorithms like BM3D have achieved outstanding\nperformance. An important idea for the success of these methods is to exploit\nthe recurrence of similar patches in an input image to estimate the underlying\nimage structures. However, in these algorithms, the similar patches used for\ndenoising are obtained via Nearest Neighbour Search (NNS) and are sometimes not\noptimal. First, due to the existence of noise, NNS can select similar patches\nwith similar noise patterns to the reference patch. Second, the unreliable\nnoisy pixels in digital images can bring a bias to the patch searching process\nand result in a loss of color fidelity in the final denoising result. We\nobserve that given a set of good similar patches, their distribution is not\nnecessarily centered at the noisy reference patch and can be approximated by a\nGaussian component. Based on this observation, we present a patch searching\nmethod that clusters similar patch candidates into patch groups using Gaussian\nMixture Model-based clustering, and selects the patch group that contains the\nreference patch as the final patches for denoising. We also use an unreliable\npixel estimation algorithm to pre-process the input noisy images to further\nimprove the patch searching. Our experiments show that our approach can better\ncapture the underlying patch structures and can consistently enable the\nstate-of-the-art patch-based denoising algorithms, such as BM3D, LPCA and PLOW,\nto better denoise images by providing them with patches found by our approach\nwhile without modifying these algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 01:04:01 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Lu", "Si", ""]]}, {"id": "1901.06047", "submitter": "Xiaoguang Han", "authors": "Zizheng Yan, Xiaoguang Han, Changmiao Wang, Yuda Qiu, Zixiang Xiong,\n  Shuguang Cui", "title": "Learning Mutually Local-global U-nets For High-resolution Retinal Lesion\n  Segmentation in Fundus Images", "comments": "4 pages, Accepted by ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy is the most important complication of diabetes. Early\ndiagnosis of retinal lesions helps to avoid visual loss or blindness. Due to\nhigh-resolution and small-size lesion regions, applying existing methods, such\nas U-Nets, to perform segmentation on fundus photography is very challenging.\nAlthough downsampling the input images could simplify the problem, it loses\ndetailed information. Conducting patch-level analysis helps reaching fine-scale\nsegmentation yet usually leads to misunderstanding as the lack of context\ninformation. In this paper, we propose an efficient network that combines them\ntogether, not only being aware of local details but also taking fully use of\nthe context perceptions. This is implemented by integrating the decoder parts\nof a global-level U-net and a patch-level one. The two streams are jointly\noptimized, ensuring that they are enhanced mutually. Experimental results\ndemonstrate our new framework significantly outperforms existing patch-based\nand global-based methods, especially when the lesion regions are scattered and\nsmall-scaled.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 01:05:48 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Yan", "Zizheng", ""], ["Han", "Xiaoguang", ""], ["Wang", "Changmiao", ""], ["Qiu", "Yuda", ""], ["Xiong", "Zixiang", ""], ["Cui", "Shuguang", ""]]}, {"id": "1901.06081", "submitter": "Sheng He", "authors": "Sheng He and Lambert Schomaker", "title": "DeepOtsu: Document Enhancement and Binarization using Iterative Deep\n  Learning", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2019.01.025", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a novel iterative deep learning framework and apply it\nfor document enhancement and binarization. Unlike the traditional methods which\npredict the binary label of each pixel on the input image, we train the neural\nnetwork to learn the degradations in document images and produce the uniform\nimages of the degraded input images, which allows the network to refine the\noutput iteratively. Two different iterative methods have been studied in this\npaper: recurrent refinement (RR) which uses the same trained neural network in\neach iteration for document enhancement and stacked refinement (SR) which uses\na stack of different neural networks for iterative output refinement. Given the\nlearned uniform and enhanced image, the binarization map can be easy to obtain\nby a global or local threshold. The experimental results on several public\nbenchmark data sets show that our proposed methods provide a new clean version\nof the degraded image which is suitable for visualization and promising results\nof binarization using the global Otsu's threshold based on the enhanced images\nlearned iteratively by the neural network.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 04:23:51 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["He", "Sheng", ""], ["Schomaker", "Lambert", ""]]}, {"id": "1901.06110", "submitter": "Unni V. S.", "authors": "Unni V. S., Sanjay Ghosh and Kunal N. Chaudhury", "title": "Linearized ADMM and Fast Nonlocal Denoising for Efficient Plug-and-Play\n  Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In plug-and-play image restoration, the regularization is performed using\npowerful denoisers such as nonlocal means (NLM) or BM3D. This is done within\nthe framework of alternating direction method of multipliers (ADMM), where the\nregularization step is formally replaced by an off-the-shelf denoiser. Each\nplug-and-play iteration involves the inversion of the forward model followed by\na denoising step. In this paper, we present a couple of ideas for improving the\nefficiency of the inversion and denoising steps. First, we propose to use\nlinearized ADMM, which generally allows us to perform the inversion at a lower\ncost than standard ADMM. Moreover, we can easily incorporate hard constraints\ninto the optimization framework as a result. Second, we develop a fast\nalgorithm for doubly stochastic NLM, originally proposed by Sreehari et al.\n(IEEE TCI, 2016), which is about 80x faster than brute-force computation. This\nparticular denoiser can be expressed as the proximal map of a convex\nregularizer and, as a consequence, we can guarantee convergence for linearized\nplug-and-play ADMM. We demonstrate the effectiveness of our proposals for\nsuper-resolution and single-photon imaging.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 07:20:32 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["S.", "Unni V.", ""], ["Ghosh", "Sanjay", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1901.06111", "submitter": "Shanshan Wang", "authors": "Ziwen Ke, Shanshan Wang, Huitao Cheng, Leslie Ying, Qiegen Liu,\n  Hairong Zheng, Dong Liang", "title": "CRDN: Cascaded Residual Dense Networks for Dynamic MR Imaging with\n  Edge-enhanced Loss Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic magnetic resonance (MR) imaging has generated great research\ninterest, as it can provide both spatial and temporal information for clinical\ndiagnosis. However, slow imaging speed or long scanning time is still one of\nthe challenges for dynamic MR imaging. Most existing methods reconstruct\nDynamic MR images from incomplete k-space data under the guidance of compressed\nsensing (CS) or low rank theory, which suffer from long iterative\nreconstruction time. Recently, deep learning has shown great potential in\naccelerating dynamic MR. Our previous work proposed a dynamic MR imaging method\nwith both k-space and spatial prior knowledge integrated via multi-supervised\nnetwork training. Nevertheless, there was still a certain degree of smooth in\nthe reconstructed images at high acceleration factors. In this work, we propose\ncascaded residual dense networks for dynamic MR imaging with edge-enhance loss\nconstraint, dubbed as CRDN. Specifically, the cascaded residual dense networks\nfully exploit the hierarchical features from all the convolutional layers with\nboth local and global feature fusion. We further utilize the total variation\n(TV) loss function, which has the edge enhancement properties, for training the\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 07:22:48 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Ke", "Ziwen", ""], ["Wang", "Shanshan", ""], ["Cheng", "Huitao", ""], ["Ying", "Leslie", ""], ["Liu", "Qiegen", ""], ["Zheng", "Hairong", ""], ["Liang", "Dong", ""]]}, {"id": "1901.06112", "submitter": "Pravin Nair", "authors": "Pravin Nair and Kunal N. Chaudhury", "title": "Fast High-Dimensional Kernel Filtering", "comments": null, "journal-ref": "IEEE Signal Processing Letters 2019", "doi": "10.1109/LSP.2019.2891879", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral and nonlocal means filters are instances of kernel-based\nfilters that are popularly used in image processing. It was recently shown that\nfast and accurate bilateral filtering of grayscale images can be performed\nusing a low-rank approximation of the kernel matrix. More specifically, based\non the eigendecomposition of the kernel matrix, the overall filtering was\napproximated using spatial convolutions, for which efficient algorithms are\navailable. Unfortunately, this technique cannot be scaled to high-dimensional\ndata such as color and hyperspectral images. This is simply because one needs\nto compute/store a large matrix and perform its eigendecomposition in this\ncase. We show how this problem can be solved using the Nystr\\\"om method, which\nis generally used for approximating the eigendecomposition of large matrices.\nThe resulting algorithm can also be used for nonlocal means filtering. We\ndemonstrate the effectiveness of our proposal for bilateral and nonlocal means\nfiltering of color and hyperspectral images. In particular, our method is shown\nto be competitive with state-of-the-art fast algorithms, and moreover it comes\nwith a theoretical guarantee on the approximation error.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 07:24:28 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Nair", "Pravin", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1901.06129", "submitter": "Weitao Feng", "authors": "Weitao Feng, Zhihao Hu, Wei Wu, Junjie Yan, Wanli Ouyang", "title": "Multi-Object Tracking with Multiple Cues and Switcher-Aware\n  Classification", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified Multi-Object Tracking (MOT) framework\nlearning to make full use of long term and short term cues for handling complex\ncases in MOT scenes. Besides, for better association, we propose switcher-aware\nclassification (SAC), which takes the potential identity-switch causer\n(switcher) into consideration. Specifically, the proposed framework includes a\nSingle Object Tracking (SOT) sub-net to capture short term cues, a\nre-identification (ReID) sub-net to extract long term cues and a switcher-aware\nclassifier to make matching decisions using extracted features from the main\ntarget and the switcher. Short term cues help to find false negatives, while\nlong term cues avoid critical mistakes when occlusion happens, and the SAC\nlearns to combine multiple cues in an effective way and improves robustness.\nThe method is evaluated on the challenging MOT benchmarks and achieves the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 08:44:01 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Feng", "Weitao", ""], ["Hu", "Zhihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1901.06140", "submitter": "Youngmin Ro", "authors": "Youngmin Ro, Jongwon Choi, Dae Ung Jo, Byeongho Heo, Jongin Lim, Jin\n  Young Choi", "title": "Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network\n  for Person Re-Identification", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person re-identification (ReID) task, because of its shortage of trainable\ndataset, it is common to utilize fine-tuning method using a classification\nnetwork pre-trained on a large dataset. However, it is relatively difficult to\nsufficiently fine-tune the low-level layers of the network due to the gradient\nvanishing problem. In this work, we propose a novel fine-tuning strategy that\nallows low-level layers to be sufficiently trained by rolling back the weights\nof high-level layers to their initial pre-trained weights. Our strategy\nalleviates the problem of gradient vanishing in low-level layers and robustly\ntrains the low-level layers to fit the ReID dataset, thereby increasing the\nperformance of ReID tasks. The improved performance of the proposed strategy is\nvalidated via several experiments. Furthermore, without any add-ons such as\npose estimation or segmentation, our strategy exhibits state-of-the-art\nperformance using only vanilla deep convolutional neural network architecture.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 09:08:53 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Ro", "Youngmin", ""], ["Choi", "Jongwon", ""], ["Jo", "Dae Ung", ""], ["Heo", "Byeongho", ""], ["Lim", "Jongin", ""], ["Choi", "Jin Young", ""]]}, {"id": "1901.06199", "submitter": "Kaizhu Huang", "authors": "Zhuang Qian, Kaizhu Huang, Qiufeng Wang, Jimin Xiao, Rui Zhang", "title": "Generative Adversarial Classifier for Handwriting Characters\n  Super-Resolution", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) receive great attentions recently due\nto its excellent performance in image generation, transformation, and\nsuper-resolution. However, GAN has rarely been studied and trained for\nclassification, leading that the generated images may not be appropriate for\nclassification. In this paper, we propose a novel Generative Adversarial\nClassifier (GAC) particularly for low-resolution Handwriting Character\nRecognition. Specifically, involving additionally a classifier in the training\nprocess of normal GANs, GAC is calibrated for learning suitable structures and\nrestored characters images that benefits the classification. Experimental\nresults show that our proposed method can achieve remarkable performance in\nhandwriting characters 8x super-resolution, approximately 10% and 20% higher\nthan the present state-of-the-art methods respectively on benchmark data\nCASIA-HWDB1.1 and MNIST.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 12:10:50 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Qian", "Zhuang", ""], ["Huang", "Kaizhu", ""], ["Wang", "Qiufeng", ""], ["Xiao", "Jimin", ""], ["Zhang", "Rui", ""]]}, {"id": "1901.06219", "submitter": "Oleksandr Bailo", "authors": "Oleksandr Bailo, DongShik Ham, Young Min Shin", "title": "Red blood cell image generation for data augmentation using Conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how to apply image-to-image translation techniques\nto medical blood smear data to generate new data samples and meaningfully\nincrease small datasets. Specifically, given the segmentation mask of the\nmicroscopy image, we are able to generate photorealistic images of blood cells\nwhich are further used alongside real data during the network training for\nsegmentation and object detection tasks. This image data generation approach is\nbased on conditional generative adversarial networks which have proven\ncapabilities to high-quality image synthesis. In addition to synthesizing blood\nimages, we synthesize segmentation mask as well which leads to a diverse\nvariety of generated samples. The effectiveness of the technique is thoroughly\nanalyzed and quantified through a number of experiments on a manually collected\nand annotated dataset of blood smear taken under a microscope.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:23:41 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 10:08:18 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Bailo", "Oleksandr", ""], ["Ham", "DongShik", ""], ["Shin", "Young Min", ""]]}, {"id": "1901.06314", "submitter": "Yinhao Zhu", "authors": "Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, Paris\n  Perdikaris", "title": "Physics-Constrained Deep Learning for High-dimensional Surrogate\n  Modeling and Uncertainty Quantification without Labeled Data", "comments": "51 pages, 18 figures, submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2019.05.024", "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate modeling and uncertainty quantification tasks for PDE systems are\nmost often considered as supervised learning problems where input and output\ndata pairs are used for training. The construction of such emulators is by\ndefinition a small data problem which poses challenges to deep learning\napproaches that have been developed to operate in the big data regime. Even in\ncases where such models have been shown to have good predictive capability in\nhigh dimensions, they fail to address constraints in the data implied by the\nPDE model. This paper provides a methodology that incorporates the governing\nequations of the physical model in the loss/likelihood functions. The resulting\nphysics-constrained, deep learning models are trained without any labeled data\n(e.g. employing only input data) and provide comparable predictive responses\nwith data-driven models while obeying the constraints of the problem at hand.\nThis work employs a convolutional encoder-decoder neural network approach as\nwell as a conditional flow-based generative model for the solution of PDEs,\nsurrogate model construction, and uncertainty quantification tasks. The\nmethodology is posed as a minimization problem of the reverse Kullback-Leibler\n(KL) divergence between the model predictive density and the reference\nconditional density, where the later is defined as the Boltzmann-Gibbs\ndistribution at a given inverse temperature with the underlying potential\nrelating to the PDE system of interest. The generalization capability of these\nmodels to out-of-distribution input is considered. Quantification and\ninterpretation of the predictive uncertainty is provided for a number of\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 15:59:30 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhu", "Yinhao", ""], ["Zabaras", "Nicholas", ""], ["Koutsourelakis", "Phaedon-Stelios", ""], ["Perdikaris", "Paris", ""]]}, {"id": "1901.06322", "submitter": "Tianfu Wu", "authors": "Wei Sun and Tianfu Wu", "title": "Learning Spatial Pyramid Attentive Pooling in Image Synthesis and\n  Image-to-Image Translation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis and image-to-image translation are two important generative\nlearning tasks. Remarkable progress has been made by learning Generative\nAdversarial Networks (GANs)~\\cite{goodfellow2014generative} and\ncycle-consistent GANs (CycleGANs)~\\cite{zhu2017unpaired} respectively. This\npaper presents a method of learning Spatial Pyramid Attentive Pooling (SPAP)\nwhich is a novel architectural unit and can be easily integrated into both\ngenerators and discriminators in GANs and CycleGANs. The proposed SPAP\nintegrates Atrous spatial pyramid~\\cite{chen2018deeplab}, a proposed cascade\nattention mechanism and residual connections~\\cite{he2016deep}. It leverages\nthe advantages of the three components to facilitate effective end-to-end\ngenerative learning: (i) the capability of fusing multi-scale information by\nASPP; (ii) the capability of capturing relative importance between both spatial\nlocations (especially multi-scale context) or feature channels by attention;\n(iii) the capability of preserving information and enhancing optimization\nfeasibility by residual connections. Coarse-to-fine and fine-to-coarse SPAP are\nstudied and intriguing attention maps are observed in both tasks. In\nexperiments, the proposed SPAP is tested in GANs on the Celeba-HQ-128\ndataset~\\cite{karras2017progressive}, and tested in CycleGANs on the\nImage-to-Image translation datasets including the Cityscape\ndataset~\\cite{cordts2016cityscapes}, Facade and Aerial Maps\ndataset~\\cite{zhu2017unpaired}, both obtaining better performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 16:23:37 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Sun", "Wei", ""], ["Wu", "Tianfu", ""]]}, {"id": "1901.06340", "submitter": "Fan Yang", "authors": "Fan Yang, Lei Zhang, Sijia Yu, Danil Prokhorov, Xue Mei, and Haibin\n  Ling", "title": "Feature Pyramid and Hierarchical Boosting Network for Pavement Crack\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pavement crack detection is a critical task for insuring road safety. Manual\ncrack detection is extremely time-consuming. Therefore, an automatic road crack\ndetection method is required to boost this progress. However, it remains a\nchallenging task due to the intensity inhomogeneity of cracks and complexity of\nthe background, e.g., the low contrast with surrounding pavements and possible\nshadows with similar intensity. Inspired by recent advances of deep learning in\ncomputer vision, we propose a novel network architecture, named Feature Pyramid\nand Hierarchical Boosting Network (FPHBN), for pavement crack detection. The\nproposed network integrates semantic information to low-level features for\ncrack detection in a feature pyramid way. And, it balances the contribution of\nboth easy and hard samples to loss by nested sample reweighting in a\nhierarchical way. To demonstrate the superiority and generality of the proposed\nmethod, we evaluate the proposed method on five crack datasets and compare it\nwith state-of-the-art crack detection, edge detection, semantic segmentation\nmethods. Extensive experiments show that the proposed method outperforms these\nstate-of-the-art methods in terms of accuracy and generality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 16:58:59 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:14:41 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Yang", "Fan", ""], ["Zhang", "Lei", ""], ["Yu", "Sijia", ""], ["Prokhorov", "Danil", ""], ["Mei", "Xue", ""], ["Ling", "Haibin", ""]]}, {"id": "1901.06345", "submitter": "Pavel Ostyakov", "authors": "Pavel Ostyakov and Sergey I. Nikolenko", "title": "Adapting Convolutional Neural Networks for Geographical Domain Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the winning solution for the Inclusive Images Competition\norganized as part of the Conference on Neural Information Processing Systems\n(NeurIPS 2018) Competition Track. The competition was organized to study ways\nto cope with domain shift in image processing, specifically geographical shift:\nthe training and two test sets in the competition had different geographical\ndistributions. Our solution has proven to be relatively straightforward and\nsimple: it is an ensemble of several CNNs where only the last layer is\nfine-tuned with the help of a small labeled set of tuning labels made available\nby the organizers. We believe that while domain shift remains a formidable\nproblem, our approach opens up new possibilities for alleviating this problem\nin practice, where small labeled datasets from the target domain are usually\neither available or can be obtained and labeled cheaply.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 17:19:37 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Ostyakov", "Pavel", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1901.06355", "submitter": "Laura Beggel", "authors": "Laura Beggel, Michael Pfeiffer, Bernd Bischl", "title": "Robust Anomaly Detection in Images using Adversarial Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably detecting anomalies in a given set of images is a task of high\npractical relevance for visual quality inspection, surveillance, or medical\nimage analysis. Autoencoder neural networks learn to reconstruct normal images,\nand hence can classify those images as anomalies, where the reconstruction\nerror exceeds some threshold. Here we analyze a fundamental problem of this\napproach when the training set is contaminated with a small fraction of\noutliers. We find that continued training of autoencoders inevitably reduces\nthe reconstruction error of outliers, and hence degrades the anomaly detection\nperformance. In order to counteract this effect, an adversarial autoencoder\narchitecture is adapted, which imposes a prior distribution on the latent\nrepresentation, typically placing anomalies into low likelihood-regions.\nUtilizing the likelihood model, potential anomalies can be identified and\nrejected already during training, which results in an anomaly detector that is\nsignificantly more robust to the presence of outliers during training.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 17:48:15 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Beggel", "Laura", ""], ["Pfeiffer", "Michael", ""], ["Bischl", "Bernd", ""]]}, {"id": "1901.06358", "submitter": "Mansi Khemka", "authors": "Mayank Singh Chauhan, Arshdeep Singh, Mansi Khemka, Arneish Prateek,\n  Rijurekha Sen", "title": "Embedded CNN based vehicle classification and counting in non-laned road\n  traffic", "comments": "*These authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying and counting vehicles in road traffic has numerous applications\nin the transportation engineering domain. However, the wide variety of vehicles\n(two-wheelers, three-wheelers, cars, buses, trucks etc.) plying on roads of\ndeveloping regions without any lane discipline, makes vehicle classification\nand counting a hard problem to automate. In this paper, we use state of the art\nConvolutional Neural Network (CNN) based object detection models and train them\nfor multiple vehicle classes using data from Delhi roads. We get upto 75% MAP\non an 80-20 train-test split using 5562 video frames from four different\nlocations. As robust network connectivity is scarce in developing regions for\ncontinuous video transmissions from the road to cloud servers, we also evaluate\nthe latency, energy and hardware cost of embedded implementations of our CNN\nmodel based inferences.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 17:59:36 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Chauhan", "Mayank Singh", ""], ["Singh", "Arshdeep", ""], ["Khemka", "Mansi", ""], ["Prateek", "Arneish", ""], ["Sen", "Rijurekha", ""]]}, {"id": "1901.06359", "submitter": "Youbao Tang", "authors": "Youbao Tang, Ke Yan, Yuxing Tang, Jiamin Liu, Jing Xiao, Ronald M.\n  Summers", "title": "ULDor: A Universal Lesion Detector for CT Scans with Pseudo Masks and\n  Hard Negative Example Mining", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging (ISBI) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lesion detection from computed tomography (CT) scans is an\nimportant task in medical imaging analysis. It is still very challenging due to\nsimilar appearances (e.g. intensity and texture) between lesions and other\ntissues, making it especially difficult to develop a universal lesion detector.\nInstead of developing a specific-type lesion detector, this work builds a\nUniversal Lesion Detector (ULDor) based on Mask R-CNN, which is able to detect\nall different kinds of lesions from whole body parts. As a state-of-the-art\nobject detector, Mask R-CNN adds a branch for predicting segmentation masks on\neach Region of Interest (RoI) to improve the detection performance. However, it\nis almost impossible to manually annotate a large-scale dataset with\npixel-level lesion masks to train the Mask R-CNN for lesion detection. To\naddress this problem, this work constructs a pseudo mask for each lesion region\nthat can be considered as a surrogate of the real mask, based on which the Mask\nR-CNN is employed for lesion detection. On the other hand, this work proposes a\nhard negative example mining strategy to reduce the false positives for\nimproving the detection performance. Experimental results on the NIH DeepLesion\ndataset demonstrate that the ULDor is enhanced using pseudo masks and the\nproposed hard negative example mining strategy and achieves a sensitivity of\n86.21% with five false positives per image.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 17:59:47 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Tang", "Youbao", ""], ["Yan", "Ke", ""], ["Tang", "Yuxing", ""], ["Liu", "Jiamin", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1901.06403", "submitter": "Ramazan Gokberk Cinbis", "authors": "Gencer Sumbul, Ramazan Gokberk Cinbis, Selim Aksoy", "title": "Multisource Region Attention Network for Fine-Grained Object Recognition\n  in Remote Sensing Imagery", "comments": "G. Sumbul, R. G. Cinbis, S. Aksoy, \"Multisource Region Attention\n  Network for Fine-Grained Object Recognition in Remote Sensing Imagery\", IEEE\n  Transactions on Geoscience and Remote Sensing (TGRS), in press, 2019", "journal-ref": null, "doi": "10.1109/TGRS.2019.2894425", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained object recognition concerns the identification of the type of an\nobject among a large number of closely related sub-categories. Multisource data\nanalysis, that aims to leverage the complementary spectral, spatial, and\nstructural information embedded in different sources, is a promising direction\ntowards solving the fine-grained recognition problem that involves low\nbetween-class variance, small training set sizes for rare classes, and class\nimbalance. However, the common assumption of co-registered sources may not hold\nat the pixel level for small objects of interest. We present a novel\nmethodology that aims to simultaneously learn the alignment of multisource data\nand the classification model in a unified framework. The proposed method\ninvolves a multisource region attention network that computes per-source\nfeature representations, assigns attention scores to candidate regions sampled\naround the expected object locations by using these representations, and\nclassifies the objects by using an attention-driven multisource representation\nthat combines the feature representations and the attention scores from all\nsources. All components of the model are realized using deep neural networks\nand are learned in an end-to-end fashion. Experiments using RGB, multispectral,\nand LiDAR elevation data for classification of street trees showed that our\napproach achieved 64.2% and 47.3% accuracies for the 18-class and 40-class\nsettings, respectively, which correspond to 13% and 14.3% improvement relative\nto the commonly used feature concatenation approach from multiple sources.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 19:43:33 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sumbul", "Gencer", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Aksoy", "Selim", ""]]}, {"id": "1901.06405", "submitter": "Francis Tom", "authors": "Francis Tom, Himanshu Sharma, Dheeraj Mundhra, Tathagato Rai Dastidar,\n  Debdoot Sheet", "title": "Learning a Deep Convolution Network with Turing Test Adversaries for\n  Microscopy Image Super Resolution", "comments": "To appear in the Proceedings of the 2019 IEEE International Symposium\n  on Biomedical Imaging (ISBI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarially trained deep neural networks have significantly improved\nperformance of single image super resolution, by hallucinating photorealistic\nlocal textures, thereby greatly reducing the perception difference between a\nreal high resolution image and its super resolved (SR) counterpart. However,\napplication to medical imaging requires preservation of diagnostically relevant\nfeatures while refraining from introducing any diagnostically confusing\nartifacts. We propose using a deep convolutional super resolution network\n(SRNet) trained for (i) minimising reconstruction loss between the real and SR\nimages, and (ii) maximally confusing learned relativistic visual Turing test\n(rVTT) networks to discriminate between (a) pair of real and SR images (T1) and\n(b) pair of patches in real and SR selected from region of interest (T2). The\nadversarial loss of T1 and T2 while backpropagated through SRNet helps it learn\nto reconstruct pathorealism in the regions of interest such as white blood\ncells (WBC) in peripheral blood smears or epithelial cells in histopathology of\ncancerous biopsy tissues, which are experimentally demonstrated here.\nExperiments performed for measuring signal distortion loss using peak signal to\nnoise ratio (pSNR) and structural similarity (SSIM) with variation of SR scale\nfactors, impact of rVTT adversarial losses, and impact on reporting using SR on\na commercially available artificial intelligence (AI) digital pathology system\nsubstantiate our claims.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 19:55:19 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Tom", "Francis", ""], ["Sharma", "Himanshu", ""], ["Mundhra", "Dheeraj", ""], ["Dastidar", "Tathagato Rai", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1901.06433", "submitter": "Thiam Khean Hah", "authors": "Timothy Whithing, Thiam Khean Hah", "title": "Machine Learning with Clos Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new methodology for improving the accuracy of small neural\nnetworks by applying the concept of a clos network to achieve maximum\nexpression in a smaller network. We explore the design space to show that more\nlayers is beneficial, given the same number of parameters. We also present\nfindings on how the relu nonlinearity ffects accuracy in separable networks. We\npresent results on early work with Cifar-10 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 22:17:32 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Whithing", "Timothy", ""], ["Hah", "Thiam Khean", ""]]}, {"id": "1901.06447", "submitter": "Paul Henderson", "authors": "Paul Henderson, Vittorio Ferrari", "title": "Learning single-image 3D reconstruction by generative modelling of\n  shape, pose and shading", "comments": "Extension of arXiv:1807.09259, accepted to IJCV. Differentiable\n  renderer available at https://github.com/pmh47/dirt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework tackling two problems: class-specific 3D\nreconstruction from a single image, and generation of new 3D shape samples.\nThese tasks have received considerable attention recently; however, most\nexisting approaches rely on 3D supervision, annotation of 2D images with\nkeypoints or poses, and/or training with multiple views of each object\ninstance. Our framework is very general: it can be trained in similar settings\nto existing approaches, while also supporting weaker supervision. Importantly,\nit can be trained purely from 2D images, without pose annotations, and with\nonly a single view per instance. We employ meshes as an output representation,\ninstead of voxels used in most prior work. This allows us to reason over\nlighting parameters and exploit shading information during training, which\nprevious 2D-supervised methods cannot. Thus, our method can learn to generate\nand reconstruct concave object classes. We evaluate our approach in various\nsettings, showing that: (i) it learns to disentangle shape from pose and\nlighting; (ii) using shading in the loss improves performance compared to just\nsilhouettes; (iii) when using a standard single white light, our model\noutperforms state-of-the-art 2D-supervised methods, both with and without pose\nsupervision, thanks to exploiting shading cues; (iv) performance improves\nfurther when using multiple coloured lights, even approaching that of\nstate-of-the-art 3D-supervised methods; (v) shapes produced by our model\ncapture smooth surfaces and fine details better than voxel-based approaches;\nand (vi) our approach supports concave classes such as bathtubs and sofas,\nwhich methods based on silhouettes cannot learn.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 00:34:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 22:41:10 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Henderson", "Paul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1901.06459", "submitter": "Aniesh Chawla", "authors": "Chittayong Surakitbanharn, Calvin Yau, Guizhen Wang, Aniesh Chawla,\n  Yinuo Pan, Zhaoya Sun, Sam Yellin, David Ebert, Yung-Hsiang Lu, George K.\n  Thiruvathukal", "title": "Cross-referencing Social Media and Public Surveillance Camera Data for\n  Disaster Response", "comments": "Best Paper award in IEEE HST Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical media (like surveillance cameras) and social media (like Instagram\nand Twitter) may both be useful in attaining on-the-ground information during\nan emergency or disaster situation. However, the intersection and reliability\nof both surveillance cameras and social media during a natural disaster are not\nfully understood. To address this gap, we tested whether social media is of\nutility when physical surveillance cameras went off-line during Hurricane Irma\nin 2017. Specifically, we collected and compared geo-tagged Instagram and\nTwitter posts in the state of Florida during times and in areas where public\nsurveillance cameras went off-line. We report social media content and\nfrequency and content to determine the utility for emergency managers or first\nresponders during a natural disaster.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 03:06:45 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Surakitbanharn", "Chittayong", ""], ["Yau", "Calvin", ""], ["Wang", "Guizhen", ""], ["Chawla", "Aniesh", ""], ["Pan", "Yinuo", ""], ["Sun", "Zhaoya", ""], ["Yellin", "Sam", ""], ["Ebert", "David", ""], ["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "1901.06474", "submitter": "Brojeshwar Bhowmick", "authors": "Dipanjan Das, Ratul Ghosh and Brojeshwar Bhowmick", "title": "Deep Representation Learning Characterized by Inter-class Separation for\n  Image Clustering", "comments": "Published in WACV, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in clustering methods in recent years, the\noutcome of clustering of a natural image dataset is still unsatisfactory due to\ntwo important drawbacks. Firstly, clustering of images needs a good feature\nrepresentation of an image and secondly, we need a robust method which can\ndiscriminate these features for making them belonging to different clusters\nsuch that intra-class variance is less and inter-class variance is high. Often\nthese two aspects are dealt with independently and thus the features are not\nsufficient enough to partition the data meaningfully. In this paper, we propose\na method where we discover these features required for the separation of the\nimages using deep autoencoder. Our method learns the image representation\nfeatures automatically for the purpose of clustering and also select a coherent\nimage and an incoherent image simultaneously for a given image so that the\nfeature representation learning can learn better discriminative features for\ngrouping the similar images in a cluster and at the same time separating the\ndissimilar images across clusters. Experiment results show that our method\nproduces significantly better result than the state-of-the-art methods and we\nalso show that our method is more generalized across different dataset without\nusing any pre-trained model like other existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 06:47:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Das", "Dipanjan", ""], ["Ghosh", "Ratul", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1901.06484", "submitter": "Xiaole Zhao", "authors": "Zhao Xiaole, Huali Zhang, Hangfei Liu, Yun Qin, Tao Zhang, Xueming Zou", "title": "Single MR Image Super-Resolution via Channel Splitting and Serial Fusion\n  Network", "comments": "12 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial resolution is a critical imaging parameter in magnetic resonance\nimaging (MRI). Acquiring high resolution MRI data usually takes long scanning\ntime and would subject to motion artifacts due to hardware, physical, and\nphysiological limitations. Single image super-resolution (SISR), especially\nthat based on deep learning techniques, is an effective and promising\nalternative technique to improve the current spatial resolution of magnetic\nresonance (MR) images. However, the deeper network is more difficult to be\neffectively trained because the information is gradually weakened as the\nnetwork deepens. This problem becomes more serious for medical images due to\nthe degradation of training examples. In this paper, we present a novel channel\nsplitting and serial fusion network (CSSFN) for single MR image\nsuper-resolution. Specifically, the proposed CSSFN network splits the\nhierarchical features into a series of subfeatures, which are then integrated\ntogether in a serial manner. Thus, the network becomes deeper and can deal with\nthe subfeatures on different channels discriminatively. Besides, a dense global\nfeature fusion (DGFF) is adopted to integrate the intermediate features, which\nfurther promotes the information flow in the network. Extensive experiments on\nseveral typical MR images show the superiority of our CSSFN model over other\nadvanced SISR methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 08:44:12 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Xiaole", "Zhao", ""], ["Zhang", "Huali", ""], ["Liu", "Hangfei", ""], ["Qin", "Yun", ""], ["Zhang", "Tao", ""], ["Zou", "Xueming", ""]]}, {"id": "1901.06490", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves and Sarah Latus and Jan Bergmeier and Tobias\n  Ortmaier and L\\\"uder A. Kahrs and Alexander Schlaefer", "title": "Endoscopic vs. volumetric OCT imaging of mastoid bone structure for pose\n  estimation in minimally invasive cochlear implant surgery", "comments": "Accepted for publication at the 33rd international conference on\n  Computer Assisted Radiology and Surgery (CARS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The facial recess is a delicate structure that must be protected in\nminimally invasive cochlear implant surgery. Current research estimates the\ndrill trajectory by using endoscopy of the unique mastoid patterns. However,\nmissing depth information limits available features for a registration to\npreoperative CT data. Therefore, this paper evaluates OCT for enhanced imaging\nof drill holes in mastoid bone and compares OCT data to original endoscopic\nimages.\n  Methods: A catheter-based OCT probe is inserted into a drill trajectory of a\nmastoid phantom in a translation-rotation manner to acquire the inner surface\nstate. The images are undistorted and stitched to create volumentric data of\nthe drill hole. The mastoid cell pattern is segmented automatically and\ncompared to ground truth.\n  Results: The mastoid pattern segmented on images acquired with OCT show a\nsimilarity of J = 73.6 % to ground truth based on endoscopic images and\nmeasured with the Jaccard metric. Leveraged by additional depth information,\nautomated segmentation tends to be more robust and fail-safe compared to\nendoscopic images.\n  Conclusion: The feasibility of using a clinically approved OCT probe for\nimaging the drill hole in cochlear implantation is shown. The resulting\nvolumentric images provide additional information on the shape of caveties in\nthe bone structure, which will be useful for image-to-patient registration and\nto estimate the drill trajectory. This will be another step towards safe\nminimally invasive cochlear implantation.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 09:40:13 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 10:39:48 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Latus", "Sarah", ""], ["Bergmeier", "Jan", ""], ["Ortmaier", "Tobias", ""], ["Kahrs", "L\u00fcder A.", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1901.06494", "submitter": "Himanshu Ladia", "authors": "Sourya Dipta Das, Himanshu Ladia, Vaibhav Kumar, Shivansh Mishra", "title": "Writer Independent Offline Signature Recognition Using Ensemble Learning", "comments": "6 pages, 2 figures, International Conference on Data Science, Machine\n  Learning & Applications (ICDSMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of Handwritten Signature Verification has been broadly researched in\nthe last decades, but remains an open research problem. In offline (static)\nsignature verification, the dynamic information of the signature writing\nprocess is lost, and it is difficult to design good feature extractors that can\ndistinguish genuine signatures and skilled forgeries. This verification task is\neven harder in writer independent scenarios which is undeniably fiscal for\nrealistic cases. In this paper, we have proposed an Ensemble model for offline\nwriter, independent signature verification task with Deep learning. We have\nused two CNNs for feature extraction, after that RGBT for classification &\nStacking to generate final prediction vector. We have done extensive\nexperiments on various datasets from various sources to maintain a variance in\nthe dataset. We have achieved the state of the art performance on various\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 09:57:15 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Ladia", "Himanshu", ""], ["Kumar", "Vaibhav", ""], ["Mishra", "Shivansh", ""]]}, {"id": "1901.06514", "submitter": "Alberto Garcia-Garcia", "authors": "Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John\n  Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez and\n  Alvaro Jover-Alvarez", "title": "The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor\n  Dataset of Sequences with Robot Trajectories and Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enter the RobotriX, an extremely photorealistic indoor dataset designed to\nenable the application of deep learning techniques to a wide variety of robotic\nvision problems. The RobotriX consists of hyperrealistic indoor scenes which\nare explored by robot agents which also interact with objects in a visually\nrealistic manner in that simulated world. Photorealistic scenes and robots are\nrendered by Unreal Engine into a virtual reality headset which captures gaze so\nthat a human operator can move the robot and use controllers for the robotic\nhands; scene information is dumped on a per-frame basis so that it can be\nreproduced offline to generate raw data and ground truth labels. By taking this\napproach, we were able to generate a dataset of 38 semantic classes totaling 8M\nstills recorded at +60 frames per second with full HD resolution. For each\nframe, RGB-D and 3D information is provided with full annotations in both\nspaces. Thanks to the high quality and quantity of both raw information and\nannotations, the RobotriX will serve as a new milestone for investigating 2D\nand 3D robotic vision tasks with large-scale data-driven techniques.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 12:49:56 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Garcia-Garcia", "Alberto", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Castro-Vargas", "John Alejandro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""], ["Jover-Alvarez", "Alvaro", ""]]}, {"id": "1901.06528", "submitter": "Vivek Kumar Mr.", "authors": "Vivek Kumar, Atul Samadhiya", "title": "Image De-Noising For Salt and Pepper Noise by Introducing New Enhanced\n  Filter", "comments": "5 pages, 3 Figures, 2 Tables, International Conference on Innovations\n  in Engineering and Technology (ICIET'2013) Dec. 25-26, 2013 Bangkok\n  (Thailand)", "journal-ref": "ICIET'2013 pp 53-57, 2013", "doi": "10.15242/IIE.E1213577", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an image is formed, factors such as lighting (spectra, source, and\nintensity) and camera characteristics (sensor response, lenses) affect the\nappearance of the image. Therefore, the prime factor that reduces the quality\nof the image is noise. It hides the important details and information of\nimages. In order to enhance the qualities of the image, the removal of noises\nbecome imperative and that should not at the cost of any loss of image\ninformation. Noise removal is one of the pre-processing stages of image\nprocessing. In this paper a new method for the enhancement of grayscale images\nis introduced, when images are corrupted by fixed valued impulse noise (salt\nand pepper noise). The proposed methodology ensures a better output for the low\nand medium density of fixed value impulse noise as compared to the other famous\nfilters like Standard Median Filter (SMF), Decision Based Median Filter (DBMF)\nand Modified Decision Based Median Filter (MDBMF) etc. The main objective of\nthe proposed method was to improve peak signal to noise ratio (PSNR), visual\nperception and reduction in the blurring of the image. The proposed algorithm\nreplaced the noisy pixel by trimmed mean value. When previous pixel values, 0s,\nand 255s are present in the particular window and all the pixel values are 0s\nand 255s then the remaining noisy pixels are replaced by mean value. The\ngray-scale image of mandrill and Lena were tested via the proposed method. The\nexperimental result shows better peak signal to noise ratio (PSNR), mean square\nerror values with better visual and human perception.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 14:10:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kumar", "Vivek", ""], ["Samadhiya", "Atul", ""]]}, {"id": "1901.06529", "submitter": "Vivek Kumar Mr.", "authors": "Vivek Kumar, Atul Samadhiya", "title": "Comparative Performance Analysis of Image De-noising Techniques", "comments": "6 pages, 9 figures, 1 Table, International Conference on Innovations\n  in Engineering and Technology (ICIET'2013) Dec. 25-26, 2013 Bangkok\n  (Thailand)", "journal-ref": "ICIET pages 47-52, 2013", "doi": "10.15242/IIE.E1213576", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an important factor which when get added to an image reduces its\nquality and appearance. So in order to enhance the image qualities, it has to\nbe removed with preserving the textural information and structural features of\nimage. There are different types of noises exist who corrupt the images.\nSelection of the denoising algorithm is application dependent. Hence, it is\nnecessary to have knowledge about the noise present in the image so as to\nselect the appropriate denoising algorithm. Objective of this paper is to\npresent brief account on types of noises, its types and different noise removal\nalgorithms. In the first section types of noises on the basis of their additive\nand multiplicative nature are being discussed. In second section a precise\nclassification and analysis of the different potential image denoising\nalgorithm is presented. At the end of paper, a comparative study of all these\nalgorithms in context of performance evaluation is done and concluded with\nseveral promising directions for future research work.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 14:16:36 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kumar", "Vivek", ""], ["Samadhiya", "Atul", ""]]}, {"id": "1901.06551", "submitter": "Gil Shamai", "authors": "Gil Shamai, Ron Slossberg, Ron Kimmel", "title": "Synthesizing facial photometries and corresponding geometries using\n  generative adversarial networks", "comments": "23 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial data synthesis is currently a well studied topic with useful\napplications in data science, computer vision, graphics and many other fields.\nGenerating realistic data is especially challenging since human perception is\nhighly sensitive to non realistic appearance. In recent times, new levels of\nrealism have been achieved by advances in GAN training procedures and\narchitectures. These successful models, however, are tuned mostly for use with\nregularly sampled data such as images, audio and video. Despite the successful\napplication of the architecture on these types of media, applying the same\ntools to geometric data poses a far greater challenge. The study of geometric\ndeep learning is still a debated issue within the academic community as the\nlack of intrinsic parametrization inherent to geometric objects prohibits the\ndirect use of convolutional filters, a main building block of today's machine\nlearning systems. In this paper we propose a new method for generating\nrealistic human facial geometries coupled with overlayed textures. We\ncircumvent the parametrization issue by imposing a global mapping from our data\nto the unit rectangle. We further discuss how to design such a mapping to\ncontrol the mapping distortion and conserve area within the mapped image. By\nrepresenting geometric textures and geometries as images, we are able to use\nadvanced GAN methodologies to generate new geometries. We address the often\nneglected topic of relation between texture and geometry and propose to use\nthis correlation to match between generated textures and their corresponding\ngeometries. We offer a new method for training GAN models on partially\ncorrupted data. Finally, we provide empirical evidence demonstrating our\ngenerative model's ability to produce examples of new identities independent\nfrom the training data while maintaining a high level of realism, two traits\nthat are often at odds.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 16:36:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shamai", "Gil", ""], ["Slossberg", "Ron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1901.06563", "submitter": "Tao Kong", "authors": "Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Jianbo Shi", "title": "Consistent Optimization for Single-Shot Object Detection", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present consistent optimization for single stage object detection.\nPrevious works of single stage object detectors usually rely on the regular,\ndense sampled anchors to generate hypothesis for the optimization of the model.\nThrough an examination of the behavior of the detector, we observe that the\nmisalignment between the optimization target and inference configurations has\nhindered the performance improvement. We propose to bride this gap by\nconsistent optimization, which is an extension of the traditional single stage\ndetector's optimization strategy. Consistent optimization focuses on matching\nthe training hypotheses and the inference quality by utilizing of the refined\nanchors during training. To evaluate its effectiveness, we conduct various\ndesign choices based on the state-of-the-art RetinaNet detector. We demonstrate\nit is the consistent optimization, not the architecture design, that yields the\nperformance boosts. Consistent optimization is nearly cost-free, and achieves\nstable performance gains independent of the model capacities or input scales.\nSpecifically, utilizing consistent optimization improves RetinaNet from 39.1 AP\nto 40.1 AP on COCO dataset without any bells or whistles, which surpasses the\naccuracy of all existing state-of-the-art one-stage detectors when adopting\nResNet-101 as backbone. The code will be made available.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 17:57:37 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 13:07:24 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Kong", "Tao", ""], ["Sun", "Fuchun", ""], ["Liu", "Huaping", ""], ["Jiang", "Yuning", ""], ["Shi", "Jianbo", ""]]}, {"id": "1901.06580", "submitter": "Senthil Yogamani", "authors": "Arindam Das, Saranya Kandan, Senthil Yogamani, Pavel Krizek", "title": "Design of Real-time Semantic Segmentation Decoder for Automated Driving", "comments": "Accepted at VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation remains a computationally intensive algorithm for\nembedded deployment even with the rapid growth of computation power. Thus\nefficient network design is a critical aspect especially for applications like\nautomated driving which requires real-time performance. Recently, there has\nbeen a lot of research on designing efficient encoders that are mostly task\nagnostic. Unlike image classification and bounding box object detection tasks,\ndecoders are computationally expensive as well for semantic segmentation task.\nIn this work, we focus on efficient design of the segmentation decoder and\nassume that an efficient encoder is already designed to provide shared features\nfor a multi-task learning system. We design a novel efficient non-bottleneck\nlayer and a family of decoders which fit into a small run-time budget using\nVGG10 as efficient encoder. We demonstrate in our dataset that experimentation\nwith various design choices led to an improvement of 10\\% from a baseline\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 19:51:31 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Das", "Arindam", ""], ["Kandan", "Saranya", ""], ["Yogamani", "Senthil", ""], ["Krizek", "Pavel", ""]]}, {"id": "1901.06585", "submitter": "Hira Ahmad", "authors": "Hira Ahmad", "title": "Face Detection and Face Recognition In the Wild Using Off-the-Shelf\n  Freely Available Components", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an easy and efficient face detection and face recognition\napproach using free software components from the internet. Face detection and\nface recognition problems have wide applications in home and office security.\nTherefore this work will helpful for those searching for a free face\noff-the-shelf face detection system. Using this system, faces can be detected\nin uncontrolled environments. In the detection phase, every individual face is\ndetected and in the recognition phase the detected faces are compared with the\nfaces in a given data set and recognized.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 20:33:35 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ahmad", "Hira", ""]]}, {"id": "1901.06595", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Ishan Misra, Laurens van der Maaten", "title": "Evaluating Text-to-Image Matching using Binary Image Selection (BISON)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing systems the ability to relate linguistic and visual content is one\nof the hallmarks of computer vision. Tasks such as text-based image retrieval\nand image captioning were designed to test this ability but come with\nevaluation measures that have a high variance or are difficult to interpret. We\nstudy an alternative task for systems that match text and images: given a text\nquery, the system is asked to select the image that best matches the query from\na pair of semantically similar images. The system's accuracy on this Binary\nImage SelectiON (BISON) task is interpretable, eliminates the reliability\nproblems of retrieval evaluations, and focuses on the system's ability to\nunderstand fine-grained visual structure. We gather a BISON dataset that\ncomplements the COCO dataset and use it to evaluate modern text-based image\nretrieval and image captioning systems. Our results provide novel insights into\nthe performance of these systems. The COCO-BISON dataset and corresponding\nevaluation code are publicly available from \\url{http://hexianghu.com/bison/}.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 22:12:01 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 16:34:48 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hu", "Hexiang", ""], ["Misra", "Ishan", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1901.06618", "submitter": "Marzyeh Ghassemi", "authors": "Denny Wu, Hirofumi Kobayashi, Charles Ding, Lei Cheng, Keisuke Goda\n  Marzyeh Ghassemi", "title": "Modeling the Biological Pathology Continuum with HSIC-regularized\n  Wasserstein Auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial challenge in image-based modeling of biomedical data is to identify\ntrends and features that separate normality and pathology. In many cases, the\nmorphology of the imaged object exhibits continuous change as it deviates from\nnormality, and thus a generative model can be trained to model this\nmorphological continuum. Moreover, given side information that correlates to\ncertain trend in morphological change, a latent variable model can be\nregularized such that its latent representation reflects this side information.\nIn this work, we use the Wasserstein Auto-encoder to model this pathology\ncontinuum, and apply the Hilbert-Schmitt Independence Criterion (HSIC) to\nenforce dependency between certain latent features and the provided side\ninformation. We experimentally show that the model can provide disentangled and\ninterpretable latent representations and also generate a continuum of\nmorphological changes that corresponds to change in the side information.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 03:40:55 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wu", "Denny", ""], ["Kobayashi", "Hirofumi", ""], ["Ding", "Charles", ""], ["Cheng", "Lei", ""], ["Ghassemi", "Keisuke Goda Marzyeh", ""]]}, {"id": "1901.06651", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Rui Zhu, Xiaobo Wang, Hailin Shi, Tianyu Fu, Shuo Wang,\n  Tao Mei, Stan Z. Li", "title": "Improved Selective Refinement Network for Face Detection", "comments": "Technical report, 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a long-standing problem in computer vision, face detection has attracted\nmuch attention in recent decades for its practical applications. With the\navailability of face detection benchmark WIDER FACE dataset, much of the\nprogresses have been made by various algorithms in recent years. Among them,\nthe Selective Refinement Network (SRN) face detector introduces the two-step\nclassification and regression operations selectively into an anchor-based face\ndetector to reduce false positives and improve location accuracy\nsimultaneously. Moreover, it designs a receptive field enhancement block to\nprovide more diverse receptive field. In this report, to further improve the\nperformance of SRN, we exploit some existing techniques via extensive\nexperiments, including new data augmentation strategy, improved backbone\nnetwork, MS COCO pretraining, decoupled classification module, segmentation\nbranch and Squeeze-and-Excitation block. Some of these techniques bring\nperformance improvements, while few of them do not well adapt to our baseline.\nAs a consequence, we present an improved SRN face detector by combining these\nuseful techniques together and obtain the best performance on widely used face\ndetection benchmark WIDER FACE dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 10:01:34 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 13:54:09 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 01:18:31 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Shifeng", ""], ["Zhu", "Rui", ""], ["Wang", "Xiaobo", ""], ["Shi", "Hailin", ""], ["Fu", "Tianyu", ""], ["Wang", "Shuo", ""], ["Mei", "Tao", ""], ["Li", "Stan Z.", ""]]}, {"id": "1901.06656", "submitter": "Arild N{\\o}kland", "authors": "Arild N{\\o}kland and Lars Hiller Eidnes", "title": "Training Neural Networks with Local Error Signals", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised training of neural networks for classification is typically\nperformed with a global loss function. The loss function provides a gradient\nfor the output layer, and this gradient is back-propagated to hidden layers to\ndictate an update direction for the weights. An alternative approach is to\ntrain the network with layer-wise loss functions. In this paper we demonstrate,\nfor the first time, that layer-wise training can approach the state-of-the-art\non a variety of image datasets. We use single-layer sub-networks and two\ndifferent supervised loss functions to generate local error signals for the\nhidden layers, and we show that the combination of these losses help with\noptimization in the context of local learning. Using local errors could be a\nstep towards more biologically plausible deep learning because the global error\ndoes not have to be transported back to hidden layers. A completely backprop\nfree variant outperforms previously reported results among methods aiming for\nhigher biological plausibility. Code is available\nhttps://github.com/anokland/local-loss\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 10:59:53 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 18:49:45 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["N\u00f8kland", "Arild", ""], ["Eidnes", "Lars Hiller", ""]]}, {"id": "1901.06672", "submitter": "Cong Gao", "authors": "Cong Gao, Mathias Unberath, Russell Taylor, and Mehran Armand", "title": "Localizing dexterous surgical tools in X-ray for image-based navigation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray image based surgical tool navigation is fast and supplies accurate\nimages of deep seated structures. Typically, recovering the 6 DOF rigid pose\nand deformation of tools with respect to the X-ray camera can be accurately\nachieved through intensity-based 2D/3D registration of 3D images or models to\n2D X-rays. However, the capture range of image-based 2D/3D registration is\ninconveniently small suggesting that automatic and robust initialization\nstrategies are of critical importance. This manuscript describes a first step\ntowards leveraging semantic information of the imaged object to initialize\n2D/3D registration within the capture range of image-based registration by\nperforming concurrent segmentation and localization of dexterous surgical tools\nin X-ray images.\n  We presented a learning-based strategy to simultaneously localize and segment\ndexterous surgical tools in X-ray images and demonstrate promising performance\non synthetic and ex vivo data. We currently investigate methods to use semantic\ninformation extracted by the proposed network to reliably and robustly\ninitialize image-based 2D/3D registration.\n  While image-based 2D/3D registration has been an obvious focus of the CAI\ncommunity, robust initialization thereof (albeit critical) has largely been\nneglected. This manuscript discusses learning-based retrieval of semantic\ninformation on imaged-objects as a stepping stone for such initialization and\nmay therefore be of interest to the IPCAI community. Since results are still\npreliminary and only focus on localization, we target the Long Abstract\ncategory.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 12:48:29 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 16:13:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Gao", "Cong", ""], ["Unberath", "Mathias", ""], ["Taylor", "Russell", ""], ["Armand", "Mehran", ""]]}, {"id": "1901.06706", "submitter": "Ning Xie", "authors": "Ning Xie, Farley Lai, Derek Doran, Asim Kadav", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing visual reasoning datasets such as Visual Question Answering (VQA),\noften suffer from biases conditioned on the question, image or answer\ndistributions. The recently proposed CLEVR dataset addresses these limitations\nand requires fine-grained reasoning but the dataset is synthetic and consists\nof similar objects and sentence structures across the dataset.\n  In this paper, we introduce a new inference task, Visual Entailment (VE) -\nconsisting of image-sentence pairs whereby a premise is defined by an image,\nrather than a natural language sentence as in traditional Textual Entailment\ntasks. The goal of a trained VE model is to predict whether the image\nsemantically entails the text. To realize this task, we build a dataset SNLI-VE\nbased on the Stanford Natural Language Inference corpus and Flickr30k dataset.\nWe evaluate various existing VQA baselines and build a model called Explainable\nVisual Entailment (EVE) system to address the VE task. EVE achieves up to 71%\naccuracy and outperforms several other state-of-the-art VQA based models.\nFinally, we demonstrate the explainability of EVE through cross-modal attention\nvisualizations. The SNLI-VE dataset is publicly available at\nhttps://github.com/ necla-ml/SNLI-VE.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 17:55:05 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Xie", "Ning", ""], ["Lai", "Farley", ""], ["Doran", "Derek", ""], ["Kadav", "Asim", ""]]}, {"id": "1901.06722", "submitter": "Jean Lienard", "authors": "Jean F. Li\\'enard", "title": "Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Point clouds obtained from photogrammetry are noisy and incomplete models of\nreality. We propose an evolutionary optimization methodology that is able to\napproximate the underlying object geometry on such point clouds. This approach\nassumes a priori knowledge on the 3D structure modeled and enables the\nidentification of a collection of primitive shapes approximating the scene.\nBuilt-in mechanisms that enforce high shape diversity and adaptive population\nsize make this method suitable to modeling both simple and complex scenes. We\nfocus here on the case of cylinder approximations and we describe, test, and\ncompare a set of mutation operators designed for optimal exploration of their\nsearch space. We assess the robustness and limitations of this algorithm\nthrough a series of synthetic examples, and we finally demonstrate its general\napplicability on two real-life cases in vegetation and industrial settings.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 20:12:34 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li\u00e9nard", "Jean F.", ""]]}, {"id": "1901.06763", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le, Bipin Indurkhya and Masaki Nakagawa", "title": "Pattern Generation Strategies for Improving Recognition of Handwritten\n  Mathematical Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging\nproblem because of the ambiguity and complexity of two-dimensional handwriting.\nMoreover, the lack of large training data is a serious issue, especially for\nacademic recognition systems. In this paper, we propose pattern generation\nstrategies that generate shape and structural variations to improve the\nperformance of recognition systems based on a small training set. For data\ngeneration, we employ the public databases: CROHME 2014 and 2016 of online\nHMEs. The first strategy employs local and global distortions to generate shape\nvariations. The second strategy decomposes an online HME into sub-online HMEs\nto get more structural variations. The hybrid strategy combines both these\nstrategies to maximize shape and structural variations. The generated online\nHMEs are converted to images for offline HME recognition. We tested our\nstrategies in an end-to-end recognition system constructed from a recent deep\nlearning model: Convolutional Neural Network and attention-based\nencoder-decoder. The results of experiments on the CROHME 2014 and 2016\ndatabases demonstrate the superiority and effectiveness of our strategies: our\nhybrid strategy achieved classification rates of 48.78% and 45.60%,\nrespectively, on these databases. These results are competitive compared to\nothers reported in recent literature. Our generated datasets are openly\navailable for research community and constitute a useful resource for the HME\nrecognition research in future.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 01:41:33 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Le", "Anh Duc", ""], ["Indurkhya", "Bipin", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "1901.06765", "submitter": "Guoxian Song", "authors": "Guoxian Song and Jianfei Cai and Tat-Jen Cham and Jianmin Zheng and\n  Juyong Zhang and Henry Fuchs", "title": "Real-time 3D Face-Eye Performance Capture of a Person Wearing VR Headset", "comments": "ACM Multimedia Conference 2018", "journal-ref": null, "doi": "10.1145/3240508.3240570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleconference or telepresence based on virtual reality (VR) headmount\ndisplay (HMD) device is a very interesting and promising application since HMD\ncan provide immersive feelings for users. However, in order to facilitate\nface-to-face communications for HMD users, real-time 3D facial performance\ncapture of a person wearing HMD is needed, which is a very challenging task due\nto the large occlusion caused by HMD. The existing limited solutions are very\ncomplex either in setting or in approach as well as lacking the performance\ncapture of 3D eye gaze movement. In this paper, we propose a convolutional\nneural network (CNN) based solution for real-time 3D face-eye performance\ncapture of HMD users without complex modification to devices. To address the\nissue of lacking training data, we generate massive pairs of HMD face-label\ndataset by data synthesis as well as collecting VR-IR eye dataset from multiple\nsubjects. Then, we train a dense-fitting network for facial region and an eye\ngaze network to regress 3D eye model parameters. Extensive experimental results\ndemonstrate that our system can efficiently and effectively produce in real\ntime a vivid personalized 3D avatar with the correct identity, pose, expression\nand eye motion corresponding to the HMD user.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 01:58:15 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Song", "Guoxian", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Zheng", "Jianmin", ""], ["Zhang", "Juyong", ""], ["Fuchs", "Henry", ""]]}, {"id": "1901.06767", "submitter": "Jianan Li", "authors": "Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, Tingfa Xu", "title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "comments": "Accepted as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout is important for graphic design and scene generation. We propose a\nnovel Generative Adversarial Network, called LayoutGAN, that synthesizes\nlayouts by modeling geometric relations of different types of 2D elements. The\ngenerator of LayoutGAN takes as input a set of randomly-placed 2D graphic\nelements and uses self-attention modules to refine their labels and geometric\nparameters jointly to produce a realistic layout. Accurate alignment is\ncritical for good layouts. We thus propose a novel differentiable wireframe\nrendering layer that maps the generated layout to a wireframe image, upon which\na CNN-based discriminator is used to optimize the layouts in image space. We\nvalidate the effectiveness of LayoutGAN in various experiments including MNIST\ndigit generation, document layout generation, clipart abstract scene generation\nand tangram graphic design.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 02:14:14 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li", "Jianan", ""], ["Yang", "Jimei", ""], ["Hertzmann", "Aaron", ""], ["Zhang", "Jianming", ""], ["Xu", "Tingfa", ""]]}, {"id": "1901.06778", "submitter": "Haofan Wang", "authors": "Haofan Wang, Zhenghua Chen, Yi Zhou", "title": "Hybrid coarse-fine classification for head pose estimation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation, which computes the intrinsic Euler angles (yaw, pitch,\nroll) from the human, is crucial for gaze estimation, face alignment, and 3D\nreconstruction. Traditional approaches heavily relies on the accuracy of facial\nlandmarks. It limits their performances, especially when the visibility of the\nface is not in good condition. In this paper, to do the estimation without\nfacial landmarks, we combine the coarse and fine regression output together for\na deep network. Utilizing more quantization units for the angles, a fine\nclassifier is trained with the help of other auxiliary coarse units.\nIntegrating regression is adopted to get the final prediction. The proposed\napproach is evaluated on three challenging benchmarks. It achieves the\nstate-of-the-art on AFLW2000, BIWI and performs favorably on AFLW. The code has\nbeen released on Github.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 03:07:05 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 23:25:54 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wang", "Haofan", ""], ["Chen", "Zhenghua", ""], ["Zhou", "Yi", ""]]}, {"id": "1901.06782", "submitter": "Yanxiang Gong", "authors": "Yanxiang Gong, Linjie Deng, Zheng Ma and Mei Xie", "title": "Generating Text Sequence Images for Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, methods based on deep learning have dominated the field of text\nrecognition. With a large number of training data, most of them can achieve the\nstate-of-the-art performances. However, it is hard to harvest and label\nsufficient text sequence images from the real scenes. To mitigate this issue,\nseveral methods to synthesize text sequence images were proposed, yet they\nusually need complicated preceding or follow-up steps. In this work, we present\na method which is able to generate infinite training data without any auxiliary\npre/post-process. We tackle the generation task as an image-to-image\ntranslation one and utilize conditional adversarial networks to produce\nrealistic text sequence images in the light of the semantic ones. Some\nevaluation metrics are involved to assess our method and the results\ndemonstrate that the caliber of the data is satisfactory. The code and dataset\nwill be publicly available soon.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 03:44:23 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Gong", "Yanxiang", ""], ["Deng", "Linjie", ""], ["Ma", "Zheng", ""], ["Xie", "Mei", ""]]}, {"id": "1901.06783", "submitter": "Weihao Gan", "authors": "Yiru Wang, Weihao Gan, Jie Yang, Wei Wu, Junjie Yan", "title": "Dynamic Curriculum Learning for Imbalanced Data Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human attribute analysis is a challenging task in the field of computer\nvision, since the data is largely imbalance-distributed. Common techniques such\nas re-sampling and cost-sensitive learning require prior-knowledge to train the\nsystem. To address this problem, we propose a unified framework called Dynamic\nCurriculum Learning (DCL) to online adaptively adjust the sampling strategy and\nloss learning in single batch, which resulting in better generalization and\ndiscrimination. Inspired by the curriculum learning, DCL consists of two level\ncurriculum schedulers: (1) sampling scheduler not only manages the data\ndistribution from imbalanced to balanced but also from easy to hard; (2) loss\nscheduler controls the learning importance between classification and metric\nlearning loss. Learning from these two schedulers, we demonstrate our DCL\nframework with the new state-of-the-art performance on the widely used face\nattribute dataset CelebA and pedestrian attribute dataset RAP.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 03:48:10 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 05:58:59 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wang", "Yiru", ""], ["Gan", "Weihao", ""], ["Yang", "Jie", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "1901.06792", "submitter": "Sunder Ali Khowaja", "authors": "Sunder Ali Khowaja and Seok-Lyong Lee", "title": "Semantic Image Networks for Human Action Recognition", "comments": "30 pages", "journal-ref": "International Journal of Computer Vision-2019", "doi": "10.1007/s11263-019-01248-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the use of a semantic image, an improved\nrepresentation for video analysis, principally in combination with Inception\nnetworks. The semantic image is obtained by applying localized sparse\nsegmentation using global clustering (LSSGC) prior to the approximate rank\npooling which summarizes the motion characteristics in single or multiple\nimages. It incorporates the background information by overlaying a static\nbackground from the window onto the subsequent segmented frames. The idea is to\nimprove the action-motion dynamics by focusing on the region which is important\nfor action recognition and encoding the temporal variances using the frame\nranking method. We also propose the sequential combination of\nInception-ResNetv2 and long-short-term memory network (LSTM) to leverage the\ntemporal variances for improved recognition performance. Extensive analysis has\nbeen carried out on UCF101 and HMDB51 datasets which are widely used in action\nrecognition studies. We show that (i) the semantic image generates better\nactivations and converges faster than its original variant, (ii) using\nsegmentation prior to approximate rank pooling yields better recognition\nperformance, (iii) The use of LSTM leverages the temporal variance information\nfrom approximate rank pooling to model the action behavior better than the base\nnetwork, (iv) the proposed representations can be adaptive as they can be used\nwith existing methods such as temporal segment networks to improve the\nrecognition performance, and (v) our proposed four-stream network architecture\ncomprising of semantic images and semantic optical flows achieves\nstate-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101\nand HMDB51, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 05:27:24 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Khowaja", "Sunder Ali", ""], ["Lee", "Seok-Lyong", ""]]}, {"id": "1901.06802", "submitter": "Anders Eriksson", "authors": "Mateusz Michalkiewicz and Jhony K. Pontes and Dominic Jack and Mahsa\n  Baktashmotlagh and Anders Eriksson", "title": "Deep Level Sets: Implicit Surface Representations for 3D Shape Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing 3D surface representation approaches are unable to accurately\nclassify pixels and their orientation lying on the boundary of an object. Thus\nresulting in coarse representations which usually require post-processing steps\nto extract 3D surface meshes. To overcome this limitation, we propose an\nend-to-end trainable model that directly predicts implicit surface\nrepresentations of arbitrary topology by optimising a novel geometric loss\nfunction. Specifically, we propose to represent the output as an oriented level\nset of a continuous embedding function, and incorporate this in a deep\nend-to-end learning framework by introducing a variational shape inference\nformulation. We investigate the benefits of our approach on the task of 3D\nsurface prediction and demonstrate its ability to produce a more accurate\nreconstruction compared to voxel-based representations. We further show that\nour model is flexible and can be applied to a variety of shape inference\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 06:32:31 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Michalkiewicz", "Mateusz", ""], ["Pontes", "Jhony K.", ""], ["Jack", "Dominic", ""], ["Baktashmotlagh", "Mahsa", ""], ["Eriksson", "Anders", ""]]}, {"id": "1901.06823", "submitter": "Pingping Zhang Mr", "authors": "Pingping Zhang, Wei Liu, Huchuan Lu and Chunhua Shen", "title": "Salient Object Detection with Lossless Feature Reflection and Weighted\n  Structural Loss", "comments": "To appear in IEEE Transaction on Image Processing. This paper is\n  extended from arXiv:1802.06527", "journal-ref": null, "doi": "10.1109/TIP.2019.2893535", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD), which aims to identify and locate the most\nsalient pixels or regions in images, has been attracting more and more interest\ndue to its various real-world applications. However, this vision task is quite\nchallenging, especially under complex image scenes. Inspired by the intrinsic\nreflection of natural images, in this paper we propose a novel feature learning\nframework for large-scale salient object detection. Specifically, we design a\nsymmetrical fully convolutional network (SFCN) to effectively learn\ncomplementary saliency features under the guidance of lossless feature\nreflection. The location information, together with contextual and semantic\ninformation, of salient objects are jointly utilized to supervise the proposed\nnetwork for more accurate saliency predictions. In addition, to overcome the\nblurry boundary problem, we propose a new weighted structural loss function to\nensure clear object boundaries and spatially consistent saliency. The coarse\nprediction results are effectively refined by these structural information for\nperformance improvements. Extensive experiments on seven saliency detection\ndatasets demonstrate that our approach achieves consistently superior\nperformance and outperforms the very recent state-of-the-art methods with a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 08:45:34 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zhang", "Pingping", ""], ["Liu", "Wei", ""], ["Lu", "Huchuan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1901.06829", "submitter": "Dongliang He", "authors": "Dongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao Liu, Shilei Wen", "title": "Read, Watch, and Move: Reinforcement Learning for Temporally Grounding\n  Natural Language Descriptions in Videos", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of video grounding, which temporally localizes a natural language\ndescription in a video, plays an important role in understanding videos.\nExisting studies have adopted strategies of sliding window over the entire\nvideo or exhaustively ranking all possible clip-sentence pairs in a\npre-segmented video, which inevitably suffer from exhaustively enumerated\ncandidates. To alleviate this problem, we formulate this task as a problem of\nsequential decision making by learning an agent which regulates the temporal\ngrounding boundaries progressively based on its policy. Specifically, we\npropose a reinforcement learning based framework improved by multi-task\nlearning and it shows steady performance gains by considering additional\nsupervised boundary information during training. Our proposed framework\nachieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset\nand Charades-STA dataset while observing only 10 or less clips per video.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 09:00:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["He", "Dongliang", ""], ["Zhao", "Xiang", ""], ["Huang", "Jizhou", ""], ["Li", "Fu", ""], ["Liu", "Xiao", ""], ["Wen", "Shilei", ""]]}, {"id": "1901.06882", "submitter": "Sunoh Kim", "authors": "Sunoh Kim, Kimin Yun, Jongyoul Park and Jin Young Choi", "title": "Skeleton-based Action Recognition of People Handling Objects", "comments": "Accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual surveillance systems, it is necessary to recognize the behavior of\npeople handling objects such as a phone, a cup, or a plastic bag. In this\npaper, to address this problem, we propose a new framework for recognizing\nobject-related human actions by graph convolutional networks using human and\nobject poses. In this framework, we construct skeletal graphs of reliable human\nposes by selectively sampling the informative frames in a video, which include\nhuman joints with high confidence scores obtained in pose estimation. The\nskeletal graphs generated from the sampled frames represent human poses related\nto the object position in both the spatial and temporal domains, and these\ngraphs are used as inputs to the graph convolutional networks. Through\nexperiments over an open benchmark and our own data sets, we verify the\nvalidity of our framework in that our method outperforms the state-of-the-art\nmethod for skeleton-based action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:20:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kim", "Sunoh", ""], ["Yun", "Kimin", ""], ["Park", "Jongyoul", ""], ["Choi", "Jin Young", ""]]}, {"id": "1901.06919", "submitter": "Mikael Le Pendu", "authors": "Mikael Le Pendu and Christine Guillemot and Aljosa Smolic", "title": "A Fourier Disparity Layer representation for Light Fields", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2922099", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a new Light Field representation for efficient\nLight Field processing and rendering called Fourier Disparity Layers (FDL). The\nproposed FDL representation samples the Light Field in the depth (or\nequivalently the disparity) dimension by decomposing the scene as a discrete\nsum of layers. The layers can be constructed from various types of Light Field\ninputs including a set of sub-aperture images, a focal stack, or even a\ncombination of both. From our derivations in the Fourier domain, the layers are\nsimply obtained by a regularized least square regression performed\nindependently at each spatial frequency, which is efficiently parallelized in a\nGPU implementation. Our model is also used to derive a gradient descent based\ncalibration step that estimates the input view positions and an optimal set of\ndisparity values required for the layer construction. Once the layers are\nknown, they can be simply shifted and filtered to produce different viewpoints\nof the scene while controlling the focus and simulating a camera aperture of\narbitrary shape and size. Our implementation in the Fourier domain allows real\ntime Light Field rendering. Finally, direct applications such as view\ninterpolation or extrapolation and denoising are presented and evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:11:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pendu", "Mikael Le", ""], ["Guillemot", "Christine", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1901.06920", "submitter": "Debarghya China", "authors": "Sumanth Nandamuri, Debarghya China, Pabitra Mitra, Debdoot Sheet", "title": "SUMNet: Fully Convolutional Model for Fast Segmentation of Anatomical\n  Structures in Ultrasound Volumes", "comments": "Accepted in ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging is generally employed for real-time investigation of\ninternal anatomy of the human body for disease identification. Delineation of\nthe anatomical boundary of organs and pathological lesions is quite challenging\ndue to the stochastic nature of speckle intensity in the images, which also\nintroduces visual fatigue for the observer. This paper introduces a fully\nconvolutional neural network based method to segment organ and pathologies in\nultrasound volume by learning the spatial-relationship between closely related\nclasses in the presence of stochastically varying speckle intensity. We propose\na convolutional encoder-decoder like framework with (i) feature concatenation\nacross matched layers in encoder and decoder and (ii) index passing based\nunpooling at the decoder for semantic segmentation of ultrasound volumes. We\nhave experimentally evaluated the performance on publicly available datasets\nconsisting of $10$ intravascular ultrasound pullback acquired at $20$ MHz and\n$16$ freehand thyroid ultrasound volumes acquired $11 - 16$ MHz. We have\nobtained a dice score of $0.93 \\pm 0.08$ and $0.92 \\pm 0.06$ respectively,\nfollowing a $10$-fold cross-validation experiment while processing frame of\n$256 \\times 384$ pixel in $0.035$s and a volume of $256 \\times 384 \\times 384$\nvoxel in $13.44$s.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:16:18 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Nandamuri", "Sumanth", ""], ["China", "Debarghya", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1901.06926", "submitter": "Debarghya China", "authors": "Debarghya China, Pabitra Mitra, Debdoot Sheet", "title": "Segmentation of Lumen and External Elastic Laminae in Intravascular\n  Ultrasound Images using Ultrasonic Backscattering Physics Initialized\n  Multiscale Random Walks", "comments": "ICVGIP MedImage 2016", "journal-ref": null, "doi": "10.1007/978-3-319-68124-5_34", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery disease accounts for a large number of deaths across the\nworld and clinicians generally prefer using x-ray computed tomography or\nmagnetic resonance imaging for localizing vascular pathologies. Interventional\nimaging modalities like intravascular ultrasound (IVUS) are used to adjunct\ndiagnosis of atherosclerotic plaques in vessels, and help assess morphological\nstate of the vessel and plaque, which play a significant role for treatment\nplanning. Since speckle intensity in IVUS images are inherently stochastic in\nnature and challenge clinicians with accurate visibility of the vessel wall\nboundaries, it requires automation. In this paper we present a method for\nsegmenting the lumen and external elastic laminae of the artery wall in IVUS\nimages using random walks over a multiscale pyramid of Gaussian decomposed\nframes. The seeds for the random walker are initialized by supervised learning\nof ultrasonic backscattering and attenuation statistical mechanics from\nlabelled training samples. We have experimentally evaluated the performance\nusing $77$ IVUS images acquired at $40$ MHz that are available in the IVUS\nsegmentation challenge\ndataset\\footnote{http://www.cvc.uab.es/IVUSchallenge2011/dataset.html} to\nobtain a Jaccard score of $0.89 \\pm 0.14$ for lumen and $0.85 \\pm 0.12$ for\nexternal elastic laminae segmentation over a $10$-fold cross-validation study.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:29:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["China", "Debarghya", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1901.06955", "submitter": "Erwei Wang", "authors": "Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu,\n  Wayne Luk, Peter Y. K. Cheung, George A. Constantinides", "title": "Deep Neural Network Approximation for Custom Hardware: Where We've Been,\n  Where We're Going", "comments": "Accepted manuscript uploaded 21/01/19. DOA 15/01/19", "journal-ref": "ACM Comput. Surv. 52, 2, Article 40 (May 2019), 39 pages", "doi": "10.1145/3309551", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proven to be particularly effective in visual and\naudio recognition tasks. Existing models tend to be computationally expensive\nand memory intensive, however, and so methods for hardware-oriented\napproximation have become a hot topic. Research has shown that custom\nhardware-based neural network accelerators can surpass their general-purpose\nprocessor equivalents in terms of both throughput and energy efficiency.\nApplication-tailored accelerators, when co-designed with approximation-based\nnetwork training methods, transform large, dense and computationally expensive\nnetworks into small, sparse and hardware-efficient alternatives, increasing the\nfeasibility of network deployment. In this article, we provide a comprehensive\nevaluation of approximation methods for high-performance network inference\nalong with in-depth discussion of their effectiveness for custom hardware\nimplementation. We also include proposals for future research based on a\nthorough analysis of current trends. This article represents the first survey\nproviding detailed comparisons of custom hardware accelerators featuring\napproximation for both convolutional and recurrent neural networks, through\nwhich we hope to inspire exciting new developments in the field.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 15:14:31 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 12:33:10 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 14:46:51 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 14:30:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Erwei", ""], ["Davis", "James J.", ""], ["Zhao", "Ruizhe", ""], ["Ng", "Ho-Cheung", ""], ["Niu", "Xinyu", ""], ["Luk", "Wayne", ""], ["Cheung", "Peter Y. K.", ""], ["Constantinides", "George A.", ""]]}, {"id": "1901.06988", "submitter": "Daniele Ravi", "authors": "Daniele Rav\\`i, Agnieszka Barbara Szczotka, Stephen P Pereira, Tom\n  Vercauteren", "title": "Adversarial training with cycle consistency for unsupervised\n  super-resolution in endomicroscopy", "comments": "Accepted for publication on Medical Image Analysis journal", "journal-ref": null, "doi": "10.1016/j.media.2019.01.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, endomicroscopy has become increasingly used for diagnostic\npurposes and interventional guidance. It can provide intraoperative aids for\nreal-time tissue characterization and can help to perform visual investigations\naimed for example to discover epithelial cancers. Due to physical constraints\non the acquisition process, endomicroscopy images, still today have a low\nnumber of informative pixels which hampers their quality. Post-processing\ntechniques, such as Super-Resolution (SR), are a potential solution to increase\nthe quality of these images. SR techniques are often supervised, requiring\naligned pairs of low-resolution (LR) and high-resolution (HR) images patches to\ntrain a model. However, in our domain, the lack of HR images hinders the\ncollection of such pairs and makes supervised training unsuitable. For this\nreason, we propose an unsupervised SR framework based on an adversarial deep\nneural network with a physically-inspired cycle consistency, designed to impose\nsome acquisition properties on the super-resolved images. Our framework can\nexploit HR images, regardless of the domain where they are coming from, to\ntransfer the quality of the HR images to the initial LR images. This property\ncan be particularly useful in all situations where pairs of LR/HR are not\navailable during the training. Our quantitative analysis, validated using a\ndatabase of 238 endomicroscopy video sequences from 143 patients, shows the\nability of the pipeline to produce convincing super-resolved images. A Mean\nOpinion Score (MOS) study also confirms this quantitative image quality\nassessment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 16:23:32 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 18:31:03 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Rav\u00ec", "Daniele", ""], ["Szczotka", "Agnieszka Barbara", ""], ["Pereira", "Stephen P", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1901.07012", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Ruizhou Ding, Ting-Wu Chin, Diana Marculescu", "title": "Understanding the Impact of Label Granularity on CNN-based Image\n  Classification", "comments": "10 pages. Accepted at DSBDA with ICDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, supervised learning using Convolutional Neural Networks\n(CNNs) has achieved great success in image classification tasks, and large\nscale labeled datasets have contributed significantly to this achievement.\nHowever, the definition of a label is often application dependent. For example,\nan image of a cat can be labeled as \"cat\" or perhaps more specifically \"Persian\ncat.\" We refer to this as label granularity. In this paper, we conduct\nextensive experiments using various datasets to demonstrate and analyze how and\nwhy training based on fine-grain labeling, such as \"Persian cat\" can improve\nCNN accuracy on classifying coarse-grain classes, in this case \"cat.\" The\nexperimental results show that training CNNs with fine-grain labels improves\nboth network's optimization and generalization capabilities, as intuitively it\nencourages the network to learn more features, and hence increases\nclassification accuracy on coarse-grain classes under all datasets considered.\nMoreover, fine-grain labels enhance data efficiency in CNN training. For\nexample, a CNN trained with fine-grain labels and only 40% of the total\ntraining data can achieve higher accuracy than a CNN trained with the full\ntraining dataset and coarse-grain labels. These results point to two possible\napplications of this work: (i) with sufficient human resources, one can improve\nCNN performance by re-labeling the dataset with fine-grain labels, and (ii)\nwith limited human resources, to improve CNN performance, rather than\ncollecting more training data, one may instead use fine-grain labels for the\ndataset. We further propose a metric called Average Confusion Ratio to\ncharacterize the effectiveness of fine-grain labeling, and show its use through\nextensive experimentation. Code is available at\nhttps://github.com/cmu-enyac/Label-Granularity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 17:53:43 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Chen", "Zhuo", ""], ["Ding", "Ruizhou", ""], ["Chin", "Ting-Wu", ""], ["Marculescu", "Diana", ""]]}, {"id": "1901.07017", "submitter": "Nicholas Watters", "authors": "Nicholas Watters, Loic Matthey, Christopher P. Burgess, Alexander\n  Lerchner", "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning\n  Disentangled Representations in VAEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple neural rendering architecture that helps variational\nautoencoders (VAEs) learn disentangled representations. Instead of the\ndeconvolutional network typically used in the decoder of VAEs, we tile\n(broadcast) the latent vector across space, concatenate fixed X- and\nY-\"coordinate\" channels, and apply a fully convolutional network with 1x1\nstride. This provides an architectural prior for dissociating positional from\nnon-positional features in the latent distribution of VAEs, yet without\nproviding any explicit supervision to this effect. We show that this\narchitecture, which we term the Spatial Broadcast decoder, improves\ndisentangling, reconstruction accuracy, and generalization to held-out regions\nin data space. It provides a particularly dramatic benefit when applied to\ndatasets with small objects. We also emphasize a method for visualizing learned\nlatent spaces that helped us diagnose our models and may prove useful for\nothers aiming to assess data representations. Finally, we show the Spatial\nBroadcast Decoder is complementary to state-of-the-art (SOTA) disentangling\ntechniques and when incorporated improves their performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 18:08:49 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 10:02:46 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Watters", "Nicholas", ""], ["Matthey", "Loic", ""], ["Burgess", "Christopher P.", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1901.07031", "submitter": "Jeremy Irvin", "authors": "Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana\n  Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie\n  Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K.\n  Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel,\n  Matthew P. Lungren, Andrew Y. Ng", "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and\n  Expert Comparison", "comments": "Published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large, labeled datasets have driven deep learning methods to achieve\nexpert-level performance on a variety of medical imaging tasks. We present\nCheXpert, a large dataset that contains 224,316 chest radiographs of 65,240\npatients. We design a labeler to automatically detect the presence of 14\nobservations in radiology reports, capturing uncertainties inherent in\nradiograph interpretation. We investigate different approaches to using the\nuncertainty labels for training convolutional neural networks that output the\nprobability of these observations given the available frontal and lateral\nradiographs. On a validation set of 200 chest radiographic studies which were\nmanually annotated by 3 board-certified radiologists, we find that different\nuncertainty approaches are useful for different pathologies. We then evaluate\nour best model on a test set composed of 500 chest radiographic studies\nannotated by a consensus of 5 board-certified radiologists, and compare the\nperformance of our model to that of 3 additional radiologists in the detection\nof 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the\nmodel ROC and PR curves lie above all 3 radiologist operating points. We\nrelease the dataset to the public as a standard benchmark to evaluate\nperformance of chest radiograph interpretation models.\n  The dataset is freely available at\nhttps://stanfordmlgroup.github.io/competitions/chexpert .\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 18:41:59 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Irvin", "Jeremy", ""], ["Rajpurkar", "Pranav", ""], ["Ko", "Michael", ""], ["Yu", "Yifan", ""], ["Ciurea-Ilcus", "Silviana", ""], ["Chute", "Chris", ""], ["Marklund", "Henrik", ""], ["Haghgoo", "Behzad", ""], ["Ball", "Robyn", ""], ["Shpanskaya", "Katie", ""], ["Seekins", "Jayne", ""], ["Mong", "David A.", ""], ["Halabi", "Safwan S.", ""], ["Sandberg", "Jesse K.", ""], ["Jones", "Ricky", ""], ["Larson", "David B.", ""], ["Langlotz", "Curtis P.", ""], ["Patel", "Bhavik N.", ""], ["Lungren", "Matthew P.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1901.07042", "submitter": "Alistair Johnson", "authors": "Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum,\n  Matthew P. Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark,\n  Seth J. Berkowitz, Steven Horng", "title": "MIMIC-CXR-JPG, a large publicly available database of labeled chest\n  radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography is an extremely powerful imaging modality, allowing for a\ndetailed inspection of a patient's thorax, but requiring specialized training\nfor proper interpretation. With the advent of high performance general purpose\ncomputer vision algorithms, the accurate automated analysis of chest\nradiographs is becoming increasingly of interest to researchers. However, a key\nchallenge in the development of these techniques is the lack of sufficient\ndata. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest\nx-rays associated with 227,827 imaging studies sourced from the Beth Israel\nDeaconess Medical Center between 2011 - 2016. Images are provided with 14\nlabels derived from two natural language processing tools applied to the\ncorresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely\nfrom the MIMIC-CXR database, and aims to provide a convenient processed version\nof MIMIC-CXR, as well as to provide a standard reference for data splits and\nimage labels. All images have been de-identified to protect patient privacy.\nThe dataset is made freely available to facilitate and encourage a wide range\nof research in medical computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 19:01:00 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 03:57:01 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 17:06:27 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 14:46:18 GMT"}, {"version": "v5", "created": "Thu, 14 Nov 2019 17:34:51 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Johnson", "Alistair E. W.", ""], ["Pollard", "Tom J.", ""], ["Greenbaum", "Nathaniel R.", ""], ["Lungren", "Matthew P.", ""], ["Deng", "Chih-ying", ""], ["Peng", "Yifan", ""], ["Lu", "Zhiyong", ""], ["Mark", "Roger G.", ""], ["Berkowitz", "Seth J.", ""], ["Horng", "Steven", ""]]}, {"id": "1901.07066", "submitter": "Zhiwen Zuo", "authors": "Zhiwen Zuo, Lei Zhao, Liwen Zuo, Feng Jiang, Wei Xing, Dongming Lu", "title": "On Compression of Unsupervised Neural Nets by Pruning Weak Connections", "comments": "This paper needs to be further revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep\nBelif Networks(DBNs), are powerful in automatic feature extraction,unsupervised\nweight initialization and density estimation. In this paper,we demonstrate that\nthe parameters of these neural nets can be dramatically reduced without\naffecting their performance. We describe a method to reduce the parameters\nrequired by RBM which is the basic building block for deep architectures.\nFurther we propose an unsupervised sparse deep architectures selection\nalgorithm to form sparse deep neural networks.Experimental results show that\nthere is virtually no loss in either generative or discriminative performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:19:27 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 17:05:28 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 14:19:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zuo", "Zhiwen", ""], ["Zhao", "Lei", ""], ["Zuo", "Liwen", ""], ["Jiang", "Feng", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "1901.07076", "submitter": "Yanwu Xu", "authors": "Yanwu Xu, Mingming Gong, Tongliang Liu, Kayhan Batmanghelich, and\n  Chaohui Wang", "title": "Robust Angular Local Descriptor Learning", "comments": "Accepted by ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the learned local descriptors have outperformed handcrafted\nones by a large margin, due to the powerful deep convolutional neural network\narchitectures such as L2-Net [1] and triplet based metric learning [2].\nHowever, there are two problems in the current methods, which hinders the\noverall performance. Firstly, the widely-used margin loss is sensitive to\nincorrect correspondences, which are prevalent in the existing local descriptor\nlearning datasets. Second, the L2 distance ignores the fact that the feature\nvectors have been normalized to unit norm. To tackle these two problems and\nfurther boost the performance, we propose a robust angular loss which 1) uses\ncosine similarity instead of L2 distance to compare descriptors and 2) relies\non a robust loss function that gives smaller penalty to triplets with negative\nrelative similarity. The resulting descriptor shows robustness on different\ndatasets, reaching the state-of-the-art result on Brown dataset , as well as\ndemonstrating excellent generalization ability on the Hpatches dataset and a\nWide Baseline Stereo dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:55:53 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 19:38:59 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Xu", "Yanwu", ""], ["Gong", "Mingming", ""], ["Liu", "Tongliang", ""], ["Batmanghelich", "Kayhan", ""], ["Wang", "Chaohui", ""]]}, {"id": "1901.07124", "submitter": "Wei Jiang", "authors": "Wei Jiang, Weiwei Sun, Andrea Tagliasacchi, Eduard Trulls, Kwang Moo\n  Yi", "title": "Linearized Multi-Sampling for Differentiable Image Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel image sampling method for differentiable image\ntransformation in deep neural networks. The sampling schemes currently used in\ndeep learning, such as Spatial Transformer Networks, rely on bilinear\ninterpolation, which performs poorly under severe scale changes, and more\nimportantly, results in poor gradient propagation. This is due to their strict\nreliance on direct neighbors. Instead, we propose to generate random auxiliary\nsamples in the vicinity of each pixel in the sampled image, and create a linear\napproximation with their intensity values. We then use this approximation as a\ndifferentiable formula for the transformed image. We demonstrate that our\napproach produces more representative gradients with a wider basin of\nconvergence for image alignment, which leads to considerable performance\nimprovements when training networks for classification tasks. This is not only\ntrue under large downsampling, but also when there are no scale changes. We\ncompare our approach with multi-scale sampling and show that we outperform it.\nWe then demonstrate that our improvements to the sampler are compatible with\nother tangential improvements to Spatial Transformer Networks and that it\nfurther improves their performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 00:07:12 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 20:05:28 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 17:17:32 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Jiang", "Wei", ""], ["Sun", "Weiwei", ""], ["Tagliasacchi", "Andrea", ""], ["Trulls", "Eduard", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1901.07169", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng", "title": "Energy Confused Adversarial Metric Learning for Zero-Shot Image\n  Retrieval and Clustering", "comments": "AAAI 2019, Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has been widely applied in many computer vision tasks,\nand recently, it is more attractive in \\emph{zero-shot image retrieval and\nclustering}(ZSRC) where a good embedding is requested such that the unseen\nclasses can be distinguished well. Most existing works deem this 'good'\nembedding just to be the discriminative one and thus race to devise powerful\nmetric objectives or hard-sample mining strategies for leaning discriminative\nembedding. However, in this paper, we first emphasize that the generalization\nability is a core ingredient of this 'good' embedding as well and largely\naffects the metric performance in zero-shot settings as a matter of fact. Then,\nwe propose the Energy Confused Adversarial Metric Learning(ECAML) framework to\nexplicitly optimize a robust metric. It is mainly achieved by introducing an\ninteresting Energy Confusion regularization term, which daringly breaks away\nfrom the traditional metric learning idea of discriminative objective devising,\nand seeks to 'confuse' the learned model so as to encourage its generalization\nability by reducing overfitting on the seen classes. We train this confusion\nterm together with the conventional metric objective in an adversarial manner.\nAlthough it seems weird to 'confuse' the network, we show that our ECAML indeed\nserves as an efficient regularization technique for metric learning and is\napplicable to various conventional metric methods. This paper empirically and\nexperimentally demonstrates the importance of learning embedding with good\ngeneralization, achieving state-of-the-art performances on the popular CUB,\nCARS, Stanford Online Products and In-Shop datasets for ZSRC tasks.\n\\textcolor[rgb]{1, 0, 0}{Code available at http://www.bhchen.cn/}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 04:23:21 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""]]}, {"id": "1901.07172", "submitter": "Ronald Salloum", "authors": "Ronald Salloum and C.-C. Jay Kuo", "title": "Efficient Image Splicing Localization via Contrastive Feature Extraction", "comments": "This manuscript was submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new data visualization and clustering technique\nfor discovering discriminative structures in high-dimensional data. This\ntechnique, referred to as cPCA++, utilizes the fact that the interesting\nfeatures of a \"target\" dataset may be obscured by high variance components\nduring traditional PCA. By analyzing what is referred to as a \"background\"\ndataset (i.e., one that exhibits the high variance principal components but not\nthe interesting structures), our technique is capable of efficiently\nhighlighting the structure that is unique to the \"target\" dataset. Similar to\nanother recently proposed algorithm called \"contrastive PCA\" (cPCA), the\nproposed cPCA++ method identifies important dataset specific patterns that are\nnot detected by traditional PCA in a wide variety of settings. However, the\nproposed cPCA++ method is significantly more efficient than cPCA, because it\ndoes not require the parameter sweep in the latter approach. We applied the\ncPCA++ method to the problem of image splicing localization. In this\napplication, we utilize authentic edges as the background dataset and the\nspliced edges as the target dataset. The proposed method is significantly more\nefficient than state-of-the-art methods, as the former does not require\niterative updates of filter weights via stochastic gradient descent and\nbackpropagation, nor the training of a classifier. Furthermore, the cPCA++\nmethod is shown to provide performance scores comparable to the\nstate-of-the-art Multi-task Fully Convolutional Network (MFCN).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 04:37:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Salloum", "Ronald", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1901.07196", "submitter": "Haimeng Zhao", "authors": "Haimeng Zhao, Peiyuan Liao", "title": "CAE-ADMM: Implicit Bitrate Optimization via ADMM-based Pruning in\n  Compressive Autoencoders", "comments": "5 pages, 4 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ADMM-pruned Compressive AutoEncoder (CAE-ADMM) that uses\nAlternative Direction Method of Multipliers (ADMM) to optimize the trade-off\nbetween distortion and efficiency of lossy image compression. Specifically,\nADMM in our method is to promote sparsity to implicitly optimize the bitrate,\ndifferent from entropy estimators used in the previous research. The\nexperiments on public datasets show that our method outperforms the original\nCAE and some traditional codecs in terms of SSIM/MS-SSIM metrics, at reasonable\ninference speed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 07:57:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 14:39:51 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 04:46:46 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 03:27:31 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhao", "Haimeng", ""], ["Liao", "Peiyuan", ""]]}, {"id": "1901.07213", "submitter": "Joohyung Lee", "authors": "Joohyung Lee, Ji Eun Oh, Min Ju Kim, Bo Yun Hur, and Dae Kyung Sohn", "title": "Reducing the Model Variance of a Rectal Cancer Segmentation Network", "comments": "published at IEEE ACCESS", "journal-ref": "IEEE Access, vol. 7, Issue. 1, pp. 182725-182733, 2019", "doi": "10.1109/ACCESS.2019.2960371", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In preoperative imaging, the demarcation of rectal cancer with magnetic\nresonance images provides an important basis for cancer staging and treatment\nplanning. Recently, deep learning has greatly improved the state-of-the-art\nmethod in automatic segmentation. However, limitations in data availability in\nthe medical field can cause large variance and consequent overfitting to\nmedical image segmentation networks. In this study, we propose methods to\nreduce the model variance of a rectal cancer segmentation network by adding a\nrectum segmentation task and performing data augmentation; the geometric\ncorrelation between the rectum and rectal cancer motivated the former approach.\nMoreover, we propose a method to perform a bias-variance analysis within an\narbitrary region-of-interest (ROI) of a segmentation network, which we applied\nto assess the efficacy of our approaches in reducing model variance. As a\nresult, adding a rectum segmentation task reduced the model variance of the\nrectal cancer segmentation network within tumor regions by a factor of 0.90;\ndata augmentation further reduced the variance by a factor of 0.89. These\napproaches also reduced the training duration by a factor of 0.96 and a further\nfactor of 0.78, respectively. Our approaches will improve the quality of rectal\ncancer staging by increasing the accuracy of its automatic demarcation and by\nproviding rectum boundary information since rectal cancer staging requires the\ndemarcation of both rectum and rectal cancer. Besides such clinical benefits,\nour method also enables segmentation networks to be assessed with bias-variance\nanalysis within an arbitrary ROI, such as a cancerous region.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 08:59:53 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 13:20:48 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 16:26:51 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 07:46:13 GMT"}, {"version": "v5", "created": "Tue, 31 Dec 2019 02:04:51 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lee", "Joohyung", ""], ["Oh", "Ji Eun", ""], ["Kim", "Min Ju", ""], ["Hur", "Bo Yun", ""], ["Sohn", "Dae Kyung", ""]]}, {"id": "1901.07222", "submitter": "Deepak Gupta", "authors": "Deepak K. Gupta, Rohit K. Shrivastava, Suhas Phadke and Jeroen\n  Goudswaard", "title": "Unsupervised Automated Event Detection using an Iterative Clustering\n  based Segmentation Approach", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of vision problems, less commonly studied, consists of detecting\nobjects in imagery obtained from physics-based experiments. These objects can\nspan in 4D (x, y, z, t) and are visible as disturbances (caused due to physical\nphenomena) in the image with background distribution being approximately\nuniform. Such objects, occasionally referred to as `events', can be considered\nas high energy blobs in the image. Unlike the images analyzed in conventional\nvision problems, very limited features are associated with such events, and\ntheir shape, size and count can vary significantly. This poses a challenge on\nthe use of pre-trained models obtained from supervised approaches.\n  In this paper, we propose an unsupervised approach involving iterative\nclustering based segmentation (ICS) which can detect target objects (events) in\nreal-time. In this approach, a test image is analyzed over several cycles, and\none event is identified per cycle. Each cycle consists of the following steps:\n(1) image segmentation using a modified k-means clustering method, (2)\nelimination of empty (with no events) segments based on statistical analysis of\neach segment, (3) merging segments that overlap (correspond to same event), and\n(4) selecting the strongest event. These four steps are repeated until all the\nevents have been identified. The ICS approach consists of a few\nhyper-parameters that have been chosen based on statistical study performed\nover a set of test images. The applicability of ICS method is demonstrated on\nseveral 2D and 3D test examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 09:24:58 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Gupta", "Deepak K.", ""], ["Shrivastava", "Rohit K.", ""], ["Phadke", "Suhas", ""], ["Goudswaard", "Jeroen", ""]]}, {"id": "1901.07223", "submitter": "Rong Kang", "authors": "Rong Kang, Jieqi Shi, Xueming Li, Yang Liu, Xiao Liu", "title": "DF-SLAM: A Deep-Learning Enhanced Visual SLAM System based on Deep Local\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the foundation of driverless vehicle and intelligent robots, Simultaneous\nLocalization and Mapping(SLAM) has attracted much attention these days.\nHowever, non-geometric modules of traditional SLAM algorithms are limited by\ndata association tasks and have become a bottleneck preventing the development\nof SLAM. To deal with such problems, many researchers seek to Deep Learning for\nhelp. But most of these studies are limited to virtual datasets or specific\nenvironments, and even sacrifice efficiency for accuracy. Thus, they are not\npractical enough.\n  We propose DF-SLAM system that uses deep local feature descriptors obtained\nby the neural network as a substitute for traditional hand-made features.\nExperimental results demonstrate its improvements in efficiency and stability.\nDF-SLAM outperforms popular traditional SLAM systems in various scenes,\nincluding challenging scenes with intense illumination changes. Its versatility\nand mobility fit well into the need for exploring new environments. Since we\nadopt a shallow network to extract local descriptors and remain others the same\nas original SLAM systems, our DF-SLAM can still run in real-time on GPU.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 09:25:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:22:55 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Kang", "Rong", ""], ["Shi", "Jieqi", ""], ["Li", "Xueming", ""], ["Liu", "Yang", ""], ["Liu", "Xiao", ""]]}, {"id": "1901.07249", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, Lingqiao Liu", "title": "RPC: A Large-Scale Retail Product Checkout Dataset", "comments": "Project page: https://rpc-dataset.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years, emerging interest has occurred in integrating computer\nvision technology into the retail industry. Automatic checkout (ACO) is one of\nthe critical problems in this area which aims to automatically generate the\nshopping list from the images of the products to purchase. The main challenge\nof this problem comes from the large scale and the fine-grained nature of the\nproduct categories as well as the difficulty for collecting training images\nthat reflect the realistic checkout scenarios due to continuous update of the\nproducts. Despite its significant practical and research value, this problem is\nnot extensively studied in the computer vision community, largely due to the\nlack of a high-quality dataset. To fill this gap, in this work we propose a new\ndataset to facilitate relevant research. Our dataset enjoys the following\ncharacteristics: (1) It is by far the largest dataset in terms of both product\nimage quantity and product categories. (2) It includes single-product images\ntaken in a controlled environment and multi-product images taken by the\ncheckout system. (3) It provides different levels of annotations for the\ncheck-out images. Comparing with the existing datasets, ours is closer to the\nrealistic setting and can derive a variety of research problems. Besides the\ndataset, we also benchmark the performance on this dataset with various\napproaches. The dataset and related resources can be found at\n\\url{https://rpc-dataset.github.io/}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 10:38:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Cui", "Quan", ""], ["Yang", "Lei", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""]]}, {"id": "1901.07261", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Bo Zhang and Hailong Ma and Ruijun Xu and Qingyuan\n  Li", "title": "Fast, Accurate and Lightweight Super-Resolution with Neural Architecture\n  Search", "comments": "Accepted to ICPR20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks demonstrate impressive results in the\nsuper-resolution domain. A series of studies concentrate on improving peak\nsignal noise ratio (PSNR) by using much deeper layers, which are not friendly\nto constrained resources. Pursuing a trade-off between the restoration capacity\nand the simplicity of models is still non-trivial. Recent contributions are\nstruggling to manually maximize this balance, while our work achieves the same\ngoal automatically with neural architecture search. Specifically, we handle\nsuper-resolution with a multi-objective approach. We also propose an elastic\nsearch tactic at both micro and macro level, based on a hybrid controller that\nprofits from evolutionary computation and reinforcement learning. Quantitative\nexperiments help us to draw a conclusion that our generated models dominate\nmost of the state-of-the-art methods with respect to the individual FLOPS.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 11:08:14 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 07:08:39 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 13:45:56 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Ma", "Hailong", ""], ["Xu", "Ruijun", ""], ["Li", "Qingyuan", ""]]}, {"id": "1901.07273", "submitter": "Ijaz Akhter", "authors": "Ijaz Akhter, Cheong Loong Fah, Richard Hartley", "title": "Super-Trajectories: A Compact Yet Rich Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new video representation in terms of an over-segmentation of\ndense trajectories covering the whole video. Trajectories are often used to\nencode long-temporal information in several computer vision applications.\nSimilar to temporal superpixels, a temporal slice of super-trajectories are\nsuperpixels, but the later contains more information because it maintains the\nlong dense pixel-wise tracking information as well. The main challenge in using\ntrajectories for any application, is the accumulation of tracking error in the\ntrajectory construction. For our problem, this results in disconnected\nsuperpixels. We exploit constraints for edges in addition to trajectory based\ncolor and position similarity. Analogous to superpixels as a preprocessing tool\nfor images, the proposed representation has its applications for videos,\nespecially in trajectory based video analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 11:47:18 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Akhter", "Ijaz", ""], ["Fah", "Cheong Loong", ""], ["Hartley", "Richard", ""]]}, {"id": "1901.07278", "submitter": "Ga\\\"el \\'Ecorchard", "authors": "Ga\\\"el \\'Ecorchard and Adam Heinrich and Libor P\\v{r}eu\\v{c}il", "title": "Ego-motion Sensor for Unmanned Aerial Vehicles Based on a Single-Board\n  Computer", "comments": "Proceedings of CLAWAR 2017: 20th International Conference on Climbing\n  and Walking Robots and the Support Technologies for Mobile Machines", "journal-ref": "Human-Centric Robotics, pp. 189-196, 2017", "doi": "10.1142/9789813231047_0025", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and implementation of a ground-related\nodometry sensor suitable for micro aerial vehicles. The sensor is based on a\nground-facing camera and a single-board Linux-based embedded computer with a\nmultimedia System on a Chip (SoC). The SoC features a hardware video encoder\nwhich is used to estimate the optical flow online. The optical flow is then\nused in combination with a distance sensor to estimate the vehicle's velocity.\nThe proposed sensor is compared to a similar existing solution and evaluated in\nboth indoor and outdoor environments.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 12:01:00 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["\u00c9corchard", "Ga\u00ebl", ""], ["Heinrich", "Adam", ""], ["P\u0159eu\u010dil", "Libor", ""]]}, {"id": "1901.07288", "submitter": "Mingyang Geng", "authors": "Mingyang Geng, Suning Shang, Bo Ding, Huaimin Wang, Pengfei Zhang, Lei\n  Zhang", "title": "Unsupervised Learning-based Depth Estimation aided Visual SLAM Approach", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RGB-D camera maintains a limited range for working and is hard to\naccurately measure the depth information in a far distance. Besides, the RGB-D\ncamera will easily be influenced by strong lighting and other external factors,\nwhich will lead to a poor accuracy on the acquired environmental depth\ninformation. Recently, deep learning technologies have achieved great success\nin the visual SLAM area, which can directly learn high-level features from the\nvisual inputs and improve the estimation accuracy of the depth information.\nTherefore, deep learning technologies maintain the potential to extend the\nsource of the depth information and improve the performance of the SLAM system.\nHowever, the existing deep learning-based methods are mainly supervised and\nrequire a large amount of ground-truth depth data, which is hard to acquire\nbecause of the realistic constraints. In this paper, we first present an\nunsupervised learning framework, which not only uses image reconstruction for\nsupervising but also exploits the pose estimation method to enhance the\nsupervised signal and add training constraints for the task of monocular depth\nand camera motion estimation. Furthermore, we successfully exploit our\nunsupervised learning framework to assist the traditional ORB-SLAM system when\nthe initialization module of ORB-SLAM method could not match enough features.\nQualitative and quantitative experiments have shown that our unsupervised\nlearning framework performs the depth estimation task comparable to the\nsupervised methods and outperforms the previous state-of-the-art approach by\n$13.5\\%$ on KITTI dataset. Besides, our unsupervised learning framework could\nsignificantly accelerate the initialization process of ORB-SLAM system and\neffectively improve the accuracy on environmental mapping in strong lighting\nand weak texture scenes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 13:07:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Geng", "Mingyang", ""], ["Shang", "Suning", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Zhang", "Pengfei", ""], ["Zhang", "Lei", ""]]}, {"id": "1901.07295", "submitter": "Tian Xia", "authors": "Tian Xia, Agisilaos Chartsias, Sotirios A. Tsaftaris", "title": "Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 20:20:59 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Xia", "Tian", ""], ["Chartsias", "Agisilaos", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1901.07355", "submitter": "Senthil Yogamani", "authors": "Hazem Rashed, Senthil Yogamani, Ahmad El-Sallab, Pavel Krizek and\n  Mohamed El-Helw", "title": "Optical Flow augmented Semantic Segmentation networks for Automated\n  Driving", "comments": "Accepted for Oral Presentation at VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion is a dominant cue in automated driving systems. Optical flow is\ntypically computed to detect moving objects and to estimate depth using\ntriangulation. In this paper, our motivation is to leverage the existing dense\noptical flow to improve the performance of semantic segmentation. To provide a\nsystematic study, we construct four different architectures which use RGB only,\nflow only, RGBF concatenated and two-stream RGB + flow. We evaluate these\nnetworks on two automotive datasets namely Virtual KITTI and Cityscapes using\nthe state-of-the-art flow estimator FlowNet v2. We also make use of the ground\ntruth optical flow in Virtual KITTI to serve as an ideal estimator and a\nstandard Farneback optical flow algorithm to study the effect of noise. Using\nthe flow ground truth in Virtual KITTI, two-stream architecture achieves the\nbest results with an improvement of 4% IoU. As expected, there is a large\nimprovement for moving objects like trucks, vans and cars with 38%, 28% and 6%\nincrease in IoU. FlowNet produces an improvement of 2.4% in average IoU with\nlarger improvement in the moving objects corresponding to 26%, 11% and 5% in\ntrucks, vans and cars. In Cityscapes, flow augmentation provided an improvement\nfor moving objects like motorcycle and train with an increase of 17% and 7% in\nIoU.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 19:10:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Rashed", "Hazem", ""], ["Yogamani", "Senthil", ""], ["El-Sallab", "Ahmad", ""], ["Krizek", "Pavel", ""], ["El-Helw", "Mohamed", ""]]}, {"id": "1901.07366", "submitter": "James Hahn", "authors": "James Hahn, Adriana Kovashka", "title": "Measuring Effectiveness of Video Advertisements", "comments": "9 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertisements are unavoidable in modern society. Times Square is notorious\nfor its incessant display of advertisements. Its popularity is worldwide and\nsmaller cities possess miniature versions of the display, such as Pittsburgh\nand its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district\nrecently rose to popularity due to its upscale shops and constant onslaught of\nadvertisements to pedestrians. Advertisements arise in other mediums as well.\nFor example, they help popular streaming services, such as Spotify, Hulu, and\nYoutube TV gather significant streams of revenue to reduce the cost of monthly\nsubscriptions for consumers. Ads provide an additional source of money for\ncompanies and entire industries to allocate resources toward alternative\nbusiness motives. They are attractive to companies and nearly unavoidable for\nconsumers. One challenge for advertisers is examining a advertisement's\neffectiveness or usefulness in conveying a message to their targeted\ndemographics. Rather than constructing a single, static image of content, a\nvideo advertisement possesses hundreds of frames of data with varying scenes,\nactors, objects, and complexity. Therefore, measuring effectiveness of video\nadvertisements is important to impacting a billion-dollar industry. This paper\nexplores the combination of human-annotated features and common video\nprocessing techniques to predict effectiveness ratings of advertisements\ncollected from Youtube. This task is seen as a binary (effective vs.\nnon-effective), four-way, and five-way machine learning classification task.\nThe first findings in terms of accuracy and inference on this dataset, as well\nas some of the first ad research, on a small dataset are presented. Accuracies\nof 84\\%, 65\\%, and 55\\% are reached on the binary, four-way, and five-way tasks\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 03:41:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 20:15:03 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hahn", "James", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1901.07368", "submitter": "Jiangbei Li", "authors": "Yunfeng Lin, Jiangbei Li, Hanjing Wang", "title": "DCNN-GAN: Reconstructing Realistic Image from fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing the perceptual content by analyzing human functional magnetic\nresonance imaging (fMRI) has been an active research area. However, due to its\nhigh dimensionality, complex dimensional structure, and small number of samples\navailable, reconstructing realistic images from fMRI remains challenging.\nRecently with the development of convolutional neural network (CNN) and\ngenerative adversarial network (GAN), mapping multi-voxel fMRI data to complex,\nrealistic images has been made possible. In this paper, we propose a model,\nDCNN-GAN, by combining a reconstruction network and GAN. We utilize the CNN for\nhierarchical feature extraction and the DCNN-GAN to reconstruct more realistic\nimages. Extensive experiments have been conducted, showing that our method\noutperforms previous works, regarding reconstruction quality and computational\ncost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 12:34:37 GMT"}], "update_date": "2019-01-27", "authors_parsed": [["Lin", "Yunfeng", ""], ["Li", "Jiangbei", ""], ["Wang", "Hanjing", ""]]}, {"id": "1901.07370", "submitter": "Azhar Hussain", "authors": "Azhar Hussain", "title": "SAML-QC: a Stochastic Assessment and Machine Learning based QC technique\n  for Industrial Printing", "comments": "7 Pages, 10 figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the advancement in industrial automation and high-speed printing\nhas raised numerous challenges related to the printing quality inspection of\nfinal products. This paper proposes a machine vision based technique to assess\nthe printing quality of text on industrial objects. The assessment is based on\nthree quality defects such as text misalignment, varying printing shades, and\nmisprinted text. The proposed scheme performs the quality inspection through\nstochastic assessment technique based on the second-order statistics of\nprinting. First: the text-containing area on printed product is identified\nthrough image processing techniques. Second: the alignment testing of the\nidentified text-containing area is performed. Third: optical character\nrecognition is performed to divide the text into different small boxes and only\nthe intensity value of each text-containing box is taken as a random variable\nand second-order statistics are estimated to determine the varying printing\ndefects in the text under one, two and three sigma thresholds. Fourth: the\nK-Nearest Neighbors based supervised machine learning is performed to provide\nthe stochastic process for misprinted text detection. Finally, the technique is\ndeployed on an industrial image for the printing quality assessment with\nvarying values of n and m. The results have shown that the proposed SAML-QC\ntechnique can perform real-time automated inspection for industrial printing.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 00:53:39 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Hussain", "Azhar", ""]]}, {"id": "1901.07375", "submitter": "Jay Hoon Jung", "authors": "Jay Hoon Jung, Yousun Shin, YoungMin Kwon", "title": "Extension of Convolutional Neural Network with General Image Processing\n  Kernels", "comments": "4 pages, 6 figures", "journal-ref": "TENCON 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied pre-defined kernels also known as filters or masks developed for\nimage processing to convolution neural network. Instead of letting neural\nnetworks find its own kernels, we used 41 different general-purpose kernels of\nblurring, edge detecting, sharpening, discrete cosine transformation, etc. for\nthe first layer of the convolution neural networks. This architecture, thus\nnamed as general filter convolutional neural network (GFNN), can reduce\ntraining time by 30% with a better accuracy compared to the regular\nconvolutional neural network (CNN). GFNN also can be trained to achieve 90%\naccuracy with only 500 samples. Furthermore, even though these kernels are not\nspecialized for the MNIST dataset, we achieved 99.56% accuracy without ensemble\nnor any other special algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 07:44:58 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Jung", "Jay Hoon", ""], ["Shin", "Yousun", ""], ["Kwon", "YoungMin", ""]]}, {"id": "1901.07387", "submitter": "Asifullah Khan", "authors": "Asifullah Khan, Aqsa Saeed Qureshi, Noorul Wahab, Mutawara Hussain,\n  and Muhammad Yousaf Hamza", "title": "A Recent Survey on the Applications of Genetic Programming in Image\n  Processing", "comments": "31 pages, 12 figures, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming (GP) has been primarily used to tackle optimization,\nclassification, and feature selection related tasks. The widespread use of GP\nis due to its flexible and comprehensible tree-type structure. Similarly,\nresearch is also gaining momentum in the field of Image Processing, because of\nits promising results over vast areas of applications ranging from medical\nImage Processing to multispectral imaging. Image Processing is mainly involved\nin applications such as computer vision, pattern recognition, image\ncompression, storage, and medical diagnostics. This universal nature of images\nand their associated algorithm, i.e., complexities, gave an impetus to the\nexploration of GP. GP has thus been used in different ways for Image Processing\nsince its inception. Many interesting GP techniques have been developed and\nemployed in the field of Image Processing, and consequently, we aim to provide\nthe research community an extensive view of these techniques. This survey thus\npresents the diverse applications of GP in Image Processing and provides useful\nresources for further research. Also, the comparison of different parameters\nused in different applications of Image Processing is summarized in tabular\nform. Moreover, analysis of the different parameters used in Image Processing\nrelated tasks is carried-out to save the time needed in the future for\nevaluating the parameters of GP. As more advancement is made in GP\nmethodologies, its success in solving complex tasks, not only in Image\nProcessing but also in other fields, may increase. Additionally, guidelines are\nprovided for applying GP in Image Processing related tasks, the pros and cons\nof GP techniques are discussed, and some future directions are also set.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 14:12:32 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 09:22:33 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 20:42:22 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Khan", "Asifullah", ""], ["Qureshi", "Aqsa Saeed", ""], ["Wahab", "Noorul", ""], ["Hussain", "Mutawara", ""], ["Hamza", "Muhammad Yousaf", ""]]}, {"id": "1901.07419", "submitter": "Richard McKinley", "authors": "Richard McKinley, Rik Wepfer, Fabian Aschwanden, Lorenz Grunder,\n  Raphaela Muri, Christian Rummel, Rajeev Verma, Christian Weisstanner,\n  Mauricio Reyes, Anke Salmen, Andrew Chan, Franca Wagner, Roland Wiest", "title": "Simultaneous lesion and neuroanatomy segmentation in Multiple Sclerosis\n  using deep neural networks", "comments": "Substantially revised version after comments from reviewers,\n  including comparison to 3D Unet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of white matter lesions and deep grey matter structures is an\nimportant task in the quantification of magnetic resonance imaging in multiple\nsclerosis. In this paper we explore segmentation solutions based on\nconvolutional neural networks (CNNs) for providing fast, reliable segmentations\nof lesions and grey-matter structures in multi-modal MR imaging, and the\nperformance of these methods when applied to out-of-centre data.\n  We trained two state-of-the-art fully convolutional CNN architectures on the\n2016 MSSEG training dataset, which was annotated by seven independent human\nraters: a reference implementation of a 3D Unet, and a more recently proposed\n3D-to-2D architecture (DeepSCAN). We then retrained those methods on a larger\ndataset from a single centre, with and without labels for other brain\nstructures. We quantified changes in performance owing to dataset shift, and\nchanges in performance by adding the additional brain-structure labels. We also\ncompared performance with freely available reference methods.\n  Both fully-convolutional CNN methods substantially outperform other\napproaches in the literature when trained and evaluated in cross-validation on\nthe MSSEG dataset, showing agreement with human raters in the range of human\ninter-rater variability. Both architectures showed drops in performance when\ntrained on single-centre data and tested on the MSSEG dataset. When trained\nwith the addition of weak anatomical labels derived from Freesurfer, the\nperformance of the 3D Unet degraded, while the performance of the DeepSCAN net\nimproved. Overall, the DeepSCAN network predicting both lesion and anatomical\nlabels was the best-performing network examined.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 15:35:14 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 14:49:05 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:19:14 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["McKinley", "Richard", ""], ["Wepfer", "Rik", ""], ["Aschwanden", "Fabian", ""], ["Grunder", "Lorenz", ""], ["Muri", "Raphaela", ""], ["Rummel", "Christian", ""], ["Verma", "Rajeev", ""], ["Weisstanner", "Christian", ""], ["Reyes", "Mauricio", ""], ["Salmen", "Anke", ""], ["Chan", "Andrew", ""], ["Wagner", "Franca", ""], ["Wiest", "Roland", ""]]}, {"id": "1901.07439", "submitter": "Bo Jiang", "authors": "Bo Jiang and Ziyan Zhang and Jin Tang and Bin Luo", "title": "Multiple Graph Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Graph Convolutional Networks (GCNs) have been widely studied for\ngraph-structured data representation and learning. However, in many real\napplications, data are coming with multiple graphs, and it is non-trivial to\nadapt GCNs to deal with data representation with multiple graph structures. One\nmain challenge for multi-graph representation is how to exploit both structure\ninformation of each individual graph and correlation information across\nmultiple graphs simultaneously. In this paper, we propose a novel Multiple\nGraph Adversarial Learning (MGAL) framework for multi-graph representation and\nlearning. MGAL aims to learn an optimal structure-invariant and consistent\nrepresentation for multiple graphs in a common subspace via a novel adversarial\nlearning framework, which thus incorporates both structure information of\nintra-graph and correlation information of inter-graphs simultaneously. Based\non MGAL, we then provide a unified network for semi-supervised learning task.\nPromising experimental results demonstrate the effectiveness of MGAL model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 16:02:26 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Jiang", "Bo", ""], ["Zhang", "Ziyan", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1901.07441", "submitter": "Antonio Pertusa", "authors": "Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, Maria de la\n  Iglesia-Vay\\'a", "title": "PadChest: A large chest x-ray image dataset with multi-label annotated\n  reports", "comments": null, "journal-ref": "Med. Image Anal., 66 (2020), 101797", "doi": "10.1016/j.media.2020.101797", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a labeled large-scale, high resolution chest x-ray dataset for the\nautomated exploration of medical images along with their associated reports.\nThis dataset includes more than 160,000 images obtained from 67,000 patients\nthat were interpreted and reported by radiologists at Hospital San Juan\nHospital (Spain) from 2009 to 2017, covering six different position views and\nadditional information on image acquisition and patient demography. The reports\nwere labeled with 174 different radiographic findings, 19 differential\ndiagnoses and 104 anatomic locations organized as a hierarchical taxonomy and\nmapped onto standard Unified Medical Language System (UMLS) terminology. Of\nthese reports, 27% were manually annotated by trained physicians and the\nremaining set was labeled using a supervised method based on a recurrent neural\nnetwork with attention mechanisms. The labels generated were then validated in\nan independent test set achieving a 0.93 Micro-F1 score. To the best of our\nknowledge, this is one of the largest public chest x-ray database suitable for\ntraining supervised models concerning radiographs, and the first to contain\nradiographic reports in Spanish. The PadChest dataset can be downloaded from\nhttp://bimcv.cipf.es/bimcv-projects/padchest/.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 16:04:27 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 09:41:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bustos", "Aurelia", ""], ["Pertusa", "Antonio", ""], ["Salinas", "Jose-Maria", ""], ["de la Iglesia-Vay\u00e1", "Maria", ""]]}, {"id": "1901.07446", "submitter": "Kanji Tanaka", "authors": "Koji Takeda, Kanji Tanaka", "title": "Use of First and Third Person Views for Deep Intersection Classification", "comments": "5 pages, 5 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of intersection classification using monocular\non-board passive vision, with the goal of classifying traffic scenes with\nrespect to road topology. We divide the existing approaches into two broad\ncategories according to the type of input data: (a) first person vision (FPV)\napproaches, which use an egocentric view sequence as the intersection is\npassed; and (b) third person vision (TPV) approaches, which use a single view\nimmediately before entering the intersection. The FPV and TPV approaches each\nhave advantages and disadvantages. Therefore, we aim to combine them into a\nunified deep learning framework. Experimental results show that the proposed\nFPV-TPV scheme outperforms previous methods and only requires minimal FPV/TPV\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 16:26:05 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Takeda", "Koji", ""], ["Tanaka", "Kanji", ""]]}, {"id": "1901.07474", "submitter": "Xiao Wang", "authors": "Xiao Wang, Shaofei Zheng, Rui Yang, Bin Luo and Jin Tang", "title": "Pedestrian Attribute Recognition: A Survey", "comments": "Check our project page for High Resolution version of this survey:\n  https://sites.google.com/view/ahu-pedestrianattributes/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing pedestrian attributes is an important task in computer vision\ncommunity due to it plays an important role in video surveillance. Many\nalgorithms has been proposed to handle this task. The goal of this paper is to\nreview existing works using traditional methods or based on deep learning\nnetworks. Firstly, we introduce the background of pedestrian attributes\nrecognition (PAR, for short), including the fundamental concepts of pedestrian\nattributes and corresponding challenges. Secondly, we introduce existing\nbenchmarks, including popular datasets and evaluation criterion. Thirdly, we\nanalyse the concept of multi-task learning and multi-label learning, and also\nexplain the relations between these two learning algorithms and pedestrian\nattribute recognition. We also review some popular network architectures which\nhave widely applied in the deep learning community. Fourthly, we analyse\npopular solutions for this task, such as attributes group, part-based,\n\\emph{etc}. Fifthly, we shown some applications which takes pedestrian\nattributes into consideration and achieve better performance. Finally, we\nsummarized this paper and give several possible research directions for\npedestrian attributes recognition. The project page of this paper can be found\nfrom the following website:\n\\url{https://sites.google.com/view/ahu-pedestrianattributes/}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 17:16:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Xiao", ""], ["Zheng", "Shaofei", ""], ["Yang", "Rui", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1901.07518", "submitter": "Kai Chen", "authors": "Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang\n  Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy,\n  Dahua Lin", "title": "Hybrid Task Cascade for Instance Segmentation", "comments": "CVPR 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascade is a classic yet powerful architecture that has boosted performance\non various tasks. However, how to introduce cascade to instance segmentation\nremains an open question. A simple combination of Cascade R-CNN and Mask R-CNN\nonly brings limited gain. In exploring a more effective approach, we find that\nthe key to a successful instance segmentation cascade is to fully leverage the\nreciprocal relationship between detection and segmentation. In this work, we\npropose a new framework, Hybrid Task Cascade (HTC), which differs in two\nimportant aspects: (1) instead of performing cascaded refinement on these two\ntasks separately, it interweaves them for a joint multi-stage processing; (2)\nit adopts a fully convolutional branch to provide spatial context, which can\nhelp distinguishing hard foreground from cluttered background. Overall, this\nframework can learn more discriminative features progressively while\nintegrating complementary features together in each stage. Without bells and\nwhistles, a single HTC obtains 38.4 and 1.5 improvement over a strong Cascade\nMask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves\n48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018\nChallenge Object Detection Task. Code is available at:\nhttps://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:50:36 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:58:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Kai", ""], ["Pang", "Jiangmiao", ""], ["Wang", "Jiaqi", ""], ["Xiong", "Yu", ""], ["Li", "Xiaoxiao", ""], ["Sun", "Shuyang", ""], ["Feng", "Wansen", ""], ["Liu", "Ziwei", ""], ["Shi", "Jianping", ""], ["Ouyang", "Wanli", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1901.07528", "submitter": "Hongyu Yang", "authors": "Hongyu Yang, Di Huang, Yunhong Wang, Anil K. Jain", "title": "Learning Continuous Face Age Progression: A Pyramid of GANs", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.10352", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two underlying requirements of face age progression, i.e. aging accuracy\nand identity permanence, are not well studied in the literature. This paper\npresents a novel generative adversarial network based approach to address the\nissues in a coupled manner. It separately models the constraints for the\nintrinsic subject-specific characteristics and the age-specific facial changes\nwith respect to the elapsed time, ensuring that the generated faces present\ndesired aging effects while simultaneously keeping personalized properties\nstable. To ensure photo-realistic facial details, high-level age-specific\nfeatures conveyed by the synthesized face are estimated by a pyramidal\nadversarial discriminator at multiple scales, which simulates the aging effects\nwith finer details. Further, an adversarial learning scheme is introduced to\nsimultaneously train a single generator and multiple parallel discriminators,\nresulting in smooth continuous face aging sequences. The proposed method is\napplicable even in the presence of variations in pose, expression, makeup,\netc., achieving remarkably vivid aging effects. Quantitative evaluations by a\nCOTS face recognition system demonstrate that the target age distributions are\naccurately recovered, and 99.88% and 99.98% age progressed faces can be\ncorrectly verified at 0.001% FAR after age transformations of approximately 28\nand 23 years elapsed time on the MORPH and CACD databases, respectively. Both\nvisual and quantitative assessments show that the approach advances the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 05:53:35 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Yang", "Hongyu", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""], ["Jain", "Anil K.", ""]]}, {"id": "1901.07590", "submitter": "Salman Khan Dr.", "authors": "Salman Khan, Munawar Hayat, Waqas Zamir, Jianbing Shen, Ling Shao", "title": "Striking the Right Balance with Uncertainty", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning unbiased models on imbalanced datasets is a significant challenge.\nRare classes tend to get a concentrated representation in the classification\nspace which hampers the generalization of learned boundaries to new test\nexamples. In this paper, we demonstrate that the Bayesian uncertainty estimates\ndirectly correlate with the rarity of classes and the difficulty level of\nindividual samples. Subsequently, we present a novel framework for uncertainty\nbased class imbalance learning that follows two key insights: First,\nclassification boundaries should be extended further away from a more uncertain\n(rare) class to avoid overfitting and enhance its generalization. Second, each\nsample should be modeled as a multi-variate Gaussian distribution with a mean\nvector and a covariance matrix defined by the sample's uncertainty. The learned\nboundaries should respect not only the individual samples but also their\ndistribution in the feature space. Our proposed approach efficiently utilizes\nsample and class uncertainty information to learn robust features and more\ngeneralizable classifiers. We systematically study the class imbalance problem\nand derive a novel loss formulation for max-margin learning based on Bayesian\nuncertainty measure. The proposed method shows significant performance\nimprovements on six benchmark datasets for face verification, attribute\nprediction, digit/object classification and skin lesion detection.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 19:34:36 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 20:21:17 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 12:32:59 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Zamir", "Waqas", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1901.07593", "submitter": "Min Ho Cho", "authors": "Min Ho Cho, Sebastian Kurtek, and Steven N. MacEachern", "title": "Aggregated Pairwise Classification of Statistical Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of shapes is of great interest in diverse areas ranging\nfrom medical imaging to computer vision and beyond. While many statistical\nframeworks have been developed for the classification problem, most are\nstrongly tied to early formulations of the problem - with an object to be\nclassified described as a vector in a relatively low-dimensional Euclidean\nspace. Statistical shape data have two main properties that suggest a need for\na novel approach: (i) shapes are inherently infinite dimensional with strong\ndependence among the positions of nearby points, and (ii) shape space is not\nEuclidean, but is fundamentally curved. To accommodate these features of the\ndata, we work with the square-root velocity function of the curves to provide a\nuseful formal description of the shape, pass to tangent spaces of the manifold\nof shapes at different projection points which effectively separate shapes for\npairwise classification in the training data, and use principal components\nwithin these tangent spaces to reduce dimensionality. We illustrate the impact\nof the projection point and choice of subspace on the misclassification rate\nwith a novel method of combining pairwise classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 19:43:37 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Cho", "Min Ho", ""], ["Kurtek", "Sebastian", ""], ["MacEachern", "Steven N.", ""]]}, {"id": "1901.07647", "submitter": "Jong Chul Ye", "authors": "Jong Chul Ye and Woon Kyoung Sung", "title": "Understanding Geometry of Encoder-Decoder CNNs", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder networks using convolutional neural network (CNN)\narchitecture have been extensively used in deep learning literatures thanks to\nits excellent performance for various inverse problems. However, it is still\ndifficult to obtain coherent geometric view why such an architecture gives the\ndesired performance. Inspired by recent theoretical understanding on\ngeneralizability, expressivity and optimization landscape of neural networks,\nas well as the theory of convolutional framelets, here we provide a unified\ntheoretical framework that leads to a better understanding of geometry of\nencoder-decoder CNNs. Our unified mathematical framework shows that\nencoder-decoder CNN architecture is closely related to nonlinear basis\nrepresentation using combinatorial convolution frames, whose expressibility\nincreases exponentially with the network depth. We also demonstrate the\nimportance of skipped connection in terms of expressibility, and optimization\nlandscape.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 23:37:43 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 13:56:00 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Ye", "Jong Chul", ""], ["Sung", "Woon Kyoung", ""]]}, {"id": "1901.07659", "submitter": "Yazeed Alaudah", "authors": "Yazeed Alaudah, Patrycja Michalowicz, Motaz Alfarraj, and Ghassan\n  AlRegib", "title": "A Machine Learning Benchmark for Facies Classification", "comments": "Submitted to the SEG Interpretation journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent interest in using deep learning for seismic interpretation tasks,\nsuch as facies classification, has been facing a significant obstacle, namely\nthe absence of large publicly available annotated datasets for training and\ntesting models. As a result, researchers have often resorted to annotating\ntheir own training and testing data. However, different researchers may\nannotate different classes, or use different train and test splits. In\naddition, it is common for papers that apply machine learning for facies\nclassification to not contain quantitative results, and rather rely solely on\nvisual inspection of the results. All of these practices have lead to\nsubjective results and have greatly hindered the ability to compare different\nmachine learning models against each other and understand the advantages and\ndisadvantages of each approach.\n  To address these issues, we open-source a fully-annotated 3D geological model\nof the Netherlands F3 Block. This model is based on the study of the 3D seismic\ndata in addition to 26 well logs, and is grounded on the careful study of the\ngeology of the region. Furthermore, we propose two baseline models for facies\nclassification based on a deconvolution network architecture and make their\ncodes publicly available. Finally, we propose a scheme for evaluating different\nmodels on this dataset, and we share the results of our baseline models. In\naddition to making the dataset and the code publicly available, this work helps\nadvance research in this area by creating an objective benchmark for comparing\nthe results of different machine learning approaches for facies classification.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 19:04:58 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 07:09:35 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Alaudah", "Yazeed", ""], ["Michalowicz", "Patrycja", ""], ["Alfarraj", "Motaz", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.07677", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Christoph Feichtenhofer, Michael Auli, David Grangier", "title": "Modeling Human Motion with Quaternion-based Neural Networks", "comments": "Follow-up work of arXiv:1805.06485. This is a pre-print of an article\n  published in IJCV. The final authenticated version is available online at\n  https://doi.org/10.1007/s11263-019-01245-6", "journal-ref": "International Journal of Computer Vision (Special Issue on Machine\n  Vision with Deep Learning), 2019. Online ISSN: 1573-1405", "doi": "10.1007/s11263-019-01245-6", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on predicting or generating 3D human pose sequences regresses\neither joint rotations or joint positions. The former strategy is prone to\nerror accumulation along the kinematic chain, as well as discontinuities when\nusing Euler angles or exponential maps as parameterizations. The latter\nrequires re-projection onto skeleton constraints to avoid bone stretching and\ninvalid configurations. This work addresses both limitations. QuaterNet\nrepresents rotations with quaternions and our loss function performs forward\nkinematics on a skeleton to penalize absolute position errors instead of angle\nerrors. We investigate both recurrent and convolutional architectures and\nevaluate on short-term prediction and long-term generation. For the latter, our\napproach is qualitatively judged as realistic as recent neural strategies from\nthe graphics literature. Our experiments compare quaternions to Euler angles as\nwell as exponential maps and show that only a very short context is required to\nmake reliable future predictions. Finally, we show that the standard evaluation\nprotocol for Human3.6M produces high variance results and we propose a simple\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 23:56:54 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 14:49:53 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Pavllo", "Dario", ""], ["Feichtenhofer", "Christoph", ""], ["Auli", "Michael", ""], ["Grangier", "David", ""]]}, {"id": "1901.07680", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Ping Liu, Xiaochuan Fan, and Chi Zhang", "title": "A Top-down Approach to Articulated Human Pose Estimation and Tracking", "comments": "To appear in ECCVW (2018). Workshop: 2nd PoseTrack Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the tasks of multi-person human pose estimation and pose tracking in\nvideos are quite challenging. Existing methods can be categorized into two\ngroups: top-down and bottom-up approaches. In this paper, following the\ntop-down approach, we aim to build a strong baseline system with three modules:\nhuman candidate detector, single-person pose estimator and human pose tracker.\nFirstly, we choose a generic object detector among state-of-the-art methods to\ndetect human candidates. Then, the cascaded pyramid network is used to estimate\nthe corresponding human pose. Finally, we use a flow-based pose tracker to\nrender keypoint-association across frames, i.e., assigning each human candidate\na unique and temporally-consistent id, for the multi-target pose tracking\npurpose. We conduct extensive ablative experiments to validate various choices\nof models and configurations. We take part in two ECCV 18 PoseTrack challenges:\npose estimation and pose tracking.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 01:19:29 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ning", "Guanghan", ""], ["Liu", "Ping", ""], ["Fan", "Xiaochuan", ""], ["Zhang", "Chi", ""]]}, {"id": "1901.07683", "submitter": "Fanman Meng", "authors": "Fanman Meng and Kaixu Huang and Hongliang Li and Qingbo Wu", "title": "Class Activation Map Generation by Representative Class Selection and\n  Multi-Layer Feature Fusion", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing method generates class activation map (CAM) by a set of fixed\nclasses (i.e., using all the classes), while the discriminative cues between\nclass pairs are not considered. Note that activation maps by considering\ndifferent class pair are complementary, and therefore can provide more\ndiscriminative cues to overcome the shortcoming of the existing CAM generation\nthat the highlighted regions are usually local part regions rather than global\nobject regions due to the lack of object cues. In this paper, we generate CAM\nby using a few of representative classes, with aim of extracting more\ndiscriminative cues by considering each class pair to obtain CAM more globally.\nThe advantages are twofold. Firstly, the representative classes are able to\nobtain activation regions that are complementary to each other, and therefore\nleads to generating activation map more accurately. Secondly, we only need to\nconsider a small number of representative classes, making the CAM generation\nsuitable for small networks. We propose a clustering based method to select the\nrepresentative classes. Multiple binary classification models rather than a\nmultiple class classification model are used to generate the CAM. Moreover, we\npropose a multi-layer fusion based CAM generation method to simultaneously\ncombine high-level semantic features and low-level detail features. We validate\nthe proposed method on the PASCAL VOC and COCO database in terms of\nsegmentation groundtruth. Various networks such as classical network\n(Resnet-50, Resent-101 and Resnet-152) and small network (VGG-19, Resnet-18 and\nMobilenet) are considered. Experimental results show that the proposed method\nimproves the CAM generation obviously.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 01:39:08 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Meng", "Fanman", ""], ["Huang", "Kaixu", ""], ["Li", "Hongliang", ""], ["Wu", "Qingbo", ""]]}, {"id": "1901.07689", "submitter": "Jie Liang", "authors": "Jie Liang, Jufeng Yang, Ming-Ming Cheng, Paul L. Rosin, Liang Wang", "title": "Simultaneous Subspace Clustering and Cluster Number Estimating based on\n  Triplet Relationship", "comments": "13 pages, 4 figures, 6 tables", "journal-ref": null, "doi": "10.1109/TIP.2019.2903294", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a unified framework to simultaneously discover the\nnumber of clusters and group the data points into them using subspace\nclustering. Real data distributed in a high-dimensional space can be\ndisentangled into a union of low-dimensional subspaces, which can benefit\nvarious applications. To explore such intrinsic structure, state-of-the-art\nsubspace clustering approaches often optimize a self-representation problem\namong all samples, to construct a pairwise affinity graph for spectral\nclustering. However, a graph with pairwise similarities lacks robustness for\nsegmentation, especially for samples which lie on the intersection of two\nsubspaces. To address this problem, we design a hyper-correlation based data\nstructure termed as the \\textit{triplet relationship}, which reveals high\nrelevance and local compactness among three samples. The triplet relationship\ncan be derived from the self-representation matrix, and be utilized to\niteratively assign the data points to clusters. Three samples in each triplet\nare encouraged to be highly correlated and are considered as a meta-element\nduring clustering, which show more robustness than pairwise relationships when\nsegmenting two densely distributed subspaces. Based on the triplet\nrelationship, we propose a unified optimizing scheme to automatically calculate\nclustering assignments. Specifically, we optimize a model selection reward and\na fusion reward by simultaneously maximizing the similarity of triplets from\ndifferent clusters while minimizing the correlation of triplets from same\ncluster. The proposed algorithm also automatically reveals the number of\nclusters and fuses groups to avoid over-segmentation. Extensive experimental\nresults on both synthetic and real-world datasets validate the effectiveness\nand robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 02:13:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liang", "Jie", ""], ["Yang", "Jufeng", ""], ["Cheng", "Ming-Ming", ""], ["Rosin", "Paul L.", ""], ["Wang", "Liang", ""]]}, {"id": "1901.07702", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Yi-Ting Chen, Xitong Yang, Teruhisa Misu, Larry Davis", "title": "Exploring Uncertainty in Conditional Multi-Modal Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast visual retrieval as a regression problem by posing triplet loss as a\nregression loss. This enables epistemic uncertainty estimation using dropout as\na Bayesian approximation framework in retrieval. Accordingly, Monte Carlo (MC)\nsampling is leveraged to boost retrieval performance. Our approach is evaluated\non two applications: person re-identification and autonomous car driving.\nComparable state-of-the-art results are achieved on multiple datasets for the\nformer application.\n  We leverage the Honda driving dataset (HDD) for autonomous car driving\napplication. It provides multiple modalities and similarity notions for\nego-motion action understanding. Hence, we present a multi-modal conditional\nretrieval network. It disentangles embeddings into separate representations to\nencode different similarities. This form of joint learning eliminates the need\nto train multiple independent networks without any performance degradation.\nQuantitative evaluation highlights our approach competence, achieving 6%\nimprovement in a highly uncertain environment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 03:00:24 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Taha", "Ahmed", ""], ["Chen", "Yi-Ting", ""], ["Yang", "Xitong", ""], ["Misu", "Teruhisa", ""], ["Davis", "Larry", ""]]}, {"id": "1901.07711", "submitter": "Salman Khan Dr.", "authors": "Munawar Hayat, Salman Khan, Waqas Zamir, Jianbing Shen, Ling Shao", "title": "Max-margin Class Imbalanced Learning with Gaussian Affinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world object classes appear in imbalanced ratios. This poses a\nsignificant challenge for classifiers which get biased towards frequent\nclasses. We hypothesize that improving the generalization capability of a\nclassifier should improve learning on imbalanced datasets. Here, we introduce\nthe first hybrid loss function that jointly performs classification and\nclustering in a single formulation. Our approach is based on an `affinity\nmeasure' in Euclidean space that leads to the following benefits: (1) direct\nenforcement of maximum margin constraints on classification boundaries, (2) a\ntractable way to ensure uniformly spaced and equidistant cluster centers, (3)\nflexibility to learn multiple class prototypes to support diversity and\ndiscriminability in feature space. Our extensive experiments demonstrate the\nsignificant performance improvements on visual classification and verification\ntasks on multiple imbalanced datasets. The proposed loss can easily be plugged\nin any deep architecture as a differentiable block and demonstrates robustness\nagainst different levels of data imbalance and corrupted labels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 03:53:48 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Hayat", "Munawar", ""], ["Khan", "Salman", ""], ["Zamir", "Waqas", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1901.07720", "submitter": "Lizhao Li", "authors": "Lizhao Li and Song Xiao", "title": "Joint group and residual sparse coding for image compressive sensing", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal self-similarity and group sparsity have been widely utilized in\nimage compressive sensing (CS). However, when the sampling rate is low, the\ninternal prior information of degraded images may be not enough for accurate\nrestoration, resulting in loss of image edges and details. In this paper, we\npropose a joint group and residual sparse coding method for CS image recovery\n(JGRSC-CS). In the proposed JGRSC-CS, patch group is treated as the basic unit\nof sparse coding and two dictionaries (namely internal and external\ndictionaries) are applied to exploit the sparse representation of each group\nsimultaneously. The internal self-adaptive dictionary is used to remove\nartifacts, and an external Gaussian Mixture Model (GMM) dictionary, learned\nfrom clean training images, is used to enhance details and texture. To make the\nproposed method effective and robust, the split Bregman method is adopted to\nreconstruct the whole image. Experimental results manifest the proposed\nJGRSC-CS algorithm outperforms existing state-of-the-art methods in both peak\nsignal to noise ratio (PSNR) and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 04:36:02 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Li", "Lizhao", ""], ["Xiao", "Song", ""]]}, {"id": "1901.07733", "submitter": "Peng Jiang Dr.", "authors": "Shucai Li, Bin Liu, Yuxiao Ren, Yangkang Chen, Senlin Yang, Yunhai\n  Wang, Peng Jiang", "title": "Deep-Learning Inversion of Seismic Data", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.\n  3, pp. 2135-2149, March 2020", "doi": "10.1109/TGRS.2019.2953473", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to tackle the mapping challenge from time-series data\nto spatial image in the field of seismic exploration, i.e., reconstructing the\nvelocity model directly from seismic data by deep neural networks (DNNs). The\nconventional way of addressing this ill-posed inversion problem is through\niterative algorithms, which suffer from poor nonlinear mapping and strong\nnonuniqueness. Other attempts may either import human intervention errors or\nunderuse seismic data. The challenge for DNNs mainly lies in the weak spatial\ncorrespondence, the uncertain reflection-reception relationship between seismic\ndata and velocity model, as well as the time-varying property of seismic data.\nTo tackle these challenges, we propose end-to-end seismic inversion networks\n(SeisInvNets) with novel components to make the best use of all seismic data.\nSpecifically, we start with every seismic trace and enhance it with its\nneighborhood information, its observation setup, and the global context of its\ncorresponding seismic profile. From the enhanced seismic traces, the spatially\naligned feature maps can be learned and further concatenated to reconstruct a\nvelocity model. In general, we let every seismic trace contribute to the\nreconstruction of the whole velocity model by finding spatial correspondence.\nThe proposed SeisInvNet consistently produces improvements over the baselines\nand achieves promising performance on our synthesized and proposed SeisInv data\nset according to various evaluation metrics. The inversion results are more\nconsistent with the target from the aspects of velocity values, subsurface\nstructures, and geological interfaces. Moreover, the mechanism and the\ngeneralization of the proposed method are discussed and verified. Nevertheless,\nthe generalization of deep-learning-based inversion methods on real data is\nstill challenging and considering physics may be one potential solution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 05:51:05 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:34:55 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Li", "Shucai", ""], ["Liu", "Bin", ""], ["Ren", "Yuxiao", ""], ["Chen", "Yangkang", ""], ["Yang", "Senlin", ""], ["Wang", "Yunhai", ""], ["Jiang", "Peng", ""]]}, {"id": "1901.07757", "submitter": "Yu Shu", "authors": "Yu Shu, Yemin Shi, Yaowei Wang, Yixiong Zou, Qingsheng Yuan, Yonghong\n  Tian", "title": "ODN: Opening the Deep Network for Open-set Action Recognition", "comments": "6 pages, 3 figures, ICME 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the performance of action recognition has been significantly\nimproved with the help of deep neural networks. Most of the existing action\nrecognition works hold the \\textit{closed-set} assumption that all action\ncategories are known beforehand while deep networks can be well trained for\nthese categories. However, action recognition in the real world is essentially\nan \\textit{open-set} problem, namely, it is impossible to know all action\ncategories beforehand and consequently infeasible to prepare sufficient\ntraining samples for those emerging categories. In this case, applying\nclosed-set recognition methods will definitely lead to unseen-category errors.\nTo address this challenge, we propose the Open Deep Network (ODN) for the\nopen-set action recognition task. Technologically, ODN detects new categories\nby applying a multi-class triplet thresholding method, and then dynamically\nreconstructs the classification layer and \"opens\" the deep network by adding\npredictors for new categories continually. In order to transfer the learned\nknowledge to the new category, two novel methods, Emphasis Initialization and\nAllometry Training, are adopted to initialize and incrementally train the new\npredictor so that only few samples are needed to fine-tune the model. Extensive\nexperiments show that ODN can effectively detect and recognize new categories\nwith little human intervention, thus applicable to the open-set action\nrecognition tasks in the real world. Moreover, ODN can even achieve comparable\nperformance to some closed-set methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 07:49:31 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Shu", "Yu", ""], ["Shi", "Yemin", ""], ["Wang", "Yaowei", ""], ["Zou", "Yixiong", ""], ["Yuan", "Qingsheng", ""], ["Tian", "Yonghong", ""]]}, {"id": "1901.07759", "submitter": "Cheng Xue", "authors": "Cheng Xue, Qi Dou, Xueying Shi, Hao Chen, Pheng Ann Heng", "title": "Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion\n  Classification", "comments": "Accepted for publication at ISBI 2019", "journal-ref": "IEEE International Symposium on Biomedical Imaging (ISBI 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved great success in a wide variety of\nmedical image analysis tasks. However, these achievements indispensably rely on\nthe accurately-annotated datasets. If with the noisy-labeled images, the\ntraining procedure will immediately encounter difficulties, leading to a\nsuboptimal classifier. This problem is even more crucial in the medical field,\ngiven that the annotation quality requires great expertise. In this paper, we\npropose an effective iterative learning framework for noisy-labeled medical\nimage classification, to combat the lacking of high quality annotated medical\ndata. Specifically, an online uncertainty sample mining method is proposed to\neliminate the disturbance from noisy-labeled images. Next, we design a sample\nre-weighting strategy to preserve the usefulness of correctly-labeled hard\nsamples. Our proposed method is validated on skin lesion classification task,\nand achieved very promising results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:03:51 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:36:04 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Xue", "Cheng", ""], ["Dou", "Qi", ""], ["Shi", "Xueying", ""], ["Chen", "Hao", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "1901.07765", "submitter": "Wei Peng", "authors": "Wei Peng, Xiaopeng Hong, Yingyue Xu, Guoying Zhao", "title": "A Boost in Revealing Subtle Facial Expressions: A Consolidated Eulerian\n  Framework", "comments": "conference IEEE FG2019", "journal-ref": "2019 14th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2019)", "doi": "10.1109/FG.2019.8756541", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial Micro-expression Recognition (MER) distinguishes the underlying\nemotional states of spontaneous subtle facialexpressions. Automatic MER is\nchallenging because that 1) the intensity of subtle facial muscle movement is\nextremely lowand 2) the duration of ME is transient.Recent works adopt motion\nmagnification or time interpolation to resolve these issues. Nevertheless,\nexisting works dividethem into two separate modules due to their non-linearity.\nThough such operation eases the difficulty in implementation, itignores their\nunderlying connections and thus results in inevitable losses in both accuracy\nand speed. Instead, in this paper, weexplore their underlying joint\nformulations and propose a consolidated Eulerian framework to reveal the subtle\nfacial movements.It expands the temporal duration and amplifies the muscle\nmovements in micro-expressions simultaneously. Compared toexisting approaches,\nthe proposed method can not only process ME clips more efficiently but also\nmake subtle ME movementsmore distinguishable. Experiments on two public MER\ndatabases indicate that our model outperforms the state-of-the-art inboth speed\nand accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:18:00 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Peng", "Wei", ""], ["Hong", "Xiaopeng", ""], ["Xu", "Yingyue", ""], ["Zhao", "Guoying", ""]]}, {"id": "1901.07766", "submitter": "Yu Ji", "authors": "Yu Ji, Zixin Liu, Xing Hu, Peiqi Wang, Youhui Zhang", "title": "Programmable Neural Network Trojan for Pre-Trained Feature Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) trojaning attack is an emerging and important attack\nmodel that can broadly damage the system deployed with NN models. Existing\nstudies have explored the outsourced training attack scenario and transfer\nlearning attack scenario in some small datasets for specific domains, with\nlimited numbers of fixed target classes. In this paper, we propose a more\npowerful trojaning attack method for both outsourced training attack and\ntransfer learning attack, which outperforms existing studies in the capability,\ngenerality, and stealthiness. First, The attack is programmable that the\nmalicious misclassification target is not fixed and can be generated on demand\neven after the victim's deployment. Second, our trojan attack is not limited in\na small domain; one trojaned model on a large-scale dataset can affect\napplications of different domains that reuse its general features. Thirdly, our\ntrojan design is hard to be detected or eliminated even if the victims\nfine-tune the whole model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:18:48 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ji", "Yu", ""], ["Liu", "Zixin", ""], ["Hu", "Xing", ""], ["Wang", "Peiqi", ""], ["Zhang", "Youhui", ""]]}, {"id": "1901.07821", "submitter": "Yochai Blau", "authors": "Yochai Blau, Tomer Michaeli", "title": "Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff", "comments": "ICML 2019 (long oral presentation), see talk at:\n  https://slideslive.com/38917633/applications-computer-vision", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:675-685, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 11:13:33 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 20:10:48 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 14:20:20 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 13:01:06 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Blau", "Yochai", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1901.07827", "submitter": "Shaohui Lin", "authors": "Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, Xuelong Li", "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter\n  Pruning", "comments": "IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of convolutional neural networks (CNNs) in computer vision\napplications has been accompanied by a significant increase of computation and\nmemory costs, which prohibits its usage on resource-limited environments such\nas mobile or embedded devices. To this end, the research of CNN compression has\nrecently become emerging. In this paper, we propose a novel filter pruning\nscheme, termed structured sparsity regularization (SSR), to simultaneously\nspeedup the computation and reduce the memory overhead of CNNs, which can be\nwell supported by various off-the-shelf deep learning libraries. Concretely,\nthe proposed scheme incorporates two different regularizers of structured\nsparsity into the original objective function of filter pruning, which fully\ncoordinates the global outputs and local pruning operations to adaptively prune\nfilters. We further propose an Alternative Updating with Lagrange Multipliers\n(AULM) scheme to efficiently solve its optimization. AULM follows the principle\nof ADMM and alternates between promoting the structured sparsity of CNNs and\noptimizing the recognition loss, which leads to a very efficient solver (2.5x\nto the most recent work that directly solves the group sparsity-based\nregularization). Moreover, by imposing the structured sparsity, the online\ninference is extremely memory-light, since the number of filters and the output\nfeature maps are simultaneously reduced. The proposed scheme has been deployed\nto a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG,\nResNet and GoogLeNet over different datasets. Quantitative results demonstrate\nthat the proposed scheme achieves superior performance over the\nstate-of-the-art methods. We further demonstrate the proposed compression\nscheme for the task of transfer learning, including domain adaptation and\nobject detection, which also show exciting performance gains over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 11:29:39 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 01:29:58 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Lin", "Shaohui", ""], ["Ji", "Rongrong", ""], ["Li", "Yuchao", ""], ["Deng", "Cheng", ""], ["Li", "Xuelong", ""]]}, {"id": "1901.07828", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang and Truong Q. Nguyen", "title": "Random Forest with Learned Representations for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2905081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a random forest framework that learns the weights,\nshapes, and sparsities of feature representations for real-time semantic\nsegmentation. Typical filters (kernels) have predetermined shapes and\nsparsities and learn only weights. A few feature extraction methods fix weights\nand learn only shapes and sparsities. These predetermined constraints restrict\nlearning and extracting optimal features. To overcome this limitation, we\npropose an unconstrained representation that is able to extract optimal\nfeatures by learning weights, shapes, and sparsities. We, then, present the\nrandom forest framework that learns the flexible filters using an iterative\noptimization algorithm and segments input images using the learned\nrepresentations. We demonstrate the effectiveness of the proposed method using\na hand segmentation dataset for hand-object interaction and using two semantic\nsegmentation datasets. The results show that the proposed method achieves\nreal-time semantic segmentation using limited computational and memory\nresources.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 11:36:33 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1901.07838", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang and Subarna Tripathi and Truong Q. Nguyen", "title": "Toward Joint Image Generation and Compression using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generative adversarial network framework that\ngenerates compressed images instead of synthesizing raw RGB images and\ncompressing them separately. In the real world, most images and videos are\nstored and transferred in a compressed format to save storage capacity and data\ntransfer bandwidth. However, since typical generative adversarial networks\ngenerate raw RGB images, those generated images need to be compressed by a\npost-processing stage to reduce the data size. Among image compression methods,\nJPEG has been one of the most commonly used lossy compression methods for still\nimages. Hence, we propose a novel framework that generates JPEG compressed\nimages using generative adversarial networks. The novel generator consists of\nthe proposed locally connected layers, chroma subsampling layers, quantization\nlayers, residual blocks, and convolution layers. The locally connected layer is\nproposed to enable block-based operations. We also discuss training strategies\nfor the proposed architecture including the loss function and the\ntransformation between its generator and its discriminator. The proposed method\nis evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom\ndataset. The results demonstrate that the proposed method is able to generate\ncompressed data with competitive qualities. The proposed method is a promising\nbaseline method for joint image generation and compression using generative\nadversarial networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 12:07:43 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Tripathi", "Subarna", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1901.07849", "submitter": "Wei Li", "authors": "Wei Li, Chengwei Pan, Rong Zhang, Jiaping Ren, Yuexin Ma, Jin Fang,\n  Feilong Yan, Qichuan Geng, Xinyu Huang, Huajun Gong, Weiwei Xu, Guoping Wang,\n  Dinesh Manocha, Ruigang Yang", "title": "AADS: Augmented Autonomous Driving Simulation using Data-driven\n  Algorithms", "comments": null, "journal-ref": "Sci. Robotics. 4 (2019) eaaw0863", "doi": "10.1126/scirobotics.aaw0863", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation systems have become an essential component in the development and\nvalidation of autonomous driving technologies. The prevailing state-of-the-art\napproach for simulation is to use game engines or high-fidelity computer\ngraphics (CG) models to create driving scenarios. However, creating CG models\nand vehicle movements (e.g., the assets for simulation) remains a manual task\nthat can be costly and time-consuming. In addition, the fidelity of CG images\nstill lacks the richness and authenticity of real-world images and using these\nimages for training leads to degraded performance.\n  In this paper we present a novel approach to address these issues: Augmented\nAutonomous Driving Simulation (AADS). Our formulation augments real-world\npictures with a simulated traffic flow to create photo-realistic simulation\nimages and renderings. More specifically, we use LiDAR and cameras to scan\nstreet scenes. From the acquired trajectory data, we generate highly plausible\ntraffic flows for cars and pedestrians and compose them into the background.\nThe composite images can be re-synthesized with different viewpoints and sensor\nmodels. The resulting images are photo-realistic, fully annotated, and ready\nfor end-to-end training and testing of autonomous driving systems from\nperception to planning. We explain our system design and validate our\nalgorithms with a number of autonomous driving tasks from detection to\nsegmentation and predictions.\n  Compared to traditional approaches, our method offers unmatched scalability\nand realism. Scalability is particularly important for AD simulation and we\nbelieve the complexity and diversity of the real world cannot be realistically\ncaptured in a virtual environment. Our augmented approach combines the\nflexibility in a virtual environment (e.g., vehicle movements) with the\nrichness of the real world to allow effective simulation of anywhere on earth.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 12:30:25 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 15:38:14 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 02:52:29 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Li", "Wei", ""], ["Pan", "Chengwei", ""], ["Zhang", "Rong", ""], ["Ren", "Jiaping", ""], ["Ma", "Yuexin", ""], ["Fang", "Jin", ""], ["Yan", "Feilong", ""], ["Geng", "Qichuan", ""], ["Huang", "Xinyu", ""], ["Gong", "Huajun", ""], ["Xu", "Weiwei", ""], ["Wang", "Guoping", ""], ["Manocha", "Dinesh", ""], ["Yang", "Ruigang", ""]]}, {"id": "1901.07851", "submitter": "Mengke Qiao", "authors": "Ke-Wei Huang and Mengke Qiao and Xuanqi Liu and Siyuan Liu and Mingxi\n  Dai", "title": "Computer Vision and Metrics Learning for Hypothesis Testing: An\n  Application of Q-Q Plot for Normality Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new deep-learning method to construct test statistics\nby computer vision and metrics learning. The application highlighted in this\npaper is applying computer vision on Q-Q plot to construct a new test statistic\nfor normality test. To the best of our knowledge, there is no similar\napplication documented in the literature. Traditionally, there are two families\nof approaches for verifying the probability distribution of a random variable.\nResearchers either subjectively assess the Q-Q plot or objectively use a\nmathematical formula, such as Kolmogorov-Smirnov test, to formally conduct a\nnormality test. Graphical assessment by human beings is not rigorous whereas\nnormality test statistics may not be accurate enough when the uniformly most\npowerful test does not exist. It may take tens of years for statistician to\ndevelop a new test statistic that is more powerful statistically. Our proposed\nmethod integrates four components based on deep learning: an image\nrepresentation learning component of a Q-Q plot, a dimension reduction\ncomponent, a metrics learning component that best quantifies the differences\nbetween two Q-Q plots for normality test, and a new normality hypothesis\ntesting process. Our experimentation results show that the\nmachine-learning-based test statistics can outperform several widely-used\ntraditional normality tests. This study provides convincing evidence that the\nproposed method could objectively create a powerful test statistic based on Q-Q\nplots and this method could be modified to construct many more powerful test\nstatistics for other applications in the future.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 12:38:17 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 08:47:43 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Huang", "Ke-Wei", ""], ["Qiao", "Mengke", ""], ["Liu", "Xuanqi", ""], ["Liu", "Siyuan", ""], ["Dai", "Mingxi", ""]]}, {"id": "1901.07858", "submitter": "Bo Liu", "authors": "Bo Liu, Wenhao Chi, Xinran Li, Peng Li, Wenhua Liang, Haiping Liu, Wei\n  Wang, Jianxing He", "title": "Evolving the pulmonary nodules diagnosis from classical approaches to\n  deep learning aided decision support: three decades development course and\n  future prospect", "comments": "We have substantially revised the article. The previous version had\n  74 pages and 2 figures, and the lateset version had 66 pages and 6 figures", "journal-ref": "Journal of Cancer Research and Clinical Oncology 146.1 (2020):\n  153-185", "doi": "10.1007/s00432-019-03098-5", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the commonest cause of cancer deaths worldwide, and its\nmortality can be reduced significantly by performing early diagnosis and\nscreening. Since the 1960s, driven by the pressing needs to accurately and\neffectively interpret the massive volume of chest images generated daily,\ncomputer-assisted diagnosis of pulmonary nodule has opened up new opportunities\nto relax the limitation from physicians' subjectivity, experiences and fatigue.\nAnd the fair access to the reliable and affordable computer-assisted diagnosis\nwill fight the inequalities in incidence and mortality between populations. It\nhas been witnessed that significant and remarkable advances have been achieved\nsince the 1980s, and consistent endeavors have been exerted to deal with the\ngrand challenges on how to accurately detect the pulmonary nodules with high\nsensitivity at low false-positives rate as well as on how to precisely\ndifferentiate between benign and malignant nodules. There is a lack of\ncomprehensive examination of the techniques' development which is evolving the\npulmonary nodules diagnosis from classical approaches to machine\nlearning-assisted decision support. The main goal of this investigation is to\nprovide a comprehensive state-of-the-art review of the computer-assisted\nnodules detection and benign-malignant classification techniques developed over\n3 decades, which have evolved from the complicated ad hoc analysis pipeline of\nconventional approaches to the simplified seamlessly integrated deep learning\ntechniques. This review also identifies challenges and highlights opportunities\nfor future work in learning models, learning algorithms and enhancement schemes\nfor bridging current state to future prospect and satisfying future demand.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 13:04:59 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 05:54:34 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 17:31:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Bo", ""], ["Chi", "Wenhao", ""], ["Li", "Xinran", ""], ["Li", "Peng", ""], ["Liang", "Wenhua", ""], ["Liu", "Haiping", ""], ["Wang", "Wei", ""], ["He", "Jianxing", ""]]}, {"id": "1901.07878", "submitter": "Christian Otto", "authors": "Christian Otto and Sebastian Holzki and Ralph Ewerth", "title": "\"Is this an example image?\" -- Predicting the Relative Abstractness\n  Level of Image and Text", "comments": "14 pages, 6 figures, accepted at ECIR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful multimodal search and retrieval requires the automatic\nunderstanding of semantic cross-modal relations, which, however, is still an\nopen research problem. Previous work has suggested the metrics cross-modal\nmutual information and semantic correlation to model and predict cross-modal\nsemantic relations of image and text. In this paper, we present an approach to\npredict the (cross-modal) relative abstractness level of a given image-text\npair, that is whether the image is an abstraction of the text or vice versa.\nFor this purpose, we introduce a new metric that captures this specific\nrelationship between image and text at the Abstractness Level (ABS). We present\na deep learning approach to predict this metric, which relies on an autoencoder\narchitecture that allows us to significantly reduce the required amount of\nlabeled training data. A comprehensive set of publicly available scientific\ndocuments has been gathered. Experimental results on a challenging test set\ndemonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 13:42:02 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Otto", "Christian", ""], ["Holzki", "Sebastian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1901.07925", "submitter": "Danfeng Hong", "authors": "Xin Wu, Danfeng Hong, Jiaojiao Tian, Jocelyn Chanussot, Wei Li, Ran\n  Tao", "title": "ORSIm Detector: A Novel Object Detection Framework in Optical Remote\n  Sensing Imagery Using Spatial-Frequency Channel Features", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2019, 57(7):\n  5146-5158", "doi": "10.1109/TGRS.2019.2897139", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of spaceborne imaging techniques, object detection\nin optical remote sensing imagery has drawn much attention in recent decades.\nWhile many advanced works have been developed with powerful learning\nalgorithms, the incomplete feature representation still cannot meet the demand\nfor effectively and efficiently handling image deformations, particularly\nobjective scaling and rotation. To this end, we propose a novel object\ndetection framework, called optical remote sensing imagery detector (ORSIm\ndetector), integrating diverse channel features extraction, feature learning,\nfast image pyramid matching, and boosting strategy. ORSIm detector adopts a\nnovel spatial-frequency channel feature (SFCF) by jointly considering the\nrotation-invariant channel features constructed in frequency domain and the\noriginal spatial channel features (e.g., color channel, gradient magnitude).\nSubsequently, we refine SFCF using learning-based strategy in order to obtain\nthe high-level or semantically meaningful features. In the test phase, we\nachieve a fast and coarsely-scaled channel computation by mathematically\nestimating a scaling factor in the image domain. Extensive experimental results\nconducted on the two different airborne datasets are performed to demonstrate\nthe superiority and effectiveness in comparison with previous state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 14:45:07 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 04:23:02 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Wu", "Xin", ""], ["Hong", "Danfeng", ""], ["Tian", "Jiaojiao", ""], ["Chanussot", "Jocelyn", ""], ["Li", "Wei", ""], ["Tao", "Ran", ""]]}, {"id": "1901.07929", "submitter": "Jos\\'e Ignacio Orlando PhD", "authors": "Jos\\'e Ignacio Orlando, Philipp Seeb\\\"ock, Hrvoje Bogunovi\\'c, Sophie\n  Klimscha, Christoph Grechenig, Sebastian Waldstein, Bianca S. Gerendas,\n  Ursula Schmidt-Erfurth", "title": "U2-Net: A Bayesian U-Net model with epistemic uncertainty feedback for\n  photoreceptor layer segmentation in pathological OCT scans", "comments": "Accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2019", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "doi": "10.1109/ISBI.2019.8759581", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a Bayesian deep learning based model for\nsegmenting the photoreceptor layer in pathological OCT scans. Our architecture\nprovides accurate segmentations of the photoreceptor layer and produces\npixel-wise epistemic uncertainty maps that highlight potential areas of\npathologies or segmentation errors. We empirically evaluated this approach in\ntwo sets of pathological OCT scans of patients with age-related macular\ndegeneration, retinal vein oclussion and diabetic macular edema, improving the\nperformance of the baseline U-Net both in terms of the Dice index and the area\nunder the precision/recall curve. We also observed that the uncertainty\nestimates were inversely correlated with the model performance, underlying its\nutility for highlighting areas where manual inspection/correction might be\nneeded.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 14:52:33 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 01:00:11 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Seeb\u00f6ck", "Philipp", ""], ["Bogunovi\u0107", "Hrvoje", ""], ["Klimscha", "Sophie", ""], ["Grechenig", "Christoph", ""], ["Waldstein", "Sebastian", ""], ["Gerendas", "Bianca S.", ""], ["Schmidt-Erfurth", "Ursula", ""]]}, {"id": "1901.07973", "submitter": "Yuying Ge", "authors": "Yuying Ge and Ruimao Zhang and Lingyun Wu and Xiaogang Wang and Xiaoou\n  Tang and Ping Luo", "title": "DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation,\n  Segmentation and Re-Identification of Clothing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding fashion images has been advanced by benchmarks with rich\nannotations such as DeepFashion, whose labels include clothing categories,\nlandmarks, and consumer-commercial image pairs. However, DeepFashion has\nnonnegligible issues such as single clothing-item per image, sparse landmarks\n(4~8 only), and no per-pixel masks, making it had significant gap from\nreal-world scenarios. We fill in the gap by presenting DeepFashion2 to address\nthese issues. It is a versatile benchmark of four tasks including clothes\ndetection, pose estimation, segmentation, and retrieval. It has 801K clothing\nitems where each item has rich annotations such as style, scale, viewpoint,\nocclusion, bounding box, dense landmarks and masks. There are also 873K\nCommercial-Consumer clothes pairs. A strong baseline is proposed, called Match\nR-CNN, which builds upon Mask R-CNN to solve the above four tasks in an\nend-to-end manner. Extensive evaluations are conducted with different\ncriterions in DeepFashion2.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 16:06:17 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ge", "Yuying", ""], ["Zhang", "Ruimao", ""], ["Wu", "Lingyun", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""], ["Luo", "Ping", ""]]}, {"id": "1901.08001", "submitter": "Jonathan Schwartz", "authors": "Jonathan Schwartz, Yi Jiang, Yongjie Wang, Anthony Aiello, Pallab\n  Bhattacharya, Hui Yuan, Zetian Mi, Nabil Bassim, Robert Hovden", "title": "Removing Stripes, Scratches, and Curtaining with Non-Recoverable\n  Compressed Sensing", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.1017/S1431927619000254", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Highly-directional image artifacts such as ion mill curtaining, mechanical\nscratches, or image striping from beam instability degrade the interpretability\nof micrographs. These unwanted, aperiodic features extend the image along a\nprimary direction and occupy a small wedge of information in Fourier space.\nDeleting this wedge of data replaces stripes, scratches, or curtaining, with\nmore complex streaking and blurring artifacts-known within the tomography\ncommunity as missing wedge artifacts. Here, we overcome this problem by\nrecovering the missing region using total variation minimization, which\nleverages image sparsity based reconstruction techniques-colloquially referred\nto as compressed sensing-to reliably restore images corrupted by stripe like\nfeatures. Our approach removes beam instability, ion mill curtaining,\nmechanical scratches, or any stripe features and remains robust at low\nsignal-to-noise. The success of this approach is achieved by exploiting\ncompressed sensings inability to recover directional structures that are highly\nlocalized and missing in Fourier Space.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 16:59:30 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Schwartz", "Jonathan", ""], ["Jiang", "Yi", ""], ["Wang", "Yongjie", ""], ["Aiello", "Anthony", ""], ["Bhattacharya", "Pallab", ""], ["Yuan", "Hui", ""], ["Mi", "Zetian", ""], ["Bassim", "Nabil", ""], ["Hovden", "Robert", ""]]}, {"id": "1901.08043", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Jiacheng Zhuo, Philipp Kr\\\"ahenb\\\"uhl", "title": "Bottom-up Object Detection by Grouping Extreme and Center Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of deep learning, object detection drifted from a bottom-up\nto a top-down recognition problem. State of the art algorithms enumerate a\nnear-exhaustive list of object locations and classify each into: object or not.\nIn this paper, we show that bottom-up approaches still perform competitively.\nWe detect four extreme points (top-most, left-most, bottom-most, right-most)\nand one center point of objects using a standard keypoint estimation network.\nWe group the five keypoints into a bounding box if they are geometrically\naligned. Object detection is then a purely appearance-based keypoint estimation\nproblem, without region classification or implicit feature learning. The\nproposed method performs on-par with the state-of-the-art region based\ndetection methods, with a bounding box AP of 43.2% on COCO test-dev. In\naddition, our estimated extreme points directly span a coarse octagonal mask,\nwith a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding\nboxes. Extreme point guided segmentation further improves this to 34.6% Mask\nAP.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 18:50:09 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 05:47:59 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 17:02:24 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Zhou", "Xingyi", ""], ["Zhuo", "Jiacheng", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1901.08097", "submitter": "Matteo Fabbri Ing.", "authors": "Federico Fulgeri, Matteo Fabbri, Stefano Alletto, Simone Calderara,\n  Rita Cucchiara", "title": "Can Adversarial Networks Hallucinate Occluded People With a Plausible\n  Aspect?", "comments": "Under review at CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When you see a person in a crowd, occluded by other persons, you miss visual\ninformation that can be used to recognize, re-identify or simply classify him\nor her. You can imagine its appearance given your experience, nothing more.\nSimilarly, AI solutions can try to hallucinate missing information with\nspecific deep learning architectures, suitably trained with people with and\nwithout occlusions. The goal of this work is to generate a complete image of a\nperson, given an occluded version in input, that should be a) without occlusion\nb) similar at pixel level to a completely visible people shape c) capable to\nconserve similar visual attributes (e.g. male/female) of the original one. For\nthe purpose, we propose a new approach by integrating the state-of-the-art of\nneural network architectures, namely U-nets and GANs, as well as discriminative\nattribute classification nets, with an architecture specifically designed to\nde-occlude people shapes. The network is trained to optimize a Loss function\nwhich could take into account the aforementioned objectives. As well we propose\ntwo datasets for testing our solution: the first one, occluded RAP, created\nautomatically by occluding real shapes of the RAP dataset (which collects also\nattributes of the people aspect); the second is a large synthetic dataset, AiC,\ngenerated in computer graphics with data extracted from the GTA video game,\nthat contains 3D data of occluded objects by construction. Results are\nimpressive and outperform any other previous proposal. This result could be an\ninitial step to many further researches to recognize people and their behavior\nin an open crowded world.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:43:58 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Fulgeri", "Federico", ""], ["Fabbri", "Matteo", ""], ["Alletto", "Stefano", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1901.08101", "submitter": "Matteo Fabbri Ing.", "authors": "Matteo Fabbri, Guido Borghi, Fabio Lanzi, Roberto Vezzani, Simone\n  Calderara, Rita Cucchiara", "title": "Domain Translation with Conditional GANs: from Depth to RGB Face-to-Face", "comments": "Accepted at ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can faces acquired by low-cost depth sensors be useful to catch some\ncharacteristic details of the face? Typically the answer is no. However, new\ndeep architectures can generate RGB images from data acquired in a different\nmodality, such as depth data. In this paper, we propose a new\n\\textit{Deterministic Conditional GAN}, trained on annotated RGB-D face\ndatasets, effective for a face-to-face translation from depth to RGB. Although\nthe network cannot reconstruct the exact somatic features for unknown\nindividual faces, it is capable to reconstruct plausible faces; their\nappearance is accurate enough to be used in many pattern recognition tasks. In\nfact, we test the network capability to hallucinate with some\n\\textit{Perceptual Probes}, as for instance face aspect classification or\nlandmark detection. Depth face can be used in spite of the correspondent RGB\nimages, that often are not available due to difficult luminance conditions.\nExperimental results are very promising and are as far as better than\npreviously proposed approaches: this domain translation can constitute a new\nway to exploit depth data in new future applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:49:23 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Fabbri", "Matteo", ""], ["Borghi", "Guido", ""], ["Lanzi", "Fabio", ""], ["Vezzani", "Roberto", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1901.08109", "submitter": "Alvaro Gomariz", "authors": "Alvaro Gomariz, Weiye Li, Ece Ozkan, Christine Tanner, Orcun Goksel", "title": "Siamese Networks with Location Prior for Landmark Tracking in Liver\n  Ultrasound Sequences", "comments": "Accepted at the IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-guided radiation therapy can benefit from accurate motion tracking by\nultrasound imaging, in order to minimize treatment margins and radiate moving\nanatomical targets, e.g., due to breathing. One way to formulate this tracking\nproblem is the automatic localization of given tracked anatomical landmarks\nthroughout a temporal ultrasound sequence. For this, we herein propose a\nfully-convolutional Siamese network that learns the similarity between pairs of\nimage regions containing the same landmark. Accordingly, it learns to localize\nand thus track arbitrary image features, not only predefined anatomical\nstructures. We employ a temporal consistency model as a location prior, which\nwe combine with the network-predicted location probability map to track a\ntarget iteratively in ultrasound sequences. We applied this method on the\ndataset of the Challenge on Liver Ultrasound Tracking (CLUST) with competitive\nresults, where our work is the first to effectively apply CNNs on this tracking\nproblem, thanks to our temporal regularization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:59:36 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Gomariz", "Alvaro", ""], ["Li", "Weiye", ""], ["Ozkan", "Ece", ""], ["Tanner", "Christine", ""], ["Goksel", "Orcun", ""]]}, {"id": "1901.08150", "submitter": "Song Bai", "authors": "Song Bai, Feihu Zhang, Philip H.S. Torr", "title": "Hypergraph Convolution and Hypergraph Attention", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks have attracted great attention and achieved\nprominent performance in various research fields. Most of those algorithms have\nassumed pairwise relationships of objects of interest. However, in many real\napplications, the relationships between objects are in higher-order, beyond a\npairwise formulation. To efficiently learn deep embeddings on the high-order\ngraph-structured data, we introduce two end-to-end trainable operators to the\nfamily of graph neural networks, i.e., hypergraph convolution and hypergraph\nattention. Whilst hypergraph convolution defines the basic formulation of\nperforming convolution on a hypergraph, hypergraph attention further enhances\nthe capacity of representation learning by leveraging an attention module. With\nthe two operators, a graph neural network is readily extended to a more\nflexible model and applied to diverse applications where non-pairwise\nrelationships are observed. Extensive experimental results with semi-supervised\nnode classification demonstrate the effectiveness of hypergraph convolution and\nhypergraph attention.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 22:09:21 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 14:44:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bai", "Song", ""], ["Zhang", "Feihu", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1901.08190", "submitter": "John Edgar Vargas Mu\\~noz", "authors": "John E. Vargas-Mu\\~noz, Sylvain Lobry, Alexandre X. Falc\\~ao, Devis\n  Tuia", "title": "Correcting rural building annotations in OpenStreetMap using\n  convolutional neural networks", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 147, pages 283\n  - 293, 2019", "doi": "10.1016/j.isprsjprs.2018.11.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rural building mapping is paramount to support demographic studies and plan\nactions in response to crisis that affect those areas. Rural building\nannotations exist in OpenStreetMap (OSM), but their quality and quantity are\nnot sufficient for training models that can create accurate rural building\nmaps. The problems with these annotations essentially fall into three\ncategories: (i) most commonly, many annotations are geometrically misaligned\nwith the updated imagery; (ii) some annotations do not correspond to buildings\nin the images (they are misannotations or the buildings have been destroyed);\nand (iii) some annotations are missing for buildings in the images (the\nbuildings were never annotated or were built between subsequent image\nacquisitions). First, we propose a method based on Markov Random Field (MRF) to\nalign the buildings with their annotations. The method maximizes the\ncorrelation between annotations and a building probability map while enforcing\nthat nearby buildings have similar alignment vectors. Second, the annotations\nwith no evidence in the building probability map are removed. Third, we present\na method to detect non-annotated buildings with predefined shapes and add their\nannotation. The proposed methodology shows considerable improvement in accuracy\nof the OSM annotations for two regions of Tanzania and Zimbabwe, being more\naccurate than state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 01:44:12 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Vargas-Mu\u00f1oz", "John E.", ""], ["Lobry", "Sylvain", ""], ["Falc\u00e3o", "Alexandre X.", ""], ["Tuia", "Devis", ""]]}, {"id": "1901.08204", "submitter": "Peng Wei", "authors": "Weibo Huang, Peng Wei", "title": "A PCB Dataset for Defects Detection and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To coupe with the difficulties in the process of inspection and\nclassification of defects in Printed Circuit Board (PCB), other researchers\nhave proposed many methods. However, few of them published their dataset\nbefore, which hindered the introduction and comparison of new methods. In this\npaper, we published a synthesized PCB dataset containing 1386 images with 6\nkinds of defects for the use of detection, classification and registration\ntasks. Besides, we proposed a reference based method to inspect and trained an\nend-to-end convolutional neural network to classify the defects. Unlike\nconventional approaches that require pixel-by-pixel processing, our method\nfirstly locate the defects and then classify them by neural networks, which\nshows superior performance on our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 02:52:52 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Huang", "Weibo", ""], ["Wei", "Peng", ""]]}, {"id": "1901.08211", "submitter": "Qi Dou", "authors": "Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng", "title": "Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain\n  Adaptation for Medical Image Segmentation", "comments": "AAAI 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unsupervised domain adaptation framework, called\nSynergistic Image and Feature Adaptation (SIFA), to effectively tackle the\nproblem of domain shift. Domain adaptation has become an important and hot\ntopic in recent studies on deep learning, aiming to recover performance\ndegradation when applying the neural networks to new testing domains. Our\nproposed SIFA is an elegant learning diagram which presents synergistic fusion\nof adaptations from both image and feature perspectives. In particular, we\nsimultaneously transform the appearance of images across domains and enhance\ndomain-invariance of the extracted features towards the segmentation task. The\nfeature encoder layers are shared by both perspectives to grasp their mutual\nbenefits during the end-to-end learning procedure. Without using any annotation\nfrom the target domain, the learning of our unified model is guided by\nadversarial losses, with multiple discriminators employed from various aspects.\nWe have extensively validated our method with a challenging application of\ncross-modality medical image segmentation of cardiac structures. Experimental\nresults demonstrate that our SIFA model recovers the degraded performance from\n17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:16:09 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:56:04 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 08:08:49 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 11:52:33 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Chen", "Cheng", ""], ["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1901.08212", "submitter": "Sudhir Bagul Mr", "authors": "Manan Oza, Himanshu Vaghela, Sudhir Bagul", "title": "Semi-Supervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-to-image translation is a long-established and a difficult problem in\ncomputer vision. In this paper we propose an adversarial based model for\nimage-to-image translation. The regular deep neural-network based methods\nperform the task of image-to-image translation by comparing gram matrices and\nusing image segmentation which requires human intervention. Our generative\nadversarial network based model works on a conditional probability approach.\nThis approach makes the image translation independent of any local, global and\ncontent or style features. In our approach we use a bidirectional\nreconstruction model appended with the affine transform factor that helps in\nconserving the content and photorealism as compared to other models. The\nadvantage of using such an approach is that the image-to-image translation is\nsemi-supervised, independant of image segmentation and inherits the properties\nof generative adversarial networks tending to produce realistic. This method\nhas proven to produce better results than Multimodal Unsupervised\nImage-to-image translation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:26:00 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Oza", "Manan", ""], ["Vaghela", "Himanshu", ""], ["Bagul", "Sudhir", ""]]}, {"id": "1901.08213", "submitter": "Sudhir Bagul Mr", "authors": "Himanshu Vaghela, Manan Oza, Sudhir Bagul", "title": "MREAK : Morphological Retina Keypoint Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A variety of computer vision applications depend on the efficiency of image\nmatching algorithms used. Various descriptors are designed to detect and match\nfeatures in images. Deployment of this algorithms in mobile applications\ncreates a need for low computation time. Binary descriptors requires less\ncomputation time than float-point based descriptors because of the intensity\ncomparison between pairs of sample points and comparing after creating a binary\nstring. In order to decrease time complexity, quality of keypoints matched is\noften compromised. We propose a keypoint descriptor named Morphological Retina\nKeypoint Descriptor (MREAK) inspired by the function of human pupil which\ndilates and constricts responding to the amount of light. By using\nmorphological operators of opening and closing and modifying the retinal\nsampling pattern accordingly, an increase in the number of accurately matched\nkeypoints is observed. Our results show that matched keypoints are more\nefficient than FREAK descriptor and requires low computation time than various\ndescriptors like SIFT, BRISK and SURF.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:26:07 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Vaghela", "Himanshu", ""], ["Oza", "Manan", ""], ["Bagul", "Sudhir", ""]]}, {"id": "1901.08225", "submitter": "Seung-Hwan Bae", "authors": "Seung-Hwan Bae", "title": "Object Detection based on Region Decomposition and Assembly", "comments": "Accepted to 2019 AAAI Conference on Artificial Intelligence (AAAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based object detection infers object regions for one or more\ncategories in an image. Due to the recent advances in deep learning and region\nproposal methods, object detectors based on convolutional neural networks\n(CNNs) have been flourishing and provided the promising detection results.\nHowever, the detection accuracy is degraded often because of the low\ndiscriminability of object CNN features caused by occlusions and inaccurate\nregion proposals. In this paper, we therefore propose a region decomposition\nand assembly detector (R-DAD) for more accurate object detection.\n  In the proposed R-DAD, we first decompose an object region into multiple\nsmall regions. To capture an entire appearance and part details of the object\njointly, we extract CNN features within the whole object region and decomposed\nregions. We then learn the semantic relations between the object and its parts\nby combining the multi-region features stage by stage with region assembly\nblocks, and use the combined and high-level semantic features for the object\nclassification and localization. In addition, for more accurate region\nproposals, we propose a multi-scale proposal layer that can generate object\nproposals of various scales. We integrate the R-DAD into several feature\nextractors, and prove the distinct performance improvement on PASCAL07/12 and\nMSCOCO18 compared to the recent convolutional detectors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 04:09:10 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 04:30:17 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bae", "Seung-Hwan", ""]]}, {"id": "1901.08227", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Ke Li, Jianbo Shi, Jitendra Malik", "title": "Trajectory Normalized Gradients for Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers proposed various low-precision gradient compression,\nfor efficient communication in large-scale distributed optimization. Based on\nthese work, we try to reduce the communication complexity from a new direction.\nWe pursue an ideal bijective mapping between two spaces of gradient\ndistribution, so that the mapped gradient carries greater information entropy\nafter the compression. In our setting, all servers should share a reference\ngradient in advance, and they communicate via the normalized gradients, which\nare the subtraction or quotient, between current gradients and the reference.\nTo obtain a reference vector that yields a stronger signal-to-noise ratio,\ndynamically in each iteration, we extract and fuse information from the past\ntrajectory in hindsight, and search for an optimal reference for compression.\nWe name this to be the trajectory-based normalized gradients (TNG). It bridges\nthe research from different societies, like coding, optimization, systems, and\nlearning. It is easy to implement and can universally combine with existing\nalgorithms. Our experiments on benchmarking hard non-convex functions, convex\nproblems like logistic regression demonstrate that TNG is more\ncompression-efficient for communication of distributed optimization of general\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 04:24:31 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Li", "Ke", ""], ["Shi", "Jianbo", ""], ["Malik", "Jitendra", ""]]}, {"id": "1901.08236", "submitter": "Shilei Fu", "authors": "Shilei Fu, Feng Xu, Ya-Qiu Jin", "title": "Reciprocal Translation between SAR and Optical Remote Sensing Images\n  with Cascaded-Residual Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advantages of all-weather and all-day high-resolution imaging,\nsynthetic aperture radar (SAR) images are much less viewed and used by general\npeople because human vision is not adapted to microwave scattering phenomenon.\nHowever, expert interpreters can be trained by comparing side-by-side SAR and\noptical images to learn the mapping rules from SAR to optical. This paper\nattempts to develop machine intelligence that are trainable with large-volume\nco-registered SAR and optical images to translate SAR image to optical version\nfor assisted SAR image interpretation. Reciprocal SAR-Optical image translation\nis a challenging task because it is raw data translation between two physically\nvery different sensing modalities. This paper proposes a novel reciprocal\nadversarial network scheme where cascaded residual connections and hybrid\nL1-GAN loss are employed. It is trained and tested on both spaceborne GF-3 and\nairborne UAVSAR images. Results are presented for datasets of different\nresolutions and polarizations and compared with other state-of-the-art methods.\nThe FID is used to quantitatively evaluate the translation performance. The\npossibility of unsupervised learning with unpaired SAR and optical images is\nalso explored. Results show that the proposed translation network works well\nunder many scenarios and it could potentially be used for assisted SAR\ninterpretation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 05:17:39 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 07:11:16 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Fu", "Shilei", ""], ["Xu", "Feng", ""], ["Jin", "Ya-Qiu", ""]]}, {"id": "1901.08239", "submitter": "Zhimin Chen", "authors": "Zhimin Chen, Darius Parvin, Maedbh King, Susan Hao", "title": "Visualizing Topographic Independent Component Analysis with Movies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Independent component analysis (ICA) has often been used as a tool to model\nnatural image statistics by separating multivariate signals in the image into\ncomponents that are assumed to be independent. However, these estimated\ncomponents oftentimes have higher order dependencies, such as co-activation of\ncomponents, that are not accounted for in the model. Topographic independent\ncomponent analysis(TICA), a modification of ICA, takes into account higher\norder dependencies and orders components topographically as a function of\ndependence. Here, we aim to visualize the time course of TICA basis activations\nto movie stimuli. We find that the activity of TICA bases are often clustered\nand move continuously, potentially resembling activity of topographically\norganized cells in the visual cortex.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 05:47:01 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Chen", "Zhimin", ""], ["Parvin", "Darius", ""], ["King", "Maedbh", ""], ["Hao", "Susan", ""]]}, {"id": "1901.08242", "submitter": "Taewon Kang", "authors": "Taewon Kang, Kwang Hee Lee", "title": "Unsupervised Image-to-Image Translation with Self-Attention Networks", "comments": "Accepted by 2020 IEEE International Conference on Big Data and Smart\n  Computing (IEEE BigComp 2020). This paper has been accepted as a REGULAR\n  paper presented at IEEE International Conference on BigComp 2020. 7 pages, 11\n  figures; v4: corrected typos, figures", "journal-ref": "2020 IEEE International Conference on Big Data and Smart Computing\n  (BigComp), (2020), 102-108", "doi": "10.1109/BigComp48618.2020.00-92", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image translation aims to learn the transformation from a source\ndomain to another target domain given unpaired training data. Several\nstate-of-the-art works have yielded impressive results in the GANs-based\nunsupervised image-to-image translation. It fails to capture strong geometric\nor structural changes between domains, or it produces unsatisfactory result for\ncomplex scenes, compared to local texture mapping tasks such as style transfer.\nRecently, SAGAN (Han Zhang, 2018) showed that the self-attention network\nproduces better results than the convolution-based GAN. However, the\neffectiveness of the self-attention network in unsupervised image-to-image\ntranslation tasks have not been verified. In this paper, we propose an\nunsupervised image-to-image translation with self-attention networks, in which\nlong range dependency helps to not only capture strong geometric change but\nalso generate details using cues from all feature locations. In experiments, we\nqualitatively and quantitatively show superiority of the proposed method\ncompared to existing state-of-the-art unsupervised image-to-image translation\ntask. The source code and our results are online:\nhttps://github.com/itsss/img2img_sa and\nhttp://itsc.kr/2019/01/24/2019_img2img_sa\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 05:59:40 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 08:28:44 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 11:11:43 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2020 15:56:06 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kang", "Taewon", ""], ["Lee", "Kwang Hee", ""]]}, {"id": "1901.08274", "submitter": "Zaiqiang Wu", "authors": "Zaiqiang Wu, Wei Jiang, Hao Luo, Lin Cheng", "title": "A Novel Self-Intersection Penalty Term for Statistical Body Shape Models\n  and Its Applications in 3D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical body shape models are widely used in 3D pose estimation due to\ntheir low-dimensional parameters representation. However, it is difficult to\navoid self-intersection between body parts accurately. Motivated by this fact,\nwe proposed a novel self-intersection penalty term for statistical body shape\nmodels applied in 3D pose estimation. To avoid the trouble of computing\nself-intersection for complex surfaces like the body meshes, the gradient of\nour proposed self-intersection penalty term is manually derived from the\nperspective of geometry. First, the self-intersection penalty term is defined\nas the volume of the self-intersection region. To calculate the partial\nderivatives with respect to the coordinates of the vertices, we employed\ndetection rays to divide vertices of statistical body shape models into\ndifferent groups depending on whether the vertex is in the region of\nself-intersection. Second, the partial derivatives could be easily derived by\nthe normal vectors of neighboring triangles of the vertices. Finally, this\npenalty term could be applied in gradient-based optimization algorithms to\nremove the self-intersection of triangular meshes without using any\napproximation. Qualitative and quantitative evaluations were conducted to\ndemonstrate the effectiveness and generality of our proposed method compared\nwith previous approaches. The experimental results show that our proposed\npenalty term can avoid self-intersection to exclude unreasonable predictions\nand improves the accuracy of 3D pose estimation indirectly. Further more, the\nproposed method could be employed universally in triangular mesh based 3D\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:19:37 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wu", "Zaiqiang", ""], ["Jiang", "Wei", ""], ["Luo", "Hao", ""], ["Cheng", "Lin", ""]]}, {"id": "1901.08278", "submitter": "Michael Mahoney", "authors": "Charles H. Martin and Michael W. Mahoney", "title": "Heavy-Tailed Universality Predicts Trends in Test Accuracies for Very\n  Large Pre-Trained Deep Neural Networks", "comments": "Updated as will appear in SDM20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two or more Deep Neural Networks (DNNs) with the same or similar\narchitectures, and trained on the same dataset, but trained with different\nsolvers, parameters, hyper-parameters, regularization, etc., can we predict\nwhich DNN will have the best test accuracy, and can we do so without peeking at\nthe test data? In this paper, we show how to use a new Theory of Heavy-Tailed\nSelf-Regularization (HT-SR) to answer this. HT-SR suggests, among other things,\nthat modern DNNs exhibit what we call Heavy-Tailed Mechanistic Universality\n(HT-MU), meaning that the correlations in the layer weight matrices can be fit\nto a power law (PL) with exponents that lie in common Universality classes from\nHeavy-Tailed Random Matrix Theory (HT-RMT). From this, we develop a Universal\ncapacity control metric that is a weighted average of PL exponents. Rather than\nconsidering small toy NNs, we examine over 50 different, large-scale\npre-trained DNNs, ranging over 15 different architectures, trained on\nImagetNet, each of which has been reported to have different test accuracies.\nWe show that this new capacity metric correlates very well with the reported\ntest accuracies of these DNNs, looking across each architecture\n(VGG16/.../VGG19, ResNet10/.../ResNet152, etc.). We also show how to\napproximate the metric by the more familiar Product Norm capacity measure, as\nthe average of the log Frobenius norm of the layer weight matrices. Our\napproach requires no changes to the underlying DNN or its loss function, it\ndoes not require us to train a model (although it could be used to monitor\ntraining), and it does not even require access to the ImageNet data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:27:03 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 05:24:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Martin", "Charles H.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1901.08292", "submitter": "Santhosh Kelathodi Kumaran", "authors": "Santhosh Kelathodi Kumaran, Debi Prosad Dogra and Partha Pratim Roy", "title": "Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey", "comments": null, "journal-ref": "ACM Computing Surveys (2020), 6(53):Article 119, 2020", "doi": "10.1145/3417989", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has evolved in the last decade as a key technology for\nnumerous applications replacing human supervision. In this paper, we present a\nsurvey on relevant visual surveillance related researches for anomaly detection\nin public places, focusing primarily on roads. Firstly, we revisit the surveys\ndone in the last 10 years in this field. Since the underlying building block of\na typical anomaly detection is learning, we emphasize more on learning methods\napplied on video scenes. We then summarize the important contributions made\nduring last six years on anomaly detection primarily focusing on features,\nunderlying techniques, applied scenarios and types of anomalies using single\nstatic camera. Finally, we discuss the challenges in the computer vision\nrelated anomaly detection techniques and some of the important future\npossibilities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 09:02:05 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kumaran", "Santhosh Kelathodi", ""], ["Dogra", "Debi Prosad", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1901.08296", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky", "title": "Deep Learning on Attributed Graphs: A Journey from Graphs to Their\n  Embeddings and Back", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is a powerful concept for representation of relations between pairs\nof entities. Data with underlying graph structure can be found across many\ndisciplines and there is a natural desire for understanding such data better.\nDeep learning (DL) has achieved significant breakthroughs in a variety of\nmachine learning tasks in recent years, especially where data is structured on\na grid, such as in text, speech, or image understanding. However, surprisingly\nlittle has been done to explore the applicability of DL on arbitrary\ngraph-structured data directly.\n  The goal of this thesis is to investigate architectures for DL on graphs and\nstudy how to transfer, adapt or generalize concepts that work well on\nsequential and image data to this domain. We concentrate on two important\nprimitives: embedding graphs or their nodes into a continuous vector space\nrepresentation (encoding) and, conversely, generating graphs from such vectors\nback (decoding). To that end, we make the following contributions.\n  First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like\noperation on graphs performed in the spatial domain where filters are\ndynamically generated based on edge attributes. The method is used to encode\ngraphs with arbitrary and varying structure.\n  Second, we propose SuperPoint Graph, an intermediate point cloud\nrepresentation with rich edge attributes encoding the contextual relationship\nbetween object parts. Based on this representation, ECC is employed to segment\nlarge-scale point clouds without major sacrifice in fine details.\n  Third, we present GraphVAE, a graph generator allowing us to decode graphs\nwith variable but upper-bounded number of nodes making use of approximate graph\nmatching for aligning the predictions of an autoencoder with its inputs. The\nmethod is applied to the task of molecule generation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 09:12:33 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Simonovsky", "Martin", ""]]}, {"id": "1901.08317", "submitter": "Leslie Solorzano", "authors": "Leslie Solorzano, Gabriela M. Almeida, B\\'arbara Mesquita, Diana\n  Martins, Carla Oliveira, Carolina W\\\"ahlby", "title": "Whole slide image registration for the study of tumor heterogeneity", "comments": "MICCAI2018 - Computational Pathology and Ophthalmic Medical Image\n  Analysis - COMPAY", "journal-ref": "vol 11039, 2018, p95-102", "doi": "10.1007/978-3-030-00949-6_12", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consecutive thin sections of tissue samples make it possible to study local\nvariation in e.g. protein expression and tumor heterogeneity by staining for a\nnew protein in each section. In order to compare and correlate patterns of\ndifferent proteins, the images have to be registered with high accuracy. The\nproblem we want to solve is registration of gigapixel whole slide images (WSI).\nThis presents 3 challenges: (i) Images are very large; (ii) Thin sections\nresult in artifacts that make global affine registration prone to very large\nlocal errors; (iii) Local affine registration is required to preserve correct\ntissue morphology (local size, shape and texture). In our approach we compare\nWSI registration based on automatic and manual feature selection on either the\nfull image or natural sub-regions (as opposed to square tiles). Working with\nnatural sub-regions, in an interactive tool makes it possible to exclude\nregions containing scientifically irrelevant information. We also present a new\nway to visualize local registration quality by a Registration Confidence Map\n(RCM). With this method, intra-tumor heterogeneity and charateristics of the\ntumor microenvironment can be observed and quantified.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:02:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Solorzano", "Leslie", ""], ["Almeida", "Gabriela M.", ""], ["Mesquita", "B\u00e1rbara", ""], ["Martins", "Diana", ""], ["Oliveira", "Carla", ""], ["W\u00e4hlby", "Carolina", ""]]}, {"id": "1901.08339", "submitter": "Zakaria Laskar", "authors": "Zakaria Laskar, Juho Kannala", "title": "Semi-Supervised Semantic Matching", "comments": "Accepted to ECCVW (GMDL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied to solve\nthe problem of correspondence estimation between semantically related images.\nDue to non-availability of large training datasets, existing methods resort to\nself-supervised or unsupervised training paradigm. In this paper we propose a\nsemi-supervised learning framework that imposes cyclic consistency constraint\non unlabeled image pairs. Together with the supervised loss the proposed model\nachieves state-of-the-art on a benchmark semantic matching dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:46:41 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Laskar", "Zakaria", ""], ["Kannala", "Juho", ""]]}, {"id": "1901.08341", "submitter": "Zakaria Laskar", "authors": "Zakaria Laskar, Hamed R. Tavakoli, Juho Kannala", "title": "Semantic Matching by Weakly Supervised 2D Point Set Registration", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of establishing correspondences between\ndifferent instances of the same object. The problem is posed as finding the\ngeometric transformation that aligns a given image pair. We use a convolutional\nneural network (CNN) to directly regress the parameters of the transformation\nmodel. The alignment problem is defined in the setting where an unordered set\nof semantic key-points per image are available, but, without the correspondence\ninformation. To this end we propose a novel loss function based on cyclic\nconsistency that solves this 2D point set registration problem by inferring the\noptimal geometric transformation model parameters. We train and test our\napproach on a standard benchmark dataset Proposal-Flow\n(PF-PASCAL)\\cite{proposal_flow}. The proposed approach achieves\nstate-of-the-art results demonstrating the effectiveness of the method. In\naddition, we show our approach further benefits from additional training\nsamples in PF-PASCAL generated by using category level information.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:48:39 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Laskar", "Zakaria", ""], ["Tavakoli", "Hamed R.", ""], ["Kannala", "Juho", ""]]}, {"id": "1901.08362", "submitter": "Zun Li", "authors": "Zun Li, Congyan Lang, Yunpeng Chen, Junhao Liew, Jiashi Feng", "title": "Deep Reasoning with Multi-Scale Context for Salient Object Detection", "comments": "10 pages, 8 figures, 3 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect salient objects accurately, existing methods usually design complex\nbackbone network architectures to learn and fuse powerful features. However,\nthe saliency inference module that performs saliency prediction from the fused\nfeatures receives much less attention on its architecture design and typically\nadopts only a few fully convolutional layers. In this paper, we find the\nlimited capacity of the saliency inference module indeed makes a fundamental\nperformance bottleneck, and enhancing its capacity is critical for obtaining\nbetter saliency prediction. Correspondingly, we propose a deep yet light-weight\nsaliency inference module that adopts a multi-dilated depth-wise convolution\narchitecture. Such a deep inference module, though with simple architecture,\ncan directly perform reasoning about salient objects from the multi-scale\nconvolutional features fast, and give superior salient object detection\nperformance with less computational cost. To our best knowledge, we are the\nfirst to reveal the importance of the inference module for salient object\ndetection, and present a novel architecture design with attractive efficiency\nand accuracy. Extensive experimental evaluations demonstrate that our simple\nframework performs favorably compared with the state-of-the-art methods with\ncomplex backbone design.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 11:34:56 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:37:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Li", "Zun", ""], ["Lang", "Congyan", ""], ["Chen", "Yunpeng", ""], ["Liew", "Junhao", ""], ["Feng", "Jiashi", ""]]}, {"id": "1901.08373", "submitter": "Xuesong Li", "authors": "Xuesong Li, Jose Guivant, Ngaiming Kwok, Yongzhi Xu, Ruowei Li,\n  Hongkun Wu", "title": "Three-dimensional Backbone Network for 3D Object Detection in Traffic\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of detecting 3D objects in traffic scenes has a pivotal role in many\nreal-world applications. However, the performance of 3D object detection is\nlower than that of 2D object detection due to the lack of powerful 3D feature\nextraction methods. To address this issue, this study proposes a 3D backbone\nnetwork to acquire comprehensive 3D feature maps for 3D object detection. It\nprimarily consists of sparse 3D convolutional neural network operations in the\npoint cloud. The 3D backbone network can inherently learn 3D features from the\nraw data without compressing the point cloud into multiple 2D images. The\nsparse 3D convolutional neural network takes full advantage of the sparsity in\nthe 3D point cloud to accelerate computation and save memory, which makes the\n3D backbone network feasible in a real-world application. Empirical experiments\nwere conducted on the KITTI benchmark and comparable results were obtained with\nrespect to the state-of-the-art performance for 3D object detection.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 12:11:05 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 08:12:30 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Li", "Xuesong", ""], ["Guivant", "Jose", ""], ["Kwok", "Ngaiming", ""], ["Xu", "Yongzhi", ""], ["Li", "Ruowei", ""], ["Wu", "Hongkun", ""]]}, {"id": "1901.08379", "submitter": "David Romo-Bucheli PhD", "authors": "Philipp Seeb\\\"ock, David Romo-Bucheli, Sebastian Waldstein, Hrvoje\n  Bogunovi\\'c, Jos\\'e Ignacio Orlando, Bianca S. Gerendas, Georg Langs, Ursula\n  Schmidt-Erfurth", "title": "Using CycleGANs for effectively reducing image variability across OCT\n  devices and improving retinal fluid segmentation", "comments": "* Contributed equally (order was defined by flipping a coin)\n  --------------- Accepted for publication in the \"IEEE International Symposium\n  on Biomedical Imaging (ISBI) 2019\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) has become the most important imaging\nmodality in ophthalmology. A substantial amount of research has recently been\ndevoted to the development of machine learning (ML) models for the\nidentification and quantification of pathological features in OCT images. Among\nthe several sources of variability the ML models have to deal with, a major\nfactor is the acquisition device, which can limit the ML model's\ngeneralizability. In this paper, we propose to reduce the image variability\nacross different OCT devices (Spectralis and Cirrus) by using CycleGAN, an\nunsupervised unpaired image transformation algorithm. The usefulness of this\napproach is evaluated in the setting of retinal fluid segmentation, namely\nintraretinal cystoid fluid (IRC) and subretinal fluid (SRF). First, we train a\nsegmentation model on images acquired with a source OCT device. Then we\nevaluate the model on (1) source, (2) target and (3) transformed versions of\nthe target OCT images. The presented transformation strategy shows an F1 score\nof 0.4 (0.51) for IRC (SRF) segmentations. Compared with traditional\ntransformation approaches, this means an F1 score gain of 0.2 (0.12).\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 12:37:14 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 09:12:41 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Seeb\u00f6ck", "Philipp", ""], ["Romo-Bucheli", "David", ""], ["Waldstein", "Sebastian", ""], ["Bogunovi\u0107", "Hrvoje", ""], ["Orlando", "Jos\u00e9 Ignacio", ""], ["Gerendas", "Bianca S.", ""], ["Langs", "Georg", ""], ["Schmidt-Erfurth", "Ursula", ""]]}, {"id": "1901.08394", "submitter": "Robin Chan", "authors": "Robin Chan, Matthias Rottmann, Fabian H\\\"uger, Peter Schlicht, Hanno\n  Gottschalk", "title": "Application of Decision Rules for Handling Class Imbalance in Semantic\n  Segmentation", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of autonomous car driving systems, semantic segmentation is an\nessential component to obtain a full understanding of the car's environment.\nOne difficulty, that occurs while training neural networks for this purpose, is\nclass imbalance of training data. Consequently, a neural network trained on\nunbalanced data in combination with maximum a-posteriori classification may\neasily ignore classes that are rare in terms of their frequency in the dataset.\nHowever, these classes are often of highest interest. We approach such\npotential misclassifications by weighting the posterior class probabilities\nwith the prior class probabilities which in our case are the inverse\nfrequencies of the corresponding classes in the training dataset. More\nprecisely, we adopt a localized method by computing the priors pixel-wise such\nthat the impact can be analyzed at pixel level as well. In our experiments, we\ntrain one network from scratch using a proprietary dataset containing 20,000\nannotated frames of video sequences recorded from street scenes. The evaluation\non our test set shows an increase of average recall with regard to instances of\npedestrians and info signs by $25\\%$ and $23.4\\%$, respectively. In addition,\nwe significantly reduce the non-detection rate for instances of the same\nclasses by $61\\%$ and $38\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 13:20:25 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Chan", "Robin", ""], ["Rottmann", "Matthias", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1901.08396", "submitter": "Bjarne Sievers", "authors": "Jonathan Sauder and Bjarne Sievers", "title": "Self-Supervised Deep Learning on Point Clouds by Reconstructing Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds provide a flexible and natural representation usable in\ncountless applications such as robotics or self-driving cars. Recently, deep\nneural networks operating on raw point cloud data have shown promising results\non supervised learning tasks such as object classification and semantic\nsegmentation. While massive point cloud datasets can be captured using modern\nscanning technology, manually labelling such large 3D point clouds for\nsupervised learning tasks is a cumbersome process. This necessitates methods\nthat can learn from unlabelled data to significantly reduce the number of\nannotated samples needed in supervised learning. We propose a self-supervised\nlearning task for deep learning on raw point cloud data in which a neural\nnetwork is trained to reconstruct point clouds whose parts have been randomly\nrearranged. While solving this task, representations that capture semantic\nproperties of the point cloud are learned. Our method is agnostic of network\narchitecture and outperforms current unsupervised learning approaches in\ndownstream object classification tasks. We show experimentally, that\npre-training with our method before supervised training improves the\nperformance of state-of-the-art models and significantly improves sample\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 13:30:19 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 20:06:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Sauder", "Jonathan", ""], ["Sievers", "Bjarne", ""]]}, {"id": "1901.08419", "submitter": "Michal Mackiewicz", "authors": "Michal Mackiewicz, Hans Jakob Rivertz, Graham D. Finlayson", "title": "Spherical sampling methods for the calculation of metamer mismatch\n  volumes", "comments": "One print or electronic copy may be made for personal use only.\n  Systematic reproduction and distribution, duplication of any material in this\n  paper for a fee or for commercial purposes, or modifications of this paper\n  are prohibited. Optical Society of America", "journal-ref": "Vol. 36, No. 1 / Jan 2019 / Journal of the Optical Society of\n  America A", "doi": "10.1364/JOSAA.36.000096", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two methods of calculating theoretically maximal\nmetamer mismatch volumes. Unlike prior art techniques, our methods do not make\nany assumptions on the shape of spectra on the boundary of the mismatch\nvolumes. Both methods utilize a spherical sampling approach, but they calculate\nmismatch volumes in two different ways. The first method uses a linear\nprogramming optimization, while the second is a computational geometry approach\nbased on half-space intersection. We show that under certain conditions the\ntheoretically maximal metamer mismatch volume is significantly larger than the\none approximated using a prior art method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 18:33:05 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Mackiewicz", "Michal", ""], ["Rivertz", "Hans Jakob", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "1901.08449", "submitter": "Frank Zijlstra", "authors": "Frank Zijlstra, Koen Willemsen, Mateusz C. Florkow, Ralph J.B.\n  Sakkers, Harrie H. Weinans, Bart C.H. van der Wal, Marijn van Stralen, Peter\n  R. Seevinck", "title": "CT synthesis from MR images for orthopedic applications in the lower arm\n  using a conditional generative adversarial network", "comments": "This work has been accepted at the SPIE Medical Imaging 2019, Image\n  Processing conference, paper 10949-54", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To assess the feasibility of deep learning-based high resolution\nsynthetic CT generation from MRI scans of the lower arm for orthopedic\napplications.\n  Methods: A conditional Generative Adversarial Network was trained to\nsynthesize CT images from multi-echo MR images. A training set of MRI and CT\nscans of 9 ex vivo lower arms was acquired and the CT images were registered to\nthe MRI images. Three-fold cross-validation was applied to generate independent\nresults for the entire dataset. The synthetic CT images were quantitatively\nevaluated with the mean absolute error metric, and Dice similarity and surface\nto surface distance on cortical bone segmentations.\n  Results: The mean absolute error was 63.5 HU on the overall tissue volume and\n144.2 HU on the cortical bone. The mean Dice similarity of the cortical bone\nsegmentations was 0.86. The average surface to surface distance between bone on\nreal and synthetic CT was 0.48 mm. Qualitatively, the synthetic CT images\ncorresponded well with the real CT scans and partially maintained high\nresolution structures in the trabecular bone. The bone segmentations on\nsynthetic CT images showed some false positives on tendons, but the general\nshape of the bone was accurately reconstructed.\n  Conclusions: This study demonstrates that high quality synthetic CT can be\ngenerated from MRI scans of the lower arm. The good correspondence of the bone\nsegmentations demonstrates that synthetic CT could be competitive with real CT\nin applications that depend on such segmentations, such as planning of\northopedic surgery and 3D printing.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 15:16:38 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Zijlstra", "Frank", ""], ["Willemsen", "Koen", ""], ["Florkow", "Mateusz C.", ""], ["Sakkers", "Ralph J. B.", ""], ["Weinans", "Harrie H.", ""], ["van der Wal", "Bart C. H.", ""], ["van Stralen", "Marijn", ""], ["Seevinck", "Peter R.", ""]]}, {"id": "1901.08486", "submitter": "HsuanKung Yang", "authors": "Hsuan-Kung Yang, Po-Han Chiang, Kuan-Wei Ho, Min-Fong Hong, and\n  Chun-Yi Lee", "title": "Never Forget: Balancing Exploration and Exploitation via Learning\n  Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration bonus derived from the novelty of the states in an environment\nhas become a popular approach to motivate exploration for deep reinforcement\nlearning agents in the past few years. Recent methods such as curiosity-driven\nexploration usually estimate the novelty of new observations by the prediction\nerrors of their system dynamics models. Due to the capacity limitation of the\nmodels and difficulty of performing next-frame prediction, however, these\nmethods typically fail to balance between exploration and exploitation in\nhigh-dimensional observation tasks, resulting in the agents forgetting the\nvisited paths and exploring those states repeatedly. Such inefficient\nexploration behavior causes significant performance drops, especially in large\nenvironments with sparse reward signals. In this paper, we propose to introduce\nthe concept of optical flow estimation from the field of computer vision to\ndeal with the above issue. We propose to employ optical flow estimation errors\nto examine the novelty of new observations, such that agents are able to\nmemorize and understand the visited states in a more comprehensive fashion. We\ncompare our method against the previous approaches in a number of experimental\nexperiments. Our results indicate that the proposed method appears to deliver\nsuperior and long-lasting performance than the previous methods. We further\nprovide a set of comprehensive ablative analysis of the proposed method, and\ninvestigate the impact of optical flow estimation on the learning curves of the\nDRL agents.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 16:26:16 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Yang", "Hsuan-Kung", ""], ["Chiang", "Po-Han", ""], ["Ho", "Kuan-Wei", ""], ["Hong", "Min-Fong", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1901.08534", "submitter": "Adria Ruiz", "authors": "Adria Ruiz, Oriol Martinez, Xavier Binefa, Jakob Verbeek", "title": "Learning Disentangled Representations with Reference-Based Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations from visual data, where different\nhigh-level generative factors are independently encoded, is of importance for\nmany computer vision tasks. Solving this problem, however, typically requires\nto explicitly label all the factors of interest in training images. To\nalleviate the annotation cost, we introduce a learning setting which we refer\nto as \"reference-based disentangling\". Given a pool of unlabeled images, the\ngoal is to learn a representation where a set of target factors are\ndisentangled from others. The only supervision comes from an auxiliary\n\"reference set\" containing images where the factors of interest are constant.\nIn order to address this problem, we propose reference-based variational\nautoencoders, a novel deep generative model designed to exploit the\nweak-supervision provided by the reference set. By addressing tasks such as\nfeature learning, conditional image generation or attribute transfer, we\nvalidate the ability of the proposed model to learn disentangled\nrepresentations from this minimal form of supervision.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 17:54:54 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Ruiz", "Adria", ""], ["Martinez", "Oriol", ""], ["Binefa", "Xavier", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1901.08616", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Yi-Ting Chen, Teruhisa Misu, Abhinav Shrivastava, Larry\n  Davis", "title": "Boosting Standard Classification Architectures Through a Ranking\n  Regularizer", "comments": "WACV 2020 Camera ready + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ triplet loss as a feature embedding regularizer to boost\nclassification performance. Standard architectures, like ResNet and Inception,\nare extended to support both losses with minimal hyper-parameter tuning. This\npromotes generality while fine-tuning pretrained networks. Triplet loss is a\npowerful surrogate for recently proposed embedding regularizers. Yet, it is\navoided due to large batch-size requirement and high computational cost.\nThrough our experiments, we re-assess these assumptions.\n  During inference, our network supports both classification and embedding\ntasks without any computational overhead. Quantitative evaluation highlights a\nsteady improvement on five fine-grained recognition datasets. Further\nevaluation on an imbalanced video dataset achieves significant improvement.\nTriplet loss brings feature embedding characteristics like nearest neighbor to\nclassification models. Code available at \\url{http://bit.ly/2LNYEqL}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 19:19:31 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 11:59:22 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:08:34 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Taha", "Ahmed", ""], ["Chen", "Yi-Ting", ""], ["Misu", "Teruhisa", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "1901.08658", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Sungmin Eum, and Heesung Kwon", "title": "Is Pretraining Necessary for Hyperspectral Image Classification?", "comments": "IGARSS 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address two questions for training a convolutional neural network (CNN)\nfor hyperspectral image classification: i) is it possible to build a\npre-trained network? and ii) is the pre-training effective in furthering the\nperformance? To answer the first question, we have devised an approach that\npre-trains a network on multiple source datasets that differ in their\nhyperspectral characteristics and fine-tunes on a target dataset. This approach\neffectively resolves the architectural issue that arises when transferring\nmeaningful information between the source and the target networks. To answer\nthe second question, we carried out several ablation experiments. Based on the\nexperimental results, a network trained from scratch performs as good as a\nnetwork fine-tuned from a pre-trained network. However, we observed that\npre-training the network has its own advantage in achieving better performances\nwhen deeper networks are required.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 22:01:48 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1901.08688", "submitter": "Poojan Oza", "authors": "Poojan Oza and Vishal M. Patel", "title": "One-Class Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2889273", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Convolutional Neural Network (CNN) based approach for one\nclass classification. The idea is to use a zero centered Gaussian noise in the\nlatent space as the pseudo-negative class and train the network using the\ncross-entropy loss to learn a good representation as well as the decision\nboundary for the given class. A key feature of the proposed approach is that\nany pre-trained CNN can be used as the base network for one class\nclassification. The proposed One Class CNN (OC-CNN) is evaluated on the\nUMDAA-02 Face, Abnormality-1001, FounderType-200 datasets. These datasets are\nrelated to a variety of one class application problems such as user\nauthentication, abnormality detection and novelty detection. Extensive\nexperiments demonstrate that the proposed method achieves significant\nimprovements over the recent state-of-the-art methods. The source code is\navailable at : github.com/otkupjnoz/oc-cnn.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 23:31:11 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1901.08707", "submitter": "Nima Tajbakhsh", "authors": "Nima Tajbakhsh, Yufei Hu, Junli Cao, Xingjian Yan, Yi Xiao, Yong Lu,\n  Jianming Liang, Demetri Terzopoulos, Xiaowei Ding", "title": "Surrogate Supervision for Medical Image Analysis: Effective Deep\n  Learning From Limited Quantities of Labeled Data", "comments": "Accepted in IEEE International Symposium on Biomedical Imaging (ISBI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effectiveness of a simple solution to the common problem\nof deep learning in medical image analysis with limited quantities of labeled\ntraining data. The underlying idea is to assign artificial labels to abundantly\navailable unlabeled medical images and, through a process known as surrogate\nsupervision, pre-train a deep neural network model for the target medical image\nanalysis task lacking sufficient labeled training data. In particular, we\nemploy 3 surrogate supervision schemes, namely rotation, reconstruction, and\ncolorization, in 4 different medical imaging applications representing\nclassification and segmentation for both 2D and 3D medical images. 3 key\nfindings emerge from our research: 1) pre-training with surrogate supervision\nis effective for small training sets; 2) deep models trained from initial\nweights pre-trained through surrogate supervision outperform the same models\nwhen trained from scratch, suggesting that pre-training with surrogate\nsupervision should be considered prior to training any deep 3D models; 3)\npre-training models in the medical domain with surrogate supervision is more\neffective than transfer learning from an unrelated domain (e.g., natural\nimages), indicating the practical value of abundant unlabeled medical image\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 01:19:47 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Tajbakhsh", "Nima", ""], ["Hu", "Yufei", ""], ["Cao", "Junli", ""], ["Yan", "Xingjian", ""], ["Xiao", "Yi", ""], ["Lu", "Yong", ""], ["Liang", "Jianming", ""], ["Terzopoulos", "Demetri", ""], ["Ding", "Xiaowei", ""]]}, {"id": "1901.08723", "submitter": "Lecheng Zheng", "authors": "Lecheng Zheng and Yu Cheng and Jingrui He", "title": "Deep Multimodality Model for Multi-task Multi-view Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems exhibit the coexistence of multiple types of\nheterogeneity, such as view heterogeneity (i.e., multi-view property) and task\nheterogeneity (i.e., multi-task property). For example, in an image\nclassification problem containing multiple poses of the same object, each pose\ncan be considered as one view, and the detection of each type of object can be\ntreated as one task. Furthermore, in some problems, the data type of multiple\nviews might be different. In a web classification problem, for instance, we\nmight be provided an image and text mixed data set, where the web pages are\ncharacterized by both images and texts. A common strategy to solve this kind of\nproblem is to leverage the consistency of views and the relatedness of tasks to\nbuild the prediction model. In the context of deep neural network, multi-task\nrelatedness is usually realized by grouping tasks at each layer, while\nmulti-view consistency is usually enforced by finding the maximal correlation\ncoefficient between views. However, there is no existing deep learning\nalgorithm that jointly models task and view dual heterogeneity, particularly\nfor a data set with multiple modalities (text and image mixed data set or text\nand video mixed data set, etc.). In this paper, we bridge this gap by proposing\na deep multi-task multi-view learning framework that learns a deep\nrepresentation for such dual-heterogeneity problems. Empirical studies on\nmultiple real-world data sets demonstrate the effectiveness of our proposed\nDeep-MTMV algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 03:26:21 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Zheng", "Lecheng", ""], ["Cheng", "Yu", ""], ["He", "Jingrui", ""]]}, {"id": "1901.08759", "submitter": "Priyank Palod", "authors": "Priyank Palod, Ayush Patwari, Sudhanshu Bahety, Saurabh Bagchi and\n  Pawan Goyal", "title": "Misleading Metadata Detection on YouTube", "comments": "Accepted at European Conference on Information Retrieval(ECIR) 2019.\n  7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  YouTube is the leading social media platform for sharing videos. As a result,\nit is plagued with misleading content that includes staged videos presented as\nreal footages from an incident, videos with misrepresented context and videos\nwhere audio/video content is morphed. We tackle the problem of detecting such\nmisleading videos as a supervised classification task. We develop UCNet - a\ndeep network to detect fake videos and perform our experiments on two datasets\n- VAVD created by us and publicly available FVC [8]. We achieve a macro\naveraged F-score of 0.82 while training and testing on a 70:30 split of FVC,\nwhile the baseline model scores 0.36. We find that the proposed model\ngeneralizes well when trained on one dataset and tested on the other.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 07:09:14 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Palod", "Priyank", ""], ["Patwari", "Ayush", ""], ["Bahety", "Sudhanshu", ""], ["Bagchi", "Saurabh", ""], ["Goyal", "Pawan", ""]]}, {"id": "1901.08787", "submitter": "Kwangjin Yoon", "authors": "Kwangjin Yoon, Young-min Song, Moongu Jeon", "title": "Multiple Hypothesis Tracking Algorithm for Multi-Target Multi-Camera\n  Tracking with Disjoint Views", "comments": "published in IET image processing, 2018", "journal-ref": "IET image processing, Volume: 12, Issue: 7, 7 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, a multiple hypothesis tracking (MHT) algorithm for\nmulti-target multi-camera tracking (MCT) with disjoint views is proposed. Our\nmethod forms track-hypothesis trees, and each branch of them represents a\nmulti-camera track of a target that may move within a camera as well as move\nacross cameras. Furthermore, multi-target tracking within a camera is performed\nsimultaneously with the tree formation by manipulating a status of each track\nhypothesis. Each status represents three different stages of a multi-camera\ntrack: tracking, searching, and end-of-track. The tracking status means targets\nare tracked by a single camera tracker. In the searching status, the\ndisappeared targets are examined if they reappear in other cameras. The\nend-of-track status does the target exited the camera network due to its\nlengthy invisibility. These three status assists MHT to form the\ntrack-hypothesis trees for multi-camera tracking. Furthermore, they present a\ngating technique for eliminating of unlikely observation-to-track association.\nIn the experiments, they evaluate the proposed method using two datasets,\nDukeMTMC and NLPR-MCT, which demonstrates that the proposed method outperforms\nthe state-of-the-art method in terms of improvement of the accuracy. In\naddition, they show that the proposed method can operate in real-time and\nonline.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 09:01:05 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Yoon", "Kwangjin", ""], ["Song", "Young-min", ""], ["Jeon", "Moongu", ""]]}, {"id": "1901.08811", "submitter": "Matteo Ferrara", "authors": "Matteo Ferrara, Annalisa Franco and Davide Maltoni", "title": "Face morphing detection in the presence of printing/scanning and\n  heterogeneous image sources", "comments": "This paper is a preprint of a paper accepted by IET Biometrics and is\n  subject to Institution of Engineering and Technology Copyright. When the\n  final version is published, the copy of record will be available at the IET\n  Digital Library", "journal-ref": null, "doi": "10.1049/bme2.12021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face morphing represents nowadays a big security threat in the context of\nelectronic identity documents as well as an interesting challenge for\nresearchers in the field of face recognition. Despite of the good performance\nobtained by state-of-the-art approaches on digital images, no satisfactory\nsolutions have been identified so far to deal with cross-database testing and\nprinted-scanned images (typically used in many countries for document issuing).\nIn this work, novel approaches are proposed to train Deep Neural Networks for\nmorphing detection: in particular generation of simulated printed-scanned\nimages together with other data augmentation strategies and pre-training on\nlarge face recognition datasets, allowed to reach state-of-the-art accuracy on\nchallenging datasets from heterogeneous image sources.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 10:10:47 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:54:34 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:55:34 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 07:58:00 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ferrara", "Matteo", ""], ["Franco", "Annalisa", ""], ["Maltoni", "Davide", ""]]}, {"id": "1901.08824", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana,\n  Keerthi Ram, Mohanasankar Sivaprakasam", "title": "Joint shape learning and segmentation for medical images using a\n  minimalistic deep network", "comments": "Under review at MIDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, state-of-the-art results have been achieved in semantic\nsegmentation using fully convolutional networks (FCNs). Most of these networks\nemploy encoder-decoder style architecture similar to U-Net and are trained with\nimages and the corresponding segmentation maps as a pixel-wise classification\ntask. Such frameworks only exploit class information by using the ground truth\nsegmentation maps. In this paper, we propose a multi-task learning framework\nwith the main aim of exploiting structural and spatial information along with\nthe class information. We modify the decoder part of the FCN to exploit class\ninformation and the structural information as well. We intend to do this while\nalso keeping the parameters of the network as low as possible. We obtain the\nstructural information using either of the two ways: i) using the contour map\nand ii) using the distance map, both of which can be obtained from ground truth\nsegmentation maps with no additional annotation costs. We also explore\ndifferent ways in which distance maps can be computed and study the effects of\ndifferent distance maps on the segmentation performance. We also experiment\nextensively on two different medical image segmentation applications: i.e i)\nusing color fundus images for optic disc and cup segmentation and ii) using\nendoscopic images for polyp segmentation. Through our experiments, we report\nresults comparable to, and in some cases performing better than the current\nstate-of-the-art architectures and with an order of 2x reduction in the number\nof parameters.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 10:53:57 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Murugesan", "Balamurali", ""], ["Sarveswaran", "Kaushik", ""], ["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "1901.08864", "submitter": "Sarthak Jeevaraj Shetty", "authors": "Sarthak J Shetty", "title": "Vision-based inspection system employing computer vision & neural\n  networks for detection of fractures in manufactured components", "comments": "Artificial Intelligence International Conference, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proceeding towards the age of automation and robotic integration of\nour production lines [5]. Effective quality-control systems have to be put in\nplace to maintain the quality of manufactured components. Among different\nquality-control systems, vision-based inspection systems have gained\nconsiderable amount of popularity [8] due to developments in computing power\nand image processing techniques. In this paper, we present a vision-based\ninspection system (VBI) as a quality-control system, which not only detects the\npresence of defects, such as in conventional VBIs, but also leverage\ndevelopments in machine learning to predict the presence of surface fractures\nand wearing. We use OpenCV, an open source computer-vision framework, and\nTensorflow, an open source machine-learning framework developed by Google Inc.,\nto accomplish the tasks of detection and prediction of presence of surface\ndefects such as fractures of manufactured gears.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 13:18:04 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Shetty", "Sarthak J", ""]]}, {"id": "1901.08906", "submitter": "Priyanka Mandikal", "authors": "Priyanka Mandikal and R. Venkatesh Babu", "title": "Dense 3D Point Cloud Reconstruction Using a Deep Pyramid Network", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a high-resolution 3D model of an object is a challenging task\nin computer vision. Designing scalable and light-weight architectures is\ncrucial while addressing this problem. Existing point-cloud based\nreconstruction approaches directly predict the entire point cloud in a single\nstage. Although this technique can handle low-resolution point clouds, it is\nnot a viable solution for generating dense, high-resolution outputs. In this\nwork, we introduce DensePCR, a deep pyramidal network for point cloud\nreconstruction that hierarchically predicts point clouds of increasing\nresolution. Towards this end, we propose an architecture that first predicts a\nlow-resolution point cloud, and then hierarchically increases the resolution by\naggregating local and global point features to deform a grid. Our method\ngenerates point clouds that are accurate, uniform and dense. Through extensive\nquantitative and qualitative evaluation on synthetic and real datasets, we\ndemonstrate that DensePCR outperforms the existing state-of-the-art point cloud\nreconstruction works, while also providing a light-weight and scalable\narchitecture for predicting high-resolution outputs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 15:07:44 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Mandikal", "Priyanka", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1901.08933", "submitter": "Shikun Liu", "authors": "Shikun Liu, Andrew J. Davison, Edward Johns", "title": "Self-Supervised Generalisation with Meta Auxiliary Learning", "comments": "Published at Conference on Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with auxiliary tasks can improve the ability of a primary task to\ngeneralise. However, this comes at the cost of manually labelling auxiliary\ndata. We propose a new method which automatically learns appropriate labels for\nan auxiliary task, such that any supervised learning task can be improved\nwithout requiring access to any further data. The approach is to train two\nneural networks: a label-generation network to predict the auxiliary labels,\nand a multi-task network to train the primary task alongside the auxiliary\ntask. The loss for the label-generation network incorporates the loss of the\nmulti-task network, and so this interaction between the two networks can be\nseen as a form of meta learning with a double gradient. We show that our\nproposed method, Meta AuXiliary Learning (MAXL), outperforms single-task\nlearning on 7 image datasets, without requiring any additional data. We also\nshow that MAXL outperforms several other baselines for generating auxiliary\nlabels, and is even competitive when compared with human-defined auxiliary\nlabels. The self-supervised nature of our method leads to a promising new\ndirection towards automated generalisation. Source code can be found at\nhttps://github.com/lorenmt/maxl.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 15:46:59 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 14:58:15 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 20:37:33 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Liu", "Shikun", ""], ["Davison", "Andrew J.", ""], ["Johns", "Edward", ""]]}, {"id": "1901.08942", "submitter": "Yiwei Sun", "authors": "Yimin Zhou, Yiwei Sun, Vasant Honavar", "title": "Improving Image Captioning by Leveraging Knowledge Graphs", "comments": "Accepted by WACV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We explore the use of a knowledge graphs, that capture general or commonsense\nknowledge, to augment the information extracted from images by the\nstate-of-the-art methods for image captioning. The results of our experiments,\non several benchmark data sets such as MS COCO, as measured by CIDEr-D, a\nperformance metric for image captioning, show that the variants of the\nstate-of-the-art methods for image captioning that make use of the information\nextracted from knowledge graphs can substantially outperform those that rely\nsolely on the information extracted from images.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 16:00:57 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Zhou", "Yimin", ""], ["Sun", "Yiwei", ""], ["Honavar", "Vasant", ""]]}, {"id": "1901.08954", "submitter": "Samet Akcay", "authors": "Samet Ak\\c{c}ay, Amir Atapour-Abarghouei, and Toby P. Breckon", "title": "Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder\n  Anomaly Detection", "comments": "Conference Submission. 8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite inherent ill-definition, anomaly detection is a research endeavor of\ngreat interest within machine learning and visual scene understanding alike.\nMost commonly, anomaly detection is considered as the detection of outliers\nwithin a given data distribution based on some measure of normality. The most\nsignificant challenge in real-world anomaly detection problems is that\navailable data is highly imbalanced towards normality (i.e. non-anomalous) and\ncontains a most a subset of all possible anomalous samples - hence limiting the\nuse of well-established supervised learning methods. By contrast, we introduce\nan unsupervised anomaly detection model, trained only on the normal\n(non-anomalous, plentiful) samples in order to learn the normality distribution\nof the domain and hence detect abnormality based on deviation from this model.\nOur proposed approach employs an encoder-decoder convolutional neural network\nwith skip connections to thoroughly capture the multi-scale distribution of the\nnormal data distribution in high-dimensional image space. Furthermore,\nutilizing an adversarial training scheme for this chosen architecture provides\nsuperior reconstruction both within high-dimensional image space and a\nlower-dimensional latent vector space encoding. Minimizing the reconstruction\nerror metric within both the image and hidden vector spaces during training\naids the model to learn the distribution of normality as required. Higher\nreconstruction metrics during subsequent test and deployment are thus\nindicative of a deviation from this normal distribution, hence indicative of an\nanomaly. Experimentation over established anomaly detection benchmarks and\nchallenging real-world datasets, within the context of X-ray security\nscreening, shows the unique promise of such a proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 16:18:22 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ak\u00e7ay", "Samet", ""], ["Atapour-Abarghouei", "Amir", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1901.08971", "submitter": "Andreas R\\\"ossler", "authors": "Andreas R\\\"ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess,\n  Justus Thies, Matthias Nie{\\ss}ner", "title": "FaceForensics++: Learning to Detect Manipulated Facial Images", "comments": "Video: https://youtu.be/x2g48Q2I2ZQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress in synthetic image generation and manipulation has now\ncome to a point where it raises significant concerns for the implications\ntowards society. At best, this leads to a loss of trust in digital content, but\ncould potentially cause further harm by spreading false information or fake\nnews. This paper examines the realism of state-of-the-art image manipulations,\nand how difficult it is to detect them, either automatically or by humans. To\nstandardize the evaluation of detection methods, we propose an automated\nbenchmark for facial manipulation detection. In particular, the benchmark is\nbased on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent\nrepresentatives for facial manipulations at random compression level and size.\nThe benchmark is publicly available and contains a hidden test set as well as a\ndatabase of over 1.8 million manipulated images. This dataset is over an order\nof magnitude larger than comparable, publicly available, forgery datasets.\nBased on this data, we performed a thorough analysis of data-driven forgery\ndetectors. We show that the use of additional domainspecific knowledge improves\nforgery detection to unprecedented accuracy, even in the presence of strong\ncompression, and clearly outperforms human observers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 16:38:21 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 16:46:57 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 17:59:54 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["R\u00f6ssler", "Andreas", ""], ["Cozzolino", "Davide", ""], ["Verdoliva", "Luisa", ""], ["Riess", "Christian", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1901.09002", "submitter": "Tai Sing Lee", "authors": "Jielin Qiu, Ge Huang, Tai Sing Lee", "title": "A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal\n  Sequence Learning and Prediction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we developed a hierarchical network model, called Hierarchical\nPrediction Network (HPNet), to understand how spatiotemporal memories might be\nlearned and encoded in the recurrent circuits in the visual cortical hierarchy\nfor predicting future video frames. This neurally inspired model operates in\nthe analysis-by-synthesis framework. It contains a feed-forward path that\ncomputes and encodes spatiotemporal features of successive complexity and a\nfeedback path for the successive levels to project their interpretations to the\nlevel below. Within each level, the feed-forward path and the feedback path\nintersect in a recurrent gated circuit, instantiated in a LSTM module, to\ngenerate a prediction or explanation of the incoming signals. The network\nlearns its internal model of the world by minimizing the errors of its\nprediction of the incoming signals at each level of the hierarchy. We found\nthat hierarchical interaction in the network increases semantic clustering of\nglobal movement patterns in the population codes of the units along the\nhierarchy, even in the earliest module. This facilitates the learning of\nrelationships among movement patterns, yielding state-of-the-art performance in\nlong range video sequence predictions in the benchmark datasets. The network\nmodel automatically reproduces a variety of prediction suppression and\nfamiliarity suppression neurophysiological phenomena observed in the visual\ncortex, suggesting that hierarchical prediction might indeed be an important\nprinciple for representational learning in the visual cortex.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 18:03:17 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Qiu", "Jielin", ""], ["Huang", "Ge", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1901.09005", "submitter": "Lucas Beyer", "authors": "Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer", "title": "Revisiting Self-Supervised Visual Representation Learning", "comments": "All three authors contributed equally. Code is available at\n  https://github.com/google/revisiting-self-supervised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised visual representation learning remains a largely unsolved\nproblem in computer vision research. Among a big body of recently proposed\napproaches for unsupervised learning of visual representations, a class of\nself-supervised techniques achieves superior performance on many challenging\nbenchmarks. A large number of the pretext tasks for self-supervised learning\nhave been studied, but other important aspects, such as the choice of\nconvolutional neural networks (CNN), has not received equal attention.\nTherefore, we revisit numerous previously proposed self-supervised models,\nconduct a thorough large scale study and, as a result, uncover multiple crucial\ninsights. We challenge a number of common practices in selfsupervised visual\nrepresentation learning and observe that standard recipes for CNN design do not\nalways translate to self-supervised representation learning. As part of our\nstudy, we drastically boost the performance of previously proposed techniques\nand outperform previously published state-of-the-art results by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 18:08:31 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Zhai", "Xiaohua", ""], ["Beyer", "Lucas", ""]]}, {"id": "1901.09054", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Deep Learning on Small Datasets without Pre-Training using Cosine Loss", "comments": "Presented at WACV 2020", "journal-ref": "2020 IEEE Winter Conference on Applications of Computer Vision\n  (WACV), Snowmass Village, CO, USA, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two things seem to be indisputable in the contemporary deep learning\ndiscourse: 1. The categorical cross-entropy loss after softmax activation is\nthe method of choice for classification. 2. Training a CNN classifier from\nscratch on small datasets does not work well. In contrast to this, we show that\nthe cosine loss function provides significantly better performance than\ncross-entropy on datasets with only a handful of samples per class. For\nexample, the accuracy achieved on the CUB-200-2011 dataset without pre-training\nis by 30% higher than with the cross-entropy loss. Further experiments on other\npopular datasets confirm our findings. Moreover, we demonstrate that\nintegrating prior knowledge in the form of class hierarchies is straightforward\nwith the cosine loss and improves classification performance further.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 19:13:03 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:15:46 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1901.09079", "submitter": "Pengkai Zhu", "authors": "Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama", "title": "Learning Classifiers for Domain Adaptation, Zero and Few-Shot\n  Recognition Based on Learning Latent Semantic Parts", "comments": "9 pages, 3 figures, 3 tables. Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision applications, such as domain adaptation (DA), few shot\nlearning (FSL) and zero-shot learning (ZSL), we encounter new objects and\nenvironments, for which insufficient examples exist to allow for training\n\"models from scratch,\" and methods that adapt existing models, trained on the\npresented training environment, to the new scenario are required. We propose a\nnovel visual attribute encoding method that encodes each image as a\nlow-dimensional probability vector composed of prototypical part-type\nprobabilities. The prototypes are learnt to be representative of all training\ndata. At test-time we utilize this encoding as an input to a classifier. At\ntest-time we freeze the encoder and only learn/adapt the classifier component\nto limited annotated labels in FSL; new semantic attributes in ZSL. We conduct\nextensive experiments on benchmark datasets. Our method outperforms\nstate-of-art methods trained for the specific contexts (ZSL, FSL, DA).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 20:49:35 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:21:48 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 19:11:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhu", "Pengkai", ""], ["Wang", "Hanxiao", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1901.09088", "submitter": "Angela Zhang", "authors": "Angela Zhang, Po-Yu Kao, Ronald Sahyouni, Ashutosh Shelat, Jefferson\n  Chen, B.S. Manjunath", "title": "Automated Segmentation of CT Scans for Normal Pressure Hydrocephalus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normal Pressure Hydrocephalus (NPH) is one of the few reversible forms of\ndementia, Due to their low cost and versatility, Computed Tomography (CT) scans\nhave long been used as an aid to help diagnose intracerebral anomalies such as\nNPH. However, no well-defined and effective protocol currently exists for the\nanalysis of CT scan-based ventricular, cerebral mass and subarachnoid space\nvolumes in the setting of NPH. The Evan's ratio, an approximation of the ratio\nof ventricle to brain volume using only one 2D slice of the scan, has been\nproposed but is not robust. Instead of manually measuring a 2-dimensional proxy\nfor the ratio of ventricle volume to brain volume, this study proposes an\nautomated method of calculating the brain volumes for better recognition of NPH\nfrom a radiological standpoint. The method first aligns the subject CT volume\nto a common space through an affine transformation, then uses a random forest\nclassifier to mask relevant tissue types. A 3D morphological segmentation\nmethod is used to partition the brain volume, which in turn is used to train\nmachine learning methods to classify the subjects into non-NPH vs. NPH based on\nvolumetric information. The proposed algorithm has increased sensitivity\ncompared to the Evan's ratio thresholding method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 21:29:14 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 17:47:32 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhang", "Angela", ""], ["Kao", "Po-Yu", ""], ["Sahyouni", "Ronald", ""], ["Shelat", "Ashutosh", ""], ["Chen", "Jefferson", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1901.09097", "submitter": "Hesham Mohamed Eraqi", "authors": "Hesham M. Eraqi, Yehya Abouelnaga, Mohamed H. Saad, Mohamed N.\n  Moustafa", "title": "Driver Distraction Identification with an Ensemble of Convolutional\n  Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1706.09498", "journal-ref": "Journal of Advanced Transportation, Machine Learning in\n  Transportation (MLT) Issue, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Health Organization (WHO) reported 1.25 million deaths yearly due\nto road traffic accidents worldwide and the number has been continuously\nincreasing over the last few years. Nearly fifth of these accidents are caused\nby distracted drivers. Existing work of distracted driver detection is\nconcerned with a small set of distractions (mostly, cell phone usage).\nUnreliable ad-hoc methods are often used.In this paper, we present the first\npublicly available dataset for driver distraction identification with more\ndistraction postures than existing alternatives. In addition, we propose a\nreliable deep learning-based solution that achieves a 90% accuracy. The system\nconsists of a genetically-weighted ensemble of convolutional neural networks,\nwe show that a weighted ensemble of classifiers using a genetic algorithm\nyields in a better classification confidence. We also study the effect of\ndifferent visual elements in distraction detection by means of face and hand\nlocalizations, and skin segmentation. Finally, we present a thinned version of\nour ensemble that could achieve 84.64% classification accuracy and operate in a\nreal-time environment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 10:47:00 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Eraqi", "Hesham M.", ""], ["Abouelnaga", "Yehya", ""], ["Saad", "Mohamed H.", ""], ["Moustafa", "Mohamed N.", ""]]}, {"id": "1901.09107", "submitter": "Huda Alamri", "authors": "Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop\n  Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks, Chiori Hori, Peter Anderson,\n  Stefan Lee, Devi Parikh", "title": "Audio-Visual Scene-Aware Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce the task of scene-aware dialog. Our goal is to generate a\ncomplete and natural response to a question about a scene, given video and\naudio of the scene and the history of previous turns in the dialog. To answer\nsuccessfully, agents must ground concepts from the question in the video while\nleveraging contextual cues from the dialog history. To benchmark this task, we\nintroduce the Audio Visual Scene-Aware Dialog (AVSD) Dataset. For each of more\nthan 11,000 videos of human actions from the Charades dataset, our dataset\ncontains a dialog about the video, plus a final summary of the video by one of\nthe dialog participants. We train several baseline systems for this task and\nevaluate the performance of the trained models using both qualitative and\nquantitative metrics. Our results indicate that models must utilize all the\navailable inputs (video, audio, question, and dialog history) to perform best\non this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 22:23:39 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 18:02:30 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Alamri", "Huda", ""], ["Cartillier", "Vincent", ""], ["Das", "Abhishek", ""], ["Wang", "Jue", ""], ["Cherian", "Anoop", ""], ["Essa", "Irfan", ""], ["Batra", "Dhruv", ""], ["Marks", "Tim K.", ""], ["Hori", "Chiori", ""], ["Anderson", "Peter", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""]]}, {"id": "1901.09124", "submitter": "Dingwen Tao", "authors": "Sian Jin, Sheng Di, Xin Liang, Jiannan Tian, Dingwen Tao, Franck\n  Cappello", "title": "DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using\n  Error-Bounded Lossy Compression", "comments": "12 pages, 6 figures, accepted by HPDC'19", "journal-ref": null, "doi": "10.1145/3307681.3326608", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNNs have been quickly and broadly exploited to improve the data analysis\nquality in many complex science and engineering applications. Today's DNNs are\nbecoming deeper and wider because of increasing demand on the analysis quality\nand more and more complex applications to resolve. The wide and deep DNNs,\nhowever, require large amounts of resources, significantly restricting their\nutilization on resource-constrained systems. Although some network\nsimplification methods have been proposed to address this issue, they suffer\nfrom either low compression ratios or high compression errors, which may\nintroduce a costly retraining process for the target accuracy. In this paper,\nwe propose DeepSZ: an accuracy-loss bounded neural network compression\nframework, which involves four key steps: network pruning, error bound\nassessment, optimization for error bound configuration, and compressed model\ngeneration, featuring a high compression ratio and low encoding time. The\ncontribution is three-fold. (1) We develop an adaptive approach to select the\nfeasible error bounds for each layer. (2) We build a model to estimate the\noverall loss of accuracy based on the accuracy degradation caused by individual\ndecompressed layers. (3) We develop an efficient optimization algorithm to\ndetermine the best-fit configuration of error bounds in order to maximize the\ncompression ratio under the user-set accuracy constraint. Experiments show that\nDeepSZ can compress AlexNet and VGG-16 on the ImageNet by a compression ratio\nof 46X and 116X, respectively, and compress LeNet-300-100 and LeNet-5 on the\nMNIST by a compression ratio of 57X and 56X, respectively, with only up to 0.3%\nloss of accuracy. Compared with other state-of-the-art methods, DeepSZ can\nimprove the compression ratio by up to 1.43X, the DNN encoding performance by\nup to 4.0X (with four Nvidia Tesla V100 GPUs), and the decoding performance by\nup to 6.2X.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 00:24:15 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 02:10:32 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Jin", "Sian", ""], ["Di", "Sheng", ""], ["Liang", "Xin", ""], ["Tian", "Jiannan", ""], ["Tao", "Dingwen", ""], ["Cappello", "Franck", ""]]}, {"id": "1901.09156", "submitter": "Norimichi Ukita", "authors": "Norimichi Ukita", "title": "Human Pose Estimation using Motion Priors and Ensemble Models", "comments": "6 pages", "journal-ref": "Presented at the 2017 International Conference on Advanced\n  Computer Science and Information Systems (ICACSIS)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation in images and videos is one of key technologies for\nrealizing a variety of human activity recognition tasks (e.g., human-computer\ninteraction, gesture recognition, surveillance, and video summarization). This\npaper presents two types of human pose estimation methodologies; 1) 3D human\npose tracking using motion priors and 2) 2D human pose estimation with ensemble\nmodeling.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 04:22:30 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Ukita", "Norimichi", ""]]}, {"id": "1901.09193", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Hongyuan Zhu, Shijian Lu", "title": "Scene Text Synthesis for Efficient and Effective Deep Network Training", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of annotated training images is critical for training accurate\nand robust deep network models but the collection of a large amount of\nannotated training images is often time-consuming and costly. Image synthesis\nalleviates this constraint by generating annotated training images\nautomatically by machines which has attracted increasing interest in the recent\ndeep learning research. We develop an innovative image synthesis technique that\ncomposes annotated training images by realistically embedding foreground\nobjects of interest (OOI) into background images. The proposed technique\nconsists of two key components that in principle boost the usefulness of the\nsynthesized images in deep network training. The first is context-aware\nsemantic coherence which ensures that the OOI are placed around semantically\ncoherent regions within the background image. The second is harmonious\nappearance adaptation which ensures that the embedded OOI are agreeable to the\nsurrounding background from both geometry alignment and appearance realism. The\nproposed technique has been evaluated over two related but very different\ncomputer vision challenges, namely, scene text detection and scene text\nrecognition. Experiments over a number of public datasets demonstrate the\neffectiveness of our proposed image synthesis technique - the use of our\nsynthesized images in deep network training is capable of achieving similar or\neven better scene text detection and scene text recognition performance as\ncompared with using real images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 10:15:24 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Zhan", "Fangneng", ""], ["Zhu", "Hongyuan", ""], ["Lu", "Shijian", ""]]}, {"id": "1901.09197", "submitter": "Ahmed Shahin", "authors": "Ahmed H. Shahin, Karim Amer, Mustafa A. Elattar", "title": "Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution\n  Skip Connections for Skin Lesion Segmentation", "comments": "Accepted for publication at the IEEE International Symposium on\n  Biomedical Imaging (ISBI 2019)", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of skin melanoma is rapidly increasing as well as the recorded\ndeath cases of its patients. Automatic image segmentation tools play an\nimportant role in providing standardized computer-assisted analysis for skin\nmelanoma patients. Current state-of-the-art segmentation methods are based on\nfully convolutional neural networks, which utilize an encoder-decoder approach.\nHowever, these methods produce coarse segmentation masks due to the loss of\nlocation information during the encoding layers. Inspired by Pyramid Scene\nParsing Network (PSP-Net), we propose an encoder-decoder model that utilizes\npyramid pooling modules in the deep skip connections which aggregate the global\ncontext and compensate for the lost spatial information. We trained and\nvalidated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma\nDetection grand challenge dataset. Our approach showed a validation accuracy\nwith a Jaccard index of 0.837, which outperforms U-Net. We believe that with\nthis reported reliable accuracy, this method can be introduced for clinical\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 10:55:27 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:27:37 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Shahin", "Ahmed H.", ""], ["Amer", "Karim", ""], ["Elattar", "Mustafa A.", ""]]}, {"id": "1901.09221", "submitter": "Dongwei Ren", "authors": "Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, Deyu Meng", "title": "Progressive Image Deraining Networks: A Better and Simpler Baseline", "comments": "Accepted to CVPR 2019. The codes, pre-trained models and results are\n  available at https://github.com/csdwren/PReNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the deraining performance improvement of deep networks, their\nstructures and learning become more and more complicated and diverse, making it\ndifficult to analyze the contribution of various network modules when\ndeveloping new deraining networks. To handle this issue, this paper provides a\nbetter and simpler baseline deraining network by considering network\narchitecture, input and output, and loss functions. Specifically, by repeatedly\nunfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take\nadvantage of recursive computation. A recurrent layer is further introduced to\nexploit the dependencies of deep features across stages, forming our\nprogressive recurrent network (PReNet). Furthermore, intra-stage recursive\ncomputation of ResNet can be adopted in PRN and PReNet to notably reduce\nnetwork parameters with graceful degradation in deraining performance. For\nnetwork input and output, we take both stage-wise result and original rainy\nimage as input to each ResNet and finally output the prediction of {residual\nimage}. As for loss functions, single MSE or negative SSIM losses are\nsufficient to train PRN and PReNet. Experiments show that PRN and PReNet\nperform favorably on both synthetic and real rainy images. Considering its\nsimplicity, efficiency and effectiveness, our models are expected to serve as a\nsuitable baseline in future deraining research. The source codes are available\nat https://github.com/csdwren/PReNet.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 14:44:17 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 04:32:49 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 04:05:09 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ren", "Dongwei", ""], ["Zuo", "Wangmeng", ""], ["Hu", "Qinghua", ""], ["Zhu", "Pengfei", ""], ["Meng", "Deyu", ""]]}, {"id": "1901.09237", "submitter": "Anubhav Jain", "authors": "Anubhav Jain, Richa Singh, Mayank Vatsa", "title": "On Detecting GANs and Retouching based Synthetic Alterations", "comments": "The 9th IEEE International Conference on Biometrics: Theory,\n  Applications, and Systems (BTAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitally retouching images has become a popular trend, with people posting\naltered images on social media and even magazines posting flawless facial\nimages of celebrities. Further, with advancements in Generative Adversarial\nNetworks (GANs), now changing attributes and retouching have become very easy.\nSuch synthetic alterations have adverse effect on face recognition algorithms.\nWhile researchers have proposed to detect image tampering, detecting GANs\ngenerated images has still not been explored. This paper proposes a supervised\ndeep learning algorithm using Convolutional Neural Networks (CNNs) to detect\nsynthetically altered images. The algorithm yields an accuracy of 99.65% on\ndetecting retouching on the ND-IIITD dataset. It outperforms the previous state\nof the art which reported an accuracy of 87% on the database. For\ndistinguishing between real images and images generated using GANs, the\nproposed algorithm yields an accuracy of 99.83%.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 16:13:03 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Jain", "Anubhav", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1901.09244", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Du Tran, Lorenzo Torresani and Deva Ramanan", "title": "DistInit: Learning Video Representations Without a Single Labeled Video", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video recognition models have progressed significantly over the past few\nyears, evolving from shallow classifiers trained on hand-crafted features to\ndeep spatiotemporal networks. However, labeled video data required to train\nsuch models have not been able to keep up with the ever-increasing depth and\nsophistication of these networks. In this work, we propose an alternative\napproach to learning video representations that require no semantically labeled\nvideos and instead leverages the years of effort in collecting and labeling\nlarge and clean still-image datasets. We do so by using state-of-the-art models\npre-trained on image datasets as \"teachers\" to train video models in a\ndistillation framework. We demonstrate that our method learns truly\nspatiotemporal features, despite being trained only using supervision from\nstill-image networks. Moreover, it learns good representations across different\ninput modalities, using completely uncurated raw video data sources and with\ndifferent 2D teacher models. Our method obtains strong transfer performance,\noutperforming standard techniques for bootstrapping video architectures with\nimage-based models by 16%. We believe that our approach opens up new approaches\nfor learning spatiotemporal representations from unlabeled video data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 16:50:38 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 05:02:40 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Girdhar", "Rohit", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""], ["Ramanan", "Deva", ""]]}, {"id": "1901.09260", "submitter": "Aljo\\v{s}a O\\v{s}ep", "authors": "Aljosa Osep, Paul Voigtlaender, Mark Weber, Jonathon Luiten, Bastian\n  Leibe", "title": "4D Generic Video Object Proposals", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high-level video understanding methods require input in the form of\nobject proposals. Currently, such proposals are predominantly generated with\nthe help of networks that were trained for detecting and segmenting a set of\nknown object classes, which limits their applicability to cases where all\nobjects of interest are represented in the training set. This is a restriction\nfor automotive scenarios, where unknown objects can frequently occur. We\npropose an approach that can reliably extract spatio-temporal object proposals\nfor both known and unknown object categories from stereo video. Our 4D Generic\nVideo Tubes (4D-GVT) method leverages motion cues, stereo data, and object\ninstance segmentation to compute a compact set of video-object proposals that\nprecisely localizes object candidates and their contours in 3D space and time.\nWe show that given only a small amount of labeled data, our 4D-GVT proposal\ngenerator generalizes well to real-world scenarios, in which unknown categories\nappear. It outperforms other approaches that try to detect as many objects as\npossible by increasing the number of classes in the training set to several\nthousand.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 18:31:23 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 15:37:53 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 18:45:15 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Osep", "Aljosa", ""], ["Voigtlaender", "Paul", ""], ["Weber", "Mark", ""], ["Luiten", "Jonathon", ""], ["Leibe", "Bastian", ""]]}, {"id": "1901.09263", "submitter": "Eytan Kats", "authors": "Eytan Kats, Jacob Goldberger, Hayit Greenspan", "title": "Soft labeling by Distilling Anatomical knowledge for Improved MS Lesion\n  Segmentation", "comments": "Accepted for ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of a soft ground-truth mask (\"soft mask'') to\ntrain a Fully Convolutional Neural Network (FCNN) for segmentation of Multiple\nSclerosis (MS) lesions. Detection and segmentation of MS lesions is a complex\ntask largely due to the extreme unbalanced data, with very small number of\nlesion pixels that can be used for training. Utilizing the anatomical knowledge\nthat the lesion surrounding pixels may also include some lesion level\ninformation, we suggest to increase the data set of the lesion class with\nneighboring pixel data - with a reduced confidence weight. A soft mask is\nconstructed by morphological dilation of the binary segmentation mask provided\nby a given expert, where expert-marked voxels receive label 1 and voxels of the\ndilated region are assigned a soft label. In the methodology proposed, the FCNN\nis trained using the soft mask. On the ISBI 2015 challenge dataset, this is\nshown to provide a better precision-recall tradeoff and to achieve a higher\naverage Dice similarity coefficient. We also show that by using this soft mask\nscheme we can improve the network segmentation performance when compared to a\nsecond independent expert.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 18:52:46 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kats", "Eytan", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1901.09270", "submitter": "Senthil Yogamani", "authors": "Michal Uricar, David Hurych, Pavel Krizek and Senthil Yogamani", "title": "Challenges in Designing Datasets and Validation for Autonomous Driving", "comments": "Accepted at VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is getting a lot of attention in the last decade and will\nbe the hot topic at least until the first successful certification of a car\nwith Level 5 autonomy. There are many public datasets in the academic\ncommunity. However, they are far away from what a robust industrial production\nsystem needs. There is a large gap between academic and industrial setting and\na substantial way from a research prototype, built on public datasets, to a\ndeployable solution which is a challenging task. In this paper, we focus on bad\npractices that often happen in the autonomous driving from an industrial\ndeployment perspective. Data design deserves at least the same amount of\nattention as the model design. There is very little attention paid to these\nissues in the scientific community, and we hope this paper encourages better\nformalization of dataset design. More specifically, we focus on the datasets\ndesign and validation scheme for autonomous driving, where we would like to\nhighlight the common problems, wrong assumptions, and steps towards avoiding\nthem, as well as some open problems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 19:51:59 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Uricar", "Michal", ""], ["Hurych", "David", ""], ["Krizek", "Pavel", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1901.09280", "submitter": "Stefan Milz", "authors": "Stefan Milz, Martin Simon, Kai Fischer, Maximillian P\\\"opperl", "title": "Points2Pix: 3D Point-Cloud to Image Translation using conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first approach for 3D point-cloud to image translation based\non conditional Generative Adversarial Networks (cGAN). The model handles\nmulti-modal information sources from different domains, i.e. raw point-sets and\nimages. The generator is capable of processing three conditions, whereas the\npoint-cloud is encoded as raw point-set and camera projection. An image\nbackground patch is used as constraint to bias environmental texturing. A\nglobal approximation function within the generator is directly applied on the\npoint-cloud (Point-Net). Hence, the representative learning model incorporates\nglobal 3D characteristics directly at the latent feature space. Conditions are\nused to bias the background and the viewpoint of the generated image. This\nopens up new ways in augmenting or texturing 3D data to aim the generation of\nfully individual images. We successfully evaluated our method on the Kitti and\nSunRGBD dataset with an outstanding object detection inception score.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 21:17:21 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 12:02:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Milz", "Stefan", ""], ["Simon", "Martin", ""], ["Fischer", "Kai", ""], ["P\u00f6pperl", "Maximillian", ""]]}, {"id": "1901.09287", "submitter": "Faisal Qureshi", "authors": "Wesley Taylor and Faisal Z. Qureshi", "title": "Real-time Video Summarization on Commodity Hardware", "comments": "Appeared in Proc. 12th ACM International Conference on Distributed\n  Smart Cameras (ICDSC 18), pages 8pp, Eidenhoven, September 2018", "journal-ref": null, "doi": "10.1145/3243394.3243689 10.1145/3243394.3243689", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for creating video summaries in real-time on commodity\nhardware. Real-time here refers to the fact that the time required for video\nsummarization is less than the duration of the input video. First, low-level\nfeatures are use to discard undesirable frames. Next, video is divided into\nsegments, and segment-level features are extracted for each segment. Tree-based\nmodels trained on widely available video summarization and computational\naesthetics datasets are then used to rank individual segments, and top-ranked\nsegments are selected to generate the final video summary. We evaluate the\nproposed method on SUMME dataset and show that our method is able to achieve\nsummarization accuracy that is comparable to that of a current state-of-the-art\ndeep learning method, while posting significantly faster run-times. Our method\non average is able to generate a video summary in time that is shorter than the\nduration of the video.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 22:20:50 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Taylor", "Wesley", ""], ["Qureshi", "Faisal Z.", ""]]}, {"id": "1901.09321", "submitter": "Hongyi Zhang", "authors": "Hongyi Zhang, Yann N. Dauphin, Tengyu Ma", "title": "Fixup Initialization: Residual Learning Without Normalization", "comments": "Updating reference. Accepted for publication at ICLR 2019; see\n  https://openreview.net/forum?id=H1gsz30cKX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers are a staple in state-of-the-art deep neural network\narchitectures. They are widely believed to stabilize training, enable higher\nlearning rate, accelerate convergence and improve generalization, though the\nreason for their effectiveness is still an active research topic. In this work,\nwe challenge the commonly-held beliefs by showing that none of the perceived\nbenefits is unique to normalization. Specifically, we propose fixed-update\ninitialization (Fixup), an initialization motivated by solving the exploding\nand vanishing gradient problem at the beginning of training via properly\nrescaling a standard initialization. We find training residual networks with\nFixup to be as stable as training with normalization -- even for networks with\n10,000 layers. Furthermore, with proper regularization, Fixup enables residual\nnetworks without normalization to achieve state-of-the-art performance in image\nclassification and machine translation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 05:30:11 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 00:31:18 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Zhang", "Hongyi", ""], ["Dauphin", "Yann N.", ""], ["Ma", "Tengyu", ""]]}, {"id": "1901.09351", "submitter": "Robert Robinson", "authors": "Robert Robinson (1), Vanya V. Valindria (1), Wenjia Bai (1), Ozan\n  Oktay (1), Bernhard Kainz (1), Hideaki Suzuki (2), Mihir M. Sanghvi (4 and\n  5), Nay Aung (4 and 5), Jos$\\'e$ Miguel Paiva (4), Filip Zemrak (4 and 5),\n  Kenneth Fung (4 and 5), Elena Lukaschuk (6), Aaron M. Lee (4 and 5),\n  Valentina Carapella (6), Young Jin Kim (6 and 7), Stefan K. Piechnik (6),\n  Stefan Neubauer (6), Steffen E. Petersen (4 and 5), Chris Page (3), Paul M.\n  Matthews (2 and 8), Daniel Rueckert (1) and Ben Glocker (1) ((1) Biomedical\n  Image Analysis Group, Department of Computing, Imperial College London, (2)\n  Division of Brain Sciences, Dept. of Medicine, Imperial College London, (3)\n  GlaxoSmithKline Research and Development, UK, (4) William Harvey Research\n  Institute, Queen Mary University of London, (5) Barts Heart Centre, London,\n  (6) Division of Cardiovascular Medicine, Radcliffe Department of Medicine,\n  University of Oxford, (7) Department of Radiology, Severance Hospital, South\n  Korea, (8) UK Dementia Research Institute, Imperial College London)", "title": "Automated Quality Control in Image Segmentation: Application to the UK\n  Biobank Cardiac MR Imaging Study", "comments": "14 pages, 7 figures, Journal of Cardiovascular Magnetic Resonance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The trend towards large-scale studies including population\nimaging poses new challenges in terms of quality control (QC). This is a\nparticular issue when automatic processing tools, e.g. image segmentation\nmethods, are employed to derive quantitative measures or biomarkers for later\nanalyses. Manual inspection and visual QC of each segmentation isn't feasible\nat large scale. However, it's important to be able to automatically detect when\na segmentation method fails so as to avoid inclusion of wrong measurements into\nsubsequent analyses which could lead to incorrect conclusions. Methods: To\novercome this challenge, we explore an approach for predicting segmentation\nquality based on Reverse Classification Accuracy, which enables us to\ndiscriminate between successful and failed segmentations on a per-cases basis.\nWe validate this approach on a new, large-scale manually-annotated set of 4,800\ncardiac magnetic resonance scans. We then apply our method to a large cohort of\n7,250 cardiac MRI on which we have performed manual QC. Results: We report\nresults used for predicting segmentation quality metrics including Dice\nSimilarity Coefficient (DSC) and surface-distance measures. As initial\nvalidation, we present data for 400 scans demonstrating 99% accuracy for\nclassifying low and high quality segmentations using predicted DSC scores. As\nfurther validation we show high correlation between real and predicted scores\nand 95% classification accuracy on 4,800 scans for which manual segmentations\nwere available. We mimic real-world application of the method on 7,250 cardiac\nMRI where we show good agreement between predicted quality metrics and manual\nvisual QC scores. Conclusions: We show that RCA has the potential for accurate\nand fully automatic segmentation QC on a per-case basis in the context of\nlarge-scale population imaging as in the UK Biobank Imaging Study.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 10:52:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Robinson", "Robert", "", "4 and\n  5"], ["Valindria", "Vanya V.", "", "4 and\n  5"], ["Bai", "Wenjia", "", "4 and\n  5"], ["Oktay", "Ozan", "", "4 and\n  5"], ["Kainz", "Bernhard", "", "4 and\n  5"], ["Suzuki", "Hideaki", "", "4 and\n  5"], ["Sanghvi", "Mihir M.", "", "4 and\n  5"], ["Aung", "Nay", "", "4 and 5"], ["Paiva", "Jos$\u00e9$ Miguel", "", "4 and 5"], ["Zemrak", "Filip", "", "4 and 5"], ["Fung", "Kenneth", "", "4 and 5"], ["Lukaschuk", "Elena", "", "4 and 5"], ["Lee", "Aaron M.", "", "4 and 5"], ["Carapella", "Valentina", "", "6 and 7"], ["Kim", "Young Jin", "", "6 and 7"], ["Piechnik", "Stefan K.", "", "4 and 5"], ["Neubauer", "Stefan", "", "4 and 5"], ["Petersen", "Steffen E.", "", "4 and 5"], ["Page", "Chris", "", "2 and 8"], ["Matthews", "Paul M.", "", "2 and 8"], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1901.09364", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Meirav Galun, Ronen Basri", "title": "Resultant Based Incremental Recovery of Camera Pose from Pairwise\n  Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental (online) structure from motion pipelines seek to recover the\ncamera matrix associated with an image $I_n$ given $n-1$ images,\n$I_1,...,I_{n-1}$, whose camera matrices have already been recovered. In this\npaper, we introduce a novel solution to the six-point online algorithm to\nrecover the exterior parameters associated with $I_n$. Our algorithm uses just\nsix corresponding pairs of 2D points, extracted each from $I_n$ and from\n\\textit{any} of the preceding $n-1$ images, allowing the recovery of the full\nsix degrees of freedom of the $n$'th camera, and unlike common methods, does\nnot require tracking feature points in three or more images. Our novel solution\nis based on constructing a Dixon resultant, yielding a solution method that is\nboth efficient and accurate compared to existing solutions. We further use\nBernstein's theorem to prove a tight bound on the number of complex solutions.\nOur experiments demonstrate the utility of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 12:13:51 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kasten", "Yoni", ""], ["Galun", "Meirav", ""], ["Basri", "Ronen", ""]]}, {"id": "1901.09366", "submitter": "Jin Liu", "authors": "Jin Liu, Sheng He", "title": "6D Object Pose Estimation Based on 2D Bounding Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple but powerful method to tackle the problem\nof estimating the 6D pose of objects from a single RGB image. Our system trains\na novel convolutional neural network to regress the unit quaternion, which\nrepresents the 3D rotation, from the partial image inside the bounding box\nreturned by 2D detection systems. Then we propose an algorithm we call Bounding\nBox Equation to efficiently and accurately obtain the 3D translation, using 3D\nrotation and 2D bounding box. Considering that the quadratic sum of the\nquaternion's four elements equals to one, we add a normalization layer to keep\nthe network's output on the unit sphere and put forward a special loss function\nfor unit quaternion regression. We evaluate our method on the LineMod dataset\nand experiment shows that our approach outperforms base-line and some state of\nthe art methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 12:40:04 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Liu", "Jin", ""], ["He", "Sheng", ""]]}, {"id": "1901.09394", "submitter": "Edoardo Remelli", "authors": "Edoardo Remelli, Pierre Baque, Pascal Fua", "title": "NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most algorithms that rely on deep learning-based approaches to generate 3D\npoint sets can only produce clouds containing fixed number of points.\nFurthermore, they typically require large networks parameterized by many\nweights, which makes them hard to train. In this paper, we propose an\nauto-encoder architecture that can both encode and decode clouds of arbitrary\nsize and demonstrate its effectiveness at upsampling sparse point clouds.\nInterestingly, we can do so using less than half as many parameters as\nstate-of-the-art architectures while still delivering better performance. We\nwill make our code base fully available.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 15:38:49 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Remelli", "Edoardo", ""], ["Baque", "Pierre", ""], ["Fua", "Pascal", ""]]}, {"id": "1901.09402", "submitter": "Amlaan Bhoi", "authors": "Amlaan Bhoi", "title": "Monocular Depth Estimation: A Survey", "comments": "8 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is often described as an ill-posed and inherently\nambiguous problem. Estimating depth from 2D images is a crucial step in scene\nreconstruction, 3Dobject recognition, segmentation, and detection. The problem\ncan be framed as: given a single RGB image as input, predict a dense depth map\nfor each pixel. This problem is worsened by the fact that most scenes have\nlarge texture and structural variations, object occlusions, and rich geometric\ndetailing. All these factors contribute to difficulty in accurate depth\nestimation. In this paper, we review five papers that attempt to solve the\ndepth estimation problem with various techniques including supervised,\nweakly-supervised, and unsupervised learning techniques. We then compare these\npapers and understand the improvements made over one another. Finally, we\nexplore potential improvements that can aid to better solve this problem.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 16:46:23 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Bhoi", "Amlaan", ""]]}, {"id": "1901.09403", "submitter": "Amlaan Bhoi", "authors": "Amlaan Bhoi", "title": "Spatio-temporal Action Recognition: A Survey", "comments": "15 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of action recognition or action detection involves analyzing videos\nand determining what action or motion is being performed. The primary subject\nof these videos are predominantly humans performing some action. However, this\nrequirement can be relaxed to generalize over other subjects such as animals or\nrobots. The applications can range from anywhere between human-computer\ninter-action to automated video editing proposals. When we consider\nspatiotemporal action recognition, we deal with action localization. This task\nnot only involves determining what action is being performed but also when and\nwhere itis being performed in said video. This paper aims to survey the\nplethora of approaches and algorithms attempted to solve this task, give a\ncomprehensive comparison between them, explore various datasets available for\nthe problem, and determine the most promising approaches.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 16:53:55 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Bhoi", "Amlaan", ""]]}, {"id": "1901.09407", "submitter": "Yu-Len Huang", "authors": "Dar-Ren Chen, Yu-Chih Lin and Yu-Len Huang", "title": "3D Contouring for Breast Tumor in Sonography", "comments": "18 pages, 1 table and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malignant and benign breast tumors present differently in their shape and\nsize on sonography. Morphological information provided by tumor contours are\nimportant in clinical diagnosis. However, ultrasound images contain noises and\ntissue texture; clinical diagnosis thus highly depends on the experience of\nphysicians. The manual way to sketch three-dimensional (3D) contours of breast\ntumor is a time-consuming and complicate task. If automatic contouring could\nprovide a precise breast tumor contour that might assist physicians in making\nan accurate diagnosis. This study presents an efficient method for\nautomatically contouring breast tumors in 3D sonography. The proposed method\nutilizes an efficient segmentation procedure, i.e. level-set method (LSM), to\nautomatic detect contours of breast tumors. This study evaluates 20 cases\ncomprising ten benign and ten malignant tumors. The results of computer\nsimulation reveal that the proposed 3D segmentation method provides robust\ncontouring for breast tumor on ultrasound images. This approach consistently\nobtains contours similar to those obtained by manual contouring of the breast\ntumor and can save much of the time required to sketch precise contours.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 17:40:30 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 16:56:15 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Chen", "Dar-Ren", ""], ["Lin", "Yu-Chih", ""], ["Huang", "Yu-Len", ""]]}, {"id": "1901.09425", "submitter": "Omar Boudraa", "authors": "Omar Boudraa, Walid Khaled Hidouci and Dominique Michelucci", "title": "Degraded Historical Documents Images Binarization Using a Combination of\n  Enhanced Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image binarization is the initial step and a crucial in many\ndocument analysis and recognition scheme. In fact, it is still a relevant\nresearch subject and a fundamental challenge due to its importance and\ninfluence. This paper provides an original multi-phases system that hybridizes\nvarious efficient image thresholding methods in order to get the best\nbinarization output. First, to improve contrast in particularly defective\nimages, the application of CLAHE algorithm is suggested and justified. We then\nuse a cooperative technique to segment image into two separated classes. At the\nend, a special transformation is applied for the purpose of removing scattered\nnoise and of correcting characters forms. Experimentations demonstrate the\nprecision and the robustness of our framework applied on historical degraded\ndocuments images within three benchmarks compared to other noted methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 19:37:52 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Boudraa", "Omar", ""], ["Hidouci", "Walid Khaled", ""], ["Michelucci", "Dominique", ""]]}, {"id": "1901.09447", "submitter": "Xiang Xu", "authors": "Xiang Xu and Ioannis A. Kakadiaris", "title": "Open Source Face Recognition Performance Evaluation Package", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics-related research has been accelerated significantly by deep\nlearning technology. However, there are limited open-source resources to help\nresearchers evaluate their deep learning-based biometrics algorithms\nefficiently, especially for the face recognition tasks. In this work, we design\nand implement a light-weight, maintainable, scalable, generalizable, and\nextendable face recognition evaluation toolbox named FaRE that supports both\nonline and offline evaluation to provide feedback to algorithm development and\naccelerate biometrics-related research. FaRE consists of a set of evaluation\nmetric functions and provides various APIs for commonly-used face recognition\ndatasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be\neasily extended to include other customized datasets. The package and the\npre-trained baseline models will be released for public academic research use\nafter obtaining university approval.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 21:53:26 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Xu", "Xiang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1901.09458", "submitter": "Xiangru Huang", "authors": "Xiangru Huang, Zhenxiao Liang, Xiaowei Zhou, Yao Xie, Leonidas Guibas\n  and Qixing Huang", "title": "Learning Transformation Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the 3D model of a physical object typically requires us to\nalign the depth scans obtained from different camera poses into the same\ncoordinate system. Solutions to this global alignment problem usually proceed\nin two steps. The first step estimates relative transformations between pairs\nof scans using an off-the-shelf technique. Due to limited information presented\nbetween pairs of scans, the resulting relative transformations are generally\nnoisy. The second step then jointly optimizes the relative transformations\namong all input depth scans. A natural constraint used in this step is the\ncycle-consistency constraint, which allows us to prune incorrect relative\ntransformations by detecting inconsistent cycles. The performance of such\napproaches, however, heavily relies on the quality of the input relative\ntransformations. Instead of merely using the relative transformations as the\ninput to perform transformation synchronization, we propose to use a neural\nnetwork to learn the weights associated with each relative transformation. Our\napproach alternates between transformation synchronization using weighted\nrelative transformations and predicting new weights of the input relative\ntransformations using a neural network. We demonstrate the usefulness of this\napproach across a wide range of datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 23:09:21 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:14:07 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Huang", "Xiangru", ""], ["Liang", "Zhenxiao", ""], ["Zhou", "Xiaowei", ""], ["Xie", "Yao", ""], ["Guibas", "Leonidas", ""], ["Huang", "Qixing", ""]]}, {"id": "1901.09473", "submitter": "Ning Ma", "authors": "Ning Ma, Xin Zhao, Mark Bolin", "title": "An End-to-End Solution for Effectively Demoting Watermarked Images in\n  Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end solution, from watermark feature generation to\nmetric design, for effectively demoting watermarked images surfed by a real\nworld image search engine. We use a few fundamental techniques to obtain\neffective watermark features of images in the image search index, and utilize\nthe signals in a commercial search engine to improve the image search quality.\nWe collect a diverse and large set (about 1M) of images with human labels\nindicating whether the image contains visible watermark. We train a few deep\nconvolutional neural networks to extract watermark information from the raw\nimages. The deep CNN classifiers we trained can achieve high accuracy on the\nwatermark test data set. We also analyze the images based on their domains to\nget watermark information from a domain-based watermark classifier. We design a\nnew novel hybrid metric which includes the relevance, image attractiveness and\nwatermark information all together. We demonstrate that using these watermark\nsignals together with the new metric in image search ranker can significantly\ndemote the watermarked images during the online image ranking.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 00:38:16 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 05:34:39 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ma", "Ning", ""], ["Zhao", "Xin", ""], ["Bolin", "Mark", ""]]}, {"id": "1901.09482", "submitter": "Rosaura VidalMata", "authors": "Rosaura G. VidalMata, Sreya Banerjee, Brandon RichardWebster, Michael\n  Albright, Pedro Davalos, Scott McCloskey, Ben Miller, Asong Tambo, Sushobhan\n  Ghosh, Sudarshan Nagesh, Ye Yuan, Yueyu Hu, Junru Wu, Wenhan Yang, Xiaoshuai\n  Zhang, Jiaying Liu, Zhangyang Wang, Hwann-Tzong Chen, Tzu-Wei Huang, Wen-Chi\n  Chin, Yi-Chun Li, Mahmoud Lababidi, Charles Otto, and Walter J. Scheirer", "title": "Bridging the Gap Between Computational Photography and Visual\n  Recognition", "comments": "CVPR Prize Challenge: http://www.ug2challenge.org", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2996538", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the current state-of-the-art for image restoration and enhancement\napplied to degraded images acquired under less than ideal circumstances? Can\nthe application of such algorithms as a pre-processing step to improve image\ninterpretability for manual analysis or automatic visual recognition to\nclassify scene content? While there have been important advances in the area of\ncomputational photography to restore or enhance the visual quality of an image,\nthe capabilities of such techniques have not always translated in a useful way\nto visual recognition tasks. Consequently, there is a pressing need for the\ndevelopment of algorithms that are designed for the joint problem of improving\nvisual appearance and recognition, which will be an enabling factor for the\ndeployment of visual recognition tools in many real-world scenarios. To address\nthis, we introduce the UG^2 dataset as a large-scale benchmark composed of\nvideo imagery captured under challenging conditions, and two enhancement tasks\ndesigned to test algorithmic impact on visual quality and automatic object\nrecognition. Furthermore, we propose a set of metrics to evaluate the joint\nimprovement of such tasks as well as individual algorithmic advances, including\na novel psychophysics-based evaluation regime for human assessment and a\nrealistic set of quantitative measures for object recognition performance. We\nintroduce six new algorithms for image restoration or enhancement, which were\ncreated as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR\n2018. Under the proposed evaluation regime, we present an in-depth analysis of\nthese algorithms and a host of deep learning-based and classic baseline\napproaches. From the observed results, it is evident that we are in the early\ndays of building a bridge between computational photography and visual\nrecognition, leaving many opportunities for innovation in this area.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 01:34:32 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 01:06:37 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 19:05:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["VidalMata", "Rosaura G.", ""], ["Banerjee", "Sreya", ""], ["RichardWebster", "Brandon", ""], ["Albright", "Michael", ""], ["Davalos", "Pedro", ""], ["McCloskey", "Scott", ""], ["Miller", "Ben", ""], ["Tambo", "Asong", ""], ["Ghosh", "Sushobhan", ""], ["Nagesh", "Sudarshan", ""], ["Yuan", "Ye", ""], ["Hu", "Yueyu", ""], ["Wu", "Junru", ""], ["Yang", "Wenhan", ""], ["Zhang", "Xiaoshuai", ""], ["Liu", "Jiaying", ""], ["Wang", "Zhangyang", ""], ["Chen", "Hwann-Tzong", ""], ["Huang", "Tzu-Wei", ""], ["Chin", "Wen-Chi", ""], ["Li", "Yi-Chun", ""], ["Lababidi", "Mahmoud", ""], ["Otto", "Charles", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1901.09483", "submitter": "Samuel Kadoury", "authors": "Francisco Perdigon Romero, Andre Diler, Gabriel Bisson-Gregoire, Simon\n  Turcotte, Real Lapointe, Franck Vandenbroucke-Menu, An Tang and Samuel\n  Kadoury", "title": "End-to-End Discriminative Deep Network for Liver Lesion Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal liver metastasis is one of most aggressive liver malignancies.\nWhile the definition of lesion type based on CT images determines the diagnosis\nand therapeutic strategy, the discrimination between cancerous and\nnon-cancerous lesions are critical and requires highly skilled expertise,\nexperience and time. In the present work we introduce an end-to-end deep\nlearning approach to assist in the discrimination between liver metastases from\ncolorectal cancer and benign cysts in abdominal CT images of the liver. Our\napproach incorporates the efficient feature extraction of InceptionV3 combined\nwith residual connections and pre-trained weights from ImageNet. The\narchitecture also includes fully connected classification layers to generate a\nprobabilistic output of lesion type. We use an in-house clinical biobank with\n230 liver lesions originating from 63 patients. With an accuracy of 0.96 and a\nF1-score of 0.92, the results obtained with the proposed approach surpasses\nstate of the art methods. Our work provides the basis for incorporating machine\nlearning tools in specialized radiology software to assist physicians in the\nearly detection and treatment of liver lesions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 01:41:16 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Romero", "Francisco Perdigon", ""], ["Diler", "Andre", ""], ["Bisson-Gregoire", "Gabriel", ""], ["Turcotte", "Simon", ""], ["Lapointe", "Real", ""], ["Vandenbroucke-Menu", "Franck", ""], ["Tang", "An", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1901.09548", "submitter": "Yiping Lu", "authors": "Bin Dong, Haocheng Ju, Yiping Lu, Zuoqiang Shi", "title": "CURE: Curvature Regularization For Missing Data Recovery", "comments": "17 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data recovery is an important and yet challenging problem in imaging\nand data science. Successful models often adopt certain carefully chosen\nregularization. Recently, the low dimension manifold model (LDMM) was\nintroduced by S.Osher et al. and shown effective in image inpainting. They\nobserved that enforcing low dimensionality on image patch manifold serves as a\ngood image regularizer. In this paper, we observe that having only the low\ndimension manifold regularization is not enough sometimes, and we need\nsmoothness as well. For that, we introduce a new regularization by combining\nthe low dimension manifold regularization with a higher order Curvature\nRegularization, and we call this new regularization CURE for short. The key\nstep of solving CURE is to solve a biharmonic equation on a manifold. We\nfurther introduce a weighted version of CURE, called WeCURE, in a similar\nmanner as the weighted nonlocal Laplacian (WNLL) method. Numerical experiments\nfor image inpainting and semi-supervised learning show that the proposed CURE\nand WeCURE significantly outperform LDMM and WNLL respectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 08:42:39 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 15:19:31 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 00:28:54 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Dong", "Bin", ""], ["Ju", "Haocheng", ""], ["Lu", "Yiping", ""], ["Shi", "Zuoqiang", ""]]}, {"id": "1901.09575", "submitter": "Xiandong Meng", "authors": "Xiandong Meng, Xuan Deng, Shuyuan Zhu and Bing Zeng", "title": "Enhancing Quality for VVC Compressed Videos by Jointly Exploiting\n  Spatial Details and Temporal Structure", "comments": "Accepted to IEEE International Conference on Image Processing (ICIP)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a quality enhancement network of versatile video\ncoding (VVC) compressed videos by jointly exploiting spatial details and\ntemporal structure (SDTS). The proposed network consists of a temporal\nstructure fusion subnet and a spatial detail enhancement subnet. The former\nsubnet is used to estimate and compensate the temporal motion across frames,\nand the latter subnet is used to reduce the compression artifacts and enhance\nthe reconstruction quality of compressed video. Experimental results\ndemonstrate the effectiveness of our SDTS-based method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 09:42:43 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 05:30:06 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Meng", "Xiandong", ""], ["Deng", "Xuan", ""], ["Zhu", "Shuyuan", ""], ["Zeng", "Bing", ""]]}, {"id": "1901.09593", "submitter": "Vinay Kaushik", "authors": "Vinay Kaushik, Brejesh Lall", "title": "Fast Hierarchical Depth Map Computation from Stereo", "comments": "Submitted to International Conference on Pattern Recognition and\n  Artificial Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparity by Block Matching stereo is usually used in applications with\nlimited computational power in order to get depth estimates. However, the\nresearch on simple stereo methods has been lesser than the energy based\ncounterparts which promise a better quality depth map with more potential for\nfuture improvements. Semi-global-matching (SGM) methods offer good performance\nand easy implementation but suffer from the problem of very high memory\nfootprint because it's working on the full disparity space image. On the other\nhand, Block matching stereo needs much less memory. In this paper, we introduce\na novel multi-scale-hierarchical block-matching approach using a pyramidal\nvariant of depth and cost functions which drastically improves the results of\nstandard block matching stereo techniques while preserving the low memory\nfootprint and further reducing the complexity of standard block matching. We\ntested our new multi block matching scheme on the Middlebury stereo benchmark.\nFor the Middlebury benchmark we get results that are only slightly worse than\nstate of the art SGM implementations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 10:56:04 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kaushik", "Vinay", ""], ["Lall", "Brejesh", ""]]}, {"id": "1901.09614", "submitter": "Doyun Kim", "authors": "Doyun Kim, Kyoung-Young Kim, Sangsoo Ko, Sanghyuck Ha", "title": "A Simple Method to Reduce Off-chip Memory Accesses on Convolutional\n  Neural Networks", "comments": "9 pages, 10 figures, under review (by ICML2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For convolutional neural networks, a simple algorithm to reduce off-chip\nmemory accesses is proposed by maximally utilizing on-chip memory in a neural\nprocess unit. Especially, the algorithm provides an effective way to process a\nmodule which consists of multiple branches and a merge layer. For Inception-V3\non Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm\nmakes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59\n% reduction in the amount of feature-map data to be transferred from/to\noff-chip memory.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:43:25 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kim", "Doyun", ""], ["Kim", "Kyoung-Young", ""], ["Ko", "Sangsoo", ""], ["Ha", "Sanghyuck", ""]]}, {"id": "1901.09615", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Maryam Babaee, Stefan H\\\"ormann, Gerhard Rigoll", "title": "Convolutional Neural Networks with Layer Reuse", "comments": "Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolutional layer in a Convolutional Neural Network (CNN) consists of\nmany filters which apply convolution operation to the input, capture some\nspecial patterns and pass the result to the next layer. If the same patterns\nalso occur at the deeper layers of the network, why wouldn't the same\nconvolutional filters be used also in those layers? In this paper, we propose a\nCNN architecture, Layer Reuse Network (LruNet), where the convolutional layers\nare used repeatedly without the need of introducing new layers to get a better\nperformance. This approach introduces several advantages: (i) Considerable\namount of parameters are saved since we are reusing the layers instead of\nintroducing new layers, (ii) the Memory Access Cost (MAC) can be reduced since\nreused layer parameters can be fetched only once, (iii) the number of\nnonlinearities increases with layer reuse, and (iv) reused layers get gradient\nupdates from multiple parts of the network. The proposed approach is evaluated\non CIFAR-10, CIFAR-100 and Fashion-MNIST datasets for image classification\ntask, and layer reuse improves the performance by 5.14%, 5.85% and 2.29%,\nrespectively. The source code and pretrained models are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:45:50 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 10:32:50 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Babaee", "Maryam", ""], ["H\u00f6rmann", "Stefan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1901.09723", "submitter": "Rafael Reisenhofer", "authors": "Rafael Reisenhofer, Emily J. King", "title": "Edge, Ridge, and Blob Detection with Symmetric Molecules", "comments": "Accepted version. Supplemental materials available at\n  www.math.uni-bremen.de/cda/publications.html", "journal-ref": "SIAM J. Imaging Sci. 12(4), 2019, 1585-1626", "doi": "10.1137/19M1240861", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to the detection and characterization of edges,\nridges, and blobs in two-dimensional images which exploits the symmetry\nproperties of directionally sensitive analyzing functions in multiscale systems\nthat are constructed in the framework of alpha-molecules. The proposed feature\ndetectors are inspired by the notion of phase congruency, stable in the\npresence of noise, and by definition invariant to changes in contrast. We also\nshow how the behavior of coefficients corresponding to differently scaled and\noriented analyzing functions can be used to obtain a comprehensive\ncharacterization of the geometry of features in terms of local tangent\ndirections, widths, and heights. The accuracy and robustness of the proposed\nmeasures are validated and compared to various state-of-the-art algorithms in\nextensive numerical experiments in which we consider sets of clean and\ndistorted synthetic images that are associated with reliable ground truths. To\nfurther demonstrate the applicability, we show how the proposed ridge measure\ncan be used to detect and characterize blood vessels in digital retinal images\nand how the proposed blob measure can be applied to automatically count the\nnumber of cell colonies in a Petri dish.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 15:08:37 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 14:14:12 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 10:00:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Reisenhofer", "Rafael", ""], ["King", "Emily J.", ""]]}, {"id": "1901.09764", "submitter": "Jong Chul Ye", "authors": "Dongwook Lee, Junyoung Kim, Won-Jin Moon, Jong Chul Ye", "title": "CollaGAN : Collaborative GAN for Missing Image Data Imputation", "comments": "CVPR 2019 Camera Ready Version (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications requiring multiple inputs to obtain a desired output, if\nany of the input data is missing, it often introduces large amounts of bias.\nAlthough many techniques have been developed for imputing missing data, the\nimage imputation is still difficult due to complicated nature of natural\nimages. To address this problem, here we proposed a novel framework for missing\nimage data imputation, called Collaborative Generative Adversarial Network\n(CollaGAN). CollaGAN converts an image imputation problem to a multi-domain\nimages-to-image translation task so that a single generator and discriminator\nnetwork can successfully estimate the missing data using the remaining clean\ndata set. We demonstrate that CollaGAN produces the images with a higher visual\nquality compared to the existing competing approaches in various image\nimputation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:10:12 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 11:58:57 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 19:46:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lee", "Dongwook", ""], ["Kim", "Junyoung", ""], ["Moon", "Won-Jin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1901.09774", "submitter": "Hao Tang", "authors": "Hao Tang, Xinya Chen, Wei Wang, Dan Xu, Jason J. Corso, Nicu Sebe, Yan\n  Yan", "title": "Attribute-Guided Sketch Generation", "comments": "7 pages, 6 figures, accepted to FG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes are important since they provide a detailed description and\ndetermine the visual appearance of human faces. In this paper, we aim at\nconverting a face image to a sketch while simultaneously generating facial\nattributes. To this end, we propose a novel Attribute-Guided Sketch Generative\nAdversarial Network (ASGAN) which is an end-to-end framework and contains two\npairs of generators and discriminators, one of which is used to generate faces\nwith attributes while the other one is employed for image-to-sketch\ntranslation. The two generators form a W-shaped network (W-net) and they are\ntrained jointly with a weight-sharing constraint. Additionally, we also propose\ntwo novel discriminators, the residual one focusing on attribute generation and\nthe triplex one helping to generate realistic looking sketches. To validate our\nmodel, we have created a new large dataset with 8,804 images, named the\nAttribute Face Photo & Sketch (AFPS) dataset which is the first dataset\ncontaining attributes associated to face sketch images. The experimental\nresults demonstrate that the proposed network (i) generates more\nphoto-realistic faces with sharper facial attributes than baselines and (ii)\nhas good generalization capability on different generative tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:20:20 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 15:27:38 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Tang", "Hao", ""], ["Chen", "Xinya", ""], ["Wang", "Wei", ""], ["Xu", "Dan", ""], ["Corso", "Jason J.", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1901.09780", "submitter": "Milan Pultar", "authors": "Milan Pultar, Dmytro Mishkin, Ji\\v{r}\\'i Matas", "title": "Leveraging Outdoor Webcams for Local Descriptor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present AMOS Patches, a large set of image cut-outs, intended primarily\nfor the robustification of trainable local feature descriptors to illumination\nand appearance changes. Images contributing to AMOS Patches originate from the\nAMOS dataset of recordings from a large set of outdoor webcams.\n  The semiautomatic method used to generate AMOS Patches is described. It\nincludes camera selection, viewpoint clustering and patch selection. For\ntraining, we provide both the registered full source images as well as the\npatches.\n  A new descriptor, trained on the AMOS Patches and 6Brown datasets, is\nintroduced. It achieves state-of-the-art in matching under illumination changes\non standard benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:28:52 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 19:12:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Pultar", "Milan", ""], ["Mishkin", "Dmytro", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1901.09819", "submitter": "Fernando Pereira Dos Santos", "authors": "Fernando Pereira dos Santos, Leonardo Sampaio Ferraz Ribeiro, Moacir\n  Antonelli Ponti", "title": "Generalization of feature embeddings transferred from different video\n  anomaly detection domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting anomalous activity in video surveillance often involves using only\nnormal activity data in order to learn an accurate detector. Due to lack of\nannotated data for some specific target domain, one could employ existing data\nfrom a source domain to produce better predictions. Hence, transfer learning\npresents itself as an important tool. But how to analyze the resulting data\nspace? This paper investigates video anomaly detection, in particular feature\nembeddings of pre-trained CNN that can be used with non-fully supervised data.\nBy proposing novel cross-domain generalization measures, we study how source\nfeatures can generalize for different target video domains, as well as analyze\nunsupervised transfer learning. The proposed generalization measures are not\nonly a theorical approach, but show to be useful in practice as a way to\nunderstand which datasets can be used or transferred to describe video frames,\nwhich it is possible to better discriminate between normal and anomalous\nactivity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:10:47 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Santos", "Fernando Pereira dos", ""], ["Ribeiro", "Leonardo Sampaio Ferraz", ""], ["Ponti", "Moacir Antonelli", ""]]}, {"id": "1901.09822", "submitter": "Guanyu Cai", "authors": "Haifeng Shi, Guanyu Cai, Yuqin Wang, Shaohua Shang and Lianghua He", "title": "Virtual Conditional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained on multimodal image datasets, normal Generative Adversarial\nNetworks (GANs) are usually outperformed by class-conditional GANs and ensemble\nGANs, but conditional GANs is restricted to labeled datasets and ensemble GANs\nlack efficiency. We propose a novel GAN variant called virtual conditional GAN\n(vcGAN) which is not only an ensemble GAN with multiple generative paths while\nadding almost zero network parameters, but also a conditional GAN that can be\ntrained on unlabeled datasets without explicit clustering steps or objectives\nother than the adversary loss. Inside the vcGAN's generator, a learnable\n``analog-to-digital converter (ADC)\" module maps a slice of the inputted\nmultivariate Gaussian noise to discrete/digital noise (virtual label),\naccording to which a selector selects the corresponding generative path to\nproduce the sample. All the generative paths share the same decoder network\nwhile in each path the decoder network is fed with a concatenation of a\ndifferent pre-computed amplified one-hot vector and the inputted Gaussian\nnoise. We conducted a lot of experiments on several balanced/imbalanced image\ndatasets to demonstrate that vcGAN converges faster and achieves improved\nFrech\\'et Inception Distance (FID). In addition, we show the training byproduct\nthat the ADC in vcGAN learned the categorical probability of each mode and that\neach generative path generates samples of specific mode, which enables\nclass-conditional sampling. Codes are available at\n\\url{https://github.com/annonnymmouss/vcgan}\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 07:15:17 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Shi", "Haifeng", ""], ["Cai", "Guanyu", ""], ["Wang", "Yuqin", ""], ["Shang", "Shaohua", ""], ["He", "Lianghua", ""]]}, {"id": "1901.09854", "submitter": "Arkabandhu Chowdhury", "authors": "Indrani Bhattacharya, Arkabandhu Chowdhury, Vikas Raykar", "title": "Multi-modal dialog for browsing large visual catalogs using\n  exploration-exploitation paradigm in a joint embedding space", "comments": "10 pages including reference, 8 figures. First two authors are equal\n  contributors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-modal dialog system to assist online shoppers in visually\nbrowsing through large catalogs. Visual browsing is different from visual\nsearch in that it allows the user to explore the wide range of products in a\ncatalog, beyond the exact search matches. We focus on a slightly asymmetric\nversion of the complete multi-modal dialog where the system can understand both\ntext and image queries but responds only in images. We formulate our problem of\n\"showing $k$ best images to a user\" based on the dialog context so far, as\nsampling from a Gaussian Mixture Model in a high dimensional joint multi-modal\nembedding space, that embed both the text and the image queries. Our system\nremembers the context of the dialog and uses an exploration-exploitation\nparadigm to assist in visual browsing. We train and evaluate the system on a\nmulti-modal dialog dataset that we generate from large catalog data. Our\nexperiments are promising and show that the agent is capable of learning and\ncan display relevant results with an average cosine similarity of 0.85 to the\nground truth. Our preliminary human evaluation also corroborates the fact that\nsuch a multi-modal dialog system for visual browsing is well-received and is\ncapable of engaging human users.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:49:55 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 21:13:03 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Bhattacharya", "Indrani", ""], ["Chowdhury", "Arkabandhu", ""], ["Raykar", "Vikas", ""]]}, {"id": "1901.09878", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah\n  Hanif, Maurizio Martina, Muhammad Shafique", "title": "CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks preserve the hierarchical spatial relationships between\nobjects, and thereby bears a potential to surpass the performance of\ntraditional Convolutional Neural Networks (CNNs) in performing tasks like image\nclassification. A large body of work has explored adversarial examples for\nCNNs, but their effectiveness on Capsule Networks has not yet been well\nstudied. In our work, we perform an analysis to study the vulnerabilities in\nCapsule Networks to adversarial attacks. These perturbations, added to the test\ninputs, are small and imperceptible to humans, but can fool the network to\nmispredict. We propose a greedy algorithm to automatically generate targeted\nimperceptible adversarial examples in a black-box attack scenario. We show that\nthis kind of attacks, when applied to the German Traffic Sign Recognition\nBenchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind\nof adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the\noutcome, compared to the Capsule Networks to study differences in their\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:50:52 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 12:52:46 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Marchisio", "Alberto", ""], ["Nanfa", "Giorgio", ""], ["Khalid", "Faiq", ""], ["Hanif", "Muhammad Abdullah", ""], ["Martina", "Maurizio", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1901.09886", "submitter": "Tapabrata Chakraborti", "authors": "Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal", "title": "CoCoNet: A Collaborative Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end deep network for fine-grained visual categorization\ncalled Collaborative Convolutional Network (CoCoNet). The network uses a\ncollaborative layer after the convolutional layers to represent an image as an\noptimal weighted collaboration of features learned from training samples as a\nwhole rather than one at a time. This gives CoCoNet more power to encode the\nfine-grained nature of the data with limited samples. We perform a detailed\nstudy of the performance with 1-stage and 2-stage transfer learning. The\nablation study shows that the proposed method outperforms its constituent parts\nconsistently. CoCoNet also outperforms few state-of-the-art competing methods.\nExperiments have been performed on the fine-grained bird species classification\nproblem as a representative example, but the method may be applied to other\nsimilar tasks. We also introduce a new public dataset for fine-grained species\nrecognition, that of Indian endemic birds and have reported initial results on\nit.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:58:50 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 17:42:58 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 08:26:55 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2020 20:44:25 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chakraborti", "Tapabrata", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Pal", "Umapada", ""]]}, {"id": "1901.09891", "submitter": "Tao Hu", "authors": "Tao Hu, Honggang Qi, Qingming Huang, Yan Lu", "title": "See Better Before Looking Closer: Weakly Supervised Data Augmentation\n  Network for Fine-Grained Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is usually adopted to increase the amount of training data,\nprevent overfitting and improve the performance of deep models. However, in\npractice, random data augmentation, such as random image cropping, is\nlow-efficiency and might introduce many uncontrolled background noises. In this\npaper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to\nexplore the potential of data augmentation. Specifically, for each training\nimage, we first generate attention maps to represent the object's\ndiscriminative parts by weakly supervised learning. Next, we augment the image\nguided by these attention maps, including attention cropping and attention\ndropping. The proposed WS-DAN improves the classification accuracy in two\nfolds. In the first stage, images can be seen better since more discriminative\nparts' features will be extracted. In the second stage, attention regions\nprovide accurate location of object, which ensures our model to look at the\nobject closer and further improve the performance. Comprehensive experiments in\ncommon fine-grained visual classification datasets show that our WS-DAN\nsurpasses the state-of-the-art methods, which demonstrates its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 02:03:27 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 16:27:57 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hu", "Tao", ""], ["Qi", "Honggang", ""], ["Huang", "Qingming", ""], ["Lu", "Yan", ""]]}, {"id": "1901.09953", "submitter": "Xiao-Yang Liu", "authors": "Zihan Ding, Xiao-Yang Liu, Miao Yin, Linghe Kong", "title": "TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been successfully applied to many applications.\nHowever, existing works experience limitations when generating large images\n(the literature usually generates small images, e.g. 32 * 32 or 128 * 128). In\nthis paper, we propose a novel scheme, called deep tensor adversarial\ngenerative nets (TGAN), that generates large high-quality images by exploring\ntensor structures. Essentially, the adversarial process of TGAN takes place in\na tensor space. First, we impose tensor structures for concise image\nrepresentation, which is superior in capturing the pixel proximity information\nand the spatial patterns of elementary objects in images, over the\nvectorization preprocess in existing works. Secondly, we propose TGAN that\nintegrates deep convolutional generative adversarial networks and tensor\nsuper-resolution in a cascading manner, to generate high-quality images from\nrandom distributions. More specifically, we design a tensor super-resolution\nprocess that consists of tensor dictionary learning and tensor coefficients\nlearning. Finally, on three datasets, the proposed TGAN generates images with\nmore realistic textures, compared with state-of-the-art adversarial\nautoencoders. The size of the generated images is increased by over 8.5 times,\nnamely 374 * 374 in PASCAL2.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:29:39 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 23:07:37 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ding", "Zihan", ""], ["Liu", "Xiao-Yang", ""], ["Yin", "Miao", ""], ["Kong", "Linghe", ""]]}, {"id": "1901.09960", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kimin Lee and Mantas Mazeika", "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty", "comments": "ICML 2019. PyTorch code here:\n  https://github.com/hendrycks/pre-training Figure 3 updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  He et al. (2018) have called into question the utility of pre-training by\nshowing that training from scratch can often yield similar performance to\npre-training. We show that although pre-training may not improve performance on\ntraditional classification metrics, it improves model robustness and\nuncertainty estimates. Through extensive experiments on adversarial examples,\nlabel corruption, class imbalance, out-of-distribution detection, and\nconfidence calibration, we demonstrate large gains from pre-training and\ncomplementary effects with task-specific methods. We introduce adversarial\npre-training and show approximately a 10% absolute improvement over the\nprevious state-of-the-art in adversarial robustness. In some cases, using\npre-training without task-specific methods also surpasses the state-of-the-art,\nhighlighting the need for pre-training when evaluating future methods on\nrobustness and uncertainty tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:37:07 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 05:52:57 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 16:37:36 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2019 17:14:48 GMT"}, {"version": "v5", "created": "Sun, 20 Oct 2019 20:09:20 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hendrycks", "Dan", ""], ["Lee", "Kimin", ""], ["Mazeika", "Mantas", ""]]}, {"id": "1901.09971", "submitter": "Felix Stephenson", "authors": "Felix Stephenson, Toby Breckon and Ioannis Katramados", "title": "DeGraF-Flow: Extending DeGraF Features for accurate and efficient\n  sparse-to-dense optical flow estimation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern optical flow methods make use of salient scene feature points detected\nand matched within the scene as a basis for sparse-to-dense optical flow\nestimation. Current feature detectors however either give sparse, non uniform\npoint clouds (resulting in flow inaccuracies) or lack the efficiency for\nframe-rate real-time applications. In this work we use the novel Dense Gradient\nBased Features (DeGraF) as the input to a sparse-to-dense optical flow scheme.\nThis consists of three stages: 1) efficient detection of uniformly distributed\nDense Gradient Based Features (DeGraF); 2) feature tracking via robust local\noptical flow; and 3) edge preserving flow interpolation to recover overall\ndense optical flow. The tunable density and uniformity of DeGraF features yield\nsuperior dense optical flow estimation compared to other popular feature\ndetectors within this three stage pipeline. Furthermore, the comparable speed\nof feature detection also lends itself well to the aim of real-time optical\nflow recovery. Evaluation on established real-world benchmark datasets show\ntest performance in an autonomous vehicle setting where DeGraF-Flow shows\npromising results in terms of accuracy with competitive computational\nefficiency among non-GPU based methods, including a marked increase in speed\nover the conceptually similar EpicFlow approach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:55:40 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 16:42:22 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Stephenson", "Felix", ""], ["Breckon", "Toby", ""], ["Katramados", "Ioannis", ""]]}, {"id": "1901.09972", "submitter": "David Mac\\^edo", "authors": "Jefferson L. P. Lima, David Mac\\^edo, Cleber Zanchettin", "title": "Heartbeat Anomaly Detection using Adversarial Oversampling", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN.2019.8852242", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases are one of the most common causes of death in the\nworld. Prevention, knowledge of previous cases in the family, and early\ndetection is the best strategy to reduce this fact. Different machine learning\napproaches to automatic diagnostic are being proposed to this task. As in most\nhealth problems, the imbalance between examples and classes is predominant in\nthis problem and affects the performance of the automated solution. In this\npaper, we address the classification of heartbeats images in different\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\nNetwork for classification after using a InfoGAN architecture for generating\nsynthetic images to unbalanced classes. We call this proposal Adversarial\nOversampling and compare it with the classical oversampling methods as SMOTE,\nADASYN, and RandomOversampling. The results show that the proposed approach\nimproves the classifier performance for the minority classes without harming\nthe performance in the balanced classes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:55:42 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lima", "Jefferson L. P.", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.09983", "submitter": "Yibo Xu", "authors": "Yibo Xu, Weidi Liu and Kevin F. Kelly (Department of Electrical &\n  Computer Engineering, Rice University, Houston, USA)", "title": "Compressed Domain Image Classification Using a Dynamic-Rate Neural\n  Network", "comments": null, "journal-ref": "in IEEE Access, vol. 8, pp. 217711-217722, 2020", "doi": "10.1109/ACCESS.2020.3041807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed domain image classification performs classification directly on\ncompressive measurements acquired from the single-pixel camera, bypassing the\nimage reconstruction step. It is of great importance for extending high-speed\nobject detection and classification beyond the visible spectrum in a\ncost-effective manner especially for resource-limited platforms. Previous\nneural network methods require training a dedicated neural network for each\ndifferent measurement rate (MR), which is costly in computation and storage. In\nthis work, we develop an efficient training scheme that provides a neural\nnetwork with dynamic-rate property, where a single neural network is capable of\nclassifying over any MR within the range of interest with a given sensing\nmatrix. This training scheme uses only a few selected MRs for training and the\ntrained neural network is valid over the full range of MRs of interest. We\ndemonstrate the performance of the dynamic-rate neural network on datasets of\nMNIST, CIFAR-10, Fashion-MNIST, COIL-100, and show that it generates\napproximately equal performance at each MR as that of a single-rate neural\nnetwork valid only for one MR. Robustness to noise of the dynamic-rate model is\nalso demonstrated. The dynamic-rate training scheme can be regarded as a\ngeneral approach compatible with different types of sensing matrices, various\nneural network architectures, and is a valuable step towards wider adoption of\ncompressive inference techniques and other compressive sensing related tasks\nvia neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 20:16:24 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 16:23:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xu", "Yibo", "", "Department of Electrical &\n  Computer Engineering, Rice University, Houston, USA"], ["Liu", "Weidi", "", "Department of Electrical &\n  Computer Engineering, Rice University, Houston, USA"], ["Kelly", "Kevin F.", "", "Department of Electrical &\n  Computer Engineering, Rice University, Houston, USA"]]}, {"id": "1901.10034", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Alex Wong, Stefano Soatto", "title": "Dense Depth Posterior (DDP) from Single Image and Sparse Range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning system to infer the posterior distribution of a\ndense depth map associated with an image, by exploiting sparse range\nmeasurements, for instance from a lidar. While the lidar may provide a depth\nvalue for a small percentage of the pixels, we exploit regularities reflected\nin the training set to complete the map so as to have a probability over depth\nfor each pixel in the image. We exploit a Conditional Prior Network, that\nallows associating a probability to each depth value given an image, and\ncombine it with a likelihood term that uses the sparse measurements. Optionally\nwe can also exploit the availability of stereo during training, but in any case\nonly require a single image and a sparse point cloud at run-time. We test our\napproach on both unsupervised and supervised depth completion using the KITTI\nbenchmark, and improve the state-of-the-art in both.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 23:26:07 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 07:26:47 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yang", "Yanchao", ""], ["Wong", "Alex", ""], ["Soatto", "Stefano", ""]]}, {"id": "1901.10042", "submitter": "Quanshi Zhang", "authors": "Shipeng Xie, Da Chen, Rong Zhang, Hui Xue", "title": "Deep Features Analysis with Attention Networks", "comments": "In AAAI-19 Workshop on Network Interpretability for Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have recently draw lots of attention, as it\nconsistently produce impressive results in many computer vision tasks such as\nimage classification, object detection, etc. However, interpreting such model\nand show the reason why it performs quite well becomes a challenging question.\nIn this paper, we propose a novel method to interpret the neural network models\nwith attention mechanism. Inspired by the heatmap visualization, we analyze the\nrelation between classification accuracy with the attention based heatmap. An\nimproved attention based method is also included and illustrate that a better\nclassifier can be interpreted by the attention based heatmap.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 18:44:43 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Xie", "Shipeng", ""], ["Chen", "Da", ""], ["Zhang", "Rong", ""], ["Xue", "Hui", ""]]}, {"id": "1901.10077", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani, Parvaneh Saeedi", "title": "Cloud-Net: An end-to-end Cloud Detection Algorithm for Landsat 8 Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud detection in satellite images is an important first-step in many remote\nsensing applications. This problem is more challenging when only a limited\nnumber of spectral bands are available. To address this problem, a deep\nlearning-based algorithm is proposed in this paper. This algorithm consists of\na Fully Convolutional Network (FCN) that is trained by multiple patches of\nLandsat 8 images. This network, which is called Cloud-Net, is capable of\ncapturing global and local cloud features in an image using its convolutional\nblocks. Since the proposed method is an end-to-end solution, no complicated\npre-processing step is required. Our experimental results prove that the\nproposed method outperforms the state-of-the-art method over a benchmark\ndataset by 8.7\\% in Jaccard Index.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 02:49:50 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "1901.10100", "submitter": "Guangcong Wang", "authors": "Guangcong Wang and Jianhuang Lai and Zhenyu Xie and Xiaohua Xie", "title": "Discovering Underlying Person Structure Pattern with Relative Local\n  Distance for Person Re-identification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the underlying person structure for person re-identification (re-ID)\nis difficult due to diverse deformable poses, changeable camera views and\nimperfect person detectors. How to exploit underlying person structure\ninformation without extra annotations to improve the performance of person\nre-ID remains largely unexplored. To address this problem, we propose a novel\nRelative Local Distance (RLD) method that integrates a relative local distance\nconstraint into convolutional neural networks (CNNs) in an end-to-end way. It\nis the first time that the relative local constraint is proposed to guide the\nglobal feature representation learning. Specially, a relative local distance\nmatrix is computed by using feature maps and then regarded as a regularizer to\nguide CNNs to learn a structure-aware feature representation. With the\ndiscovered underlying person structure, the RLD method builds a bridge between\nthe global and local feature representation and thus improves the capacity of\nfeature representation for person re-ID. Furthermore, RLD also significantly\naccelerates deep network training compared with conventional methods. The\nexperimental results show the effectiveness of RLD on the CUHK03, Market-1501,\nand DukeMTMC-reID datasets. Code is available at\n\\url{https://github.com/Wanggcong/RLD_codes}.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 04:57:49 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Wang", "Guangcong", ""], ["Lai", "Jianhuang", ""], ["Xie", "Zhenyu", ""], ["Xie", "Xiaohua", ""]]}, {"id": "1901.10112", "submitter": "Hao Ren", "authors": "Hao Ren, Jianlin Su, Hong Lu", "title": "Evaluating Generalization Ability of Convolutional Neural Networks and\n  Capsule Networks for Image Classification via Top-2 Classification", "comments": "This paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is a challenging problem which aims to identify the\ncategory of object in the image. In recent years, deep Convolutional Neural\nNetworks (CNNs) have been applied to handle this task, and impressive\nimprovement has been achieved. However, some research showed the output of CNNs\ncan be easily altered by adding relatively small perturbations to the input\nimage, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are\nproposed, which can help eliminating this limitation. Experiments on MNIST\ndataset revealed that capsules can better characterize the features of object\nthan CNNs. But it's hard to find a suitable quantitative method to compare the\ngeneralization ability of CNNs and CapsNets. In this paper, we propose a new\nimage classification task called Top-2 classification to evaluate the\ngeneralization ability of CNNs and CapsNets. The models are trained on single\nlabel image samples same as the traditional image classification task. But in\nthe test stage, we randomly concatenate two test image samples which contain\ndifferent labels, and then use the trained models to predict the top-2 labels\non the unseen newly-created two label image samples. This task can provide us\nprecise quantitative results to compare the generalization ability of CNNs and\nCapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism\namong all capsules, it requires many parameters. To reduce the number of\nparameters, we introduce the Parameter-Sharing (PS) mechanism between capsules.\nExperiments on five widely used benchmark image datasets demonstrate the method\nsignificantly reduces the number of parameters, without losing the\neffectiveness of extracting features. Further, on the Top-2 classification\ntask, the proposed PS CapsNets obtain impressive higher accuracy compared to\nthe traditional CNNs and FC CapsNets by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 05:34:40 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 08:33:06 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ren", "Hao", ""], ["Su", "Jianlin", ""], ["Lu", "Hong", ""]]}, {"id": "1901.10124", "submitter": "Shubham Atreja", "authors": "Shanu Kumar, Shubham Atreja, Anjali Singh, Mohit Jain", "title": "Adversarial Adaptation of Scene Graph Models for Understanding Civic\n  Issues", "comments": "Accepted at WWW'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen engagement and technology usage are two emerging trends driven by\nsmart city initiatives. Governments around the world are adopting technology\nfor faster resolution of civic issues. Typically, citizens report issues, such\nas broken roads, garbage dumps, etc. through web portals and mobile apps, in\norder for the government authorities to take appropriate actions. Several\nmediums -- text, image, audio, video -- are used to report these issues.\nThrough a user study with 13 citizens and 3 authorities, we found that image is\nthe most preferred medium to report civic issues. However, analyzing civic\nissue related images is challenging for the authorities as it requires manual\neffort. Moreover, previous works have been limited to identifying a specific\nset of issues from images. In this work, given an image, we propose to generate\na Civic Issue Graph consisting of a set of objects and the semantic relations\nbetween them, which are representative of the underlying civic issue. We also\nrelease two multi-modal (text and images) datasets, that can help in further\nanalysis of civic issues from images. We present a novel approach for\nadversarial training of existing scene graph models that enables the use of\nscene graphs for new applications in the absence of any labelled training data.\nWe conduct several experiments to analyze the efficacy of our approach, and\nusing human evaluation, we establish the appropriateness of our model at\nrepresenting different civic issues.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 06:02:45 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kumar", "Shanu", ""], ["Atreja", "Shubham", ""], ["Singh", "Anjali", ""], ["Jain", "Mohit", ""]]}, {"id": "1901.10125", "submitter": "Jiwei Li", "authors": "Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li,\n  Qinghong Han, Xiaofei Sun, Jiwei Li", "title": "Glyce: Glyph-vectors for Chinese Character Representations", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is intuitive that NLP tasks for logographic languages like Chinese should\nbenefit from the use of the glyph information in those languages. However, due\nto the lack of rich pictographic evidence in glyphs and the weak generalization\nability of standard computer vision models on character data, an effective way\nto utilize the glyph information remains to be found. In this paper, we address\nthis gap by presenting Glyce, the glyph-vectors for Chinese character\nrepresentations. We make three major innovations: (1) We use historical Chinese\nscripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to\nenrich the pictographic evidence in characters; (2) We design CNN structures\n(called tianzege-CNN) tailored to Chinese character image processing; and (3)\nWe use image-classification as an auxiliary task in a multi-task learning setup\nto increase the model's ability to generalize. We show that glyph-based models\nare able to consistently outperform word/char ID-based models in a wide range\nof Chinese NLP tasks. We are able to set new state-of-the-art results for a\nvariety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair\nclassification, single sentence classification tasks, dependency parsing, and\nsemantic role labeling. For example, the proposed model achieves an F1 score of\n80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost\nperfect accuracy of 99.8\\% on the Fudan corpus for text classification. Code\nfound at https://github.com/ShannonAI/glyce.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 06:15:36 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:20:19 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 06:36:27 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 12:15:19 GMT"}, {"version": "v5", "created": "Thu, 21 May 2020 09:05:11 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Meng", "Yuxian", ""], ["Wu", "Wei", ""], ["Wang", "Fei", ""], ["Li", "Xiaoya", ""], ["Nie", "Ping", ""], ["Yin", "Fan", ""], ["Li", "Muyu", ""], ["Han", "Qinghong", ""], ["Sun", "Xiaofei", ""], ["Li", "Jiwei", ""]]}, {"id": "1901.10137", "submitter": "Yuru Chen", "authors": "Yuru Chen, Haitao Zhao, Zhengwei Hu", "title": "Attention-based Context Aggregation Network for Monocular Depth\n  Estimation", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is a traditional computer vision task, which plays a crucial\nrole in understanding 3D scene geometry. Recently,\ndeep-convolutional-neural-networks based methods have achieved promising\nresults in the monocular depth estimation field. Specifically, the framework\nthat combines the multi-scale features extracted by the dilated convolution\nbased block (atrous spatial pyramid pooling, ASPP) has gained the significant\nimprovement in the dense labeling task. However, the discretized and predefined\ndilation rates cannot capture the continuous context information that differs\nin diverse scenes and easily introduce the grid artifacts in depth estimation.\nIn this paper, we propose an attention-based context aggregation network (ACAN)\nto tackle these difficulties. Based on the self-attention model, ACAN\nadaptively learns the task-specific similarities between pixels to model the\ncontext information. First, we recast the monocular depth estimation as a dense\nlabeling multi-class classification problem. Then we propose a soft ordinal\ninference to transform the predicted probabilities to continuous depth values,\nwhich can reduce the discretization error (about 1% decrease in RMSE). Second,\nthe proposed ACAN aggregates both the image-level and pixel-level context\ninformation for depth estimation, where the former expresses the statistical\ncharacteristic of the whole image and the latter extracts the long-range\nspatial dependencies for each pixel. Third, for further reducing the\ninconsistency between the RGB image and depth map, we construct an attention\nloss to minimize their information entropy. We evaluate on public monocular\ndepth-estimation benchmark datasets (including NYU Depth V2, KITTI). The\nexperiments demonstrate the superiority of our proposed ACAN and achieve the\ncompetitive results with the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 07:01:20 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chen", "Yuru", ""], ["Zhao", "Haitao", ""], ["Hu", "Zhengwei", ""]]}, {"id": "1901.10143", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Enkelejda Kasneci", "title": "Learning to Validate the Quality of Detected Landmarks", "comments": "Will be published in the proceedings of the ICMV 2019 conference", "journal-ref": null, "doi": "10.1117/12.2559517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new loss function for the validation of image landmarks detected\nvia Convolutional Neural Networks (CNN). The network learns to estimate how\naccurate its landmark estimation is. This loss function is applicable to all\nregression-based location estimations and allows the exclusion of unreliable\nlandmarks from further processing. In addition, we formulate a novel batch\nbalancing approach which weights the importance of samples based on their\nproduced loss. This is done by computing a probability distribution mapping on\nan interval from which samples can be selected using a uniform random selection\nscheme. We conducted experiments on the 300W, AFLW, and WFLW facial landmark\ndatasets. In the first experiments, the influence of our batch balancing\napproach is evaluated by comparing it against uniform sampling. In addition, we\nevaluated the impact of the validation loss on the landmark accuracy based on\nuniform sampling. The last experiments evaluate the correlation of the\nvalidation signal with the landmark accuracy. All experiments were performed\nfor all three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 07:19:44 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 08:45:56 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 06:49:47 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1901.10163", "submitter": "Yan Hao", "authors": "Yutong Xie, Haiyang Wang, Yan Hao, Zihao Xu", "title": "Visual Rhythm Prediction with Feature-Aligning Network", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data-driven visual rhythm prediction method,\nwhich overcomes the previous works' deficiency that predictions are made\nprimarily by human-crafted hard rules. In our approach, we first extract\nfeatures including original frames and their residuals, optical flow, scene\nchange, and body pose. These visual features will be next taken into an\nend-to-end neural network as inputs. Here we observe that there are some slight\nmisaligning between features over the timeline and assume that this is due to\nthe distinctions between how different features are computed. To solve this\nproblem, the extracted features are aligned by an elaborately designed layer,\nwhich can also be applied to other models suffering from mismatched features,\nand boost performance. Then these aligned features are fed into sequence\nlabeling layers implemented with BiLSTM and CRF to predict the onsets. Due to\nthe lack of existing public training and evaluation set, we experiment on a\ndataset constructed by ourselves based on professionally edited Music Videos\n(MVs), and the F1 score of our approach reaches 79.6.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:29:08 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Xie", "Yutong", ""], ["Wang", "Haiyang", ""], ["Hao", "Yan", ""], ["Xu", "Zihao", ""]]}, {"id": "1901.10170", "submitter": "Aarno Vuola", "authors": "Aarno Oskar Vuola, Saad Ullah Akram, Juho Kannala", "title": "Mask-RCNN and U-net Ensembled for Nuclei Segmentation", "comments": "To appear in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei segmentation is both an important and in some ways ideal task for\nmodern computer vision methods, e.g. convolutional neural networks. While\nrecent developments in theory and open-source software have made these tools\neasier to implement, expert knowledge is still required to choose the right\nmodel architecture and training setup. We compare two popular segmentation\nframeworks, U-Net and Mask-RCNN in the nuclei segmentation task and find that\nthey have different strengths and failures. To get the best of both worlds, we\ndevelop an ensemble model to combine their predictions that can outperform both\nmodels by a significant margin and should be considered when aiming for best\nnuclei segmentation performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:40:16 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Vuola", "Aarno Oskar", ""], ["Akram", "Saad Ullah", ""], ["Kannala", "Juho", ""]]}, {"id": "1901.10172", "submitter": "Peizhao Li", "authors": "Peizhao Li, Yanjing Li, Xiaolong Jiang, Xiantong Zhen", "title": "Two-Stream Multi-Task Network for Fashion Recognition", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a two-stream multi-task network for fashion\nrecognition. This task is challenging as fashion clothing always contain\nmultiple attributes, which need to be predicted simultaneously for real-time\nindustrial systems. To handle these challenges, we formulate fashion\nrecognition into a multi-task learning problem, including landmark detection,\ncategory and attribute classifications, and solve it with the proposed deep\nconvolutional neural network. We design two knowledge sharing strategies which\nenable information transfer between tasks and improve the overall performance.\nThe proposed model achieves state-of-the-art results on large-scale fashion\ndataset comparing to the existing methods, which demonstrates its great\neffectiveness and superiority for fashion recognition.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:42:16 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 17:11:54 GMT"}, {"version": "v3", "created": "Sun, 12 May 2019 10:04:17 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Li", "Peizhao", ""], ["Li", "Yanjing", ""], ["Jiang", "Xiaolong", ""], ["Zhen", "Xiantong", ""]]}, {"id": "1901.10177", "submitter": "Hong-Xing Yu", "authors": "Hong-Xing Yu, Ancong Wu, Wei-Shi Zheng", "title": "Unsupervised Person Re-identification by Deep Asymmetric Metric\n  Embedding", "comments": "To appear in TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2886878", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims to match identities across\nnon-overlapping camera views. Researchers have proposed many supervised Re-ID\nmodels which require quantities of cross-view pairwise labelled data. This\nlimits their scalabilities to many applications where a large amount of data\nfrom multiple disjoint camera views is available but unlabelled. Although some\nunsupervised Re-ID models have been proposed to address the scalability\nproblem, they often suffer from the view-specific bias problem which is caused\nby dramatic variances across different camera views, e.g., different\nillumination, viewpoints and occlusion. The dramatic variances induce specific\nfeature distortions in different camera views, which can be very disturbing in\nfinding cross-view discriminative information for Re-ID in the unsupervised\nscenarios, since no label information is available to help alleviate the bias.\nWe propose to explicitly address this problem by learning an unsupervised\nasymmetric distance metric based on cross-view clustering. The asymmetric\ndistance metric allows specific feature transformations for each camera view to\ntackle the specific feature distortions. We then design a novel unsupervised\nloss function to embed the asymmetric metric into a deep neural network, and\ntherefore develop a novel unsupervised deep framework named the DEep\nClustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL\njointly learns the feature representation and the unsupervised asymmetric\nmetric. DECAMEL learns a compact cross-view cluster structure of Re-ID data,\nand thus help alleviate the view-specific bias and facilitate mining the\npotential cross-view discriminative information for unsupervised Re-ID.\nExtensive experiments on seven benchmark datasets whose sizes span several\norders show the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:49:26 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Yu", "Hong-Xing", ""], ["Wu", "Ancong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1901.10178", "submitter": "Thomas Lacombe", "authors": "Pierre Nagorny (SYMME), Thomas Lacombe (SYMME), Hugues Favreliere\n  (SYMME), Maurice Pillet (SYMME), Eric Pairel (SYMME), Ronan Le Goff (IPC),\n  Marlene Wali (IPC), Jerome Loureaux (IPC), Patrice Kiener", "title": "Generative Adversarial Networks for geometric surfaces prediction in\n  injection molding", "comments": "IEEE. 2018 IEEE International Conference on Industrial Technology\n  (ICIT), Feb 2018, Lyon, France, http://www.icit2018.org", "journal-ref": "IEEE, 2018 IEEE International Conference on Industrial Technology\n  (ICIT), pp.1514-1519, 2018", "doi": "10.1109/ICIT.2018.8352405", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometrical and appearance quality requirements set the limits of the current\nindustrial performance in injection molding. To guarantee the product's\nquality, it is necessary to adjust the process settings in a closed loop. Those\nadjustments cannot rely on the final quality because a part takes days to be\ngeometrically stable. Thus, the final part geometry must be predicted from\nmeasurements on hot parts. In this paper, we use recent success of Generative\nAdversarial Networks (GAN) with the pix2pix network architecture to predict the\nfinal part geometry, using only hot parts thermographic images, measured right\nafter production. Our dataset is really small, and the GAN learns to translate\nthermography to geometry. We firstly study prediction performances using\ndifferent image similarity comparison algorithms. Moreover, we introduce the\ninnovative use of Discrete Modal Decomposition (DMD) to analyze network\npredictions. The DMD is a geometrical parameterization technique using a modal\nspace projection to geometrically describe surfaces. We study GAN performances\nto retrieve geometrical parameterization of surfaces.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:49:59 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Nagorny", "Pierre", "", "SYMME"], ["Lacombe", "Thomas", "", "SYMME"], ["Favreliere", "Hugues", "", "SYMME"], ["Pillet", "Maurice", "", "SYMME"], ["Pairel", "Eric", "", "SYMME"], ["Goff", "Ronan Le", "", "IPC"], ["Wali", "Marlene", "", "IPC"], ["Loureaux", "Jerome", "", "IPC"], ["Kiener", "Patrice", ""]]}, {"id": "1901.10208", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio and Manuel Lopez-Antequera and Nicolai Petkov", "title": "A Push-Pull Layer Improves Robustness of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new layer in Convolutional Neural Networks (CNNs) to increase\ntheir robustness to several types of noise perturbations of the input images.\nWe call this a push-pull layer and compute its response as the combination of\ntwo half-wave rectified convolutions, with kernels of opposite polarity. It is\nbased on a biologically-motivated non-linear model of certain neurons in the\nvisual system that exhibit a response suppression phenomenon, known as\npush-pull inhibition. We validate our method by substituting the first\nconvolutional layer of the LeNet-5 and WideResNet architectures with our\npush-pull layer. We train the networks on nonperturbed training images from the\nMNIST, CIFAR-10 and CIFAR-100 data sets, and test on images perturbed by noise\nthat is unseen by the training process. We demonstrate that our push-pull\nlayers contribute to a considerable improvement in robustness of classification\nof images perturbed by noise, while maintaining state-of-the-art performance on\nthe original image classification task.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 10:42:04 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Strisciuglio", "Nicola", ""], ["Lopez-Antequera", "Manuel", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1901.10231", "submitter": "Gururaj Awate", "authors": "GuruRaj Awate", "title": "Detection of Alzheimers Disease from MRI using Convolutional Neural\n  Networks, Exploring Transfer Learning And BellCNN", "comments": "IEEE Conference, Intended for non-technical audiences", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for automatic diagnosis of certain diseases from medical\nimages that could help medical practitioners for further assessment towards\ntreating the illness. Alzheimers disease is a good example of a disease that is\noften misdiagnosed. Alzheimers disease (Hear after referred to as AD), is\ncaused by atrophy of certain brain regions and by brain cell death and is the\nleading cause of dementia and memory loss [1]. MRI scans reveal this\ninformation but atrophied regions are different for different individuals which\nmakes the diagnosis a bit more trickier and often gets misdiagnosed [1, 13]. We\nbelieve that our approach to this particular problem would improve the\nassessment quality by pre-flagging the images which are more likely to have AD.\nWe propose two solutions to this; one with transfer learning [9] and other by\nBellCNN [14], a custom made Convolutional Neural Network (Hear after referred\nto as CNN). Advantages and disadvantages of each approach will also be\ndiscussed in their respective sections. The dataset used for this project is\nprovided by Open Access Series of Imaging Studies (Hear after referred to as\nOASIS) [2, 3, 4], which contains over 400 subjects, 100 of whom have mild to\nsevere dementia. The dataset has labeled these subjects by two standards of\ndiagnosis; MiniMental State Examination (Hear after referred to as MMSE) and\nClinical Dementia Rating (Hear after referred to as CDR). These are some of the\ngeneral tools and concepts which are prerequisites to our solution; CNN [5, 6],\nNeural Networks [10] (Hear after referred to as NN), Anaconda bundle for\npython, Regression, Tensorflow [7]. Keywords: Alzheimers Disease, Convolutional\nNeural Network, BellCNN, Image Recognition, Machine Learning, MRI, OASIS,\nTensorflow\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:32:06 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Awate", "GuruRaj", ""]]}, {"id": "1901.10233", "submitter": "Denis Volkhonskiy", "authors": "Denis Volkhonskiy, Ekaterina Muravleva, Oleg Sudakov, Denis Orlov,\n  Boris Belozerov, Evgeny Burnaev, Dmitry Koroteev", "title": "Reconstruction of 3D Porous Media From 2D Slices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many branches of earth sciences, the problem of rock study on the\nmicro-level arises. However, a significant number of representative samples is\nnot always feasible. Thus the problem of the generation of samples with similar\nproperties becomes actual. In this paper, we propose a novel deep learning\narchitecture for three-dimensional porous media reconstruction from\ntwo-dimensional slices. We fit a distribution on all possible three-dimensional\nstructures of a specific type based on the given dataset of samples. Then,\ngiven partial information (central slices) we recover the three-dimensional\nstructure around such slices as the most probable one according to that\nconstructed distribution. Technically, we implement this in the form of a deep\nneural network with encoder, generator and discriminator modules. Numerical\nexperiments show that this method provides good reconstruction in terms of\nMinkowski functionals.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:41:39 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 10:53:32 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 12:27:06 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Muravleva", "Ekaterina", ""], ["Sudakov", "Oleg", ""], ["Orlov", "Denis", ""], ["Belozerov", "Boris", ""], ["Burnaev", "Evgeny", ""], ["Koroteev", "Dmitry", ""]]}, {"id": "1901.10237", "submitter": "Hai Duong Nguyen", "authors": "Hai-Duong Nguyen, Soo-Hyung Kim", "title": "Automatic Whole-body Bone Age Assessment Using Deep Hierarchical\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age assessment gives us evidence to analyze the children growth status\nand the rejuvenation involved chronological and biological ages. All the\nprevious works consider left-hand X-ray image of a child in their works. In\nthis paper, we carry out a study on estimating human age using whole-body bone\nCT images and a novel convolutional neural network. Our model with additional\nconnections shows an effective way to generate a massive number of vital\nfeatures while reducing overfitting influence on small training data in the\nmedical image analysis research area. A dataset and a comparison with common\ndeep architectures will be provided for future research in this field.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:53:30 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Nguyen", "Hai-Duong", ""], ["Kim", "Soo-Hyung", ""]]}, {"id": "1901.10244", "submitter": "James Clough", "authors": "James R. Clough, Ilkay Oksuz, Nicholas Byrne, Julia A. Schnabel,\n  Andrew P. King", "title": "Explicit topological priors for deep-learning based image segmentation\n  using persistent homology", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to explicitly incorporate topological prior\nknowledge into deep learning based segmentation, which is, to our knowledge,\nthe first work to do so. Our method uses the concept of persistent homology, a\ntool from topological data analysis, to capture high-level topological\ncharacteristics of segmentation results in a way which is differentiable with\nrespect to the pixelwise probability of being assigned to a given class. The\ntopological prior knowledge consists of the sequence of desired Betti numbers\nof the segmentation. As a proof-of-concept we demonstrate our approach by\napplying it to the problem of left-ventricle segmentation of cardiac MR images\nof 500 subjects from the UK Biobank dataset, where we show that it improves\nsegmentation performance in terms of topological correctness without\nsacrificing pixelwise accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:11:55 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Clough", "James R.", ""], ["Oksuz", "Ilkay", ""], ["Byrne", "Nicholas", ""], ["Schnabel", "Julia A.", ""], ["King", "Andrew P.", ""]]}, {"id": "1901.10254", "submitter": "Xun Xu", "authors": "Xun Xu, Loong-Fah Cheong, Zhuwen Li", "title": "Learning for Multi-Model and Multi-Type Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-model fitting has been extensively studied from the random sampling and\nclustering perspectives. Most assume that only a single type/class of model is\npresent and their generalizations to fitting multiple types of\nmodels/structures simultaneously are non-trivial. The inherent challenges\ninclude choice of types and numbers of models, sampling imbalance and parameter\ntuning, all of which render conventional approaches ineffective. In this work,\nwe formulate the multi-model multi-type fitting problem as one of learning deep\nfeature embedding that is clustering-friendly. In other words, points of the\nsame clusters are embedded closer together through the network. For inference,\nwe apply K-means to cluster the data in the embedded feature space and model\nselection is enabled by analyzing the K-means residuals. Experiments are\ncarried out on both synthetic and real world multi-type fitting datasets,\nproducing state-of-the-art results. Comparisons are also made on single-type\nmulti-model fitting tasks with promising results as well.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:40:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Xu", "Xun", ""], ["Cheong", "Loong-Fah", ""], ["Li", "Zhuwen", ""]]}, {"id": "1901.10265", "submitter": "Vijay Keswani", "authors": "L. Elisa Celis and Vijay Keswani", "title": "Implicit Diversity in Image Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that the people depicted in image search results tend to\nbe of majority groups with respect to socially salient attributes. This skew\ngoes beyond that which already exists in the world - e.g., Kay et al. showed\nthat although 28% of CEOs in US are women, only 10% of the top 100 results for\nCEO in Google Image Search are women. Most existing approaches to correct for\nthis kind of bias assume that the images of people include socially salient\nattribute labels. However, such labels are often unknown. Further, using\nautomated techniques to infer these labels may often not be possible within\nacceptable accuracy ranges, and may not be desirable due to the additional\nbiases this process could incur. We develop a novel approach that takes as\ninput a visibly diverse control set of images and uses this set to select a set\nof images of people in response to a query. The goal is to have a resulting set\nthat is more visibly diverse in a manner that emulates the diversity depicted\nin the control set. Importantly, this approach does not require images to be\nlabelled at any point; effectively, it gives a way to implicitly diversify the\nset of images selected. We provide two variants of our approach: the first is a\nmodification of the MMR algorithm to incorporate the diversity scores, and\nsecond is a more efficient variant that does not consider within-list\nredundancy. We evaluate these approaches empirically on two datasets 1) a new\ndataset containing top Google image results for 96 occupations, for which we\nevaluate gender and skin-tone diversity with respect to occupations and 2) the\nCelebA dataset for which we evaluate gender diversity with respect to facial\nfeatures. Our approaches produce image sets that significantly improve the\nvisible diversity of the results, compared to current Google search and other\ndiverse image summarization algorithms, at a minimal cost to accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:13:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 14:45:53 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 21:00:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Celis", "L. Elisa", ""], ["Keswani", "Vijay", ""]]}, {"id": "1901.10271", "submitter": "Jakob Wasserthal", "authors": "Jakob Wasserthal, Peter Neher, Dusan Hirjak, Klaus H. Maier-Hein", "title": "Combined tract segmentation and orientation mapping for bundle-specific\n  tractography", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2019.101559", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the major white matter tracts are of great interest to numerous studies\nin neuroscience and medicine, their manual dissection in larger cohorts from\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\nhard to reproduce. In previous work we presented tract orientation mapping\n(TOM) as a novel concept for bundle-specific tractography. It is based on a\nlearned mapping from the original fiber orientation distribution function (FOD)\npeaks to tract specific peaks, called tract orientation maps. Each tract\norientation map represents the voxel-wise principal orientation of one tract.\nHere, we present an extension of this approach that combines TOM with accurate\nsegmentations of the tract outline and its start and end region. We also\nintroduce a custom probabilistic tracking algorithm that samples from a\nGaussian distribution with fixed standard deviation centered on each peak thus\nenabling more complete trackings on the tract orientation maps than\ndeterministic tracking. These extensions enable the automatic creation of\nbundle-specific tractograms with previously unseen accuracy. We show for 72\ndifferent bundles on high quality, low quality and phantom data that our\napproach runs faster and produces more accurate bundle-specific tractograms\nthan 7 state of the art benchmark methods while avoiding cumbersome processing\nsteps like whole brain tractography, non-linear registration, clustering or\nmanual dissection. Moreover, we show on 17 datasets that our approach\ngeneralizes well to datasets acquired with different scanners and settings as\nwell as with pathologies. The code of our method is openly available at\nhttps://github.com/MIC-DKFZ/TractSeg.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:25:50 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 13:02:18 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Wasserthal", "Jakob", ""], ["Neher", "Peter", ""], ["Hirjak", "Dusan", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1901.10277", "submitter": "Samuli Laine", "authors": "Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila", "title": "High-Quality Self-Supervised Deep Image Denoising", "comments": "NeurIPS 2019 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:37:16 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 09:59:59 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 12:36:21 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Laine", "Samuli", ""], ["Karras", "Tero", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1901.10312", "submitter": "Aythami Morales", "authors": "Alejandro Acien, Aythami Morales, Ruben Vera-Rodriguez, and Julian\n  Fierrez", "title": "MultiLock: Mobile Active Authentication based on Multiple Biometric and\n  Behavioral Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate mobile active authentication based on an ensemble\nof biometrics and behavior-based profiling signals. We consider seven different\ndata channels and their combination. Touch dynamics (touch gestures and\nkeystroking), accelerometer, gyroscope, WiFi, GPS location and app usage are\nall collected during human-mobile interaction to authenticate the users. We\nevaluate two approaches: one-time authentication and active authentication. In\none-time authentication, we employ the information of all channels available\nduring one session. For active authentication we take advantage of mobile user\nbehavior across multiple sessions by updating a confidence value of the\nauthentication score. Our experiments are conducted on the semi-uncontrolled\nUMDAA-02 database. This database comprises smartphone sensor signals acquired\nduring natural human-mobile interaction. Our results show that different traits\ncan be complementary and multimodal systems clearly increase the performance\nwith accuracies ranging from 82.2% to 97.1% depending on the authentication\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 14:39:37 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "1901.10323", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Ahmet Gunduz, Neslihan Kose, Gerhard Rigoll", "title": "Real-time Hand Gesture Detection and Classification Using Convolutional\n  Neural Networks", "comments": "Published at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2019) - Best student paper award! -", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time recognition of dynamic hand gestures from video streams is a\nchallenging task since (i) there is no indication when a gesture starts and\nends in the video, (ii) performed gestures should only be recognized once, and\n(iii) the entire architecture should be designed considering the memory and\npower budget. In this work, we address these challenges by proposing a\nhierarchical structure enabling offline-working convolutional neural network\n(CNN) architectures to operate online efficiently by using sliding window\napproach. The proposed architecture consists of two models: (1) A detector\nwhich is a lightweight CNN architecture to detect gestures and (2) a classifier\nwhich is a deep CNN to classify the detected gestures. In order to evaluate the\nsingle-time activations of the detected gestures, we propose to use Levenshtein\ndistance as an evaluation metric since it can measure misclassifications,\nmultiple detections, and missing detections at the same time. We evaluate our\narchitecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic\nHand Gesture Datasets - which require temporal detection and classification of\nthe performed hand gestures. ResNeXt-101 model, which is used as a classifier,\nachieves the state-of-the-art offline classification accuracy of 94.04% and\n83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In\nreal-time detection and classification, we obtain considerable early detections\nwhile achieving performances close to offline operation. The codes and\npretrained models used in this work are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 14:52:51 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 13:17:34 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 08:14:35 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Gunduz", "Ahmet", ""], ["Kose", "Neslihan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1901.10332", "submitter": "Zhuoran Liu", "authors": "Zhuoran Liu, Zhengyu Zhao, Martha Larson", "title": "Who's Afraid of Adversarial Queries? The Impact of Image Modifications\n  on Content-based Image Retrieval", "comments": "To appear at the ACM International Conference on Multimedia Retrieval\n  (ICMR 2019). Our code is available at https://github.com/liuzrcc/PIRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An adversarial query is an image that has been modified to disrupt\ncontent-based image retrieval (CBIR) while appearing nearly untouched to the\nhuman eye. This paper presents an analysis of adversarial queries for CBIR\nbased on neural, local, and global features. We introduce an innovative neural\nimage perturbation approach, called Perturbations for Image Retrieval Error\n(PIRE), that is capable of blocking neural-feature-based CBIR. PIRE differs\nsignificantly from existing approaches that create images adversarial with\nrespect to CNN classifiers because it is unsupervised, i.e., it needs no\nlabelled data from the data set to which it is applied. Our experimental\nanalysis demonstrates the surprising effectiveness of PIRE in blocking CBIR,\nand also covers aspects of PIRE that must be taken into account in practical\nsettings, including saving images, image quality and leaking adversarial\nqueries into the background collection. Our experiments also compare PIRE (a\nneural approach) with existing keypoint removal and injection approaches (which\nmodify local features). Finally, we discuss the challenges that face multimedia\nresearchers in the future study of adversarial queries.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:09:14 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 11:28:35 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:57:18 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Liu", "Zhuoran", ""], ["Zhao", "Zhengyu", ""], ["Larson", "Martha", ""]]}, {"id": "1901.10345", "submitter": "Md Sahidullah", "authors": "Arnab Poddar, Md Sahidullah and Goutam Saha", "title": "Quality Measures for Speaker Verification with Short Utterances", "comments": "Accepted for publication in Digital Signal Processing: A Review\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performances of the automatic speaker verification (ASV) systems degrade\ndue to the reduction in the amount of speech used for enrollment and\nverification. Combining multiple systems based on different features and\nclassifiers considerably reduces speaker verification error rate with short\nutterances. This work attempts to incorporate supplementary information during\nthe system combination process. We use quality of the estimated model\nparameters as supplementary information. We introduce a class of novel quality\nmeasures formulated using the zero-order sufficient statistics used during the\ni-vector extraction process. We have used the proposed quality measures as side\ninformation for combining ASV systems based on Gaussian mixture model-universal\nbackground model (GMM-UBM) and i-vector. The proposed methods demonstrate\nconsiderable improvement in speaker recognition performance on NIST SRE\ncorpora, especially in short duration conditions. We have also observed\nimprovement over existing systems based on different duration-based quality\nmeasures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:45:35 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 12:33:02 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Poddar", "Arnab", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1901.10364", "submitter": "Federico Landi", "authors": "Federico Landi, Cees G. M. Snoek and Rita Cucchiara", "title": "Anomaly Locality in Video Surveillance", "comments": "Submitted to International Conference on Image Processing, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives for the detection of real-world anomalies such as\nburglaries and assaults in surveillance videos. Although anomalies are\ngenerally local, as they happen in a limited portion of the frame, none of the\nprevious works on the subject has ever studied the contribution of locality. In\nthis work, we explore the impact of considering spatiotemporal tubes instead of\nwhole-frame video segments. For this purpose, we enrich existing surveillance\nvideos with spatial and temporal annotations: it is the first dataset for\nanomaly detection with bounding box supervision in both its train and test set.\nOur experiments show that a network trained with spatiotemporal tubes performs\nbetter than its analogous model trained with whole-frame videos. In addition,\nwe discover that the locality is robust to different kinds of errors in the\ntube extraction phase at test time. Finally, we demonstrate that our network\ncan provide spatiotemporal proposals for unseen surveillance videos leveraging\nonly video-level labels. By doing, we enlarge our spatiotemporal anomaly\ndataset without the need for further human labeling.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 16:30:17 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Landi", "Federico", ""], ["Snoek", "Cees G. M.", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1901.10415", "submitter": "Juncai He", "authors": "Juncai He and Jinchao Xu", "title": "MgNet: A Unified Framework of Multigrid and Convolutional Neural Network", "comments": "30 pages", "journal-ref": "Sci. China Math. 62 (2019) 1331-1354", "doi": "10.1007/s11425-019-9547-2", "report-no": null, "categories": "cs.CV cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified model, known as MgNet, that simultaneously recovers some\nconvolutional neural networks (CNN) for image classification and multigrid (MG)\nmethods for solving discretized partial differential equations (PDEs). This\nmodel is based on close connections that we have observed and uncovered between\nthe CNN and MG methodologies. For example, pooling operation and feature\nextraction in CNN correspond directly to restriction operation and iterative\nsmoothers in MG, respectively. As the solution space is often the dual of the\ndata space in PDEs, the analogous concept of feature space and data space\n(which are dual to each other) is introduced in CNN. With such connections and\nnew concept in the unified model, the function of various convolution\noperations and pooling used in CNN can be better understood. As a result,\nmodified CNN models (with fewer weights and hyper parameters) are developed\nthat exhibit competitive and sometimes better performance in comparison with\nexisting CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 17:30:59 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 20:49:32 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["He", "Juncai", ""], ["Xu", "Jinchao", ""]]}, {"id": "1901.10422", "submitter": "Anna Khoreva", "authors": "Dan Zhang, Anna Khoreva", "title": "Progressive Augmentation of GANs", "comments": "Accepted at NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of Generative Adversarial Networks (GANs) is notoriously fragile,\nrequiring to maintain a careful balance between the generator and the\ndiscriminator in order to perform well. To mitigate this issue we introduce a\nnew regularization technique - progressive augmentation of GANs (PA-GAN). The\nkey idea is to gradually increase the task difficulty of the discriminator by\nprogressively augmenting its input or feature space, thus enabling continuous\nlearning of the generator. We show that the proposed progressive augmentation\npreserves the original GAN objective, does not compromise the discriminator's\noptimality and encourages a healthy competition between the generator and\ndiscriminator, leading to the better-performing generator. We experimentally\ndemonstrate the effectiveness of PA-GAN across different architectures and on\nmultiple benchmarks for the image synthesis task, on average achieving ~3 point\nimprovement of the FID score.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 17:47:39 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 19:00:39 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 15:52:31 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Dan", ""], ["Khoreva", "Anna", ""]]}, {"id": "1901.10431", "submitter": "Jus Lozej", "authors": "Ju\\v{s} Lozej, Dejan \\v{S}tepec, Vitomir \\v{S}truc and Peter Peer", "title": "Influence of segmentation on deep iris recognition performance", "comments": "6 pages, 3 figures, 3 tables, submitted to IWBF 2019", "journal-ref": null, "doi": "10.1109/IWBF.2019.8739225", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the rise of deep learning in numerous areas of computer vision and\nimage processing, iris recognition has not benefited considerably from these\ntrends so far. Most of the existing research on deep iris recognition is\nfocused on new models for generating discriminative and robust iris\nrepresentations and relies on methodologies akin to traditional iris\nrecognition pipelines. Hence, the proposed models do not approach iris\nrecognition in an end-to-end manner, but rather use standard heuristic iris\nsegmentation (and unwrapping) techniques to produce normalized inputs for the\ndeep learning models. However, because deep learning is able to model very\ncomplex data distributions and nonlinear data changes, an obvious question\narises. How important is the use of traditional segmentation methods in a deep\nlearning setting? To answer this question, we present in this paper an\nempirical analysis of the impact of iris segmentation on the performance of\ndeep learning models using a simple two stage pipeline consisting of a\nsegmentation and a recognition step. We evaluate how the accuracy of\nsegmentation influences recognition performance but also examine if\nsegmentation is needed at all. We use the CASIA Thousand and SBVPI datasets for\nthe experiments and report several interesting findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:01:42 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 18:25:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lozej", "Ju\u0161", ""], ["\u0160tepec", "Dejan", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1901.10436", "submitter": "Michele Merler", "authors": "Michele Merler and Nalini Ratha and Rogerio S. Feris and John R. Smith", "title": "Diversity in Faces", "comments": "Updated statistics after slight modification to dataset due to\n  inactive links and deletions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a long standing challenge in the field of Artificial\nIntelligence (AI). The goal is to create systems that accurately detect,\nrecognize, verify, and understand human faces. There are significant technical\nhurdles in making these systems accurate, particularly in unconstrained\nsettings due to confounding factors related to pose, resolution, illumination,\nocclusion, and viewpoint. However, with recent advances in neural networks,\nface recognition has achieved unprecedented accuracy, largely built on\ndata-driven deep learning methods. While this is encouraging, a critical aspect\nthat is limiting facial recognition accuracy and fairness is inherent facial\ndiversity. Every face is different. Every face reflects something unique about\nus. Aspects of our heritage - including race, ethnicity, culture, geography -\nand our individual identify - age, gender, and other visible manifestations of\nself-expression, are reflected in our faces. We expect face recognition to work\nequally accurately for every face. Face recognition needs to be fair. As we\nrely on data-driven methods to create face recognition technology, we need to\nensure necessary balance and coverage in training data. However, there are\nstill scientific questions about how to represent and extract pertinent facial\nfeatures and quantitatively measure facial diversity. Towards this goal,\nDiversity in Faces (DiF) provides a data set of one million annotated human\nface images for advancing the study of facial diversity. The annotations are\ngenerated using ten well-established facial coding schemes from the scientific\nliterature. The facial coding schemes provide human-interpretable quantitative\nmeasures of facial features. We believe that by making the extracted coding\nschemes available on a large set of faces, we can accelerate research and\ndevelopment towards creating more fair and accurate facial recognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:24:50 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 15:38:35 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 15:26:51 GMT"}, {"version": "v4", "created": "Sat, 16 Feb 2019 17:17:08 GMT"}, {"version": "v5", "created": "Wed, 20 Feb 2019 22:51:46 GMT"}, {"version": "v6", "created": "Mon, 8 Apr 2019 21:27:14 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Merler", "Michele", ""], ["Ratha", "Nalini", ""], ["Feris", "Rogerio S.", ""], ["Smith", "John R.", ""]]}, {"id": "1901.10469", "submitter": "Ovidiu Vaduvescu", "authors": "D. Copandean, O. Vaduvescu, D. Gorgan", "title": "Automated Prototype for Asteroids Detection", "comments": "13th International Conference on Intelligent Computer Communication\n  and Processing (ICCP), Cluj-Napoca, Romania. arXiv admin note: text overlap\n  with arXiv:1901.02542", "journal-ref": "IEEE, 2017", "doi": "10.1109/ICCP.2017.8117033", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near Earth Asteroids (NEAs) are discovered daily, mainly by few major\nsurveys, nevertheless many of them remain unobserved for years, even decades.\nEven so, there is room for new discoveries, including those submitted by\nsmaller projects and amateur astronomers. Besides the well-known surveys that\nhave their own automated system of asteroid detection, there are only a few\nsoftware solutions designed to help amateurs and mini-surveys in NEAs\ndiscovery. Some of these obtain their results based on the blink method in\nwhich a set of reduced images are shown one after another and the astronomer\nhas to visually detect real moving objects in a series of images. This\ntechnique becomes harder with the increase in size of the CCD cameras. Aiming\nto replace manual detection we propose an automated pipeline prototype for\nasteroids detection, written in Python under Linux, which calls some 3rd party\nastrophysics libraries.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:27:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Copandean", "D.", ""], ["Vaduvescu", "O.", ""], ["Gorgan", "D.", ""]]}, {"id": "1901.10503", "submitter": "Vivien Sainte Fare Garnot", "authors": "Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, Nesrine\n  Chehata", "title": "Time-Space tradeoff in deep learning models for crop classification on\n  satellite multi-spectral image time series", "comments": "Currently under review", "journal-ref": "International Geoscience and Remote Sensing Symposium 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate several structured deep learning models for\ncrop type classification on multi-spectral time series. In particular, our aim\nis to assess the respective importance of spatial and temporal structures in\nsuch data. With this objective, we consider several designs of convolutional,\nrecurrent, and hybrid neural networks, and assess their performance on a large\ndataset of freely available Sentinel-2 imagery. We find that the\nbest-performing approaches are hybrid configurations for which most of the\nparameters (up to 90%) are allocated to modeling the temporal structure of the\ndata. Our results thus constitute a set of guidelines for the design of bespoke\ndeep learning models for crop type classification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 19:25:52 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""], ["Giordano", "Sebastien", ""], ["Chehata", "Nesrine", ""]]}, {"id": "1901.10513", "submitter": "Justin Gilmer", "authors": "Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk", "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the phenomenon of adversarial examples ---\nmaliciously constructed inputs that fool trained machine learning models ---\nhas captured the attention of the research community, especially when the\nadversary is restricted to small modifications of a correctly handled input.\nLess surprisingly, image classifiers also lack human-level performance on\nrandomly corrupted images, such as images with additive Gaussian noise. In this\npaper we provide both empirical and theoretical evidence that these are two\nmanifestations of the same underlying phenomenon, establishing close\nconnections between the adversarial robustness and corruption robustness\nresearch programs. This suggests that improving adversarial robustness should\ngo hand in hand with improving performance in the presence of more general and\nrealistic image corruptions. Based on our results we recommend that future\nadversarial defenses consider evaluating the robustness of their methods to\ndistributional shift with benchmarks such as Imagenet-C.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:01:39 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ford", "Nic", ""], ["Gilmer", "Justin", ""], ["Carlini", "Nicolas", ""], ["Cubuk", "Dogus", ""]]}, {"id": "1901.10553", "submitter": "Zhoutong Wang", "authors": "Zhoutong Wang, Qianhui Liang, Fabio Duarte, Fan Zhang, Louis Charron,\n  Lenna Johnsen, Bill Cai, Carlo Ratti", "title": "Quantifying Legibility of Indoor Spaces Using Deep Convolutional Neural\n  Networks: Case Studies in Train Stations", "comments": "20 pages, 19 figures, 7 tables", "journal-ref": null, "doi": "10.1016/j.buildenv.2019.04.035", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legibility is the extent to which a space can be easily recognized.\nEvaluating legibility is particularly desirable in indoor spaces, since it has\na large impact on human behavior and the efficiency of space utilization.\nHowever, indoor space legibility has only been studied through survey and\ntrivial simulations and lacks reliable quantitative measurement. We utilized a\nDeep Convolutional Neural Network (DCNN), which is structurally similar to a\nhuman perception system, to model legibility in indoor spaces. To implement the\nmodeling of legibility for any indoor spaces, we designed an end-to-end\nprocessing pipeline from indoor data retrieving to model training to spatial\nlegibility analysis. Although the model performed very well (98% top-1\naccuracy) overall, there are still discrepancies in accuracy among different\nspaces, reflecting legibility differences. To prove the validity of the\npipeline, we deployed a survey on Amazon Mechanical Turk, collecting 4,015\nsamples. The human samples showed a similar behavior pattern and mechanism as\nthe DCNN models. Further, we used model results to visually explain legibility\nin different architectural programs, building age, building style, visual\nclusterings of spaces and visual explanations for building age and\narchitectural functions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:52:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Wang", "Zhoutong", ""], ["Liang", "Qianhui", ""], ["Duarte", "Fabio", ""], ["Zhang", "Fan", ""], ["Charron", "Louis", ""], ["Johnsen", "Lenna", ""], ["Cai", "Bill", ""], ["Ratti", "Carlo", ""]]}, {"id": "1901.10584", "submitter": "Nitthilan Kannappan Jayakodi", "authors": "Nitthilan Kannappan Jayakodi, Anwesha Chatterjee, Wonje Choi,\n  Janardhan Rao Doppa, Partha Pratim Pande", "title": "Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A\n  Co-Design Approach", "comments": "Published in IEEE Trans. on CAD of Integrated Circuits and Systems", "journal-ref": "Vol. 37, No. 11, Pages 2881-2893, Nov 2018", "doi": "10.1109/TCAD.2018.2857338", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have seen tremendous success for different modalities of\ndata including images, videos, and speech. This success has led to their\ndeployment in mobile and embedded systems for real-time applications. However,\nmaking repeated inferences using deep networks on embedded systems poses\nsignificant challenges due to constrained resources (e.g., energy and computing\npower). To address these challenges, we develop a principled co-design\napproach. Building on prior work, we develop a formalism referred to as\nCoarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers of\nvarying complexity to make predictions. We propose a principled optimization\nalgorithm to automatically configure C2F Nets for a specified trade-off between\naccuracy and energy consumption for inference. The key idea is to select a\nclassifier on-the-fly whose complexity is proportional to the hardness of the\ninput example: simple classifiers for easy inputs and complex classifiers for\nhard inputs. We perform comprehensive experimental evaluation using four\ndifferent C2F Net architectures on multiple real-world image classification\ntasks. Our results show that optimized C2F Net can reduce the Energy Delay\nProduct (EDP) by 27 to 60 percent with no loss in accuracy when compared to the\nbaseline solution, where all predictions are made using the most complex\nclassifier in C2F Net.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 22:07:41 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Jayakodi", "Nitthilan Kannappan", ""], ["Chatterjee", "Anwesha", ""], ["Choi", "Wonje", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""]]}, {"id": "1901.10585", "submitter": "Henry Kvinge", "authors": "Henry Kvinge and Elin Farnell and Jingya Li and Yujia Chen", "title": "Rare geometries: revealing rare categories via dimension-driven\n  statistics", "comments": "9 pages. Section IV substantially expanded with minor improvements to\n  other parts of the paper. Two new co-authors responsible for implementation\n  of the algorithm on real data added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations, classes of data points of primary interest also happen to\nbe those that are least numerous. A well-known example is detection of\nfraudulent transactions among the collection of all financial transactions, the\nvast majority of which are legitimate. These types of problems fall under the\nlabel of `rare-category detection.' There are two challenging aspects of these\nproblems. The first is a general lack of labeled examples of the rare class and\nthe second is the potential non-separability of the rare class from the\nmajority (in terms of available features). Statistics related to the geometry\nof the rare class (such as its intrinsic dimension) can be significantly\ndifferent from those for the majority class, reflecting the different dynamics\ndriving variation in the different classes. In this paper we present a new\nsupervised learning algorithm that uses a dimension-driven statistic, called\nthe kappa-profile, to classify whether unlabeled points belong to a rare class.\nOur algorithm requires very few labeled examples and is invariant with respect\nto translation so that it performs equivalently on both separable and\nnon-separable classes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 22:09:42 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 04:36:34 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Li", "Jingya", ""], ["Chen", "Yujia", ""]]}, {"id": "1901.10609", "submitter": "Di Feng", "authors": "Di Feng, Xiao Wei, Lars Rosenbaum, Atsuto Maki, Klaus Dietmayer", "title": "Deep Active Learning for Efficient Training of a LiDAR 3D Object\n  Detector", "comments": "30th IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep object detector for autonomous driving requires a huge amount\nof labeled data. While recording data via on-board sensors such as camera or\nLiDAR is relatively easy, annotating data is very tedious and time-consuming,\nespecially when dealing with 3D LiDAR points or radar data. Active learning has\nthe potential to minimize human annotation efforts while maximizing the object\ndetector's performance. In this work, we propose an active learning method to\ntrain a LiDAR 3D object detector with the least amount of labeled training data\nnecessary. The detector leverages 2D region proposals generated from the RGB\nimages to reduce the search space of objects and speed up the learning process.\nExperiments show that our proposed method works under different uncertainty\nestimations and query functions, and can save up to 60% of the labeling efforts\nwhile reaching the same network performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 23:19:26 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 14:30:55 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Feng", "Di", ""], ["Wei", "Xiao", ""], ["Rosenbaum", "Lars", ""], ["Maki", "Atsuto", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1901.10644", "submitter": "Ziling Wu", "authors": "Ziling Wu, Abdulaziz Alorf, Ting Yang, Ling Li, Yunhui Zhu", "title": "Robust X-ray Sparse-view Phase Tomography via Hierarchical Synthesis\n  Convolutional Neural Networks", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) based image reconstruction methods have\nbeen intensely used for X-ray computed tomography (CT) reconstruction\napplications. Despite great success, good performance of this data-based\napproach critically relies on a representative big training data set and a\ndense convoluted deep network. The indiscriminating convolution connections\nover all dense layers could be prone to over-fitting, where sampling biases are\nwrongly integrated as features for the reconstruction. In this paper, we report\na robust hierarchical synthesis reconstruction approach, where training data is\npre-processed to separate the information on the domains where sampling biases\nare suspected. These split bands are then trained separately and combined\nsuccessively through a hierarchical synthesis network. We apply the\nhierarchical synthesis reconstruction for two important and classical\ntomography reconstruction scenarios: the spares-view reconstruction and the\nphase reconstruction. Our simulated and experimental results show that\ncomparable or improved performances are achieved with a dramatic reduction of\nnetwork complexity and computational cost. This method can be generalized to a\nwide range of applications including material characterization, in-vivo\nmonitoring and dynamic 4D imaging.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 02:14:15 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Wu", "Ziling", ""], ["Alorf", "Abdulaziz", ""], ["Yang", "Ting", ""], ["Li", "Ling", ""], ["Zhu", "Yunhui", ""]]}, {"id": "1901.10650", "submitter": "Song Bai", "authors": "Song Bai, Yingwei Li, Yuyin Zhou, Qizhu Li, Philip H.S. Torr", "title": "Adversarial Metric Attack and Defense for Person Re-identification", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) has attracted much attention recently due to\nits great importance in video surveillance. In general, distance metrics used\nto identify two person images are expected to be robust under various\nappearance changes. However, our work observes the extreme vulnerability of\nexisting distance metrics to adversarial examples, generated by simply adding\nhuman-imperceptible perturbations to person images. Hence, the security danger\nis dramatically increased when deploying commercial re-ID systems in video\nsurveillance. Although adversarial examples have been extensively applied for\nclassification analysis, it is rarely studied in metric analysis like person\nre-identification. The most likely reason is the natural gap between the\ntraining and testing of re-ID networks, that is, the predictions of a re-ID\nnetwork cannot be directly used during testing without an effective metric. In\nthis work, we bridge the gap by proposing Adversarial Metric Attack, a parallel\nmethodology to adversarial classification attacks. Comprehensive experiments\nclearly reveal the adversarial effects in re-ID systems. Meanwhile, we also\npresent an early attempt of training a metric-preserving network, thereby\ndefending the metric against adversarial attacks. At last, by benchmarking\nvarious adversarial settings, we expect that our work can facilitate the\ndevelopment of adversarial attack and defense in metric-based applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 02:41:50 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 04:23:51 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 14:50:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bai", "Song", ""], ["Li", "Yingwei", ""], ["Zhou", "Yuyin", ""], ["Li", "Qizhu", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1901.10713", "submitter": "Chih-Yuan Yang", "authors": "Chih-Yuan Yang and Heeseung Yun and Srenavis Varadaraj and Jane\n  Yung-jen Hsu", "title": "A Mobile Robot Generating Video Summaries of Seniors' Indoor Activities", "comments": "accepted by MobileHCI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a system which generates summaries from seniors' indoor-activity\nvideos captured by a social robot to help remote family members know their\nseniors' daily activities at home. Unlike the traditional video summarization\ndatasets, indoor videos captured from a moving robot poses additional\nchallenges, namely, (i) the video sequences are very long (ii) a significant\nnumber of video-frames contain no-subject or with subjects at ill-posed\nlocations and scales (iii) most of the well-posed frames contain highly\nredundant information. To address this problem, we propose to \\hl{exploit} pose\nestimation \\hl{for detecting} people in frames\\hl{. This guides the robot} to\nfollow the user and capture effective videos. We use person identification to\ndistinguish a target senior from other people. We \\hl{also make use of} action\nrecognition to analyze seniors' major activities at different moments, and\ndevelop a video summarization method to select diverse and representative\nkeyframes as summaries.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 08:54:31 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 04:35:57 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yang", "Chih-Yuan", ""], ["Yun", "Heeseung", ""], ["Varadaraj", "Srenavis", ""], ["Hsu", "Jane Yung-jen", ""]]}, {"id": "1901.10744", "submitter": "Pasquale De Luca", "authors": "Pasquale De Luca, Vincenzo Maria Russiello, Raffaele Ciro Sannino and\n  Lorenzo Valente", "title": "A study for Image compression using Re-Pair algorithm", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The compression is an important topic in computer science which allows we to\nstorage more amount of data on our data storage. There are several techniques\nto compress any file. In this manuscript will be described the most important\nalgorithm to compress images such as JPEG and it will be compared with another\nmethod to retrieve good reason to not use this method on images. So to compress\nthe text the most encoding technique known is the Huffman Encoding which it\nwill be explained in exhaustive way. In this manuscript will showed how to\ncompute a text compression method on images in particular the method and the\nreason to choice a determinate image format against the other. The method\nstudied and analyzed in this manuscript is the Re-Pair algorithm which is\npurely for grammatical context to be compress. At the and it will be showed the\ngood result of this application.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:17:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:56:41 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 11:36:43 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["De Luca", "Pasquale", ""], ["Russiello", "Vincenzo Maria", ""], ["Sannino", "Raffaele Ciro", ""], ["Valente", "Lorenzo", ""]]}, {"id": "1901.10747", "submitter": "Mehmet Turkan", "authors": "Kemal Alkin Gunbay, Mert Arikan, Mehmet Turkan", "title": "Autonomous Cars: Vision based Steering Wheel Angle Estimation", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning models, which are frequently used in self-driving cars, are\ntrained by matching the captured images of the road and the measured angle of\nthe steering wheel. The angle of the steering wheel is generally fetched from\nsteering angle sensor, which is tightly-coupled to the physical aspects of the\nvehicle at hand. Therefore, a model-agnostic autonomous car-kit is very\ndifficult to be developed and autonomous vehicles need more training data. The\nproposed vision based steering angle estimation system argues a new approach\nwhich basically matches the images of the road captured by an outdoor camera\nand the images of the steering wheel from an onboard camera, avoiding the\nburden of collecting model-dependent training data and the use of any other\nelectromechanical hardware.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:22:37 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Gunbay", "Kemal Alkin", ""], ["Arikan", "Mert", ""], ["Turkan", "Mehmet", ""]]}, {"id": "1901.10772", "submitter": "Irtiza Hasan", "authors": "Theodore Tsesmelis, Irtiza Hasan, Marco Cristani, Alessio Del Bue,\n  Fabio Galasso", "title": "Human-centric light sensing and estimation from RGBD images: The\n  invisible light switch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lighting design in indoor environments is of primary importance for at least\ntwo reasons: 1) people should perceive an adequate light; 2) an effective\nlighting design means consistent energy saving. We present the Invisible Light\nSwitch (ILS) to address both aspects. ILS dynamically adjusts the room\nillumination level to save energy while maintaining constant the light level\nperception of the users. So the energy saving is invisible to them. Our\nproposed ILS leverages a radiosity model to estimate the light level which is\nperceived by a person within an indoor environment, taking into account the\nperson position and her/his viewing frustum (head pose). ILS may therefore dim\nthose luminaires, which are not seen by the user, resulting in an effective\nenergy saving, especially in large open offices (where light may otherwise be\nON everywhere for a single person). To quantify the system performance, we have\ncollected a new dataset where people wear luxmeter devices while working in\noffice rooms. The luxmeters measure the amount of light (in Lux) reaching the\npeople gaze, which we consider a proxy to their illumination level perception.\nOur initial results are promising: in a room with 8 LED luminaires, the energy\nconsumption in a day may be reduced from 18585 to 6206 watts with ILS\n(currently needing 1560 watts for operations). While doing so, the drop in\nperceived lighting decreases by just 200 lux, a value considered negligible\nwhen the original illumination level is above 1200 lux, as is normally the case\nin offices.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 11:37:17 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Tsesmelis", "Theodore", ""], ["Hasan", "Irtiza", ""], ["Cristani", "Marco", ""], ["Del Bue", "Alessio", ""], ["Galasso", "Fabio", ""]]}, {"id": "1901.10788", "submitter": "Gal Katzhendler", "authors": "Gal Katzhendler and Daphna Weinshall", "title": "Blurred Images Lead to Bad Local Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blurred Images Lead to Bad Local Minima\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 12:47:44 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Katzhendler", "Gal", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1901.10799", "submitter": "Sebastian Mathias Keller", "authors": "Sebastian Mathias Keller, Maxim Samarin, Mario Wieser, Volker Roth", "title": "Deep Archetypal Analysis", "comments": "Published at the German Conference on Pattern Recognition 2019 (GCPR)", "journal-ref": "41th German Conference on Pattern Recognition, GCPR 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Deep Archetypal Analysis\" generates latent representations of\nhigh-dimensional datasets in terms of fractions of intuitively understandable\nbasic entities called archetypes. The proposed method is an extension of linear\n\"Archetypal Analysis\" (AA), an unsupervised method to represent multivariate\ndata points as sparse convex combinations of extremal elements of the dataset.\nUnlike the original formulation of AA, \"Deep AA\" can also handle side\ninformation and provides the ability for data-driven representation learning\nwhich reduces the dependence on expert knowledge. Our method is motivated by\nstudies of evolutionary trade-offs in biology where archetypes are species\nhighly adapted to a single task. Along these lines, we demonstrate that \"Deep\nAA\" also lends itself to the supervised exploration of chemical space, marking\na distinct starting point for de novo molecular design. In the unsupervised\nsetting we show how \"Deep AA\" is used on CelebA to identify archetypal faces.\nThese can then be superimposed in order to generate new faces which inherit\ndominant traits of the archetypes they are based on.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:04:53 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 16:37:27 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Keller", "Sebastian Mathias", ""], ["Samarin", "Maxim", ""], ["Wieser", "Mario", ""], ["Roth", "Volker", ""]]}, {"id": "1901.10802", "submitter": "Md Ashraful Alam Milton", "authors": "Md Ashraful Alam Milton", "title": "Automated Skin Lesion Classification Using Ensemble of Deep Neural\n  Networks in ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection\n  Challenge", "comments": "ISIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we studied extensively on different deep learning based\nmethods to detect melanoma and skin lesion cancers. Melanoma, a form of\nmalignant skin cancer is very threatening to health. Proper diagnosis of\nmelanoma at an earlier stage is crucial for the success rate of complete cure.\nDermoscopic images with Benign and malignant forms of skin cancer can be\nanalyzed by computer vision system to streamline the process of skin cancer\ndetection. In this study, we experimented with various neural networks which\nemploy recent deep learning based models like PNASNet-5-Large,\nInceptionResNetV2, SENet154, InceptionV4. Dermoscopic images are properly\nprocessed and augmented before feeding them into the network. We tested our\nmethods on International Skin Imaging Collaboration (ISIC) 2018 challenge\ndataset. Our system has achieved best validation score of 0.76 for\nPNASNet-5-Large model. Further improvement and optimization of the proposed\nmethods with a bigger training dataset and carefully chosen hyper-parameter\ncould improve the performances. The code available for download at\nhttps://github.com/miltonbd/ISIC_2018_classification\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:10:11 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Milton", "Md Ashraful Alam", ""]]}, {"id": "1901.10824", "submitter": "Babajide Ayinde", "authors": "Babajide O. Ayinde and Keishin Nishihama and Jacek M. Zurada", "title": "Diversity Regularized Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two key players in Generative Adversarial Networks (GANs), the\ndiscriminator and generator, are usually parameterized as deep neural networks\n(DNNs). On many generative tasks, GANs achieve state-of-the-art performance but\nare often unstable to train and sometimes miss modes. A typical failure mode is\nthe collapse of the generator to a single parameter configuration where its\noutputs are identical. When this collapse occurs, the gradient of the\ndiscriminator may point in similar directions for many similar points. We\nhypothesize that some of these shortcomings are in part due to primitive and\nredundant features extracted by discriminator and this can easily make the\ntraining stuck. We present a novel approach for regularizing adversarial models\nby enforcing diverse feature learning. In order to do this, both generator and\ndiscriminator are regularized by penalizing both negatively and positively\ncorrelated features according to their differentiation and based on their\nrelative cosine distances. In addition to the gradient information from the\nadversarial loss made available by the discriminator, diversity regularization\nalso ensures that a more stable gradient is provided to update both the\ngenerator and discriminator. Results indicate our regularizer enforces diverse\nfeatures, stabilizes training, and improves image synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:44:08 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ayinde", "Babajide O.", ""], ["Nishihama", "Keishin", ""], ["Zurada", "Jacek M.", ""]]}, {"id": "1901.10841", "submitter": "Guoqiang Wei", "authors": "Guoqiang Wei and Cuiling Lan and Wenjun Zeng and Zhibo Chen", "title": "View Invariant 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of deep networks has significantly advanced 3D human pose\nestimation from 2D images. The diversity of capturing viewpoints and the\nflexibility of the human poses, however, remain some significant challenges. In\nthis paper, we propose a view invariant 3D human pose estimation module to\nalleviate the effects of viewpoint diversity. The framework consists of a base\nnetwork, which provides an initial estimation of a 3D pose, a view-invariant\nhierarchical correction network (VI-HC) on top of that to learn the 3D pose\nrefinement under consistent views, and a view-invariant discriminative network\n(VID) to enforce high-level constraints over body configurations. In VI-HC, the\ninitial 3D pose inputs are automatically transformed to consistent views for\nfurther refinements at the global body and local body parts level,\nrespectively. For the VID, under consistent viewpoints, we use adversarial\nlearning to differentiate between estimated poses and real poses to avoid\nimplausible 3D poses. Experimental results demonstrate that the consistent\nviewpoints can dramatically enhance the performance. Our module shows\nrobustness for different 3D pose base networks and achieves a significant\nimprovement (about 9%) over a powerful baseline on the public 3D pose\nestimation benchmark Human3.6M.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:20:31 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Wei", "Guoqiang", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "1901.10889", "submitter": "Jiaojiao Zhao", "authors": "Jiaojiao Zhao, Jungong Han, Ling Shao, Cees G. M. Snoek", "title": "Pixelated Semantic Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many image colorization algorithms have recently shown the capability\nof producing plausible color versions from gray-scale photographs, they still\nsuffer from limited semantic understanding. To address this shortcoming, we\npropose to exploit pixelated object semantics to guide image colorization. The\nrationale is that human beings perceive and distinguish colors based on the\nsemantic categories of objects. Starting from an autoregressive model, we\ngenerate image color distributions, from which diverse colored results are\nsampled. We propose two ways to incorporate object semantics into the\ncolorization model: through a pixelated semantic embedding and a pixelated\nsemantic generator. Specifically, the proposed convolutional neural network\nincludes two branches. One branch learns what the object is, while the other\nbranch learns the object colors. The network jointly optimizes a color\nembedding loss, a semantic segmentation loss and a color generation loss, in an\nend-to-end fashion. Experiments on PASCAL VOC2012 and COCO-stuff reveal that\nour network, when trained with semantic segmentation labels, produces more\nrealistic and finer results compared to the colorization state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 20:28:48 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:12:34 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Zhao", "Jiaojiao", ""], ["Han", "Jungong", ""], ["Shao", "Ling", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1901.10895", "submitter": "Zhibin Yu", "authors": "Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Wu, Bing Zheng and Ping\n  Lin", "title": "Generative Adversarial Network with Multi-Branch Discriminator for\n  Cross-Species Image-to-Image Translation", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current approaches have made great progress on image-to-image translation\ntasks benefiting from the success of image synthesis methods especially\ngenerative adversarial networks (GANs). However, existing methods are limited\nto handling translation tasks between two species while keeping the content\nmatching on the semantic level. A more challenging task would be the\ntranslation among more than two species. To explore this new area, we propose a\nsimple yet effective structure of a multi-branch discriminator for enhancing an\narbitrary generative adversarial architecture (GAN), named GAN-MBD. It takes\nadvantage of the boosting strategy to break a common discriminator into several\nsmaller ones with fewer parameters, which can enhance the generation and\nsynthesis abilities of GANs efficiently and effectively. Comprehensive\nexperiments show that the proposed multi-branch discriminator can dramatically\nimprove the performance of popular GANs on cross-species image-to-image\ntranslation tasks while reducing the number of parameters for computation. The\ncode and some datasets are attached as supplementary materials for reference.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 07:14:07 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Yu", "Zhibin", ""], ["Zheng", "Haiyong", ""], ["Wu", "Yang", ""], ["Zheng", "Bing", ""], ["Lin", "Ping", ""]]}, {"id": "1901.10900", "submitter": "Babajide Ayinde", "authors": "Babajide O. Ayinde and Tamer Inanc and Jacek M. Zurada", "title": "On Correlation of Features Extracted by Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy in deep neural network (DNN) models has always been one of their\nmost intriguing and important properties. DNNs have been shown to\noverparameterize, or extract a lot of redundant features. In this work, we\nexplore the impact of size (both width and depth), activation function, and\nweight initialization on the susceptibility of deep neural network models to\nextract redundant features. To estimate the number of redundant features in\neach layer, all the features of a given layer are hierarchically clustered\naccording to their relative cosine distances in feature space and a set\nthreshold. It is shown that both network size and activation function are the\ntwo most important components that foster the tendency of DNNs to extract\nredundant features. The concept is illustrated using deep multilayer perceptron\nand convolutional neural networks on MNIST digits recognition and CIFAR-10\ndataset, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 15:31:35 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ayinde", "Babajide O.", ""], ["Inanc", "Tamer", ""], ["Zurada", "Jacek M.", ""]]}, {"id": "1901.10909", "submitter": "Zhiling Long", "authors": "Zhiling Long, Yazeed Alaudah, Muhammad Ali Qureshi, Motaz Al Farraj,\n  Zhen Wang, Asjad Amin, Mohamed Deriche, and Ghassan AlRegib", "title": "Characterization of migrated seismic volumes using texture attributes: a\n  comparative study", "comments": null, "journal-ref": "Proceedings of the SEG 85th Annual Meeting, New Orleans, LA, Oct.\n  2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine several typical texture attributes developed in the\nimage processing community in recent years with respect to their capability of\ncharacterizing a migrated seismic volume. These attributes are generated in\neither frequency or space domain, including steerable pyramid, curvelet, local\nbinary pattern, and local radius index. The comparative study is performed\nwithin an image retrieval framework. We evaluate these attributes in terms of\nretrieval accuracy. It is our hope that this comparative study will help\nacquaint the seismic interpretation community with the many available powerful\nimage texture analysis techniques, providing more alternative attributes for\ntheir seismic exploration.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 15:42:19 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Long", "Zhiling", ""], ["Alaudah", "Yazeed", ""], ["Qureshi", "Muhammad Ali", ""], ["Farraj", "Motaz Al", ""], ["Wang", "Zhen", ""], ["Amin", "Asjad", ""], ["Deriche", "Mohamed", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.10915", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin, Alexey Dosovitskiy and Vladlen Koltun", "title": "Benchmarking Classic and Learned Navigation in Complex 3D Environments", "comments": "Added CNN-Monodepth and OpenCV Stereo agents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation research is attracting renewed interest with the advent of\nlearning-based methods. However, this new line of work is largely disconnected\nfrom well-established classic navigation approaches. In this paper, we take a\nstep towards coordinating these two directions of research. We set up classic\nand learning-based navigation systems in common simulated environments and\nthoroughly evaluate them in indoor spaces of varying complexity, with access to\ndifferent sensory modalities. Additionally, we measure human performance in the\nsame environments. We find that a classic pipeline, when properly tuned, can\nperform very well in complex cluttered environments. On the other hand, learned\nsystems can operate more robustly with a limited sensor suite. Overall, both\napproaches are still far from human-level performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 15:50:54 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 11:58:29 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1901.10957", "submitter": "Zhiling Long", "authors": "Tariq Alshawi, Zhiling Long, and Ghassan AlRegib", "title": "Understanding spatial correlation in eye-fixation maps for visual\n  attention in videos", "comments": "Proceedings of IEEE International Conference on Multimedia and Expo\n  (ICME), Seattle, WA, Nov. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an analysis of recorded eye-fixation data from\nhuman subjects viewing video sequences. The purpose is to better understand\nvisual attention for videos. Utilizing the eye-fixation data provided in the\nCRCNS (Collaborative Research in Computational Neuroscience) dataset, this\npaper focuses on the relation between the saliency of a pixel and that of its\ndirect neighbors, without making any assumption about the structure of the\neye-fixation maps. By employing some basic concepts from information theory,\nthe analysis shows substantial correlation between the saliency of a pixel and\nthe saliency of its neighborhood. The analysis also provides insights into the\nstructure and dynamics of the eye-fixation maps, which can be very useful in\nunderstanding video saliency and its applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:13:21 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Alshawi", "Tariq", ""], ["Long", "Zhiling", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.10968", "submitter": "Alexandre Coninx", "authors": "L\\'eni K. Le Goff, Ghanim Mukhtar, Alexandre Coninx and St\\'ephane\n  Doncieux", "title": "Bootstrapping Robotic Ecological Perception from a Limited Set of\n  Hypotheses Through Interactive Perception", "comments": "21 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve its task, a robot needs to have the ability to interpret its\nperceptions. In vision, this interpretation is particularly difficult and\nrelies on the understanding of the structure of the scene, at least to the\nextent of its task and sensorimotor abilities. A robot with the ability to\nbuild and adapt this interpretation process according to its own tasks and\ncapabilities would push away the limits of what robots can achieve in a non\ncontrolled environment. A solution is to provide the robot with processes to\nbuild such representations that are not specific to an environment or a\nsituation. A lot of works focus on objects segmentation, recognition and\nmanipulation. Defining an object solely on the basis of its visual appearance\nis challenging given the wide range of possible objects and environments.\nTherefore, current works make simplifying assumptions about the structure of a\nscene. Such assumptions reduce the adaptivity of the object extraction process\nto the environments in which the assumption holds. To limit such assumptions,\nwe introduce an exploration method aimed at identifying moveable elements in a\nscene without considering the concept of object. By using the interactive\nperception framework, we aim at bootstrapping the acquisition process of a\nrepresentation of the environment with a minimum of context specific\nassumptions. The robotic system builds a perceptual map called relevance map\nwhich indicates the moveable parts of the current scene. A classifier is\ntrained online to predict the category of each region (moveable or\nnon-moveable). It is also used to select a region with which to interact, with\nthe goal of minimizing the uncertainty of the classification. A specific\nclassifier is introduced to fit these needs: the collaborative mixture models\nclassifier. The method is tested on a set of scenarios of increasing\ncomplexity, using both simulations and a PR2 robot.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:35:42 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Goff", "L\u00e9ni K. Le", ""], ["Mukhtar", "Ghanim", ""], ["Coninx", "Alexandre", ""], ["Doncieux", "St\u00e9phane", ""]]}, {"id": "1901.11074", "submitter": "Adri\\'an Bazaga", "authors": "Adri\\'an Bazaga, M\\`onica Rold\\'an, Carmen Badosa, Cecilia\n  Jim\\'enez-Mallebrera, Josep M. Porta", "title": "A Convolutional Neural Network for the Automatic Diagnosis of Collagen\n  VI related Muscular Dystrophies", "comments": "Submitted for review to Expert Systems With Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of machine learning systems for the diagnosis of rare\ndiseases is challenging mainly due the lack of data to study them. Despite this\nchallenge, this paper proposes a system for the Computer Aided Diagnosis (CAD)\nof low-prevalence, congenital muscular dystrophies from confocal microscopy\nimages. The proposed CAD system relies on a Convolutional Neural Network (CNN)\nwhich performs an independent classification for non-overlapping patches tiling\nthe input image, and generates an overall decision summarizing the individual\ndecisions for the patches on the query image. This decision scheme points to\nthe possibly problematic areas in the input images and provides a global\nquantitative evaluation of the state of the patients, which is fundamental for\ndiagnosis and to monitor the efficiency of therapies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:59:33 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Bazaga", "Adri\u00e1n", ""], ["Rold\u00e1n", "M\u00f2nica", ""], ["Badosa", "Carmen", ""], ["Jim\u00e9nez-Mallebrera", "Cecilia", ""], ["Porta", "Josep M.", ""]]}, {"id": "1901.11078", "submitter": "Khashayar Asadi", "authors": "Idris Jeelani, Khashayar Asadi, Hariharan Ramshankar, Kevin Han, Alex\n  Albert", "title": "Real-world Mapping of Gaze Fixations Using Instance Segmentation for\n  Road Construction Safety Applications", "comments": "2019 TRB Annual meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research studies have shown that a large proportion of hazards remain\nunrecognized, which expose construction workers to unanticipated safety risks.\nRecent studies have also found that a strong correlation exists between viewing\npatterns of workers, captured using eye-tracking devices, and their hazard\nrecognition performance. Therefore, it is important to analyze the viewing\npatterns of workers to gain a better understanding of their hazard recognition\nperformance. This paper proposes a method that can automatically map the gaze\nfixations collected using a wearable eye-tracker to the predefined areas of\ninterests. The proposed method detects these areas or objects (i.e., hazards)\nof interests through a computer vision-based segmentation technique and\ntransfer learning. The mapped fixation data is then used to analyze the viewing\nbehaviors of workers and compute their attention distribution. The proposed\nmethod is implemented on an under construction road as a case study to evaluate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:02:27 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 19:47:25 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Jeelani", "Idris", ""], ["Asadi", "Khashayar", ""], ["Ramshankar", "Hariharan", ""], ["Han", "Kevin", ""], ["Albert", "Alex", ""]]}, {"id": "1901.11082", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Dana Lynn Ona Lansigan, Philip Marcus, Matthias\n  Nie{\\ss}ner", "title": "DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for\ngeometric deep learning. The DDSL is a differentiable layer compatible with\ndeep neural networks for bridging simplex mesh-based geometry representations\n(point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images\n(e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to\nperform differentiable, efficient, anti-aliased rasterization of simplex-based\nsignals. We present a complete theoretical framework for the process as well as\nan efficient backpropagation algorithm. Compared to previous differentiable\nrenderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees\nand dimensions. In particular, we explore its applications to 2D shapes and\nillustrate two applications of this method: (1) mesh editing and optimization\nguided by neural network outputs, and (2) using DDSL for a differentiable\nrasterization loss to facilitate end-to-end training of polygon generators. We\nare able to validate the effectiveness of gradient-based shape optimization\nwith the example of airfoil optimization, and using the differentiable\nrasterization loss to facilitate end-to-end training, we surpass state of the\nart for polygonal image segmentation given ground-truth bounding boxes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:17:50 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 23:17:43 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 22:28:26 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Lansigan", "Dana Lynn Ona", ""], ["Marcus", "Philip", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1901.11094", "submitter": "Aydogan Ozcan", "authors": "Kevin de Haan, Zachary S. Ballard, Yair Rivenson, Yichen Wu and\n  Aydogan Ozcan", "title": "Resolution enhancement in scanning electron microscopy using deep\n  learning", "comments": "8 pages, 4 figures", "journal-ref": "Scientific Reports (2019)", "doi": "10.1038/s41598-019-48444-2", "report-no": null, "categories": "cs.CV cs.LG physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report resolution enhancement in scanning electron microscopy (SEM) images\nusing a generative adversarial network. We demonstrate the veracity of this\ndeep learning-based super-resolution technique by inferring unresolved features\nin low-resolution SEM images and comparing them with the accurately\nco-registered high-resolution SEM images of the same samples. Through spatial\nfrequency analysis, we also report that our method generates images with\nfrequency spectra matching higher resolution SEM images of the same\nfields-of-view. By using this technique, higher resolution SEM images can be\ntaken faster, while also reducing both electron charging and damage to the\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:48:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["de Haan", "Kevin", ""], ["Ballard", "Zachary S.", ""], ["Rivenson", "Yair", ""], ["Wu", "Yichen", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1901.11095", "submitter": "Zhiling Long", "authors": "Muhammad Amir Shafiq, Zhiling Long, Tariq Alshawi, and Ghassan AlRegib", "title": "Saliency detection for seismic applications using multi-dimensional\n  spectral projections and directional comparisons", "comments": null, "journal-ref": "Proceedings of IEEE International Conference on Image Processing\n  (ICIP), Beijing, China, Sep. 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for saliency detection for seismic\napplications using 3D-FFT local spectra and multi-dimensional plane\nprojections. We develop a projection scheme by dividing a 3D-FFT local spectrum\nof a data volume into three distinct components, each depicting changes along a\ndifferent dimension of the data. The saliency detection results obtained using\neach projected component are then combined to yield a saliency map. To\naccommodate the directional nature of seismic data, in this work, we modify the\ncenter-surround model, proven to be biologically plausible for visual\nattention, to incorporate directional comparisons around each voxel in a 3D\nvolume. Experimental results on real seismic dataset from the F3 block in\nNetherlands offshore in the North Sea prove that the proposed algorithm is\neffective, efficient, and scalable. Furthermore, a subjective comparison of the\nresults shows that it outperforms the state-of-the-art methods for saliency\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:49:26 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Shafiq", "Muhammad Amir", ""], ["Long", "Zhiling", ""], ["Alshawi", "Tariq", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1901.11111", "submitter": "Remi Giraud", "authors": "Remi Giraud, Vinh-Thong Ta, Nicolas Papadakis, Yannick Berthoumieu", "title": "Texture-Aware Superpixel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most superpixel algorithms compute a trade-off between spatial and color\nfeatures at the pixel level. Hence, they may need fine parameter tuning to\nbalance the two measures, and highly fail to group pixels with similar local\ntexture properties. In this paper, we address these issues with a new\nTexture-Aware SuperPixel (TASP) method. To accurately segment textured and\nsmooth areas, TASP automatically adjusts its spatial constraint according to\nthe local feature variance. Then, to ensure texture homogeneity within\nsuperpixels, a new pixel to superpixel patch-based distance is proposed. TASP\noutperforms the segmentation accuracy of the state-of-the-art methods on\ntexture and also natural color image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 21:37:35 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 20:43:22 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2019 19:16:45 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Giraud", "Remi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""], ["Berthoumieu", "Yannick", ""]]}, {"id": "1901.11112", "submitter": "Narayan Hegde", "authors": "Narayan Hegde, Jason D. Hipp, Yun Liu, Michael E. Buck, Emily Reif,\n  Daniel Smilkov, Michael Terry, Carrie J. Cai, Mahul B. Amin, Craig H. Mermel,\n  Phil Q. Nelson, Lily H. Peng, Greg S. Corrado and Martin C. Stumpe", "title": "Similar Image Search for Histopathology: SMILY", "comments": "23 Pages with 6 figures and 3 tables. The file also has 6 pages of\n  supplemental material. Improved figure resolution, edited metadata", "journal-ref": "Nature Partner Journal Digital Medicine (2019)", "doi": "10.1038/s41746-019-0131-z", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of large institutional and public histopathology\nimage datasets is enabling the searching of these datasets for diagnosis,\nresearch, and education. Though these datasets typically have associated\nmetadata such as diagnosis or clinical notes, even carefully curated datasets\nrarely contain annotations of the location of regions of interest on each\nimage. Because pathology images are extremely large (up to 100,000 pixels in\neach dimension), further laborious visual search of each image may be needed to\nfind the feature of interest. In this paper, we introduce a deep learning based\nreverse image search tool for histopathology images: Similar Medical Images\nLike Yours (SMILY). We assessed SMILY's ability to retrieve search results in\ntwo ways: using pathologist-provided annotations, and via prospective studies\nwhere pathologists evaluated the quality of SMILY search results. As a negative\ncontrol in the second evaluation, pathologists were blinded to whether search\nresults were retrieved by SMILY or randomly. In both types of assessments,\nSMILY was able to retrieve search results with similar histologic features,\norgan site, and prostate cancer Gleason grade compared with the original query.\nSMILY may be a useful general-purpose tool in the pathologist's arsenal, to\nimprove the efficiency of searching large archives of histopathology images,\nwithout the need to develop and implement specific tools for each application.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 21:41:14 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 21:55:51 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 02:15:17 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Hegde", "Narayan", ""], ["Hipp", "Jason D.", ""], ["Liu", "Yun", ""], ["Buck", "Michael E.", ""], ["Reif", "Emily", ""], ["Smilkov", "Daniel", ""], ["Terry", "Michael", ""], ["Cai", "Carrie J.", ""], ["Amin", "Mahul B.", ""], ["Mermel", "Craig H.", ""], ["Nelson", "Phil Q.", ""], ["Peng", "Lily H.", ""], ["Corrado", "Greg S.", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1901.11153", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Shengping\n  Zhang", "title": "Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view\n  Images", "comments": "ICCV 2019", "journal-ref": null, "doi": "10.1109/ICCV.2019.00278", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D representation of an object from single-view or multi-view\nRGB images by deep neural networks has attracted increasing attention in the\npast few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural\nnetworks (RNNs) to fuse multiple feature maps extracted from input images\nsequentially. However, when given the same set of input images with different\norders, RNN-based approaches are unable to produce consistent reconstruction\nresults. Moreover, due to long-term memory loss, RNNs cannot fully exploit\ninput images to refine reconstruction results. To solve these problems, we\npropose a novel framework for single-view and multi-view 3D reconstruction,\nnamed Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse\n3D volume from each input image. Then, a context-aware fusion module is\nintroduced to adaptively select high-quality reconstructions for each part\n(e.g., table legs) from different coarse 3D volumes to obtain a fused 3D\nvolume. Finally, a refiner further refines the fused 3D volume to generate the\nfinal output. Experimental results on the ShapeNet and Pix3D benchmarks\nindicate that the proposed Pix2Vox outperforms state-of-the-arts by a large\nmargin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in\nterms of backward inference time. The experiments on ShapeNet unseen 3D\ncategories have shown the superior generalization abilities of our method.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 00:01:25 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 01:50:59 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Xie", "Haozhe", ""], ["Yao", "Hongxun", ""], ["Sun", "Xiaoshuai", ""], ["Zhou", "Shangchen", ""], ["Zhang", "Shengping", ""]]}, {"id": "1901.11164", "submitter": "Cleison Correia de Amorim", "authors": "Cleison Correia de Amorim, David Mac\\^edo, and Cleber Zanchettin", "title": "Spatial-Temporal Graph Convolutional Networks for Sign Language\n  Recognition", "comments": null, "journal-ref": "2019 International Conference on Artificial Neural Networks\n  (ICANN)", "doi": "10.1007/978-3-030-30493-5_59", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of sign language is a challenging task with an important role\nin society to facilitate the communication of deaf persons. We propose a new\napproach of Spatial-Temporal Graph Convolutional Network to sign language\nrecognition based on the human skeletal movements. The method uses graphs to\ncapture the signs dynamics in two dimensions, spatial and temporal, considering\nthe complex aspects of the language. Additionally, we present a new dataset of\nhuman skeletons for sign language based on ASLLVD to contribute to future\nrelated studies.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 01:25:47 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 01:19:54 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["de Amorim", "Cleison Correia", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.11179", "submitter": "Rafal Pilarczyk", "authors": "Rafal Pilarczyk and Xin Chang and Wladyslaw Skarbek", "title": "Human Face Expressions from Images - 2D Face Geometry and 3D Face Local\n  Motion versus Deep Neural Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several computer algorithms for recognition of visible human emotions are\ncompared at the web camera scenario using CNN/MMOD face detector. The\nrecognition refers to four face expressions: smile, surprise, anger, and\nneutral. At the feature extraction stage, the following three concepts of face\ndescription are confronted: (a) static 2D face geometry represented by its 68\ncharacteristic landmarks (FP68); (b) dynamic 3D geometry defined by motion\nparameters for eight distinguished face parts (denoted as AU8) of personalized\nCandide-3 model; (c) static 2D visual description as 2D array of gray scale\npixels (known as facial raw image). At the classification stage, the\nperformance of two major models are analyzed: (a) support vector machine (SVM)\nwith kernel options; (b) convolutional neural network (CNN) with variety of\nrelevant tensor processing layers and blocks of them. The models are trained\nfor frontal views of human faces while they are tested for arbitrary head\nposes. For geometric features, the success rate (accuracy) indicate nearly\ntriple increase of performance of CNN with respect to SVM classifiers. For raw\nimages, CNN outperforms in accuracy its best geometric counterpart (AU/CNN) by\nabout 30 percent while the best SVM solutions are inferior nearly four times.\nFor F-score the high advantage of raw/CNN over geometric/CNN and geometric/SVM\nis observed, as well. We conclude that contrary to CNN based emotion\nclassifiers, the generalization capability wrt human head pose is for SVM based\nemotion classifiers poor.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 02:32:47 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Pilarczyk", "Rafal", ""], ["Chang", "Xin", ""], ["Skarbek", "Wladyslaw", ""]]}, {"id": "1901.11186", "submitter": "Rafal Pilarczyk", "authors": "Rafal Pilarczyk and Wladyslaw Skarbek", "title": "On Intra-Class Variance for Deep Learning of Classifiers", "comments": "changed abstract, character-level errors have been fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel technique for deep learning of image classifiers is presented. The\nlearned CNN models offer better separation of deep features (also known as\nembedded vectors) measured by Euclidean proximity and also no deterioration of\nthe classification results by class membership probability. The latter feature\ncan be used for enhancing image classifiers having the classes at the model's\nexploiting stage different from from classes during the training stage. While\nthe Shannon information of SoftMax probability for target class is extended for\nmini-batch by the intra-class variance, the trained network itself is extended\nby the Hadamard layer with the parameters representing the class centers.\nContrary to the existing solutions, this extra neural layer enables interfacing\nof the training algorithm to the standard stochastic gradient optimizers, e.g.\nAdaM algorithm. Moreover, this approach makes the computed centroids\nimmediately adapting to the updating embedded vectors and finally getting the\ncomparable accuracy in less epochs.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 02:54:14 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 08:45:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Pilarczyk", "Rafal", ""], ["Skarbek", "Wladyslaw", ""]]}, {"id": "1901.11188", "submitter": "Yi Ren", "authors": "Houpu Yao, Zhe Wang, Guangyu Nie, Yassine Mazboudi, Yezhou Yang, Yi\n  Ren", "title": "Augmenting Model Robustness with Transformation-Invariant Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of neural networks under adversarial attacks has raised\nserious concerns and motivated extensive research. It has been shown that both\nneural networks and adversarial attacks against them can be sensitive to input\ntransformations such as linear translation and rotation, and that human vision,\nwhich is robust against adversarial attacks, is invariant to natural input\ntransformations. Based on these, this paper tests the hypothesis that model\nrobustness can be further improved when it is adversarially trained against\ntransformed attacks and transformation-invariant attacks. Experiments on MNIST,\nCIFAR-10, and restricted ImageNet show that while transformations of attacks\nalone do not affect robustness, transformation-invariant attacks can improve\nmodel robustness by 2.5\\% on MNIST, 3.7\\% on CIFAR-10, and 1.1\\% on restricted\nImageNet. We discuss the intuition behind this phenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 02:56:28 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 22:32:45 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Yao", "Houpu", ""], ["Wang", "Zhe", ""], ["Nie", "Guangyu", ""], ["Mazboudi", "Yassine", ""], ["Yang", "Yezhou", ""], ["Ren", "Yi", ""]]}, {"id": "1901.11195", "submitter": "Caiyong Wang", "authors": "Caiyong Wang, Yuhao Zhu, Yunfan Liu, Ran He, Zhenan Sun", "title": "Joint Iris Segmentation and Localization Using Deep Multi-task Learning\n  Framework", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris segmentation and localization in non-cooperative environment is\nchallenging due to illumination variations, long distances, moving subjects and\nlimited user cooperation, etc. Traditional methods often suffer from poor\nperformance when confronted with iris images captured in these conditions.\nRecent studies have shown that deep learning methods could achieve impressive\nperformance on iris segmentation task. In addition, as iris is defined as an\nannular region between pupil and sclera, geometric constraints could be imposed\nto help locating the iris more accurately and improve the segmentation results.\nIn this paper, we propose a deep multi-task learning framework, named as\nIrisParseNet, to exploit the inherent correlations between pupil, iris and\nsclera to boost up the performance of iris segmentation and localization in a\nunified model. In particular, IrisParseNet firstly applies a Fully\nConvolutional Encoder-Decoder Attention Network to simultaneously estimate\npupil center, iris segmentation mask and iris inner/outer boundary. Then, an\neffective post-processing method is adopted for iris inner/outer circle\nlocalization.To train and evaluate the proposed method, we manually label three\nchallenging iris datasets, namely CASIA-Iris-Distance, UBIRIS.v2, and MICHE-I,\nwhich cover various types of noises. Extensive experiments are conducted on\nthese newly annotated datasets, and results show that our method outperforms\nstate-of-the-art methods on various benchmarks. All the ground-truth\nannotations, annotation codes and evaluation protocols are publicly available\nat https://github.com/xiamenwcy/IrisParseNet.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 03:16:15 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:54:03 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wang", "Caiyong", ""], ["Zhu", "Yuhao", ""], ["Liu", "Yunfan", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1901.11203", "submitter": "Mingming Zhang PhD.", "authors": "Mingming Zhang, Quan Zhou and Yanlang Hu", "title": "A lossless data hiding scheme in JPEG images with segment coding", "comments": "14 pages, 5 figures, 8 tables", "journal-ref": null, "doi": "10.1117/1.JEI.28.5.053015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a lossless data hiding scheme in JPEG images. After\nquantified DCT transform, coefficients have characteristics that distribution\nin high frequencies is relatively sparse and absolute values are small. To\nimprove encoding efficiency, we put forward an encoding algorithm that searches\nfor a high frequency as terminate point and recode the coefficients above, so\nspare space is reserved to embed secret data and appended data with no file\nexpansion. Receiver can obtain terminate point through data analysis, extract\nadditional data and recover original JPEG images lossless. Experimental results\nshow that the proposed method has a larger capacity than state-of-the-art\nworks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 04:01:52 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zhang", "Mingming", ""], ["Zhou", "Quan", ""], ["Hu", "Yanlang", ""]]}, {"id": "1901.11210", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Paul Bertin and Vincent Frappier", "title": "Chester: A Web Delivered Locally Computed Chest X-Ray Disease Prediction\n  System", "comments": "Submitted to MIDL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to bridge the gap between Deep Learning researchers and medical\nprofessionals we develop a very accessible free prototype system which can be\nused by medical professionals to understand the reality of Deep Learning tools\nfor chest X-ray diagnostics. The system is designed to be a second opinion\nwhere a user can process an image to confirm or aid in their diagnosis. Code\nand network weights are delivered via a URL to a web browser (including cell\nphones) but the patient data remains on the users machine and all processing\noccurs locally. This paper discusses the three main components in detail:\nout-of-distribution detection, disease prediction, and prediction explanation.\nThe system open source and freely available here: https://mlmed.org/tools/xray\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 04:53:53 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:03:48 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2020 18:34:09 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Bertin", "Paul", ""], ["Frappier", "Vincent", ""]]}, {"id": "1901.11228", "submitter": "Vineet Edupuganti", "authors": "Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala, John Pauly", "title": "Uncertainty Quantification in Deep MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable MRI is crucial for accurate interpretation in therapeutic and\ndiagnostic tasks. However, undersampling during MRI acquisition as well as the\noverparameterized and non-transparent nature of deep learning (DL) leaves\nsubstantial uncertainty about the accuracy of DL reconstruction. With this in\nmind, this study aims to quantify the uncertainty in image recovery with DL\nmodels. To this end, we first leverage variational autoencoders (VAEs) to\ndevelop a probabilistic reconstruction scheme that maps out (low-quality) short\nscans with aliasing artifacts to the diagnostic-quality ones. The VAE encodes\nthe acquisition uncertainty in a latent code and naturally offers a posterior\nof the image from which one can generate pixel variance maps using Monte-Carlo\nsampling. Accurately predicting risk requires knowledge of the bias as well,\nfor which we leverage Stein's Unbiased Risk Estimator (SURE) as a proxy for\nmean-squared-error (MSE). Extensive empirical experiments are performed for\nKnee MRI reconstruction under different training losses (adversarial and\npixel-wise) and unrolled recurrent network architectures. Our key observations\nindicate that: 1) adversarial losses introduce more uncertainty; and 2)\nrecurrent unrolled nets reduce the prediction uncertainty and risk.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 06:33:48 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 08:13:07 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 19:26:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Edupuganti", "Vineet", ""], ["Mardani", "Morteza", ""], ["Vasanawala", "Shreyas", ""], ["Pauly", "John", ""]]}, {"id": "1901.11252", "submitter": "Aydogan Ozcan", "authors": "Yichen Wu, Yair Rivenson, Hongda Wang, Yilin Luo, Eyal Ben-David,\n  Laurent A. Bentolila, Christian Pritz, Aydogan Ozcan", "title": "Three-dimensional virtual refocusing of fluorescence microscopy images\n  using deep learning", "comments": "47 pages, 5 figures (main text)", "journal-ref": "Nature Methods (2019)", "doi": "10.1038/s41592-019-0622-5", "report-no": null, "categories": "cs.CV cs.LG physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) fluorescence microscopy in general requires axial\nscanning to capture images of a sample at different planes. Here we demonstrate\nthat a deep convolutional neural network can be trained to virtually refocus a\n2D fluorescence image onto user-defined 3D surfaces within the sample volume.\nWith this data-driven computational microscopy framework, we imaged the neuron\nactivity of a Caenorhabditis elegans worm in 3D using a time-sequence of\nfluorescence images acquired at a single focal plane, digitally increasing the\ndepth-of-field of the microscope by 20-fold without any axial scanning,\nadditional hardware, or a trade-off of imaging resolution or speed.\nFurthermore, we demonstrate that this learning-based approach can correct for\nsample drift, tilt, and other image aberrations, all digitally performed after\nthe acquisition of a single fluorescence image. This unique framework also\ncross-connects different imaging modalities to each other, enabling 3D\nrefocusing of a single wide-field fluorescence image to match confocal\nmicroscopy images acquired at different sample planes. This deep learning-based\n3D image refocusing method might be transformative for imaging and tracking of\n3D biological samples, especially over extended periods of time, mitigating\nphoto-toxicity, sample drift, aberration and defocusing related challenges\nassociated with standard 3D fluorescence microscopy techniques.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 07:56:34 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 07:29:25 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wu", "Yichen", ""], ["Rivenson", "Yair", ""], ["Wang", "Hongda", ""], ["Luo", "Yilin", ""], ["Ben-David", "Eyal", ""], ["Bentolila", "Laurent A.", ""], ["Pritz", "Christian", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1901.11259", "submitter": "Xuefei Zhe", "authors": "Ming Zhang, Xuefei Zhe, Le Ou-Yang, Shifeng Chen and Hong Yan", "title": "Semantic Hierarchy Preserving Deep Hashing for Large-scale Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing models have been proposed as an efficient method for large-scale\nsimilarity search. However, most existing deep hashing methods only utilize\nfine-level labels for training while ignoring the natural semantic hierarchy\nstructure. This paper presents an effective method that preserves the classwise\nsimilarity of full-level semantic hierarchy for large-scale image retrieval.\nExperiments on two benchmark datasets show that our method helps improve the\nfine-level retrieval performance. Moreover, with the help of the semantic\nhierarchy, it can produce significantly better binary codes for hierarchical\nretrieval, which indicates its potential of providing more user-desired\nretrieval results.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 08:19:24 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 08:36:59 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 11:05:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhang", "Ming", ""], ["Zhe", "Xuefei", ""], ["Ou-Yang", "Le", ""], ["Chen", "Shifeng", ""], ["Yan", "Hong", ""]]}, {"id": "1901.11284", "submitter": "Sascha Wirges", "authors": "Sascha Wirges and Marcel Reith-Braun and Martin Lauer and Christoph\n  Stiller", "title": "Capturing Object Detection Uncertainty in Multi-Layer Grid Maps", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep convolutional object detector for automated driving\napplications that also estimates classification, pose and shape uncertainty of\neach detected object. The input consists of a multi-layer grid map which is\nwell-suited for sensor fusion, free-space estimation and machine learning.\nBased on the estimated pose and shape uncertainty we approximate object hulls\nwith bounded collision probability which we find helpful for subsequent\ntrajectory planning tasks. We train our models based on the KITTI object\ndetection data set. In a quantitative and qualitative evaluation some models\nshow a similar performance and superior robustness compared to previously\ndeveloped object detectors. However, our evaluation also points to undesired\ndata set properties which should be addressed when training data-driven models\nor creating new data sets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:33:26 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wirges", "Sascha", ""], ["Reith-Braun", "Marcel", ""], ["Lauer", "Martin", ""], ["Stiller", "Christoph", ""]]}, {"id": "1901.11341", "submitter": "Fabian Isensee", "authors": "Fabian Isensee, Marianne Schell, Irada Tursunova, Gianluca Brugnara,\n  David Bonekamp, Ulf Neuberger, Antje Wick, Heinz-Peter Schlemmer, Sabine\n  Heiland, Wolfgang Wick, Martin Bendszus, Klaus Hermann Maier-Hein, Philipp\n  Kickingereder", "title": "Automated brain extraction of multi-sequence MRI using artificial neural\n  networks", "comments": "Fabian Isensee, Marianne Schell and Irada Tursunova share the first\n  authorship", "journal-ref": "Hum Brain Mapp. 2019; 1-13", "doi": "10.1002/hbm.24750", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction is a critical preprocessing step in the analysis of MRI\nneuroimaging studies and influences the accuracy of downstream analyses. The\nmajority of brain extraction algorithms are, however, optimized for processing\nhealthy brains and thus frequently fail in the presence of pathologically\naltered brain or when applied to heterogeneous MRI datasets. Here we introduce\na new, rigorously validated algorithm (termed HD-BET) relying on artificial\nneural networks that aims to overcome these limitations. We demonstrate that\nHD-BET outperforms six popular, publicly available brain extraction algorithms\nin several large-scale neuroimaging datasets, including one from a prospective\nmulticentric trial in neuro-oncology, yielding state-of-the-art performance\nwith median improvements of +1.16 to +2.11 points for the DICE coefficient and\n-0.66 to -2.51 mm for the Hausdorff distance. Importantly, the HD-BET algorithm\nshows robust performance in the presence of pathology or treatment-induced\ntissue alterations, is applicable to a broad range of MRI sequence types and is\nnot influenced by variations in MRI hardware and acquisition parameters\nencountered in both research and clinical practice. For broader accessibility\nour HD-BET prediction algorithm is made freely available\n(http://www.neuroAI-HD.org) and may become an essential component for robust,\nautomated, high-throughput processing of MRI neuroimaging data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 13:10:39 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 13:52:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Isensee", "Fabian", ""], ["Schell", "Marianne", ""], ["Tursunova", "Irada", ""], ["Brugnara", "Gianluca", ""], ["Bonekamp", "David", ""], ["Neuberger", "Ulf", ""], ["Wick", "Antje", ""], ["Schlemmer", "Heinz-Peter", ""], ["Heiland", "Sabine", ""], ["Wick", "Wolfgang", ""], ["Bendszus", "Martin", ""], ["Maier-Hein", "Klaus Hermann", ""], ["Kickingereder", "Philipp", ""]]}, {"id": "1901.11357", "submitter": "Evgeniy Martyushev", "authors": "Evgeniy Martyushev, Bo Li", "title": "Efficient Relative Pose Estimation for Cameras and Generalized Cameras\n  in Case of Known Relative Rotation Angle", "comments": "15 pages, 9 eps-figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two minimal solutions to the problem of relative pose estimation\nof (i) a calibrated camera from four points in two views and (ii) a calibrated\ngeneralized camera from five points in two views. In both cases, the relative\nrotation angle between the views is assumed to be known. In practice, such\nangle can be derived from the readings of a 3d gyroscope. We represent the\nrotation part of the motion in terms of unit quaternions in order to construct\npolynomial equations encoding the epipolar constraints. The Gr\\\"{o}bner basis\ntechnique is then used to efficiently derive the solutions. Our first solver\nfor regular cameras significantly improves the existing state-of-the-art\nsolution. The second solver for generalized cameras is novel.\n  The presented minimal solvers can be used in a hypothesize-and-test\narchitecture such as RANSAC for reliable pose estimation. Experiments on\nsynthetic and real datasets confirm that our algorithms are numerically stable,\nfast and robust.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 13:52:50 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Martyushev", "Evgeniy", ""], ["Li", "Bo", ""]]}, {"id": "1901.11365", "submitter": "Joshua Batson", "authors": "Joshua Batson, Loic Royer", "title": "Noise2Self: Blind Denoising by Self-Supervision", "comments": "10 pages, 6 figures, and supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for denoising high-dimensional measurements\nwhich requires no prior on the signal, no estimate of the noise, and no clean\ntraining data. The only assumption is that the noise exhibits statistical\nindependence across different dimensions of the measurement, while the true\nsignal exhibits some correlation. For a broad class of functions\n(\"$\\mathcal{J}$-invariant\"), it is then possible to estimate the performance of\na denoiser from noisy data alone. This allows us to calibrate\n$\\mathcal{J}$-invariant versions of any parameterised denoising algorithm, from\nthe single hyperparameter of a median filter to the millions of weights of a\ndeep neural network. We demonstrate this on natural image and microscopy data,\nwhere we exploit noise independence between pixels, and on single-cell gene\nexpression data, where we exploit independence between detections of individual\nmolecules. This framework generalizes recent work on training neural nets from\nnoisy images and on cross-validation for matrix factorization.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 18:05:47 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 23:46:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Batson", "Joshua", ""], ["Royer", "Loic", ""]]}, {"id": "1901.11369", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Yu-Chi Hu, Neelam Tyagi, Pengpeng Zhang, Andreas Rimner,\n  Joseph O. Deasy, Harini Veeraraghavan", "title": "Cross-modality (CT-MRI) prior augmented deep learning for robust lung\n  tumor segmentation from small MR datasets", "comments": "Submitted to Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13695", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of large expert annotated MR datasets makes training deep learning\nmodels difficult. Therefore, a cross-modality (MR-CT) deep learning\nsegmentation approach that augments training data using pseudo MR images\nproduced by transforming expert-segmented CT images was developed. Eighty-One\nT2-weighted MRI scans from 28 patients with non-small cell lung cancers were\nanalyzed. Cross-modality prior encoding the transformation of CT to pseudo MR\nimages resembling T2w MRI was learned as a generative adversarial deep learning\nmodel. This model augmented training data arising from 6 expert-segmented T2w\nMR patient scans with 377 pseudo MRI from non-small cell lung cancer CT patient\nscans with obtained from the Cancer Imaging Archive. A two-dimensional Unet\nimplemented with batch normalization was trained to segment the tumors from T2w\nMRI. This method was benchmarked against (a) standard data augmentation and two\nstate-of-the art cross-modality pseudo MR-based augmentation and (b) two\nsegmentation networks. Segmentation accuracy was computed using Dice similarity\ncoefficient (DSC), Hausdroff distance metrics, and volume ratio. The proposed\napproach produced the lowest statistical variability in the intensity\ndistribution between pseudo and T2w MR images measured as Kullback-Leibler\ndivergence of 0.069. This method produced the highest segmentation accuracy\nwith a DSC of 0.75 and the lowest Hausdroff distance on the test dataset. This\napproach produced highly similar estimations of tumor growth as an expert (P =\n0.37). A novel deep learning MR segmentation was developed that overcomes the\nlimitation of learning robust models from small datasets by leveraging learned\ncross-modality priors to augment training. The results show the feasibility of\nthe approach and the corresponding improvement over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:17:36 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 18:27:47 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Jiang", "Jue", ""], ["Hu", "Yu-Chi", ""], ["Tyagi", "Neelam", ""], ["Zhang", "Pengpeng", ""], ["Rimner", "Andreas", ""], ["Deasy", "Joseph O.", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "1901.11379", "submitter": "Yijun Tian", "authors": "Yijun Tian", "title": "TUNet: Incorporating segmentation maps to improve classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the localization of specific protein in human cells is important\nfor understanding cellular functions and biological processes of underlying\ndiseases. Among imaging techniques, high-throughput fluorescence microscopy\nimaging is an efficient biotechnology to stain the protein of interest in a\ncell. In this work, we present a novel classification model Twin U-Net (TUNet)\nfor processing and classifying the belonging of protein in the Atlas images.\nSeveral notable Deep Learning models including GoogleNet and Resnet have been\nemployed for comparison. Results have shown that our system obtaining\ncompetitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 23:58:03 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Tian", "Yijun", ""]]}, {"id": "1901.11382", "submitter": "Monika Sharma", "authors": "Monika Sharma, Abhishek Verma, Lovekesh Vig", "title": "Learning to Clean: A GAN Perspective", "comments": null, "journal-ref": "IWRR 2018 Workshop at ACCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the big data era, the impetus to digitize the vast reservoirs of data\ntrapped in unstructured scanned documents such as invoices, bank documents and\ncourier receipts has gained fresh momentum. The scanning process often results\nin the introduction of artifacts such as background noise, blur due to camera\nmotion, watermarkings, coffee stains, or faded text. These artifacts pose many\nreadability challenges to current text recognition algorithms and significantly\ndegrade their performance. Existing learning based denoising techniques require\na dataset comprising of noisy documents paired with cleaned versions. In such\nscenarios, a model can be trained to generate clean documents from noisy\nversions. However, very often in the real world such a paired dataset is not\navailable, and all we have for training our denoising model are unpaired sets\nof noisy and clean images. This paper explores the use of GANs to generate\ndenoised versions of the noisy documents. In particular, where paired\ninformation is available, we formulate the problem as an image-to-image\ntranslation task i.e, translating a document from noisy domain ( i.e.,\nbackground noise, blurred, faded, watermarked ) to a target clean document\nusing Generative Adversarial Networks (GAN). However, in the absence of paired\nimages for training, we employed CycleGAN which is known to learn a mapping\nbetween the distributions of the noisy images to the denoised images using\nunpaired data to achieve image-to-image translation for cleaning the noisy\ndocuments. We compare the performance of CycleGAN for document cleaning tasks\nusing unpaired images with a Conditional GAN trained on paired data from the\nsame dataset. Experiments were performed on a public document dataset on which\ndifferent types of noise were artificially induced, results demonstrate that\nCycleGAN learns a more robust mapping from the space of noisy to clean\ndocuments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 09:50:54 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Sharma", "Monika", ""], ["Verma", "Abhishek", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1901.11383", "submitter": "Monika Sharma", "authors": "Rohit Rahul, Shubham Paliwal, Monika Sharma, Lovekesh Vig", "title": "Automatic Information Extraction from Piping and Instrumentation\n  Diagrams", "comments": null, "journal-ref": "IEEE ICPRAM 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common modes of representing engineering schematics are\nPiping and Instrumentation diagrams (P&IDs) that describe the layout of an\nengineering process flow along with the interconnected process equipment. Over\nthe years, P&ID diagrams have been manually generated, scanned and stored as\nimage files. These files need to be digitized for purposes of inventory\nmanagement and updation, and easy reference to different components of the\nschematics. There are several challenging vision problems associated with\ndigitizing real world P&ID diagrams. Real world P&IDs come in several different\nresolutions, and often contain noisy textual information. Extraction of\ninstrumentation information from these diagrams involves accurate detection of\nsymbols that frequently have minute visual differences between them.\nIdentification of pipelines that may converge and diverge at different points\nin the image is a further cause for concern. Due to these reasons, to the best\nof our knowledge, no system has been proposed for end-to-end data extraction\nfrom P&ID diagrams. However, with the advent of deep learning and the\nspectacular successes it has achieved in vision, we hypothesized that it is now\npossible to re-examine this problem armed with the latest deep learning models.\nTo that end, we present a novel pipeline for information extraction from P&ID\nsheets via a combination of traditional vision techniques and state-of-the-art\ndeep learning models to identify and isolate pipeline codes, pipelines, inlets\nand outlets, and for detecting symbols. This is followed by association of the\ndetected components with the appropriate pipeline. The extracted pipeline\ninformation is used to populate a tree-like data-structure for capturing the\nstructure of the piping schematics. We evaluated proposed method on a real\nworld dataset of P&ID sheets obtained from an oil firm and have obtained\npromising results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 09:50:45 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Rahul", "Rohit", ""], ["Paliwal", "Shubham", ""], ["Sharma", "Monika", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1901.11384", "submitter": "Isabela Maria Carneiro de Albuquerque", "authors": "Isabela Albuquerque, Jo\\~ao Monteiro, Tiago H. Falk", "title": "Learning to navigate image manifolds induced by generative adversarial\n  networks for unsupervised video generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a two-step framework for generative modeling of\ntemporal data. Specifically, the generative adversarial networks (GANs) setting\nis employed to generate synthetic scenes of moving objects. To do so, we\npropose a two-step training scheme within which: a generator of static frames\nis trained first. Afterwards, a recurrent model is trained with the goal of\nproviding a sequence of inputs to the previously trained frames generator, thus\nyielding scenes which look natural. The adversarial setting is employed in both\ntraining steps. However, with the aim of avoiding known training instabilities\nin GANs, a multiple discriminator approach is used to train both models.\nResults in the studied video dataset indicate that, by employing such an\napproach, the recurrent part is able to learn how to coherently navigate the\nimage manifold induced by the frames generator, thus yielding more\nnatural-looking scenes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 23:21:08 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Albuquerque", "Isabela", ""], ["Monteiro", "Jo\u00e3o", ""], ["Falk", "Tiago H.", ""]]}, {"id": "1901.11388", "submitter": "Zhihao Cao", "authors": "Zhihao Cao, Xinxin Zhang", "title": "Tree Recognition APP of Mount Tai Based on CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mount Tai has abundant sunshine, abundant rainfall and favorable climatic\nconditions, forming dense vegetation with various kinds of trees. In order to\nmake it easier for tourists to understand each tree and experience the culture\nof Mount Tai, this paper develops an App for tree recognition of Mount Tai\nbased on convolution neural network (CNN), taking advantage of CNN efficient\nimage recognition ability and easy-to-carry characteristics of Android mobile\nphone. The APP can accurately identify several common trees in Mount Tai, and\ngive a brief introduction for tourists.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:37:35 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Cao", "Zhihao", ""], ["Zhang", "Xinxin", ""]]}, {"id": "1901.11390", "submitter": "Christopher Burgess", "authors": "Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,\n  Irina Higgins, Matt Botvinick, Alexander Lerchner", "title": "MONet: Unsupervised Scene Decomposition and Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to decompose scenes in terms of abstract building blocks is\ncrucial for general intelligence. Where those basic building blocks share\nmeaningful properties, interactions and other regularities across scenes, such\ndecompositions can simplify reasoning and facilitate imagination of novel\nscenarios. In particular, representing perceptual observations in terms of\nentities should improve data efficiency and transfer performance on a wide\nrange of tasks. Thus we need models capable of discovering useful\ndecompositions of scenes by identifying units with such regularities and\nrepresenting them in a common format. To address this problem, we have\ndeveloped the Multi-Object Network (MONet). In this model, a VAE is trained\nend-to-end together with a recurrent attention network -- in a purely\nunsupervised manner -- to provide attention masks around, and reconstructions\nof, regions of images. We show that this model is capable of learning to\ndecompose and represent challenging 3D scenes into semantically meaningful\ncomponents, such as objects and background elements.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:55:34 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Burgess", "Christopher P.", ""], ["Matthey", "Loic", ""], ["Watters", "Nicholas", ""], ["Kabra", "Rishabh", ""], ["Higgins", "Irina", ""], ["Botvinick", "Matt", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1901.11391", "submitter": "Sina Shahhosseini", "authors": "Sina Shahhosseini, Ahmad Albaqsami, Masoomeh Jasemi, Nader Bagherzadeh", "title": "Partition Pruning: Parallelization-Aware Pruning for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameters of recent neural networks require a huge amount of memory. These\nparameters are used by neural networks to perform machine learning tasks when\nprocessing inputs. To speed up inference, we develop Partition Pruning, an\ninnovative scheme to reduce the parameters used while taking into consideration\nparallelization. We evaluated the performance and energy consumption of\nparallel inference of partitioned models, which showed a 7.72x speed up of\nperformance and a 2.73x reduction in the energy used for computing pruned\nlayers of TinyVGG16 in comparison to running the unpruned model on a single\naccelerator. In addition, our method showed a limited reduction some numbers in\naccuracy while partitioning fully connected layers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 19:34:21 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 07:13:53 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Shahhosseini", "Sina", ""], ["Albaqsami", "Ahmad", ""], ["Jasemi", "Masoomeh", ""], ["Bagherzadeh", "Nader", ""]]}, {"id": "1901.11396", "submitter": "Hadi Amirpour", "authors": "Hadi Amirpour, Antonio Pinheiro, Manuela Pereira, and Mohammad\n  Ghanbari", "title": "Fast and Efficient Lenslet Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging is characterized by capturing brightness, color, and\ndirectional information of light rays in a scene. This leads to image\nrepresentations with huge amount of data that require efficient coding schemes.\nIn this paper, lenslet images are rendered into sub-aperture images. These\nimages are organized as a pseudo-sequence input for the HEVC video codec. To\nbetter exploit redundancy among the neighboring sub-aperture images and\nconsequently decrease the distances between a sub-aperture image and its\nreferences used for prediction, sub-aperture images are divided into four\nsmaller groups that are scanned in a serpentine order. The most central\nsub-aperture image, which has the highest similarity to all the other images,\nis used as the initial reference image for each of the four regions.\nFurthermore, a structure is defined that selects spatially adjacent\nsub-aperture images as prediction references with the highest similarity to the\ncurrent image. In this way, encoding efficiency increases, and furthermore it\nleads to a higher similarity among the co-located Coding Three Units (CTUs).\nThe similarities among the co-located CTUs are exploited to predict Coding Unit\ndepths.Moreover, independent encoding of each group division enables parallel\nprocessing, that along with the proposed coding unit depth prediction decrease\nthe encoding execution time by almost 80% on average. Simulation results show\nthat Rate-Distortion performance of the proposed method has higher compression\ngain than the other state-of-the-art lenslet compression methods with lower\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 12:14:58 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Amirpour", "Hadi", ""], ["Pinheiro", "Antonio", ""], ["Pereira", "Manuela", ""], ["Ghanbari", "Mohammad", ""]]}, {"id": "1901.11397", "submitter": "Abby Stylianou", "authors": "Abby Stylianou, Hong Xuan, Maya Shende, Jonathan Brandt, Richard\n  Souvenir, Robert Pless", "title": "Hotels-50K: A Global Hotel Recognition Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing a hotel from an image of a hotel room is important for human\ntrafficking investigations. Images directly link victims to places and can help\nverify where victims have been trafficked, and where their traffickers might\nmove them or others in the future. Recognizing the hotel from images is\nchallenging because of low image quality, uncommon camera perspectives, large\nocclusions (often the victim), and the similarity of objects (e.g., furniture,\nart, bedding) across different hotel rooms.\n  To support efforts towards this hotel recognition task, we have curated a\ndataset of over 1 million annotated hotel room images from 50,000 hotels. These\nimages include professionally captured photographs from travel websites and\ncrowd-sourced images from a mobile application, which are more similar to the\ntypes of images analyzed in real-world investigations. We present a baseline\napproach based on a standard network architecture and a collection of\ndata-augmentation approaches tuned to this problem domain.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 22:22:46 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Stylianou", "Abby", ""], ["Xuan", "Hong", ""], ["Shende", "Maya", ""], ["Brandt", "Jonathan", ""], ["Souvenir", "Richard", ""], ["Pless", "Robert", ""]]}, {"id": "1901.11398", "submitter": "Zahra Sadeghi", "authors": "Zahra Sadeghi", "title": "Visual Categorization of Objects into Animal and Plant Classes Using\n  Global Shape Descriptors", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How humans can distinguish between general categories of objects? Are the\nsubcategories of living things visually distinctive? In a number of\nsemantic-category deficits, patients are good at making broad categorization\nbut are unable to remember fine and specific details. It has been well accepted\nthat general information about concepts are more robust to damages related to\nsemantic memory. Results from patients with semantic memory disorders\ndemonstrate the loss of ability in subcategory recognition. While bottom-up\nfeature construction has been studied in detail, little attention has been\nserved to top-down approach and the type of features that could account for\ngeneral categorization. In this paper, we show that broad categories of animal\nand plant are visually distinguishable without processing textural information.\nTo this aim, we utilize shape descriptors with an additional phase of feature\nlearning. The results are evaluated with both supervised and unsupervised\nlearning mechanisms. The obtained results demonstrate that global encoding of\nvisual appearance of objects accounts for high discrimination between animal\nand plant object categories.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 23:57:36 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Sadeghi", "Zahra", ""]]}, {"id": "1901.11399", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Peter Bailis, Gregory Valiant", "title": "Equivariant Transformer Networks", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can prior knowledge on the transformation invariances of a domain be\nincorporated into the architecture of a neural network? We propose Equivariant\nTransformers (ETs), a family of differentiable image-to-image mappings that\nimprove the robustness of models towards pre-defined continuous transformation\ngroups. Through the use of specially-derived canonical coordinate systems, ETs\nincorporate functions that are equivariant by construction with respect to\nthese transformations. We show empirically that ETs can be flexibly composed to\nimprove model robustness towards more complicated transformation groups in\nseveral parameters. On a real-world image classification task, ETs improve the\nsample efficiency of ResNet classifiers, achieving relative improvements in\nerror rate of up to 15% in the limited data regime while increasing model\nparameter count by less than 1%.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 22:29:48 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 20:36:43 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Bailis", "Peter", ""], ["Valiant", "Gregory", ""]]}, {"id": "1901.11409", "submitter": "Vighnesh Birodkar", "authors": "Vighnesh Birodkar, Hossein Mobahi, Samy Bengio", "title": "Semantic Redundancies in Image-Classification Datasets: The 10% You\n  Don't Need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets have been crucial to the success of deep learning models in\nthe recent years, which keep performing better as they are trained with more\nlabelled data. While there have been sustained efforts to make these models\nmore data-efficient, the potential benefit of understanding the data itself, is\nlargely untapped. Specifically, focusing on object recognition tasks, we wonder\nif for common benchmark datasets we can do better than random subsets of the\ndata and find a subset that can generalize on par with the full dataset when\ntrained on. To our knowledge, this is the first result that can find notable\nredundancies in CIFAR-10 and ImageNet datasets (at least 10%). Interestingly,\nwe observe semantic correlations between required and redundant images. We hope\nthat our findings can motivate further research into identifying additional\nredundancies and exploiting them for more efficient training or\ndata-collection.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:27:37 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Birodkar", "Vighnesh", ""], ["Mobahi", "Hossein", ""], ["Bengio", "Samy", ""]]}, {"id": "1901.11420", "submitter": "Ayellet Tal", "authors": "Shay Perera and Ayellet Tal and Lihi Zelnik-Manor", "title": "Is Image Memorability Prediction Solved?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the prediction of the memorability of a given image. We\nstart by proposing an algorithm that reaches human-level performance on the\nLaMem dataset - the only large scale benchmark for memorability prediction. The\nsuggested algorithm is based on three observations we make regarding\nconvolutional neural networks (CNNs) that affect memorability prediction.\nHaving reached human-level performance we were humbled, and asked ourselves\nwhether indeed we have resolved memorability prediction - and answered this\nquestion in the negative. We studied a few factors and made some\nrecommendations that should be taken into account when designing the next\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 15:15:27 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Perera", "Shay", ""], ["Tal", "Ayellet", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1901.11447", "submitter": "Jason Wei", "authors": "Jason W. Wei, Jerry W. Wei, Christopher R. Jackson, Bing Ren, Arief A.\n  Suriawinata, Saeed Hassanpour", "title": "Automated detection of celiac disease on duodenal biopsy slides: a deep\n  learning approach", "comments": "Accepted in Journal of Pathology Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Celiac disease prevalence and diagnosis have increased substantially in\nrecent years. The current gold standard for celiac disease confirmation is\nvisual examination of duodenal mucosal biopsies. An accurate computer-aided\nbiopsy analysis system using deep learning can help pathologists diagnose\nceliac disease more efficiently. In this study, we trained a deep learning\nmodel to detect celiac disease on duodenal biopsy images. Our model uses a\nstate-of-the-art residual convolutional neural network to evaluate patches of\nduodenal tissue and then aggregates those predictions for whole-slide\nclassification. We tested the model on an independent set of 212 images and\nevaluated its classification results against reference standards established by\npathologists. Our model identified celiac disease, normal tissue, and\nnonspecific duodenitis with accuracies of 95.3%, 91.0%, and 89.2%,\nrespectively. The area under the receiver operating characteristic curve was\ngreater than 0.95 for all classes. We have developed an automated biopsy\nanalysis system that achieves high performance in detecting celiac disease on\nbiopsy slides. Our system can highlight areas of interest and provide\npreliminary classification of duodenal biopsies prior to review by\npathologists. This technology has great potential for improving the accuracy\nand efficiency of celiac disease diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:06:53 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wei", "Jason W.", ""], ["Wei", "Jerry W.", ""], ["Jackson", "Christopher R.", ""], ["Ren", "Bing", ""], ["Suriawinata", "Arief A.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1901.11461", "submitter": "Edward Smith", "authors": "Edward J. Smith, Scott Fujimoto, Adriana Romero, David Meger", "title": "GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh models are a promising approach for encoding the structure of 3D\nobjects. Current mesh reconstruction systems predict uniformly distributed\nvertex locations of a predetermined graph through a series of graph\nconvolutions, leading to compromises with respect to performance or resolution.\nIn this paper, we argue that the graph representation of geometric objects\nallows for additional structure, which should be leveraged for enhanced\nreconstruction. Thus, we propose a system which properly benefits from the\nadvantages of the geometric structure of graph encoded objects by introducing\n(1) a graph convolutional update preserving vertex information; (2) an adaptive\nsplitting heuristic allowing detail to emerge; and (3) a training objective\noperating both on the local surfaces defined by vertices as well as the global\nstructure defined by the mesh. Our proposed method is evaluated on the task of\n3D object reconstruction from images with the ShapeNet dataset, where we\ndemonstrate state of the art performance, both visually and numerically, while\nhaving far smaller space requirements by generating adaptive meshes\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:37:38 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Smith", "Edward J.", ""], ["Fujimoto", "Scott", ""], ["Romero", "Adriana", ""], ["Meger", "David", ""]]}, {"id": "1901.11464", "submitter": "Caixia Zhang", "authors": "Bo Wang, Hao Hu, Caixia Zhang", "title": "New insights on Multi-Solution Distribution of the P3P Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the P3P problem is solved by firstly transforming its 3\nquadratic equations into a quartic one, then by locating the roots of the\nresulting quartic equation and verifying whether a root does really correspond\nto a true solution of the P3P problem itself. However, a root of the quartic\nequation does not always correspond to a solution of the P3P problem. In this\nwork, we show that when the optical center is outside of all the 6 toroids\ndefined by the control point triangle, each positive root of the Grunert's\nquartic equation must correspond to a true solution of the P3P problem, and the\ncorresponding P3P problem cannot have a unique solution, it must have either 2\npositive solutions or 4 positive solutions. In addition, we show that when the\noptical center passes through any one of the 3 toroids among these 6 toroids (\nexcept possibly for two concentric circles) , the number of the solutions of\nthe corresponding P3P problem always changes by 1, either increased by 1 or\ndecreased by 1.Furthermore we show that such changed solutions always locate in\na small neighborhood of control points, hence the 3 toroids are critical\nsurfaces of the P3P problem and the 3 control points are 3 singular points of\nsolutions. A notable example is that when the optical center passes through the\nouter surface of the union of the 6 toroids from the outside to inside, the\nnumber of the solutions must always decrease by 1. Our results are the first to\ngive an explicit and geometrically intuitive relationship between the P3P\nsolutions and the roots of its quartic equation. It could act as some\ntheoretical guidance for P3P practitioners to properly arrange their control\npoints to avoid undesirable solutions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:34:49 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wang", "Bo", ""], ["Hu", "Hao", ""], ["Zhang", "Caixia", ""]]}, {"id": "1901.11477", "submitter": "Amarnath R", "authors": "Amarnath R and P Nagabhushan", "title": "Text line Segmentation in Compressed Representation of Handwritten\n  Document using Tunneling Algorithm", "comments": "Compressed Representation, Handwritten Document Image, Text-Line\n  Terminal Point, Text-Line Segmentation, Search Space, Grid", "journal-ref": "International Journal of Intelligent Systems and Applications in\n  Engineering, Vol 6, No 4 (2018)", "doi": "10.18201/ijisae.2018448451", "report-no": null, "categories": "cs.CV cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research work, we perform text line segmentation directly in\ncompressed representation of an unconstrained handwritten document image. In\nthis relation, we make use of text line terminal points which is the current\nstate-of-the-art. The terminal points spotted along both margins (left and\nright) of a document image for every text line are considered as source and\ntarget respectively. The tunneling algorithm uses a single agent (or robot) to\nidentify the coordinate positions in the compressed representation to perform\ntext-line segmentation of the document. The agent starts at a source point and\nprogressively tunnels a path routing in between two adjacent text lines and\nreaches the probable target. The agent's navigation path from source to the\ntarget bypassing obstacles, if any, results in segregating the two adjacent\ntext lines. However, the target point would be known only when the agent\nreaches the destination; this is applicable for all source points and\nhenceforth we could analyze the correspondence between source and target nodes.\nArtificial Intelligence in Expert systems, dynamic programming and greedy\nstrategies are employed for every search space while tunneling. An exhaustive\nexperimentation is carried out on various benchmark datasets including ICDAR13\nand the performances are reported.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 05:19:38 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["R", "Amarnath", ""], ["Nagabhushan", "P", ""]]}, {"id": "1901.11489", "submitter": "Jason Wei", "authors": "Jason W. Wei, Laura J. Tafe, Yevgeniy A. Linnik, Louis J. Vaickus,\n  Naofumi Tomita, and Saeed Hassanpour", "title": "Pathologist-level classification of histologic patterns on resected lung\n  adenocarcinoma slides with deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of histologic patterns in lung adenocarcinoma is critical for\ndetermining tumor grade and treatment for patients. However, this task is often\nchallenging due to the heterogeneous nature of lung adenocarcinoma and the\nsubjective criteria for evaluation. In this study, we propose a deep learning\nmodel that automatically classifies the histologic patterns of lung\nadenocarcinoma on surgical resection slides. Our model uses a convolutional\nneural network to identify regions of neoplastic cells, then aggregates those\nclassifications to infer predominant and minor histologic patterns for any\ngiven whole-slide image. We evaluated our model on an independent set of 143\nwhole-slide images. It achieved a kappa score of 0.525 and an agreement of\n66.6% with three pathologists for classifying the predominant patterns,\nslightly higher than the inter-pathologist kappa score of 0.485 and agreement\nof 62.7% on this test set. All evaluation metrics for our model and the three\npathologists were within 95% confidence intervals of agreement. If confirmed in\nclinical practice, our model can assist pathologists in improving\nclassification of lung adenocarcinoma patterns by automatically pre-screening\nand highlighting cancerous regions prior to review. Our approach can be\ngeneralized to any whole-slide image classification task, and code is made\npublicly available at https://github.com/BMIRDS/deepslide.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 17:34:46 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wei", "Jason W.", ""], ["Tafe", "Laura J.", ""], ["Linnik", "Yevgeniy A.", ""], ["Vaickus", "Louis J.", ""], ["Tomita", "Naofumi", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1901.11494", "submitter": "Quanshi Zhang", "authors": "Xianglei Xing, Song-Chun Zhu, Ying Nian Wu", "title": "Inducing Sparse Coding and And-Or Grammar from Generator Network", "comments": "In AAAI-19 Workshop on Network Interpretability for Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an explainable generative model by applying sparse operation on\nthe feature maps of the generator network. Meaningful hierarchical\nrepresentations are obtained using the proposed generative model with sparse\nactivations. The convolutional kernels from the bottom layer to the top layer\nof the generator network can learn primitives such as edges and colors, object\nparts, and whole objects layer by layer. From the perspective of the generator\nnetwork, we propose a method for inducing both sparse coding and the AND-OR\ngrammar for images. Experiments show that our method is capable of learning\nmeaningful and explainable hierarchical representations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 18:47:18 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Xing", "Xianglei", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}]