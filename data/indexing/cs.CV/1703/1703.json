[{"id": "1703.00035", "submitter": "Bernhard Kainz", "authors": "Steven McDonagh, Benjamin Hou, Konstantinos Kamnitsas, Ozan Oktay,\n  Amir Alansary, Mary Rutherford, Jo V. Hajnal, and Bernhard Kainz", "title": "Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance\n  Imaging", "comments": "11 pages, 6 figures, published in Proc MICCAI RAMBO'17\n  https://link.springer.com/chapter/10.1007/978-3-319-67564-0_12", "journal-ref": "Springer LNCS 10555 2017", "doi": "10.1007/978-3-319-67564-0_12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Magnetic Resonance Imaging (MRI) is often a trade-off between fast but\nlow-resolution image acquisition and highly detailed but slow image\nacquisition. Fast imaging is required for targets that move to avoid motion\nartefacts. This is in particular difficult for fetal MRI. Spatially independent\nupsampling techniques, which are the state-of-the-art to address this problem,\nare error prone and disregard contextual information. In this paper we propose\na context-sensitive upsampling method based on a residual convolutional neural\nnetwork model that learns organ specific appearance and adopts semantically to\ninput data allowing for the generation of high resolution images with sharp\nedges and fine scale detail. By making contextual decisions about appearance\nand shape, present in different parts of an image, we gain a maximum of\nstructural detail at a similar contrast as provided by high-resolution data. We\nexperiment on $145$ fetal scans and show that our approach yields an increased\nPSNR of $1.25$ $dB$ when applied to under-sampled fetal data \\emph{cf.}\nbaseline upsampling. Furthermore, our method yields an increased PSNR of $1.73$\n$dB$ when utilizing under-sampled fetal data to perform brain volume\nreconstruction on motion corrupted captured data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 19:36:34 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 14:47:40 GMT"}, {"version": "v3", "created": "Sat, 23 Sep 2017 10:21:06 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["McDonagh", "Steven", ""], ["Hou", "Benjamin", ""], ["Kamnitsas", "Konstantinos", ""], ["Oktay", "Ozan", ""], ["Alansary", "Amir", ""], ["Rutherford", "Mary", ""], ["Hajnal", "Jo V.", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1703.00069", "submitter": "Yi-Hsuan Tsai", "authors": "Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu,\n  Ming-Hsuan Yang", "title": "Deep Image Harmonization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositing is one of the most common operations in photo editing. To\ngenerate realistic composites, the appearances of foreground and background\nneed to be adjusted to make them compatible. Previous approaches to harmonize\ncomposites have focused on learning statistical relationships between\nhand-crafted appearance features of the foreground and background, which is\nunreliable especially when the contents in the two layers are vastly different.\nIn this work, we propose an end-to-end deep convolutional neural network for\nimage harmonization, which can capture both the context and semantic\ninformation of the composite images during harmonization. We also introduce an\nefficient way to collect large-scale and high-quality training data that can\nfacilitate the training process. Experiments on the synthesized dataset and\nreal composite images show that the proposed network outperforms previous\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:58:45 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Tsai", "Yi-Hsuan", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Sunkavalli", "Kalyan", ""], ["Lu", "Xin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1703.00075", "submitter": "Rachid Haddadi", "authors": "Rachid Haddadi, Elhassane Abdelmounim, Mustapha El Hanine, Abdelaziz\n  Belaguid", "title": "Discrete Wavelet Transform Based Algorithm for Recognition of QRS\n  Complexes", "comments": null, "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT) ; ISSN: 2221-0741; Vol. 4, No. 9, 127-132, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the application of Discrete Wavelet Transform (DWT) to\ndetect the QRS (ECG is characterized by a recurrent wave sequence of P, QRS and\nT-wave) of an electrocardiogram (ECG) signal. Wavelet Transform provides\nlocalization in both time and frequency. In preprocessing stage, DWT is used to\nremove the baseline wander in the ECG signal. The performance of the algorithm\nof QRS detection is evaluated against the standard MIT BIH (Massachusetts\nInstitute of Technology, Beth Israel Hospital) Arrhythmia database. The average\nQRS complexes detection rate of 98.1 % is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 22:12:00 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Haddadi", "Rachid", ""], ["Abdelmounim", "Elhassane", ""], ["Hanine", "Mustapha El", ""], ["Belaguid", "Abdelaziz", ""]]}, {"id": "1703.00087", "submitter": "Mostafa Jahanifar", "authors": "Mostafa Jahanifar, Neda Zamani Tajeddin, Babak Mohammadzadeh Asl, Ali\n  Gooya", "title": "Supervised Saliency Map Driven Segmentation of the Lesions in\n  Dermoscopic Images", "comments": "ISIC2017, JBHI", "journal-ref": null, "doi": "10.1109/JBHI.2018.2839647", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 23:27:25 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 10:09:32 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 22:20:16 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 06:47:19 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Jahanifar", "Mostafa", ""], ["Tajeddin", "Neda Zamani", ""], ["Asl", "Babak Mohammadzadeh", ""], ["Gooya", "Ali", ""]]}, {"id": "1703.00121", "submitter": "Gong Cheng", "authors": "Gong Cheng, Junwei Han, and Xiaoqiang Lu", "title": "Remote Sensing Image Scene Classification: Benchmark and State of the\n  Art", "comments": "This manuscript is the accepted version for Proceedings of the IEEE", "journal-ref": "Proceedings of the IEEE, 105 (10): 1865-1883, 2017", "doi": "10.1109/JPROC.2017.2675998", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Remote sensing image scene classification plays an important role in a wide\nrange of applications and hence has been receiving remarkable attention. During\nthe past years, significant efforts have been made to develop various datasets\nor present a variety of approaches for scene classification from remote sensing\nimages. However, a systematic review of the literature concerning datasets and\nmethods for scene classification is still lacking. In addition, almost all\nexisting datasets have a number of limitations, including the small scale of\nscene classes and the image numbers, the lack of image variations and\ndiversity, and the saturation of accuracy. These limitations severely limit the\ndevelopment of new approaches especially deep learning-based methods. This\npaper first provides a comprehensive review of the recent progress. Then, we\npropose a large-scale dataset, termed \"NWPU-RESISC45\", which is a publicly\navailable benchmark for REmote Sensing Image Scene Classification (RESISC),\ncreated by Northwestern Polytechnical University (NWPU). This dataset contains\n31,500 images, covering 45 scene classes with 700 images in each class. The\nproposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total\nimage number, (ii) holds big variations in translation, spatial resolution,\nviewpoint, object pose, illumination, background, and occlusion, and (iii) has\nhigh within-class diversity and between-class similarity. The creation of this\ndataset will enable the community to develop and evaluate various data-driven\nalgorithms. Finally, several representative methods are evaluated using the\nproposed dataset and the results are reported as a useful baseline for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 03:38:13 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Cheng", "Gong", ""], ["Han", "Junwei", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1703.00122", "submitter": "Hao Chen", "authors": "Hao Chen, Y.F. Li, and Dan Su", "title": "RGB-D Salient Object Detection Based on Discriminative Cross-modal\n  Transfer Learning", "comments": "This paper has been rejected by CVPR2017, we plan to withdraw this\n  manuscript for further revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose to utilize Convolutional Neural Networks to boost\nthe performance of depth-induced salient object detection by capturing the\nhigh-level representative features for depth modality. We formulate the\ndepth-induced saliency detection as a CNN-based cross-modal transfer problem to\nbridge the gap between the \"data-hungry\" nature of CNNs and the unavailability\nof sufficient labeled training data in depth modality. In the proposed\napproach, we leverage the auxiliary data from the source modality effectively\nby training the RGB saliency detection network to obtain the task-specific\npre-understanding layers for the target modality. Meanwhile, we exploit the\ndepth-specific information by pre-training a modality classification network\nthat encourages modal-specific representations during the optimizing course.\nThus, it could make the feature representations of the RGB and depth modalities\nas discriminative as possible. These two modules are pre-trained independently\nand then stitched to initialize and optimize the eventual depth-induced\nsaliency detection model. Experiments demonstrate the effectiveness of the\nproposed novel pre-training strategy as well as the significant and consistent\nimprovements of the proposed approach over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 03:38:53 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 03:51:50 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Chen", "Hao", ""], ["Li", "Y. F.", ""], ["Su", "Dan", ""]]}, {"id": "1703.00144", "submitter": "Liang Zhao", "authors": "Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan and\n  Bo Yuan", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low\n  Displacement Rank", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently low displacement rank (LDR) matrices, or so-called structured\nmatrices, have been proposed to compress large-scale neural networks. Empirical\nresults have shown that neural networks with weight matrices of LDR matrices,\nreferred as LDR neural networks, can achieve significant reduction in space and\ncomputational complexity while retaining high accuracy. We formally study LDR\nmatrices in deep learning. First, we prove the universal approximation property\nof LDR neural networks with a mild condition on the displacement operators. We\nthen show that the error bounds of LDR neural networks are as efficient as\ngeneral neural networks with both single-layer and multiple-layer structure.\nFinally, we propose back-propagation based training algorithm for general LDR\nneural networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 05:38:16 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 16:15:40 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 15:57:19 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 01:53:39 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Zhao", "Liang", ""], ["Liao", "Siyu", ""], ["Wang", "Yanzhi", ""], ["Li", "Zhe", ""], ["Tang", "Jian", ""], ["Pan", "Victor", ""], ["Yuan", "Bo", ""]]}, {"id": "1703.00152", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu, Chi Zhang, Wataru Shimoda, Yuming Fang, Boxin Shi", "title": "Saliency Detection by Forward and Backward Cues in Deep-CNNs", "comments": "5 pages,4 figures,and 1 table. the content of this work is accepted\n  for ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As prior knowledge of objects or object features helps us make relations for\nsimilar objects on attentional tasks, pre-trained deep convolutional neural\nnetworks (CNNs) can be used to detect salient objects on images regardless of\nthe object class is in the network knowledge or not. In this paper, we propose\na top-down saliency model using CNN, a weakly supervised CNN model trained for\n1000 object labelling task from RGB images. The model detects attentive regions\nbased on their objectness scores predicted by selected features from CNNs. To\nestimate the salient objects effectively, we combine both forward and backward\nfeatures, while demonstrating that partially-guided backpropagation will\nprovide sufficient information for selecting the features from forward run of\nCNN model. Finally, these top-down cues are enhanced with a state-of-the-art\nbottom-up model as complementing the overall saliency. As the proposed model is\nan effective integration of forward and backward cues through objectness\nwithout any supervision or regression to ground truth data, it gives promising\nresults compared to state-of-the-art models in two different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 06:56:37 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 09:04:55 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Zhang", "Chi", ""], ["Shimoda", "Wataru", ""], ["Fang", "Yuming", ""], ["Shi", "Boxin", ""]]}, {"id": "1703.00154", "submitter": "Arno Solin", "authors": "Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala", "title": "Inertial Odometry on Handheld Smartphones", "comments": "Appearing in Proceedings of the International Conference on\n  Information Fusion (FUSION 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a complete inertial navigation system using the limited quality data\nprovided by current smartphones has been regarded challenging, if not\nimpossible. This paper shows that by careful crafting and accounting for the\nweak information in the sensor samples, smartphones are capable of pure\ninertial navigation. We present a probabilistic approach for orientation and\nuse-case free inertial odometry, which is based on double-integrating rotated\naccelerations. The strength of the model is in learning additive and\nmultiplicative IMU biases online. We are able to track the phone position,\nvelocity, and pose in real-time and in a computationally lightweight fashion by\nsolving the inference with an extended Kalman filter. The information fusion is\ncompleted with zero-velocity updates (if the phone remains stationary),\naltitude correction from barometric pressure readings (if available), and\npseudo-updates constraining the momentary speed. We demonstrate our approach\nusing an iPad and iPhone in several indoor dead-reckoning applications and in a\nmeasurement tool setup.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 07:00:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 20:40:20 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Solin", "Arno", ""], ["Cortes", "Santiago", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1703.00160", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu, Zhixuan Wei, Huangjun Shi, Yuki Yoshida, Myagmarbayar\n  Nergui, Jose Gonzalez, Dongyun Gu, Weidong Chen, Kenzo Nonami, Wenwei Yu", "title": "Saliency Fusion in Eigenvector Space with Multi-Channel Pulse Coupled\n  Neural Network", "comments": "8 pages, 9 figures, 1 table. This submission includes detailed\n  explanation of partial section (saliency detection) of the work \"An Improved\n  Saliency for RGB-D Visual Tracking and Control Strategies for a\n  Bio-monitoring Mobile Robot\", Evaluating AAL Systems Through Competitive\n  Benchmarking, Communications in Computer and Information Science, vol. 386,\n  pp.1-12, 2013", "journal-ref": "Evaluating AAL Systems Through Competitive Benchmarking,\n  Communications in Computer and Information Science, 2013", "doi": "10.1007/978-3-642-41043-7_1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency computation has become a popular research field for many\napplications due to the useful information provided by saliency maps. For a\nsaliency map, local relations around the salient regions in multi-channel\nperspective should be taken into consideration by aiming uniformity on the\nregion of interest as an internal approach. And, irrelevant salient regions\nhave to be avoided as much as possible. Most of the works achieve these\ncriteria with external processing modules; however, these can be accomplished\nduring the conspicuity map fusion process. Therefore, in this paper, a new\nmodel is proposed for saliency/conspicuity map fusion with two concepts: a)\ninput image transformation relying on the principal component analysis (PCA),\nand b) saliency conspicuity map fusion with multi-channel pulsed coupled neural\nnetwork (m-PCNN). Experimental results, which are evaluated by precision,\nrecall, F-measure, and area under curve (AUC), support the reliability of the\nproposed method by enhancing the saliency computation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 07:25:41 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Wei", "Zhixuan", ""], ["Shi", "Huangjun", ""], ["Yoshida", "Yuki", ""], ["Nergui", "Myagmarbayar", ""], ["Gonzalez", "Jose", ""], ["Gu", "Dongyun", ""], ["Chen", "Weidong", ""], ["Nonami", "Kenzo", ""], ["Yu", "Wenwei", ""]]}, {"id": "1703.00177", "submitter": "Thiemo Alldieck", "authors": "Thiemo Alldieck, Marc Kassubeck, Marcus Magnor", "title": "Optical Flow-based 3D Human Motion Estimation from Monocular Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative method to estimate 3D human motion and body shape\nfrom monocular video. Under the assumption that starting from an initial pose\noptical flow constrains subsequent human motion, we exploit flow to find\ntemporally coherent human poses of a motion sequence. We estimate human motion\nby minimizing the difference between computed flow fields and the output of an\nartificial flow renderer. A single initialization step is required to estimate\nmotion over multiple frames. Several regularization functions enhance\nrobustness over time. Our test scenarios demonstrate that optical flow\neffectively regularizes the under-constrained problem of human shape and motion\nestimation from monocular video.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 08:29:09 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 12:24:53 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Alldieck", "Thiemo", ""], ["Kassubeck", "Marc", ""], ["Magnor", "Marcus", ""]]}, {"id": "1703.00196", "submitter": "Yihang Lou", "authors": "Yan Bai, Feng Gao, Yihang Lou, Shiqi Wang, Tiejun Huang, Ling-Yu Duan", "title": "Incorporating Intra-Class Variance to Fine-Grained Visual Recognition", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual recognition aims to capture discriminative\ncharacteristics amongst visually similar categories. The state-of-the-art\nresearch work has significantly improved the fine-grained recognition\nperformance by deep metric learning using triplet network. However, the impact\nof intra-category variance on the performance of recognition and robust feature\nrepresentation has not been well studied. In this paper, we propose to leverage\nintra-class variance in metric learning of triplet network to improve the\nperformance of fine-grained recognition. Through partitioning training images\nwithin each category into a few groups, we form the triplet samples across\ndifferent categories as well as different groups, which is called Group\nSensitive TRiplet Sampling (GS-TRS). Accordingly, the triplet loss function is\nstrengthened by incorporating intra-class variance with GS-TRS, which may\ncontribute to the optimization objective of triplet network. Extensive\nexperiments over benchmark datasets CompCar and VehicleID show that the\nproposed GS-TRS has significantly outperformed state-of-the-art approaches in\nboth classification and retrieval tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 09:41:02 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Bai", "Yan", ""], ["Gao", "Feng", ""], ["Lou", "Yihang", ""], ["Wang", "Shiqi", ""], ["Huang", "Tiejun", ""], ["Duan", "Ling-Yu", ""]]}, {"id": "1703.00234", "submitter": "Yihang Lou", "authors": "Feng Gao, Yihang Lou, Yan Bai, Shiqi Wang, Tiejun Huang, Ling-Yu Duan", "title": "Improving Object Detection with Region Similarity Learning", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection aims to identify instances of semantic objects of a certain\nclass in images or videos. The success of state-of-the-art approaches is\nattributed to the significant progress of object proposal and convolutional\nneural networks (CNNs). Most promising detectors involve multi-task learning\nwith an optimization objective of softmax loss and regression loss. The first\nis for multi-class categorization, while the latter is for improving\nlocalization accuracy. However, few of them attempt to further investigate the\nhardness of distinguishing different sorts of distracting background regions\n(i.e., negatives) from true object regions (i.e., positives). To improve the\nperformance of classifying positive object regions vs. a variety of negative\nbackground regions, we propose to incorporate triplet embedding into learning\nobjective. The triplet units are formed by assigning each negative region to a\nmeaningful object class and establishing class- specific negatives, followed by\ntriplets construction. Over the benchmark PASCAL VOC 2007, the proposed triplet\nem- bedding has improved the performance of well-known FastRCNN model with a\nmAP gain of 2.1%. In particular, the state-of-the-art approach OHEM can benefit\nfrom the triplet embedding and has achieved a mAP improvement of 1.2%.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 11:16:13 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Gao", "Feng", ""], ["Lou", "Yihang", ""], ["Bai", "Yan", ""], ["Wang", "Shiqi", ""], ["Huang", "Tiejun", ""], ["Duan", "Ling-Yu", ""]]}, {"id": "1703.00249", "submitter": "Carlos Del-R\\'io", "authors": "Adur Lagunas, Oier Dominguez, Susana Martinez-Conde, Stephen L.\n  Macknik, Carlos del-Rio", "title": "Human Eye Visual Hyperacuity: A New Paradigm for Sensing?", "comments": "8 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human eye appears to be using a low number of sensors for image\ncapturing. Furthermore, regarding the physical dimensions of\ncones-photoreceptors responsible for the sharp central vision-, we may realize\nthat these sensors are of a relatively small size and area. Nonetheless, the\neye is capable to obtain high resolution images due to visual hyperacuity and\npresents an impressive sensitivity and dynamic range when set against\nconventional digital cameras of similar characteristics. This article is based\non the hypothesis that the human eye may be benefiting from diffraction to\nimprove both image resolution and acquisition process. The developed method\nintends to explain and simulate using MATLAB software the visual hyperacuity:\nthe introduction of a controlled diffraction pattern at an initial stage,\nenables the use of a reduced number of sensors for capturing the image and\nmakes possible a subsequent processing to improve the final image resolution.\nThe results have been compared with the outcome of an equivalent system but in\nabsence of diffraction, achieving promising results. The main conclusion of\nthis work is that diffraction could be helpful for capturing images or signals\nwhen a small number of sensors available, which is far from being a\nresolution-limiting factor.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 11:48:32 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Lagunas", "Adur", ""], ["Dominguez", "Oier", ""], ["Martinez-Conde", "Susana", ""], ["Macknik", "Stephen L.", ""], ["del-Rio", "Carlos", ""]]}, {"id": "1703.00297", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Lan Tang and Xin Liu", "title": "Group Sparsity Residual Constraint for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group-based sparse representation has shown great potential in image\ndenoising. However, most existing methods only consider the nonlocal\nself-similarity (NSS) prior of noisy input image. That is, the similar patches\nare collected only from degraded input, which makes the quality of image\ndenoising largely depend on the input itself. However, such methods often\nsuffer from a common drawback that the denoising performance may degrade\nquickly with increasing noise levels. In this paper we propose a new prior\nmodel, called group sparsity residual constraint (GSRC). Unlike the\nconventional group-based sparse representation denoising methods, two kinds of\nprior, namely, the NSS priors of noisy and pre-filtered images, are used in\nGSRC. In particular, we integrate these two NSS priors through the mechanism of\nsparsity residual, and thus, the task of image denoising is converted to the\nproblem of reducing the group sparsity residual. To this end, we first obtain a\ngood estimation of the group sparse coefficients of the original image by\npre-filtering, and then the group sparse coefficients of the noisy image are\nused to approximate this estimation. To improve the accuracy of the nonlocal\nsimilar patch selection, an adaptive patch search scheme is designed.\nFurthermore, to fuse these two NSS prior better, an effective iterative\nshrinkage algorithm is developed to solve the proposed GSRC model. Experimental\nresults demonstrate that the proposed GSRC modeling outperforms many\nstate-of-the-art denoising methods in terms of the objective and the perceptual\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 13:52:40 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 20:33:43 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 03:36:27 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 06:56:49 GMT"}, {"version": "v5", "created": "Tue, 25 Apr 2017 08:47:22 GMT"}, {"version": "v6", "created": "Mon, 31 Jul 2017 16:42:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Zhang", "Xinggan", ""], ["Wang", "Qiong", ""], ["Tang", "Lan", ""], ["Liu", "Xin", ""]]}, {"id": "1703.00311", "submitter": "Masaharu Sakamoto", "authors": "Masaharu Sakamoto, Hiroki Nakano, Kun Zhao and Taro Sekiyama", "title": "Multi-stage Neural Networks with Single-sided Classifiers for False\n  Positive Reduction and its Evaluation using Lung X-ray CT Images", "comments": "arXiv admin note: text overlap with arXiv:1611.07136", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodule classification is a class imbalanced problem because nodules are\nfound with much lower frequency than non-nodules. In the class imbalanced\nproblem, conventional classifiers tend to be overwhelmed by the majority class\nand ignore the minority class. We therefore propose cascaded convolutional\nneural networks to cope with the class imbalanced problem. In the proposed\napproach, multi-stage convolutional neural networks that perform as\nsingle-sided classifiers filter out obvious non-nodules. Successively, a\nconvolutional neural network trained with a balanced data set calculates nodule\nprobabilities. The proposed method achieved the sensitivity of 92.4\\% and 94.5%\nat 4 and 8 false positives per scan in Free Receiver Operating Characteristics\n(FROC) curve analysis, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 14:24:42 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:14:55 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 08:20:05 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Sakamoto", "Masaharu", ""], ["Nakano", "Hiroki", ""], ["Zhao", "Kun", ""], ["Sekiyama", "Taro", ""]]}, {"id": "1703.00312", "submitter": "Raphael Meier", "authors": "Raphael Meier, Urspeter Knecht, Alain Jungo, Roland Wiest, Mauricio\n  Reyes", "title": "Perturb-and-MPM: Quantifying Segmentation Uncertainty in Dense\n  Multi-Label CRFs", "comments": "Deactivated review mode (line spacing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for uncertainty quantification in dense\nConditional Random Fields (CRFs). The presented approach, called\nPerturb-and-MPM, enables efficient, approximate sampling from dense multi-label\nCRFs via random perturbations. An analytic error analysis was performed which\nidentified the main cause of approximation error as well as showed that the\nerror is bounded. Spatial uncertainty maps can be derived from the\nPerturb-and-MPM model, which can be used to visualize uncertainty in image\nsegmentation results. The method is validated on synthetic and clinical\nMagnetic Resonance Imaging data. The effectiveness of the approach is\ndemonstrated on the challenging problem of segmenting the tumor core in\nglioblastoma. We found that areas of high uncertainty correspond well to\nwrongly segmented image regions. Furthermore, we demonstrate the potential use\nof uncertainty maps to refine imaging biomarkers in the case of extent of\nresection and residual tumor volume in brain tumor patients.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 14:26:29 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:04:33 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Meier", "Raphael", ""], ["Knecht", "Urspeter", ""], ["Jungo", "Alain", ""], ["Wiest", "Roland", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1703.00356", "submitter": "Renata Khasanova", "authors": "Renata Khasanova and Pascal Frossard", "title": "Graph-based Isometry Invariant Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning transformation invariant representations of visual data is an\nimportant problem in computer vision. Deep convolutional networks have\ndemonstrated remarkable results for image and video classification tasks.\nHowever, they have achieved only limited success in the classification of\nimages that undergo geometric transformations. In this work we present a novel\nTransformation Invariant Graph-based Network (TIGraNet), which learns\ngraph-based features that are inherently invariant to isometric transformations\nsuch as rotation and translation of input images. In particular, images are\nrepresented as signals on graphs, which permits to replace classical\nconvolution and pooling layers in deep networks with graph spectral convolution\nand dynamic graph pooling layers that together contribute to invariance to\nisometric transformations. Our experiments show high performance on rotated and\ntranslated images from the test set compared to classical architectures that\nare very sensitive to transformations in the data. The inherent invariance\nproperties of our framework provide key advantages, such as increased\nresiliency to data variability and sustained performance with limited training\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 15:51:13 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Khasanova", "Renata", ""], ["Frossard", "Pascal", ""]]}, {"id": "1703.00395", "submitter": "Lucas Theis", "authors": "Lucas Theis, Wenzhe Shi, Andrew Cunningham, Ferenc Husz\\'ar", "title": "Lossy Image Compression with Compressive Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of optimizing autoencoders for lossy\nimage compression. New media formats, changing hardware technology, as well as\ndiverse requirements and content types create a need for compression algorithms\nwhich are more flexible than existing codecs. Autoencoders have the potential\nto address this need, but are difficult to optimize directly due to the\ninherent non-differentiabilty of the compression loss. We here show that\nminimal changes to the loss are sufficient to train deep autoencoders\ncompetitive with JPEG 2000 and outperforming recently proposed approaches based\non RNNs. Our network is furthermore computationally efficient thanks to a\nsub-pixel architecture, which makes it suitable for high-resolution images.\nThis is in contrast to previous work on autoencoders for compression using\ncoarser approximations, shallower architectures, computationally expensive\nmethods, or focusing on small images.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:13:47 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Theis", "Lucas", ""], ["Shi", "Wenzhe", ""], ["Cunningham", "Andrew", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1703.00495", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Kristen Grauman", "title": "Making 360$^{\\circ}$ Video Watchable in 2D: Learning Videography for\n  Click Free Viewing", "comments": "CVPR 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360$^{\\circ}$ video requires human viewers to actively control \"where\" to\nlook while watching the video. Although it provides a more immersive experience\nof the visual content, it also introduces additional burden for viewers;\nawkward interfaces to navigate the video lead to suboptimal viewing\nexperiences. Virtual cinematography is an appealing direction to remedy these\nproblems, but conventional methods are limited to virtual environments or rely\non hand-crafted heuristics. We propose a new algorithm for virtual\ncinematography that automatically controls a virtual camera within a\n360$^{\\circ}$ video. Compared to the state of the art, our algorithm allows\nmore general camera control, avoids redundant outputs, and extracts its output\nvideos substantially more efficiently. Experimental results on over 7 hours of\nreal \"in the wild\" video show that our generalized camera control is crucial\nfor viewing 360$^{\\circ}$ video, while the proposed efficient algorithm is\nessential for making the generalized control computationally tractable.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 20:58:19 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 20:11:26 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1703.00503", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Xiaofeng Gao, Michael S. Ryoo and Song-Chun Zhu", "title": "Learning Social Affordance Grammar from Videos: Transferring Human\n  Interactions to Human-Robot Interactions", "comments": "The 2017 IEEE International Conference on Robotics and Automation\n  (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a general framework for learning social affordance\ngrammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human\ninteractions, and transfer the grammar to humanoids to enable a real-time\nmotion inference for human-robot interaction (HRI). Based on Gibbs sampling,\nour weakly supervised grammar learning can automatically construct a\nhierarchical representation of an interaction with long-term joint sub-tasks of\nboth agents and short term atomic actions of individual agents. Based on a new\nRGB-D video dataset with rich instances of human interactions, our experiments\nof Baxter simulation, human evaluation, and real Baxter test demonstrate that\nthe model learned from limited training data successfully generates human-like\nbehaviors in unseen scenarios and outperforms both baselines.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 21:05:10 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Shu", "Tianmin", ""], ["Gao", "Xiaofeng", ""], ["Ryoo", "Michael S.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1703.00523", "submitter": "Matt Berseth", "authors": "Matt Berseth", "title": "ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection", "comments": "ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our system addresses Part 1, Lesion Segmentation and Part 3, Lesion\nClassification of the ISIC 2017 challenge. Both algorithms make use of deep\nconvolutional networks to achieve the challenge objective.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 21:41:58 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Berseth", "Matt", ""]]}, {"id": "1703.00534", "submitter": "Hao Chang", "authors": "Hao Chang", "title": "Skin cancer reorganization and classification with deep neural network", "comments": "5 pages, 2 figures. ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one kind of skin cancer, melanoma is very dangerous. Dermoscopy based\nearly detection and recarbonization strategy is critical for melanoma therapy.\nHowever, well-trained dermatologists dominant the diagnostic accuracy. In order\nto solve this problem, many effort focus on developing automatic image analysis\nsystems. Here we report a novel strategy based on deep learning technique, and\nachieve very high skin lesion segmentation and melanoma diagnosis accuracy: 1)\nwe build a segmentation neural network (skin_segnn), which achieved very high\nlesion boundary detection accuracy; 2) We build another very deep neural\nnetwork based on Google inception v3 network (skin_recnn) and its well-trained\nweight. The novel designed transfer learning based deep neural network\nskin_inceptions_v3_nn helps to achieve a high prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 22:21:21 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Chang", "Hao", ""]]}, {"id": "1703.00551", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Shujon Naha, Mrigank Rochan, Neil Bruce, Yang Wang", "title": "Label Refinement Network for Coarse-to-Fine Semantic Segmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of semantic image segmentation using deep\nconvolutional neural networks. We propose a novel network architecture called\nthe label refinement network that predicts segmentation labels in a\ncoarse-to-fine fashion at several resolutions. The segmentation labels at a\ncoarse resolution are used together with convolutional features to obtain finer\nresolution segmentation labels. We define loss functions at several stages in\nthe network to provide supervisions at different stages. Our experimental\nresults on several standard datasets demonstrate that the proposed model\nprovides an effective way of producing pixel-wise dense image labeling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:42:30 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Islam", "Md Amirul", ""], ["Naha", "Shujon", ""], ["Rochan", "Mrigank", ""], ["Bruce", "Neil", ""], ["Wang", "Yang", ""]]}, {"id": "1703.00552", "submitter": "Kanji Tanaka", "authors": "Murase Tomoya, Tanaka Kanji", "title": "Change Detection under Global Viewpoint Uncertainty", "comments": "8 pages, 9 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of change detection from a novel perspective\nof long-term map learning. We are particularly interested in designing an\napproach that can scale to large maps and that can function under global\nuncertainty in the viewpoint (i.e., GPS-denied situations). Our approach, which\nutilizes a compact bag-of-words (BoW) scene model, makes several contributions\nto the problem:\n  1) Two kinds of prior information are extracted from the view sequence map\nand used for change detection. Further, we propose a novel type of prior,\ncalled motion prior, to predict the relative motions of stationary objects and\nanomaly ego-motion detection. The proposed prior is also useful for\ndistinguishing stationary from non-stationary objects.\n  2) A small set of good reference images (e.g., 10) are efficiently retrieved\nfrom the view sequence map by employing the recently developed\nBag-of-Local-Convolutional-Features (BoLCF) scene model.\n  3) Change detection is reformulated as a scene retrieval over these reference\nimages to find changed objects using a novel spatial Bag-of-Words (SBoW) scene\nmodel. Evaluations conducted of individual techniques and also their\ncombinations on a challenging dataset of highly dynamic scenes in the publicly\navailable Malaga dataset verify their efficacy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:51:03 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Tomoya", "Murase", ""], ["Kanji", "Tanaka", ""]]}, {"id": "1703.00555", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony Price, Daniel\n  Rueckert", "title": "A Deep Cascade of Convolutional Neural Networks for MR Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acquisition of Magnetic Resonance Imaging (MRI) is inherently slow.\nInspired by recent advances in deep learning, we propose a framework for\nreconstructing MR images from undersampled data using a deep cascade of\nconvolutional neural networks to accelerate the data acquisition process. We\nshow that for Cartesian undersampling of 2D cardiac MR images, the proposed\nmethod outperforms the state-of-the-art compressed sensing approaches, such as\ndictionary learning-based MRI (DLMRI) reconstruction, in terms of\nreconstruction error, perceptual quality and reconstruction speed for both\n3-fold and 6-fold undersampling. Compared to DLMRI, the error produced by the\nmethod proposed is approximately twice as small, allowing to preserve\nanatomical structures more faithfully. Using our method, each image can be\nreconstructed in 23 ms, which is fast enough to enable real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:54:12 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Schlemper", "Jo", ""], ["Caballero", "Jose", ""], ["Hajnal", "Joseph V.", ""], ["Price", "Anthony", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1703.00577", "submitter": "Yuexiang Li", "authors": "Yuexiang Li and Linlin Shen", "title": "Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning\n  Network", "comments": "ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion is a severe disease in world-wide extent. Early detection of\nmelanoma in dermoscopy images significantly increases the survival rate.\nHowever, the accurate recognition of melanoma is extremely challenging due to\nthe following reasons, e.g. low contrast between lesions and skin, visual\nsimilarity between melanoma and non-melanoma lesions, etc. Hence, reliable\nautomatic detection of skin tumors is very useful to increase the accuracy and\nefficiency of pathologists. International Skin Imaging Collaboration (ISIC) is\na challenge focusing on the automatic analysis of skin lesion. In this paper,\nwe proposed two deep learning methods to address all the three tasks announced\nin ISIC 2017, i.e. lesion segmentation (task 1), lesion dermoscopic feature\nextraction (task 2) and lesion classification (task 3). A deep learning\nframework consisting of two fully-convolutional residual networks (FCRN) is\nproposed to simultaneously produce the segmentation result and the coarse\nclassification result. A lesion index calculation unit (LICU) is developed to\nrefine the coarse classification results by calculating the distance heat-map.\nA straight-forward CNN is proposed for the dermoscopic feature extraction task.\nTo our best knowledges, we are not aware of any previous work proposed for this\ntask. The proposed deep learning frameworks were evaluated on the ISIC 2017\ntesting set. Experimental results show the promising accuracies of our\nframeworks, i.e. 0.718 for task 1, 0.833 for task 2 and 0.823 for task 3 were\nachieved.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 01:24:04 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 12:13:09 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Yuexiang", ""], ["Shen", "Linlin", ""]]}, {"id": "1703.00586", "submitter": "Jim Jing-Yan Wang", "authors": "Yanyan Geng, Guohui Zhang, Weizhi Li, Yi Gu, Ru-Ze Liang, Gaoyuan\n  Liang, Jingbin Wang, Yanbin Wu, Nitin Patil, Jing-Yan Wang", "title": "A novel image tag completion method based on convolutional neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problems of image retrieval and annotation, complete textual tag lists\nof images play critical roles. However, in real-world applications, the image\ntags are usually incomplete, thus it is important to learn the complete tags\nfor images. In this paper, we study the problem of image tag complete and\nproposed a novel method for this problem based on a popular image\nrepresentation method, convolutional neural network (CNN). The method estimates\nthe complete tags from the convolutional filtering outputs of images based on a\nlinear predictor. The CNN parameters, linear predictor, and the complete tags\nare learned jointly by our method. We build a minimization problem to encourage\nthe consistency between the complete tags and the available incomplete tags,\nreduce the estimation error, and reduce the model complexity. An iterative\nalgorithm is developed to solve the minimization problem. Experiments over\nbenchmark image data sets show its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 02:15:05 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 12:21:31 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Geng", "Yanyan", ""], ["Zhang", "Guohui", ""], ["Li", "Weizhi", ""], ["Gu", "Yi", ""], ["Liang", "Ru-Ze", ""], ["Liang", "Gaoyuan", ""], ["Wang", "Jingbin", ""], ["Wu", "Yanbin", ""], ["Patil", "Nitin", ""], ["Wang", "Jing-Yan", ""]]}, {"id": "1703.00645", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Robert Gillies, Kunlin Cao, Qi Song, Ulas Bagci", "title": "TumorNet: Lung Nodule Characterization Using Multi-View Convolutional\n  Neural Network with Gaussian Process", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of lung nodules as benign or malignant is one of the most\nimportant tasks in lung cancer diagnosis, staging and treatment planning. While\nthe variation in the appearance of the nodules remains large, there is a need\nfor a fast and robust computer aided system. In this work, we propose an\nend-to-end trainable multi-view deep Convolutional Neural Network (CNN) for\nnodule characterization. First, we use median intensity projection to obtain a\n2D patch corresponding to each dimension. The three images are then\nconcatenated to form a tensor, where the images serve as different channels of\nthe input image. In order to increase the number of training samples, we\nperform data augmentation by scaling, rotating and adding noise to the input\nimage. The trained network is used to extract features from the input image\nfollowed by a Gaussian Process (GP) regression to obtain the malignancy score.\nWe also empirically establish the significance of different high level nodule\nattributes such as calcification, sphericity and others for malignancy\ndetermination. These attributes are found to be complementary to the deep\nmulti-view CNN features and a significant improvement over other methods is\nobtained.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:26:37 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Gillies", "Robert", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Bagci", "Ulas", ""]]}, {"id": "1703.00663", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Introduction to Nonnegative Matrix Factorization", "comments": "18 pages, 4 figures", "journal-ref": "SIAG/OPT Views and News 25 (1), pp. 7-16 (2017)", "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and provide a short overview of nonnegative\nmatrix factorization (NMF). Several aspects of NMF are discussed, namely, the\napplication in hyperspectral imaging, geometry and uniqueness of NMF solutions,\ncomplexity, algorithms, and its link with extended formulations of polyhedra.\nIn order to put NMF into perspective, the more general problem class of\nconstrained low-rank matrix approximation problems is first briefly introduced.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 08:23:04 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1703.00686", "submitter": "Jakub Sochor", "authors": "Jakub Sochor, Jakub \\v{S}pa\\v{n}hel, Adam Herout", "title": "BoxCars: Improving Fine-Grained Recognition of Vehicles using 3-D\n  Bounding Boxes in Traffic Surveillance", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, 2018,\n  ISSN: 1524-9050", "doi": "10.1109/TITS.2018.2799228", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on fine-grained recognition of vehicles mainly in\ntraffic surveillance applications. We propose an approach that is orthogonal to\nrecent advancements in fine-grained recognition (automatic part discovery and\nbilinear pooling). In addition, in contrast to other methods focused on\nfine-grained recognition of vehicles, we do not limit ourselves to a\nfrontal/rear viewpoint, but allow the vehicles to be seen from any viewpoint.\nOur approach is based on 3-D bounding boxes built around the vehicles. The\nbounding box can be automatically constructed from traffic surveillance data.\nFor scenarios where it is not possible to use precise construction, we propose\na method for an estimation of the 3-D bounding box. The 3-D bounding box is\nused to normalize the image viewpoint by \"unpacking\" the image into a plane. We\nalso propose to randomly alter the color of the image and add a rectangle with\nrandom noise to a random position in the image during the training of\nconvolutional neural networks (CNNs). We have collected a large fine-grained\nvehicle data set BoxCars116k, with 116k images of vehicles from various\nviewpoints taken by numerous surveillance cameras. We performed a number of\nexperiments, which show that our proposed method significantly improves CNN\nclassification accuracy (the accuracy is increased by up to 12% points and the\nerror is reduced by up to 50% compared with CNNs without the proposed\nmodifications). We also show that our method outperforms the state-of-the-art\nmethods for fine-grained recognition.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 09:51:51 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 08:23:55 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 12:01:57 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Sochor", "Jakub", ""], ["\u0160pa\u0148hel", "Jakub", ""], ["Herout", "Adam", ""]]}, {"id": "1703.00737", "submitter": "Dimitri Block", "authors": "Malte Schmidt, Dimitri Block, Uwe Meier", "title": "Wireless Interference Identification with Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE 15th International Conference on Industrial Informatics\n  (INDIN)", "doi": "10.1109/indin.2017.8104767", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steadily growing use of license-free frequency bands requires reliable\ncoexistence management for deterministic medium utilization. For interference\nmitigation, proper wireless interference identification (WII) is essential. In\nthis work we propose the first WII approach based upon deep convolutional\nneural networks (CNNs). The CNN naively learns its features through\nself-optimization during an extensive data-driven GPU-based training process.\nWe propose a CNN example which is based upon sensing snapshots with a limited\nduration of 12.8 {\\mu}s and an acquisition bandwidth of 10 MHz. The CNN differs\nbetween 15 classes. They represent packet transmissions of IEEE 802.11 b/g,\nIEEE 802.15.4 and IEEE 802.15.1 with overlapping frequency channels within the\n2.4 GHz ISM band. We show that the CNN outperforms state-of-the-art WII\napproaches and has a classification accuracy greater than 95% for\nsignal-to-noise ratio of at least -5 dB.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:52:47 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Schmidt", "Malte", ""], ["Block", "Dimitri", ""], ["Meier", "Uwe", ""]]}, {"id": "1703.00767", "submitter": "Ambedkar Dukkipati", "authors": "Pranav Shyam and Shubham Gupta and Ambedkar Dukkipati", "title": "Attentive Recurrent Comparators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid learning requires flexible representations to quickly adopt to new\nevidence. We develop a novel class of models called Attentive Recurrent\nComparators (ARCs) that form representations of objects by cycling through them\nand making observations. Using the representations extracted by ARCs, we\ndevelop a way of approximating a \\textit{dynamic representation space} and use\nit for one-shot learning. In the task of one-shot classification on the\nOmniglot dataset, we achieve the state of the art performance with an error\nrate of 1.5\\%. This represents the first super-human result achieved for this\ntask with a generic model that uses only pixel information.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 12:47:40 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 12:23:16 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 07:37:56 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Shyam", "Pranav", ""], ["Gupta", "Shubham", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1703.00792", "submitter": "Shagan Sah", "authors": "Felipe Petroski Such and Shagan Sah and Miguel Dominguez and Suhas\n  Pillai and Chao Zhang and Andrew Michael and Nathan Cahill and Raymond Ptucha", "title": "Robust Spatial Filtering with Graph Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2017.2726981", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently led to incredible\nbreakthroughs on a variety of pattern recognition problems. Banks of finite\nimpulse response filters are learned on a hierarchy of layers, each\ncontributing more abstract information than the previous layer. The simplicity\nand elegance of the convolutional filtering process makes them perfect for\nstructured problems such as image, video, or voice, where vertices are\nhomogeneous in the sense of number, location, and strength of neighbors. The\nvast majority of classification problems, for example in the pharmaceutical,\nhomeland security, and financial domains are unstructured. As these problems\nare formulated into unstructured graphs, the heterogeneity of these problems,\nsuch as number of vertices, number of connections per vertex, and edge\nstrength, cannot be tackled with standard convolutional techniques. We propose\na novel neural learning framework that is capable of handling both homogeneous\nand heterogeneous data, while retaining the benefits of traditional CNN\nsuccesses.\n  Recently, researchers have proposed variations of CNNs that can handle graph\ndata. In an effort to create learnable filter banks of graphs, these methods\neither induce constraints on the data or require preprocessing. As opposed to\nspectral methods, our framework, which we term Graph-CNNs, defines filters as\npolynomials of functions of the graph adjacency matrix. Graph-CNNs can handle\nboth heterogeneous and homogeneous graph data, including graphs having entirely\ndifferent vertex or edge sets. We perform experiments to validate the\napplicability of Graph-CNNs to a variety of structured and unstructured\nclassification problems and demonstrate state-of-the-art results on document\nand molecule classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 14:09:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 03:17:43 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 19:57:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Such", "Felipe Petroski", ""], ["Sah", "Shagan", ""], ["Dominguez", "Miguel", ""], ["Pillai", "Suhas", ""], ["Zhang", "Chao", ""], ["Michael", "Andrew", ""], ["Cahill", "Nathan", ""], ["Ptucha", "Raymond", ""]]}, {"id": "1703.00797", "submitter": "Huan-Chih Wang M.D.", "authors": "Huan-Chih Wang, Shih-Hao Ho, Furen Xiao, Jen-Hai Chou", "title": "A Simple, Fast and Fully Automated Approach for Midline Shift\n  Measurement on Brain Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain CT has become a standard imaging tool for emergent evaluation of brain\ncondition, and measurement of midline shift (MLS) is one of the most important\nfeatures to address for brain CT assessment. We present a simple method to\nestimate MLS and propose a new alternative parameter to MLS: the ratio of MLS\nover the maximal width of intracranial region (MLS/ICWMAX). Three neurosurgeons\nand our automated system were asked to measure MLS and MLS/ICWMAX in the same\nsets of axial CT images obtained from 41 patients admitted to ICU under\nneurosurgical service. A weighted midline (WML) was plotted based on individual\npixel intensities, with higher weighted given to the darker portions. The MLS\ncould then be measured as the distance between the WML and ideal midline (IML)\nnear the foramen of Monro. The average processing time to output an automatic\nMLS measurement was around 10 seconds. Our automated system achieved an overall\naccuracy of 90.24% when the CT images were calibrated automatically, and\nperformed better when the calibrations of head rotation were done manually\n(accuracy: 92.68%). MLS/ICWMAX and MLS both gave results in same confusion\nmatrices and produced similar ROC curve results. We demonstrated a simple, fast\nand accurate automated system of MLS measurement and introduced a new parameter\n(MLS/ICWMAX) as a good alternative to MLS in terms of estimating the degree of\nbrain deformation, especially when non-DICOM images (e.g. JPEG) are more easily\naccessed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 14:33:14 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Wang", "Huan-Chih", ""], ["Ho", "Shih-Hao", ""], ["Xiao", "Furen", ""], ["Chou", "Jen-Hai", ""]]}, {"id": "1703.00832", "submitter": "Guangcan Mai", "authors": "Guangcan Mai and Kai Cao and Pong C. Yuen and Anil K. Jain", "title": "On the Reconstruction of Face Images from Deep Face Templates", "comments": "To appear in TPAMI, IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2018", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2827389", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art face recognition systems are based on deep (convolutional)\nneural networks. Therefore, it is imperative to determine to what extent face\ntemplates derived from deep networks can be inverted to obtain the original\nface image. In this paper, we study the vulnerabilities of a state-of-the-art\nface recognition system based on template reconstruction attack. We propose a\nneighborly de-convolutional neural network (\\textit{NbNet}) to reconstruct face\nimages from their deep templates. In our experiments, we assumed that no\nknowledge about the target subject and the deep network are available. To train\nthe \\textit{NbNet} reconstruction models, we augmented two benchmark face\ndatasets (VGG-Face and Multi-PIE) with a large collection of images synthesized\nusing a face generator. The proposed reconstruction was evaluated using type-I\n(comparing the reconstructed images against the original face images used to\ngenerate the deep template) and type-II (comparing the reconstructed images\nagainst a different face image of the same subject) attacks. Given the images\nreconstructed from \\textit{NbNets}, we show that for verification, we achieve\nTAR of 95.20\\% (58.05\\%) on LFW under type-I (type-II) attacks @ FAR of 0.1\\%.\nBesides, 96.58\\% (92.84\\%) of the images reconstruction from templates of\npartition \\textit{fa} (\\textit{fb}) can be identified from partition\n\\textit{fa} in color FERET. Our study demonstrates the need to secure deep\ntemplates in face recognition systems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:29:07 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 09:01:47 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 03:15:54 GMT"}, {"version": "v4", "created": "Sun, 29 Apr 2018 03:39:50 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mai", "Guangcan", ""], ["Cao", "Kai", ""], ["Yuen", "Pong C.", ""], ["Jain", "Anil K.", ""]]}, {"id": "1703.00845", "submitter": "Luis Angel Contreras-Toledo", "authors": "Luis Contreras and Walterio Mayol-Cuevas", "title": "Towards CNN Map Compression for camera relocalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on the use of Convolutional Neural Networks for\ncamera relocalisation and its application to map compression. We follow state\nof the art visual relocalisation results and evaluate response to different\ndata inputs -- namely, depth, grayscale, RGB, spatial position and combinations\nof these. We use a CNN map representation and introduce the notion of CNN map\ncompression by using a smaller CNN architecture. We evaluate our proposal in a\nseries of publicly available datasets. This formulation allows us to improve\nrelocalisation accuracy by increasing the number of training trajectories while\nmaintaining a constant-size CNN.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 16:12:29 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Contreras", "Luis", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1703.00848", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu and Thomas Breuel and Jan Kautz", "title": "Unsupervised Image-to-Image Translation Networks", "comments": "NIPS 2017, 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation aims at learning a joint distribution\nof images in different domains by using images from the marginal distributions\nin individual domains. Since there exists an infinite set of joint\ndistributions that can arrive the given marginal distributions, one could infer\nnothing about the joint distribution from the marginal distributions without\nadditional assumptions. To address the problem, we make a shared-latent space\nassumption and propose an unsupervised image-to-image translation framework\nbased on Coupled GANs. We compare the proposed framework with competing\napproaches and present high quality image translation results on various\nchallenging unsupervised image translation tasks, including street scene image\ntranslation, animal image translation, and face image translation. We also\napply the proposed framework to domain adaptation and achieve state-of-the-art\nperformance on benchmark datasets. Code and additional results are available in\nhttps://github.com/mingyuliutw/unit .\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 16:29:30 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 17:55:21 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 03:14:21 GMT"}, {"version": "v4", "created": "Mon, 9 Oct 2017 18:14:27 GMT"}, {"version": "v5", "created": "Thu, 15 Feb 2018 15:33:48 GMT"}, {"version": "v6", "created": "Mon, 23 Jul 2018 03:39:28 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Breuel", "Thomas", ""], ["Kautz", "Jan", ""]]}, {"id": "1703.00856", "submitter": "Rafael Sousa", "authors": "Rafael Teixeira Sousa and Larissa Vasconcellos de Moraes", "title": "Araguaia Medical Vision Lab at ISIC 2017 Skin Lesion Classification\n  Challenge", "comments": "Abstract submitted as a requirement to ISIC2017 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes the participation of Araguaia Medical Vision Lab at the\nInternational Skin Imaging Collaboration 2017 Skin Lesion Challenge. We\ndescribe the use of deep convolutional neural networks in attempt to classify\nimages of Melanoma and Seborrheic Keratosis lesions. With use of finetuned\nGoogleNet and AlexNet we attained results of 0.950 and 0.846 AUC on Seborrheic\nKeratosis and Melanoma respectively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:01:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Sousa", "Rafael Teixeira", ""], ["de Moraes", "Larissa Vasconcellos", ""]]}, {"id": "1703.00862", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Binarized Convolutional Landmark Localizers for Human Pose Estimation\n  and Face Alignment with Limited Resources", "comments": "ICCV 2017 Oral", "journal-ref": null, "doi": "10.1109/ICCV.2017.400", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment. We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:26:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 15:35:04 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1703.00868", "submitter": "Atilim Gunes Baydin", "authors": "Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, Frank Wood", "title": "Using Synthetic Data to Train Neural Networks is Model-Based Reasoning", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We draw a formal connection between using synthetic training data to optimize\nneural network parameters and approximate, Bayesian, model-based reasoning. In\nparticular, training a neural network using synthetic data can be viewed as\nlearning a proposal distribution generator for approximate inference in the\nsynthetic-data generative model. We demonstrate this connection in a\nrecognition task where we develop a novel Captcha-breaking architecture and\ntrain it using synthetic data, demonstrating both state-of-the-art performance\nand a way of computing task-specific posterior uncertainty. Using a neural\nnetwork trained this way, we also demonstrate successful breaking of real-world\nCaptchas currently used by Facebook and Wikipedia. Reasoning from these\nempirical results and drawing connections with Bayesian modeling, we discuss\nthe robustness of synthetic data results and suggest important considerations\nfor ensuring good neural network generalization when training with synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:43:19 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Le", "Tuan Anh", ""], ["Baydin", "Atilim Gunes", ""], ["Zinkov", "Robert", ""], ["Wood", "Frank", ""]]}, {"id": "1703.00919", "submitter": "Krzysztof Wegner", "authors": "Krzysztof Wegner, Olgierd Stankiewicz, Marek Domanski", "title": "Depth Estimation using Modified Cost Function for Occlusion Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel approach to occlusion handling problem in depth\nestimation using three views. A solution based on modification of similarity\ncost function is proposed. During the depth estimation via optimization\nalgorithms like Graph Cut similarity metric is constantly updated so that only\nnon-occluded fragments in side views are considered. At each iteration of the\nalgorithm non-occluded fragments are detected based on side view virtual depth\nmaps synthesized from the best currently estimated depth map of the center\nview. Then similarity metric is updated for correspondence search only in\nnon-occluded regions of the side views. The experimental results, conducted on\nwell-known 3D video test sequences, have proved that the depth maps estimated\nwith the proposed approach provide about 1.25 dB virtual view quality\nimprovement in comparison to the virtual view synthesized based on depth maps\ngenerated by the state-of-the-art MPEG Depth Estimation Reference Software.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 19:05:05 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 12:15:20 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wegner", "Krzysztof", ""], ["Stankiewicz", "Olgierd", ""], ["Domanski", "Marek", ""]]}, {"id": "1703.00981", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson", "title": "A Restaurant Process Mixture Model for Connectivity Based Parcellation\n  of the Cortex", "comments": "In the Proceedings of Information Processing in Medical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary objectives of human brain mapping is the division of the\ncortical surface into functionally distinct regions, i.e. parcellation. While\nit is generally agreed that at macro-scale different regions of the cortex have\ndifferent functions, the exact number and configuration of these regions is not\nknown. Methods for the discovery of these regions are thus important,\nparticularly as the volume of available information grows. Towards this end, we\npresent a parcellation method based on a Bayesian non-parametric mixture model\nof cortical connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:03:56 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Moyer", "Daniel", ""], ["Gutman", "Boris A", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1703.00986", "submitter": "Wei Ping", "authors": "Wei Ping, Alexander Ihler", "title": "Belief Propagation in Conditional RBMs for Structured Prediction", "comments": "Artificial Intelligence and Statistics (AISTATS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular\nmodels for a wide range of applications. In previous work, learning on such\nmodels has been dominated by contrastive divergence~(CD) and its variants.\nBelief propagation~(BP) algorithms are believed to be slow for structured\nprediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as\nCD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we\npresent a matrix-based implementation of belief propagation algorithms on\nCRBMs, which is easily scalable to tens of thousands of visible and hidden\nunits. We demonstrate that, in both maximum likelihood and max-margin learning,\ntraining conditional RBMs with BP as the inference routine can provide\nsignificantly better results than current state-of-the-art CD methods on\nstructured prediction problems. We also include practical guidelines on\ntraining CRBMs with BP, and some insights on the interaction of learning and\ninference algorithms for CRBMs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:28:53 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Ping", "Wei", ""], ["Ihler", "Alexander", ""]]}, {"id": "1703.00992", "submitter": "Ryuta Mizutani", "authors": "Ryuta Mizutani, Rino Saiga, Susumu Takekoshi, Chie Inomoto, Naoya\n  Nakamura, Makoto Arai, Kenichi Oshima, Masanari Itokawa, Akihisa Takeuchi,\n  Kentaro Uesugi, Yasuko Terada and Yoshio Suzuki", "title": "Estimating the resolution of real images", "comments": "4 pages, 2 figures", "journal-ref": "J. Phys. Conf. Ser. 849, 012042 (2017)", "doi": "10.1088/1742-6596/849/1/012042", "report-no": null, "categories": "physics.data-an cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image resolvability is the primary concern in imaging. This paper reports an\nestimation of the full width at half maximum of the point spread function from\na Fourier domain plot of real sample images by neither using test objects, nor\ndefining a threshold criterion. We suggest that this method can be applied to\nany type of image, independently of the imaging modality.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:55:13 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Mizutani", "Ryuta", ""], ["Saiga", "Rino", ""], ["Takekoshi", "Susumu", ""], ["Inomoto", "Chie", ""], ["Nakamura", "Naoya", ""], ["Arai", "Makoto", ""], ["Oshima", "Kenichi", ""], ["Itokawa", "Masanari", ""], ["Takeuchi", "Akihisa", ""], ["Uesugi", "Kentaro", ""], ["Terada", "Yasuko", ""], ["Suzuki", "Yoshio", ""]]}, {"id": "1703.01025", "submitter": "Xulei Yang", "authors": "Xulei Yang, Zeng Zeng, Si Yong Yeo, Colin Tan, Hong Liang Tey, Yi Su", "title": "A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and\n  Classification", "comments": "Submission to support ISIC 2017 challenge results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a multi-task deep neural network is proposed for skin lesion\nanalysis. The proposed multi-task learning model solves different tasks (e.g.,\nlesion segmentation and two independent binary lesion classifications) at the\nsame time by exploiting commonalities and differences across tasks. This\nresults in improved learning efficiency and potential prediction accuracy for\nthe task-specific models, when compared to training the individual models\nseparately. The proposed multi-task deep learning model is trained and\nevaluated on the dermoscopic image sets from the International Skin Imaging\nCollaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma\nDetection, which consists of 2000 training samples and 150 evaluation samples.\nThe experimental results show that the proposed multi-task deep learning model\nachieves promising performances on skin lesion segmentation and classification.\nThe average value of Jaccard index for lesion segmentation is 0.724, while the\naverage values of area under the receiver operating characteristic curve (AUC)\non two individual lesion classifications are 0.880 and 0.972, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 03:22:16 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Yang", "Xulei", ""], ["Zeng", "Zeng", ""], ["Yeo", "Si Yong", ""], ["Tan", "Colin", ""], ["Tey", "Hong Liang", ""], ["Su", "Yi", ""]]}, {"id": "1703.01028", "submitter": "Takuro Ina", "authors": "Takuro Ina and Atsushi Hashimoto and Masaaki Iiyama and Hidekazu\n  Kasahara and Mikihiko Mori and Michihiko Minoh", "title": "Outlier Cluster Formation in Spectral Clustering", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 04:02:11 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Ina", "Takuro", ""], ["Hashimoto", "Atsushi", ""], ["Iiyama", "Masaaki", ""], ["Kasahara", "Hidekazu", ""], ["Mori", "Mikihiko", ""], ["Minoh", "Michihiko", ""]]}, {"id": "1703.01040", "submitter": "Jangwon Lee", "authors": "Jangwon Lee and Michael S. Ryoo", "title": "Learning Robot Activities from First-Person Human Videos Using\n  Convolutional Future Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new approach that allows robot learning of new activities from\nunlabeled human example videos. Given videos of humans executing the same\nactivity from a human's viewpoint (i.e., first-person videos), our objective is\nto make the robot learn the temporal structure of the activity as its future\nregression network, and learn to transfer such model for its own motor\nexecution. We present a new deep learning model: We extend the state-of-the-art\nconvolutional object detection network for the representation/estimation of\nhuman hands in training videos, and newly introduce the concept of using a\nfully convolutional network to regress (i.e., predict) the intermediate scene\nrepresentation corresponding to the future frame (e.g., 1-2 seconds later).\nCombining these allows direct prediction of future locations of human hands and\nobjects, which enables the robot to infer the motor control plan using our\nmanipulation network. We experimentally confirm that our approach makes\nlearning of robot activities from unlabeled human interaction videos possible,\nand demonstrate that our robot is able to execute the learned collaborative\nactivities in real-time directly based on its camera input.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 05:27:50 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 08:02:11 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Lee", "Jangwon", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1703.01041", "submitter": "Esteban Real", "authors": "Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon\n  Suematsu, Jie Tan, Quoc Le, Alex Kurakin", "title": "Large-Scale Evolution of Image Classifiers", "comments": "Accepted for publication at ICML 2017 (34th International Conference\n  on Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have proven effective at solving difficult problems but\ndesigning their architectures can be challenging, even for image classification\nproblems alone. Our goal is to minimize human participation, so we employ\nevolutionary algorithms to discover such networks automatically. Despite\nsignificant computational requirements, we show that it is now possible to\nevolve models with accuracies within the range of those published in the last\nyear. Specifically, we employ simple evolutionary techniques at unprecedented\nscales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting\nfrom trivial initial conditions and reaching accuracies of 94.6% (95.6% for\nensemble) and 77.0%, respectively. To do this, we use novel and intuitive\nmutation operators that navigate large search spaces; we stress that no human\nparticipation is required once evolution starts and that the output is a\nfully-trained model. Throughout this work, we place special emphasis on the\nrepeatability of results, the variability in the outcomes and the computational\nrequirements.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 05:41:30 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 08:42:28 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Real", "Esteban", ""], ["Moore", "Sherry", ""], ["Selle", "Andrew", ""], ["Saxena", "Saurabh", ""], ["Suematsu", "Yutaka Leon", ""], ["Tan", "Jie", ""], ["Le", "Quoc", ""], ["Kurakin", "Alex", ""]]}, {"id": "1703.01053", "submitter": "Xi Jia", "authors": "Xi Jia and Linlin Shen", "title": "Skin Lesion Classification using Class Activation Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a two stage framework with only one network to analyze skin\nlesion images, we firstly trained a convolutional network to classify these\nimages, and cropped the import regions which the network has the maximum\nactivation value. In the second stage, we retrained this CNN with the image\nregions extracted from stage one and output the final probabilities. The two\nstage framework achieved a mean AUC of 0.857 in ISIC-2017 skin lesion\nvalidation set and is 0.04 higher than that of the original inputs, 0.821.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 06:38:30 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jia", "Xi", ""], ["Shen", "Linlin", ""]]}, {"id": "1703.01086", "submitter": "Yingbin Zheng", "authors": "Jianqi Ma and Weiyuan Shao and Hao Ye and Li Wang and Hong Wang and\n  Yingbin Zheng and Xiangyang Xue", "title": "Arbitrary-Oriented Scene Text Detection via Rotation Proposals", "comments": "Code is available at: https://github.com/mjq11302010044/RRPN", "journal-ref": "IEEE Transactions on Multimedia, vol. 20, no. 11, pp. 3111-3122,\n  2018", "doi": "10.1109/TMM.2018.2818020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel rotation-based framework for arbitrary-oriented\ntext detection in natural scene images. We present the Rotation Region Proposal\nNetworks (RRPN), which are designed to generate inclined proposals with text\norientation angle information. The angle information is then adapted for\nbounding box regression to make the proposals more accurately fit into the text\nregion in terms of the orientation. The Rotation Region-of-Interest (RRoI)\npooling layer is proposed to project arbitrary-oriented proposals to a feature\nmap for a text region classifier. The whole framework is built upon a\nregion-proposal-based architecture, which ensures the computational efficiency\nof the arbitrary-oriented text detection compared with previous text detection\nsystems. We conduct experiments using the rotation-based framework on three\nreal-world scene text detection datasets and demonstrate its superiority in\nterms of effectiveness and efficiency over previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 09:24:41 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 14:31:33 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 07:07:36 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ma", "Jianqi", ""], ["Shao", "Weiyuan", ""], ["Ye", "Hao", ""], ["Wang", "Li", ""], ["Wang", "Hong", ""], ["Zheng", "Yingbin", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1703.01101", "submitter": "Volker Fischer", "authors": "Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas\n  Brox", "title": "Adversarial Examples for Semantic Image Segmentation", "comments": "ICLR 2017 workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods in general and Deep Neural Networks in particular\nhave shown to be vulnerable to adversarial perturbations. So far this\nphenomenon has mainly been studied in the context of whole-image\nclassification. In this contribution, we analyse how adversarial perturbations\ncan affect the task of semantic segmentation. We show how existing adversarial\nattackers can be transferred to this task and that it is possible to create\nimperceptible adversarial perturbations that lead a deep network to misclassify\nalmost all pixels of a chosen class while leaving network prediction nearly\nunchanged outside this class.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 10:27:16 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Fischer", "Volker", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Metzen", "Jan Hendrik", ""], ["Brox", "Thomas", ""]]}, {"id": "1703.01120", "submitter": "Jong Chul Ye", "authors": "Dongwook Lee, Jaejun Yoo and Jong Chul Ye", "title": "Deep artifact learning for compressed sensing and parallel MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Compressed sensing MRI (CS-MRI) from single and parallel coils is\none of the powerful ways to reduce the scan time of MR imaging with performance\nguarantee. However, the computational costs are usually expensive. This paper\naims to propose a computationally fast and accurate deep learning algorithm for\nthe reconstruction of MR images from highly down-sampled k-space data.\n  Theory: Based on the topological analysis, we show that the data manifold of\nthe aliasing artifact is easier to learn from a uniform subsampling pattern\nwith additional low-frequency k-space data. Thus, we develop deep aliasing\nartifact learning networks for the magnitude and phase images to estimate and\nremove the aliasing artifacts from highly accelerated MR acquisition.\n  Methods: The aliasing artifacts are directly estimated from the distorted\nmagnitude and phase images reconstructed from subsampled k-space data so that\nwe can get an aliasing-free images by subtracting the estimated aliasing\nartifact from corrupted inputs. Moreover, to deal with the globally distributed\naliasing artifact, we develop a multi-scale deep neural network with a large\nreceptive field.\n  Results: The experimental results confirm that the proposed deep artifact\nlearning network effectively estimates and removes the aliasing artifacts.\nCompared to existing CS methods from single and multi-coli data, the proposed\nnetwork shows minimal errors by removing the coherent aliasing artifacts.\nFurthermore, the computational time is by order of magnitude faster.\n  Conclusion: As the proposed deep artifact learning network immediately\ngenerates accurate reconstruction, it has great potential for clinical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 12:02:32 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Lee", "Dongwook", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1703.01135", "submitter": "Jong Chul Ye", "authors": "Yo Seob Han, Jaejun Yoo and Jong Chul Ye", "title": "Deep Learning with Domain Adaptation for Accelerated\n  Projection-Reconstruction MR", "comments": "This paper has been accepted and will soon appear in Magnetic\n  Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The radial k-space trajectory is a well-established sampling\ntrajectory used in conjunction with magnetic resonance imaging. However, the\nradial k-space trajectory requires a large number of radial lines for\nhigh-resolution reconstruction. Increasing the number of radial lines causes\nlonger acquisition time, making it more difficult for routine clinical use. On\nthe other hand, if we reduce the number of radial lines, streaking artifact\npatterns are unavoidable. To solve this problem, we propose a novel deep\nlearning approach with domain adaptation to restore high-resolution MR images\nfrom under-sampled k-space data.\n  Methods: The proposed deep network removes the streaking artifacts from the\nartifact corrupted images. To address the situation given the limited available\ndata, we propose a domain adaptation scheme that employs a pre-trained network\nusing a large number of x-ray computed tomography (CT) or synthesized radial MR\ndatasets, which is then fine-tuned with only a few radial MR datasets.\n  Results: The proposed method outperforms existing compressed sensing\nalgorithms, such as the total variation and PR-FOCUSS methods. In addition, the\ncalculation time is several orders of magnitude faster than the total variation\nand PR-FOCUSS methods.Moreover, we found that pre-training using CT or MR data\nfrom similar organ data is more important than pre-training using data from the\nsame modality for different organ.\n  Conclusion: We demonstrate the possibility of a domain-adaptation when only a\nlimited amount of MR data is available. The proposed method surpasses the\nexisting compressed sensing algorithms in terms of the image quality and\ncomputation time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 12:49:36 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 04:28:28 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Han", "Yo Seob", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1703.01170", "submitter": "Huang-Chia Shih", "authors": "Huang-Chia Shih", "title": "A Survey on Content-Aware Video Analysis for Sports", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2655624", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports data analysis is becoming increasingly large-scale, diversified, and\nshared, but difficulty persists in rapidly accessing the most crucial\ninformation. Previous surveys have focused on the methodologies of sports video\nanalysis from the spatiotemporal viewpoint instead of a content-based\nviewpoint, and few of these studies have considered semantics. This study\ndevelops a deeper interpretation of content-aware sports video analysis by\nexamining the insight offered by research into the structure of content under\ndifferent scenarios. On the basis of this insight, we provide an overview of\nthe themes particularly relevant to the research on content-aware systems for\nbroadcast sports. Specifically, we focus on the video content analysis\ntechniques applied in sportscasts over the past decade from the perspectives of\nfundamentals and general review, a content hierarchical model, and trends and\nchallenges. Content-aware analysis methods are discussed with respect to\nobject-, event-, and context-oriented groups. In each group, the gap between\nsensation and content excitement must be bridged using proper strategies. In\nthis regard, a content-aware approach is required to determine user demands.\nFinally, the paper summarizes the future trends and challenges for sports video\nanalysis. We believe that our findings can advance the field of research on\ncontent-aware video analysis for broadcast sports.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 14:28:03 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Shih", "Huang-Chia", ""]]}, {"id": "1703.01210", "submitter": "Fabian Benitez-Quiroz Carlos F Benitez-Quiroz", "authors": "C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, Qianli Feng, Yan\n  Wang, Aleix M. Martinez", "title": "EmotioNet Challenge: Recognition of facial expressions of emotion in the\n  wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details the methodology and results of the EmotioNet challenge.\nThis challenge is the first to test the ability of computer vision algorithms\nin the automatic analysis of a large number of images of facial expressions of\nemotion in the wild. The challenge was divided into two tracks. The first track\ntested the ability of current computer vision algorithms in the automatic\ndetection of action units (AUs). Specifically, we tested the detection of 11\nAUs. The second track tested the algorithms' ability to recognize emotion\ncategories in images of facial expressions. Specifically, we tested the\nrecognition of 16 basic and compound emotion categories. The results of the\nchallenge suggest that current computer vision and machine learning algorithms\nare unable to reliably solve these two tasks. The limitations of current\nalgorithms are more apparent when trying to recognize emotion. We also show\nthat current algorithms are not affected by mild resolution changes, small\noccluders, gender or age, but that 3D pose is a major limiting factor on\nperformance. We provide an in-depth discussion of the points that need special\nattention moving forward.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:35:52 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Benitez-Quiroz", "C. Fabian", ""], ["Srinivasan", "Ramprakash", ""], ["Feng", "Qianli", ""], ["Wang", "Yan", ""], ["Martinez", "Aleix M.", ""]]}, {"id": "1703.01220", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Anil Anthony Bharath", "title": "Denoising Adversarial Autoencoders", "comments": "submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning is of growing interest because it unlocks the potential\nheld in vast amounts of unlabelled data to learn useful representations for\ninference. Autoencoders, a form of generative model, may be trained by learning\nto reconstruct unlabelled input data from a latent representation space. More\nrobust representations may be produced by an autoencoder if it learns to\nrecover clean input samples from corrupted ones. Representations may be further\nimproved by introducing regularisation during training to shape the\ndistribution of the encoded data in latent space. We suggest denoising\nadversarial autoencoders, which combine denoising and regularisation, shaping\nthe distribution of latent space using adversarial training. We introduce a\nnovel analysis that shows how denoising may be incorporated into the training\nand sampling of adversarial autoencoders. Experiments are performed to assess\nthe contributions that denoising makes to the learning of representations for\nclassification and sample synthesis. Our results suggest that autoencoders\ntrained using a denoising criterion achieve higher classification performance,\nand can synthesise samples that are more consistent with the input data than\nthose trained without a corruption process.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:59:16 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 20:21:44 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 16:07:58 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 17:18:16 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1703.01226", "submitter": "Zakaria Laskar", "authors": "Zakaria Laskar, and Juho Kannala", "title": "Context Aware Query Image Representation for Particular Object Retrieval", "comments": "14 pages, Extended version of a manuscript submitted to SCIA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current models of image representation based on Convolutional Neural\nNetworks (CNN) have shown tremendous performance in image retrieval. Such\nmodels are inspired by the information flow along the visual pathway in the\nhuman visual cortex. We propose that in the field of particular object\nretrieval, the process of extracting CNN representations from query images with\na given region of interest (ROI) can also be modelled by taking inspiration\nfrom human vision. Particularly, we show that by making the CNN pay attention\non the ROI while extracting query image representation leads to significant\nimprovement over the baseline methods on challenging Oxford5k and Paris6k\ndatasets. Furthermore, we propose an extension to a recently introduced\nencoding method for CNN representations, regional maximum activations of\nconvolutions (R-MAC). The proposed extension weights the regional\nrepresentations using a novel saliency measure prior to aggregation. This leads\nto further improvement in retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:14:53 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Laskar", "Zakaria", ""], ["Kannala", "Juho", ""]]}, {"id": "1703.01229", "submitter": "Lingxi Xie", "authors": "Yan Wang, Lingxi Xie, Ya Zhang, Wenjun Zhang, Alan Yuille", "title": "Deep Collaborative Learning for Visual Recognition", "comments": "Submitted to CVPR 2017 (10 pages, 5 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are playing an important role in state-of-the-art visual\nrecognition. To represent high-level visual concepts, modern networks are\nequipped with large convolutional layers, which use a large number of filters\nand contribute significantly to model complexity. For example, more than half\nof the weights of AlexNet are stored in the first fully-connected layer (4,096\nfilters).\n  We formulate the function of a convolutional layer as learning a large visual\nvocabulary, and propose an alternative way, namely Deep Collaborative Learning\n(DCL), to reduce the computational complexity. We replace a convolutional layer\nwith a two-stage DCL module, in which we first construct a couple of smaller\nconvolutional layers individually, and then fuse them at each spatial position\nto consider feature co-occurrence. In mathematics, DCL can be explained as an\nefficient way of learning compositional visual concepts, in which the\nvocabulary size increases exponentially while the model complexity only\nincreases linearly. We evaluate DCL on a wide range of visual recognition\ntasks, including a series of multi-digit number classification datasets, and\nsome generic image classification datasets such as SVHN, CIFAR and ILSVRC2012.\nWe apply DCL to several state-of-the-art network structures, improving the\nrecognition accuracy meanwhile reducing the number of parameters (16.82% fewer\nin AlexNet).\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:17:45 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wang", "Yan", ""], ["Xie", "Lingxi", ""], ["Zhang", "Ya", ""], ["Zhang", "Wenjun", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.01243", "submitter": "Long Chen Long Chen", "authors": "Long Chen, Wen Tang, Nigel W. John, Tao Ruan Wan, Jian Jun Zhang", "title": "Augmented Reality for Depth Cues in Monocular Minimally Invasive Surgery", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in Minimally Invasive Surgery (MIS) such as\nlaparoscopy is the lack of depth perception. In recent years, laparoscopic\nscene tracking and surface reconstruction has been a focus of investigation to\nprovide rich additional information to aid the surgical process and compensate\nfor the depth perception issue. However, robust 3D surface reconstruction and\naugmented reality with depth perception on the reconstructed scene are yet to\nbe reported. This paper presents our work in this area. First, we adopt a\nstate-of-the-art visual simultaneous localization and mapping (SLAM) framework\n- ORB-SLAM - and extend the algorithm for use in MIS scenes for reliable\nendoscopic camera tracking and salient point mapping. We then develop a robust\nglobal 3D surface reconstruction frame- work based on the sparse point clouds\nextracted from the SLAM framework. Our approach is to combine an outlier\nremoval filter within a Moving Least Squares smoothing algorithm and then\nemploy Poisson surface reconstruction to obtain smooth surfaces from the\nunstructured sparse point cloud. Our proposed method has been quantitatively\nevaluated compared with ground-truth camera trajectories and the organ model\nsurface we used to render the synthetic simulation videos. In vivo laparoscopic\nvideos used in the tests have demonstrated the robustness and accuracy of our\nproposed framework on both camera tracking and surface reconstruction,\nillustrating the potential of our algorithm for depth augmentation and\ndepth-corrected augmented reality in MIS with monocular endoscopes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:01:52 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Chen", "Long", ""], ["Tang", "Wen", ""], ["John", "Nigel W.", ""], ["Wan", "Tao Ruan", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "1703.01248", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Xiaorong Hou", "title": "Incident Light Frequency-based Image Defogging Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the problem of color distortion caused by the defogging algorithm\nbased on dark channel prior, an improved algorithm was proposed to calculate\nthe transmittance of all channels respectively. First, incident light\nfrequency's effect on the transmittance of various color channels was analyzed\naccording to the Beer-Lambert's Law, from which a proportion among various\nchannel transmittances was derived; afterwards, images were preprocessed by\ndown-sampling to refine transmittance, and then the original size was restored\nto enhance the operational efficiency of the algorithm; finally, the\ntransmittance of all color channels was acquired in accordance with the\nproportion, and then the corresponding transmittance was used for image\nrestoration in each channel. The experimental results show that compared with\nthe existing algorithm, this improved image defogging algorithm could make\nimage colors more natural, solve the problem of slightly higher color\nsaturation caused by the existing algorithm, and shorten the operation time by\nfour to nine times.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 17:03:17 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zhang", "Wenbo", ""], ["Hou", "Xiaorong", ""]]}, {"id": "1703.01289", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner and Michael Arens", "title": "Instance Flow Based Online Multiple Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to perform online Multiple Object Tracking (MOT) of known\nobject categories in monocular video data. Current Tracking-by-Detection MOT\napproaches build on top of 2D bounding box detections. In contrast, we exploit\nstate-of-the-art instance aware semantic segmentation techniques to compute 2D\nshape representations of target objects in each frame. We predict position and\nshape of segmented instances in subsequent frames by exploiting optical flow\ncues. We define an affinity matrix between instances of subsequent frames which\nreflects locality and visual similarity. The instance association is solved by\napplying the Hungarian method. We evaluate different configurations of our\nalgorithm using the MOT 2D 2015 train dataset. The evaluation shows that our\ntracking approach is able to track objects with high relative motions. In\naddition, we provide results of our approach on the MOT 2D 2015 test set for\ncomparison with previous works. We achieve a MOTA score of 32.1.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 18:54:55 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:14:30 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""]]}, {"id": "1703.01290", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Deyu Meng, Long Zhao, Junwei Han", "title": "Bridging Saliency Detection to Weakly Supervised Object Detection Based\n  on Self-paced Curriculum Learning", "comments": "Has published in IJCAI 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object detection (WOD) is a challenging problems in\ncomputer vision. The key problem is to simultaneously infer the exact object\nlocations in the training images and train the object detectors, given only the\ntraining images with weak image-level labels. Intuitively, by simulating the\nselective attention mechanism of human visual system, saliency detection\ntechnique can select attractive objects in scenes and thus is a potential way\nto provide useful priors for WOD. However, the way to adopt saliency detection\nin WOD is not trivial since the detected saliency region might be possibly\nhighly ambiguous in complex cases. To this end, this paper first\ncomprehensively analyzes the challenges in applying saliency detection to WOD.\nThen, we make one of the earliest efforts to bridge saliency detection to WOD\nvia the self-paced curriculum learning, which can guide the learning procedure\nto gradually achieve faithful knowledge of multi-class objects from easy to\nhard. The experimental results demonstrate that the proposed approach can\nsuccessfully bridge saliency detection and WOD tasks and achieve the\nstate-of-the-art object detection results under the weak supervision.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 18:55:10 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zhang", "Dingwen", ""], ["Meng", "Deyu", ""], ["Zhao", "Long", ""], ["Han", "Junwei", ""]]}, {"id": "1703.01382", "submitter": "Jong Chul Ye", "authors": "Jawook Gu and Jong Chul Ye", "title": "Multi-Scale Wavelet Domain Residual Learning for Limited-Angle CT\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited-angle computed tomography (CT) is often used in clinical applications\nsuch as C-arm CT for interventional imaging. However, CT images from limited\nangles suffers from heavy artifacts due to incomplete projection data. Existing\niterative methods require extensive calculations but can not deliver\nsatisfactory results. Based on the observation that the artifacts from limited\nangles have some directional property and are globally distributed, we propose\na novel multi-scale wavelet domain residual learning architecture, which\ncompensates for the artifacts. Experiments have shown that the proposed method\neffectively eliminates artifacts, thereby preserving edge and global structures\nof the image.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 02:22:59 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1703.01383", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, junhong Min and Jong Chul Ye", "title": "Wavelet Domain Residual Network (WavResNet) for Low-Dose X-ray CT\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally complex because of the repeated use of the forward and\nbackward projection. Inspired by this success of deep learning in computer\nvision applications, we recently proposed a deep convolutional neural network\n(CNN) for low-dose X-ray CT and won the second place in 2016 AAPM Low-Dose CT\nGrand Challenge. However, some of the texture are not fully recovered, which\nwas unfamiliar to some radiologists. To cope with this problem, here we propose\na direct residual learning approach on directional wavelet domain to solve this\nproblem and to improve the performance against previous work. In particular,\nthe new network estimates the noise of each input wavelet transform, and then\nthe de-noised wavelet coefficients are obtained by subtracting the noise from\nthe input wavelet transform bands. The experimental results confirm that the\nproposed network has significantly improved performance, preserving the detail\ntexture of the original images.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 02:28:54 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kang", "Eunhee", ""], ["Min", "junhong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1703.01386", "submitter": "Pongsate Tangseng", "authors": "Pongsate Tangseng, Zhipeng Wu, Kota Yamaguchi", "title": "Looking at Outfit to Parse Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends fully-convolutional neural networks (FCN) for the clothing\nparsing problem. Clothing parsing requires higher-level knowledge on clothing\nsemantics and contextual cues to disambiguate fine-grained categories. We\nextend FCN architecture with a side-branch network which we refer outfit\nencoder to predict a consistent set of clothing labels to encourage\ncombinatorial preference, and with conditional random field (CRF) to explicitly\nconsider coherent label assignment to the given image. The empirical results\nusing Fashionista and CFPD datasets show that our model achieves\nstate-of-the-art performance in clothing parsing, without additional\nsupervision during training. We also study the qualitative influence of\nannotation on the current clothing parsing benchmarks, with our Web-based tool\nfor multi-scale pixel-wise annotation and manual refinement effort to the\nFashionista dataset. Finally, we show that the image representation of the\noutfit encoder is useful for dress-up image retrieval application.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 03:09:36 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Tangseng", "Pongsate", ""], ["Wu", "Zhipeng", ""], ["Yamaguchi", "Kota", ""]]}, {"id": "1703.01396", "submitter": "Cheng Yaw Low", "authors": "Cheng-Yaw Low, Andrew Beng-Jin Teoh", "title": "Stacking-based Deep Neural Network: Deep Analytic Network on\n  Convolutional Spectral Histogram Features", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural\nnetwork (DNN) resemblance in terms of its very deep, feedforward network\narchitecture. The typical S-DNN aggregates a variable number of individually\nlearnable modules in series to assemble a DNN-alike alternative to the targeted\nobject recognition tasks. This work likewise devises an S-DNN instantiation,\ndubbed deep analytic network (DAN), on top of the spectral histogram (SH)\nfeatures. The DAN learning principle relies on ridge regression, and some key\nDNN constituents, specifically, rectified linear unit, fine-tuning, and\nnormalization. The DAN aptitude is scrutinized on three repositories of varying\ndomains, including FERET (faces), MNIST (handwritten digits), and CIFAR10\n(natural objects). The empirical results unveil that DAN escalates the SH\nbaseline performance over a sufficiently deep layer.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 04:31:43 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 15:19:50 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Low", "Cheng-Yaw", ""], ["Teoh", "Andrew Beng-Jin", ""]]}, {"id": "1703.01398", "submitter": "Fangchang Ma", "authors": "Fangchang Ma, Luca Carlone, Ulas Ayaz, Sertac Karaman", "title": "Sparse Depth Sensing for Resource-Constrained Robots", "comments": "35 pages, 31 figures, 2 tables; added new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the case in which a robot has to navigate in an unknown\nenvironment but does not have enough on-board power or payload to carry a\ntraditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few\n(point-wise) depth measurements. We address the following question: is it\npossible to reconstruct the geometry of an unknown environment using sparse and\nincomplete depth measurements? Reconstruction from incomplete data is not\npossible in general, but when the robot operates in man-made environments, the\ndepth exhibits some regularity (e.g., many planar surfaces with only a few\nedges); we leverage this regularity to infer depth from a small number of\nmeasurements. Our first contribution is a formulation of the depth\nreconstruction problem that bridges robot perception with the compressive\nsensing literature in signal processing. The second contribution includes a set\nof formal results that ascertain the exactness and stability of the depth\nreconstruction in 2D and 3D problems, and completely characterize the geometry\nof the profiles that we can reconstruct. Our third contribution is a set of\npractical algorithms for depth reconstruction: our formulation directly\ntranslates into algorithms for depth estimation based on convex programming. In\nreal-world problems, these convex programs are very large and general-purpose\nsolvers are relatively slow. For this reason, we discuss ad-hoc solvers that\nenable fast depth reconstruction in real problems. The last contribution is an\nextensive experimental evaluation in 2D and 3D problems, including Monte Carlo\nruns on simulated instances and testing on multiple real datasets. Empirical\nresults confirm that the proposed approach ensures accurate depth\nreconstruction, outperforms interpolation-based strategies, and performs well\neven when the assumption of structured environment is violated.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 05:20:30 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 04:12:34 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 05:02:51 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ma", "Fangchang", ""], ["Carlone", "Luca", ""], ["Ayaz", "Ulas", ""], ["Karaman", "Sertac", ""]]}, {"id": "1703.01402", "submitter": "Terrance DeVries", "authors": "Terrance DeVries, Dhanesh Ramachandram", "title": "Skin Lesion Classification Using Deep Multi-scale Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach to the ISIC 2017 Skin Lesion\nClassification Challenge using a multi-scale convolutional neural network. Our\napproach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,\nwhich is fine-tuned for skin lesion classification using two different scales\nof input images.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 06:32:15 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["DeVries", "Terrance", ""], ["Ramachandram", "Dhanesh", ""]]}, {"id": "1703.01425", "submitter": "Lianwen Jin", "authors": "Yuliang Liu, Lianwen Jin", "title": "Deep Matching Prior Network: Toward Tighter Multi-oriented Text\n  Detection", "comments": "8 Pages, 7 figures. Accepted to appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting incidental scene text is a challenging task because of\nmulti-orientation, perspective distortion, and variation of text size, color\nand scale. Retrospective research has only focused on using rectangular\nbounding box or horizontal sliding window to localize text, which may result in\nredundant background noise, unnecessary overlap or even information loss. To\naddress these issues, we propose a new Convolutional Neural Networks (CNNs)\nbased method, named Deep Matching Prior Network (DMPNet), to detect text with\ntighter quadrangle. First, we use quadrilateral sliding windows in several\nspecific intermediate convolutional layers to roughly recall the text with\nhigher overlapping area and then a shared Monte-Carlo method is proposed for\nfast and accurate computing of the polygonal areas. After that, we designed a\nsequential protocol for relative regression which can exactly predict text with\ncompact quadrangle. Moreover, a auxiliary smooth Ln loss is also proposed for\nfurther regressing the position of text, which has better overall performance\nthan L2 loss and smooth L1 loss in terms of robustness and stability. The\neffectiveness of our approach is evaluated on a public word-level,\nmulti-oriented scene text database, ICDAR 2015 Robust Reading Competition\nChallenge 4 \"Incidental scene text localization\". The performance of our method\nis evaluated by using F-measure and found to be 70.64%, outperforming the\nexisting state-of-the-art method with F-measure 63.76%.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 09:40:41 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""]]}, {"id": "1703.01437", "submitter": "Bharath Bhat", "authors": "Rahul Anand Sharma, Bharath Bhat, Vineet Gandhi, C.V.Jawahar", "title": "Automated Top View Registration of Broadcast Football Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to register football broadcast video\nframes on the static top view model of the playing surface. The proposed method\nis fully automatic in contrast to the current state of the art which requires\nmanual initialization of point correspondences between the image and the static\nmodel. Automatic registration using existing approaches has been difficult due\nto the lack of sufficient point correspondences. We investigate an alternate\napproach exploiting the edge information from the line markings on the field.\nWe formulate the registration problem as a nearest neighbour search over a\nsynthetically generated dictionary of edge map and homography pairs. The\nsynthetic dictionary generation allows us to exhaustively cover a wide variety\nof camera angles and positions and reduce this problem to a minimal per-frame\nedge map matching procedure. We show that the per-frame results can be improved\nin videos using an optimization framework for temporal camera stabilization. We\ndemonstrate the efficacy of our approach by presenting extensive results on a\ndataset collected from matches of football World Cup 2014.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 10:51:09 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Sharma", "Rahul Anand", ""], ["Bhat", "Bharath", ""], ["Gandhi", "Vineet", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1703.01467", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar, David Budden and Nir Shavit", "title": "Generative Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image and video compression algorithms rely on hand-crafted\nencoder/decoder pairs (codecs) that lack adaptability and are agnostic to the\ndata being compressed. Here we describe the concept of generative compression,\nthe compression of data using generative models, and suggest that it is a\ndirection worth pursuing to produce more accurate and visually pleasing\nreconstructions at much deeper compression levels for both image and video\ndata. We also demonstrate that generative compression is orders-of-magnitude\nmore resilient to bit error rates (e.g. from noisy wireless channels) than\ntraditional variable-length coding schemes.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 14:43:26 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 23:31:42 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Santurkar", "Shibani", ""], ["Budden", "David", ""], ["Shavit", "Nir", ""]]}, {"id": "1703.01499", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sambit Ghadai, Gavin Young, Soumik Sarkar, Adarsh\n  Krishnamurthy", "title": "A Machine-Learning Framework for Design for Manufacturability", "comments": "this is a duplicate submission. Hence want to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  this is a duplicate submission(original is arXiv:1612.02141). Hence want to\nwithdraw it\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 17:37:32 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 14:55:52 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Balu", "Aditya", ""], ["Ghadai", "Sambit", ""], ["Young", "Gavin", ""], ["Sarkar", "Soumik", ""], ["Krishnamurthy", "Adarsh", ""]]}, {"id": "1703.01506", "submitter": "Felipe Gutierrez-Barragan", "authors": "Felipe Gutierrez-Barragan, Vamsi K. Ithapu, Chris Hinrichs, Camille\n  Maumet, Sterling C. Johnson, Thomas E. Nichols, Vikas Singh, and the ADNI", "title": "Accelerating Permutation Testing in Voxel-wise Analysis through Subspace\n  Tracking: A new plugin for SnPM", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.07.025", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation testing is a non-parametric method for obtaining the max null\ndistribution used to compute corrected $p$-values that provide strong control\nof false positives. In neuroimaging, however, the computational burden of\nrunning such an algorithm can be significant. We find that by viewing the\npermutation testing procedure as the construction of a very large permutation\ntesting matrix, $T$, one can exploit structural properties derived from the\ndata and the test statistics to reduce the runtime under certain conditions. In\nparticular, we see that $T$ is low-rank plus a low-variance residual. This\nmakes $T$ a good candidate for low-rank matrix completion, where only a very\nsmall number of entries of $T$ ($\\sim0.35\\%$ of all entries in our experiments)\nhave to be computed to obtain a good estimate. Based on this observation, we\npresent RapidPT, an algorithm that efficiently recovers the max null\ndistribution commonly obtained through regular permutation testing in\nvoxel-wise analysis. We present an extensive validation on a synthetic dataset\nand four varying sized datasets against two baselines: Statistical\nNonParametric Mapping (SnPM13) and a standard permutation testing\nimplementation (referred as NaivePT). We find that RapidPT achieves its best\nruntime performance on medium sized datasets ($50 \\leq n \\leq 200$), with\nspeedups of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger\ndatasets ($n \\geq 200$) RapidPT outperforms NaivePT (6x - 200x) on all\ndatasets, and provides large speedups over SnPM13 when more than 10000\npermutations (2x - 15x) are needed. The implementation is a standalone toolbox\nand also integrated within SnPM13, able to leverage multi-core architectures\nwhen available.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 19:07:42 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 18:03:54 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Gutierrez-Barragan", "Felipe", ""], ["Ithapu", "Vamsi K.", ""], ["Hinrichs", "Chris", ""], ["Maumet", "Camille", ""], ["Johnson", "Sterling C.", ""], ["Nichols", "Thomas E.", ""], ["Singh", "Vikas", ""], ["ADNI", "the", ""]]}, {"id": "1703.01513", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Alan Yuille", "title": "Genetic CNN", "comments": "Submitted to CVPR 2017 (10 pages, 5 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Convolutional Neural Network (CNN) is the state-of-the-art solution\nfor large-scale visual recognition. Following basic principles such as\nincreasing the depth and constructing highway connections, researchers have\nmanually designed a lot of fixed network structures and verified their\neffectiveness.\n  In this paper, we discuss the possibility of learning deep network structures\nautomatically. Note that the number of possible network structures increases\nexponentially with the number of layers in the network, which inspires us to\nadopt the genetic algorithm to efficiently traverse this large search space. We\nfirst propose an encoding method to represent each network structure in a\nfixed-length binary string, and initialize the genetic algorithm by generating\na set of randomized individuals. In each generation, we define standard genetic\noperations, e.g., selection, mutation and crossover, to eliminate weak\nindividuals and then generate more competitive ones. The competitiveness of\neach individual is defined as its recognition accuracy, which is obtained via\ntraining the network from scratch and evaluating it on a validation set. We run\nthe genetic process on two small datasets, i.e., MNIST and CIFAR10,\ndemonstrating its ability to evolve and find high-quality structures which are\nlittle studied before. These structures are also transferrable to the\nlarge-scale ILSVRC2012 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 19:44:16 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Xie", "Lingxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.01515", "submitter": "Zheng Shou", "authors": "Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu\n  Chang", "title": "CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action\n  Localization in Untrimmed Videos", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 20:00:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 04:14:57 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Shou", "Zheng", ""], ["Chan", "Jonathan", ""], ["Zareian", "Alireza", ""], ["Miyazawa", "Kazuyuki", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1703.01526", "submitter": "Prashanth R", "authors": "R. Prashanth, Sumantra Dutta Roy, Pravat K. Mandal, Shantanu Ghosh", "title": "High Accuracy Classification of Parkinson's Disease through Shape\n  Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging", "comments": "9 pages, 5 figures, Accepted in the IEEE Journal of Biomedical and\n  Health Informatics, Additional supplementary documents available at\n  http://ieeexplore.ieee.org/document/7442754/", "journal-ref": null, "doi": "10.1109/JBHI.2016.2547901", "report-no": null, "categories": "stat.AP cs.CV physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate identification of parkinsonian syndromes (PS) involving\npresynaptic degeneration from non-degenerative variants such as Scans Without\nEvidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for\neffective patient management as the course, therapy and prognosis differ\nsubstantially between the two groups. In this study, we use Single Photon\nEmission Computed Tomography (SPECT) images from healthy normal, early PD and\nSWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative\n(PPMI) database, and process them to compute shape- and surface fitting-based\nfeatures for the three groups. We use these features to develop and compare\nvarious classification models that can discriminate between scans showing\ndopaminergic deficit, as in PD, from scans without the deficit, as in healthy\nnormal or SWEDD. Along with it, we also compare these features with Striatal\nBinding Ratio (SBR)-based features, which are well-established and clinically\nused, by computing a feature importance score using Random forests technique.\nWe observe that the Support Vector Machine (SVM) classifier gave the best\nperformance with an accuracy of 97.29%. These features also showed higher\nimportance than the SBR-based features. We infer from the study that shape\nanalysis and surface fitting are useful and promising methods for extracting\ndiscriminatory features that can be used to develop diagnostic models that\nmight have the potential to help clinicians in the diagnostic process.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 21:50:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Prashanth", "R.", ""], ["Roy", "Sumantra Dutta", ""], ["Mandal", "Pravat K.", ""], ["Ghosh", "Shantanu", ""]]}, {"id": "1703.01550", "submitter": "Bruno Korbar", "authors": "Bruno Korbar, Andrea M. Olofson, Allen P. Miraflor, Katherine M.\n  Nicka, Matthew A. Suriawinata, Lorenzo Torresani, Arief A. Suriawinata, Saeed\n  Hassanpour", "title": "Deep-Learning for Classification of Colorectal Polyps on Whole-Slide\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological characterization of colorectal polyps is an important\nprinciple for determining the risk of colorectal cancer and future rates of\nsurveillance for patients. This characterization is time-intensive, requires\nyears of specialized training, and suffers from significant inter-observer and\nintra-observer variability. In this work, we built an automatic\nimage-understanding method that can accurately classify different types of\ncolorectal polyps in whole-slide histology images to help pathologists with\nhistopathological characterization and diagnosis of colorectal polyps. The\nproposed image-understanding method is based on deep-learning techniques, which\nrely on numerous levels of abstraction for data representation and have shown\nstate-of-the-art results for various image analysis tasks. Our\nimage-understanding method covers all five polyp types (hyperplastic polyp,\nsessile serrated polyp, traditional serrated adenoma, tubular adenoma, and\ntubulovillous/villous adenoma) that are included in the US multi-society task\nforce guidelines for colorectal cancer risk assessment and surveillance, and\nencompasses the most common occurrences of colorectal polyps. Our evaluation on\n239 independent test samples shows our proposed method can identify the types\nof colorectal polyps in whole-slide images with a high efficacy (accuracy:\n93.0%, precision: 89.7%, recall: 88.3%, F1 score: 88.8%). The presented method\nin this paper can reduce the cognitive burden on pathologists and improve their\naccuracy and efficiency in histopathological characterization of colorectal\npolyps, and in subsequent risk assessment and follow-up recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 03:17:35 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 22:26:56 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Korbar", "Bruno", ""], ["Olofson", "Andrea M.", ""], ["Miraflor", "Allen P.", ""], ["Nicka", "Katherine M.", ""], ["Suriawinata", "Matthew A.", ""], ["Torresani", "Lorenzo", ""], ["Suriawinata", "Arief A.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1703.01560", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Anitha Kannan, Dhruv Batra, Devi Parikh", "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image\n  Generation", "comments": "21 pages, 22 figures, published as a conference paper at ICLR 2017,\n  code available on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LR-GAN: an adversarial image generation model which takes scene\nstructure and context into account. Unlike previous generative adversarial\nnetworks (GANs), the proposed GAN learns to generate image background and\nforegrounds separately and recursively, and stitch the foregrounds on the\nbackground in a contextually relevant manner to produce a complete natural\nimage. For each foreground, the model learns to generate its appearance, shape\nand pose. The whole model is unsupervised, and is trained in an end-to-end\nmanner with gradient descent methods. The experiments demonstrate that LR-GAN\ncan generate more natural images with objects that are more human recognizable\nthan DCGAN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 05:17:56 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 10:30:54 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 03:51:57 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Yang", "Jianwei", ""], ["Kannan", "Anitha", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1703.01564", "submitter": "Connor Schenck", "authors": "Conor Schenck and Dieter Fox", "title": "Perceiving and Reasoning About Liquids Using Fully Convolutional\n  Networks", "comments": "In The International Journal of Robotics Research (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquids are an important part of many common manipulation tasks in human\nenvironments. If we wish to have robots that can accomplish these types of\ntasks, they must be able to interact with liquids in an intelligent manner. In\nthis paper, we investigate ways for robots to perceive and reason about\nliquids. That is, a robot asks the questions What in the visual data stream is\nliquid? and How can I use that to infer all the potential places where liquid\nmight be? We collected two datasets to evaluate these questions, one using a\nrealistic liquid simulator and another on our robot. We used fully\nconvolutional neural networks to learn to detect and track liquids across\npouring sequences. Our results show that these networks are able to perceive\nand reason about liquids, and that integrating temporal information is\nimportant to performing such tasks well.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 07:10:50 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 19:40:49 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Schenck", "Conor", ""], ["Fox", "Dieter", ""]]}, {"id": "1703.01597", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, K\\'evin Bailly, S\\'everine Dubuisson", "title": "Face Alignment with Cascaded Semi-Parametric Deep Greedy Neural Forests", "comments": "10 pages, 1 page appendix, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment is an active topic in computer vision, consisting in aligning\na shape model on the face. To this end, most modern approaches refine the shape\nin a cascaded manner, starting from an initial guess. Those shape updates can\neither be applied in the feature point space (\\textit{i.e.} explicit updates)\nor in a low-dimensional, parametric space. In this paper, we propose a\nsemi-parametric cascade that first aligns a parametric shape, then captures\nmore fine-grained deformations of an explicit shape. For the purpose of\nlearning shape updates at each cascade stage, we introduce a deep greedy neural\nforest (GNF) model, which is an improved version of deep neural forest (NF).\nGNF appears as an ideal regressor for face alignment, as it combines\ndifferentiability, high expressivity and fast evaluation runtime. The proposed\nframework is very fast and achieves high accuracies on multiple challenging\nbenchmarks, including small, medium and large pose experiments.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 13:47:46 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""], ["Dubuisson", "S\u00e9verine", ""]]}, {"id": "1703.01605", "submitter": "Yongwei Nie", "authors": "Yongwei Nie, Xu Cao, Chengjiang Long, Ping Li, Guiqing Li", "title": "L2GSCI: Local to Global Seam Cutting and Integrating for Accurate Face\n  Contour Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current face alignment algorithms can robustly find a set of landmarks along\nface contour. However, the landmarks are sparse and lack curve details,\nespecially in chin and cheek areas where a lot of concave-convex bending\ninformation exists. In this paper, we propose a local to global seam cutting\nand integrating algorithm (L2GSCI) to extract continuous and accurate face\ncontour. Our method works in three steps with the help of a rough initial\ncurve. First, we sample small and overlapped squares along the initial curve.\nSecond, the seam cutting part of L2GSCI extracts a local seam in each square\nregion. Finally, the seam integrating part of L2GSCI connects all the redundant\nseams together to form a continuous and complete face curve. Overall, the\nproposed method is much more straightforward than existing face alignment\nalgorithms, but can achieve pixel-level continuous face curves rather than\ndiscrete and sparse landmarks. Moreover, experiments on two face benchmark\ndatasets (i.e., LFPW and HELEN) show that our method can precisely reveal\nconcave-convex bending details of face contours, which has significantly\nimproved the performance when compared with the state-ofthe- art face alignment\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 15:06:28 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Nie", "Yongwei", ""], ["Cao", "Xu", ""], ["Long", "Chengjiang", ""], ["Li", "Ping", ""], ["Li", "Guiqing", ""]]}, {"id": "1703.01622", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christian Knipfer, Nicolai Oetter, Christian\n  Jaremenko, Erik Rodner, Joachim Denzler, Christopher Bohr, Helmut Neumann,\n  Florian Stelzle and Andreas Maier", "title": "Automatic Classification of Cancerous Tissue in Laserendomicroscopy\n  Images of the Oral Cavity using Deep Learning", "comments": "12 pages, 5 figures", "journal-ref": "Scientific Reports 7, Article number: 11979 (2017)", "doi": "10.1038/s41598-017-12320-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral\nepithelium. Despite their high impact on mortality, sufficient screening\nmethods for early diagnosis of OSCC often lack accuracy and thus OSCCs are\nmostly diagnosed at a late stage. Early detection and accurate outline\nestimation of OSCCs would lead to a better curative outcome and an reduction in\nrecurrence rates after surgical treatment.\n  Confocal Laser Endomicroscopy (CLE) records sub-surface micro-anatomical\nimages for in vivo cell structure analysis. Recent CLE studies showed great\nprospects for a reliable, real-time ultrastructural imaging of OSCC in situ.\n  We present and evaluate a novel automatic approach for a highly accurate OSCC\ndiagnosis using deep learning technologies on CLE images. The method is\ncompared against textural feature-based machine learning approaches that\nrepresent the current state of the art.\n  For this work, CLE image sequences (7894 images) from patients diagnosed with\nOSCC were obtained from 4 specific locations in the oral cavity, including the\nOSCC lesion. The present approach is found to outperform the state of the art\nin CLE image recognition with an area under the curve (AUC) of 0.96 and a mean\naccuracy of 88.3% (sensitivity 86.6%, specificity 90%).\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 16:48:01 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 10:58:24 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Aubreville", "Marc", ""], ["Knipfer", "Christian", ""], ["Oetter", "Nicolai", ""], ["Jaremenko", "Christian", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""], ["Bohr", "Christopher", ""], ["Neumann", "Helmut", ""], ["Stelzle", "Florian", ""], ["Maier", "Andreas", ""]]}, {"id": "1703.01656", "submitter": "Connor Schenck", "authors": "Connor Schenck and Dieter Fox", "title": "Reasoning About Liquids via Closed-Loop Simulation", "comments": "Robotics: Science & Systems (RSS), July 12-16, 2017. Cambridge, MA,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulators are powerful tools for reasoning about a robot's interactions with\nits environment. However, when simulations diverge from reality, that reasoning\nbecomes less useful. In this paper, we show how to close the loop between\nliquid simulation and real-time perception. We use observations of liquids to\ncorrect errors when tracking the liquid's state in a simulator. Our results\nshow that closed-loop simulation is an effective way to prevent large\ndivergence between the simulated and real liquid states. As a direct\nconsequence of this, our method can enable reasoning about liquids that would\notherwise be infeasible due to large divergences, such as reasoning about\noccluded liquid.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 19:53:37 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 22:51:08 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Schenck", "Connor", ""], ["Fox", "Dieter", ""]]}, {"id": "1703.01661", "submitter": "Bolei Zhou", "authors": "Jay M. Wong, Vincent Kee, Tiffany Le, Syler Wagner, Gian-Luca\n  Mariottini, Abraham Schneider, Lei Hamilton, Rahul Chipalkatty, Mitchell\n  Hebert, David M.S. Johnson, Jimmy Wu, Bolei Zhou, and Antonio Torralba", "title": "SegICP: Integrated Deep Semantic Segmentation and Pose Estimation", "comments": "IROS camera-ready", "journal-ref": null, "doi": "10.1109/IROS.2017.8206470", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent robotic manipulation competitions have highlighted that sophisticated\nrobots still struggle to achieve fast and reliable perception of task-relevant\nobjects in complex, realistic scenarios. To improve these systems' perceptive\nspeed and robustness, we present SegICP, a novel integrated solution to object\nrecognition and pose estimation. SegICP couples convolutional neural networks\nand multi-hypothesis point cloud registration to achieve both robust pixel-wise\nsemantic segmentation as well as accurate and real-time 6-DOF pose estimation\nfor relevant objects. Our architecture achieves 1cm position error and\n<5^\\circ$ angle error in real time without an initial seed. We evaluate and\nbenchmark SegICP against an annotated dataset generated by motion capture.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 20:42:44 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 19:59:39 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wong", "Jay M.", ""], ["Kee", "Vincent", ""], ["Le", "Tiffany", ""], ["Wagner", "Syler", ""], ["Mariottini", "Gian-Luca", ""], ["Schneider", "Abraham", ""], ["Hamilton", "Lei", ""], ["Chipalkatty", "Rahul", ""], ["Hebert", "Mitchell", ""], ["Johnson", "David M. S.", ""], ["Wu", "Jimmy", ""], ["Zhou", "Bolei", ""], ["Torralba", "Antonio", ""]]}, {"id": "1703.01664", "submitter": "Yijun Li", "authors": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang", "title": "Diversified Texture Synthesis with Feed-forward Networks", "comments": "accepted by CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progresses on deep discriminative and generative modeling have shown\npromising results on texture synthesis. However, existing feed-forward based\nmethods trade off generality for efficiency, which suffer from many issues,\nsuch as shortage of generality (i.e., build one network per texture), lack of\ndiversity (i.e., always produce visually identical output) and suboptimality\n(i.e., generate less satisfying visual effects). In this work, we focus on\nsolving these issues for improved texture synthesis. We propose a deep\ngenerative feed-forward network which enables efficient synthesis of multiple\ntextures within one single network and meaningful interpolation between them.\nMeanwhile, a suite of important techniques are introduced to achieve better\nconvergence and diversity. With extensive experiments, we demonstrate the\neffectiveness of the proposed model and techniques for synthesizing a large\nnumber of textures and show its applications with the stylization.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 21:09:19 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Li", "Yijun", ""], ["Fang", "Chen", ""], ["Yang", "Jimei", ""], ["Wang", "Zhaowen", ""], ["Lu", "Xin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1703.01698", "submitter": "Abhineet Singh", "authors": "Mennatullah Siam, Abhineet Singh, Camilo Perez and Martin Jagersand", "title": "4-DoF Tracking for Robot Fine Manipulation Tasks", "comments": "accepted in CRV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two visual trackers from the different paradigms of\nlearning and registration based tracking and evaluates their application in\nimage based visual servoing. They can track object motion with four degrees of\nfreedom (DoF) which, as we will show here, is sufficient for many fine\nmanipulation tasks. One of these trackers is a newly developed learning based\ntracker that relies on learning discriminative correlation filters while the\nother is a refinement of a recent 8 DoF RANSAC based tracker adapted with a new\nappearance model for tracking 4 DoF motion.\n  Both trackers are shown to provide superior performance to several state of\nthe art trackers on an existing dataset for manipulation tasks. Further, a new\ndataset with challenging sequences for fine manipulation tasks captured from\nrobot mounted eye-in-hand (EIH) cameras is also presented. These sequences have\na variety of challenges encountered during real tasks including jittery camera\nmovement, motion blur, drastic scale changes and partial occlusions.\nQuantitative and qualitative results on these sequences are used to show that\nthese two trackers are robust to failures while providing high precision that\nmakes them suitable for such fine manipulation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 00:59:46 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 01:33:14 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Siam", "Mennatullah", ""], ["Singh", "Abhineet", ""], ["Perez", "Camilo", ""], ["Jagersand", "Martin", ""]]}, {"id": "1703.01702", "submitter": "Jingwu He", "authors": "Jingwu He, Linbo Wang, Wenzhe Zhou, Hongjie Zhang, Xiufen Cui, and\n  Yanwen Guo", "title": "Viewpoint Selection for Photographing Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of how to choose good viewpoints for taking\nphotographs of architectures. We achieve this by learning from professional\nphotographs of world famous landmarks that are available on the Internet.\nUnlike previous efforts devoted to photo quality assessment which mainly rely\non 2D image features, we show in this paper combining 2D image features\nextracted from images with 3D geometric features computed on the 3D models can\nresult in more reliable evaluation of viewpoint quality. Specifically, we\ncollect a set of photographs for each of 15 world famous architectures as well\nas their 3D models from the Internet. Viewpoint recovery for images is carried\nout through an image-model registration process, after which a newly proposed\nviewpoint clustering strategy is exploited to validate users' viewpoint\npreferences when photographing landmarks. Finally, we extract a number of 2D\nand 3D features for each image based on multiple visual and geometric cues and\nperform viewpoint recommendation by learning from both 2D and 3D features using\na specifically designed SVM-2K multi-view learner, achieving superior\nperformance over using solely 2D or 3D features. We show the effectiveness of\nthe proposed approach through extensive experiments. The experiments also\ndemonstrate that our system can be used to recommend viewpoints for rendering\ntextured 3D models of buildings for the use of architectural design, in\naddition to viewpoint evaluation of photographs and recommendation of\nviewpoints for photographing architectures in practice.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 01:44:35 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["He", "Jingwu", ""], ["Wang", "Linbo", ""], ["Zhou", "Wenzhe", ""], ["Zhang", "Hongjie", ""], ["Cui", "Xiufen", ""], ["Guo", "Yanwen", ""]]}, {"id": "1703.01725", "submitter": "Jack Hessel", "authors": "Jack Hessel, Lillian Lee, David Mimno", "title": "Cats and Captions vs. Creators and the Clock: Comparing Multimodal\n  Content to Context in Predicting Relative Popularity", "comments": "10 pages, data and models available at\n  http://www.cs.cornell.edu/~jhessel/cats/cats.html, Proceedings of WWW 2017", "journal-ref": null, "doi": "10.1145/3038912.3052684", "report-no": null, "categories": "cs.SI cs.CL cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The content of today's social media is becoming more and more rich,\nincreasingly mixing text, images, videos, and audio. It is an intriguing\nresearch question to model the interplay between these different modes in\nattracting user attention and engagement. But in order to pursue this study of\nmultimodal content, we must also account for context: timing effects, community\npreferences, and social factors (e.g., which authors are already popular) also\naffect the amount of feedback and reaction that social-media posts receive. In\nthis work, we separate out the influence of these non-content factors in\nseveral ways. First, we focus on ranking pairs of submissions posted to the\nsame community in quick succession, e.g., within 30 seconds, this framing\nencourages models to focus on time-agnostic and community-specific content\nfeatures. Within that setting, we determine the relative performance of author\nvs. content features. We find that victory usually belongs to \"cats and\ncaptions,\" as visual and textual features together tend to outperform\nidentity-based features. Moreover, our experiments show that when considered in\nisolation, simple unigram text features and deep neural network visual features\nyield the highest accuracy individually, and that the combination of the two\nmodalities generally leads to the best accuracies overall.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 04:56:19 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""], ["Mimno", "David", ""]]}, {"id": "1703.01775", "submitter": "Edouard Oyallon", "authors": "Edouard Oyallon", "title": "Building a Regular Decision Boundary with Deep Networks", "comments": "CVPR 2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we build a generic architecture of Convolutional Neural\nNetworks to discover empirical properties of neural networks. Our first\ncontribution is to introduce a state-of-the-art framework that depends upon few\nhyper parameters and to study the network when we vary them. It has no max\npooling, no biases, only 13 layers, is purely convolutional and yields up to\n95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the\nnonlinearity of a deep network does not need to be continuous, non expansive or\npoint-wise, to achieve good performance. We show that increasing the width of\nour network permits being competitive with very deep networks. Our second\ncontribution is an analysis of the contraction and separation properties of\nthis network. Indeed, a 1-nearest neighbor classifier applied on deep features\nprogressively improves with depth, which indicates that the representation is\nprogressively more regular. Besides, we defined and analyzed local support\nvectors that separate classes locally. All our experiments are reproducible and\ncode is available online, based on TensorFlow.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:21:35 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Oyallon", "Edouard", ""]]}, {"id": "1703.01790", "submitter": "Maedeh Aghaei", "authors": "Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva", "title": "All the people around me: face discovery in egocentric photo-streams", "comments": "5 pages, 3 figures, accepted in IEEE International Conference on\n  Image Processing (ICIP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an unconstrained stream of images captured by a wearable photo-camera\n(2fpm), we propose an unsupervised bottom-up approach for automatic clustering\nappearing faces into the individual identities present in these data. The\nproblem is challenging since images are acquired under real world conditions;\nhence the visible appearance of the people in the images undergoes intensive\nvariations. Our proposed pipeline consists of first arranging the photo-stream\ninto events, later, localizing the appearance of multiple people in them, and\nfinally, grouping various appearances of the same person across different\nevents. Experimental results performed on a dataset acquired by wearing a\nphoto-camera during one month, demonstrate the effectiveness of the proposed\napproach for the considered purpose.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:50:39 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 11:29:38 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1703.01827", "submitter": "Di Xie", "authors": "Di Xie and Jiang Xiong and Shiliang Pu", "title": "All You Need is Beyond a Good Init: Exploring Better Solution for\n  Training Extremely Deep Convolutional Neural Networks with Orthonormality and\n  Modulation", "comments": "Updating experiments; CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network is difficult to train and this predicament becomes worse\nas the depth increases. The essence of this problem exists in the magnitude of\nbackpropagated errors that will result in gradient vanishing or exploding\nphenomenon. We show that a variant of regularizer which utilizes orthonormality\namong different filter banks can alleviate this problem. Moreover, we design a\nbackward error modulation mechanism based on the quasi-isometry assumption\nbetween two consecutive parametric layers. Equipped with these two ingredients,\nwe propose several novel optimization solutions that can be utilized for\ntraining a specific-structured (repetitively triple modules of Conv-BNReLU)\nextremely deep convolutional neural network (CNN) WITHOUT any shortcuts/\nidentity mappings from scratch. Experiments show that our proposed solutions\ncan achieve distinct improvements for a 44-layer and a 110-layer plain networks\non both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train\nplain CNNs to match the performance of the residual counterparts.\n  Besides, we propose new principles for designing network structure from the\ninsights evoked by orthonormality. Combined with residual structure, we achieve\ncomparative performance on the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 11:54:43 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 08:22:09 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 02:12:29 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Xie", "Di", ""], ["Xiong", "Jiang", ""], ["Pu", "Shiliang", ""]]}, {"id": "1703.01842", "submitter": "Mathilde M\\'enoret", "authors": "Mathilde M\\'enoret, Nicolas Farrugia, Bastien Pasdeloup and Vincent\n  Gripon", "title": "Evaluating Graph Signal Processing for Neuroimaging Through\n  Classification and Dimensionality Reduction", "comments": "5 pages, GlobalSIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Signal Processing (GSP) is a promising framework to analyze\nmulti-dimensional neuroimaging datasets, while taking into account both the\nspatial and functional dependencies between brain signals. In the present work,\nwe apply dimensionality reduction techniques based on graph representations of\nthe brain to decode brain activity from real and simulated fMRI datasets. We\nintroduce seven graphs obtained from a) geometric structure and/or b)\nfunctional connectivity between brain areas at rest, and compare them when\nperforming dimension reduction for classification. We show that mixed graphs\nusing both a) and b) offer the best performance. We also show that graph\nsampling methods perform better than classical dimension reduction including\nPrincipal Component Analysis (PCA) and Independent Component Analysis (ICA).\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 12:45:39 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 08:40:56 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 12:41:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["M\u00e9noret", "Mathilde", ""], ["Farrugia", "Nicolas", ""], ["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""]]}, {"id": "1703.01883", "submitter": "Guido Borghi", "authors": "Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara", "title": "Deep Head Pose Estimation from Depth Data for In-car Automotive\n  Applications", "comments": "2nd International Workshop on Understanding Human Activities through\n  3D Sensors (ICPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches have achieved promising results in various\nfields of computer vision. In this paper, we tackle the problem of head pose\nestimation through a Convolutional Neural Network (CNN). Differently from other\nproposals in the literature, the described system is able to work directly and\nbased only on raw depth data. Moreover, the head pose estimation is solved as a\nregression problem and does not rely on visual facial features like facial\nlandmarks. We tested our system on a well known public dataset, Biwi Kinect\nHead Pose, showing that our approach achieves state-of-art results and is able\nto meet real time performance requirements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 14:11:55 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Venturelli", "Marco", ""], ["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1703.01918", "submitter": "Ronald Kemker", "authors": "Ronald Kemker, Carl Salvaggio, and Christopher Kanan", "title": "High-Resolution Multispectral Dataset for Semantic Segmentation", "comments": "9 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aircraft have decreased the cost required to collect remote sensing\nimagery, which has enabled researchers to collect high-spatial resolution data\nfrom multiple sensor modalities more frequently and easily. The increase in\ndata will push the need for semantic segmentation frameworks that are able to\nclassify non-RGB imagery, but this type of algorithmic development requires an\nincrease in publicly available benchmark datasets with class labels. In this\npaper, we introduce a high-resolution multispectral dataset with image labels.\nThis new benchmark dataset has been pre-split into training/testing folds in\norder to standardize evaluation and continue to push state-of-the-art\nclassification frameworks for non-RGB imagery.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:16:56 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kemker", "Ronald", ""], ["Salvaggio", "Carl", ""], ["Kanan", "Christopher", ""]]}, {"id": "1703.01972", "submitter": "Benjamin Berkels", "authors": "Rosalia Tatano, Benjamin Berkels, Thomas M. Deserno", "title": "Mesh-to-raster based non-rigid registration of multi-modal images", "comments": null, "journal-ref": "Journal of Medical Imaging 4(4), 044002 (27 October 2017)", "doi": "10.1117/1.JMI.4.4.044002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region of interest (ROI) alignment in medical images plays a crucial role in\ndiagnostics, procedure planning, treatment, and follow-up. Frequently, a model\nis represented as triangulated mesh while the patient data is provided from CAT\nscanners as pixel or voxel data. Previously, we presented a 2D method for\ncurve-to-pixel registration. This paper contributes (i) a general\nmesh-to-raster (M2R) framework to register ROIs in multi-modal images; (ii) a\n3D surface-to-voxel application, and (iii) a comprehensive quantitative\nevaluation in 2D using ground truth provided by the simultaneous truth and\nperformance level estimation (STAPLE) method. The registration is formulated as\na minimization problem where the objective consists of a data term, which\ninvolves the signed distance function of the ROI from the reference image, and\na higher order elastic regularizer for the deformation. The evaluation is based\non quantitative light-induced fluoroscopy (QLF) and digital photography (DP) of\ndecalcified teeth. STAPLE is computed on 150 image pairs from 32 subjects, each\nshowing one corresponding tooth in both modalities. The ROI in each image is\nmanually marked by three experts (900 curves in total). In the QLF-DP setting,\nour approach significantly outperforms the mutual information-based\nregistration algorithm implemented with the Insight Segmentation and\nRegistration Toolkit (ITK) and Elastix.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:58:03 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 15:35:05 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Tatano", "Rosalia", ""], ["Berkels", "Benjamin", ""], ["Deserno", "Thomas M.", ""]]}, {"id": "1703.01976", "submitter": "Iv\\'an Gonz\\'alez-D\\'iaz", "authors": "Iv\\'an Gonz\\'alez D\\'iaz", "title": "Incorporating the Knowledge of Dermatologists to Convolutional Neural\n  Networks for the Diagnosis of Skin Lesions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our submission to the ISIC 2017 Challenge in Skin\nLesion Analysis Towards Melanoma Detection. We have participated in the Part 3:\nLesion Classification with a system for automatic diagnosis of nevus, melanoma\nand seborrheic keratosis. Our approach aims to incorporate the expert knowledge\nof dermatologists into the well known framework of Convolutional Neural\nNetworks (CNN), which have shown impressive performance in many visual\nrecognition tasks. In particular, we have designed several networks providing\nlesion area identification, lesion segmentation into structural patterns and\nfinal diagnosis of clinical cases. Furthermore, novel blocks for CNNs have been\ndesigned to integrate this information with the diagnosis processing pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:02:19 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 15:22:59 GMT"}, {"version": "v3", "created": "Fri, 2 Jun 2017 11:19:21 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["D\u00edaz", "Iv\u00e1n Gonz\u00e1lez", ""]]}, {"id": "1703.02000", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang,\n  Yong Yu, Jun Wang", "title": "Activation Maximization Generative Adversarial Nets", "comments": "Accepted as a conference paper on ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels have been empirically shown useful in improving the sample\nquality of generative adversarial nets (GANs). In this paper, we mathematically\nstudy the properties of the current variants of GANs that make use of class\nlabel information. With class aware gradient and cross-entropy decomposition,\nwe reveal how class labels and associated losses influence GAN's training.\nBased on that, we propose Activation Maximization Generative Adversarial\nNetworks (AM-GAN) as an advanced solution. Comprehensive experiments have been\nconducted to validate our analysis and evaluate the effectiveness of our\nsolution, where AM-GAN outperforms other strong baselines and achieves\nstate-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we\ndemonstrate that, with the Inception ImageNet classifier, Inception Score\nmainly tracks the diversity of the generator, and there is, however, no\nreliable evidence that it can reflect the true sample quality. We thus propose\na new metric, called AM Score, to provide a more accurate estimation of the\nsample quality. Our proposed model also outperforms the baseline methods in the\nnew metric.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:42:55 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 16:33:55 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 15:32:29 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 16:56:07 GMT"}, {"version": "v5", "created": "Sat, 5 Aug 2017 08:17:04 GMT"}, {"version": "v6", "created": "Wed, 8 Nov 2017 13:49:19 GMT"}, {"version": "v7", "created": "Tue, 30 Jan 2018 18:28:35 GMT"}, {"version": "v8", "created": "Wed, 11 Jul 2018 05:43:27 GMT"}, {"version": "v9", "created": "Fri, 16 Nov 2018 07:18:19 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zhou", "Zhiming", ""], ["Cai", "Han", ""], ["Rong", "Shu", ""], ["Song", "Yuxuan", ""], ["Ren", "Kan", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1703.02009", "submitter": "Lars Ruthotto", "authors": "Eldad Haber, Lars Ruthotto, Elliot Holtham, Seong-Hwan Jun", "title": "Learning across scales - A multiscale method for Convolution Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish the relation between optimal control and training\ndeep Convolution Neural Networks (CNNs). We show that the forward propagation\nin CNNs can be interpreted as a time-dependent nonlinear differential equation\nand learning as controlling the parameters of the differential equation such\nthat the network approximates the data-label relation for given training data.\nUsing this continuous interpretation we derive two new methods to scale CNNs\nwith respect to two different dimensions. The first class of multiscale methods\nconnects low-resolution and high-resolution data through prolongation and\nrestriction of CNN parameters. We demonstrate that this enables classifying\nhigh-resolution images using CNNs trained with low-resolution images and vice\nversa and warm-starting the learning process. The second class of multiscale\nmethods connects shallow and deep networks and leads to new training strategies\nthat gradually increase the depths of the CNN while re-using parameters for\ninitializations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 18:15:40 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 16:39:14 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Haber", "Eldad", ""], ["Ruthotto", "Lars", ""], ["Holtham", "Elliot", ""], ["Jun", "Seong-Hwan", ""]]}, {"id": "1703.02016", "submitter": "V\\'ictor Arellano Vicente", "authors": "Victor Arellano, Diego Gutierrez and Adrian Jarabo", "title": "Fast Back-Projection for Non-Line of Sight Reconstruction", "comments": null, "journal-ref": "Opt. Express 25, 11574-11583 (2017)", "doi": "10.1364/OE.25.011574", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated non-line of sight (NLOS) reconstruction by\nusing the time-resolved signal frommultiply scattered light. These works\ncombine ultrafast imaging systems with computation, which back-projects the\nrecorded space-time signal to build a probabilistic map of the hidden geometry.\nUnfortunately, this computation is slow, becoming a bottleneck as the imaging\ntechnology improves. In this work, we propose a new back-projection technique\nfor NLOS reconstruction, which is up to a thousand times faster than previous\nwork, with almost no quality loss. We base on the observation that the hidden\ngeometry probability map can be built as the intersection of the three-bounce\nspace-time manifolds defined by the light illuminating the hidden geometry and\nthe visible point receiving the scattered light from such hidden geometry. This\nallows us to pose the reconstruction of the hidden geometry as the voxelization\nof these space-time manifolds, which has lower theoretic complexity and is\neasily implementable in the GPU. We demonstrate the efficiency and quality of\nour technique compared against previous methods in both captured and synthetic\ndata\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 18:33:01 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 11:02:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Arellano", "Victor", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1703.02018", "submitter": "Ashvin Nair", "authors": "Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel,\n  Jitendra Malik, Sergey Levine", "title": "Combining Self-Supervised Learning and Imitation for Vision-Based Rope\n  Manipulation", "comments": "8 pages, accepted to International Conference on Robotics and\n  Automation (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation of deformable objects, such as ropes and cloth, is an important\nbut challenging problem in robotics. We present a learning-based system where a\nrobot takes as input a sequence of images of a human manipulating a rope from\nan initial to goal configuration, and outputs a sequence of actions that can\nreproduce the human demonstration, using only monocular images as input. To\nperform this task, the robot learns a pixel-level inverse dynamics model of\nrope manipulation directly from images in a self-supervised manner, using about\n60K interactions with the rope collected autonomously by the robot. The human\ndemonstration provides a high-level plan of what to do and the low-level\ninverse model is used to execute the plan. We show that by combining the high\nand low-level plans, the robot can successfully manipulate a rope into a\nvariety of target shapes using only a sequence of human-provided images for\ndirection.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 18:40:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Nair", "Ashvin", ""], ["Chen", "Dian", ""], ["Agrawal", "Pulkit", ""], ["Isola", "Phillip", ""], ["Abbeel", "Pieter", ""], ["Malik", "Jitendra", ""], ["Levine", "Sergey", ""]]}, {"id": "1703.02036", "submitter": "Jakob Wasserthal", "authors": "Jakob Wasserthal, Peter F. Neher, Fabian Isensee, Klaus H. Maier-Hein", "title": "Direct White Matter Bundle Segmentation using Stacked U-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art method for automatically segmenting white matter bundles\nin diffusion-weighted MRI is tractography in conjunction with streamline\ncluster selection. This process involves long chains of processing steps which\nare not only computationally expensive but also complex to setup and tedious\nwith respect to quality control. Direct bundle segmentation methods treat the\ntask as a traditional image segmentation problem. While they so far did not\ndeliver competitive results, they can potentially mitigate many of the\nmentioned issues. We present a novel supervised approach for direct tract\nsegmentation that shows major performance gains. It builds upon a stacked U-Net\narchitecture which is trained on manual bundle segmentations from Human\nConnectome Project subjects. We evaluate our approach \\textit{in vivo} as well\nas \\textit{in silico} using the ISMRM 2015 Tractography Challenge phantom\ndataset. We achieve human segmentation performance and a major performance gain\nover previous pipelines. We show how the learned spatial priors efficiently\nguide the segmentation even at lower image qualities with little quality loss.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 14:21:49 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Wasserthal", "Jakob", ""], ["Neher", "Peter F.", ""], ["Isensee", "Fabian", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1703.02083", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, and Ali Gholipour", "title": "Auto-context Convolutional Neural Network (Auto-Net) for Brain\n  Extraction in Magnetic Resonance Imaging", "comments": "This manuscripts has been submitted to TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction or whole brain segmentation is an important first step in\nmany of the neuroimage analysis pipelines. The accuracy and robustness of brain\nextraction, therefore, is crucial for the accuracy of the entire brain analysis\nprocess. With the aim of designing a learning-based, geometry-independent and\nregistration-free brain extraction tool in this study, we present a technique\nbased on an auto-context convolutional neural network (CNN), in which intrinsic\nlocal and global image features are learned through 2D patches of different\nwindow sizes. In this architecture three parallel 2D convolutional pathways for\nthree different directions (axial, coronal, and sagittal) implicitly learn 3D\nimage information without the need for computationally expensive 3D\nconvolutions. Posterior probability maps generated by the network are used\niteratively as context information along with the original image patches to\nlearn the local shape and connectedness of the brain, to extract it from\nnon-brain tissue.\n  The brain extraction results we have obtained from our algorithm are superior\nto the recently reported results in the literature on two publicly available\nbenchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap\ncoefficients of 97.42% and 95.40%, respectively. Furthermore, we evaluated the\nperformance of our algorithm in the challenging problem of extracting\narbitrarily-oriented fetal brains in reconstructed fetal brain magnetic\nresonance imaging (MRI) datasets. In this application our algorithm performed\nmuch better than the other methods (Dice coefficient: 95.98%), where the other\nmethods performed poorly due to the non-standard orientation and geometry of\nthe fetal brain in MRI. Our CNN-based method can provide accurate,\ngeometry-independent brain extraction in challenging applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:50:20 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 20:31:43 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Erdogmus", "Deniz", ""], ["Gholipour", "Ali", ""]]}, {"id": "1703.02124", "submitter": "Jonathan Leach", "authors": "Susan Chan, Ryan E. Warburton, Genevieve Gariepy, Jonathan Leach,\n  Daniele Faccio", "title": "Non-line-of-sight tracking of people at long range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A remote-sensing system that can determine the position of hidden objects has\napplications in many critical real-life scenarios, such as search and rescue\nmissions and safe autonomous driving. Previous work has shown the ability to\nrange and image objects hidden from the direct line of sight, employing\nadvanced optical imaging technologies aimed at small objects at short range. In\nthis work we demonstrate a long-range tracking system based on single laser\nillumination and single-pixel single-photon detection. This enables us to track\none or more people hidden from view at a stand-off distance of over 50~m. These\nresults pave the way towards next generation LiDAR systems that will\nreconstruct not only the direct-view scene but also the main elements hidden\nbehind walls or corners.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 16:57:04 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Chan", "Susan", ""], ["Warburton", "Ryan E.", ""], ["Gariepy", "Genevieve", ""], ["Leach", "Jonathan", ""], ["Faccio", "Daniele", ""]]}, {"id": "1703.02150", "submitter": "Sheng Xu", "authors": "Sheng Xu, Ruisheng Wang, Han Zheng", "title": "An optimal hierarchical clustering approach to segmentation of mobile\n  LiDAR point clouds", "comments": "Submitted to IEEE Transaction on Geoscience and Remote Sense", "journal-ref": null, "doi": "10.1109/TITS.2019.2912455", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical clustering approach for the segmentation\nof mobile LiDAR point clouds. We perform the hierarchical clustering on\nunorganized point clouds based on a proximity matrix. The dissimilarity measure\nin the proximity matrix is calculated by the Euclidean distances between\nclusters and the difference of normal vectors at given points. The main\ncontribution of this paper is that we succeed to optimize the combination of\nclusters in the hierarchical clustering. The combination is obtained by\nachieving the matching of a bipartite graph, and optimized by solving the\nminimum-cost perfect matching. Results show that the proposed optimal\nhierarchical clustering (OHC) succeeds to achieve the segmentation of multiple\nindividual objects automatically and outperforms the state-of-the-art LiDAR\npoint cloud segmentation approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 23:48:16 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 21:12:45 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Xu", "Sheng", ""], ["Wang", "Ruisheng", ""], ["Zheng", "Han", ""]]}, {"id": "1703.02161", "submitter": "Sofia Ira Ktena", "authors": "Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew\n  Lee, Ben Glocker, Daniel Rueckert", "title": "Distance Metric Learning using Graph Convolutional Networks: Application\n  to Functional Brain Networks", "comments": "International Conference on Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating similarity between graphs is of major importance in several\ncomputer vision and pattern recognition problems, where graph representations\nare often used to model objects or interactions between elements. The choice of\na distance or similarity metric is, however, not trivial and can be highly\ndependent on the application at hand. In this work, we propose a novel metric\nlearning method to evaluate distance between graphs that leverages the power of\nconvolutional neural networks, while exploiting concepts from spectral graph\ntheory to allow these operations on irregular graphs. We demonstrate the\npotential of our method in the field of connectomics, where neuronal pathways\nor functional connections between brain regions are commonly modelled as\ngraphs. In this problem, the definition of an appropriate graph similarity\nfunction is critical to unveil patterns of disruptions associated with certain\nbrain disorders. Experimental results on the ABIDE dataset show that our method\ncan learn a graph similarity metric tailored for a clinical application,\nimproving the performance of a simple k-nn classifier by 11.9% compared to a\ntraditional distance metric.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 00:49:27 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 11:05:52 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ktena", "Sofia Ira", ""], ["Parisot", "Sarah", ""], ["Ferrante", "Enzo", ""], ["Rajchl", "Martin", ""], ["Lee", "Matthew", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1703.02168", "submitter": "Junghyun Kwon", "authors": "Dinghuang Ji, Junghyun Kwon, Max McFarland, Silvio Savarese", "title": "Deep View Morphing", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNN) have been successfully applied\nto view synthesis problems. However, such CNN-based methods can suffer from\nlack of texture details, shape distortions, or high computational complexity.\nIn this paper, we propose a novel CNN architecture for view synthesis called\n\"Deep View Morphing\" that does not suffer from these issues. To synthesize a\nmiddle view of two input images, a rectification network first rectifies the\ntwo input images. An encoder-decoder network then generates dense\ncorrespondences between the rectified images and blending masks to predict the\nvisibility of pixels of the rectified images in the middle view. A view\nmorphing network finally synthesizes the middle view using the dense\ncorrespondences and blending masks. We experimentally show the proposed method\nsignificantly outperforms the state-of-the-art CNN-based view synthesis method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 01:21:01 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Ji", "Dinghuang", ""], ["Kwon", "Junghyun", ""], ["McFarland", "Max", ""], ["Savarese", "Silvio", ""]]}, {"id": "1703.02180", "submitter": "Yunpeng Chen", "authors": "Chen Yunpeng, Jin Xiaojie, Kang Bingyi, Feng Jiashi, Yan Shuicheng", "title": "Sharing Residual Units Through Collective Tensor Factorization in Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual units are wildly used for alleviating optimization difficulties when\nbuilding deep neural networks. However, the performance gain does not well\ncompensate the model size increase, indicating low parameter efficiency in\nthese residual units. In this work, we first revisit the residual function in\nseveral variations of residual units and demonstrate that these residual\nfunctions can actually be explained with a unified framework based on\ngeneralized block term decomposition. Then, based on the new explanation, we\npropose a new architecture, Collective Residual Unit (CRU), which enhances the\nparameter efficiency of deep neural networks through collective tensor\nfactorization. CRU enables knowledge sharing across different residual units\nusing shared factors. Experimental results show that our proposed CRU Network\ndemonstrates outstanding parameter efficiency, achieving comparable\nclassification performance to ResNet-200 with the model size of ResNet-50. By\nbuilding a deeper network using CRU, we can achieve state-of-the-art single\nmodel classification accuracy on ImageNet-1k and Places365-Standard benchmark\ndatasets. (Code and trained models are available on GitHub)\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:20:57 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 15:00:26 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Yunpeng", "Chen", ""], ["Xiaojie", "Jin", ""], ["Bingyi", "Kang", ""], ["Jiashi", "Feng", ""], ["Shuicheng", "Yan", ""]]}, {"id": "1703.02182", "submitter": "Wenhao Zhang", "authors": "Wenhao Zhang, Liangcai Gao, Runtao Liu", "title": "Using Deep Learning Method for Classification: A Proposed Algorithm for\n  the ISIC 2017 Skin Lesion Classification Challenge", "comments": "Skin Lesion Classification Challenge Competition, ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer, the most common human malignancy, is primarily diagnosed\nvisually by physicians [1]. Classification with an automated method like CNN\n[2, 3] shows potential for challenging tasks [1]. By now, the deep\nconvolutional neural networks are on par with human dermatologist [1]. This\nabstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\nSkin Lesion Detection Competition hosted at [6] to classify the dermatology\npictures, which is aimed at improving the diagnostic accuracy rate and general\nlevel of the human health. The challenge falls into three sub-challenges,\nincluding Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\nClassification. This project only participates in the Lesion Classification\npart. This algorithm is comprised of three steps: (1) original images\npreprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\nframework, (3) predicting the test images and calculating the scores that\nrepresent the likelihood of corresponding classification. The models are built\non the source images are using the Caffe [4] framework. The scores in\nprediction step are obtained by two different models from the source images.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:26:21 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 08:17:47 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Zhang", "Wenhao", ""], ["Gao", "Liangcai", ""], ["Liu", "Runtao", ""]]}, {"id": "1703.02184", "submitter": "Xiansheng Guo", "authors": "Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah", "title": "Indoor Localization Using Visible Light Via Fusion Of Multiple\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multiple classifiers fusion localization technique using received signal\nstrengths (RSSs) of visible light is proposed, in which the proposed system\ntransmits different intensity modulated sinusoidal signals by LEDs and the\nsignals received by a Photo Diode (PD) placed at various grid points. First, we\nobtain some {\\emph{approximate}} received signal strengths (RSSs) fingerprints\nby capturing the peaks of power spectral density (PSD) of the received signals\nat each given grid point. Unlike the existing RSSs based algorithms, several\nrepresentative machine learning approaches are adopted to train multiple\nclassifiers based on these RSSs fingerprints. The multiple classifiers\nlocalization estimators outperform the classical RSS-based LED localization\napproaches in accuracy and robustness. To further improve the localization\nperformance, two robust fusion localization algorithms, namely, grid\nindependent least square (GI-LS) and grid dependent least square (GD-LS), are\nproposed to combine the outputs of these classifiers. We also use a singular\nvalue decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical\nstability problem when the prediction matrix is singular. Experiments conducted\non intensity modulated direct detection (IM/DD) systems have demonstrated the\neffectiveness of the proposed algorithms. The experimental results show that\nthe probability of having mean square positioning error (MSPE) of less than 5cm\nachieved by GD-LS is improved by 93.03\\% and 93.15\\%, respectively, as compared\nto those by the RSS ratio (RSSR) and RSS matching methods with the FFT length\nof 2000.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:41:07 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 06:44:32 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Guo", "Xiansheng", ""], ["Shao", "Sihua", ""], ["Ansari", "Nirwan", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1703.02185", "submitter": "Xiansheng Guo", "authors": "Xiansheng Guo, Nirwan Ansari, Huiyong Li", "title": "Indoor Localization by Fusing a Group of Fingerprints Based on Random\n  Forests", "comments": "arXiv admin note: text overlap with arXiv:1609.00661", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indoor localization based on SIngle Of Fingerprint (SIOF) is rather\nsusceptible to the changing environment, multipath, and non-line-of-sight\n(NLOS) propagation. Building SIOF is also a very time-consuming process.\nRecently, we first proposed a GrOup Of Fingerprints (GOOF) to improve the\nlocalization accuracy and reduce the burden of building fingerprints. However,\nthe main drawback is the timeliness. In this paper, we propose a novel\nlocalization framework by Fusing A Group Of fingerprinTs (FAGOT) based on\nrandom forests. In the offline phase, we first build a GOOF from different\ntransformations of the received signals of multiple antennas. Then, we design\nmultiple GOOF strong classifiers based on Random Forests (GOOF-RF) by training\neach fingerprint in the GOOF. In the online phase, we input the corresponding\ntransformations of the real measurements into these strong classifiers to\nobtain multiple independent decisions. Finally, we propose a Sliding Window\naIded Mode-based (SWIM) fusion algorithm to balance the localization accuracy\nand time. Our proposed approaches can work better in an unknown indoor\nscenario. The burden of building fingerprints can also be reduced drastically.\nWe demonstrate the performance of our algorithms through simulations and real\nexperimental data using two Universal Software Radio Peripheral (USRP)\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:41:40 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Guo", "Xiansheng", ""], ["Ansari", "Nirwan", ""], ["Li", "Huiyong", ""]]}, {"id": "1703.02217", "submitter": "Arabinda Dash", "authors": "Sujaya Kumar Sathua, Arabinda Dash, Aishwaryarani Behera", "title": "Removal of Salt and Pepper noise from Gray-Scale and Color Images: An\n  Adaptive Approach", "comments": "10 pages, 16 figures", "journal-ref": "International Journal of Computer Science Trends and Technology\n  (IJCST) V5(1): Page(117-126) Jan-Feb 2017. ISSN: 2347-8578.\n  www.ijcstjournal.org.Published by Eighth Sense Research Group", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient adaptive algorithm for the removal of Salt and Pepper noise from\ngray scale and color image is presented in this paper. In this proposed method\nfirst a 3X3 window is taken and the central pixel of the window is considered\nas the processing pixel. If the processing pixel is found as uncorrupted, then\nit is left unchanged. And if the processing pixel is found corrupted one, then\nthe window size is increased according to the conditions given in the proposed\nalgorithm. Finally the processing pixel or the central pixel is replaced by\neither the mean, median or trimmed value of the elements in the current window\ndepending upon different conditions of the algorithm. The proposed algorithm\nefficiently removes noise at all densities with better Peak Signal to Noise\nRatio (PSNR) and Image Enhancement Factor (IEF). The proposed algorithm is\ncompared with different existing algorithms like MF, AMF, MDBUTMF, MDBPTGMF and\nAWMF.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 05:24:26 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Sathua", "Sujaya Kumar", ""], ["Dash", "Arabinda", ""], ["Behera", "Aishwaryarani", ""]]}, {"id": "1703.02242", "submitter": "Hua Li", "authors": "Erbo Li, Yazhou Huang, Dong Xu and Hua Li", "title": "Shape DNA: Basic Generating Functions for Geometric Moment Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric moment invariants (GMIs) have been widely used as basic tool in\nshape analysis and information retrieval. Their structure and characteristics\ndetermine efficiency and effectiveness. Two fundamental building blocks or\ngenerating functions (GFs) for invariants are discovered, which are dot product\nand vector product of point vectors in Euclidean space. The primitive\ninvariants (PIs) can be derived by carefully selecting different products of\nGFs and calculating the corresponding multiple integrals, which translates\npolynomials of coordinates of point vectors into geometric moments. Then the\ninvariants themselves are expressed in the form of product of moments. This\nprocedure is just like DNA encoding proteins. All GMIs available in the\nliterature can be decomposed into linear combinations of PIs. This paper shows\nthat Hu's seven well known GMIs in computer vision have a more deep structure,\nwhich can be further divided into combination of simpler PIs. In practical\nuses, low order independent GMIs are of particular interest. In this paper, a\nset of PIs for similarity transformation and affine transformation in 2D are\npresented, which are simpler to use, and some of which are newly reported. The\ndiscovery of the two generating functions provides a new perspective of better\nunderstanding shapes in 2D and 3D Euclidean spaces, and the method proposed can\nbe further extended to higher dimensional spaces and different manifolds, such\nas curves, surfaces and so on.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:55:31 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 06:20:48 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 19:14:45 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Li", "Erbo", ""], ["Huang", "Yazhou", ""], ["Xu", "Dong", ""], ["Li", "Hua", ""]]}, {"id": "1703.02243", "submitter": "Wei Ke", "authors": "Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye", "title": "SRN: Side-output Residual Network for Object Symmetry Detection in the\n  Wild", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a baseline for object symmetry detection in\ncomplex backgrounds by presenting a new benchmark and an end-to-end deep\nlearning approach, opening up a promising direction for symmetry detection in\nthe wild. The new benchmark, named Sym-PASCAL, spans challenges including\nobject diversity, multi-objects, part-invisibility, and various complex\nbackgrounds that are far beyond those in existing datasets. The proposed\nsymmetry detection approach, named Side-output Residual Network (SRN),\nleverages output Residual Units (RUs) to fit the errors between the object\nsymmetry groundtruth and the outputs of RUs. By stacking RUs in a\ndeep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales\nto ease the problems of fitting complex outputs with limited layers,\nsuppressing the complex backgrounds, and effectively matching object symmetry\nof different scales. Experimental results validate both the benchmark and its\nchallenging aspects related to realworld images, and the state-of-the-art\nperformance of our symmetry detection approach. The benchmark and the code for\nSRN are publicly available at https://github.com/KevinKecc/SRN.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 07:09:40 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 01:58:50 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ke", "Wei", ""], ["Chen", "Jie", ""], ["Jiao", "Jianbin", ""], ["Zhao", "Guoying", ""], ["Ye", "Qixiang", ""]]}, {"id": "1703.02271", "submitter": "Zhixian Ma", "authors": "Zhixian Ma, Weitian Li, Lei Wang, Haiguang Xu, Jie Zhu", "title": "X-ray Astronomical Point Sources Recognition Using Granular Binary-tree\n  SVM", "comments": "Accepted by ICSP2016", "journal-ref": null, "doi": "10.1109/ICSP.2016.7877984", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study on point sources in astronomical images is of special importance,\nsince most energetic celestial objects in the Universe exhibit a point-like\nappearance. An approach to recognize the point sources (PS) in the X-ray\nastronomical images using our newly designed granular binary-tree support\nvector machine (GBT-SVM) classifier is proposed. First, all potential point\nsources are located by peak detection on the image. The image and spectral\nfeatures of these potential point sources are then extracted. Finally, a\nclassifier to recognize the true point sources is build through the extracted\nfeatures. Experiments and applications of our approach on real X-ray\nastronomical images are demonstrated. comparisons between our approach and\nother SVM-based classifiers are also carried out by evaluating the precision\nand recall rates, which prove that our approach is better and achieves a higher\naccuracy of around 89%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 08:31:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ma", "Zhixian", ""], ["Li", "Weitian", ""], ["Wang", "Lei", ""], ["Xu", "Haiguang", ""], ["Zhu", "Jie", ""]]}, {"id": "1703.02291", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Kun Xu and Jun Zhu and Bo Zhang", "title": "Triple Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) have shown promise in image generation and\nsemi-supervised learning (SSL). However, existing GANs in SSL have two\nproblems: (1) the generator and the discriminator (i.e. the classifier) may not\nbe optimal at the same time; and (2) the generator cannot control the semantics\nof the generated samples. The problems essentially arise from the two-player\nformulation, where a single discriminator shares incompatible roles of\nidentifying fake samples and predicting labels and it only estimates the data\nwithout considering the labels. To address the problems, we present triple\ngenerative adversarial net (Triple-GAN), which consists of three players---a\ngenerator, a discriminator and a classifier. The generator and the classifier\ncharacterize the conditional distributions between images and labels, and the\ndiscriminator solely focuses on identifying fake image-label pairs. We design\ncompatible utilities to ensure that the distributions characterized by the\nclassifier and the generator both converge to the data distribution. Our\nresults on various datasets demonstrate that Triple-GAN as a unified model can\nsimultaneously (1) achieve the state-of-the-art classification results among\ndeep generative models, and (2) disentangle the classes and styles of the input\nand transfer smoothly in the data space via interpolation in the latent space\nclass-conditionally.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 09:26:56 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 09:12:51 GMT"}, {"version": "v3", "created": "Fri, 2 Jun 2017 08:21:45 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 17:25:11 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Li", "Chongxuan", ""], ["Xu", "Kun", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1703.02344", "submitter": "Krishnendu Chaudhury", "authors": "Devashish Shankar, Sujay Narumanchi, H A Ananya, Pramod Kompalli,\n  Krishnendu Chaudhury", "title": "Deep Learning based Large Scale Visual Recommendation and Search for\n  E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a unified end-to-end approach to build a large\nscale Visual Search and Recommendation system for e-commerce. Previous works\nhave targeted these problems in isolation. We believe a more effective and\nelegant solution could be obtained by tackling them together. We propose a\nunified Deep Convolutional Neural Network architecture, called VisNet, to learn\nembeddings to capture the notion of visual similarity, across several semantic\ngranularities. We demonstrate the superiority of our approach for the task of\nimage retrieval, by comparing against the state-of-the-art on the Exact\nStreet2Shop dataset. We then share the design decisions and trade-offs made\nwhile deploying the model to power Visual Recommendations across a catalog of\n50M products, supporting 2K queries a second at Flipkart, India's largest\ne-commerce company. The deployment of our solution has yielded a significant\nbusiness impact, as measured by the conversion-rate.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 11:58:36 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Shankar", "Devashish", ""], ["Narumanchi", "Sujay", ""], ["Ananya", "H A", ""], ["Kompalli", "Pramod", ""], ["Chaudhury", "Krishnendu", ""]]}, {"id": "1703.02363", "submitter": "Andre Ebert", "authors": "Andre Ebert, Michael Till Beck, Andy Mattausch, Lenz Belzner, Claudia\n  Linnhoff Popien", "title": "Qualitative Assessment of Recurrent Human Motion", "comments": "Published within the proceedings of the 25th European Signal\n  Processing Conference (EUSIPCO) 2017, Kos Island, Greece, IEEE 6 Pages, 5\n  Figures", "journal-ref": null, "doi": "10.23919/EUSIPCO.2017.8081218", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone applications designed to track human motion in combination with\nwearable sensors, e.g., during physical exercising, raised huge attention\nrecently. Commonly, they provide quantitative services, such as personalized\ntraining instructions or the counting of distances. But qualitative monitoring\nand assessment is still missing, e.g., to detect malpositions, to prevent\ninjuries, or to optimize training success. We address this issue by presenting\na concept for qualitative as well as generic assessment of recurrent human\nmotion by processing multi-dimensional, continuous time series tracked with\nmotion sensors. Therefore, our segmentation procedure extracts individual\nevents of specific length and we propose expressive features to accomplish a\nqualitative motion assessment by supervised classification. We verified our\napproach within a comprehensive study encompassing 27 athletes undertaking\ndifferent body weight exercises. We are able to recognize six different\nexercise types with a success rate of 100% and to assess them qualitatively\nwith an average success rate of 99.3%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 12:57:01 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 14:17:14 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ebert", "Andre", ""], ["Beck", "Michael Till", ""], ["Mattausch", "Andy", ""], ["Belzner", "Lenz", ""], ["Popien", "Claudia Linnhoff", ""]]}, {"id": "1703.02391", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo,\n  Li-Jia Li", "title": "Learning from Noisy Labels with Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of learning from noisy labels is very useful in many visual\nrecognition tasks, as a vast amount of data with noisy labels are relatively\neasy to obtain. Traditionally, the label noises have been treated as\nstatistical outliers, and approaches such as importance re-weighting and\nbootstrap have been proposed to alleviate the problem. According to our\nobservation, the real-world noisy labels exhibit multi-mode characteristics as\nthe true labels, rather than behaving like independent random outliers. In this\nwork, we propose a unified distillation framework to use side information,\nincluding a small clean dataset and label relations in knowledge graph, to\n\"hedge the risk\" of learning from noisy labels. Furthermore, unlike the\ntraditional approaches evaluated based on simulated label noises, we propose a\nsuite of new benchmark datasets, in Sports, Species and Artifacts domains, to\nevaluate the task of learning from noisy labels in the practical setting. The\nempirical study demonstrates the effectiveness of our proposed method in all\nthe domains.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 14:15:14 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 07:21:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Li", "Yuncheng", ""], ["Yang", "Jianchao", ""], ["Song", "Yale", ""], ["Cao", "Liangliang", ""], ["Luo", "Jiebo", ""], ["Li", "Li-Jia", ""]]}, {"id": "1703.02437", "submitter": "Santiago Manen", "authors": "Santiago Manen, Michael Gygli, Dengxin Dai, Luc Van Gool", "title": "PathTrack: Fast Trajectory Annotation with Path Supervision", "comments": "10 pages, ICCV submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in Multiple Object Tracking (MOT) has been historically limited by\nthe size of the available datasets. We present an efficient framework to\nannotate trajectories and use it to produce a MOT dataset of unprecedented\nsize. In our novel path supervision the annotator loosely follows the object\nwith the cursor while watching the video, providing a path annotation for each\nobject in the sequence. Our approach is able to turn such weak annotations into\ndense box trajectories. Our experiments on existing datasets prove that our\nframework produces more accurate annotations than the state of the art, in a\nfraction of the time. We further validate our approach by crowdsourcing the\nPathTrack dataset, with more than 15,000 person trajectories in 720 sequences.\nTracking approaches can benefit training on such large-scale datasets, as did\nobject recognition. We prove this by re-training an off-the-shelf person\nmatching network, originally trained on the MOT15 dataset, almost halving the\nmisclassification rate. Additionally, training on our data consistently\nimproves tracking results, both on our dataset and on MOT15. On the latter, we\nimprove the top-performing tracker (NOMT) dropping the number of IDSwitches by\n18% and fragments by 5%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:36:39 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 07:08:34 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Manen", "Santiago", ""], ["Gygli", "Michael", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1703.02442", "submitter": "Yun Liu", "authors": "Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo\n  Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev, Philip\n  Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng, Martin C. Stumpe", "title": "Detecting Cancer Metastases on Gigapixel Pathology Images", "comments": "Fig 1: normal and tumor patches were accidentally reversed - now\n  fixed. Minor grammatical corrections in appendix, section \"Image Color\n  Normalization\"", "journal-ref": "MICCAI Tutorial (2017)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, the treatment decisions for more than 230,000 breast cancer\npatients in the U.S. hinge on whether the cancer has metastasized away from the\nbreast. Metastasis detection is currently performed by pathologists reviewing\nlarge expanses of biological tissues. This process is labor intensive and\nerror-prone. We present a framework to automatically detect and localize tumors\nas small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x\n100,000 pixels. Our method leverages a convolutional neural network (CNN)\narchitecture and obtains state-of-the-art results on the Camelyon16 dataset in\nthe challenging lesion-level tumor detection task. At 8 false positives per\nimage, we detect 92.4% of the tumors, relative to 82.7% by the previous best\nautomated approach. For comparison, a human pathologist attempting exhaustive\nsearch achieved 73.2% sensitivity. We achieve image-level AUC scores above 97%\non both the Camelyon16 test set and an independent set of 110 slides. In\naddition, we discover that two slides in the Camelyon16 training set were\nerroneously labeled normal. Our approach could considerably reduce false\nnegative rates in metastasis detection.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 19:03:08 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 04:47:57 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Liu", "Yun", ""], ["Gadepalli", "Krishna", ""], ["Norouzi", "Mohammad", ""], ["Dahl", "George E.", ""], ["Kohlberger", "Timo", ""], ["Boyko", "Aleksey", ""], ["Venugopalan", "Subhashini", ""], ["Timofeev", "Aleksei", ""], ["Nelson", "Philip Q.", ""], ["Corrado", "Greg S.", ""], ["Hipp", "Jason D.", ""], ["Peng", "Lily", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1703.02445", "submitter": "Bernhard Bermeitinger", "authors": "Bernhard Bermeitinger, Andr\\'e Freitas, Simon Donig, Siegfried\n  Handschuh", "title": "Object classification in images of Neoclassical furniture using Deep\n  Learning", "comments": null, "journal-ref": "Computational History and Data-Driven Humanities. CHDDH 2016. IFIP\n  Advances in Information and Communication Technology, vol 482. Springer, Cham", "doi": "10.1007/978-3-319-46224-0_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper outlines research results on object classification in images\nof Neoclassical furniture. The motivation was to provide an object recognition\nframework which is able to support the alignment of furniture images with a\nsymbolic level model. A data-driven bottom-up research routine in the\nNeoclassica research framework is the main use-case. It strives to deliver\ntools for analyzing the spread of aesthetic forms which are considered as a\ncultural transfer process.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:48:55 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Bermeitinger", "Bernhard", ""], ["Freitas", "Andr\u00e9", ""], ["Donig", "Simon", ""], ["Handschuh", "Siegfried", ""]]}, {"id": "1703.02511", "submitter": "Sajib Saha", "authors": "Sajib Kumar Saha, Basura Fernando, Jorge Cuadros, Di Xiao, Yogesan\n  Kanagasingam", "title": "Deep Learning for Automated Quality Assessment of Color Fundus Images in\n  Diabetic Retinopathy Screening", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose To develop a computer based method for the automated assessment of\nimage quality in the context of diabetic retinopathy (DR) to guide the\nphotographer.\n  Methods A deep learning framework was trained to grade the images\nautomatically. A large representative set of 7000 color fundus images were used\nfor the experiment which were obtained from the EyePACS that were made\navailable by the California Healthcare Foundation. Three retinal image analysis\nexperts were employed to categorize these images into Accept and Reject classes\nbased on the precise definition of image quality in the context of DR. A deep\nlearning framework was trained using 3428 images.\n  Results A total of 3572 images were used for the evaluation of the proposed\nmethod. The method shows an accuracy of 100% to successfully categorise Accept\nand Reject images.\n  Conclusion Image quality is an essential prerequisite for the grading of DR.\nIn this paper we have proposed a deep learning based automated image quality\nassessment method in the context of DR. The method can be easily incorporated\nwith the fundus image capturing system and thus can guide the photographer\nwhether a recapture is necessary or not.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:28:50 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Saha", "Sajib Kumar", ""], ["Fernando", "Basura", ""], ["Cuadros", "Jorge", ""], ["Xiao", "Di", ""], ["Kanagasingam", "Yogesan", ""]]}, {"id": "1703.02518", "submitter": "Martin Jaggi", "authors": "Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi", "title": "Faster Coordinate Descent via Adaptive Importance Sampling", "comments": "appearing at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coordinate descent methods employ random partial updates of decision\nvariables in order to solve huge-scale convex optimization problems. In this\nwork, we introduce new adaptive rules for the random selection of their\nupdates. By adaptive, we mean that our selection rules are based on the dual\nresidual or the primal-dual gap estimates and can change at each iteration. We\ntheoretically characterize the performance of our selection rules and\ndemonstrate improvements over the state-of-the-art, and extend our theory and\nalgorithms to general convex objectives. Numerical evidence with hinge-loss\nsupport vector machines and Lasso confirm that the practice follows the theory.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:36:55 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Perekrestenko", "Dmytro", ""], ["Cevher", "Volkan", ""], ["Jaggi", "Martin", ""]]}, {"id": "1703.02521", "submitter": "De-An Huang", "authors": "De-An Huang, Joseph J. Lim, Li Fei-Fei, Juan Carlos Niebles", "title": "Unsupervised Visual-Linguistic Reference Resolution in Instructional\n  Videos", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised method for reference resolution in instructional\nvideos, where the goal is to temporally link an entity (e.g., \"dressing\") to\nthe action (e.g., \"mix yogurt\") that produced it. The key challenge is the\ninevitable visual-linguistic ambiguities arising from the changes in both\nvisual appearance and referring expression of an entity in the video. This\nchallenge is amplified by the fact that we aim to resolve references with no\nsupervision. We address these challenges by learning a joint visual-linguistic\nmodel, where linguistic cues can help resolve visual ambiguities and vice\nversa. We verify our approach by learning our model unsupervisedly using more\nthan two thousand unstructured cooking videos from YouTube, and show that our\nvisual-linguistic model can substantially improve upon state-of-the-art\nlinguistic only model on reference resolution in instructional videos.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:40:27 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 22:33:54 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Huang", "De-An", ""], ["Lim", "Joseph J.", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1703.02529", "submitter": "Daniel Kang", "authors": "Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia", "title": "NoScope: Optimizing Neural Network Queries over Video at Scale", "comments": "PVLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision-in the form of deep neural networks-have\nmade it possible to query increasing volumes of video data with high accuracy.\nHowever, neural network inference is computationally expensive at scale:\napplying a state-of-the-art object detector in real time (i.e., 30+ frames per\nsecond) to a single video requires a $4000 GPU. In response, we present\nNoScope, a system for querying videos that can reduce the cost of neural\nnetwork video analysis by up to three orders of magnitude via\ninference-optimized model search. Given a target video, object to detect, and\nreference neural network, NoScope automatically searches for and trains a\nsequence, or cascade, of models that preserves the accuracy of the reference\nnetwork but is specialized to the target video and are therefore far less\ncomputationally expensive. NoScope cascades two types of models: specialized\nmodels that forego the full generality of the reference model but faithfully\nmimic its behavior for the target video and object; and difference detectors\nthat highlight temporal differences across frames. We show that the optimal\ncascade architecture differs across videos and objects, so NoScope uses an\nefficient cost-based optimizer to search across models and cascades. With this\napproach, NoScope achieves two to three order of magnitude speed-ups\n(265-15,500x real-time) on binary classification tasks over fixed-angle webcam\nand surveillance video while maintaining accuracy within 1-5% of\nstate-of-the-art neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:54:28 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 07:08:53 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 21:34:30 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kang", "Daniel", ""], ["Emmons", "John", ""], ["Abuzaid", "Firas", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1703.02563", "submitter": "Christian Bailer", "authors": "Christian Bailer, Bertram Taetz, Didier Stricker", "title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large\n  Displacement Optical Flow Estimation", "comments": "Extended TPAMI version of publication (conference version:\n  arXiv:1508.05151)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern large displacement optical flow algorithms usually use an\ninitialization by either sparse descriptor matching techniques or dense\napproximate nearest neighbor fields. While the latter have the advantage of\nbeing dense, they have the major disadvantage of being very outlier-prone as\nthey are not designed to find the optical flow, but the visually most similar\ncorrespondence. In this article we present a dense correspondence field\napproach that is much less outlier-prone and thus much better suited for\noptical flow estimation than approximate nearest neighbor fields. Our approach\ndoes not require explicit regularization, smoothing (like median filtering) or\na new data term. Instead we solely rely on patch matching techniques and a\nnovel multi-scale matching strategy. We also present enhancements for outlier\nfiltering. We show that our approach is better suited for large displacement\noptical flow estimation than modern descriptor matching techniques. We do so by\ninitializing EpicFlow with our approach instead of their originally used\nstate-of-the-art descriptor matching technique. We significantly outperform the\noriginal EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this\nextended article of our former conference publication we further improve our\napproach in matching accuracy as well as runtime and present more experiments\nand insights.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 19:28:45 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 08:00:11 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Bailer", "Christian", ""], ["Taetz", "Bertram", ""], ["Stricker", "Didier", ""]]}, {"id": "1703.02589", "submitter": "G M Mashrur E Elahi", "authors": "G M Mashrur E Elahi, Sanjay Kalra, Yee-Hong Yang", "title": "Texture Classification of MR Images of the Brain in ALS using CoHOG", "comments": "We found an error in the feature selection part (Sec. 3.3) of the\n  proposed approach. In the feature selection part, by mistake, we have used\n  both the training and testing data for feature selection. We are working on\n  it. We will update the proposed approach as early as possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture analysis is a well-known research topic in computer vision and image\nprocessing and has many applications. Gradient-based texture methods have\nbecome popular in classification problems. For the first time we extend a\nwell-known gradient-based method, Co-occurrence Histograms of Oriented\nGradients (CoHOG) to extract texture features from 2D Magnetic Resonance Images\n(MRI). Unlike the original CoHOG method, we use the whole image instead of\nsub-regions for feature calculation. Also, we use a larger neighborhood size.\nGradient orientations of the image pixels are calculated using Sobel, Gaussian\nDerivative (GD) and Local Frequency Descriptor Gradient (LFDG) operators. The\nextracted feature vector size is very large and classification using a large\nnumber of similar features does not provide the best results. In our proposed\nmethod, for the first time to our best knowledge, only a minimum number of\nsignificant features are selected using area under the receiver operator\ncharacteristic (ROC) curve (AUC) thresholds with <= 0.01. In this paper, we\napply the proposed method to classify Amyotrophic Lateral Sclerosis (ALS)\npatients from the controls. It is observed that selected texture features from\ndownsampled images are significantly different between patients and controls.\nThese features are used in a linear support vector machine (SVM) classifier to\ndetermine the classification accuracy. Optimal sensitivity and specificity are\nalso calculated. Three different cohort datasets are used in the experiments.\nThe performance of the proposed method using three gradient operators and two\ndifferent neighborhood sizes is analyzed. Region based analysis is performed to\ndemonstrate that significant changes between patients and controls are limited\nto the motor cortex.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 20:50:14 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 16:50:45 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Elahi", "G M Mashrur E", ""], ["Kalra", "Sanjay", ""], ["Yang", "Yee-Hong", ""]]}, {"id": "1703.02611", "submitter": "Aliakbar Jafarpour", "authors": "Aliakbar Jafarpour and Holger Lorenz", "title": "Cellulyzer - Automated analysis and interactive visualization/simulation\n  of select cellular processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV physics.data-an q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we report on a set of programs developed at the ZMBH Bio-Imaging\nFacility for tracking real-life images of cellular processes. These programs\nperform 1) automated tracking; 2) quantitative and comparative track analyses\nof different images in different groups; 3) different interactive visualization\nschemes; and 4) interactive realistic simulation of different cellular\nprocesses for validation and optimal problem-specific adjustment of image\nacquisition parameters (tradeoff between speed, resolution, and quality with\nfeedback from the very final results). The collection of programs is primarily\ndeveloped for the common bio-image analysis software ImageJ (as a single Java\nPlugin). Some programs are also available in other languages (C++ and\nJavascript) and may be run simply with a web-browser; even on a low-end Tablet\nor Smartphone. The programs are available at\nhttps://github.com/nurlicht/CellulyzerDemo\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:29:09 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Jafarpour", "Aliakbar", ""], ["Lorenz", "Holger", ""]]}, {"id": "1703.02635", "submitter": "Quercus Hern\\'andez La\\'in", "authors": "Quercus Hernandez, Diego Gutierrez, Adrian Jarabo", "title": "A Computational Model of a Single-Photon Avalanche Diode Sensor for\n  Transient Imaging", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable\nto collect extremely fast low-energy events, due to their single-photon\nsensibility. This makes them very suitable for time-of-flight-based range\nimaging systems, allowing to reduce costs and power requirements, without\nsacrifizing much temporal resolution. In this work we describe a computational\nmodel to simulate the behaviour of SPAD sensors, aiming to provide a realistic\ncamera model for time-resolved light transport simulation, with applications on\nprototyping new reconstructions techniques based on SPAD time-of-flight data.\nOur model accounts for the major effects of the sensor on the incoming signal.\nWe compare our model against real-world measurements, and apply it to a variety\nof scenarios, including complex multiply-scattered light transport.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 10:17:34 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Hernandez", "Quercus", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1703.02710", "submitter": "Zequn Jie", "authors": "Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Feng Lu and\n  Shuicheng Yan", "title": "Tree-Structured Reinforcement Learning for Sequential Object\n  Localization", "comments": "Advances in Neural Information Processing Systems 2016", "journal-ref": "In Advances in Neural Information Processing Systems (pp. 127-135)\n  (2016)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing object proposal algorithms usually search for possible object\nregions over multiple locations and scales separately, which ignore the\ninterdependency among different objects and deviate from the human perception\nprocedure. To incorporate global interdependency between objects into object\nlocalization, we propose an effective Tree-structured Reinforcement Learning\n(Tree-RL) approach to sequentially search for objects by fully exploiting both\nthe current observation and historical search paths. The Tree-RL approach\nlearns multiple searching policies through maximizing the long-term reward that\nreflects localization accuracies over all the objects. Starting with taking the\nentire image as a proposal, the Tree-RL approach allows the agent to\nsequentially discover multiple objects via a tree-structured traversing scheme.\nAllowing multiple near-optimal policies, Tree-RL offers more diversity in\nsearch paths and is able to find multiple objects with a single feed-forward\npass. Therefore, Tree-RL can better cover different objects with various scales\nwhich is quite appealing in the context of object proposal. Experiments on\nPASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can\nachieve comparable recalls with current object proposal algorithms via much\nfewer candidate windows.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 05:24:52 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Jie", "Zequn", ""], ["Liang", "Xiaodan", ""], ["Feng", "Jiashi", ""], ["Jin", "Xiaojie", ""], ["Lu", "Wen Feng", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1703.02716", "submitter": "Yuanjun Xiong", "authors": "Yuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, Xiaoou Tang", "title": "A Pursuit of Temporal Accuracy in General Activity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting activities in untrimmed videos is an important but challenging\ntask. The performance of existing methods remains unsatisfactory, e.g., they\noften meet difficulties in locating the beginning and end of a long complex\naction. In this paper, we propose a generic framework that can accurately\ndetect a wide variety of activities from untrimmed videos. Our first\ncontribution is a novel proposal scheme that can efficiently generate\ncandidates with accurate temporal boundaries. The other contribution is a\ncascaded classification pipeline that explicitly distinguishes between\nrelevance and completeness of a candidate instance. On two challenging temporal\nactivity detection datasets, THUMOS14 and ActivityNet, the proposed framework\nsignificantly outperforms the existing state-of-the-art methods, demonstrating\nsuperior accuracy and strong adaptivity in handling activities with various\ntemporal structures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 05:52:52 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Xiong", "Yuanjun", ""], ["Zhao", "Yue", ""], ["Wang", "Limin", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1703.02719", "submitter": "Chao Peng", "authors": "Chao Peng and Xiangyu Zhang and Gang Yu and Guiming Luo and Jian Sun", "title": "Large Kernel Matters -- Improve Semantic Segmentation by Global\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of recent trends [30, 31, 14] in network architec- ture design is\nstacking small filters (e.g., 1x1 or 3x3) in the entire network because the\nstacked small filters is more ef- ficient than a large kernel, given the same\ncomputational complexity. However, in the field of semantic segmenta- tion,\nwhere we need to perform dense per-pixel prediction, we find that the large\nkernel (and effective receptive field) plays an important role when we have to\nperform the clas- sification and localization tasks simultaneously. Following\nour design principle, we propose a Global Convolutional Network to address both\nthe classification and localization issues for the semantic segmentation. We\nalso suggest a residual-based boundary refinement to further refine the ob-\nject boundaries. Our approach achieves state-of-art perfor- mance on two public\nbenchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%)\non PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:14:55 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Peng", "Chao", ""], ["Zhang", "Xiangyu", ""], ["Yu", "Gang", ""], ["Luo", "Guiming", ""], ["Sun", "Jian", ""]]}, {"id": "1703.02826", "submitter": "Kosuke Takahashi", "authors": "Kosuke Takahashi and Akihiro Miyata and Shohei Nobuhara and Takashi\n  Matsuyama", "title": "A Linear Extrinsic Calibration of Kaleidoscopic Imaging System from\n  Single 3D Point", "comments": "to appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new extrinsic calibration of kaleidoscopic imaging\nsystem by estimating normals and distances of the mirrors. The problem to be\nsolved in this paper is a simultaneous estimation of all mirror parameters\nconsistent throughout multiple reflections. Unlike conventional methods\nutilizing a pair of direct and mirrored images of a reference 3D object to\nestimate the parameters on a per-mirror basis, our method renders the\nsimultaneous estimation problem into solving a linear set of equations. The key\ncontribution of this paper is to introduce a linear estimation of multiple\nmirror parameters from kaleidoscopic 2D projections of a single 3D point of\nunknown geometry. Evaluations with synthesized and real images demonstrate the\nperformance of the proposed algorithm in comparison with conventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 13:17:13 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 11:43:49 GMT"}, {"version": "v3", "created": "Sat, 27 May 2017 13:45:09 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Takahashi", "Kosuke", ""], ["Miyata", "Akihiro", ""], ["Nobuhara", "Shohei", ""], ["Matsuyama", "Takashi", ""]]}, {"id": "1703.02898", "submitter": "Biswa Sengupta", "authors": "B Sengupta, E Vazquez, M Sasdelli, Y Qian, M Peniak, L Netherton and G\n  Delfino", "title": "Large-scale image analysis using docker sandboxing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of specialized hardware such as Graphics Processing Units\n(GPUs), large scale image localization, classification and retrieval have seen\nincreased prevalence. Designing scalable software architecture that co-evolves\nwith such specialized hardware is a challenge in the commercial setting. In\nthis paper, we describe one such architecture (\\textit{Cortexica}) that\nleverages scalability of GPUs and sandboxing offered by docker containers. This\nallows for the flexibility of mixing different computer architectures as well\nas computational algorithms with the security of a trusted environment. We\nillustrate the utility of this framework in a commercial setting i.e.,\nsearching for multiple products in an image by combining image localisation and\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 09:40:48 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Sengupta", "B", ""], ["Vazquez", "E", ""], ["Sasdelli", "M", ""], ["Qian", "Y", ""], ["Peniak", "M", ""], ["Netherton", "L", ""], ["Delfino", "G", ""]]}, {"id": "1703.02910", "submitter": "Yarin Gal", "authors": "Yarin Gal and Riashat Islam and Zoubin Ghahramani", "title": "Deep Bayesian Active Learning with Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 16:53:57 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Gal", "Yarin", ""], ["Islam", "Riashat", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1703.02921", "submitter": "Eunbyung Park", "authors": "Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C.\n  Berg", "title": "Transformation-Grounded Image Generation Network for Novel 3D View\n  Synthesis", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transformation-grounded image generation network for novel 3D\nview synthesis from a single image. Instead of taking a 'blank slate' approach,\nwe first explicitly infer the parts of the geometry visible both in the input\nand novel views and then re-cast the remaining synthesis problem as image\ncompletion. Specifically, we both predict a flow to move the pixels from the\ninput to the novel view along with a novel visibility map that helps deal with\nocculsion/disocculsion. Next, conditioned on those intermediate results, we\nhallucinate (infer) parts of the object invisible in the input image. In\naddition to the new network structure, training with a combination of\nadversarial and perceptual loss results in a reduction in common artifacts of\nnovel view synthesis such as distortions and holes, while successfully\ngenerating high frequency details and preserving visual aspects of the input\nimage. We evaluate our approach on a wide range of synthetic and real examples.\nBoth qualitative and quantitative results show our method achieves\nsignificantly better results compared to existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:16:15 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Park", "Eunbyung", ""], ["Yang", "Jimei", ""], ["Yumer", "Ersin", ""], ["Ceylan", "Duygu", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1703.02931", "submitter": "Guido Borghi", "authors": "Guido Borghi, Roberto Vezzani, Rita Cucchiara", "title": "Fast Gesture Recognition with Multiple Stream Discrete HMMs on 3D\n  Skeletons", "comments": "Accepted in ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HMMs are widely used in action and gesture recognition due to their\nimplementation simplicity, low computational requirement, scalability and high\nparallelism. They have worth performance even with a limited training set. All\nthese characteristics are hard to find together in other even more accurate\nmethods. In this paper, we propose a novel double-stage classification\napproach, based on Multiple Stream Discrete Hidden Markov Models (MSD-HMM) and\n3D skeleton joint data, able to reach high performances maintaining all\nadvantages listed above. The approach allows both to quickly classify\npre-segmented gestures (offline classification), and to perform temporal\nsegmentation on streams of gestures (online classification) faster than real\ntime. We test our system on three public datasets, MSRAction3D, UTKinect-Action\nand MSRDailyAction, and on a new dataset, Kinteract Dataset, explicitly created\nfor Human Computer Interaction (HCI). We obtain state of the art performances\non all of them.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:37:13 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1703.02942", "submitter": "Thomas K\\\"ohler", "authors": "Franziska Schirrmacher, Thomas K\\\"ohler, Lennart Husvogt, James G.\n  Fujimoto, Joachim Hornegger, and Andreas K. Maier", "title": "QuaSI: Quantile Sparse Image Prior for Spatio-Temporal Denoising of\n  Retinal OCT Data", "comments": "submitted to MICCAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) enables high-resolution and non-invasive\n3D imaging of the human retina but is inherently impaired by speckle noise.\nThis paper introduces a spatio-temporal denoising algorithm for OCT data on a\nB-scan level using a novel quantile sparse image (QuaSI) prior. To remove\nspeckle noise while preserving image structures of diagnostic relevance, we\nimplement our QuaSI prior via median filter regularization coupled with a Huber\ndata fidelity model in a variational approach. For efficient energy\nminimization, we develop an alternating direction method of multipliers (ADMM)\nscheme using a linearization of median filtering. Our spatio-temporal method\ncan handle both, denoising of single B-scans and temporally consecutive\nB-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our\nalgorithm based on 4 B-scans only achieved comparable performance to averaging\n13 B-scans and outperformed other current denoising methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:59:51 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Schirrmacher", "Franziska", ""], ["K\u00f6hler", "Thomas", ""], ["Husvogt", "Lennart", ""], ["Fujimoto", "James G.", ""], ["Hornegger", "Joachim", ""], ["Maier", "Andreas K.", ""]]}, {"id": "1703.02952", "submitter": "Sina Sajadmanesh", "authors": "Seyed Ali Osia, Ali Shahin Shamsabadi, Sina Sajadmanesh, Ali Taheri,\n  Kleomenis Katevas, Hamid R. Rabiee, Nicholas D. Lane, Hamed Haddadi", "title": "A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n  Analytics", "comments": "To appear in IEEE Internet of Things Journal", "journal-ref": "IEEE Internet of Things Journal, May 2020", "doi": "10.1109/JIOT.2020.2967734", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) devices and applications are being deployed in our\nhomes and workplaces. These devices often rely on continuous data collection to\nfeed machine learning models. However, this approach introduces several privacy\nand efficiency challenges, as the service operator can perform unwanted\ninferences on the available data. Recently, advances in edge processing have\npaved the way for more efficient, and private, data processing at the source\nfor simple tasks and lighter models, though they remain a challenge for larger,\nand more complicated models. In this paper, we present a hybrid approach for\nbreaking down large, complex deep neural networks for cooperative,\nprivacy-preserving analytics. To this end, instead of performing the whole\noperation on the cloud, we let an IoT device to run the initial layers of the\nneural network, and then send the output to the cloud to feed the remaining\nlayers and produce the final result. In order to ensure that the user's device\ncontains no extra information except what is necessary for the main task and\npreventing any secondary inference on the data, we introduce Siamese\nfine-tuning. We evaluate the privacy benefits of this approach based on the\ninformation exposed to the cloud service. We also assess the local inference\ncost of different layers on a modern handset. Our evaluations show that by\nusing Siamese fine-tuning and at a small processing cost, we can greatly reduce\nthe level of unnecessary, potentially sensitive information in the personal\ndata, and thus achieving the desired trade-off between utility, privacy, and\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 18:21:03 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 11:14:55 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 11:43:10 GMT"}, {"version": "v4", "created": "Tue, 4 Apr 2017 05:28:20 GMT"}, {"version": "v5", "created": "Wed, 18 Apr 2018 05:44:35 GMT"}, {"version": "v6", "created": "Wed, 8 May 2019 11:29:32 GMT"}, {"version": "v7", "created": "Fri, 27 Dec 2019 00:15:48 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Osia", "Seyed Ali", ""], ["Shamsabadi", "Ali Shahin", ""], ["Sajadmanesh", "Sina", ""], ["Taheri", "Ali", ""], ["Katevas", "Kleomenis", ""], ["Rabiee", "Hamid R.", ""], ["Lane", "Nicholas D.", ""], ["Haddadi", "Hamed", ""]]}, {"id": "1703.03054", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Lisa Lee and Eric P. Xing", "title": "Deep Variation-structured Reinforcement Learning for Visual Relationship\n  and Attribute Detection", "comments": "This manuscript is accepted by CVPR 2017 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite progress in visual perception tasks such as image classification and\ndetection, computers still struggle to understand the interdependency of\nobjects in the scene as a whole, e.g., relations between objects or their\nattributes. Existing methods often ignore global context cues capturing the\ninteractions among different object instances, and can only recognize a handful\nof types by exhaustively training individual detectors for all possible\nrelationships. To capture such global interdependency, we propose a deep\nVariation-structured Reinforcement Learning (VRL) framework to sequentially\ndiscover object relationships and attributes in the whole image. First, a\ndirected semantic action graph is built using language priors to provide a rich\nand compact representation of semantic correlations between object categories,\npredicates, and attributes. Next, we use a variation-structured traversal over\nthe action graph to construct a small, adaptive action set for each step based\non the current state and historical actions. In particular, an ambiguity-aware\nobject mining scheme is used to resolve semantic ambiguity among object\ncategories that the object detector fails to distinguish. We then make\nsequential predictions using a deep RL framework, incorporating global context\ncues and semantic embeddings of previously extracted phrases in the state\nvector. Our experiments on the Visual Relationship Detection (VRD) dataset and\nthe large-scale Visual Genome dataset validate the superiority of VRL, which\ncan achieve significantly better detection results on datasets involving\nthousands of relationship and attribute types. We also demonstrate that VRL is\nable to predict unseen types embedded in our action graph by learning\ncorrelations on shared graph nodes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 22:09:10 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Liang", "Xiaodan", ""], ["Lee", "Lisa", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.03055", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Liang Lin and Xiaohui Shen and Jiashi Feng and\n  Shuicheng Yan and Eric P. Xing", "title": "Interpretable Structure-Evolving LSTM", "comments": "To appear in CVPR 2017 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for learning interpretable data\nrepresentation via Long Short-Term Memory (LSTM) recurrent neural networks over\nhierarchal graph structures. Instead of learning LSTM models over the pre-fixed\nstructures, we propose to further learn the intermediate interpretable\nmulti-level graph structures in a progressive and stochastic way from data\nduring the LSTM network optimization. We thus call this model the\nstructure-evolving LSTM. In particular, starting with an initial element-level\ngraph representation where each node is a small data element, the\nstructure-evolving LSTM gradually evolves the multi-level graph representations\nby stochastically merging the graph nodes with high compatibilities along the\nstacked LSTM layers. In each LSTM layer, we estimate the compatibility of two\nconnected nodes from their corresponding LSTM gate outputs, which is used to\ngenerate a merging probability. The candidate graph structures are accordingly\ngenerated where the nodes are grouped into cliques with their merging\nprobabilities. We then produce the new graph structure with a\nMetropolis-Hasting algorithm, which alleviates the risk of getting stuck in\nlocal optimums by stochastic sampling with an acceptance probability. Once a\ngraph structure is accepted, a higher-level graph is then constructed by taking\nthe partitioned cliques as its nodes. During the evolving process,\nrepresentation becomes more abstracted in higher-levels where redundant\ninformation is filtered out, allowing more efficient propagation of long-range\ndata dependencies. We evaluate the effectiveness of structure-evolving LSTM in\nthe application of semantic object parsing and demonstrate its advantage over\nstate-of-the-art LSTM models on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 22:09:38 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Liang", "Xiaodan", ""], ["Lin", "Liang", ""], ["Shen", "Xiaohui", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.03073", "submitter": "Liangzhen Lai", "authors": "Liangzhen Lai, Naveen Suda, Vikas Chandra", "title": "Deep Convolutional Neural Network Inference with Floating-point Weights\n  and Fixed-point Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (CNN) inference requires significant amount\nof memory and computation, which limits its deployment on embedded devices. To\nalleviate these problems to some extent, prior research utilize low precision\nfixed-point numbers to represent the CNN weights and activations. However, the\nminimum required data precision of fixed-point weights varies across different\nnetworks and also across different layers of the same network. In this work, we\npropose using floating-point numbers for representing the weights and\nfixed-point numbers for representing the activations. We show that using\nfloating-point representation for weights is more efficient than fixed-point\nrepresentation for the same bit-width and demonstrate it on popular large-scale\nCNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such\na representation scheme enables compact hardware multiply-and-accumulate (MAC)\nunit design. Experimental results show that the proposed scheme reduces the\nweight storage by up to 36% and power consumption of the hardware multiplier by\nup to 50%.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 23:49:20 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Lai", "Liangzhen", ""], ["Suda", "Naveen", ""], ["Chandra", "Vikas", ""]]}, {"id": "1703.03098", "submitter": "Yu Xiang", "authors": "Yu Xiang and Dieter Fox", "title": "DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks", "comments": "Published in RSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scene understanding is important for robots to interact with the 3D world\nin a meaningful way. Most previous works on 3D scene understanding focus on\nrecognizing geometrical or semantic properties of the scene independently. In\nthis work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a\nnovel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a\nnew recurrent neural network architecture for semantic labeling on RGB-D\nvideos. The output of the network is integrated with mapping techniques such as\nKinectFusion in order to inject semantic information into the reconstructed 3D\nscene. Experiments conducted on a real world dataset and a synthetic dataset\nwith RGB-D videos demonstrate the ability of our method in semantic 3D scene\nmapping.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 01:29:23 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 20:12:35 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Xiang", "Yu", ""], ["Fox", "Dieter", ""]]}, {"id": "1703.03108", "submitter": "Kazuhisa Matsunaga", "authors": "Kazuhisa Matsunaga, Akira Hamada, Akane Minagawa, Hiroshi Koga", "title": "Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep\n  Neural Network Ensemble", "comments": "4 pages. 3 figures. ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper reports the method and the evaluation results of Casio and\nShinshu University joint team for the ISBI Challenge 2017 - Skin Lesion\nAnalysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by\nISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924\nand seborrheic keratosis classifier AUC 0.993.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 02:35:59 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Matsunaga", "Kazuhisa", ""], ["Hamada", "Akira", ""], ["Minagawa", "Akane", ""], ["Koga", "Hiroshi", ""]]}, {"id": "1703.03126", "submitter": "Thomas Vandal", "authors": "Thomas Vandal, Evan Kodra, Sangram Ganguly, Andrew Michaelis,\n  Ramakrishna Nemani, Auroop R Ganguly", "title": "DeepSD: Generating High Resolution Climate Change Projections through\n  Single Image Super-Resolution", "comments": "9 pages, 5 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impacts of climate change are felt by most critical systems, such as\ninfrastructure, ecological systems, and power-plants. However, contemporary\nEarth System Models (ESM) are run at spatial resolutions too coarse for\nassessing effects this localized. Local scale projections can be obtained using\nstatistical downscaling, a technique which uses historical climate observations\nto learn a low-resolution to high-resolution mapping. Depending on statistical\nmodeling choices, downscaled projections have been shown to vary significantly\nterms of accuracy and reliability. The spatio-temporal nature of the climate\nsystem motivates the adaptation of super-resolution image processing techniques\nto statistical downscaling. In our work, we present DeepSD, a generalized\nstacked super resolution convolutional neural network (SRCNN) framework for\nstatistical downscaling of climate variables. DeepSD augments SRCNN with\nmulti-scale input channels to maximize predictability in statistical\ndownscaling. We provide a comparison with Bias Correction Spatial\nDisaggregation as well as three Automated-Statistical Downscaling approaches in\ndownscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km)\nover the Continental United States. Furthermore, a framework using the NASA\nEarth Exchange (NEX) platform is discussed for downscaling more than 20 ESM\nmodels with multiple emission scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 04:19:17 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Vandal", "Thomas", ""], ["Kodra", "Evan", ""], ["Ganguly", "Sangram", ""], ["Michaelis", "Andrew", ""], ["Nemani", "Ramakrishna", ""], ["Ganguly", "Auroop R", ""]]}, {"id": "1703.03156", "submitter": "Ingmar Weber", "authors": "Enes Kocabey, Mustafa Camurcu, Ferda Ofli, Yusuf Aytar, Javier Marin,\n  Antonio Torralba, Ingmar Weber", "title": "Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n  Media", "comments": "This is a preprint of a short paper accepted at ICWSM'17. Please cite\n  that version instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person's weight status can have profound implications on their life,\nranging from mental health, to longevity, to financial income. At the societal\nlevel, \"fat shaming\" and other forms of \"sizeism\" are a growing concern, while\nincreasing obesity rates are linked to ever raising healthcare costs. For these\nreasons, researchers from a variety of backgrounds are interested in studying\nobesity from all angles. To obtain data, traditionally, a person would have to\naccurately self-report their body-mass index (BMI) or would have to see a\ndoctor to have it measured. In this paper, we show how computer vision can be\nused to infer a person's BMI from social media images. We hope that our tool,\nwhich we release, helps to advance the study of social aspects related to body\nweight.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 06:48:47 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Kocabey", "Enes", ""], ["Camurcu", "Mustafa", ""], ["Ofli", "Ferda", ""], ["Aytar", "Yusuf", ""], ["Marin", "Javier", ""], ["Torralba", "Antonio", ""], ["Weber", "Ingmar", ""]]}, {"id": "1703.03186", "submitter": "Lucia Maddalena", "authors": "Mario Rosario Guarracino and Lucia Maddalena", "title": "Segmenting Dermoscopic Images", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic algorithm, named SDI, for the segmentation of skin\nlesions in dermoscopic images, articulated into three main steps: selection of\nthe image ROI, selection of the segmentation band, and segmentation. We present\nextensive experimental results achieved by the SDI algorithm on the lesion\nsegmentation dataset made available for the ISIC 2017 challenge on Skin Lesion\nAnalysis Towards Melanoma Detection, highlighting its advantages and\ndisadvantages.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 09:14:40 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Guarracino", "Mario Rosario", ""], ["Maddalena", "Lucia", ""]]}, {"id": "1703.03196", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Prior-based Hierarchical Segmentation Highlighting Structures of\n  Interest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. Several applications are\npresented that illustrate the method versatility and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 09:41:28 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1703.03230", "submitter": "Wenbin Li", "authors": "Jing Huo, Wenbin Li, Yinghuan Shi, Yang Gao, Hujun Yin", "title": "WebCaricature: a benchmark for caricature recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying caricature recognition is fundamentally important to understanding\nof face perception. However, little research has been conducted in the computer\nvision community, largely due to the shortage of suitable datasets. In this\npaper, a new caricature dataset is built, with the objective to facilitate\nresearch in caricature recognition. All the caricatures and face images were\ncollected from the Web. Compared with two existing datasets, this dataset is\nmuch more challenging, with a much greater number of available images, artistic\nstyles and larger intra-personal variations. Evaluation protocols are also\noffered together with their baseline performances on the dataset to allow fair\ncomparisons. Besides, a framework for caricature face recognition is presented\nto make a thorough analyze of the challenges of caricature recognition. By\nanalyzing the challenges, the goal is to show problems that worth to be further\ninvestigated. Additionally, based on the evaluation protocols and the\nframework, baseline performances of various state-of-the-art algorithms are\nprovided. A conclusion is that there is still a large space for performance\nimprovement and the analyzed problems still need further investigation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 11:27:26 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 06:55:44 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 18:34:45 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2018 13:22:59 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Huo", "Jing", ""], ["Li", "Wenbin", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""], ["Yin", "Hujun", ""]]}, {"id": "1703.03305", "submitter": "Umut G\\\"u\\c{c}l\\\"u", "authors": "Umut G\\\"u\\c{c}l\\\"u, Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Meysam Madadi,\n  Sergio Escalera, Xavier Bar\\'o, Jordi Gonz\\'alez, Rob van Lier, Marcel A. J.\n  van Gerven", "title": "End-to-end semantic face segmentation with conditional random fields as\n  convolutional, recurrent and adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a sharp increase in the number of related yet distinct\nadvances in semantic segmentation. Here, we tackle this problem by leveraging\nthe respective strengths of these advances. That is, we formulate a conditional\nrandom field over a four-connected graph as end-to-end trainable convolutional\nand recurrent networks, and estimate them via an adversarial process.\nImportantly, our model learns not only unary potentials but also pairwise\npotentials, while aggregating multi-scale contexts and controlling higher-order\ninconsistencies. We evaluate our model on two standard benchmark datasets for\nsemantic face segmentation, achieving state-of-the-art results on both of them.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 15:48:22 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["G\u00fc\u00e7l\u00fc", "Umut", ""], ["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""], ["Bar\u00f3", "Xavier", ""], ["Gonz\u00e1lez", "Jordi", ""], ["van Lier", "Rob", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1703.03329", "submitter": "Limin Wang", "authors": "Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool", "title": "UntrimmedNets for Weakly Supervised Action Recognition and Detection", "comments": "camera-ready version to appear in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current action recognition methods heavily rely on trimmed videos for model\ntraining. However, it is expensive and time-consuming to acquire a large-scale\ntrimmed video dataset. This paper presents a new weakly supervised\narchitecture, called UntrimmedNet, which is able to directly learn action\nrecognition models from untrimmed videos without the requirement of temporal\nannotations of action instances. Our UntrimmedNet couples two important\ncomponents, the classification module and the selection module, to learn the\naction models and reason about the temporal duration of action instances,\nrespectively. These two components are implemented with feed-forward networks,\nand UntrimmedNet is therefore an end-to-end trainable architecture. We exploit\nthe learned models for action recognition (WSR) and detection (WSD) on the\nuntrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet\nonly employs weak supervision, our method achieves performance superior or\ncomparable to that of those strongly supervised approaches on these two\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 16:29:39 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 12:38:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Limin", ""], ["Xiong", "Yuanjun", ""], ["Lin", "Dahua", ""], ["Van Gool", "Luc", ""]]}, {"id": "1703.03347", "submitter": "Chaitanya Mitash", "authors": "Chaitanya Mitash, Kostas E. Bekris and Abdeslam Boularias", "title": "A Self-supervised Learning System for Object Detection using Physics\n  Simulation and Multi-view Pose Estimation", "comments": "7 pages, 6 figures, accepted at the IEEE International Conference on\n  Intelligent Robots and Systems (IROS), Vancouver, Canada, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress has been achieved recently in object detection given advancements in\ndeep learning. Nevertheless, such tools typically require a large amount of\ntraining data and significant manual effort to label objects. This limits their\napplicability in robotics, where solutions must scale to a large number of\nobjects and variety of conditions. This work proposes an autonomous process for\ntraining a Convolutional Neural Network (CNN) for object detection and pose\nestimation in robotic setups. The focus is on detecting objects placed in\ncluttered, tight environments, such as a shelf with multiple objects. In\nparticular, given access to 3D object models, several aspects of the\nenvironment are physically simulated. The models are placed in physically\nrealistic poses with respect to their environment to generate a labeled\nsynthetic dataset. To further improve object detection, the network self-trains\nover real images that are labeled using a robust multi-view pose estimation\nprocess. The proposed training process is evaluated on several existing\ndatasets and on a dataset collected for this paper with a Motoman robotic arm.\nResults show that the proposed approach outperforms popular training processes\nrelying on synthetic - but not physically realistic - data and manual\nannotation. The key contributions are the incorporation of physical reasoning\nin the synthetic data generation process and the automation of the annotation\nprocess over real images.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:14:21 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 15:04:14 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Mitash", "Chaitanya", ""], ["Bekris", "Kostas E.", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1703.03349", "submitter": "Morris Antonello", "authors": "Morris Antonello, Marco Carraro, Marco Pierobon and Emanuele Menegatti", "title": "Fast and Robust Detection of Fallen People from a Mobile Robot", "comments": null, "journal-ref": null, "doi": "10.1109/IROS.2017.8206276", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of detecting fallen people lying on the\nfloor by means of a mobile robot equipped with a 3D depth sensor. In the\nproposed algorithm, inspired by semantic segmentation techniques, the 3D scene\nis over-segmented into small patches. Fallen people are then detected by means\nof two SVM classifiers: the first one labels each patch, while the second one\ncaptures the spatial relations between them. This novel approach showed to be\nrobust and fast. Indeed, thanks to the use of small patches, fallen people in\nreal cluttered scenes with objects side by side are correctly detected.\nMoreover, the algorithm can be executed on a mobile robot fitted with a\nstandard laptop making it possible to exploit the 2D environmental map built by\nthe robot and the multiple points of view obtained during the robot navigation.\nAdditionally, this algorithm is robust to illumination changes since it does\nnot rely on RGB data but on depth data. All the methods have been thoroughly\nvalidated on the IASLAB-RGBD Fallen Person Dataset, which is published online\nas a further contribution. It consists of several static and dynamic sequences\nwith 15 different people and 2 different environments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:15:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Antonello", "Morris", ""], ["Carraro", "Marco", ""], ["Pierobon", "Marco", ""], ["Menegatti", "Emanuele", ""]]}, {"id": "1703.03372", "submitter": "Dhanesh Ramachandram", "authors": "Dhanesh Ramachandram and Terrance DeVries", "title": "LesionSeg: Semantic segmentation of skin lesions using Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for skin lesion segmentation for the ISIC 2017 Skin\nLesion Segmentation Challenge. Our approach is based on a Fully Convolutional\nNetwork architecture which is trained end to end, from scratch, on a limited\ndataset. Our semantic segmentation architecture utilizes several recent\ninnovations in particularly in the combined use of (i) use of atrous\nconvolutions to increase the effective field of view of the network's receptive\nfield without increasing the number of parameters, (ii) the use of\nnetwork-in-network $1\\times1$ convolution layers to add capacity to the network\nand (iii) state-of-art super-resolution upsampling of predictions using\nsubpixel CNN layers. We reported a mean IOU score of 0.642 on the validation\nset provided by the organisers.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:52:28 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 19:56:40 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 01:37:18 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ramachandram", "Dhanesh", ""], ["DeVries", "Terrance", ""]]}, {"id": "1703.03400", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Pieter Abbeel, Sergey Levine", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "comments": "ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL\n  results at https://sites.google.com/view/maml, Blog post at\n  http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 18:58:03 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:14:08 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 16:45:29 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Finn", "Chelsea", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1703.03468", "submitter": "Manikanta Kotaru", "authors": "Manikanta Kotaru, Sachin Katti", "title": "Position Tracking for Virtual Reality Using Commodity WiFi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, experiencing virtual reality (VR) is a cumbersome experience which\neither requires dedicated infrastructure like infrared cameras to track the\nheadset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides\nonly 3-DoF (Degrees of Freedom) tracking which severely limits the user\nexperience (e.g., Samsung Gear). To truly enable VR everywhere, we need\nposition tracking to be available as a ubiquitous service. This paper presents\nWiCapture, a novel approach which leverages commodity WiFi infrastructure,\nwhich is ubiquitous today, for tracking purposes. We prototype WiCapture using\noff-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm\ncompared to sophisticated infrared based tracking systems like the Oculus,\nwhile providing much higher range, resistance to occlusion, ubiquity and ease\nof deployment.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 21:19:48 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 18:12:40 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Kotaru", "Manikanta", ""], ["Katti", "Sachin", ""]]}, {"id": "1703.03492", "submitter": "Qiuhong Ke", "authors": "Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, Farid\n  Boussaid", "title": "A New Representation of Skeleton Sequences for 3D Action Recognition", "comments": "CVPR 2017", "journal-ref": null, "doi": "10.1109/CVPR.2017.486", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for 3D action recognition with skeleton\nsequences (i.e., 3D trajectories of human skeleton joints). The proposed method\nfirst transforms each skeleton sequence into three clips each consisting of\nseveral frames for spatial temporal feature learning using deep neural\nnetworks. Each clip is generated from one channel of the cylindrical\ncoordinates of the skeleton sequence. Each frame of the generated clips\nrepresents the temporal information of the entire skeleton sequence, and\nincorporates one particular spatial relationship between the joints. The entire\nclips include multiple frames with different spatial relationships, which\nprovide useful spatial structural information of the human skeleton. We propose\nto use deep convolutional neural networks to learn long-term temporal\ninformation of the skeleton sequence from the frames of the generated clips,\nand then use a Multi-Task Learning Network (MTLN) to jointly process all frames\nof the generated clips in parallel to incorporate spatial structural\ninformation for action recognition. Experimental results clearly show the\neffectiveness of the proposed new representation and feature learning method\nfor 3D action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 23:47:27 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 00:41:51 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 01:29:39 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Ke", "Qiuhong", ""], ["Bennamoun", "Mohammed", ""], ["An", "Senjian", ""], ["Sohel", "Ferdous", ""], ["Boussaid", "Farid", ""]]}, {"id": "1703.03567", "submitter": "Ruoyu Liu", "authors": "Ruoyu Liu, Yao Zhao, Liang Zheng, Shikui Wei and Yi Yang", "title": "A New Evaluation Protocol and Benchmarking Results for Extendable\n  Cross-media Retrieval", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new evaluation protocol for cross-media retrieval which\nbetter fits the real-word applications. Both image-text and text-image\nretrieval modes are considered. Traditionally, class labels in the training and\ntesting sets are identical. That is, it is usually assumed that the query falls\ninto some pre-defined classes. However, in practice, the content of a query\nimage/text may vary extensively, and the retrieval system does not necessarily\nknow in advance the class label of a query. Considering the inconsistency\nbetween the real-world applications and laboratory assumptions, we think that\nthe existing protocol that works under identical train/test classes can be\nmodified and improved.\n  This work is dedicated to addressing this problem by considering the protocol\nunder an extendable scenario, \\ie, the training and testing classes do not\noverlap. We provide extensive benchmarking results obtained by the existing\nprotocol and the proposed new protocol on several commonly used datasets. We\ndemonstrate a noticeable performance drop when the testing classes are unseen\nduring training. Additionally, a trivial solution, \\ie, directly using the\npredicted class label for cross-media retrieval, is tested. We show that the\ntrivial solution is very competitive in traditional non-extendable retrieval,\nbut becomes less so under the new settings. The train/test split, evaluation\ncode, and benchmarking results are publicly available on our website.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 07:56:01 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Liu", "Ruoyu", ""], ["Zhao", "Yao", ""], ["Zheng", "Liang", ""], ["Wei", "Shikui", ""], ["Yang", "Yi", ""]]}, {"id": "1703.03608", "submitter": "Rita Ammanouil", "authors": "Rita Ammanouil, Andr\\'e Ferrari, R\\'emi Flamary, Chiara Ferrari, and\n  David Mary", "title": "Multi-frequency image reconstruction for radio-interferometry with\n  self-tuned regularization parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world's largest radio telescope, the Square Kilometer Array (SKA) will\nprovide radio interferometric data with unprecedented detail. Image\nreconstruction algorithms for radio interferometry are challenged to scale well\nwith TeraByte image sizes never seen before. In this work, we investigate one\nsuch 3D image reconstruction algorithm known as MUFFIN (MUlti-Frequency image\nreconstruction For radio INterferometry). In particular, we focus on the\nchallenging task of automatically finding the optimal regularization parameter\nvalues. In practice, finding the regularization parameters using classical grid\nsearch is computationally intensive and nontrivial due to the lack of ground-\ntruth. We adopt a greedy strategy where, at each iteration, the optimal\nparameters are found by minimizing the predicted Stein unbiased risk estimate\n(PSURE). The proposed self-tuned version of MUFFIN involves parallel and\ncomputationally efficient steps, and scales well with large- scale data.\nFinally, numerical results on a 3D image are presented to showcase the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 10:17:10 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Ammanouil", "Rita", ""], ["Ferrari", "Andr\u00e9", ""], ["Flamary", "R\u00e9mi", ""], ["Ferrari", "Chiara", ""], ["Mary", "David", ""]]}, {"id": "1703.03613", "submitter": "Luca Caltagirone", "authors": "Luca Caltagirone, Samuel Scheidegger, Lennart Svensson, Mattias Wahde", "title": "Fast LIDAR-based Road Detection Using Fully Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a deep learning approach has been developed to carry out road\ndetection using only LIDAR data. Starting from an unstructured point cloud,\ntop-view images encoding several basic statistics such as mean elevation and\ndensity are generated. By considering a top-view representation, road detection\nis reduced to a single-scale problem that can be addressed with a simple and\nfast fully convolutional neural network (FCN). The FCN is specifically designed\nfor the task of pixel-wise semantic segmentation by combining a large receptive\nfield with high-resolution feature maps. The proposed system achieved excellent\nperformance and it is among the top-performing algorithms on the KITTI road\nbenchmark. Its fast inference makes it particularly suitable for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 10:26:24 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 07:30:07 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Caltagirone", "Luca", ""], ["Scheidegger", "Samuel", ""], ["Svensson", "Lennart", ""], ["Wahde", "Mattias", ""]]}, {"id": "1703.03624", "submitter": "Guido Borghi", "authors": "Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara", "title": "From Depth Data to Head Pose Estimation: a Siamese approach", "comments": "VISAPP 2017. arXiv admin note: text overlap with arXiv:1703.01883", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correct estimation of the head pose is a problem of the great importance\nfor many applications. For instance, it is an enabling technology in automotive\nfor driver attention monitoring. In this paper, we tackle the pose estimation\nproblem through a deep learning network working in regression manner.\nTraditional methods usually rely on visual facial features, such as facial\nlandmarks or nose tip position. In contrast, we exploit a Convolutional Neural\nNetwork (CNN) to perform head pose estimation directly from depth data. We\nexploit a Siamese architecture and we propose a novel loss function to improve\nthe learning of the regression network layer. The system has been tested on two\npublic datasets, Biwi Kinect Head Pose and ICT-3DHP database. The reported\nresults demonstrate the improvement in accuracy with respect to current\nstate-of-the-art approaches and the real time capabilities of the overall\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 11:08:50 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Venturelli", "Marco", ""], ["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1703.03664", "submitter": "Scott Reed", "authors": "Scott Reed, A\\\"aron van den Oord, Nal Kalchbrenner, Sergio G\\'omez\n  Colmenarejo, Ziyu Wang, Dan Belov, Nando de Freitas", "title": "Parallel Multiscale Autoregressive Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PixelCNN achieves state-of-the-art results in density estimation for natural\nimages. Although training is fast, inference is costly, requiring one network\nevaluation per pixel; O(N) for N pixels. This can be sped up by caching\nactivations, but still involves generating each pixel sequentially. In this\nwork, we propose a parallelized PixelCNN that allows more efficient inference\nby modeling certain pixel groups as conditionally independent. Our new PixelCNN\nmodel achieves competitive density estimation and orders of magnitude speedup -\nO(log N) sampling instead of O(N) - enabling the practical generation of\n512x512 images. We evaluate the model on class-conditional image generation,\ntext-to-image synthesis, and action-conditional video generation, showing that\nour model achieves the best results among non-pixel-autoregressive density\nmodels that allow efficient sampling.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 12:58:23 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Reed", "Scott", ""], ["Oord", "A\u00e4ron van den", ""], ["Kalchbrenner", "Nal", ""], ["Colmenarejo", "Sergio G\u00f3mez", ""], ["Wang", "Ziyu", ""], ["Belov", "Dan", ""], ["de Freitas", "Nando", ""]]}, {"id": "1703.03702", "submitter": "Adrian Galdran", "authors": "Adrian Galdran, Aitor Alvarez-Gila, Maria Ines Meyer, Cristina L.\n  Saratxaga, Teresa Ara\\'ujo, Estibaliz Garrote, Guilherme Aresta, Pedro Costa,\n  A.M. Mendon\\c{c}a, Aur\\'elio Campilho", "title": "Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dermoscopic skin images are often obtained with different imaging devices,\nunder varying acquisition conditions. In this work, instead of attempting to\nperform intensity and color normalization, we propose to leverage computational\ncolor constancy techniques to build an artificial data augmentation technique\nsuitable for this kind of images. Specifically, we apply the \\emph{shades of\ngray} color constancy technique to color-normalize the entire training set of\nimages, while retaining the estimated illuminants. We then draw one sample from\nthe distribution of training set illuminants and apply it on the normalized\nimage. We employ this technique for training two deep convolutional neural\nnetworks for the tasks of skin lesion segmentation and skin lesion\nclassification, in the context of the ISIC 2017 challenge and without using any\nexternal dermatologic image set. Our results on the validation set are\npromising, and will be supplemented with extended results on the hidden test\nset when available.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 14:39:11 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Galdran", "Adrian", ""], ["Alvarez-Gila", "Aitor", ""], ["Meyer", "Maria Ines", ""], ["Saratxaga", "Cristina L.", ""], ["Ara\u00fajo", "Teresa", ""], ["Garrote", "Estibaliz", ""], ["Aresta", "Guilherme", ""], ["Costa", "Pedro", ""], ["Mendon\u00e7a", "A. M.", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1703.03848", "submitter": "Lamiaa Elrefaei", "authors": "Lamiaa A. Elrefaei, Mona Omar Al-musawa, Norah Abdullah Al-gohany", "title": "Development of An Android Application for Object Detection Based on\n  Color, Shape, or Local Features", "comments": null, "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.9, No.1, February 2017", "doi": "10.5121/ijma.2017.9103", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and recognition is an important task in many computer vision\napplications. In this paper an Android application was developed using Eclipse\nIDE and OpenCV3 Library. This application is able to detect objects in an image\nthat is loaded from the mobile gallery, based on its color, shape, or local\nfeatures. The image is processed in the HSV color domain for better color\ndetection. Circular shapes are detected using Circular Hough Transform and\nother shapes are detected using Douglas-Peucker algorithm. BRISK (binary robust\ninvariant scalable keypoints) local features were applied in the developed\nAndroid application for matching an object image in another scene image. The\nsteps of the proposed detection algorithms are described, and the interfaces of\nthe application are illustrated. The application is ported and tested on Galaxy\nS3, S6, and Note1 Smartphones. Based on the experimental results, the\napplication is capable of detecting eleven different colors, detecting two\ndimensional geometrical shapes including circles, rectangles, triangles, and\nsquares, and correctly match local features of object and scene images for\ndifferent conditions. The application could be used as a standalone\napplication, or as a part of another application such as Robot systems, traffic\nsystems, e-learning applications, information retrieval and many others.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 21:39:49 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Elrefaei", "Lamiaa A.", ""], ["Al-musawa", "Mona Omar", ""], ["Al-gohany", "Norah Abdullah", ""]]}, {"id": "1703.03854", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Gopalakrishnan Srinivasan, and Kaushik Roy", "title": "Convolutional Spike Timing Dependent Plasticity based Feature Learning\n  in Spiking Neural Networks", "comments": "11 pages, 10 figures, Under Consideration in Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-inspired learning models attempt to mimic the cortical architecture and\ncomputations performed in the neurons and synapses constituting the human brain\nto achieve its efficiency in cognitive tasks. In this work, we present\nconvolutional spike timing dependent plasticity based feature learning with\nbiologically plausible leaky-integrate-and-fire neurons in Spiking Neural\nNetworks (SNNs). We use shared weight kernels that are trained to encode\nrepresentative features underlying the input patterns thereby improving the\nsparsity as well as the robustness of the learning model. We demonstrate that\nthe proposed unsupervised learning methodology learns several visual categories\nfor object recognition with fewer number of examples and outperforms\ntraditional fully-connected SNN architectures while yielding competitive\naccuracy. Additionally, we observe that the learning model performs out-of-set\ngeneralization further making the proposed biologically plausible framework a\nviable and efficient architecture for future neuromorphic applications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:09:20 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 15:06:10 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Srinivasan", "Gopalakrishnan", ""], ["Roy", "Kaushik", ""]]}, {"id": "1703.03867", "submitter": "Hossein Javidnia", "authors": "S. Bazrafkan (1), H. Javidnia (1), J. Lemley (1), P. Corcoran (1) ((1)\n  Department of Electrical and Electronic Engineering, College of Engineering\n  and Informatics, National University of Ireland, Galway)", "title": "Depth from Monocular Images using a Semi-Parallel Deep Neural Network\n  (SPDNN) Hybrid Architecture", "comments": "44 pages, 25 figures", "journal-ref": "J. of Electronic Imaging, 27(4), 043041 (2018)", "doi": "10.1117/1.JEI.27.4.043041", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are applied to a wide range of problems in recent years.\nIn this work, Convolutional Neural Network (CNN) is applied to the problem of\ndetermining the depth from a single camera image (monocular depth). Eight\ndifferent networks are designed to perform depth estimation, each of them\nsuitable for a feature level. Networks with different pooling sizes determine\ndifferent feature levels. After designing a set of networks, these models may\nbe combined into a single network topology using graph optimization techniques.\nThis \"Semi Parallel Deep Neural Network (SPDNN)\" eliminates duplicated common\nnetwork layers, and can be further optimized by retraining to achieve an\nimproved model compared to the individual topologies. In this study, four SPDNN\nmodels are trained and have been evaluated at 2 stages on the KITTI dataset.\nThe ground truth images in the first part of the experiment are provided by the\nbenchmark, and for the second part, the ground truth images are the depth map\nresults from applying a state-of-the-art stereo matching method. The results of\nthis evaluation demonstrate that using post-processing techniques to refine the\ntarget of the network increases the accuracy of depth estimation on individual\nmono images. The second evaluation shows that using segmentation data alongside\nthe original data as the input can improve the depth estimation results to a\npoint where performance is comparable with stereo depth estimation. The\ncomputational time is also discussed in this study.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 23:10:11 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 11:01:12 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 14:31:24 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Bazrafkan", "S.", ""], ["Javidnia", "H.", ""], ["Lemley", "J.", ""], ["Corcoran", "P.", ""]]}, {"id": "1703.03872", "submitter": "Ning Xu", "authors": "Ning Xu, Brian Price, Scott Cohen, Thomas Huang", "title": "Deep Image Matting", "comments": "Computer Vision and Pattern Recognition 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is a fundamental computer vision problem and has many\napplications. Previous algorithms have poor performance when an image has\nsimilar foreground and background colors or complicated textures. The main\nreasons are prior methods 1) only use low-level features and 2) lack high-level\ncontext. In this paper, we propose a novel deep learning based algorithm that\ncan tackle both these problems. Our deep model has two parts. The first part is\na deep convolutional encoder-decoder network that takes an image and the\ncorresponding trimap as inputs and predict the alpha matte of the image. The\nsecond part is a small convolutional network that refines the alpha matte\npredictions of the first network to have more accurate alpha values and sharper\nedges. In addition, we also create a large-scale image matting dataset\nincluding 49300 training images and 1000 testing images. We evaluate our\nalgorithm on the image matting benchmark, our testing set, and a wide variety\nof real images. Experimental results clearly demonstrate the superiority of our\nalgorithm over previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 23:43:17 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 22:06:51 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 00:57:05 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Xu", "Ning", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Huang", "Thomas", ""]]}, {"id": "1703.03888", "submitter": "Jose Luis Garcia-Arroyo", "authors": "Jose Luis Garcia-Arroyo and Begonya Garcia-Zapirain", "title": "Segmentation of skin lesions based on fuzzy classification of pixels and\n  histogram thresholding", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an innovative method for segmentation of skin lesions in\ndermoscopy images developed by the authors, based on fuzzy classification of\npixels and histogram thresholding.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 01:18:14 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Garcia-Arroyo", "Jose Luis", ""], ["Garcia-Zapirain", "Begonya", ""]]}, {"id": "1703.03921", "submitter": "Vahid Alizadeh", "authors": "Vahid Alizadeh", "title": "Gait Pattern Recognition Using Accelerometers", "comments": "6 pages, project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion ability is one of the most important human properties, including gait\nas a basis of human transitional movement. Gait, as a biometric for recognizing\nhuman identities, can be non-intrusively captured signals using wearable or\nportable smart devices. In this study gait patterns is collected using a\nwireless platform of two sensors located at chest and right ankle of the\nsubjects. Then the raw data has undergone some preprocessing methods and\nsegmented into 5 seconds windows. Some time and frequency domain features is\nextracted and the performance evaluated by 5 different classifiers. Decision\nTree (with all features) and K-Nearest Neighbors (with 10 selected features)\nclassifiers reached 99.4% and 100% respectively.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 07:32:01 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Alizadeh", "Vahid", ""]]}, {"id": "1703.03937", "submitter": "Xavier Alameda-Pineda", "authors": "Xavier Alameda-Pineda and Andrea Pilzer and Dan Xu and Nicu Sebe and\n  Elisa Ricci", "title": "Viraliency: Pooling Local Virality", "comments": "Accepted at IEEE CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our overly-connected world, the automatic recognition of virality - the\nquality of an image or video to be rapidly and widely spread in social networks\n- is of crucial importance, and has recently awaken the interest of the\ncomputer vision community. Concurrently, recent progress in deep learning\narchitectures showed that global pooling strategies allow the extraction of\nactivation maps, which highlight the parts of the image most likely to contain\ninstances of a certain class. We extend this concept by introducing a pooling\nlayer that learns the size of the support area to be averaged: the learned\ntop-N average (LENA) pooling. We hypothesize that the latent concepts (feature\nmaps) describing virality may require such a rich pooling strategy. We assess\nthe effectiveness of the LENA layer by appending it on top of a convolutional\nsiamese architecture and evaluate its performance on the task of predicting and\nlocalizing virality. We report experiments on two publicly available datasets\nannotated for virality and show that our method outperforms state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 10:01:11 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 07:36:58 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Alameda-Pineda", "Xavier", ""], ["Pilzer", "Andrea", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "1703.03940", "submitter": "Juan Rojas", "authors": "Ruotao He, Juan Rojas and Yisheng Guan", "title": "A 3D Object Detection and Pose Estimation Pipeline Using RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection and pose estimation has been studied extensively in\nrecent decades for its potential applications in robotics. However, there still\nremains challenges when we aim at detecting multiple objects while retaining\nlow false positive rate in cluttered environments. This paper proposes a robust\n3D object detection and pose estimation pipeline based on RGB-D images, which\ncan detect multiple objects simultaneously while reducing false positives.\nDetection begins with template matching and yields a set of template matches. A\nclustering algorithm then groups templates of similar spatial location and\nproduces multiple-object hypotheses. A scoring function evaluates the\nhypotheses using their associated templates and non-maximum suppression is\nadopted to remove duplicate results based on the scores. Finally, a combination\nof point cloud processing algorithms are used to compute objects' 3D poses.\nExisting object hypotheses are verified by computing the overlap between model\nand scene points. Experiments demonstrate that our approach provides\ncompetitive results comparable to the state-of-the-arts and can be applied to\nrobot random bin-picking.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 10:09:02 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["He", "Ruotao", ""], ["Rojas", "Juan", ""], ["Guan", "Yisheng", ""]]}, {"id": "1703.03949", "submitter": "Grigorios Kalliatakis M.A.", "authors": "Grigorios Kalliatakis, Nikolaos Vidakis and Georgios Triantafyllidis", "title": "Web-based visualisation of head pose and facial expressions changes:\n  monitoring human activity using depth data", "comments": "8th Computer Science and Electronic Engineering, (CEEC 2016),\n  University of Essex, UK, 6 pages", "journal-ref": null, "doi": "10.1109/CEEC.2016.7835887", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent advances in the field of head pose estimation and\nfacial expression recognition, raising the cognitive level when analysing human\nactivity presents serious challenges to current concepts. Motivated by the need\nof generating comprehensible visual representations from different sets of\ndata, we introduce a system capable of monitoring human activity through head\npose and facial expression changes, utilising an affordable 3D sensing\ntechnology (Microsoft Kinect sensor). An approach build on discriminative\nrandom regression forests was selected in order to rapidly and accurately\nestimate head pose changes in unconstrained environment. In order to complete\nthe secondary process of recognising four universal dominant facial expressions\n(happiness, anger, sadness and surprise), emotion recognition via facial\nexpressions (ERFE) was adopted. After that, a lightweight data exchange format\n(JavaScript Object Notation-JSON) is employed, in order to manipulate the data\nextracted from the two aforementioned settings. Such mechanism can yield a\nplatform for objective and effortless assessment of human activity within the\ncontext of serious gaming and human-computer interaction.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 11:07:22 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 10:33:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Vidakis", "Nikolaos", ""], ["Triantafyllidis", "Georgios", ""]]}, {"id": "1703.03957", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Jun Wu, Lin Feng, Feilong Wang", "title": "Neural method for Explicit Mapping of Quasi-curvature Locally Linear\n  Embedding in image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new explicit nonlinear dimensionality reduction using\nneural networks for image retrieval tasks. We first proposed a Quasi-curvature\nLocally Linear Embedding (QLLE) for training set. QLLE guarantees the linear\ncriterion in neighborhood of each sample. Then, a neural method (NM) is\nproposed for out-of-sample problem. Combining QLLE and NM, we provide a\nexplicit nonlinear dimensionality reduction approach for efficient image\nretrieval. The experimental results in three benchmark datasets illustrate that\nour method can get better performance than other state-of-the-art out-of-sample\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 11:29:01 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Liu", "Shenglan", ""], ["Wu", "Jun", ""], ["Feng", "Lin", ""], ["Wang", "Feilong", ""]]}, {"id": "1703.04019", "submitter": "Agata Migalska", "authors": "Agata Migalska and JP Lewis", "title": "Negentropic Planar Symmetry Detector", "comments": "25 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we observe that information theoretical concepts are valuable\ntools for extracting information from images and, in particular, information on\nimage symmetries. It is shown that the problem of detecting reflectional and\nrotational symmetries in a two-dimensional image can be reduced to the problem\nof detecting point-symmetry and periodicity in one-dimensional negentropy\nfunctions. Based on these findings a detector of reflectional and rotational\nglobal symmetries in greyscale images is constructed. We discuss the importance\nof high precision in symmetry detection in applications arising from quality\ncontrol and illustrate how the proposed method satisfies this requirement.\nFinally, a superior performance of our method to other existing methods,\ndemonstrated by the results of a rigorous experimental verification, is an\nindication that our approach rooted in information theory is a promising\ndirection in a development of a robust and widely applicable symmetry detector.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 19:16:38 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Migalska", "Agata", ""], ["Lewis", "JP", ""]]}, {"id": "1703.04044", "submitter": "Gustav Larsson", "authors": "Gustav Larsson, Michael Maire, Gregory Shakhnarovich", "title": "Colorization as a Proxy Task for Visual Understanding", "comments": "CVPR 2017 (Project page:\n  http://people.cs.uchicago.edu/~larsson/color-proxy/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and improve self-supervision as a drop-in replacement for\nImageNet pretraining, focusing on automatic colorization as the proxy task.\nSelf-supervised training has been shown to be more promising for utilizing\nunlabeled data than other, traditional unsupervised learning methods. We build\non this success and evaluate the ability of our self-supervised network in\nseveral contexts. On VOC segmentation and classification tasks, we present\nresults that are state-of-the-art among methods not using ImageNet labels for\npretraining representations.\n  Moreover, we present the first in-depth analysis of self-supervision via\ncolorization, concluding that formulation of the loss, training details and\nnetwork architecture play important roles in its effectiveness. This\ninvestigation is further expanded by revisiting the ImageNet pretraining\nparadigm, asking questions such as: How much training data is needed? How many\nlabels are needed? How much do features change when fine-tuned? We relate these\nquestions back to self-supervision by showing that colorization provides a\nsimilarly powerful supervisory signal as various flavors of ImageNet\npretraining.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 23:32:59 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 17:20:26 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 17:40:29 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Larsson", "Gustav", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1703.04062", "submitter": "I Gede Pasek Suta Wijaya", "authors": "I Gede Pasek Suta Wijaya, Keiichi Uchimura and Gou Koutaki", "title": "Multi-Pose Face Recognition Using Hybrid Face Features Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a multi-pose face recognition approach using hybrid face\nfeatures descriptors (HFFD). The HFFD is a face descriptor containing of rich\ndiscriminant information that is created by fusing some frequency-based\nfeatures extracted using both wavelet and DCT analysis of several different\nposes of 2D face images. The main aim of this method is to represent the\nmulti-pose face images using a dominant frequency component with still having\nreasonable achievement compared to the recent multi-pose face recognition\nmethods. The HFFD based face recognition tends to achieve better performance\nthan that of the recent 2D-based face recognition method. In addition, the\nHFFD-based face recognition also is sufficiently to handle large face\nvariability due to face pose variations .\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 02:58:07 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wijaya", "I Gede Pasek Suta", ""], ["Uchimura", "Keiichi", ""], ["Koutaki", "Gou", ""]]}, {"id": "1703.04071", "submitter": "Chunpeng Wu", "authors": "Chunpeng Wu, Wei Wen, Tariq Afzal, Yongmei Zhang, Yiran Chen, and Hai\n  Li", "title": "A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification\n  and Domain Adaptation", "comments": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, DNN model compression based on network architecture design, e.g.,\nSqueezeNet, attracted a lot attention. No accuracy drop on image classification\nis observed on these extremely compact networks, compared to well-known models.\nAn emerging question, however, is whether these model compression techniques\nhurt DNN's learning ability other than classifying images on a single dataset.\nOur preliminary experiment shows that these compression methods could degrade\ndomain adaptation (DA) ability, though the classification performance is\npreserved. Therefore, we propose a new compact network architecture and\nunsupervised DA method in this paper. The DNN is built on a new basic module\nConv-M which provides more diverse feature extractors without significantly\nincreasing parameters. The unified framework of our DA method will\nsimultaneously learn invariance across domains, reduce divergence of feature\nrepresentations, and adapt label prediction. Our DNN has 4.1M parameters, which\nis only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN\nobtains GoogLeNet-level accuracy both on classification and DA, and our DA\nmethod slightly outperforms previous competitive ones. Put all together, our DA\nstrategy based on our DNN achieves state-of-the-art on sixteen of total\neighteen DA tasks on popular Office-31 and Office-Caltech datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 05:07:00 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 03:21:57 GMT"}, {"version": "v3", "created": "Wed, 29 Mar 2017 05:40:52 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 05:17:42 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Wu", "Chunpeng", ""], ["Wen", "Wei", ""], ["Afzal", "Tariq", ""], ["Zhang", "Yongmei", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1703.04078", "submitter": "Huaixiu Zheng", "authors": "Saifeng Liu, Huaixiu Zheng, Yesu Feng, Wei Li", "title": "Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric\n  MRI", "comments": "4 pages, 4 figures, Proc. SPIE 10134, Medical Imaging 2017", "journal-ref": null, "doi": "10.1117/12.2277121", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep learning architecture (XmasNet) based on convolutional neural\nnetworks was developed for the classification of prostate cancer lesions, using\nthe 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end\ntraining was performed for XmasNet, with data augmentation done through 3D\nrotation and slicing, in order to incorporate the 3D information of the lesion.\nXmasNet outperformed traditional machine learning models based on engineered\nfeatures, for both train and test data. For the test data, XmasNet outperformed\n69 methods from 33 participating groups and achieved the second highest AUC\n(0.84) in the PROSTATEx challenge. This study shows the great potential of deep\nlearning for cancer imaging.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 07:19:55 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Liu", "Saifeng", ""], ["Zheng", "Huaixiu", ""], ["Feng", "Yesu", ""], ["Li", "Wei", ""]]}, {"id": "1703.04079", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, Asim Unmesh, Qixing Huang and Karthik Ramani", "title": "SurfNet: Generating 3D shape surfaces using deep residual networks", "comments": "CVPR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape models are naturally parameterized using vertices and faces, \\ie,\ncomposed of polygons forming a surface. However, current 3D learning paradigms\nfor predictive and generative tasks using convolutional neural networks focus\non a voxelized representation of the object. Lifting convolution operators from\nthe traditional 2D to 3D results in high computational overhead with little\nadditional benefit as most of the geometry information is contained on the\nsurface boundary. Here we study the problem of directly generating the 3D shape\nsurface of rigid and non-rigid shapes using deep convolutional neural networks.\nWe develop a procedure to create consistent `geometry images' representing the\nshape surface of a category of 3D objects. We then use this consistent\nrepresentation for category-specific shape surface generation from a parametric\nrepresentation or an image by developing novel extensions of deep residual\nnetworks for the task of geometry image generation. Our experiments indicate\nthat our network learns a meaningful representation of shape surfaces allowing\nit to interpolate between shape orientations and poses, invent new shape\nsurfaces and reconstruct 3D shape surfaces from previously unseen images.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 07:21:50 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Sinha", "Ayan", ""], ["Unmesh", "Asim", ""], ["Huang", "Qixing", ""], ["Ramani", "Karthik", ""]]}, {"id": "1703.04088", "submitter": "Yang Zhao", "authors": "Yang Zhao, Ronggang Wang, Wei Jia, Jianchao Yang, Wenmin Wang, Wen Gao", "title": "Local Patch Encoding-Based Method for Single Image Super-Resolution", "comments": "20 pages, 8 figures", "journal-ref": "Y. Zhao, R. Wang, W. Jia, J. Yang, W. Wang , W. Gao, Local patch\n  encoding-based method for single image super-resolution, Information\n  Sciences, vol.433, pp.292-305, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-based super-resolution (SR) methods often focus on dictionary\nlearning or network training. In this paper, we discuss in detail a new SR\nmethod based on local patch encoding (LPE) instead of traditional dictionary\nlearning. The proposed method consists of a learning stage and a reconstructing\nstage. In the learning stage, image patches are classified into different\nclasses by means of the proposed LPE, and then a projection matrix is computed\nfor each class by utilizing a simple constraint. In the reconstructing stage,\nan input LR patch can be simply reconstructed by computing its LPE code and\nthen multiplying the corresponding projection matrix. Furthermore, we discuss\nthe relationship between the proposed method and the anchored neighborhood\nregression methods; we also analyze the extendibility of the proposed method.\nThe experimental results on several image sets demonstrate the effectiveness of\nthe LPE-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 09:47:51 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 01:45:24 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Zhao", "Yang", ""], ["Wang", "Ronggang", ""], ["Jia", "Wei", ""], ["Yang", "Jianchao", ""], ["Wang", "Wenmin", ""], ["Gao", "Wen", ""]]}, {"id": "1703.04096", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang", "title": "Improving Interpretability of Deep Neural Networks with Semantic\n  Information", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability of deep neural networks (DNNs) is essential since it enables\nusers to understand the overall strengths and weaknesses of the models, conveys\nan understanding of how the models will behave in the future, and how to\ndiagnose and correct potential problems. However, it is challenging to reason\nabout what a DNN actually does due to its opaque or black-box nature. To\naddress this issue, we propose a novel technique to improve the\ninterpretability of DNNs by leveraging the rich semantic information embedded\nin human descriptions. By concentrating on the video captioning task, we first\nextract a set of semantically meaningful topics from the human descriptions\nthat cover a wide range of visual concepts, and integrate them into the model\nwith an interpretive loss. We then propose a prediction difference maximization\nalgorithm to interpret the learned features of each neuron. Experimental\nresults demonstrate its effectiveness in video captioning using the\ninterpretable features, which can also be transferred to video action\nrecognition. By clearly understanding the learned features, users can easily\nrevise false predictions via a human-in-the-loop procedure.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 10:38:10 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 11:48:31 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1703.04101", "submitter": "Grigorios Kalliatakis M.A.", "authors": "Grigorios Kalliatakis, Georgios Stamatiadis, Shoaib Ehsan, Ales\n  Leonardis, Juergen Gall, Anca Sticlaru and Klaus D. McDonald-Maier", "title": "Evaluating Deep Convolutional Neural Networks for Material\n  Classification", "comments": "In Proceedings of the 12th International Conference on Computer\n  Vision Theory and Applications (VISAPP 2017), 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the material category of a surface from an image is a demanding\ntask in perception that is drawing increasing attention. Following the recent\nremarkable results achieved for image classification and object detection\nutilising Convolutional Neural Networks (CNNs), we empirically study material\nclassification of everyday objects employing these techniques. More\nspecifically, we conduct a rigorous evaluation of how state-of-the art CNN\narchitectures compare on a common ground over widely used material databases.\nExperimental results on three challenging material databases show that the best\nperforming CNN architectures can achieve up to 94.99\\% mean average precision\nwhen classifying materials.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 11:29:00 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 10:36:15 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Stamatiadis", "Georgios", ""], ["Ehsan", "Shoaib", ""], ["Leonardis", "Ales", ""], ["Gall", "Juergen", ""], ["Sticlaru", "Anca", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1703.04103", "submitter": "Grigorios Kalliatakis M.A.", "authors": "Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli, Ales Leonardis,\n  Juergen Gall and Klaus D. McDonald-Maier", "title": "Detection of Human Rights Violations in Images: Can Convolutional Neural\n  Networks help?", "comments": "In Proceedings of the 12th International Conference on Computer\n  Vision Theory and Applications (VISAPP 2017), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After setting the performance benchmarks for image, video, speech and audio\nprocessing, deep convolutional networks have been core to the greatest advances\nin image recognition tasks in recent times. This raises the question of whether\nthere are any benefit in targeting these remarkable deep architectures with the\nunattempted task of recognising human rights violations through digital images.\nUnder this perspective, we introduce a new, well-sampled human rights-centric\ndataset called Human Rights Understanding (HRUN). We conduct a rigorous\nevaluation on a common ground by combining this dataset with different\nstate-of-the-art deep convolutional architectures in order to achieve\nrecognition of human rights violations. Experimental results on the HRUN\ndataset have shown that the best performing CNN architectures can achieve up to\n88.10\\% mean average precision. Additionally, our experiments demonstrate that\nincreasing the size of the training samples is crucial for achieving an\nimprovement on mean average precision principally when utilising very deep\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 11:39:41 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 10:37:25 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Ehsan", "Shoaib", ""], ["Fasli", "Maria", ""], ["Leonardis", "Ales", ""], ["Gall", "Juergen", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1703.04105", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis, Georgios Tzimiropoulos", "title": "Combining Residual Networks with LSTMs for Lipreading", "comments": "Submitted to Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end deep learning architecture for word-level visual\nspeech recognition. The system is a combination of spatiotemporal\nconvolutional, residual and bidirectional Long Short-Term Memory networks. We\ntrain and evaluate it on the Lipreading In-The-Wild benchmark, a challenging\ndatabase of 500-size target-words consisting of 1.28sec video excerpts from BBC\nTV broadcasts. The proposed network attains word accuracy equal to 83.0,\nyielding 6.8 absolute improvement over the current state-of-the-art, without\nusing information about word boundaries during training or testing.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 12:06:04 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 08:32:06 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 09:09:18 GMT"}, {"version": "v4", "created": "Fri, 8 Sep 2017 10:08:34 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Stafylakis", "Themos", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1703.04111", "submitter": "Roy Jevnisek J", "authors": "Roy J Jevnisek, Shai Avidan", "title": "Co-occurrence Filter", "comments": "accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on\nthe Bilateral Filter (BF) but instead of using a Gaussian on the range values\nto preserve edges it relies on a co-occurrence matrix. Pixel values that\nco-occur frequently in the image (i.e., inside textured regions) will have a\nhigh weight in the co-occurrence matrix. This, in turn, means that such pixel\npairs will be averaged and hence smoothed, regardless of their intensity\ndifferences. On the other hand, pixel values that rarely co-occur (i.e., across\ntexture boundaries) will have a low weight in the co-occurrence matrix. As a\nresult, they will not be averaged and the boundary between them will be\npreserved. The CoF therefore extends the BF to deal with boundaries, not just\nedges. It learns co-occurrences directly from the image. We can achieve various\nfiltering results by directing it to learn the co-occurrence matrix from a part\nof the image, or a different image. We give the definition of the filter,\ndiscuss how to use it with color images and show several use cases.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 12:41:16 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 06:52:54 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Jevnisek", "Roy J", ""], ["Avidan", "Shai", ""]]}, {"id": "1703.04135", "submitter": "Ji Li", "authors": "Ji Li, Zihao Yuan, Zhe Li, Caiwen Ding, Ao Ren, Qinru Qiu, Jeffrey\n  Draper, Yanzhi Wang", "title": "Hardware-Driven Nonlinear Activation for Stochastic Computing Based Deep\n  Convolutional Neural Networks", "comments": "This paper is accepted by 2017 International Joint Conference on\n  Neural Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Convolutional Neural Networks (DCNNs) have made unprecedented\nprogress, achieving the accuracy close to, or even better than human-level\nperception in various tasks. There is a timely need to map the latest software\nDCNNs to application-specific hardware, in order to achieve orders of magnitude\nimprovement in performance, energy efficiency and compactness. Stochastic\nComputing (SC), as a low-cost alternative to the conventional binary computing\nparadigm, has the potential to enable massively parallel and highly scalable\nhardware implementation of DCNNs. One major challenge in SC based DCNNs is\ndesigning accurate nonlinear activation functions, which have a significant\nimpact on the network-level accuracy but cannot be implemented accurately by\nexisting SC computing blocks. In this paper, we design and optimize SC based\nneurons, and we propose highly accurate activation designs for the three most\nfrequently used activation functions in software DCNNs, i.e, hyperbolic\ntangent, logistic, and rectified linear units. Experimental results on LeNet-5\nusing MNIST dataset demonstrate that compared with a binary ASIC hardware DCNN,\nthe DCNN with the proposed SC neurons can achieve up to 61X, 151X, and 2X\nimprovement in terms of area, power, and energy, respectively, at the cost of\nsmall precision degradation.In addition, the SC approach achieves up to 21X and\n41X of the area, 41X and 72X of the power, and 198200X and 96443X of the\nenergy, compared with CPU and GPU approaches, respectively, while the error is\nincreased by less than 3.07%. ReLU activation is suggested for future SC based\nDCNNs considering its superior performance under a small bit stream length.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 15:27:23 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Li", "Ji", ""], ["Yuan", "Zihao", ""], ["Li", "Zhe", ""], ["Ding", "Caiwen", ""], ["Ren", "Ao", ""], ["Qiu", "Qinru", ""], ["Draper", "Jeffrey", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1703.04197", "submitter": "Lei Bi", "authors": "Lei Bi, Jinman Kim, Euijoon Ahn, Dagan Feng", "title": "Automatic Skin Lesion Analysis using Large-scale Dermoscopy Images and\n  Deep Residual Networks", "comments": "Submission for ISIC2017 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malignant melanoma has one of the most rapidly increasing incidences in the\nworld and has a considerable mortality rate. Early diagnosis is particularly\nimportant since melanoma can be cured with prompt excision. Dermoscopy images\nplay an important role in the non-invasive early detection of melanoma [1].\nHowever, melanoma detection using human vision alone can be subjective,\ninaccurate and poorly reproducible even among experienced dermatologists. This\nis attributed to the challenges in interpreting images with diverse\ncharacteristics including lesions of varying sizes and shapes, lesions that may\nhave fuzzy boundaries, different skin colors and the presence of hair [2].\nTherefore, the automatic analysis of dermoscopy images is a valuable aid for\nclinical decision making and for image-based diagnosis to identify diseases\nsuch as melanoma [1-4]. Deep residual networks (ResNets) has achieved\nstate-of-the-art results in image classification and detection related problems\n[5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to\nexploit the deep ResNets for robust visual features learning and\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 23:32:18 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 01:09:23 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Ahn", "Euijoon", ""], ["Feng", "Dagan", ""]]}, {"id": "1703.04244", "submitter": "Yang Zhao", "authors": "Yang Zhao, Guoqing Li, Wenjun Xie, Wei Jia, Hai Min, and Xiaoping Liu", "title": "GUN: Gradual Upsampling Network for Single Image Super-Resolution", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient super-resolution (SR) method based on deep\nconvolutional neural network (CNN) is proposed, namely Gradual Upsampling\nNetwork (GUN). Recent CNN based SR methods often preliminarily magnify the low\nresolution (LR) input to high resolution (HR) and then reconstruct the HR\ninput, or directly reconstruct the LR input and then recover the HR result at\nthe last layer. The proposed GUN utilizes a gradual process instead of these\ntwo commonly used frameworks. The GUN consists of an input layer, multiple\nupsampling and convolutional layers, and an output layer. By means of the\ngradual process, the proposed network can simplify the direct SR problem to\nmultistep easier upsampling tasks with very small magnification factor in each\nstep. Furthermore, a gradual training strategy is presented for the GUN. In the\nproposed training process, an initial network can be easily trained with\nedge-like samples, and then the weights are gradually tuned with more complex\nsamples. The GUN can recover fine and vivid results, and is easy to be trained.\nThe experimental results on several image sets demonstrate the effectiveness of\nthe proposed network.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 04:34:12 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 01:49:33 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Zhao", "Yang", ""], ["Li", "Guoqing", ""], ["Xie", "Wenjun", ""], ["Jia", "Wei", ""], ["Min", "Hai", ""], ["Liu", "Xiaoping", ""]]}, {"id": "1703.04264", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Jason L. Williams, Karl Granstr\\\"om,\n  Lennart Svensson", "title": "Poisson multi-Bernoulli mixture filter: direct derivation and\n  implementation", "comments": null, "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, vol. 54,\n  no. 4, pp. 1883-1901, Aug. 2018", "doi": "10.1109/TAES.2018.2805153", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter\nfor multi-target tracking with the standard point target measurements without\nusing probability generating functionals or functional derivatives. We also\nestablish the connection with the \\delta-generalised labelled multi-Bernoulli\n(\\delta-GLMB) filter, showing that a \\delta-GLMB density represents a\nmulti-Bernoulli mixture with labelled targets so it can be seen as a special\ncase of PMBM. In addition, we propose an implementation for linear/Gaussian\ndynamic and measurement models and how to efficiently obtain typical estimators\nin the literature from the PMBM. The PMBM filter is shown to outperform other\nfilters in the literature in a challenging scenario.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 06:08:51 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 11:23:37 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 17:42:30 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 14:35:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Williams", "Jason L.", ""], ["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""]]}, {"id": "1703.04301", "submitter": "S M Jaisa", "authors": "S. M. Jaisakthi, Aravindan Chandrabose, P. Mirunalini", "title": "Automatic Skin Lesion Segmentation using Semi-supervised Learning\n  Technique", "comments": "4 pages with 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is the most common of all cancers and each year million cases of\nskin cancer are treated. Treating and curing skin cancer is easy, if it is\ndiagnosed and treated at an early stage. In this work we propose an automatic\ntechnique for skin lesion segmentation in dermoscopic images which helps in\nclassifying the skin cancer types. The proposed method comprises of two major\nphases (1) preprocessing and (2) segmentation using semi-supervised learning\nalgorithm. In the preprocessing phase noise are removed using filtering\ntechnique and in the segmentation phase skin lesions are segmented based on\nclustering technique. K-means clustering algorithm is used to cluster the\npreprocessed images and skin lesions are filtered from these clusters based on\nthe color feature. Color of the skin lesions are learned from the training\nimages using histograms calculations in RGB color space. The training images\nwere downloaded from the ISIC 2017 challenge website and the experimental\nresults were evaluated using validation and test sets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 09:30:34 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Jaisakthi", "S. M.", ""], ["Chandrabose", "Aravindan", ""], ["Mirunalini", "P.", ""]]}, {"id": "1703.04309", "submitter": "Alex Kendall", "authors": "Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan\n  Kennedy, Abraham Bachrach, Adam Bry", "title": "End-to-End Learning of Geometry and Context for Deep Stereo Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning architecture for regressing disparity from a\nrectified pair of stereo images. We leverage knowledge of the problem's\ngeometry to form a cost volume using deep feature representations. We learn to\nincorporate contextual information using 3-D convolutions over this volume.\nDisparity values are regressed from the cost volume using a proposed\ndifferentiable soft argmin operation, which allows us to train our method\nend-to-end to sub-pixel accuracy without any additional post-processing or\nregularization. We evaluate our method on the Scene Flow and KITTI datasets and\non KITTI we set a new state-of-the-art benchmark, while being significantly\nfaster than competing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 10:00:52 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Kendall", "Alex", ""], ["Martirosyan", "Hayk", ""], ["Dasgupta", "Saumitro", ""], ["Henry", "Peter", ""], ["Kennedy", "Ryan", ""], ["Bachrach", "Abraham", ""], ["Bry", "Adam", ""]]}, {"id": "1703.04332", "submitter": "Michele Alberti", "authors": "Michele Alberti, Mathias Seuret, Rolf Ingold and Marcus Liwicki\n  (University Of Fribourg, Fribourg, Switzerland)", "title": "A Pitfall of Unsupervised Pre-Training", "comments": "Conference on Neural Information Processing Systems, Deep Learning:\n  Bridging Theory and Practice, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point of this paper is to question typical assumptions in deep learning\nand suggest alternatives. A particular contribution is to prove that even if a\nStacked Convolutional Auto-Encoder is good at reconstructing pictures, it is\nnot necessarily good at discriminating their classes. When using Auto-Encoders,\nintuitively one assumes that features which are good for reconstruction will\nalso lead to high classification accuracy. Indeed, it became research practice\nand is a suggested strategy by introductory books. However, we prove that this\nis not always the case. We thoroughly investigate the quality of features\nproduced by Stacked Convolutional Auto-Encoders when trained to reconstruct\ntheir input. In particular, we analyze the relation between the reconstruction\nand classification capabilities of the network, if we were to use the same\nfeatures for both tasks. Experimental results suggest that in fact, there is no\ncorrelation between the reconstruction score and the quality of features for a\nclassification task. This means, more formally, that the sub-dimension\nrepresentation space learned from the Stacked Convolutional Auto-Encoder (while\nbeing trained for input reconstruction) is not necessarily better separable\nthan the initial input space. Furthermore, we show that the reconstruction\nerror is not a good metric to assess the quality of features, because it is\nbiased by the decoder quality. We do not question the usefulness of\npre-training, but we conclude that aiming for the lowest reconstruction error\nis not necessarily a good idea if afterwards one performs a classification\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:00 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 08:30:06 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 07:51:08 GMT"}, {"version": "v4", "created": "Sun, 17 Dec 2017 20:22:25 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Alberti", "Michele", "", "University Of Fribourg, Fribourg, Switzerland"], ["Seuret", "Mathias", "", "University Of Fribourg, Fribourg, Switzerland"], ["Ingold", "Rolf", "", "University Of Fribourg, Fribourg, Switzerland"], ["Liwicki", "Marcus", "", "University Of Fribourg, Fribourg, Switzerland"]]}, {"id": "1703.04347", "submitter": "Anjany Kumar Sekuboyina", "authors": "Anjany Sekuboyina, Alexander Valentinitsch, Jan S. Kirschke, and\n  Bjoern H. Menze", "title": "A Localisation-Segmentation Approach for Multi-label Annotation of\n  Lumbar Vertebrae using Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class segmentation of vertebrae is a non-trivial task mainly due to the\nhigh correlation in the appearance of adjacent vertebrae. Hence, such a task\ncalls for the consideration of both global and local context. Based on this\nmotivation, we propose a two-staged approach that, given a computed tomography\ndataset of the spine, segments the five lumbar vertebrae and simultaneously\nlabels them. The first stage employs a multi-layered perceptron performing\nnon-linear regression for locating the lumbar region using the global context.\nThe second stage, comprised of a fully-convolutional deep network, exploits the\nlocal context in the localised lumbar region to segment and label the lumbar\nvertebrae in one go. Aided with practical data augmentation for training, our\napproach is highly generalisable, capable of successfully segmenting both\nhealthy and abnormal vertebrae (fractured and scoliotic spines). We\nconsistently achieve an average Dice coefficient of over 90 percent on a\npublicly available dataset of the xVertSeg segmentation challenge of MICCAI\n2016. This is particularly noteworthy because the xVertSeg dataset is beset\nwith severe deformities in the form of vertebral fractures and scoliosis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:55:16 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["Valentinitsch", "Alexander", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1703.04363", "submitter": "Michael Gygli", "authors": "Michael Gygli, Mohammad Norouzi, Anelia Angelova", "title": "Deep Value Networks Learn to Evaluate and Iteratively Refine Structured\n  Outputs", "comments": "Published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach structured output prediction by optimizing a deep value network\n(DVN) to precisely estimate the task loss on different output configurations\nfor a given input. Once the model is trained, we perform inference by gradient\ndescent on the continuous relaxations of the output variables to find outputs\nwith promising scores from the value network. When applied to image\nsegmentation, the value network takes an image and a segmentation mask as\ninputs and predicts a scalar estimating the intersection over union between the\ninput and ground truth masks. For multi-label classification, the DVN's\nobjective is to correctly predict the F1 score for any potential label\nconfiguration. The DVN framework achieves the state-of-the-art results on\nmulti-label prediction and image segmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 12:49:20 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 08:10:34 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Gygli", "Michael", ""], ["Norouzi", "Mohammad", ""], ["Angelova", "Anelia", ""]]}, {"id": "1703.04364", "submitter": "S M Jaisakthi", "authors": "P. Mirunalini, Aravindan Chandrabose, Vignesh Gokul, S. M. Jaisakthi", "title": "Deep Learning for Skin Lesion Classification", "comments": "3 pages with 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma, a malignant form of skin cancer is very threatening to life.\nDiagnosis of melanoma at an earlier stage is highly needed as it has a very\nhigh cure rate. Benign and malignant forms of skin cancer can be detected by\nanalyzing the lesions present on the surface of the skin using dermoscopic\nimages. In this work, an automated skin lesion detection system has been\ndeveloped which learns the representation of the image using Google's\npretrained CNN model known as Inception-v3 \\cite{cnn}. After obtaining the\nrepresentation vector for our input dermoscopic images we have trained two\nlayer feed forward neural network to classify the images as malignant or\nbenign. The system also classifies the images based on the cause of the cancer\neither due to melanocytic or non-melanocytic cells using a different neural\nnetwork. These classification tasks are part of the challenge organized by\nInternational Skin Imaging Collaboration (ISIC) 2017. Our system learns to\nclassify the images based on the model built using the training images given in\nthe challenge and the experimental results were evaluated using validation and\ntest sets. Our system has achieved an overall accuracy of 65.8\\% for the\nvalidation set.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 12:56:00 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Mirunalini", "P.", ""], ["Chandrabose", "Aravindan", ""], ["Gokul", "Vignesh", ""], ["Jaisakthi", "S. M.", ""]]}, {"id": "1703.04391", "submitter": "Qing Hai Liao", "authors": "Qinghai Liao, Ming Liu, Lei Tai, Haoyang Ye", "title": "Extrinsic Calibration of 3D Range Finder and Camera without Auxiliary\n  Object or Human Intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion of heterogeneous extroceptive sensors is the most effient and\neffective way to representing the environment precisely, as it overcomes\nvarious defects of each homogeneous sensor. The rigid transformation (aka.\nextrinsic parameters) of heterogeneous sensory systems should be available\nbefore precisely fusing the multisensor information. Researchers have proposed\nseveral approaches to estimating the extrinsic parameters. These approaches\nrequire either auxiliary objects, like chessboards, or extra help from human to\nselect correspondences. In this paper, we proposed a novel extrinsic\ncalibration approach for the extrinsic calibration of range and image sensors.\nAs far as we know, it is the first automatic approach with no requirement of\nauxiliary objects or any human interventions. First, we estimate the initial\nextrinsic parameters from the individual motion of the range finder and the\ncamera. Then we extract lines in the image and point-cloud pairs, to refine the\nline feature associations by the initial extrinsic parameters. At the end, we\ndiscussed the degenerate case which may lead to the algorithm failure and\nvalidate our approach by simulation. The results indicate high-precision\nextrinsic calibration results against the ground-truth.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 08:49:23 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Liao", "Qinghai", ""], ["Liu", "Ming", ""], ["Tai", "Lei", ""], ["Ye", "Haoyang", ""]]}, {"id": "1703.04392", "submitter": "Mikhail Kats", "authors": "Bradley S. Gundlach, Michel Frising, Alireza Shahsafi, Gregory\n  Vershbow, Chenghao Wan, Jad Salman, Bas Rokers, Laurent Lessard, Mikhail A.\n  Kats", "title": "Enhancing human color vision by breaking binocular redundancy", "comments": "Main text and supplementary info", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To see color, the human visual system combines the response of three types of\ncone cells in the retina--a compressive process that discards a significant\namount of spectral information. Here, we present an approach to enhance human\ncolor vision by breaking its inherent binocular redundancy, providing different\nspectral content to each eye. We fabricated a set of optical filters that\n\"splits\" the response of the short-wavelength cone between the two eyes in\nindividuals with typical trichromatic vision, simulating the presence of\napproximately four distinct cone types (\"tetrachromacy\"). Such an increase in\nthe number of effective cone types can reduce the prevalence of metamers--pairs\nof distinct spectra that resolve to the same tristimulus values. This technique\nmay result in an enhancement of spectral perception, with applications ranging\nfrom camouflage detection and anti-counterfeiting to new types of artwork and\ndata visualization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 22:48:49 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 20:05:09 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 19:01:32 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gundlach", "Bradley S.", ""], ["Frising", "Michel", ""], ["Shahsafi", "Alireza", ""], ["Vershbow", "Gregory", ""], ["Wan", "Chenghao", ""], ["Salman", "Jad", ""], ["Rokers", "Bas", ""], ["Lessard", "Laurent", ""], ["Kats", "Mikhail A.", ""]]}, {"id": "1703.04393", "submitter": "D. Trinca", "authors": "D. Trinca and Y. Zhong", "title": "Randomized Iterative Reconstruction for Sparse View X-ray Computed\n  Tomography", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of more powerful computers, iterative reconstruction\nalgorithms are the subject of an ongoing work in the design of more efficient\nreconstruction algorithms for X-ray computed tomography. In this work, we show\nhow two analytical reconstruction algorithms can be improved by correcting the\ncorresponding reconstructions using a randomized iterative reconstruction\nalgorithm. The combined analytical reconstruction followed by randomized\niterative reconstruction can also be viewed as a reconstruction algorithm\nwhich, in the experiments we have conducted, uses up to $35\\%$ less projection\nangles as compared to the analytical reconstruction algorithms and produces the\nsame results in terms of quality of reconstruction, without increasing the\nexecution time significantly.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 20:24:13 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Trinca", "D.", ""], ["Zhong", "Y.", ""]]}, {"id": "1703.04394", "submitter": "Yongqin Xian", "authors": "Yongqin Xian, Bernt Schiele, Zeynep Akata", "title": "Zero-Shot Learning -- The Good, the Bad and the Ugly", "comments": "Accepted as a full paper in IEEE Computer Vision and Pattern\n  Recognition (CVPR) 2017. We introduce Proposed Split Version 2.0 (Please\n  download it from the project page)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the importance of zero-shot learning, the number of proposed\napproaches has increased steadily recently. We argue that it is time to take a\nstep back and to analyze the status quo of the area. The purpose of this paper\nis three-fold. First, given the fact that there is no agreed upon zero-shot\nlearning benchmark, we first define a new benchmark by unifying both the\nevaluation protocols and data splits. This is an important contribution as\npublished results are often not comparable and sometimes even flawed due to,\ne.g. pre-training on zero-shot test classes. Second, we compare and analyze a\nsignificant number of the state-of-the-art methods in depth, both in the\nclassic zero-shot setting but also in the more realistic generalized zero-shot\nsetting. Finally, we discuss limitations of the current status of the area\nwhich can be taken as a basis for advancing it.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 13:53:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 14:55:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Xian", "Yongqin", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""]]}, {"id": "1703.04416", "submitter": "Robert Obryk", "authors": "Jyrki Alakuijala, Robert Obryk, Zoltan Szabadka, and Jan Wassenberg", "title": "Users prefer Guetzli JPEG over same-sized libjpeg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on pairwise comparisons by human raters of JPEG images from libjpeg\nand our new Guetzli encoder. Although both files are size-matched, 75% of\nratings are in favor of Guetzli. This implies the Butteraugli psychovisual\nimage similarity metric which guides Guetzli is reasonably close to human\nperception at high quality levels. We provide access to the raw ratings and\nsource images for further analysis and study.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:33:47 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Alakuijala", "Jyrki", ""], ["Obryk", "Robert", ""], ["Szabadka", "Zoltan", ""], ["Wassenberg", "Jan", ""]]}, {"id": "1703.04418", "submitter": "Odemir Bruno PhD", "authors": "Mariane B. Neiva, Patrick Guidotti, Odemir M. Bruno", "title": "Improving LBP and its variants using anisotropic diffusion", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to propose a new preprocessing step in\norder to improve local feature descriptors and texture classification.\nPreprocessing is implemented by using transformations which help highlight\nsalient features that play a significant role in texture recognition. We\nevaluate and compare four different competing methods: three different\nanisotropic diffusion methods including the classical anisotropic Perona-Malik\ndiffusion and two subsequent regularizations of it and the application of a\nGaussian kernel, which is the classical multiscale approach in texture\nanalysis. The combination of the transformed images and the original ones are\nanalyzed. The results show that the use of the preprocessing step does lead to\nimproved texture recognition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:36:21 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Neiva", "Mariane B.", ""], ["Guidotti", "Patrick", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1703.04421", "submitter": "Robert Obryk", "authors": "Jyrki Alakuijala, Robert Obryk, Ostap Stoliarchuk, Zoltan Szabadka,\n  Lode Vandevenne, and Jan Wassenberg", "title": "Guetzli: Perceptually Guided JPEG Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guetzli is a new JPEG encoder that aims to produce visually indistinguishable\nimages at a lower bit-rate than other common JPEG encoders. It optimizes both\nthe JPEG global quantization tables and the DCT coefficient values in each JPEG\nblock using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual\ndistance metric, as the source of feedback in its optimization process. We\nreach a 29-45% reduction in data size for a given perceptual distance,\naccording to Butteraugli, in comparison to other compressors we tried.\nGuetzli's computation is currently extremely slow, which limits its\napplicability to compressing static content and serving as a proof- of-concept\nthat we can achieve significant reductions in size by combining advanced\npsychovisual models with lossy compression techniques.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:40:08 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Alakuijala", "Jyrki", ""], ["Obryk", "Robert", ""], ["Stoliarchuk", "Ostap", ""], ["Szabadka", "Zoltan", ""], ["Vandevenne", "Lode", ""], ["Wassenberg", "Jan", ""]]}, {"id": "1703.04446", "submitter": "Lars Ruthotto", "authors": "Andreas Mang and Lars Ruthotto", "title": "A Lagrangian Gauss-Newton-Krylov Solver for Mass- and\n  Intensity-Preserving Diffeomorphic Image Registration", "comments": "code available at:\n  https://github.com/C4IR/FAIR.m/tree/master/add-ons/LagLDDMM", "journal-ref": "SIAM J. Sci. Comput., 39(5), B860-B885, 2017", "doi": "10.1137/17M1114132", "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient solver for diffeomorphic image registration problems\nin the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM).\nWe use an optimal control formulation, in which the velocity field of a\nhyperbolic PDE needs to be found such that the distance between the final state\nof the system (the transformed/transported template image) and the observation\n(the reference image) is minimized. Our solver supports both stationary and\nnon-stationary (i.e., transient or time-dependent) velocity fields. As\ntransformation models, we consider both the transport equation (assuming\nintensities are preserved during the deformation) and the continuity equation\n(assuming mass-preservation).\n  We consider the reduced form of the optimal control problem and solve the\nresulting unconstrained optimization problem using a discretize-then-optimize\napproach. A key contribution is the elimination of the PDE constraint using a\nLagrangian hyperbolic PDE solver. Lagrangian methods rely on the concept of\ncharacteristic curves that we approximate here using a fourth-order Runge-Kutta\nmethod. We also present an efficient algorithm for computing the derivatives of\nfinal state of the system with respect to the velocity field. This allows us to\nuse fast Gauss-Newton based methods. We present quickly converging iterative\nlinear solvers using spectral preconditioners that render the overall\noptimization efficient and scalable. Our method is embedded into the image\nregistration framework FAIR and, thus, supports the most commonly used\nsimilarity measures and regularization functionals. We demonstrate the\npotential of our new approach using several synthetic and real world test\nproblems with up to 14.7 million degrees of freedom.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 15:29:32 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 11:04:29 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Mang", "Andreas", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1703.04454", "submitter": "Chao Zhang", "authors": "Chao Zhang, Sergi Pujades, Michael Black, and Gerard Pons-Moll", "title": "Detailed, accurate, human shape estimation from clothed 3D scan\n  sequences", "comments": "CVPR 2017, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 15:41:36 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 12:26:27 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Zhang", "Chao", ""], ["Pujades", "Sergi", ""], ["Black", "Michael", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1703.04559", "submitter": "Jeremy Kawahara", "authors": "Jeremy Kawahara, Ghassan Hamarneh", "title": "Fully Convolutional Neural Networks to Detect Clinical Dermoscopic\n  Features", "comments": "Accepted JBHI version", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, vol. 23, no. 2,\n  pp. 578-585, 2019", "doi": "10.1109/JBHI.2018.2831680", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of certain clinical dermoscopic features within a skin lesion\nmay indicate melanoma, and automatically detecting these features may lead to\nmore quantitative and reproducible diagnoses. We reformulate the task of\nclassifying clinical dermoscopic features within superpixels as a segmentation\nproblem, and propose a fully convolutional neural network to detect clinical\ndermoscopic features from dermoscopy skin lesion images. Our neural network\narchitecture uses interpolated feature maps from several intermediate network\nlayers, and addresses imbalanced labels by minimizing a negative multi-label\nDice-F$_1$ score, where the score is computed across the mini-batch for each\nlabel. Our approach ranked first place in the 2017 ISIC-ISBI Part 2:\nDermoscopic Feature Classification Task challenge over both the provided\nvalidation and test datasets, achieving a 0.895% area under the receiver\noperator characteristic curve score. We show how simple baseline models can\noutrank state-of-the-art approaches when using the official metrics of the\nchallenge, and propose to use a fuzzy Jaccard Index that ignores the empty set\n(i.e., masks devoid of positive pixels) when ranking models. Our results\nsuggest that (i) the classification of clinical dermoscopic features can be\neffectively approached as a segmentation problem, and (ii) the current metrics\nused to rank models may not well capture the efficacy of the model. We plan to\nmake our trained model and code publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 00:54:18 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 03:04:08 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Kawahara", "Jeremy", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1703.04590", "submitter": "Ashton Fagg", "authors": "Hamed Kiani Galoogahi, Ashton Fagg, Simon Lucey", "title": "Learning Background-Aware Correlation Filters for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Filters (CFs) have recently demonstrated excellent performance in\nterms of rapidly tracking objects under challenging photometric and geometric\nvariations. The strength of the approach comes from its ability to efficiently\nlearn - \"on the fly\" - how the object is changing over time. A fundamental\ndrawback to CFs, however, is that the background of the object is not be\nmodelled over time which can result in suboptimal results. In this paper we\npropose a Background-Aware CF that can model how both the foreground and\nbackground of the object varies over time. Our approach, like conventional CFs,\nis extremely computationally efficient - and extensive experiments over\nmultiple tracking benchmarks demonstrate the superior accuracy and real-time\nperformance of our method compared to the state-of-the-art trackers including\nthose based on a deep learning paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 17:16:23 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 22:48:46 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Galoogahi", "Hamed Kiani", ""], ["Fagg", "Ashton", ""], ["Lucey", "Simon", ""]]}, {"id": "1703.04611", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, and Yao Wang", "title": "Subspace Learning in The Presence of Sparse Structured Outliers and\n  Noise", "comments": "IEEE International Symposium on Circuits and Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace learning is an important problem, which has many applications in\nimage and video processing. It can be used to find a low-dimensional\nrepresentation of signals and images. But in many applications, the desired\nsignal is heavily distorted by outliers and noise, which negatively affect the\nlearned subspace. In this work, we present a novel algorithm for learning a\nsubspace for signal representation, in the presence of structured outliers and\nnoise. The proposed algorithm tries to jointly detect the outliers and learn\nthe subspace for images. We present an alternating optimization algorithm for\nsolving this problem, which iterates between learning the subspace and finding\nthe outliers. This algorithm has been trained on a large number of image\npatches, and the learned subspace is used for image segmentation, and is shown\nto achieve better segmentation results than prior methods, including least\nabsolute deviation fitting, k-means clustering based segmentation in DjVu, and\nshape primitive extraction and coding algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 17:37:53 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 20:42:30 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 02:14:53 GMT"}, {"version": "v4", "created": "Wed, 12 Jul 2017 12:53:47 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1703.04615", "submitter": "Giovanni Poggi", "authors": "Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva", "title": "Recasting Residual-based Local Descriptors as Convolutional Neural\n  Networks: an Application to Image Forgery Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local descriptors based on the image noise residual have proven extremely\neffective for a number of forensic applications, like forgery detection and\nlocalization. Nonetheless, motivated by promising results in computer vision,\nthe focus of the research community is now shifting on deep learning. In this\npaper we show that a class of residual-based descriptors can be actually\nregarded as a simple constrained convolutional neural network (CNN). Then, by\nrelaxing the constraints, and fine-tuning the net on a relatively small\ntraining set, we obtain a significant performance improvement with respect to\nthe conventional detector.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 17:42:13 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cozzolino", "Davide", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1703.04636", "submitter": "Giovanni Poggi", "authors": "Luca D'Amiano, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva", "title": "A PatchMatch-based Dense-field Algorithm for Video Copy-Move Detection\n  and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for the reliable detection and localization of\nvideo copy-move forgeries. Discovering well crafted video copy-moves may be\nvery difficult, especially when some uniform background is copied to occlude\nforeground objects. To reliably detect both additive and occlusive copy-moves\nwe use a dense-field approach, with invariant features that guarantee\nrobustness to several post-processing operations. To limit complexity, a\nsuitable video-oriented version of PatchMatch is used, with a multiresolution\nsearch strategy, and a focus on volumes of interest. Performance assessment\nrelies on a new dataset, designed ad hoc, with realistic copy-moves and a wide\nvariety of challenging situations. Experimental results show the proposed\nmethod to detect and localize video copy-moves with good accuracy even in\nadverse conditions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:08:49 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["D'Amiano", "Luca", ""], ["Cozzolino", "Davide", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1703.04653", "submitter": "G.M. Dilshan Godaliyadda", "authors": "G. M. Dilshan P. Godaliyadda, Dong Hye Ye, Michael D.Uchic, Michael A.\n  Groeber, Gregery T. Buzzard, Charles A. Bouman", "title": "A Framework for Dynamic Image Sampling Based on Supervised Learning\n  (SLADS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse sampling schemes have the potential to dramatically reduce image\nacquisition time while simultaneously reducing radiation damage to samples.\nHowever, for a sparse sampling scheme to be useful it is important that we are\nable to reconstruct the underlying object with sufficient clarity using the\nsparse measurements. In dynamic sampling, each new measurement location is\nselected based on information obtained from previous measurements. Therefore,\ndynamic sampling schemes have the potential to dramatically reduce the number\nof measurements needed for high fidelity reconstructions. However, most\nexisting dynamic sampling methods for point-wise measurement acquisition tend\nto be computationally expensive and are therefore too slow for practical\napplications.\n  In this paper, we present a framework for dynamic sampling based on machine\nlearning techniques, which we call a supervised learning approach for dynamic\nsampling (SLADS). In each step of SLADS, the objective is to find the pixel\nthat maximizes the expected reduction in distortion (ERD) given previous\nmeasurements. SLADS is fast because we use a simple regression function to\ncompute the ERD, and it is accurate because the regression function is trained\nusing data sets that are representative of the specific application. In\naddition, we introduce a method to terminate dynamic sampling at a desired\nlevel of distortion, and we extended the SLADS methodology to sample groups of\npixels at each step. Finally, we present results on computationally-generated\nsynthetic data and experimentally-collected data to demonstrate a dramatic\nimprovement over state-of-the-art static sampling methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:36:16 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Godaliyadda", "G. M. Dilshan P.", ""], ["Ye", "Dong Hye", ""], ["Uchic", "Michael D.", ""], ["Groeber", "Michael A.", ""], ["Buzzard", "Gregery T.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1703.04665", "submitter": "Alexander Broad S", "authors": "Alexander Broad and Brenna Argall", "title": "Geometry-Based Region Proposals for Real-Time Robot Detection of\n  Tabletop Objects", "comments": "Update based on work presented at RSS 2016 Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel object detection pipeline for localization and recognition\nin three dimensional environments. Our approach makes use of an RGB-D sensor\nand combines state-of-the-art techniques from the robotics and computer vision\ncommunities to create a robust, real-time detection system. We focus\nspecifically on solving the object detection problem for tabletop scenes, a\ncommon environment for assistive manipulators. Our detection pipeline locates\nobjects in a point cloud representation of the scene. These clusters are\nsubsequently used to compute a bounding box around each object in the RGB\nspace. Each defined patch is then fed into a Convolutional Neural Network (CNN)\nfor object recognition. We also demonstrate that our region proposal method can\nbe used to develop novel datasets that are both large and diverse enough to\ntrain deep learning models, and easy enough to collect that end-users can\ndevelop their own datasets. Lastly, we validate the resulting system through an\nextensive analysis of the accuracy and run-time of the full pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:51:18 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Broad", "Alexander", ""], ["Argall", "Brenna", ""]]}, {"id": "1703.04670", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos G. Derpanis,\n  Kostas Daniilidis", "title": "6-DoF Object Pose from Semantic Keypoints", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to estimating the continuous six degree\nof freedom (6-DoF) pose (3D translation and rotation) of an object from a\nsingle RGB image. The approach combines semantic keypoints predicted by a\nconvolutional network (convnet) with a deformable shape model. Unlike prior\nwork, we are agnostic to whether the object is textured or textureless, as the\nconvnet learns the optimal representation from the available training image\ndata. Furthermore, the approach can be applied to instance- and class-based\npose recovery. Empirically, we show that the proposed approach can accurately\nrecover the 6-DoF object pose for both instance- and class-based scenarios with\na cluttered background. For class-based object pose estimation,\nstate-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:58:46 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Zhou", "Xiaowei", ""], ["Chan", "Aaron", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1703.04699", "submitter": "Cheng Zhao", "authors": "Cheng Zhao, Li Sun and Rustam Stolkin", "title": "A fully end-to-end deep learning approach for real-time simultaneous 3D\n  reconstruction and material recognition", "comments": "8 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1109/ICAR.2017.8023499", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of simultaneous 3D reconstruction and\nmaterial recognition and segmentation. Enabling robots to recognise different\nmaterials (concrete, metal etc.) in a scene is important for many tasks, e.g.\nrobotic interventions in nuclear decommissioning. Previous work on 3D semantic\nreconstruction has predominantly focused on recognition of everyday domestic\nobjects (tables, chairs etc.), whereas previous work on material recognition\nhas largely been confined to single 2D images without any 3D reconstruction.\nMeanwhile, most 3D semantic reconstruction methods rely on computationally\nexpensive post-processing, using Fully-Connected Conditional Random Fields\n(CRFs), to achieve consistent segmentations. In contrast, we propose a deep\nlearning method which performs 3D reconstruction while simultaneously\nrecognising different types of materials and labelling them at the pixel level.\nUnlike previous methods, we propose a fully end-to-end approach, which does not\nrequire hand-crafted features or CRF post-processing. Instead, we use only\nlearned features, and the CRF segmentation constraints are incorporated inside\nthe fully end-to-end learned system. We present the results of experiments, in\nwhich we trained our system to perform real-time 3D semantic reconstruction for\n23 different materials in a real-world application. The run-time performance of\nthe system can be boosted to around 10Hz, using a conventional GPU, which is\nenough to achieve real-time semantic reconstruction using a 30fps RGB-D camera.\nTo the best of our knowledge, this work is the first real-time end-to-end\nsystem for simultaneous 3D reconstruction and material recognition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 20:23:48 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zhao", "Cheng", ""], ["Sun", "Li", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1703.04706", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Aaron McFadyen, Sridha Sridharan and\n  Clinton Fookes", "title": "Tree Memory Networks for Modelling Long-term Temporal Dependencies", "comments": null, "journal-ref": "Neurocomputing, Volume 304, 23 August 2018, Pages 64-81", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of sequence modelling, Recurrent Neural Networks (RNN) have\nbeen capable of achieving impressive results in a variety of application areas\nincluding visual question answering, part-of-speech tagging and machine\ntranslation. However this success in modelling short term dependencies has not\nsuccessfully transitioned to application areas such as trajectory prediction,\nwhich require capturing both short term and long term relationships. In this\npaper, we propose a Tree Memory Network (TMN) for modelling long term and short\nterm relationships in sequence-to-sequence mapping problems. The proposed\nnetwork architecture is composed of an input module, controller and a memory\nmodule. In contrast to related literature, which models the memory as a\nsequence of historical states, we model the memory as a recursive tree\nstructure. This structure more effectively captures temporal dependencies\nacross both short term and long term sequences using its hierarchical\nstructure. We demonstrate the effectiveness and flexibility of the proposed TMN\nin two practical problems, aircraft trajectory modelling and pedestrian\ntrajectory modelling in a surveillance setting, and in both cases we outperform\nthe current state-of-the-art. Furthermore, we perform an in depth analysis on\nthe evolution of the memory module content over time and provide visual\nevidence on how the proposed TMN is able to map both long term and short term\nrelationships efficiently via a hierarchical structure.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 21:13:28 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 05:18:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["McFadyen", "Aaron", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1703.04727", "submitter": "Radu Horaud P", "authors": "Beno\\^it Mass\\'e, Sil\\`eye Ba and Radu Horaud", "title": "Tracking Gaze and Visual Focus of Attention of People Involved in Social\n  Interaction", "comments": "15 pages, 8 figures, 6 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  40(11), 2711 - 2724, 2018", "doi": "10.1109/TPAMI.2017.2782819", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual focus of attention (VFOA) has been recognized as a prominent\nconversational cue. We are interested in estimating and tracking the VFOAs\nassociated with multi-party social interactions. We note that in this type of\nsituations the participants either look at each other or at an object of\ninterest; therefore their eyes are not always visible. Consequently both gaze\nand VFOA estimation cannot be based on eye detection and tracking. We propose a\nmethod that exploits the correlation between eye gaze and head movements. Both\nVFOA and gaze are modeled as latent variables in a Bayesian switching\nstate-space model. The proposed formulation leads to a tractable learning\nprocedure and to an efficient algorithm that simultaneously tracks gaze and\nvisual focus. The method is tested and benchmarked using two publicly available\ndatasets that contain typical multi-party human-robot and human-human\ninteractions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 21:06:08 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 16:05:15 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Mass\u00e9", "Beno\u00eet", ""], ["Ba", "Sil\u00e8ye", ""], ["Horaud", "Radu", ""]]}, {"id": "1703.04775", "submitter": "Andrea Tacchetti", "authors": "Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos", "title": "Discriminate-and-Rectify Encoders: Learning from Image Transformation\n  Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of a learning task is increased by transformations in the\ninput space that preserve class identity. Visual object recognition for example\nis affected by changes in viewpoint, scale, illumination or planar\ntransformations. While drastically altering the visual appearance, these\nchanges are orthogonal to recognition and should not be reflected in the\nrepresentation or feature encoding used for learning. We introduce a framework\nfor weakly supervised learning of image embeddings that are robust to\ntransformations and selective to the class distribution, using sets of\ntransforming examples (orbit sets), deep parametrizations and a novel\norbit-based loss. The proposed loss combines a discriminative, contrastive part\nfor orbits with a reconstruction error that learns to rectify orbit\ntransformations. The learned embeddings are evaluated in distance metric-based\ntasks, such as one-shot classification under geometric transformations, as well\nas face verification and retrieval under more realistic visual variability. Our\nresults suggest that orbit sets, suitably computed or observed, can be used for\nefficient, weakly-supervised learning of semantically relevant image\nembeddings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:21:48 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Voinea", "Stephen", ""], ["Evangelopoulos", "Georgios", ""]]}, {"id": "1703.04819", "submitter": "Sandra Avila", "authors": "Afonso Menegola, Julia Tavares, Michel Fornaciali, Lin Tzy Li, Sandra\n  Avila, Eduardo Valle", "title": "RECOD Titans at ISIC Challenge 2017", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract describes the participation of RECOD Titans in parts 1\nand 3 of the ISIC Challenge 2017 \"Skin Lesion Analysis Towards Melanoma\nDetection\" (ISBI 2017). Although our team has a long experience with melanoma\nclassification, the ISIC Challenge 2017 was the very first time we worked on\nskin-lesion segmentation. For part 1 (segmentation), our final submission used\nfour of our models: two trained with all 2000 samples, without a validation\nsplit, for 250 and for 500 epochs respectively; and other two trained and\nvalidated with two different 1600/400 splits, for 220 epochs. Those four\nmodels, individually, achieved between 0.780 and 0.783 official validation\nscores. Our final submission averaged the output of those four models achieved\na score of 0.793. For part 3 (classification), the submitted test run as well\nas our last official validation run were the result from a meta-model that\nassembled seven base deep-learning models: three based on Inception-V4 trained\non our largest dataset; three based on Inception trained on our smallest\ndataset; and one based on ResNet-101 trained on our smaller dataset. The\nresults of those component models were stacked in a meta-learning layer based\non an SVM trained on the validation set of our largest dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:11:04 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Menegola", "Afonso", ""], ["Tavares", "Julia", ""], ["Fornaciali", "Michel", ""], ["Li", "Lin Tzy", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1703.04824", "submitter": "Jan Haji\\v{c} Jr", "authors": "Jan Haji\\v{c} jr., Pavel Pecina", "title": "In Search of a Dataset for Handwritten Optical Music Recognition:\n  Introducing MUSCIMA++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Optical Music Recognition (OMR) has long been without an adequate dataset and\nground truth for evaluating OMR systems, which has been a major problem for\nestablishing a state of the art in the field. Furthermore, machine learning\nmethods require training data. We analyze how the OMR processing pipeline can\nbe expressed in terms of gradually more complex ground truth, and based on this\nanalysis, we design the MUSCIMA++ dataset of handwritten music notation that\naddresses musical symbol recognition and notation reconstruction. The MUSCIMA++\ndataset version 0.9 consists of 140 pages of handwritten music, with 91255\nmanually annotated notation symbols and 82261 explicitly marked relationships\nbetween symbol pairs. The dataset allows training and evaluating models for\nsymbol classification, symbol localization, and notation graph assembly, both\nin isolation and jointly. Open-source tools are provided for manipulating the\ndataset, visualizing the data and further annotation, and the dataset itself is\nmade available under an open license.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:21:26 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Haji\u010d", "Jan", "jr."], ["Pecina", "Pavel", ""]]}, {"id": "1703.04835", "submitter": "Wei-An Lin", "authors": "Wei-An Lin and Jun-Cheng Chen and Rama Chellappa", "title": "A Proximity-Aware Hierarchical Clustering of Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unsupervised face clustering algorithm called\n\"Proximity-Aware Hierarchical Clustering\" (PAHC) that exploits the local\nstructure of deep representations. In the proposed method, a similarity measure\nbetween deep features is computed by evaluating linear SVM margins. SVMs are\ntrained using nearest neighbors of sample data, and thus do not require any\nexternal training data. Clusters are then formed by thresholding the similarity\nscores. We evaluate the clustering performance using three challenging\nunconstrained face datasets, including Celebrity in Frontal-Profile (CFP),\nIARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)\ndatasets. Experimental results demonstrate that the proposed approach can\nachieve significant improvements over state-of-the-art methods. Moreover, we\nalso show that the proposed clustering algorithm can be applied to curate a set\nof large-scale and noisy training dataset while maintaining sufficient amount\nof images and their variations due to nuisance factors. The face verification\nperformance on JANUS CS3 improves significantly by finetuning a DCNN model with\nthe curated MS-Celeb-1M dataset which contains over three million face images.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:41:45 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Lin", "Wei-An", ""], ["Chen", "Jun-Cheng", ""], ["Chellappa", "Rama", ""]]}, {"id": "1703.04845", "submitter": "Juana M. Gutierrez-Arriola", "authors": "Juana M. Guti\\'errez-Arriola, Marta G\\'omez-\\'Alvarez, Victor\n  Osma-Ruiz, Nicol\\'as S\\'aenz-Lech\\'on, Rub\\'en Fraile", "title": "Skin lesion segmentation based on preprocessing, thresholding and neural\n  networks", "comments": "4 pages, 4 figures, abstract submitted to participate in the\n  challenge ISIC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract describes the segmentation system used to participate in the\nchallenge ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection. Several\npreprocessing techniques have been tested for three color representations (RGB,\nYCbCr and HSV) of 392 images. Results have been used to choose the better\npreprocessing for each channel. In each case a neural network is trained to\npredict the Jaccard Index based on object characteristics. The system includes\nblack frames and reference circle detection algorithms but no special treatment\nis done for hair removal. Segmentation is performed in two steps first the best\nchannel to be segmented is chosen by selecting the best neural network output.\nIf this output does not predict a Jaccard Index over 0.5 a more aggressive\npreprocessing is performed using open and close morphological operations and\nthe segmentation of the channel that obtains the best output from the neural\nnetworks is selected as the lesion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 00:15:01 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Guti\u00e9rrez-Arriola", "Juana M.", ""], ["G\u00f3mez-\u00c1lvarez", "Marta", ""], ["Osma-Ruiz", "Victor", ""], ["S\u00e1enz-Lech\u00f3n", "Nicol\u00e1s", ""], ["Fraile", "Rub\u00e9n", ""]]}, {"id": "1703.04853", "submitter": "Homa Foroughi", "authors": "Homa Foroughi, Moein Shakeri, Nilanjan Ray, Hong Zhang", "title": "Face Recognition using Multi-Modal Low-Rank Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been widely studied due to its importance in different\napplications; however, most of the proposed methods fail when face images are\noccluded or captured under illumination and pose variations. Recently several\nlow-rank dictionary learning methods have been proposed and achieved promising\nresults for noisy observations. While these methods are mostly developed for\nsingle-modality scenarios, recent studies demonstrated the advantages of\nfeature fusion from multiple inputs. We propose a multi-modal structured\nlow-rank dictionary learning method for robust face recognition, using raw\npixels of face images and their illumination invariant representation. The\nproposed method learns robust and discriminative representations from\ncontaminated face images, even if there are few training samples with large\nintra-class variations. Extensive experiments on different datasets validate\nthe superior performance and robustness of our method to severe illumination\nvariations and occlusion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 00:38:01 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Foroughi", "Homa", ""], ["Shakeri", "Moein", ""], ["Ray", "Nilanjan", ""], ["Zhang", "Hong", ""]]}, {"id": "1703.04856", "submitter": "Pengpeng Yang", "authors": "Pengpeng Yang, Wei Zhao, Rongrong Ni, and Yao Zhao", "title": "Source Camera Identification Based On Content-Adaptive Fusion Network", "comments": "This article has been submitted to the 2017 IEEE International\n  Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source camera identification is still a hard task in forensics community,\nespecially for the case of the small query image size. In this paper, we\npropose a solution to identify the source camera of the small-size images:\ncontent-adaptive fusion network. In order to learn better feature\nrepresentation from the input data, content-adaptive convolutional neural\nnetworks(CA-CNN) are constructed. We add a convolutional layer in preprocessing\nstage. Moreover, with the purpose of capturing more comprehensive information,\nwe parallel three CA-CNNs: CA3-CNN, CA5-CNN, CA7-CNN to get the\ncontent-adaptive fusion network. The difference of three CA-CNNs lies in the\nconvolutional kernel size of pre-processing layer. The experimental results\nshow that the proposed method is practicable and satisfactory.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 00:52:50 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Yang", "Pengpeng", ""], ["Zhao", "Wei", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""]]}, {"id": "1703.04861", "submitter": "Kun Li", "authors": "Kun Li, Jingyu Yang, Yu-Kun Lai, Daoliang Guo", "title": "Robust Non-Rigid Registration with Reweighted Position and\n  Transformation Sparsity", "comments": "IEEE Transactions on Visualization and Computer Graphics", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics ( Volume:\n  25 , Issue: 6 , June 1 2019 )", "doi": "10.1109/TVCG.2018.2832136", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid registration is challenging because it is ill-posed with high\ndegrees of freedom and is thus sensitive to noise and outliers. We propose a\nrobust non-rigid registration method using reweighted sparsities on position\nand transformation to estimate the deformations between 3-D shapes. We\nformulate the energy function with position and transformation sparsity on both\nthe data term and the smoothness term, and define the smoothness constraint\nusing local rigidity. The double sparsity based non-rigid registration model is\nenhanced with a reweighting scheme, and solved by transferring the model into\nfour alternately-optimized subproblems which have exact solutions and\nguaranteed convergence. Experimental results on both public datasets and real\nscanned datasets show that our method outperforms the state-of-the-art methods\nand is more robust to noise and outliers than conventional non-rigid\nregistration methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:00:44 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:21:05 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Li", "Kun", ""], ["Yang", "Jingyu", ""], ["Lai", "Yu-Kun", ""], ["Guo", "Daoliang", ""]]}, {"id": "1703.04877", "submitter": "Mengmeng Wang", "authors": "Mengmeng Wang, Daobilige Su, Lei Shi, Yong Liu and Jaime Valls Miro", "title": "Real-time 3D Human Tracking for Mobile Robots with Multisensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring the accurate 3-D position of a target person around a robot\nprovides fundamental and valuable information that is applicable to a wide\nrange of robotic tasks, including home service, navigation and entertainment.\nThis paper presents a real-time robotic 3-D human tracking system which\ncombines a monocular camera with an ultrasonic sensor by the extended Kalman\nfilter (EKF). The proposed system consists of three sub-modules: monocular\ncamera sensor tracking model, ultrasonic sensor tracking model and multi-sensor\nfusion. An improved visual tracking algorithm is presented to provide partial\nlocation estimation (2-D). The algorithm is designed to overcome severe\nocclusions, scale variation, target missing and achieve robust re-detection.\nThe scale accuracy is further enhanced by the estimated 3-D information. An\nultrasonic sensor array is employed to provide the range information from the\ntarget person to the robot and Gaussian Process Regression is used for partial\nlocation estimation (2-D). EKF is adopted to sequentially process multiple,\nheterogeneous measurements arriving in an asynchronous order from the vision\nsensor and the ultrasonic sensor separately. In the experiments, the proposed\ntracking system is tested in both simulation platform and actual mobile robot\nfor various indoor and outdoor scenes. The experimental results show the\nsuperior performance of the 3-D tracking system in terms of both the accuracy\nand robustness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:48:30 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Wang", "Mengmeng", ""], ["Su", "Daobilige", ""], ["Shi", "Lei", ""], ["Liu", "Yong", ""], ["Miro", "Jaime Valls", ""]]}, {"id": "1703.04960", "submitter": "Liu Liu", "authors": "Liu Liu, Alireza Rahimpour, Ali Taalimi, Hairong Qi", "title": "End-to-end Binary Representation Learning via Direct Binary Embedding", "comments": "Accepted by ICIP'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning binary representation is essential to large-scale computer vision\ntasks. Most existing algorithms require a separate quantization constraint to\nlearn effective hashing functions. In this work, we present Direct Binary\nEmbedding (DBE), a simple yet very effective algorithm to learn binary\nrepresentation in an end-to-end fashion. By appending an ingeniously designed\nDBE layer to the deep convolutional neural network (DCNN), DBE learns binary\ncode directly from the continuous DBE layer activation without quantization\nerror. By employing the deep residual network (ResNet) as DCNN component, DBE\ncaptures rich semantics from images. Furthermore, in the effort of handling\nmultilabel images, we design a joint cross entropy loss that includes both\nsoftmax cross entropy and weighted binary cross entropy in consideration of the\ncorrelation and independence of labels, respectively. Extensive experiments\ndemonstrate the significant superiority of DBE over state-of-the-art methods on\ntasks of natural object recognition, image retrieval and image annotation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:41:31 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 04:44:40 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Liu", "Liu", ""], ["Rahimpour", "Alireza", ""], ["Taalimi", "Ali", ""], ["Qi", "Hairong", ""]]}, {"id": "1703.04967", "submitter": "Holger Roth", "authors": "Mohammad Eshghi, Holger R. Roth, Masahiro Oda, Min Suk Chung, Kensaku\n  Mori", "title": "Comparison of the Deep-Learning-Based Automated Segmentation Methods for\n  the Head Sectioned Images of the Virtual Korean Human Project", "comments": "Accepted for presentation at the 15th IAPR Conference on Machine\n  Vision Applications (MVA2017), Nagoya, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end pixelwise fully automated segmentation of\nthe head sectioned images of the Visible Korean Human (VKH) project based on\nDeep Convolutional Neural Networks (DCNNs). By converting classification\nnetworks into Fully Convolutional Networks (FCNs), a coarse prediction map,\nwith smaller size than the original input image, can be created for\nsegmentation purposes. To refine this map and to obtain a dense pixel-wise\noutput, standard FCNs use deconvolution layers to upsample the coarse map.\nHowever, upsampling based on deconvolution increases the number of network\nparameters and causes loss of detail because of interpolation. On the other\nhand, dilated convolution is a new technique introduced recently that attempts\nto capture multi-scale contextual information without increasing the network\nparameters while keeping the resolution of the prediction maps high. We used\nboth a standard FCN and a dilated convolution based FCN for semantic\nsegmentation of the head sectioned images of the VKH dataset. Quantitative\nresults showed approximately 20% improvement in the segmentation accuracy when\nusing FCNs with dilated convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:49:01 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Eshghi", "Mohammad", ""], ["Roth", "Holger R.", ""], ["Oda", "Masahiro", ""], ["Chung", "Min Suk", ""], ["Mori", "Kensaku", ""]]}, {"id": "1703.04977", "submitter": "Alex Kendall", "authors": "Alex Kendall and Yarin Gal", "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer\n  Vision?", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major types of uncertainty one can model. Aleatoric uncertainty\ncaptures noise inherent in the observations. On the other hand, epistemic\nuncertainty accounts for uncertainty in the model -- uncertainty which can be\nexplained away given enough data. Traditionally it has been difficult to model\nepistemic uncertainty in computer vision, but with new Bayesian deep learning\ntools this is now possible. We study the benefits of modeling epistemic vs.\naleatoric uncertainty in Bayesian deep learning models for vision tasks. For\nthis we present a Bayesian deep learning framework combining input-dependent\naleatoric uncertainty together with epistemic uncertainty. We study models\nunder the framework with per-pixel semantic segmentation and depth regression\ntasks. Further, our explicit uncertainty formulation leads to new loss\nfunctions for these tasks, which can be interpreted as learned attenuation.\nThis makes the loss more robust to noisy data, also giving new state-of-the-art\nresults on segmentation and depth regression benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:27:12 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 13:04:51 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kendall", "Alex", ""], ["Gal", "Yarin", ""]]}, {"id": "1703.04980", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Lauge S{\\o}rensen and David M. J. Tax and\n  Jesper Holst Pedersen and Marco Loog and Marleen de Bruijne", "title": "Classification of COPD with Multiple Instance Learning", "comments": "Published at International Conference on Pattern Recognition (ICPR)\n  2014", "journal-ref": null, "doi": "10.1109/ICPR.2014.268", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic obstructive pulmonary disease (COPD) is a lung disease where early\ndetection benefits the survival rate. COPD can be quantified by classifying\npatches of computed tomography images, and combining patch labels into an\noverall diagnosis for the image. As labeled patches are often not available,\nimage labels are propagated to the patches, incorrectly labeling healthy\npatches in COPD patients as being affected by the disease. We approach\nquantification of COPD from lung images as a multiple instance learning (MIL)\nproblem, which is more suitable for such weakly labeled data. We investigate\nvarious MIL assumptions in the context of COPD and show that although a concept\nregion with COPD-related disease patterns is present, considering the whole\ndistribution of lung tissue patches improves the performance. The best method\nis based on averaging instances and obtains an AUC of 0.742, which is higher\nthan the previously reported best of 0.713 on the same dataset. Using the full\ntraining set further increases performance to 0.776, which is significantly\nhigher (DeLong test) than previous results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:41:49 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["S\u00f8rensen", "Lauge", ""], ["Tax", "David M. J.", ""], ["Pedersen", "Jesper Holst", ""], ["Loog", "Marco", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1703.04981", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Annegreet van Opbroek and M. Arfan Ikram and\n  Meike W. Vernooij and Marleen de Bruijne", "title": "Transfer Learning by Asymmetric Image Weighting for Segmentation across\n  Scanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning has been very successful for automatic segmentation of\nimages from a single scanner. However, several papers report deteriorated\nperformances when using classifiers trained on images from one scanner to\nsegment images from other scanners. We propose a transfer learning classifier\nthat adapts to differences between training and test images. This method uses a\nweighted ensemble of classifiers trained on individual images. The weight of\neach classifier is determined by the similarity between its training image and\nthe test image.\n  We examine three unsupervised similarity measures, which can be used in\nscenarios where no labeled data from a newly introduced scanner or scanning\nprotocol is available. The measures are based on a divergence, a bag distance,\nand on estimating the labels with a clustering procedure. These measures are\nasymmetric. We study whether the asymmetry can improve classification. Out of\nthe three similarity measures, the bag similarity measure is the most robust\nacross different studies and achieves excellent results on four brain tissue\nsegmentation datasets and three white matter lesion segmentation datasets,\nacquired at different centers and with different scanners and scanning\nprotocols. We show that the asymmetry can indeed be informative, and that\ncomputing the similarity from the test image to the training images is more\nappropriate than the opposite direction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:43:10 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["van Opbroek", "Annegreet", ""], ["Ikram", "M. Arfan", ""], ["Vernooij", "Meike W.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1703.04986", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Lauge S{\\o}rensen and David M. J. Tax and\n  Marleen de Bruijne and Marco Loog", "title": "Label Stability in Multiple Instance Learning", "comments": "Published at MICCAI 2015", "journal-ref": null, "doi": "10.1007/978-3-319-24553-9_66", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of \\emph{instance label stability} in multiple\ninstance learning (MIL) classifiers. These classifiers are trained only on\nglobally annotated images (bags), but often can provide fine-grained\nannotations for image pixels or patches (instances). This is interesting for\ncomputer aided diagnosis (CAD) and other medical image analysis tasks for which\nonly a coarse labeling is provided. Unfortunately, the instance labels may be\nunstable. This means that a slight change in training data could potentially\nlead to abnormalities being detected in different parts of the image, which is\nundesirable from a CAD point of view. Despite MIL gaining popularity in the CAD\nliterature, this issue has not yet been addressed. We investigate the stability\nof instance labels provided by several MIL classifiers on 5 different datasets,\nof which 3 are medical image datasets (breast histopathology, diabetic\nretinopathy and computed tomography lung images). We propose an unsupervised\nmeasure to evaluate instance stability, and demonstrate that a\nperformance-stability trade-off can be made when comparing MIL classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:46:18 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["S\u00f8rensen", "Lauge", ""], ["Tax", "David M. J.", ""], ["de Bruijne", "Marleen", ""], ["Loog", "Marco", ""]]}, {"id": "1703.05002", "submitter": "Donghui Wang", "authors": "Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang", "title": "Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths", "comments": "Accepted as a full paper in IEEE Computer Vision and Pattern\n  Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot recognition aims to accurately recognize objects of unseen classes\nby using a shared visual-semantic mapping between the image feature space and\nthe semantic embedding space. This mapping is learned on training data of seen\nclasses and is expected to have transfer ability to unseen classes. In this\npaper, we tackle this problem by exploiting the intrinsic relationship between\nthe semantic space manifold and the transfer ability of visual-semantic\nmapping. We formalize their connection and cast zero-shot recognition as a\njoint optimization problem. Motivated by this, we propose a novel framework for\nzero-shot recognition, which contains dual visual-semantic mapping paths. Our\nanalysis shows this framework can not only apply prior semantic knowledge to\ninfer underlying semantic manifold in the image feature space, but also\ngenerate optimized semantic embedding space, which can enhance the transfer\nability of the visual-semantic mapping to unseen classes. The proposed method\nis evaluated for zero-shot recognition on four benchmark datasets, achieving\noutstanding results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 08:28:58 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 01:46:27 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Li", "Yanan", ""], ["Wang", "Donghui", ""], ["Hu", "Huanhang", ""], ["Lin", "Yuetan", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1703.05020", "submitter": "Mengmeng Wang", "authors": "Mengmeng Wang, Yong Liu, Zeyi Huang", "title": "Large Margin Object Tracking with Circulant Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured output support vector machine (SVM) based tracking algorithms have\nshown favorable performance recently. Nonetheless, the time-consuming candidate\nsampling and complex optimization limit their real-time applications. In this\npaper, we propose a novel large margin object tracking method which absorbs the\nstrong discriminative ability from structured output SVM and speeds up by the\ncorrelation filter algorithm significantly. Secondly, a multimodal target\ndetection technique is proposed to improve the target localization precision\nand prevent model drift introduced by similar objects or background noise.\nThirdly, we exploit the feedback from high-confidence tracking results to avoid\nthe model corruption problem. We implement two versions of the proposed tracker\nwith the representations from both conventional hand-crafted and deep\nconvolution neural networks (CNNs) based features to validate the strong\ncompatibility of the algorithm. The experimental results demonstrate that the\nproposed tracker performs superiorly against several state-of-the-art\nalgorithms on the challenging benchmark sequences while runs at speed in excess\nof 80 frames per second. The source code and experimental results will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 09:02:11 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 02:56:31 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Wang", "Mengmeng", ""], ["Liu", "Yong", ""], ["Huang", "Zeyi", ""]]}, {"id": "1703.05061", "submitter": "Matthias Ochs", "authors": "Matthias Ochs, Henry Bradler and Rudolf Mester", "title": "Learning Rank Reduced Interpolation with Principal Component Analysis", "comments": "Accepted at Intelligent Vehicles Symposium (IV), Los Angeles, USA,\n  June 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision most iterative optimization algorithms, both sparse and\ndense, rely on a coarse and reliable dense initialization to bootstrap their\noptimization procedure. For example, dense optical flow algorithms profit\nmassively in speed and robustness if they are initialized well in the basin of\nconvergence of the used loss function. The same holds true for methods as\nsparse feature tracking when initial flow or depth information for new features\nat arbitrary positions is needed. This makes it extremely important to have\ntechniques at hand that allow to obtain from only very few available\nmeasurements a dense but still approximative sketch of a desired 2D structure\n(e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded\nas sample from a 2D random process. The method presented here exploits the\ncomplete information given by the principal component analysis (PCA) of that\nprocess, the principal basis and its prior distribution. The method is able to\ndetermine a dense reconstruction from sparse measurement. When facing\nsituations with only very sparse measurements, typically the number of\nprincipal components is further reduced which results in a loss of\nexpressiveness of the basis. We overcome this problem and inject prior\nknowledge in a maximum a posterior (MAP) approach. We test our approach on the\nKITTI and the virtual KITTI datasets and focus on the interpolation of depth\nmaps for driving scenes. The evaluation of the results show good agreement to\nthe ground truth and are clearly better than results of interpolation by the\nnearest neighbor method which disregards statistical information.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 10:22:21 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ochs", "Matthias", ""], ["Bradler", "Henry", ""], ["Mester", "Rudolf", ""]]}, {"id": "1703.05065", "submitter": "Matthias Ochs", "authors": "Henry Bradler, Matthias Ochs and Rudolf Mester", "title": "Joint Epipolar Tracking (JET): Simultaneous optimization of epipolar\n  geometry and feature correspondences", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV), Santa Rosa, USA, March 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, pose estimation is considered as a two step problem. First,\nfeature correspondences are determined by direct comparison of image patches,\nor by associating feature descriptors. In a second step, the relative pose and\nthe coordinates of corresponding points are estimated, most often by minimizing\nthe reprojection error (RPE). RPE optimization is based on a loss function that\nis merely aware of the feature pixel positions but not of the underlying image\nintensities. In this paper, we propose a sparse direct method which introduces\na loss function that allows to simultaneously optimize the unscaled relative\npose, as well as the set of feature correspondences directly considering the\nimage intensity values. Furthermore, we show how to integrate statistical prior\ninformation on the motion into the optimization process. This constructive\ninclusion of a Bayesian bias term is particularly efficient in application\ncases with a strongly predictable (short term) dynamic, e.g. in a driving\nscenario. In our experiments, we demonstrate that the JET algorithm we propose\noutperforms the classical reprojection error optimization on two synthetic\ndatasets and on the KITTI dataset. The JET algorithm runs in real-time on a\nsingle CPU thread.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 10:30:21 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Bradler", "Henry", ""], ["Ochs", "Matthias", ""], ["Mester", "Rudolf", ""]]}, {"id": "1703.05105", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, David Crandall", "title": "A Data Driven Approach for Compound Figure Separation Using\n  Convolutional Neural Networks", "comments": "Accepted to The International Conference on Document Analysis and\n  Recognition (ICDAR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in automatic analysis and understanding of scientific papers is\nto extract semantic information from non-textual paper components like figures,\ndiagrams, tables, etc. Much of this work requires a very first preprocessing\nstep: decomposing compound multi-part figures into individual subfigures.\nPrevious work in compound figure separation has been based on manually designed\nfeatures and separation rules, which often fail for less common figure types\nand layouts. Moreover, few implementations for compound figure decomposition\nare publicly available.\n  This paper proposes a data driven approach to separate compound figures using\nmodern deep Convolutional Neural Networks (CNNs) to train the separator in an\nend-to-end manner. CNNs eliminate the need for manually designing features and\nseparation rules, but require a large amount of annotated training data. We\novercome this challenge using transfer learning as well as automatically\nsynthesizing training exemplars. We evaluate our technique on the ImageCLEF\nMedical dataset, achieving 85.9% accuracy and outperforming previous\ntechniques. We have released our implementation as an easy-to-use Python\nlibrary, aiming to promote further research in scientific figure mining.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:10:36 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 23:01:35 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Crandall", "David", ""]]}, {"id": "1703.05128", "submitter": "Andres Asensio Ramos", "authors": "A. Asensio Ramos (1,2), I. S. Requerey (1,2), N. Vitas (1,2) ((1)\n  Instituto de Astrofisica de Canarias, (2) Universidad de La Laguna)", "title": "DeepVel: deep learning for the estimation of horizontal velocities at\n  the solar surface", "comments": "9 pages, 5 figures, accepted for publication in A&A", "journal-ref": null, "doi": "10.1051/0004-6361/201730783", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many phenomena taking place in the solar photosphere are controlled by plasma\nmotions. Although the line-of-sight component of the velocity can be estimated\nusing the Doppler effect, we do not have direct spectroscopic access to the\ncomponents that are perpendicular to the line-of-sight. These components are\ntypically estimated using methods based on local correlation tracking. We have\ndesigned DeepVel, an end-to-end deep neural network that produces an estimation\nof the velocity at every single pixel and at every time step and at three\ndifferent heights in the atmosphere from just two consecutive continuum images.\nWe confront DeepVel with local correlation tracking, pointing out that they\ngive very similar results in the time- and spatially-averaged cases. We use the\nnetwork to study the evolution in height of the horizontal velocity field in\nfragmenting granules, supporting the buoyancy-braking mechanism for the\nformation of integranular lanes in these granules. We also show that DeepVel\ncan capture very small vortices, so that we can potentially expand the scaling\ncascade of vortices to very small sizes and durations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:49:07 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 16:03:35 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ramos", "A. Asensio", ""], ["Requerey", "I. S.", ""], ["Vitas", "N.", ""]]}, {"id": "1703.05130", "submitter": "Trinh Van Chien", "authors": "Trinh Van Chien and Khanh Quoc Dinh and Byeungwoo Jeon and Martin\n  Burger", "title": "Block Compressive Sensing of Image and Video with Nonlocal Lagrangian\n  Multiplier and Patch-based Sparse Representation", "comments": "30 pages, 13 figures, 2 tables; Accepted by Elsevier Signal\n  Processing:Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although block compressive sensing (BCS) makes it tractable to sense\nlarge-sized images and video, its recovery performance has yet to be\nsignificantly improved because its recovered images or video usually suffer\nfrom blurred edges, loss of details, and high-frequency oscillatory artifacts,\nespecially at a low subrate. This paper addresses these problems by designing a\nmodified total variation technique that employs multi-block gradient\nprocessing, a denoised Lagrangian multiplier, and patch-based sparse\nrepresentation. In the case of video, the proposed recovery method is able to\nexploit both spatial and temporal similarities. Simulation results confirm the\nimproved performance of the proposed method for compressive sensing of images\nand video in terms of both objective and subjective qualities.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:56:25 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Van Chien", "Trinh", ""], ["Dinh", "Khanh Quoc", ""], ["Jeon", "Byeungwoo", ""], ["Burger", "Martin", ""]]}, {"id": "1703.05148", "submitter": "Yanzhi Song", "authors": "Songtao Guo, Yixin Luo, and Yanzhi Song", "title": "Random Forests and VGG-NET: An Algorithm for the ISIC 2017 Skin Lesion\n  Classification Challenge", "comments": "ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript briefly describes an algorithm developed for the ISIC 2017\nSkin Lesion Classification Competition. In this task, participants are asked to\ncomplete two independent binary image classification tasks that involve three\nunique diagnoses of skin lesions (melanoma, nevus, and seborrheic keratosis).\nIn the first binary classification task, participants are asked to distinguish\nbetween (a) melanoma and (b) nevus and seborrheic keratosis. In the second\nbinary classification task, participants are asked to distinguish between (a)\nseborrheic keratosis and (b) nevus and melanoma. The other phases of the\ncompetition are not considered. Our proposed algorithm consists of three steps:\npreprocessing, classification using VGG-NET and Random Forests, and calculation\nof a final score.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 13:33:55 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Guo", "Songtao", ""], ["Luo", "Yixin", ""], ["Song", "Yanzhi", ""]]}, {"id": "1703.05161", "submitter": "Christian Reinbacher", "authors": "Christian Reinbacher and Gottfried Munda and Thomas Pock", "title": "Real-Time Panoramic Tracking for Event Cameras", "comments": "Accepted to International Conference on Computational Photography\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are a paradigm shift in camera technology. Instead of full\nframes, the sensor captures a sparse set of events caused by intensity changes.\nSince only the changes are transferred, those cameras are able to capture quick\nmovements of objects in the scene or of the camera itself. In this work we\npropose a novel method to perform camera tracking of event cameras in a\npanoramic setting with three degrees of freedom. We propose a direct camera\ntracking formulation, similar to state-of-the-art in visual odometry. We show\nthat the minimal information needed for simultaneous tracking and mapping is\nthe spatial position of events, without using the appearance of the imaged\nscene point. We verify the robustness to fast camera movements and dynamic\nobjects in the scene on a recently proposed dataset and self-recorded\nsequences.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:03:47 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 13:08:49 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Reinbacher", "Christian", ""], ["Munda", "Gottfried", ""], ["Pock", "Thomas", ""]]}, {"id": "1703.05165", "submitter": "Yading Yuan", "authors": "Yading Yuan", "title": "Automatic skin lesion segmentation with fully\n  convolutional-deconvolutional networks", "comments": "ISIC2017 challenge, 4 pages", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 2018", "doi": "10.1109/JBHI.2017.2787487", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our method and validation results for the ISBI\nChallenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I:\nLesion Segmentation\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:18:23 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 02:26:32 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Yuan", "Yading", ""]]}, {"id": "1703.05192", "submitter": "Taeksoo Kim", "authors": "Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee and Jiwon Kim", "title": "Learning to Discover Cross-Domain Relations with Generative Adversarial\n  Networks", "comments": "Accepted to International Conference on Machine Learning (ICML) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While humans easily recognize relations between data from different domains\nwithout any supervision, learning to automatically discover them is in general\nvery challenging and needs many ground-truth pairs that illustrate the\nrelations. To avoid costly pairing, we address the task of discovering\ncross-domain relations given unpaired data. We propose a method based on\ngenerative adversarial networks that learns to discover relations between\ndifferent domains (DiscoGAN). Using the discovered relations, our proposed\nnetwork successfully transfers style from one domain to another while\npreserving key attributes such as orientation and face identity. Source code\nfor official implementation is publicly available\nhttps://github.com/SKTBrain/DiscoGAN\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:53:15 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 05:04:38 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kim", "Taeksoo", ""], ["Cha", "Moonsu", ""], ["Kim", "Hyunsoo", ""], ["Lee", "Jung Kwon", ""], ["Kim", "Jiwon", ""]]}, {"id": "1703.05230", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk and Paul F. Whelan", "title": "Texture segmentation with Fully Convolutional Networks", "comments": "13 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, deep learning has contributed to advances in a wide range\ncomputer vision tasks including texture analysis. This paper explores a new\napproach for texture segmentation using deep convolutional neural networks,\nsharing important ideas with classic filter bank based texture segmentation\nmethods. Several methods are developed to train Fully Convolutional Networks to\nsegment textures in various applications. We show in particular that these\nnetworks can learn to recognize and segment a type of texture, e.g. wood and\ngrass from texture recognition datasets (no training segmentation). We\ndemonstrate that Fully Convolutional Networks can learn from repetitive\npatterns to segment a particular texture from a single image or even a part of\nan image. We take advantage of these findings to develop a method that is\nevaluated on a series of supervised and unsupervised experiments and improve\nthe state of the art on the Prague texture segmentation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 16:14:52 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Whelan", "Paul F.", ""]]}, {"id": "1703.05235", "submitter": "Dennis Murphree", "authors": "Dennis H. Murphree and Che Ngufor", "title": "Transfer Learning for Melanoma Detection: Participation in ISIC 2017\n  Skin Lesion Classification Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript describes our participation in the International Skin Imaging\nCollaboration's 2017 Skin Lesion Analysis Towards Melanoma Detection\ncompetition. We participated in Part 3: Lesion Classification. The two stated\ngoals of this binary image classification challenge were to distinguish between\n(a) melanoma and (b) nevus and seborrheic keratosis, followed by distinguishing\nbetween (a) seborrheic keratosis and (b) nevus and melanoma. We chose a deep\nneural network approach with a transfer learning strategy, using a pre-trained\nInception V3 network as both a feature extractor to provide input for a\nmulti-layer perceptron as well as fine-tuning an augmented Inception network.\nThis approach yielded validation set AUC's of 0.84 on the second task and 0.76\non the first task, for an average AUC of 0.80. We joined the competition\nunfortunately late, and we look forward to improving on these results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 16:31:16 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Murphree", "Dennis H.", ""], ["Ngufor", "Che", ""]]}, {"id": "1703.05243", "submitter": "Kai Zhen", "authors": "Kai Zhen, Mridul Birla, David Crandall, Bingjing Zhang, Judy Qiu", "title": "A Hybrid Supervised-unsupervised Method on Image Topic Visualization\n  with Convolutional Neural Network and LDA", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the progress in image recognition with recent data driven paradigms,\nit's still expensive to manually label a large training data to fit a\nconvolutional neural network (CNN) model. This paper proposes a hybrid\nsupervised-unsupervised method combining a pre-trained AlexNet with Latent\nDirichlet Allocation (LDA) to extract image topics from both an unlabeled\nlife-logging dataset and the COCO dataset. We generate the bag-of-words\nrepresentations of an egocentric dataset from the softmax layer of AlexNet and\nuse LDA to visualize the subject's living genre with duplicated images. We use\na subset of COCO on 4 categories as ground truth, and define consistent rate to\nquantitatively analyze the performance of the method, it achieves 84% for\nconsistent rate on average comparing to 18.75% from a raw CNN model. The method\nis capable of detecting false labels and multi-labels from COCO dataset. For\nscalability test, parallelization experiments are conducted with Harp-LDA on a\nIntel Knights Landing cluster: to extract 1,000 topic assignments for 241,035\nCOCO images, it takes 10 minutes with 60 threads.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 16:35:31 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 17:42:47 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Zhen", "Kai", ""], ["Birla", "Mridul", ""], ["Crandall", "David", ""], ["Zhang", "Bingjing", ""], ["Qiu", "Judy", ""]]}, {"id": "1703.05289", "submitter": "Joe Kileel", "authors": "Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, Tomas Pajdla", "title": "A clever elimination strategy for efficient minimal solvers", "comments": "13 pages, 7 figures", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition 2017\n  (CVPR 2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new insight into the systematic generation of minimal solvers in\ncomputer vision, which leads to smaller and faster solvers. Many minimal\nproblem formulations are coupled sets of linear and polynomial equations where\nimage measurements enter the linear equations only. We show that it is useful\nto solve such systems by first eliminating all the unknowns that do not appear\nin the linear equations and then extending solutions to the rest of unknowns.\nThis can be generalized to fully non-linear systems by linearization via\nlifting. We demonstrate that this approach leads to more efficient solvers in\nthree problems of partially calibrated relative camera pose computation with\nunknown focal length and/or radial distortion. Our approach also generates new\ninteresting constraints on the fundamental matrices of partially calibrated\ncameras, which were not known before.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 17:44:37 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kukelova", "Zuzana", ""], ["Kileel", "Joe", ""], ["Sturmfels", "Bernd", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1703.05298", "submitter": "Dario Zanca", "authors": "Francesco Giannini, Vincenzo Laveglia, Alessandro Rossi, Dario Zanca,\n  Andrea Zugarini", "title": "Neural Networks for Beginners. A fast implementation in Matlab, Torch,\n  TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an introduction to some Machine Learning tools within\nthe most common development environments. It mainly focuses on practical\nproblems, skipping any theoretical introduction. It is oriented to both\nstudents trying to approach Machine Learning and experts looking for new\nframeworks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 18:01:20 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 08:32:19 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Giannini", "Francesco", ""], ["Laveglia", "Vincenzo", ""], ["Rossi", "Alessandro", ""], ["Zanca", "Dario", ""], ["Zugarini", "Andrea", ""]]}, {"id": "1703.05354", "submitter": "Peter Van Beek", "authors": "Peter van Beek and R. Wayne Oldford", "title": "Illuminant Estimation using Ensembles of Multivariate Regression Trees", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White balancing is a fundamental step in the image processing pipeline. The\nprocess involves estimating the chromaticity of the illuminant or light source\nand using the estimate to correct the image to remove any color cast. Given the\nimportance of the problem, there has been much previous work on illuminant\nestimation. Recently, an approach based on ensembles of univariate regression\ntrees that are fit using the squared-error loss function has been proposed and\nshown to give excellent performance. In this paper, we show that a simpler and\nmore accurate ensemble model can be learned by (i) using multivariate\nregression trees to take into account that the chromaticity components of the\nilluminant are correlated and constrained, and (ii) fitting each tree by\ndirectly minimizing a loss function of interest---such as recovery angular\nerror or reproduction angular error---rather than indirectly using the\nsquared-error loss function as a surrogate. We show empirically that overall\nour method leads to improved performance on diverse image sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 18:59:46 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["van Beek", "Peter", ""], ["Oldford", "R. Wayne", ""]]}, {"id": "1703.05381", "submitter": "Justinas Miseikis", "authors": "Justinas Miseikis and Matthias Ruther and Bernhard Walzel and Mario\n  Hirz and Helmut Brunner", "title": "3D Vision Guided Robotic Charging Station for Electric and Plug-in\n  Hybrid Vehicles", "comments": "6 pages, 8 figures, OAGM and ARW Joint Workshop 2017 on Vision,\n  Automation and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric vehicles (EVs) and plug-in hybrid vehicles (PHEVs) are rapidly\ngaining popularity on our roads. Besides a comparatively high purchasing price,\nthe main two problems limiting their use are the short driving range and\ninconvenient charging process. In this paper we address the following by\npresenting an automatic robot-based charging station with 3D vision guidance\nfor plugging and unplugging the charger. First of all, the whole system concept\nconsisting of a 3D vision system, an UR10 robot and a charging station is\npresented. Then we show the shape-based matching methods used to successfully\nidentify and get the exact pose of the charging port. The same approach is used\nto calibrate the camera-robot system by using just known structure of the\nconnector plug and no additional markers. Finally, a three-step robot motion\nplanning procedure for plug-in is presented and functionality is demonstrated\nin a series of successful experiments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 20:49:14 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Miseikis", "Justinas", ""], ["Ruther", "Matthias", ""], ["Walzel", "Bernhard", ""], ["Hirz", "Mario", ""], ["Brunner", "Helmut", ""]]}, {"id": "1703.05393", "submitter": "Dingding Cai ddC", "authors": "Dingding Cai, Ke Chen, Yanlin Qian, Joni-Kristian K\\\"am\\\"ar\\\"ainen", "title": "Convolutional Low-Resolution Fine-Grained Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful fine-grained image classification methods learn subtle details\nbetween visually similar (sub-)classes, but the problem becomes significantly\nmore challenging if the details are missing due to low resolution. Encouraged\nby the recent success of Convolutional Neural Network (CNN) architectures in\nimage classification, we propose a novel resolution-aware deep model which\ncombines convolutional image super-resolution and convolutional fine-grained\nclassification into a single model in an end-to-end manner. Extensive\nexperiments on the Stanford Cars and Caltech-UCSD Birds 200-2011 benchmarks\ndemonstrate that the proposed model consistently performs better than\nconventional convolutional net on classifying fine-grained object classes in\nlow-resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 21:40:48 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 09:04:12 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 11:53:48 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cai", "Dingding", ""], ["Chen", "Ke", ""], ["Qian", "Yanlin", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""]]}, {"id": "1703.05446", "submitter": "Ke Gong", "authors": "Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, Liang Lin", "title": "Look into Person: Self-supervised Structure-sensitive Learning and A New\n  Benchmark for Human Parsing", "comments": "Accepted to appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing has recently attracted a lot of research interests due to its\nhuge application potentials. However existing datasets have limited number of\nimages and annotations, and lack the variety of human appearances and the\ncoverage of challenging cases in unconstrained environment. In this paper, we\nintroduce a new benchmark \"Look into Person (LIP)\" that makes a significant\nadvance in terms of scalability, diversity and difficulty, a contribution that\nwe feel is crucial for future developments in human-centric analysis. This\ncomprehensive dataset contains over 50,000 elaborately annotated images with 19\nsemantic part labels, which are captured from a wider range of viewpoints,\nocclusions and background complexity. Given these rich annotations we perform\ndetailed analyses of the leading human parsing approaches, gaining insights\ninto the success and failures of these methods. Furthermore, in contrast to the\nexisting efforts on improving the feature discriminative capability, we solve\nhuman parsing by exploring a novel self-supervised structure-sensitive learning\napproach, which imposes human pose structures into parsing results without\nresorting to extra supervision (i.e., no need for specifically labeling human\njoints in model training). Our self-supervised learning framework can be\ninjected into any advanced neural networks to help incorporate rich high-level\nknowledge regarding human joints from a global perspective and improve the\nparsing results. Extensive evaluations on our LIP and the public\nPASCAL-Person-Part dataset demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 01:14:36 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 01:41:39 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Gong", "Ke", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Dongyu", ""], ["Shen", "Xiaohui", ""], ["Lin", "Liang", ""]]}, {"id": "1703.05451", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Wankou Yang and\n  Zhenmin Tang", "title": "Refining Image Categorization by Exploiting Web Images and General\n  Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies show that refining real-world categories into semantic subcategories\ncontributes to better image modeling and classification. Previous image\nsub-categorization work relying on labeled images and WordNet's hierarchy is\nnot only labor-intensive, but also restricted to classify images into NOUN\nsubcategories. To tackle these problems, in this work, we exploit general\ncorpus information to automatically select and subsequently classify web images\ninto semantic rich (sub-)categories. The following two major challenges are\nwell studied: 1) noise in the labels of subcategories derived from the general\ncorpus; 2) noise in the labels of images retrieved from the web. Specifically,\nwe first obtain the semantic refinement subcategories from the text perspective\nand remove the noise by the relevance-based approach. To suppress the search\nerror induced noisy images, we then formulate image selection and classifier\nlearning as a multi-class multi-instance learning problem and propose to solve\nthe employed problem by the cutting-plane algorithm. The experiments show\nsignificant performance gains by using the generated data of our way on both\nimage categorization and sub-categorization tasks. The proposed approach also\nconsistently outperforms existing weakly supervised and web-supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 01:36:49 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Hua", "Xiansheng", ""], ["Yang", "Wankou", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1703.05455", "submitter": "Zhe Jin", "authors": "Zhe Jin, Yen-Lung Lai, Jung-Yeon Hwang, Soohyung Kim, Andrew Beng Jin\n  Teoh", "title": "Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics:\n  Index-of-Max Hashing", "comments": "15 pages, 8 figures, 6 tables", "journal-ref": null, "doi": "10.1109/TIFS.2017.2753172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a ranking based locality sensitive hashing inspired\ntwo-factor cancelable biometrics, dubbed \"Index-of-Max\" (IoM) hashing for\nbiometric template protection. With externally generated random parameters, IoM\nhashing transforms a real-valued biometric feature vector into discrete index\n(max ranked) hashed code. We demonstrate two realizations from IoM hashing\nnotion, namely Gaussian Random Projection based and Uniformly Random\nPermutation based hashing schemes. The discrete indices representation nature\nof IoM hashed codes enjoy serveral merits. Firstly, IoM hashing empowers strong\nconcealment to the biometric information. This contributes to the solid ground\nof non-invertibility guarantee. Secondly, IoM hashing is insensitive to the\nfeatures magnitude, hence is more robust against biometric features variation.\nThirdly, the magnitude-independence trait of IoM hashing makes the hash codes\nbeing scale-invariant, which is critical for matching and feature alignment.\nThe experimental results demonstrate favorable accuracy performance on\nbenchmark FVC2002 and FVC2004 fingerprint databases. The analyses justify its\nresilience to the existing and newly introduced security and privacy attacks as\nwell as satisfy the revocability and unlinkability criteria of cancelable\nbiometrics.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 02:10:56 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 16:52:24 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jin", "Zhe", ""], ["Lai", "Yen-Lung", ""], ["Hwang", "Jung-Yeon", ""], ["Kim", "Soohyung", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "1703.05463", "submitter": "Walter Scheirer", "authors": "Ruth Fong, Walter Scheirer, David Cox", "title": "Using Human Brain Activity to Guide Machine Learning", "comments": "Supplemental material can be downloaded here:\n  http://www.wjscheirer.com/misc/activity_weights/fong-et-al-supplementary.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a field of computer science that builds algorithms that\nlearn. In many cases, machine learning algorithms are used to recreate a human\nability like adding a caption to a photo, driving a car, or playing a game.\nWhile the human brain has long served as a source of inspiration for machine\nlearning, little effort has been made to directly use data collected from\nworking brains as a guide for machine learning algorithms. Here we demonstrate\na new paradigm of \"neurally-weighted\" machine learning, which takes fMRI\nmeasurements of human brain activity from subjects viewing images, and infuses\nthese data into the training process of an object recognition learning\nalgorithm to make it more consistent with the human brain. After training,\nthese neurally-weighted classifiers are able to classify images without\nrequiring any additional neural data. We show that our neural-weighting\napproach can lead to large performance gains when used with traditional machine\nvision features, as well as to significant improvements with already\nhigh-performing convolutional neural network features. The effectiveness of\nthis approach points to a path forward for a new class of hybrid machine\nlearning algorithms which take both inspiration and direct constraints from\nneuronal data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 02:56:54 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 21:49:59 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Fong", "Ruth", ""], ["Scheirer", "Walter", ""], ["Cox", "David", ""]]}, {"id": "1703.05467", "submitter": "Jin Qi", "authors": "Jin Qi, Miao Le, Chunming Li, Ping Zhou", "title": "Global and Local Information Based Deep Network for Skin Lesion\n  Segmentation", "comments": "4 pages, 3 figures. ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With a large influx of dermoscopy images and a growing shortage of\ndermatologists, automatic dermoscopic image analysis plays an essential role in\nskin cancer diagnosis. In this paper, a new deep fully convolutional neural\nnetwork (FCNN) is proposed to automatically segment melanoma out of skin images\nby end-to-end learning with only pixels and labels as inputs. Our proposed FCNN\nis capable of using both local and global information to segment melanoma by\nadopting skipping layers. The public benchmark database consisting of 150\nvalidation images, 600 test images and 2000 training images in the melanoma\ndetection challenge 2017 at International Symposium Biomedical Imaging 2017 is\nused to test the performance of our algorithm. All large size images (for\nexample, $4000\\times 6000$ pixels) are reduced to much smaller images with\n$384\\times 384$ pixels (more than 10 times smaller). We got and submitted\npreliminary results to the challenge without any pre or post processing. The\nperformance of our proposed method could be further improved by data\naugmentation and by avoiding image size reduction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 03:23:48 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Qi", "Jin", ""], ["Le", "Miao", ""], ["Li", "Chunming", ""], ["Zhou", "Ping", ""]]}, {"id": "1703.05502", "submitter": "Evgeny Burnaev", "authors": "Denis Volkhonskiy and Ivan Nazarov and Evgeny Burnaev", "title": "Steganographic Generative Adversarial Networks", "comments": "15 pages, 10 figures, 5 tables, Workshop on Adversarial Training\n  (NIPS 2016, Barcelona, Spain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is collection of methods to hide secret information (\"payload\")\nwithin non-secret information \"container\"). Its counterpart, Steganalysis, is\nthe practice of determining if a message contains a hidden payload, and\nrecovering it if possible. Presence of hidden payloads is typically detected by\na binary classifier. In the present study, we propose a new model for\ngenerating image-like containers based on Deep Convolutional Generative\nAdversarial Networks (DCGAN). This approach allows to generate more\nsetganalysis-secure message embedding using standard steganography algorithms.\nExperiment results demonstrate that the new model successfully deceives the\nsteganography analyzer, and for this reason, can be used in steganographic\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 08:28:11 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 19:56:14 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Nazarov", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1703.05530", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk and Paul F. Whelan", "title": "Convolutional Neural Network on Three Orthogonal Planes for Dynamic\n  Texture Classification", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Textures (DTs) are sequences of images of moving scenes that exhibit\ncertain stationarity properties in time such as smoke, vegetation and fire. The\nanalysis of DT is important for recognition, segmentation, synthesis or\nretrieval for a range of applications including surveillance, medical imaging\nand remote sensing. Deep learning methods have shown impressive results and are\nnow the new state of the art for a wide range of computer vision tasks\nincluding image and video recognition and segmentation. In particular,\nConvolutional Neural Networks (CNNs) have recently proven to be well suited for\ntexture analysis with a design similar to a filter bank approach. In this\npaper, we develop a new approach to DT analysis based on a CNN method applied\non three orthogonal planes x y , xt and y t . We train CNNs on spatial frames\nand temporal slices extracted from the DT sequences and combine their outputs\nto obtain a competitive DT classifier. Our results on a wide range of commonly\nused DT classification benchmark datasets prove the robustness of our approach.\nSignificant improvement of the state of the art is shown on the larger\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 09:30:07 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Whelan", "Paul F.", ""]]}, {"id": "1703.05560", "submitter": "Christoph Brune", "authors": "Leonie Zeune, Stephan A. van Gils, Leon W.M.M. Terstappen, Christoph\n  Brune", "title": "Combining Contrast Invariant L1 Data Fidelities with Nonlinear Spectral\n  Image Decomposition", "comments": "13 pages, 7 figures, conference SSVM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on multi-scale approaches for variational methods and\ncorresponding gradient flows. Recently, for convex regularization functionals\nsuch as total variation, new theory and algorithms for nonlinear eigenvalue\nproblems via nonlinear spectral decompositions have been developed. Those\nmethods open new directions for advanced image filtering. However, for an\neffective use in image segmentation and shape decomposition, a clear\ninterpretation of the spectral response regarding size and intensity scales is\nneeded but lacking in current approaches. In this context, $L^1$ data\nfidelities are particularly helpful due to their interesting multi-scale\nproperties such as contrast invariance. Hence, the novelty of this work is the\ncombination of $L^1$-based multi-scale methods with nonlinear spectral\ndecompositions. We compare $L^1$ with $L^2$ scale-space methods in view of\nspectral image representation and decomposition. We show that the contrast\ninvariant multi-scale behavior of $L^1-TV$ promotes sparsity in the spectral\nresponse providing more informative decompositions. We provide a numerical\nmethod and analyze synthetic and biomedical images at which decomposition leads\nto improved segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 11:12:13 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Zeune", "Leonie", ""], ["van Gils", "Stephan A.", ""], ["Terstappen", "Leon W. M. M.", ""], ["Brune", "Christoph", ""]]}, {"id": "1703.05571", "submitter": "Antonio Foncubierta-Rodriguez", "authors": "Antonio Foncubierta-Rodr\\'iguez, Henning M\\\"uller, Adrien Depeursinge", "title": "From visual words to a visual grammar: using language modelling for\n  image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bag--of--Visual--Words (BoVW) is a visual description technique that aims\nat shortening the semantic gap by partitioning a low--level feature space into\nregions of the feature space that potentially correspond to visual concepts and\nby giving more value to this space. In this paper we present a conceptual\nanalysis of three major properties of language grammar and how they can be\nadapted to the computer vision and image understanding domain based on the bag\nof visual words paradigm. Evaluation of the visual grammar shows that a\npositive impact on classification accuracy and/or descriptor size is obtained\nwhen the technique are applied when the proposed techniques are applied.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 11:46:57 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Foncubierta-Rodr\u00edguez", "Antonio", ""], ["M\u00fcller", "Henning", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "1703.05591", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Shilpa Manandhar, Feng Yuan, Yee Hui Lee, Stefan\n  Winkler", "title": "Cloud Radiative Effect Study Using Sky Camera", "comments": "Accepted in Proc. IEEE AP-S Symposium on Antennas and Propagation and\n  USNC-URSI Radio Science Meeting, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of clouds in the earth's atmosphere is important for a variety\nof applications, viz. weather reporting, climate forecasting, and solar energy\ngeneration. In this paper, we focus our attention on the impact of cloud on the\ntotal solar irradiance reaching the earth's surface. We use weather station to\nrecord the total solar irradiance. Moreover, we employ collocated ground-based\nsky camera to automatically compute the instantaneous cloud coverage. We\nanalyze the relationship between measured solar irradiance and computed cloud\ncoverage value, and conclude that higher cloud coverage greatly impacts the\ntotal solar irradiance. Such studies will immensely help in solar energy\ngeneration and forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:47:50 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Manandhar", "Shilpa", ""], ["Yuan", "Feng", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1703.05593", "submitter": "Ignacio Rocco", "authors": "Ignacio Rocco, Relja Arandjelovi\\'c, Josef Sivic", "title": "Convolutional neural network architecture for geometric matching", "comments": "In 2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of determining correspondences between two images in\nagreement with a geometric model such as an affine or thin-plate spline\ntransformation, and estimating its parameters. The contributions of this work\nare three-fold. First, we propose a convolutional neural network architecture\nfor geometric matching. The architecture is based on three main components that\nmimic the standard steps of feature extraction, matching and simultaneous\ninlier detection and model parameter estimation, while being trainable\nend-to-end. Second, we demonstrate that the network parameters can be trained\nfrom synthetically generated imagery without the need for manual annotation and\nthat our matching layer significantly increases generalization capabilities to\nnever seen before images. Finally, we show that the same model can perform both\ninstance-level and category-level matching giving state-of-the-art results on\nthe challenging Proposal Flow dataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 13:03:54 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 22:32:43 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Rocco", "Ignacio", ""], ["Arandjelovi\u0107", "Relja", ""], ["Sivic", "Josef", ""]]}, {"id": "1703.05605", "submitter": "Li Liu", "authors": "Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling Shao", "title": "Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval", "comments": "This paper will appear as a spotlight paper in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-hand sketch-based image retrieval (SBIR) is a specific cross-view\nretrieval task, in which queries are abstract and ambiguous sketches while the\nretrieval database is formed with natural images. Work in this area mainly\nfocuses on extracting representative and shared features for sketches and\nnatural images. However, these can neither cope well with the geometric\ndistortion between sketches and images nor be feasible for large-scale SBIR due\nto the heavy continuous-valued distance computation. In this paper, we speed up\nSBIR by introducing a novel binary coding method, named \\textbf{Deep Sketch\nHashing} (DSH), where a semi-heterogeneous deep architecture is proposed and\nincorporated into an end-to-end binary coding framework. Specifically, three\nconvolutional neural networks are utilized to encode free-hand sketches,\nnatural images and, especially, the auxiliary sketch-tokens which are adopted\nas bridges to mitigate the sketch-image geometric distortion. The learned DSH\ncodes can effectively capture the cross-view similarities as well as the\nintrinsic semantic correlations between different categories. To the best of\nour knowledge, DSH is the first hashing work specifically designed for\ncategory-level SBIR with an end-to-end deep architecture. The proposed DSH is\ncomprehensively evaluated on two large-scale datasets of TU-Berlin Extension\nand Sketchy, and the experiments consistently show DSH's superior SBIR\naccuracies over several state-of-the-art methods, while achieving significantly\nreduced retrieval time and memory footprint.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 13:18:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Liu", "Li", ""], ["Shen", "Fumin", ""], ["Shen", "Yuming", ""], ["Liu", "Xianglong", ""], ["Shao", "Ling", ""]]}, {"id": "1703.05630", "submitter": "Nan Xue", "authors": "Nan Xue and Gui-Song Xia and Xiang Bai and Liangpei Zhang and Weiming\n  Shen", "title": "Anisotropic-Scale Junction Detection and Matching for Indoor Images", "comments": "This paper has been accepted for publication in the IEEE Transactions\n  on Image Processing", "journal-ref": "IEEE Transactions on Image Processing, Vol. 27, No.1, pp. 78-91,\n  2018", "doi": "10.1109/TIP.2017.2754945", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Junctions play an important role in the characterization of local geometric\nstructures in images, the detection of which is a longstanding and challenging\ntask. Existing junction detectors usually focus on identifying the junction\nlocations and the orientations of the junction branches while ignoring their\nscales; however, these scales also contain rich geometric information. This\npaper presents a novel approach to junction detection and characterization that\nexploits the locally anisotropic geometries of a junction and estimates the\nscales of these geometries using an \\emph{a contrario} model. The output\njunctions have anisotropic scales --- i.e., each branch of a junction is\nassociated with an independent scale parameter --- and are thus termed\nanisotropic-scale junctions (ASJs). We then apply the newly detected ASJs for\nthe matching of indoor images, in which there may be dramatic changes in\nviewpoint and the detected local visual features, e.g., key-points, are usually\ninsufficiently distinctive. We propose to use the anisotropic geometries of our\njunctions to improve the matching precision for indoor images. Matching results\nobtained on sets of indoor images demonstrate that our approach achieves\nstate-of-the-art performance in indoor image matching.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 14:10:44 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 08:44:04 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Xue", "Nan", ""], ["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""], ["Zhang", "Liangpei", ""], ["Shen", "Weiming", ""]]}, {"id": "1703.05680", "submitter": "Andre Ebert", "authors": "Andre Ebert, Sebastian Feld, Florian Dorfmeister", "title": "Segmented and Directional Impact Detection for Parked Vehicles using\n  Mobile Devices", "comments": "4 Pages, 6 Figures, Accepted at the The 23rd International Conference\n  on Systems, Signals and Image Processing (IWSSIP)", "journal-ref": null, "doi": "10.1109/IWSSIP.2016.7502762", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual usage of vehicles as well as car sharing became more and more\nattractive during the last years. Especially in urban environments with limited\nparking possibilities and a higher risk for traffic jams, car rentals and\nsharing services may save time and money. But when renting a vehicle it could\nalready be damaged (e.g., scratches or bumps inflicted by a previous user)\nwithout the damage being perceived by the service provider. In order to address\nsuch problems, we present an automated, motion-based system for impact\ndetection, that facilitates a common smartphone as a sensor platform. The\nsystem is capable of detecting the impact segment and the point of time of an\nimpact event on a vehicle's surface, as well as its direction of origin. With\nthis additional specific knowledge, it may be possible to reconstruct the\ncircumstances of an impact event, e.g., to prove possible innocence of a\nservice's customer.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 15:51:56 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Ebert", "Andre", ""], ["Feld", "Sebastian", ""], ["Dorfmeister", "Florian", ""]]}, {"id": "1703.05693", "submitter": "Sun Yifan", "authors": "Yifan Sun, Liang Zheng, Weijian Deng, Shengjin Wang", "title": "SVDNet for Pedestrian Retrieval", "comments": "accepted as spotlight to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes the SVDNet for retrieval problems, with focus on the\napplication of person re-identification (re-ID). We view each weight vector\nwithin a fully connected (FC) layer in a convolutional neuron network (CNN) as\na projection basis. It is observed that the weight vectors are usually highly\ncorrelated. This problem leads to correlations among entries of the FC\ndescriptor, and compromises the retrieval performance based on the Euclidean\ndistance. To address the problem, this paper proposes to optimize the deep\nrepresentation learning process with Singular Vector Decomposition (SVD).\nSpecifically, with the restraint and relaxation iteration (RRI) training\nscheme, we are able to iteratively integrate the orthogonality constraint in\nCNN training, yielding the so-called SVDNet. We conduct experiments on the\nMarket-1501, CUHK03, and Duke datasets, and show that RRI effectively reduces\nthe correlation among the projection vectors, produces more discriminative FC\ndescriptors, and significantly improves the re-ID accuracy. On the Market-1501\ndataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for\nCaffeNet, and from 73.8% to 82.3% for ResNet-50.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:11:05 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 14:22:10 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 02:14:08 GMT"}, {"version": "v4", "created": "Sun, 6 Aug 2017 05:37:09 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Sun", "Yifan", ""], ["Zheng", "Liang", ""], ["Deng", "Weijian", ""], ["Wang", "Shengjin", ""]]}, {"id": "1703.05724", "submitter": "Sailesh Conjeti", "authors": "Sailesh Conjeti, Magdalini Paschali, Amin Katouzian and Nassir Navab", "title": "Learning Robust Hash Codes for Multiple Instance Image Retrieval", "comments": "10 pages, 7 figures, under review at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the first time, we introduce a multiple instance (MI) deep\nhashing technique for learning discriminative hash codes with weak bag-level\nsupervision suited for large-scale retrieval. We learn such hash codes by\naggregating deeply learnt hierarchical representations across bag members\nthrough a dedicated MI pool layer. For better trainability and retrieval\nquality, we propose a two-pronged approach that includes robust optimization\nand training with an auxiliary single instance hashing arm which is\ndown-regulated gradually. We pose retrieval for tumor assessment as an MI\nproblem because tumors often coexist with benign masses and could exhibit\ncomplementary signatures when scanned from different anatomical views.\nExperimental validations on benchmark mammography and histology datasets\ndemonstrate improved retrieval performance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 17:07:26 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Conjeti", "Sailesh", ""], ["Paschali", "Magdalini", ""], ["Katouzian", "Amin", ""], ["Navab", "Nassir", ""]]}, {"id": "1703.05785", "submitter": "Paris Giampouras", "authors": "Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos D.\n  Koutroumbas", "title": "Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and\n  Blind Unmixing of Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the number of endmembers existing in a scene constitutes a\ncritical task in the hyperspectral unmixing process. The accuracy of this\nestimate plays a crucial role in subsequent unsupervised unmixing steps i.e.,\nthe derivation of the spectral signatures of the endmembers (endmembers'\nextraction) and the estimation of the abundance fractions of the pixels. A\ncommon practice amply followed in literature is to treat endmembers' number\nestimation and unmixing, independently as two separate tasks, providing the\noutcome of the former as input to the latter. In this paper, we go beyond this\ncomputationally demanding strategy. More precisely, we set forth a multiple\nconstrained optimization framework, which encapsulates endmembers' number\nestimation and unsupervised unmixing in a single task. This is attained by\nsuitably formulating the problem via a low-rank and sparse nonnegative matrix\nfactorization rationale, where low-rankness is promoted with the use of a\nsophisticated $\\ell_2/\\ell_1$ norm penalty term. An alternating proximal\nalgorithm is then proposed for minimizing the emerging cost function. The\nresults obtained by simulated and real data experiments verify the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 18:25:21 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Giampouras", "Paris V.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Koutroumbas", "Konstantinos D.", ""]]}, {"id": "1703.05830", "submitter": "Mohammad Sadegh Norouzzadeh", "authors": "Mohammed Sadegh Norouzzadeh, Anh Nguyen, Margaret Kosmala, Ali\n  Swanson, Meredith Palmer, Craig Packer, Jeff Clune", "title": "Automatically identifying, counting, and describing wild animals in\n  camera-trap images with deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having accurate, detailed, and up-to-date information about the location and\nbehavior of animals in the wild would revolutionize our ability to study and\nconserve ecosystems. We investigate the ability to automatically, accurately,\nand inexpensively collect such data, which could transform many fields of\nbiology, ecology, and zoology into \"big data\" sciences. Motion sensor \"camera\ntraps\" enable collecting wildlife pictures inexpensively, unobtrusively, and\nfrequently. However, extracting information from these pictures remains an\nexpensive, time-consuming, manual task. We demonstrate that such information\ncan be automatically extracted by deep learning, a cutting-edge type of\nartificial intelligence. We train deep convolutional neural networks to\nidentify, count, and describe the behaviors of 48 species in the\n3.2-million-image Snapshot Serengeti dataset. Our deep neural networks\nautomatically identify animals with over 93.8% accuracy, and we expect that\nnumber to improve rapidly in years to come. More importantly, if our system\nclassifies only images it is confident about, our system can automate animal\nidentification for 99.3% of the data while still performing at the same 96.6%\naccuracy as that of crowdsourced teams of human volunteers, saving more than\n8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000\nhours) on this 3.2-million-image dataset. Those efficiency gains immediately\nhighlight the importance of using deep neural networks to automate data\nextraction from camera-trap images. Our results suggest that this technology\ncould enable the inexpensive, unobtrusive, high-volume, and even real-time\ncollection of a wealth of information about vast numbers of animals in the\nwild.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 21:35:15 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 00:45:55 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 22:24:19 GMT"}, {"version": "v4", "created": "Wed, 5 Apr 2017 04:26:20 GMT"}, {"version": "v5", "created": "Wed, 15 Nov 2017 19:29:24 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Norouzzadeh", "Mohammed Sadegh", ""], ["Nguyen", "Anh", ""], ["Kosmala", "Margaret", ""], ["Swanson", "Ali", ""], ["Palmer", "Meredith", ""], ["Packer", "Craig", ""], ["Clune", "Jeff", ""]]}, {"id": "1703.05853", "submitter": "Vivienne Sze", "authors": "Amr Suleiman, Yu-Hsin Chen, Joel Emer, Vivienne Sze", "title": "Towards Closing the Energy Gap Between HOG and CNN Features for Embedded\n  Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision enables a wide range of applications in robotics/drones,\nself-driving cars, smart Internet of Things, and portable/wearable electronics.\nFor many of these applications, local embedded processing is preferred due to\nprivacy and/or latency concerns. Accordingly, energy-efficient embedded vision\nhardware delivering real-time and robust performance is crucial. While deep\nlearning is gaining popularity in several computer vision algorithms, a\nsignificant energy consumption difference exists compared to traditional\nhand-crafted approaches. In this paper, we provide an in-depth analysis of the\ncomputation, energy and accuracy trade-offs between learned features such as\ndeep Convolutional Neural Networks (CNN) and hand-crafted features such as\nHistogram of Oriented Gradients (HOG). This analysis is supported by\nmeasurements from two chips that implement these algorithms. Our goal is to\nunderstand the source of the energy discrepancy between the two approaches and\nto provide insight about the potential areas where CNNs can be improved and\neventually approach the energy-efficiency of HOG while maintaining its\noutstanding performance accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 00:17:50 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Suleiman", "Amr", ""], ["Chen", "Yu-Hsin", ""], ["Emer", "Joel", ""], ["Sze", "Vivienne", ""]]}, {"id": "1703.05868", "submitter": "Shanghang Zhang", "authors": "Shanghang Zhang, Guanhang Wu, Jo\\~ao P. Costeira, Jos\\'e M. F. Moura", "title": "Understanding Traffic Density from Large-Scale Web Camera Data", "comments": "Accepted by CVPR 2017. Preprint version was uploaded on\n  http://welcome.isr.tecnico.ulisboa.pt/publications/understanding-traffic-density-from-large-scale-web-camera-data/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding traffic density from large-scale web camera (webcam) videos is\na challenging problem because such videos have low spatial and temporal\nresolution, high occlusion and large perspective. To deeply understand traffic\ndensity, we explore both deep learning based and optimization based methods. To\navoid individual vehicle detection and tracking, both methods map the image\ninto vehicle density map, one based on rank constrained regression and the\nother one based on fully convolution networks (FCN). The regression based\nmethod learns different weights for different blocks in the image to increase\nfreedom degrees of weights and embed perspective information. The FCN based\nmethod jointly estimates vehicle density map and vehicle count with a residual\nlearning framework to perform end-to-end dense prediction, allowing arbitrary\nimage resolution, and adapting to different vehicle scales and perspectives. We\nanalyze and compare both methods, and get insights from optimization based\nmethod to improve deep model. Since existing datasets do not cover all the\nchallenges in our work, we collected and labelled a large-scale traffic video\ndataset, containing 60 million frames from 212 webcams. Both methods are\nextensively evaluated and compared on different counting tasks and datasets.\nFCN based method significantly reduces the mean absolute error from 10.99 to\n5.31 on the public dataset TRANCOS compared with the state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 02:14:27 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 01:36:33 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 20:10:48 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhang", "Shanghang", ""], ["Wu", "Guanhang", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1703.05870", "submitter": "Lianwen Jin", "authors": "Shuangping Huangm Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Haobin Wang", "title": "DropRegion Training of Inception Font Network for High-Performance\n  Chinese Font Recognition", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese font recognition (CFR) has gained significant attention in recent\nyears. However, due to the sparsity of labeled font samples and the structural\ncomplexity of Chinese characters, CFR is still a challenging task. In this\npaper, a DropRegion method is proposed to generate a large number of stochastic\nvariant font samples whose local regions are selectively disrupted and an\ninception font network (IFN) with two additional convolutional neural network\n(CNN) structure elements, i.e., a cascaded cross-channel parametric pooling\n(CCCP) and global average pooling, is designed. Because the distribution of\nstrokes in a font image is non-stationary, an elastic meshing technique that\nadaptively constructs a set of local regions with equalized information is\ndeveloped. Thus, DropRegion is seamlessly embedded in the IFN, which enables\nend-to-end training; the proposed DropRegion-IFN can be used for high\nperformance CFR. Experimental results have confirmed the effectiveness of our\nnew approach for CFR.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 02:24:26 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 04:51:40 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Zhong", "Shuangping Huangm Zhuoyao", ""], ["Jin", "Lianwen", ""], ["Zhang", "Shuye", ""], ["Wang", "Haobin", ""]]}, {"id": "1703.05884", "submitter": "Ashton Fagg", "authors": "Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva Ramanan, Simon\n  Lucey", "title": "Need for Speed: A Benchmark for Higher Frame Rate Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first higher frame rate video dataset (called\nNeed for Speed - NfS) and benchmark for visual object tracking. The dataset\nconsists of 100 videos (380K frames) captured with now commonly available\nhigher frame rate (240 FPS) cameras from real world scenarios. All frames are\nannotated with axis aligned bounding boxes and all sequences are manually\nlabelled with nine visual attributes - such as occlusion, fast motion,\nbackground clutter, etc. Our benchmark provides an extensive evaluation of many\nrecent and state-of-the-art trackers on higher frame rate sequences. We ranked\neach of these trackers according to their tracking accuracy and real-time\nperformance. One of our surprising conclusions is that at higher frame rates,\nsimple trackers such as correlation filters outperform complex methods based on\ndeep networks. This suggests that for practical applications (such as in\nrobotics or embedded vision), one needs to carefully tradeoff bandwidth\nconstraints associated with higher frame rate acquisition, computational costs\nof real-time analysis, and the required application accuracy. Our dataset and\nbenchmark allows for the first time (to our knowledge) systematic exploration\nof such issues, and will be made available to allow for further research in\nthis space.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 04:18:25 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 22:35:09 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Galoogahi", "Hamed Kiani", ""], ["Fagg", "Ashton", ""], ["Huang", "Chen", ""], ["Ramanan", "Deva", ""], ["Lucey", "Simon", ""]]}, {"id": "1703.05908", "submitter": "Yao-Hung Tsai", "authors": "Yao-Hung Hubert Tsai and Liang-Kang Huang and Ruslan Salakhutdinov", "title": "Learning Robust Visual-Semantic Embeddings", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the existing methods for learning joint embedding of images and text\nuse only supervised information from paired images and its textual attributes.\nTaking advantage of the recent success of unsupervised learning in deep neural\nnetworks, we propose an end-to-end learning framework that is able to extract\nmore robust multi-modal representations across domains. The proposed method\ncombines representation learning models (i.e., auto-encoders) together with\ncross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn\njoint embeddings for semantic and visual features. A novel technique of\nunsupervised-data adaptation inference is introduced to construct more\ncomprehensive embeddings for both labeled and unlabeled data. We evaluate our\nmethod on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with\na wide range of applications, including zero and few-shot image recognition and\nretrieval, from inductive to transductive settings. Empirically, we show that\nour framework improves over the current state of the art on many of the\nconsidered tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 06:59:51 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 00:28:07 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Huang", "Liang-Kang", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1703.05913", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury, Donny Sun, Matthew Bihis, Johnny Ren, Paul Hage\n  and Humairat H. Rahman", "title": "Computer Aided Detection of Anemia-like Pallor", "comments": "4 pages,2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paleness or pallor is a manifestation of blood loss or low hemoglobin\nconcentrations in the human blood that can be caused by pathologies such as\nanemia. This work presents the first automated screening system that utilizes\npallor site images, segments, and extracts color and intensity-based features\nfor multi-class classification of patients with high pallor due to anemia-like\npathologies, normal patients and patients with other abnormalities. This work\nanalyzes the pallor sites of conjunctiva and tongue for anemia screening\npurposes. First, for the eye pallor site images, the sclera and conjunctiva\nregions are automatically segmented for regions of interest. Similarly, for the\ntongue pallor site images, the inner and outer tongue regions are segmented.\nThen, color-plane based feature extraction is performed followed by machine\nlearning algorithms for feature reduction and image level classification for\nanemia. In this work, a suite of classification algorithms image-level\nclassifications for normal (class 0), pallor (class 1) and other abnormalities\n(class 2). The proposed method achieves 86% accuracy, 85% precision and 67%\nrecall in eye pallor site images and 98.2% accuracy and precision with 100%\nrecall in tongue pallor site images for classification of images with pallor.\nThe proposed pallor screening system can be further fine-tuned to detect the\nseverity of anemia-like pathologies using controlled set of local images that\ncan then be used for future benchmarking purposes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 07:35:26 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Roychowdhury", "Sohini", ""], ["Sun", "Donny", ""], ["Bihis", "Matthew", ""], ["Ren", "Johnny", ""], ["Hage", "Paul", ""], ["Rahman", "Humairat H.", ""]]}, {"id": "1703.05921", "submitter": "Thomas Schlegl", "authors": "Thomas Schlegl, Philipp Seeb\\\"ock, Sebastian M. Waldstein, Ursula\n  Schmidt-Erfurth, Georg Langs", "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to\n  Guide Marker Discovery", "comments": "To be published in the proceedings of the international conference on\n  Information Processing in Medical Imaging (IPMI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining models that capture imaging markers relevant for disease\nprogression and treatment monitoring is challenging. Models are typically based\non large amounts of data with annotated examples of known markers aiming at\nautomating detection. High annotation effort and the limitation to a vocabulary\nof known markers limit the power of such approaches. Here, we perform\nunsupervised learning to identify anomalies in imaging data as candidates for\nmarkers. We propose AnoGAN, a deep convolutional generative adversarial network\nto learn a manifold of normal anatomical variability, accompanying a novel\nanomaly scoring scheme based on the mapping from image space to a latent space.\nApplied to new data, the model labels anomalies, and scores image patches\nindicating their fit into the learned distribution. Results on optical\ncoherence tomography images of the retina demonstrate that the approach\ncorrectly identifies anomalous images, such as images containing retinal fluid\nor hyperreflective foci.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 08:27:05 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Schlegl", "Thomas", ""], ["Seeb\u00f6ck", "Philipp", ""], ["Waldstein", "Sebastian M.", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Langs", "Georg", ""]]}, {"id": "1703.05990", "submitter": "P\\'eter B\\'andi", "authors": "P\\'eter B\\'andi, Rob van de Loo, Milad Intezar, Daan Geijs, Francesco\n  Ciompi, Bram van Ginneken, Jeroen van der Laak, Geert Litjens", "title": "Comparison of Different Methods for Tissue Segmentation in\n  Histopathological Whole-Slide Images", "comments": "Accepted for poster presentation at the IEEE International Symposium\n  on Biomedical Imaging (ISBI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue segmentation is an important pre-requisite for efficient and accurate\ndiagnostics in digital pathology. However, it is well known that whole-slide\nscanners can fail in detecting all tissue regions, for example due to the\ntissue type, or due to weak staining because their tissue detection algorithms\nare not robust enough. In this paper, we introduce two different convolutional\nneural network architectures for whole slide image segmentation to accurately\nidentify the tissue sections. We also compare the algorithms to a published\ntraditional method. We collected 54 whole slide images with differing stains\nand tissue types from three laboratories to validate our algorithms. We show\nthat while the two methods do not differ significantly they outperform their\ntraditional counterpart (Jaccard index of 0.937 and 0.929 vs. 0.870, p < 0.01).\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 12:32:25 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 17:46:32 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["B\u00e1ndi", "P\u00e9ter", ""], ["van de Loo", "Rob", ""], ["Intezar", "Milad", ""], ["Geijs", "Daan", ""], ["Ciompi", "Francesco", ""], ["van Ginneken", "Bram", ""], ["van der Laak", "Jeroen", ""], ["Litjens", "Geert", ""]]}, {"id": "1703.06000", "submitter": "Christoph Baur", "authors": "Christoph Baur, Shadi Albarqouni and Nassir Navab", "title": "Semi-Supervised Deep Learning for Fully Convolutional Networks", "comments": "9 pages, 6 figures", "journal-ref": "Medical Image Computing and Computer Assisted Intervention (MICCAI\n  2017)", "doi": "10.1007/978-3-319-66179-7_36", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning usually requires large amounts of labeled training data, but\nannotating data is costly and tedious. The framework of semi-supervised\nlearning provides the means to use both labeled data and arbitrary amounts of\nunlabeled data for training. Recently, semi-supervised deep learning has been\nintensively studied for standard CNN architectures. However, Fully\nConvolutional Networks (FCNs) set the state-of-the-art for many image\nsegmentation tasks. To the best of our knowledge, there is no existing\nsemi-supervised learning method for such FCNs yet. We lift the concept of\nauxiliary manifold embedding for semi-supervised learning to FCNs with the help\nof Random Feature Embedding. In our experiments on the challenging task of MS\nLesion Segmentation, we leverage the proposed framework for the purpose of\ndomain adaptation and report substantial improvements over the baseline model.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 13:14:36 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 12:02:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Baur", "Christoph", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "1703.06003", "submitter": "Huy Phan", "authors": "Huy Q. Phan, Hongbo Fu, and Antoni B. Chan", "title": "Color Orchestra: Ordering Color Palettes for Interpolation and\n  Prediction", "comments": "IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color theme or color palette can deeply influence the quality and the feeling\nof a photograph or a graphical design. Although color palettes may come from\ndifferent sources such as online crowd-sourcing, photographs and graphical\ndesigns, in this paper, we consider color palettes extracted from fine art\ncollections, which we believe to be an abundant source of stylistic and unique\ncolor themes. We aim to capture color styles embedded in these collections by\nmeans of statistical models and to build practical applications upon these\nmodels. As artists often use their personal color themes in their paintings,\nmaking these palettes appear frequently in the dataset, we employed density\nestimation to capture the characteristics of palette data. Via density\nestimation, we carried out various predictions and interpolations on palettes,\nwhich led to promising applications such as photo-style exploration, real-time\ncolor suggestion, and enriched photo recolorization. It was, however,\nchallenging to apply density estimation to palette data as palettes often come\nas unordered sets of colors, which make it difficult to use conventional\nmetrics on them. To this end, we developed a divide-and-conquer sorting\nalgorithm to rearrange the colors in the palettes in a coherent order, which\nallows meaningful interpolation between color palettes. To confirm the\nperformance of our model, we also conducted quantitative experiments on\ndatasets of digitized paintings collected from the Internet and received\nfavorable results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 13:25:49 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Phan", "Huy Q.", ""], ["Fu", "Hongbo", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1703.06029", "submitter": "Bo Dai", "authors": "Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin", "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN", "comments": "accepted in ICCV2017 as an Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the substantial progress in recent years, the image captioning\ntechniques are still far from being perfect.Sentences produced by existing\nmethods, e.g. those based on RNNs, are often overly rigid and lacking in\nvariability. This issue is related to a learning principle widely used in\npractice, that is, to maximize the likelihood of training samples. This\nprinciple encourages high resemblance to the \"ground-truth\" captions while\nsuppressing other reasonable descriptions. Conventional evaluation metrics,\ne.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we\nexplore an alternative approach, with the aim to improve the naturalness and\ndiversity -- two essential properties of human expression. Specifically, we\npropose a new framework based on Conditional Generative Adversarial Networks\n(CGAN), which jointly learns a generator to produce descriptions conditioned on\nimages and an evaluator to assess how well a description fits the visual\ncontent. It is noteworthy that training a sequence generator is nontrivial. We\novercome the difficulty by Policy Gradient, a strategy stemming from\nReinforcement Learning, which allows the generator to receive early feedback\nalong the way. We tested our method on two large datasets, where it performed\ncompetitively against real people in our user study and outperformed other\nmethods on various tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 14:33:41 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 07:08:41 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 05:02:36 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Dai", "Bo", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""], ["Lin", "Dahua", ""]]}, {"id": "1703.06066", "submitter": "Fred Ngol\\`e", "authors": "F. M. Ngol\\`e Mboula and J.-L. Starck", "title": "PSF field learning based on Optimal Transport Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: in astronomy, observing large fractions of the sky within a\nreasonable amount of time implies using large field-of-view (fov) optical\ninstruments that typically have a spatially varying Point Spread Function\n(PSF). Depending on the scientific goals, galaxies images need to be corrected\nfor the PSF whereas no direct measurement of the PSF is available. Aims: given\na set of PSFs observed at random locations, we want to estimate the PSFs at\ngalaxies locations for shapes measurements correction. Contributions: we\npropose an interpolation framework based on Sliced Optimal Transport. A\nnon-linear dimension reduction is first performed based on local pairwise\napproximated Wasserstein distances. A low dimensional representation of the\nunknown PSFs is then estimated, which in turn is used to derive representations\nof those PSFs in the Wasserstein metric. Finally, the interpolated PSFs are\ncalculated as approximated Wasserstein barycenters. Results: the proposed\nmethod was tested on simulated monochromatic PSFs of the Euclid space mission\ntelescope (to be launched in 2020). It achieves a remarkable accuracy in terms\nof pixels values and shape compared to standard methods such as Inverse\nDistance Weighting or Radial Basis Function based interpolation methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 16:09:40 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Mboula", "F. M. Ngol\u00e8", ""], ["Starck", "J. -L.", ""]]}, {"id": "1703.06151", "submitter": "Sheng Zou", "authors": "Sheng Zou, Hao Sun, Alina Zare", "title": "Hyperspectral Unmixing with Endmember Variability using Semi-supervised\n  Partial Membership Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-supervised Partial Membership Latent Dirichlet Allocation approach is\ndeveloped for hyperspectral unmixing and endmember estimation while accounting\nfor spectral variability and spatial information. Partial Membership Latent\nDirichlet Allocation is an effective approach for spectral unmixing while\nrepresenting spectral variability and leveraging spatial information. In this\nwork, we extend Partial Membership Latent Dirichlet Allocation to incorporate\nany available (imprecise) label information to help guide unmixing.\nExperimental results on two hyperspectral datasets show that the proposed\nsemi-supervised PM-LDA can yield improved hyperspectral unmixing and endmember\nestimation results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 18:13:59 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Zou", "Sheng", ""], ["Sun", "Hao", ""], ["Zare", "Alina", ""]]}, {"id": "1703.06189", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, Ram Nevatia", "title": "TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals", "comments": "ICCV 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Action Proposal (TAP) generation is an important problem, as fast\nand accurate extraction of semantically important (e.g. human actions) segments\nfrom untrimmed videos is an important step for large-scale video analysis. We\npropose a novel Temporal Unit Regression Network (TURN) model. There are two\nsalient aspects of TURN: (1) TURN jointly predicts action proposals and refines\nthe temporal boundaries by temporal coordinate regression; (2) Fast computation\nis enabled by unit feature reuse: a long untrimmed video is decomposed into\nvideo units, which are reused as basic building blocks of temporal proposals.\nTURN outperforms the state-of-the-art methods under average recall (AR) by a\nlarge margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames\nper second (FPS) on a TITAN X GPU. We further apply TURN as a proposal\ngeneration stage for existing temporal action localization pipelines, it\noutperforms state-of-the-art performance on THUMOS-14 and ActivityNet.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 20:24:32 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 19:21:31 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Gao", "Jiyang", ""], ["Yang", "Zhenheng", ""], ["Sun", "Chen", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "1703.06211", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu,\n  Yichen Wei", "title": "Deformable Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are inherently limited to model\ngeometric transformations due to the fixed geometric structures in its building\nmodules. In this work, we introduce two new modules to enhance the\ntransformation modeling capacity of CNNs, namely, deformable convolution and\ndeformable RoI pooling. Both are based on the idea of augmenting the spatial\nsampling locations in the modules with additional offsets and learning the\noffsets from target tasks, without additional supervision. The new modules can\nreadily replace their plain counterparts in existing CNNs and can be easily\ntrained end-to-end by standard back-propagation, giving rise to deformable\nconvolutional networks. Extensive experiments validate the effectiveness of our\napproach on sophisticated vision tasks of object detection and semantic\nsegmentation. The code would be released.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 21:58:20 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 12:39:32 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 10:08:50 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Dai", "Jifeng", ""], ["Qi", "Haozhi", ""], ["Xiong", "Yuwen", ""], ["Li", "Yi", ""], ["Zhang", "Guodong", ""], ["Hu", "Han", ""], ["Wei", "Yichen", ""]]}, {"id": "1703.06217", "submitter": "Mason McGill", "authors": "Mason McGill and Pietro Perona", "title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks", "comments": "ICML 2017. Code at https://github.com/MasonMcGill/multipath-nn Video\n  abstract at https://youtu.be/NHQsDaycwyQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and systematically evaluate three strategies for training\ndynamically-routed artificial neural networks: graphs of learned\ntransformations through which different input signals may take different paths.\nThough some approaches have advantages over others, the resulting networks are\noften qualitatively similar. We find that, in dynamically-routed networks\ntrained to classify images, layers and branches become specialized to process\ndistinct categories of images. Additionally, given a fixed computational\nbudget, dynamically-routed networks tend to perform better than comparable\nstatically-routed networks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 23:52:14 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 22:14:36 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["McGill", "Mason", ""], ["Perona", "Pietro", ""]]}, {"id": "1703.06233", "submitter": "Arun Mallya", "authors": "Arun Mallya and Svetlana Lazebnik", "title": "Recurrent Models for Situation Recognition", "comments": "To appear at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes Recurrent Neural Network (RNN) models to predict\nstructured 'image situations' -- actions and noun entities fulfilling semantic\nroles related to the action. In contrast to prior work relying on Conditional\nRandom Fields (CRFs), we use a specialized action prediction network followed\nby an RNN for noun prediction. Our system obtains state-of-the-art accuracy on\nthe challenging recent imSitu dataset, beating CRF-based models, including ones\ntrained with additional data. Further, we show that specialized features\nlearned from situation prediction can be transferred to the task of image\ncaptioning to more accurately describe human-object interactions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 02:00:22 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 17:03:56 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Mallya", "Arun", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1703.06241", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz, Andrew\n  Rabinovich", "title": "RoomNet: End-to-End Room Layout Estimation", "comments": "accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the task of room layout estimation from a monocular RGB\nimage. Prior works break the problem into two sub-tasks: semantic segmentation\nof floor, walls, ceiling to produce layout hypotheses, followed by an iterative\noptimization step to rank these hypotheses. In contrast, we adopt a more direct\nformulation of this problem as one of estimating an ordered set of room layout\nkeypoints. The room layout and the corresponding segmentation is completely\nspecified given the locations of these ordered keypoints. We predict the\nlocations of the room layout keypoints using RoomNet, an end-to-end trainable\nencoder-decoder network. On the challenging benchmark datasets Hedau and LSUN,\nwe achieve state-of-the-art performance along with 200x to 600x speedup\ncompared to the most recent work. Additionally, we present optional extensions\nto the RoomNet architecture such as including recurrent computations and memory\nunits to refine the keypoint locations under the same parametric capacity.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 03:35:56 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 19:58:46 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Badrinarayanan", "Vijay", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1703.06246", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Ian Reid", "title": "Towards Context-aware Interaction Recognition", "comments": "Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing how objects interact with each other is a crucial task in visual\nrecognition. If we define the context of the interaction to be the objects\ninvolved, then most current methods can be categorized as either: (i) training\na single classifier on the combination of the interaction and its context; or\n(ii) aiming to recognize the interaction independently of its explicit context.\nBoth methods suffer limitations: the former scales poorly with the number of\ncombinations and fails to generalize to unseen combinations, while the latter\noften leads to poor interaction recognition performance due to the difficulty\nof designing a context-independent interaction classifier. To mitigate those\ndrawbacks, this paper proposes an alternative, context-aware interaction\nrecognition framework. The key to our method is to explicitly construct an\ninteraction classifier which combines the context, and the interaction. The\ncontext is encoded via word2vec into a semantic space, and is used to derive a\nclassification result for the interaction.\n  The proposed method still builds one classifier for one interaction (as per\ntype (ii) above), but the classifier built is adaptive to context via weights\nwhich are context dependent. The benefit of using the semantic space is that it\nnaturally leads to zero-shot generalizations in which semantically similar\ncontexts (subjectobject pairs) can be recognized as suitable contexts for an\ninteraction, even if they were not observed in the training set.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 03:59:21 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 02:26:11 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 23:55:42 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Zhuang", "Bohan", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1703.06256", "submitter": "Chunde Huang", "authors": "Chunde Huang, Jiaxiang Huang", "title": "A Fast HOG Descriptor Using Lookup Table and Integral Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The histogram of oriented gradients (HOG) is a widely used feature descriptor\nin computer vision for the purpose of object detection. In the paper, a\nmodified HOG descriptor is described, it uses a lookup table and the method of\nintegral image to speed up the detection performance by a factor of 5~10. By\nexploiting the special hardware features of a given platform(e.g. a digital\nsignal processor), further improvement can be made to the HOG descriptor in\norder to have real-time object detection and tracking.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 04:58:32 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Huang", "Chunde", ""], ["Huang", "Jiaxiang", ""]]}, {"id": "1703.06260", "submitter": "YangQuan Chen Prof.", "authors": "Qi Yang, Yanzhu Zhang, Tiebiao Zhao, YangQuan Chen", "title": "Single image super-resolution using self-optimizing mask via\n  fractional-order gradient interpolation and reconstruction", "comments": "24 pages, 13 figures, it is to appear in ISA Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution using self-optimizing mask via fractional-order\ngradient interpolation and reconstruction aims to recover detailed information\nfrom low-resolution images and reconstruct them into high-resolution images.\nDue to the limited amount of data and information retrieved from low-resolution\nimages, it is difficult to restore clear, artifact-free images, while still\npreserving enough structure of the image such as the texture. This paper\npresents a new single image super-resolution method which is based on adaptive\nfractional-order gradient interpolation and reconstruction. The interpolated\nimage gradient via optimal fractional-order gradient is first constructed\naccording to the image similarity and afterwards the minimum energy function is\nemployed to reconstruct the final high-resolution image. Fractional-order\ngradient based interpolation methods provide an additional degree of freedom\nwhich helps optimize the implementation quality due to the fact that an extra\nfree parameter $\\alpha$-order is being used. The proposed method is able to\nproduce a rich texture detail while still being able to maintain structural\nsimilarity even under large zoom conditions. Experimental results show that the\nproposed method performs better than current single image super-resolution\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 06:57:12 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Yang", "Qi", ""], ["Zhang", "Yanzhu", ""], ["Zhao", "Tiebiao", ""], ["Chen", "YangQuan", ""]]}, {"id": "1703.06283", "submitter": "Shiyu Huang", "authors": "Shiyu Huang and Deva Ramanan", "title": "Expecting the Unexpected: Training Detectors for Unusual Pedestrians\n  with Adversarial Imposters", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous vehicles become an every-day reality, high-accuracy pedestrian\ndetection is of paramount practical importance. Pedestrian detection is a\nhighly researched topic with mature methods, but most datasets focus on common\nscenes of people engaged in typical walking poses on sidewalks. But performance\nis most crucial for dangerous scenarios, such as children playing in the street\nor people using bicycles/skateboards in unexpected ways. Such \"in-the-tail\"\ndata is notoriously hard to observe, making both training and testing\ndifficult. To analyze this problem, we have collected a novel annotated dataset\nof dangerous scenarios called the Precarious Pedestrian dataset. Even given a\ndedicated collection effort, it is relatively small by contemporary standards\n(around 1000 images). To allow for large-scale data-driven learning, we explore\nthe use of synthetic data generated by a game engine. A significant challenge\nis selected the right \"priors\" or parameters for synthesis: we would like\nrealistic data with poses and object configurations that mimic true Precarious\nPedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a\nmassive amount of synthetic data and train a discriminative classifier to\nselect a realistic subset, which we deem the Adversarial Imposters. We\ndemonstrate that this simple pipeline allows one to synthesize realistic\ntraining data by making use of rendering/animation engines within a GAN\nframework. Interestingly, we also demonstrate that such data can be used to\nrank algorithms, suggesting that Adversarial Imposters can also be used for\n\"in-the-tail\" validation at test-time, a notoriously difficult challenge for\nreal-world deployment.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 10:52:53 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:59:25 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Huang", "Shiyu", ""], ["Ramanan", "Deva", ""]]}, {"id": "1703.06339", "submitter": "Hongzhi Li", "authors": "Hongzhi Li, Joseph G. Ellis, Lei Zhang, Shih-Fu Chang", "title": "PatternNet: Visual Pattern Mining with Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual patterns represent the discernible regularity in the visual world.\nThey capture the essential nature of visual objects or scenes. Understanding\nand modeling visual patterns is a fundamental problem in visual recognition\nthat has wide ranging applications. In this paper, we study the problem of\nvisual pattern mining and propose a novel deep neural network architecture\ncalled PatternNet for discovering these patterns that are both discriminative\nand representative. The proposed PatternNet leverages the filters in the last\nconvolution layer of a convolutional neural network to find locally consistent\nvisual patches, and by combining these filters we can effectively discover\nunique visual patterns. In addition, PatternNet can discover visual patterns\nefficiently without performing expensive image patch sampling, and this\nadvantage provides an order of magnitude speedup compared to most other\napproaches. We evaluate the proposed PatternNet subjectively by showing\nrandomly selected visual patterns which are discovered by our method and\nquantitatively by performing image classification with the identified visual\npatterns and comparing our performance with the current state-of-the-art. We\nalso directly evaluate the quality of the discovered visual patterns by\nleveraging the identified patterns as proposed objects in an image and compare\nwith other relevant methods. Our proposed network and procedure, PatterNet, is\nable to outperform competing methods for the tasks described.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 19:21:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 22:42:07 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Li", "Hongzhi", ""], ["Ellis", "Joseph G.", ""], ["Zhang", "Lei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1703.06370", "submitter": "Li Sun Mr", "authors": "Li Sun and Cheng Zhao and Rustam Stolkin", "title": "Weakly-supervised DCNN for RGB-D Object Recognition in Real-World\n  Applications Which Lack Large-scale Annotated Training Data", "comments": "8 pages, 5 figures, submitted to conference", "journal-ref": null, "doi": "10.1109/JSEN.2018.2888815", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of RGBD object recognition in real-world\napplications, where large amounts of annotated training data are typically\nunavailable. To overcome this problem, we propose a novel, weakly-supervised\nlearning architecture (DCNN-GPC) which combines parametric models (a pair of\nDeep Convolutional Neural Networks (DCNN) for RGB and D modalities) with\nnon-parametric models (Gaussian Process Classification). Our system is\ninitially trained using a small amount of labeled data, and then automatically\nprop- agates labels to large-scale unlabeled data. We first run 3D- based\nobjectness detection on RGBD videos to acquire many unlabeled object proposals,\nand then employ DCNN-GPC to label them. As a result, our multi-modal DCNN can\nbe trained end-to-end using only a small amount of human annotation. Finally,\nour 3D-based objectness detection and multi-modal DCNN are integrated into a\nreal-time detection and recognition pipeline. In our approach, bounding-box\nannotations are not required and boundary-aware detection is achieved. We also\npropose a novel way to pretrain a DCNN for the depth modality, by training on\nvirtual depth images projected from CAD models. We pretrain our multi-modal\nDCNN on public 3D datasets, achieving performance comparable to\nstate-of-the-art methods on Washington RGBS Dataset. We then finetune the\nnetwork by further training on a small amount of annotated data from our novel\ndataset of industrial objects (nuclear waste simulants). Our weakly supervised\napproach has demonstrated to be highly effective in solving a novel RGBD object\nrecognition application which lacks of human annotations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 00:29:35 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Sun", "Li", ""], ["Zhao", "Cheng", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1703.06376", "submitter": "Ehab Salahat Mr", "authors": "Ehab Salahat and Murad Qasaimeh", "title": "Recent Advances in Features Extraction and Description Algorithms: A\n  Comprehensive Survey", "comments": "Annual IEEE Industrial Electronics Societys 18th International Conf.\n  on Industrial Technology (ICIT), 22-25 March, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is one of the most active research fields in information\ntechnology today. Giving machines and robots the ability to see and comprehend\nthe surrounding world at the speed of sight creates endless potential\napplications and opportunities. Feature detection and description algorithms\ncan be indeed considered as the retina of the eyes of such machines and robots.\nHowever, these algorithms are typically computationally intensive, which\nprevents them from achieving the speed of sight real-time performance. In\naddition, they differ in their capabilities and some may favor and work better\ngiven a specific type of input compared to others. As such, it is essential to\ncompactly report their pros and cons as well as their performances and recent\nadvances. This paper is dedicated to provide a comprehensive overview on the\nstate-of-the-art and recent advances in feature detection and description\nalgorithms. Specifically, it starts by overviewing fundamental concepts. It\nthen compares, reports and discusses their performance and capabilities. The\nMaximally Stable Extremal Regions algorithm and the Scale Invariant Feature\nTransform algorithms, being two of the best of their type, are selected to\nreport their recent algorithmic derivatives.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 01:00:27 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Salahat", "Ehab", ""], ["Qasaimeh", "Murad", ""]]}, {"id": "1703.06380", "submitter": "Shichao Yang", "authors": "Shichao Yang, Sebastian Scherer", "title": "Direct Monocular Odometry Using Points and Lines", "comments": "ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most visual odometry algorithm for a monocular camera focuses on points,\neither by feature matching, or direct alignment of pixel intensity, while\nignoring a common but important geometry entity: edges. In this paper, we\npropose an odometry algorithm that combines points and edges to benefit from\nthe advantages of both direct and feature based methods. It works better in\ntexture-less environments and is also more robust to lighting changes and fast\nmotion by increasing the convergence basin. We maintain a depth map for the\nkeyframe then in the tracking part, the camera pose is recovered by minimizing\nboth the photometric error and geometric error to the matched edge in a\nprobabilistic framework. In the mapping part, edge is used to speed up and\nincrease stereo matching accuracy. On various public datasets, our algorithm\nachieves better or comparable performance than state-of-the-art monocular\nodometry methods. In some challenging texture-less environments, our algorithm\nreduces the state estimation error over 50%.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 01:59:53 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Yang", "Shichao", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1703.06389", "submitter": "Lu Jiang", "authors": "Jiang Lu, Jin Li, Ziang Yan, Changshui Zhang", "title": "Zero-Shot Learning by Generating Pseudo Feature Representations", "comments": "9 pages", "journal-ref": "Pattern Recognition, Volume 80, August 2018, Pages 129-142", "doi": "10.1016/j.patcog.2018.03.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is a challenging task aiming at recognizing novel\nclasses without any training instances. In this paper we present a simple but\nhigh-performance ZSL approach by generating pseudo feature representations\n(GPFR). Given the dataset of seen classes and side information of unseen\nclasses (e.g. attributes), we synthesize feature-level pseudo representations\nfor novel concepts, which allows us access to the formulation of unseen class\npredictor. Firstly we design a Joint Attribute Feature Extractor (JAFE) to\nacquire understandings about attributes, then construct a cognitive repository\nof attributes filtered by confidence margins, and finally generate pseudo\nfeature representations using a probability based sampling strategy to\nfacilitate subsequent training process of class predictor. We demonstrate the\neffectiveness in ZSL settings and the extensibility in supervised recognition\nscenario of our method on a synthetic colored MNIST dataset (C-MNIST). For\nseveral popular ZSL benchmark datasets, our approach also shows compelling\nresults on zero-shot recognition task, especially leading to tremendous\nimprovement to state-of-the-art mAP on zero-shot retrieval task.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 04:14:27 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lu", "Jiang", ""], ["Li", "Jin", ""], ["Yan", "Ziang", ""], ["Zhang", "Changshui", ""]]}, {"id": "1703.06408", "submitter": "Andreas K\\\"olsch", "authors": "Andreas K\\\"olsch, Muhammad Zeshan Afzal, Marcus Liwicki", "title": "Multilevel Context Representation for Improving Object Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2017.322", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the combined usage of low- and high-level blocks of\nconvolutional neural networks (CNNs) for improving object recognition. While\nrecent research focused on either propagating the context from all layers, e.g.\nResNet, (including the very low-level layers) or having multiple loss layers\n(e.g. GoogLeNet), the importance of the features close to the higher layers is\nignored. This paper postulates that the use of context closer to the high-level\nlayers provides the scale and translation invariance and works better than\nusing the top layer only. In particular, we extend AlexNet and GoogLeNet by\nadditional connections in the top $n$ layers. In order to demonstrate the\neffectiveness of the proposed approach, we evaluated it on the standard\nImageNet task. The relative reduction of the classification error is around\n1-2% without affecting the computational cost. Furthermore, we show that this\napproach is orthogonal to typical test data augmentation techniques, as\nrecently introduced by Szegedy et al. (leading to a runtime reduction of 144\nduring test time).\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 09:52:28 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["K\u00f6lsch", "Andreas", ""], ["Afzal", "Muhammad Zeshan", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1703.06412", "submitter": "Ayushman Dash", "authors": "Ayushman Dash, John Cristian Borges Gamboa, Sheraz Ahmed, Marcus\n  Liwicki, Muhammad Zeshan Afzal", "title": "TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Text Conditioned Auxiliary Classifier Generative\nAdversarial Network, (TAC-GAN) a text to image Generative Adversarial Network\n(GAN) for synthesizing images from their text descriptions. Former approaches\nhave tried to condition the generative process on the textual data; but allying\nit to the usage of class information, known to diversify the generated samples\nand improve their structural coherence, has not been explored. We trained the\npresented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the\ndiscriminability of the generated images with Inception-Score, as well as their\ndiversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our\napproach outperforms the state-of-the-art models, i.e., its inception score is\n3.45, corresponding to a relative increase of 7.8% compared to the recently\nintroduced StackGan. A comparison of the mean MS-SSIM scores of the training\nand generated samples per class shows that our approach is able to generate\nhighly diverse images with an average MS-SSIM of 0.14 over all generated\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 10:07:58 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 11:29:21 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Dash", "Ayushman", ""], ["Gamboa", "John Cristian Borges", ""], ["Ahmed", "Sheraz", ""], ["Liwicki", "Marcus", ""], ["Afzal", "Muhammad Zeshan", ""]]}, {"id": "1703.06418", "submitter": "John Lambert", "authors": "Assaf Hoogi, John W. Lambert, Yefeng Zheng, Dorin Comaniciu, Daniel L.\n  Rubin", "title": "A Fully-Automated Pipeline for Detection and Segmentation of Liver\n  Lesions and Pathological Lymph Nodes", "comments": "Workshop on Machine Learning in Healthcare, Neural Information\n  Processing Systems (NIPS). Barcelona, Spain, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully-automated method for accurate and robust detection and\nsegmentation of potentially cancerous lesions found in the liver and in lymph\nnodes. The process is performed in three steps, including organ detection,\nlesion detection and lesion segmentation. Our method applies machine learning\ntechniques such as marginal space learning and convolutional neural networks,\nas well as active contour models. The method proves to be robust in its\nhandling of extremely high lesion diversity. We tested our method on volumetric\ncomputed tomography (CT) images, including 42 volumes containing liver lesions\nand 86 volumes containing 595 pathological lymph nodes. Preliminary results\nunder 10-fold cross validation show that for both the liver lesions and the\nlymph nodes, a total detection sensitivity of 0.53 and average Dice score of\n$0.71 \\pm 0.15$ for segmentation were obtained.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 10:43:28 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Hoogi", "Assaf", ""], ["Lambert", "John W.", ""], ["Zheng", "Yefeng", ""], ["Comaniciu", "Dorin", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1703.06452", "submitter": "Ronald Kemker", "authors": "Ronald Kemker and Carl Salvaggio and Christopher Kanan", "title": "Algorithms for Semantic Segmentation of Multispectral Remote Sensing\n  Imagery using Deep Learning", "comments": "45 pages", "journal-ref": "Published in ISPRS Journal of Photogrammetry and Remote Sensing\n  2018", "doi": "10.1016/j.isprsjprs.2018.04.014", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have been used to achieve\nstate-of-the-art performance on many computer vision tasks (e.g., object\nrecognition, object detection, semantic segmentation) thanks to a large\nrepository of annotated image data. Large labeled datasets for other sensor\nmodalities, e.g., multispectral imagery (MSI), are not available due to the\nlarge cost and manpower required. In this paper, we adapt state-of-the-art DCNN\nframeworks in computer vision for semantic segmentation for MSI imagery. To\novercome label scarcity for MSI data, we substitute real MSI for generated\nsynthetic MSI in order to initialize a DCNN framework. We evaluate our network\ninitialization scheme on the new RIT-18 dataset that we present in this paper.\nThis dataset contains very-high resolution MSI collected by an unmanned\naircraft system. The models initialized with synthetic imagery were less prone\nto over-fitting and provide a state-of-the-art baseline for future work.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 15:21:32 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 13:45:12 GMT"}, {"version": "v3", "created": "Tue, 1 May 2018 20:59:31 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Kemker", "Ronald", ""], ["Salvaggio", "Carl", ""], ["Kanan", "Christopher", ""]]}, {"id": "1703.06492", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Modar Alfadly, Bernard Ghanem", "title": "VQABQ: Visual Question Answering by Basic Questions", "comments": "Accepted by CVPR 2017 VQA Challenge Workshop. (Tables updated)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking an image and question as the input of our method, it can output the\ntext-based answer of the query question about the given image, so called Visual\nQuestion Answering (VQA). There are two main modules in our algorithm. Given a\nnatural language question about an image, the first module takes the question\nas input and then outputs the basic questions of the main given question. The\nsecond module takes the main question, image and these basic questions as input\nand then outputs the text-based answer of the main question. We formulate the\nbasic questions generation problem as a LASSO optimization problem, and also\npropose a criterion about how to exploit these basic questions to help answer\nmain question. Our method is evaluated on the challenging VQA dataset and\nyields state-of-the-art accuracy, 60.34% in open-ended task.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 19:14:55 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 22:40:19 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Alfadly", "Modar", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1703.06520", "submitter": "Baoguang Shi", "authors": "Baoguang Shi, Xiang Bai, Serge Belongie", "title": "Detecting Oriented Text in Natural Images by Linking Segments", "comments": "To Appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art text detection methods are specific to horizontal Latin\ntext and are not fast enough for real-time applications. We introduce Segment\nLinking (SegLink), an oriented text detection method. The main idea is to\ndecompose text into two locally detectable elements, namely segments and links.\nA segment is an oriented box covering a part of a word or text line; A link\nconnects two adjacent segments, indicating that they belong to the same word or\ntext line. Both elements are detected densely at multiple scales by an\nend-to-end trained, fully-convolutional neural network. Final detections are\nproduced by combining segments connected by links. Compared with previous\nmethods, SegLink improves along the dimensions of accuracy, speed, and ease of\ntraining. It achieves an f-measure of 75.0% on the standard ICDAR 2015\nIncidental (Challenge 4) benchmark, outperforming the previous best by a large\nmargin. It runs at over 20 FPS on 512x512 images. Moreover, without\nmodification, SegLink is able to detect long lines of non-Latin text, such as\nChinese.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 21:43:41 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 03:18:55 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 17:40:43 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Belongie", "Serge", ""]]}, {"id": "1703.06527", "submitter": "Yuanwei Wu", "authors": "Yuanwei Wu, Yao Sui and Guanghui Wang", "title": "Vision-based Real-Time Aerial Object Localization and Tracking for UAV\n  Sensing System", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the problem of vision-based obstacle detection and\ntracking for unmanned aerial vehicle navigation. A real-time object\nlocalization and tracking strategy from monocular image sequences is developed\nby effectively integrating the object detection and tracking into a dynamic\nKalman model. At the detection stage, the object of interest is automatically\ndetected and localized from a saliency map computed via the image background\nconnectivity cue at each frame; at the tracking stage, a Kalman filter is\nemployed to provide a coarse prediction of the object state, which is further\nrefined via a local detector incorporating the saliency map and the temporal\ninformation between two consecutive frames. Compared to existing methods, the\nproposed approach does not require any manual initialization for tracking, runs\nmuch faster than the state-of-the-art trackers of its kind, and achieves\ncompetitive tracking performance on a large number of image sequences.\nExtensive experiments demonstrate the effectiveness and superior performance of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 22:19:20 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Wu", "Yuanwei", ""], ["Sui", "Yao", ""], ["Wang", "Guanghui", ""]]}, {"id": "1703.06554", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla and Sudharshan Suresh and R. Venkatesh Babu", "title": "Object category understanding via eye fixations on freehand sketches", "comments": "Accepted for publication in Transactions on Image Processing\n  (http://ieeexplore.ieee.org/document/7866001/)", "journal-ref": null, "doi": "10.1109/TIP.2017.2675539", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of eye gaze fixations on photographic images is an active research\narea. In contrast, the image subcategory of freehand sketches has not received\nas much attention for such studies. In this paper, we analyze the results of a\nfree-viewing gaze fixation study conducted on 3904 freehand sketches\ndistributed across 160 object categories. Our analysis shows that fixation\nsequences exhibit marked consistency within a sketch, across sketches of a\ncategory and even across suitably grouped sets of categories. This multi-level\nconsistency is remarkable given the variability in depiction and extreme image\ncontent sparsity that characterizes hand-drawn object sketches. In our paper,\nwe show that the multi-level consistency in the fixation data can be exploited\nto (a) predict a test sketch's category given only its fixation sequence and\n(b) build a computational model which predicts part-labels underlying fixations\non objects. We hope that our findings motivate the community to deem\nsketch-like representations worthy of gaze-based studies vis-a-vis photographic\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 01:13:33 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Suresh", "Sudharshan", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1703.06585", "submitter": "Abhishek Das", "authors": "Abhishek Das, Satwik Kottur, Jos\\'e M. F. Moura, Stefan Lee, Dhruv\n  Batra", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement\n  Learning", "comments": "11 pages, 4 figures, 2 tables, webpage: http://visualdialog.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first goal-driven training for visual question answering and\ndialog agents. Specifically, we pose a cooperative 'image guessing' game\nbetween two agents -- Qbot and Abot -- who communicate in natural language\ndialog so that Qbot can select an unseen image from a lineup of images. We use\ndeep reinforcement learning (RL) to learn the policies of these agents\nend-to-end -- from pixels to multi-agent multi-round dialog to game reward.\n  We demonstrate two experimental results.\n  First, as a 'sanity check' demonstration of pure RL (from scratch), we show\nresults on a synthetic world, where the agents communicate in ungrounded\nvocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find\nthat two bots invent their own communication protocol and start using certain\nsymbols to ask/answer about certain visual attributes (shape/color/style).\nThus, we demonstrate the emergence of grounded language and communication among\n'visual' dialog agents with no human supervision.\n  Second, we conduct large-scale real-image experiments on the VisDial dataset,\nwhere we pretrain with supervised dialog data and show that the RL 'fine-tuned'\nagents significantly outperform SL agents. Interestingly, the RL Qbot learns to\nask questions that Abot is good at, ultimately resulting in more informative\ndialog and a better team.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 03:50:57 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 17:41:23 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Das", "Abhishek", ""], ["Kottur", "Satwik", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""]]}, {"id": "1703.06618", "submitter": "Yuting Hu", "authors": "Yuting Hu, Liang Zheng, Yi Yang, and Yongfeng Huang", "title": "Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a new large-scale dataset for weakly supervised\ncross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia,\nNUS Wide and Flickr30k, have two major limitations. First, these datasets are\nlacking in content diversity, i.e., only some pre-defined classes are covered.\nSecond, texts in these datasets are written in well-organized language, leading\nto inconsistency with realistic applications. To overcome these drawbacks, the\nproposed Twitter100k dataset is characterized by two aspects: 1) it has 100,000\nimage-text pairs randomly crawled from Twitter and thus has no constraint in\nthe image categories; 2) text in Twitter100k is written in informal language by\nthe users.\n  Since strongly supervised methods leverage the class labels that may be\nmissing in practice, this paper focuses on weakly supervised learning for\ncross-media retrieval, in which only text-image pairs are exploited during\ntraining. We extensively benchmark the performance of four subspace learning\nmethods and three variants of the Correspondence AutoEncoder, along with\nvarious text features on Wikipedia, Flickr30k and Twitter100k. Novel insights\nare provided. As a minor contribution, inspired by the characteristic of\nTwitter100k, we propose an OCR-based cross-media retrieval method. In\nexperiment, we show that the proposed OCR-based method improves the baseline\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 06:56:33 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Hu", "Yuting", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Huang", "Yongfeng", ""]]}, {"id": "1703.06676", "submitter": "Hao Dong", "authors": "Hao Dong, Jingqing Zhang, Douglas McIlwraith, Yike Guo", "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation", "comments": "International Conference on Image Processing (ICIP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating information between text and image is a fundamental problem in\nartificial intelligence that connects natural language processing and computer\nvision. In the past few years, performance in image caption generation has seen\nsignificant improvement through the adoption of recurrent neural networks\n(RNN). Meanwhile, text-to-image generation begun to generate plausible images\nusing datasets of specific categories like birds and flowers. We've even seen\nimage generation from multi-category datasets such as the Microsoft Common\nObjects in Context (MSCOCO) through the use of generative adversarial networks\n(GANs). Synthesizing objects with a complex shape, however, is still\nchallenging. For example, animals and humans have many degrees of freedom,\nwhich means that they can take on many complex shapes. We propose a new\ntraining method called Image-Text-Image (I2T2I) which integrates text-to-image\nand image-to-text (image captioning) synthesis to improve the performance of\ntext-to-image synthesis. We demonstrate that %the capability of our method to\nunderstand the sentence descriptions, so as to I2T2I can generate better\nmulti-categories images using MSCOCO than the state-of-the-art. We also\ndemonstrate that I2T2I can achieve transfer learning by using a pre-trained\nimage captioning module to generate human images on the MPII Human Pose\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 11:11:38 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 18:46:42 GMT"}, {"version": "v3", "created": "Sat, 3 Jun 2017 22:46:46 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Dong", "Hao", ""], ["Zhang", "Jingqing", ""], ["McIlwraith", "Douglas", ""], ["Guo", "Yike", ""]]}, {"id": "1703.06817", "submitter": "Kaicheng Yu", "authors": "Kaicheng Yu, Mathieu Salzmann", "title": "Second-order Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been successfully applied to many\ncomputer vision tasks, such as image classification. By performing linear\ncombinations and element-wise nonlinear operations, these networks can be\nthought of as extracting solely first-order information from an input image. In\nthe past, however, second-order statistics computed from handcrafted features,\ne.g., covariances, have proven highly effective in diverse recognition tasks.\nIn this paper, we introduce a novel class of CNNs that exploit second-order\nstatistics. To this end, we design a series of new layers that (i) extract a\ncovariance matrix from convolutional activations, (ii) compute a parametric,\nsecond-order transformation of a matrix, and (iii) perform a parametric\nvectorization of a matrix. These operations can be assembled to form a\nCovariance Descriptor Unit (CDU), which replaces the fully-connected layers of\nstandard CNNs. Our experiments demonstrate the benefits of our new\narchitecture, which outperform the first-order CNNs, while relying on up to 90%\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 16:05:21 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Yu", "Kaicheng", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1703.06857", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal and Radha Poovendran", "title": "On the Limitation of Convolutional Neural Networks in Recognizing\n  Negative Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance on a variety of computer vision tasks, particularly visual\nclassification problems, where new algorithms reported to achieve or even\nsurpass the human performance. In this paper, we examine whether CNNs are\ncapable of learning the semantics of training data. To this end, we evaluate\nCNNs on negative images, since they share the same structure and semantics as\nregular images and humans can classify them correctly. Our experimental results\nindicate that when training on regular images and testing on negative images,\nthe model accuracy is significantly lower than when it is tested on regular\nimages. This leads us to the conjecture that current training methods do not\neffectively train models to generalize the concepts. We then introduce the\nnotion of semantic adversarial examples - transformed inputs that semantically\nrepresent the same objects, but the model does not classify them correctly -\nand present negative images as one class of such inputs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 17:21:19 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:53:28 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Jaiswal", "Mayoore", ""], ["Poovendran", "Radha", ""]]}, {"id": "1703.06868", "submitter": "Xun Huang", "authors": "Xun Huang, Serge Belongie", "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance\n  Normalization", "comments": "ICCV 2017. Code is available:\n  https://github.com/xunhuang1995/AdaIN-style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatys et al. recently introduced a neural algorithm that renders a content\nimage in the style of another image, achieving so-called style transfer.\nHowever, their framework requires a slow iterative optimization process, which\nlimits its practical application. Fast approximations with feed-forward neural\nnetworks have been proposed to speed up neural style transfer. Unfortunately,\nthe speed improvement comes at a cost: the network is usually tied to a fixed\nset of styles and cannot adapt to arbitrary new styles. In this paper, we\npresent a simple yet effective approach that for the first time enables\narbitrary style transfer in real-time. At the heart of our method is a novel\nadaptive instance normalization (AdaIN) layer that aligns the mean and variance\nof the content features with those of the style features. Our method achieves\nspeed comparable to the fastest existing approach, without the restriction to a\npre-defined set of styles. In addition, our approach allows flexible user\ncontrols such as content-style trade-off, style interpolation, color & spatial\ncontrols, all using a single feed-forward neural network.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 17:51:31 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 09:32:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Huang", "Xun", ""], ["Belongie", "Serge", ""]]}, {"id": "1703.06870", "submitter": "Kaiming He", "authors": "Kaiming He, Georgia Gkioxari, Piotr Doll\\'ar, Ross Girshick", "title": "Mask R-CNN", "comments": "open source; appendix on more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple, flexible, and general framework for object\ninstance segmentation. Our approach efficiently detects objects in an image\nwhile simultaneously generating a high-quality segmentation mask for each\ninstance. The method, called Mask R-CNN, extends Faster R-CNN by adding a\nbranch for predicting an object mask in parallel with the existing branch for\nbounding box recognition. Mask R-CNN is simple to train and adds only a small\noverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to\ngeneralize to other tasks, e.g., allowing us to estimate human poses in the\nsame framework. We show top results in all three tracks of the COCO suite of\nchallenges, including instance segmentation, bounding-box object detection, and\nperson keypoint detection. Without bells and whistles, Mask R-CNN outperforms\nall existing, single-model entries on every task, including the COCO 2016\nchallenge winners. We hope our simple and effective approach will serve as a\nsolid baseline and help ease future research in instance-level recognition.\nCode has been made available at: https://github.com/facebookresearch/Detectron\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 17:53:38 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 20:14:55 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 07:54:08 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["He", "Kaiming", ""], ["Gkioxari", "Georgia", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""]]}, {"id": "1703.06931", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong\n  Wang, Ke Lu", "title": "Learning Correspondence Structures for Person Re-identification", "comments": "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017.\n  The project page for this paper is available at\n  http://min.sjtu.edu.cn/lwydemo/personReID.htm arXiv admin note: text overlap\n  with arXiv:1504.06243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of handling spatial misalignments due to\ncamera-view changes or human-pose variations in person re-identification. We\nfirst introduce a boosting-based approach to learn a correspondence structure\nwhich indicates the patch-wise matching probabilities between images from a\ntarget camera pair. The learned correspondence structure can not only capture\nthe spatial correspondence pattern between cameras but also handle the\nviewpoint or human-pose variation in individual images. We further introduce a\nglobal constraint-based matching process. It integrates a global matching\nconstraint over the learned correspondence structure to exclude cross-view\nmisalignments during the image patch matching process, hence achieving a more\nreliable matching score between images. Finally, we also extend our approach by\nintroducing a multi-structure scheme, which learns a set of local\ncorrespondence structures to capture the spatial correspondence sub-patterns\nbetween a camera pair, so as to handle the spatial misalignments between\nindividual images in a more precise way. Experimental results on various\ndatasets demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 19:17:14 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 12:31:28 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 16:15:30 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Lin", "Weiyao", ""], ["Shen", "Yang", ""], ["Yan", "Junchi", ""], ["Xu", "Mingliang", ""], ["Wu", "Jianxin", ""], ["Wang", "Jingdong", ""], ["Lu", "Ke", ""]]}, {"id": "1703.06935", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon, Ondrej Chum", "title": "Fast Spectral Ranking for Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning on representing images for particular\nobject retrieval, recent studies show that the learned representations still\nlie on manifolds in a high dimensional space. This makes the Euclidean nearest\nneighbor search biased for this task. Exploring the manifolds online remains\nexpensive even if a nearest neighbor graph has been computed offline. This work\nintroduces an explicit embedding reducing manifold search to Euclidean search\nfollowed by dot product similarity search. This is equivalent to linear graph\nfiltering of a sparse signal in the frequency domain. To speed up online\nsearch, we compute an approximate Fourier basis of the graph offline. We\nimprove the state of art on particular object retrieval datasets including the\nchallenging Instre dataset containing small objects. At a scale of 10^5 images,\nthe offline cost is only a few hours, while query time is comparable to\nstandard similarity search.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 19:27:20 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 12:55:21 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 14:00:49 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Iscen", "Ahmet", ""], ["Avrithis", "Yannis", ""], ["Tolias", "Giorgos", ""], ["Furon", "Teddy", ""], ["Chum", "Ondrej", ""]]}, {"id": "1703.06953", "submitter": "Hang Zhang", "authors": "Hang Zhang and Kristin Dana", "title": "Multi-style Generative Network for Real-time Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid progress in style transfer, existing approaches using\nfeed-forward generative network for multi-style or arbitrary-style transfer are\nusually compromised of image quality and model flexibility. We find it is\nfundamentally difficult to achieve comprehensive style modeling using\n1-dimensional style embedding. Motivated by this, we introduce CoMatch Layer\nthat learns to match the second order feature statistics with the target\nstyles. With the CoMatch Layer, we build a Multi-style Generative Network\n(MSG-Net), which achieves real-time performance. We also employ an specific\nstrategy of upsampled convolution which avoids checkerboard artifacts caused by\nfractionally-strided convolution. Our method has achieved superior image\nquality comparing to state-of-the-art approaches. The proposed MSG-Net as a\ngeneral approach for real-time style transfer is compatible with most existing\ntechniques including content-style interpolation, color-preserving, spatial\ncontrol and brush stroke size control. MSG-Net is the first to achieve\nreal-time brush-size control in a purely feed-forward manner for style\ntransfer. Our implementations and pre-trained models for Torch, PyTorch and\nMXNet frameworks will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 20:24:11 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 19:38:35 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhang", "Hang", ""], ["Dana", "Kristin", ""]]}, {"id": "1703.06971", "submitter": "Jan van Gemert", "authors": "Miriam W. Huijser and Jan C. van Gemert", "title": "Active Decision Boundary Annotation with Deep Generative Models", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on active learning where the goal is to reduce the data\nannotation burden by interacting with a (human) oracle during training.\nStandard active learning methods ask the oracle to annotate data samples.\nInstead, we take a profoundly different approach: we ask for annotations of the\ndecision boundary. We achieve this using a deep generative model to create\nnovel instances along a 1d line. A point on the decision boundary is revealed\nwhere the instances change class. Experimentally we show on three data sets\nthat our method can be plugged-in to other active learning schemes, that human\noracles can effectively annotate points on the decision boundary, that our\nmethod is robust to annotation noise, and that decision boundary annotations\nimprove over annotating data samples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 21:20:21 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 09:36:55 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Huijser", "Miriam W.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1703.06993", "submitter": "Lingxi Xie", "authors": "Yan Wang, Lingxi Xie, Chenxi Liu, Ya Zhang, Wenjun Zhang, Alan Yuille", "title": "SORT: Second-Order Response Transform for Visual Recognition", "comments": "To appear in ICCV 2017 (10 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reveal the importance and benefits of introducing\nsecond-order operations into deep neural networks. We propose a novel approach\nnamed Second-Order Response Transform (SORT), which appends element-wise\nproduct transform to the linear sum of a two-branch network module. A direct\nadvantage of SORT is to facilitate cross-branch response propagation, so that\neach branch can update its weights based on the current status of the other\nbranch. Moreover, SORT augments the family of transform operations and\nincreases the nonlinearity of the network, making it possible to learn flexible\nfunctions to fit the complicated distribution of feature space. SORT can be\napplied to a wide range of network architectures, including a branched variant\nof a chain-styled network and a residual network, with very light-weighted\nmodifications. We observe consistent accuracy gain on both small (CIFAR10,\nCIFAR100 and SVHN) and big (ILSVRC2012) datasets. In addition, SORT is very\nefficient, as the extra computation overhead is less than 5%.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 22:51:56 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 20:26:23 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 13:25:00 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Wang", "Yan", ""], ["Xie", "Lingxi", ""], ["Liu", "Chenxi", ""], ["Zhang", "Ya", ""], ["Zhang", "Wenjun", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.06995", "submitter": "Behzad Hasani", "authors": "Behzad Hasani and Mohammad H. Mahoor", "title": "Spatio-Temporal Facial Expression Recognition Using Convolutional Neural\n  Networks and Conditional Random Fields", "comments": "To appear in 12th IEEE Conference on Automatic Face and Gesture\n  Recognition Workshop", "journal-ref": "2017 12th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2017)", "doi": "10.1109/FG.2017.99", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Facial Expression Recognition (FER) has been a challenging task for\ndecades. Many of the existing works use hand-crafted features such as LBP, HOG,\nLPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as\nSupport Vector Machines for expression recognition. These methods often require\nrigorous hyperparameter tuning to achieve good results. Recently Deep Neural\nNetworks (DNN) have shown to outperform traditional methods in visual object\nrecognition. In this paper, we propose a two-part network consisting of a\nDNN-based architecture followed by a Conditional Random Field (CRF) module for\nfacial expression recognition in videos. The first part captures the spatial\nrelation within facial images using convolutional layers followed by three\nInception-ResNet modules and two fully-connected layers. To capture the\ntemporal relation between the image frames, we use linear chain CRF in the\nsecond part of our network. We evaluate our proposed network on three publicly\navailable databases, viz. CK+, MMI, and FERA. Experiments are performed in\nsubject-independent and cross-database manners. Our experimental results show\nthat cascading the deep network architecture with the CRF module considerably\nincreases the recognition of facial expressions in videos and in particular it\noutperforms the state-of-the-art methods in the cross-database experiments and\nyields comparable results in the subject-independent experiments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 23:08:21 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 23:08:17 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hasani", "Behzad", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1703.07022", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural image usually conveys rich semantic content and can be viewed from\ndifferent angles. Existing image description methods are largely restricted by\nsmall sets of biased visual paragraph annotations, and fail to cover rich\nunderlying semantics. In this paper, we investigate a semi-supervised paragraph\ngenerative framework that is able to synthesize diverse and semantically\ncoherent paragraph descriptions by reasoning over local semantic regions and\nexploiting linguistic knowledge. The proposed Recurrent Topic-Transition\nGenerative Adversarial Network (RTT-GAN) builds an adversarial framework\nbetween a structured paragraph generator and multi-level paragraph\ndiscriminators. The paragraph generator generates sentences recurrently by\nincorporating region-based visual and language attention mechanisms at each\nstep. The quality of generated paragraph sentences is assessed by multi-level\nadversarial discriminators from two aspects, namely, plausibility at sentence\nlevel and topic-transition coherence at paragraph level. The joint adversarial\ntraining of RTT-GAN drives the model to generate realistic paragraphs with\nsmooth logical transition between sentence topics. Extensive quantitative\nexperiments on image and video paragraph datasets demonstrate the effectiveness\nof our RTT-GAN in both supervised and semi-supervised settings. Qualitative\nresults on telling diverse stories for an image also verify the\ninterpretability of RTT-GAN.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 01:43:12 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 20:06:15 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Liang", "Xiaodan", ""], ["Hu", "Zhiting", ""], ["Zhang", "Hao", ""], ["Gan", "Chuang", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.07023", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann,\n  Basura Fernando, Lars Petersson, Lars Andersson", "title": "Encouraging LSTMs to Anticipate Actions Very Early", "comments": "13 Pages, 7 Figures, 11 Tables. Accepted in ICCV 2017. arXiv admin\n  note: text overlap with arXiv:1611.05520", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to the widely studied problem of recognizing an action given a\ncomplete sequence, action anticipation aims to identify the action from only\npartially available videos. As such, it is therefore key to the success of\ncomputer vision applications requiring to react as early as possible, such as\nautonomous navigation. In this paper, we propose a new action anticipation\nmethod that achieves high prediction accuracy even in the presence of a very\nsmall percentage of a video sequence. To this end, we develop a multi-stage\nLSTM architecture that leverages context-aware and action-aware features, and\nintroduce a novel loss function that encourages the model to predict the\ncorrect class as early as possible. Our experiments on standard benchmark\ndatasets evidence the benefits of our approach; We outperform the\nstate-of-the-art action anticipation methods for early prediction by a relative\nincrease in accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on\nUCF-101.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 01:50:53 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 04:42:44 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 01:59:04 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Aliakbarian", "Mohammad Sadegh", ""], ["Saleh", "Fatemeh Sadat", ""], ["Salzmann", "Mathieu", ""], ["Fernando", "Basura", ""], ["Petersson", "Lars", ""], ["Andersson", "Lars", ""]]}, {"id": "1703.07026", "submitter": "Xin Huang", "authors": "Xin Huang and Yuxin Peng", "title": "Cross-modal Deep Metric Learning with Multi-task Regularization", "comments": "Revision: Added reference [7] 6 pages, 1 figure, to appear in the\n  proceedings of the IEEE International Conference on Multimedia and Expo\n  (ICME), Jul 10, 2017 - Jul 14, 2017, Hong Kong, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based cross-modal retrieval has become a research hotspot, by which users\ncan search results across various modalities like image and text. However,\nexisting methods mainly focus on the pairwise correlation and reconstruction\nerror of labeled data. They ignore the semantically similar and dissimilar\nconstraints between different modalities, and cannot take advantage of\nunlabeled data. This paper proposes Cross-modal Deep Metric Learning with\nMulti-task Regularization (CDMLMR), which integrates quadruplet ranking loss\nand semi-supervised contrastive loss for modeling cross-modal semantic\nsimilarity in a unified multi-task learning architecture. The quadruplet\nranking loss can model the semantically similar and dissimilar constraints to\npreserve cross-modal relative similarity ranking information. The\nsemi-supervised contrastive loss is able to maximize the semantic similarity on\nboth labeled and unlabeled data. Compared to the existing methods, CDMLMR\nexploits not only the similarity ranking information but also unlabeled\ncross-modal data, and thus boosts cross-modal retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 02:04:30 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 05:02:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1703.07047", "submitter": "Krzysztof J. Geras", "authors": "Krzysztof J. Geras and Stacey Wolfson and Yiqiu Shen and Nan Wu and S.\n  Gene Kim and Eric Kim and Laura Heacock and Ujas Parikh and Linda Moy and\n  Kyunghyun Cho", "title": "High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 04:11:13 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 06:39:33 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 01:21:51 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Geras", "Krzysztof J.", ""], ["Wolfson", "Stacey", ""], ["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Kim", "S. Gene", ""], ["Kim", "Eric", ""], ["Heacock", "Laura", ""], ["Parikh", "Ujas", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1703.07107", "submitter": "Marco Fiorucci", "authors": "Marco Fiorucci, Alessandro Torcinovich, Manuel Curado, Francisco\n  Escolano and Marcello Pelillo", "title": "On the Interplay between Strong Regularity and Graph Densification", "comments": "GbR2017 to appear in Lecture Notes in Computer Science (LNCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the practical implications of Szemer\\'edi's\nregularity lemma in the preservation of metric information contained in large\ngraphs. To this end, we present a heuristic algorithm to find regular\npartitions. Our experiments show that this method is quite robust to the\nnatural sparsification of proximity graphs. In addition, this robustness can be\nenforced by graph densification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 09:37:16 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Fiorucci", "Marco", ""], ["Torcinovich", "Alessandro", ""], ["Curado", "Manuel", ""], ["Escolano", "Francisco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1703.07131", "submitter": "Mandar Kulkarni Mr.", "authors": "Mandar Kulkarni, Kalpesh Patil, Shirish Karande", "title": "Knowledge distillation using unlabeled mismatched images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches for Knowledge Distillation (KD) either directly use\ntraining data or sample from the training data distribution. In this paper, we\ndemonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for\nimage classification networks. For illustration, we consider scenarios where\nthis is a complete absence of training data, or mismatched stimulus has to be\nused for augmenting a small amount of training data. We demonstrate that\nstimulus complexity is a key factor for distillation's good performance. Our\nexamples include use of various datasets for stimulating MNIST and CIFAR\nteachers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:34:59 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Kulkarni", "Mandar", ""], ["Patil", "Kalpesh", ""], ["Karande", "Shirish", ""]]}, {"id": "1703.07140", "submitter": "Youngsung Kim", "authors": "Youngsung Kim, ByungIn Yoo, Youngjun Kwak, Changkyu Choi, Junmo Kim", "title": "Deep generative-contrastive networks for facial expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the expressive depth of an emotional face differs with individuals or\nexpressions, recognizing an expression using a single facial image at a moment\nis difficult. A relative expression of a query face compared to a reference\nface might alleviate this difficulty. In this paper, we propose to utilize\ncontrastive representation that embeds a distinctive expressive factor for a\ndiscriminative purpose. The contrastive representation is calculated at the\nembedding layer of deep networks by comparing a given (query) image with the\nreference image. We attempt to utilize a generative reference image that is\nestimated based on the given image. Consequently, we deploy deep neural\nnetworks that embed a combination of a generative model, a contrastive model,\nand a discriminative model with an end-to-end training manner. In our proposed\nnetworks, we attempt to disentangle a facial expressive factor in two steps\nincluding learning of a generator network and a contrastive encoder network. We\nconducted extensive experiments on publicly available face expression databases\n(CK+, MMI, Oulu-CASIA, and in-the-wild databases) that have been widely adopted\nin the recent literatures. The proposed method outperforms the known\nstate-of-the art methods in terms of the recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:52:02 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 04:51:46 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 06:41:05 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kim", "Youngsung", ""], ["Yoo", "ByungIn", ""], ["Kwak", "Youngjun", ""], ["Choi", "Changkyu", ""], ["Kim", "Junmo", ""]]}, {"id": "1703.07144", "submitter": "Bumsub Ham", "authors": "Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce", "title": "Proposal Flow: Semantic Correspondences from Object Proposals", "comments": "arXiv admin note: text overlap with arXiv:1511.05065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding image correspondences remains a challenging problem in the presence\nof intra-class variations and large changes in scene layout. Semantic flow\nmethods are designed to handle images depicting different instances of the same\nobject or scene category. We introduce a novel approach to semantic flow,\ndubbed proposal flow, that establishes reliable correspondences using object\nproposals. Unlike prevailing semantic flow approaches that operate on pixels or\nregularly sampled local regions, proposal flow benefits from the\ncharacteristics of modern object proposals, that exhibit high repeatability at\nmultiple scales, and can take advantage of both local and geometric consistency\nconstraints among proposals. We also show that the corresponding sparse\nproposal flow can effectively be transformed into a conventional dense flow\nfield. We introduce two new challenging datasets that can be used to evaluate\nboth general semantic flow techniques and region-based approaches such as\nproposal flow. We use these benchmarks to compare different matching\nalgorithms, object proposals, and region features within proposal flow, to the\nstate of the art in semantic flow. This comparison, along with experiments on\nstandard datasets, demonstrates that proposal flow significantly outperforms\nexisting semantic flow methods in various settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:57:27 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Ham", "Bumsub", ""], ["Cho", "Minsu", ""], ["Schmid", "Cordelia", ""], ["Ponce", "Jean", ""]]}, {"id": "1703.07195", "submitter": "Huikai Wu", "authors": "Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang", "title": "GP-GAN: Towards Realistic High-Resolution Image Blending", "comments": "Accepted by ACMMM 2019. The source code is available in\n  https://github.com/wuhuikai/GP-GAN and there's also an online demo in\n  http://wuhuikai.me/DeepJS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common but challenging to address high-resolution image blending in the\nautomatic photo editing application. In this paper, we would like to focus on\nsolving the problem of high-resolution image blending, where the composite\nimages are provided. We propose a framework called Gaussian-Poisson Generative\nAdversarial Network (GP-GAN) to leverage the strengths of the classical\ngradient-based approach and Generative Adversarial Networks. To the best of our\nknowledge, it's the first work that explores the capability of GANs in\nhigh-resolution image blending task. Concretely, we propose Gaussian-Poisson\nEquation to formulate the high-resolution image blending problem, which is a\njoint optimization constrained by the gradient and color information. Inspired\nby the prior works, we obtain gradient information via applying gradient\nfilters. To generate the color information, we propose a Blending GAN to learn\nthe mapping between the composite images and the well-blended ones. Compared to\nthe alternative methods, our approach can deliver high-resolution, realistic\nimages with fewer bleedings and unpleasant artifacts. Experiments confirm that\nour approach achieves the state-of-the-art performance on Transient Attributes\ndataset. A user study on Amazon Mechanical Turk finds that the majority of\nworkers are in favor of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 12:57:58 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 12:37:34 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 09:20:50 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wu", "Huikai", ""], ["Zheng", "Shuai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1703.07220", "submitter": "Yutian Lin", "authors": "Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, Zhilan Hu, Chenggang\n  Yan and Yi Yang", "title": "Improving Person Re-identification by Attribute and Identity Learning", "comments": "Accepted to Pattern Recognition (PR)", "journal-ref": null, "doi": "10.1016/j.patcog.2019.06.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) and attribute recognition share a common\ntarget at learning pedestrian descriptions. Their difference consists in the\ngranularity. Most existing re-ID methods only take identity labels of\npedestrians into consideration. However, we find the attributes, containing\ndetailed local descriptions, are beneficial in allowing the re-ID model to\nlearn more discriminative feature representations. In this paper, based on the\ncomplementarity of attribute labels and ID labels, we propose an\nattribute-person recognition (APR) network, a multi-task network which learns a\nre-ID embedding and at the same time predicts pedestrian attributes. We\nmanually annotate attribute labels for two large-scale re-ID datasets, and\nsystematically investigate how person re-ID and attribute recognition benefit\nfrom each other. In addition, we re-weight the attribute predictions\nconsidering the dependencies and correlations among the attributes. The\nexperimental results on two large-scale re-ID benchmarks demonstrate that by\nlearning a more discriminative representation, APR achieves competitive re-ID\nperformance compared with the state-of-the-art methods. We use APR to speed up\nthe retrieval process by ten times with a minor accuracy drop of 2.92% on\nMarket-1501. Besides, we also apply APR on the attribute recognition task and\ndemonstrate improvement over the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 13:44:42 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 06:40:34 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 18:22:47 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Lin", "Yutian", ""], ["Zheng", "Liang", ""], ["Zheng", "Zhedong", ""], ["Wu", "Yu", ""], ["Hu", "Zhilan", ""], ["Yan", "Chenggang", ""], ["Yang", "Yi", ""]]}, {"id": "1703.07255", "submitter": "Hao Wang", "authors": "Hao Wang, Xiaodan Liang, Hao Zhang, Dit-Yan Yeung, Eric P. Xing", "title": "ZM-Net: Real-time Zero-shot Image Manipulation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in image processing and computer vision (e.g. colorization,\nstyle transfer) can be posed as 'manipulating' an input image into a\ncorresponding output image given a user-specified guiding signal. A holy-grail\nsolution towards generic image manipulation should be able to efficiently alter\nan input image with any personalized signals (even signals unseen during\ntraining), such as diverse paintings and arbitrary descriptive attributes.\nHowever, existing methods are either inefficient to simultaneously process\nmultiple signals (let alone generalize to unseen signals), or unable to handle\nsignals from other modalities. In this paper, we make the first attempt to\naddress the zero-shot image manipulation task. We cast this problem as\nmanipulating an input image according to a parametric model whose key\nparameters can be conditionally generated from any guiding signal (even unseen\nones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a\nfully-differentiable architecture that jointly optimizes an\nimage-transformation network (TNet) and a parameter network (PNet). The PNet\nlearns to generate key transformation parameters for the TNet given any guiding\nsignal while the TNet performs fast zero-shot image manipulation according to\nboth signal-dependent parameters from the PNet and signal-invariant parameters\nfrom the TNet itself. Extensive experiments show that our ZM-Net can perform\nhigh-quality image manipulation conditioned on different forms of guiding\nsignals (e.g. style images and attributes) in real-time (tens of milliseconds\nper image) even for unseen signals. Moreover, a large-scale style dataset with\nover 20,000 style images is also constructed to promote further research.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:01:59 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:08:40 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wang", "Hao", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Hao", ""], ["Yeung", "Dit-Yan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.07270", "submitter": "Daniel Peralta", "authors": "Daniel Peralta and Isaac Triguero and Salvador Garc\\'ia and Yvan Saeys\n  and Jose M. Benitez and Francisco Herrera", "title": "On the use of convolutional neural networks for robust classification of\n  multiple fingerprint captures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint classification is one of the most common approaches to accelerate\nthe identification in large databases of fingerprints. Fingerprints are grouped\ninto disjoint classes, so that an input fingerprint is compared only with those\nbelonging to the predicted class, reducing the penetration rate of the search.\nThe classification procedure usually starts by the extraction of features from\nthe fingerprint image, frequently based on visual characteristics. In this\nwork, we propose an approach to fingerprint classification using convolutional\nneural networks, which avoid the necessity of an explicit feature extraction\nprocess by incorporating the image processing within the training of the\nclassifier. Furthermore, such an approach is able to predict a class even for\nlow-quality fingerprints that are rejected by commonly used algorithms, such as\nFingerCode. The study gives special importance to the robustness of the\nclassification for different impressions of the same fingerprint, aiming to\nminimize the penetration in the database. In our experiments, convolutional\nneural networks yielded better accuracy and penetration rate than\nstate-of-the-art classifiers based on explicit feature extraction. The tested\nnetworks also improved on the runtime, as a result of the joint optimization of\nboth feature extraction and classification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:22:26 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 12:12:00 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 09:17:35 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Peralta", "Daniel", ""], ["Triguero", "Isaac", ""], ["Garc\u00eda", "Salvador", ""], ["Saeys", "Yvan", ""], ["Benitez", "Jose M.", ""], ["Herrera", "Francisco", ""]]}, {"id": "1703.07330", "submitter": "Syed Zain Masood", "authors": "Syed Zain Masood, Guang Shu, Afshin Dehghan and Enrique G. Ortiz", "title": "License Plate Detection and Recognition Using Deeply Learned\n  Convolutional Neural Networks", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work details Sighthounds fully automated license plate detection and\nrecognition system. The core technology of the system is built using a sequence\nof deep Convolutional Neural Networks (CNNs) interlaced with accurate and\nefficient algorithms. The CNNs are trained and fine-tuned so that they are\nrobust under different conditions (e.g. variations in pose, lighting,\nocclusion, etc.) and can work across a variety of license plate templates (e.g.\nsizes, backgrounds, fonts, etc). For quantitative analysis, we show that our\nsystem outperforms the leading license plate detection and recognition\ntechnology i.e. ALPR on several benchmarks. Our system is available to\ndevelopers through the Sighthound Cloud API at\nhttps://www.sighthound.com/products/cloud\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:32:31 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 19:41:30 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Masood", "Syed Zain", ""], ["Shu", "Guang", ""], ["Dehghan", "Afshin", ""], ["Ortiz", "Enrique G.", ""]]}, {"id": "1703.07332", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "How far are we from solving the 2D & 3D Face Alignment problem? (and a\n  dataset of 230,000 3D facial landmarks)", "comments": "accepted to ICCV 2017", "journal-ref": null, "doi": "10.1109/ICCV.2017.116", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how far a very deep neural network is from attaining\nclose to saturating performance on existing 2D and 3D face alignment datasets.\nTo this end, we make the following 5 contributions: (a) we construct, for the\nfirst time, a very strong baseline by combining a state-of-the-art architecture\nfor landmark localization with a state-of-the-art residual block, train it on a\nvery large yet synthetically expanded 2D facial landmark dataset and finally\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\nall existing datasets, leading to the creation of LS3D-W, the largest and most\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\nthat, we train a neural network for 3D face alignment and evaluate it on the\nnewly introduced LS3D-W. (d) We further look into the effect of all\n\"traditional\" factors affecting face alignment performance like large pose,\ninitialization and resolution, and introduce a \"new\" one, namely the size of\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\nperformance of remarkable accuracy which is probably close to saturating the\ndatasets used. Training and testing code as well as the dataset can be\ndownloaded from https://www.adrianbulat.com/face-alignment/\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:37:36 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 15:03:19 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 16:21:37 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1703.07334", "submitter": "Shichao Yang", "authors": "Shichao Yang, Yu Song, Michael Kaess, Sebastian Scherer", "title": "Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments", "comments": "International Conference on Intelligent Robots and Systems (IROS)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing simultaneous localization and mapping (SLAM) algorithms are not\nrobust in challenging low-texture environments because there are only few\nsalient features. The resulting sparse or semi-dense map also conveys little\ninformation for motion planning. Though some work utilize plane or scene layout\nfor dense map regularization, they require decent state estimation from other\nsources. In this paper, we propose real-time monocular plane SLAM to\ndemonstrate that scene understanding could improve both state estimation and\ndense mapping especially in low-texture environments. The plane measurements\ncome from a pop-up 3D plane model applied to each single image. We also combine\nplanes with point based SLAM to improve robustness. On a public TUM dataset,\nour algorithm generates a dense semantic 3D model with pixel depth error of 6.2\ncm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our\nmethod creates a much better 3D model with state estimation error of 0.67%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:41:46 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Yang", "Shichao", ""], ["Song", "Yu", ""], ["Kaess", "Michael", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1703.07402", "submitter": "Nicolai Wojke", "authors": "Nicolai Wojke and Alex Bewley and Dietrich Paulus", "title": "Simple Online and Realtime Tracking with a Deep Association Metric", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple Online and Realtime Tracking (SORT) is a pragmatic approach to\nmultiple object tracking with a focus on simple, effective algorithms. In this\npaper, we integrate appearance information to improve the performance of SORT.\nDue to this extension we are able to track objects through longer periods of\nocclusions, effectively reducing the number of identity switches. In spirit of\nthe original framework we place much of the computational complexity into an\noffline pre-training stage where we learn a deep association metric on a\nlarge-scale person re-identification dataset. During online application, we\nestablish measurement-to-track associations using nearest neighbor queries in\nvisual appearance space. Experimental evaluation shows that our extensions\nreduce the number of identity switches by 45%, achieving overall competitive\nperformance at high frame rates.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 19:40:25 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wojke", "Nicolai", ""], ["Bewley", "Alex", ""], ["Paulus", "Dietrich", ""]]}, {"id": "1703.07431", "submitter": "Sungmin Eum", "authors": "Sungmin Eum, Hyungtae Lee, Heesung Kwon, David Doermann", "title": "IOD-CNN: Integrating Object Detection Networks for Event Recognition", "comments": "submitted to IEEE International Conference on Image Processing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous methods have showed the importance of considering semantically\nrelevant objects for performing event recognition, yet none of the methods have\nexploited the power of deep convolutional neural networks to directly integrate\nrelevant object information into a unified network. We present a novel unified\ndeep CNN architecture which integrates architecturally different, yet\nsemantically-related object detection networks to enhance the performance of\nthe event recognition task. Our architecture allows the sharing of the\nconvolutional layers and a fully connected layer which effectively integrates\nevent recognition, rigid object detection and non-rigid object detection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 21:05:21 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Eum", "Sungmin", ""], ["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""], ["Doermann", "David", ""]]}, {"id": "1703.07464", "submitter": "Yair Movshovitz-Attias", "authors": "Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey\n  Ioffe, Saurabh Singh", "title": "No Fuss Distance Metric Learning using Proxies", "comments": "To be presented in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of distance metric learning (DML), defined as learning\na distance consistent with a notion of semantic similarity. Traditionally, for\nthis problem supervision is expressed in the form of sets of points that follow\nan ordinal relationship -- an anchor point $x$ is similar to a set of positive\npoints $Y$, and dissimilar to a set of negative points $Z$, and a loss defined\nover these distances is minimized. While the specifics of the optimization\ndiffer, in this work we collectively call this type of supervision Triplets and\nall methods that follow this pattern Triplet-Based methods. These methods are\nchallenging to optimize. A main issue is the need for finding informative\ntriplets, which is usually achieved by a variety of tricks such as increasing\nthe batch size, hard or semi-hard triplet mining, etc. Even with these tricks,\nthe convergence rate of such methods is slow. In this paper we propose to\noptimize the triplet loss on a different space of triplets, consisting of an\nanchor data point and similar and dissimilar proxy points which are learned as\nwell. These proxies approximate the original data points, so that a triplet\nloss over the proxies is a tight upper bound of the original loss. This\nproxy-based loss is empirically better behaved. As a result, the proxy-loss\nimproves on state-of-art results for three standard zero-shot learning\ndatasets, by up to 15% points, while converging three times as fast as other\ntriplet-based losses.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 23:11:56 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 21:17:05 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 19:52:13 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Movshovitz-Attias", "Yair", ""], ["Toshev", "Alexander", ""], ["Leung", "Thomas K.", ""], ["Ioffe", "Sergey", ""], ["Singh", "Saurabh", ""]]}, {"id": "1703.07473", "submitter": "Niko S\\\"underhauf", "authors": "Feras Dayoub, Niko S\\\"underhauf, Peter Corke", "title": "Episode-Based Active Learning with Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different strategies for active learning with Bayesian deep\nneural networks. We focus our analysis on scenarios where new, unlabeled data\nis obtained episodically, such as commonly encountered in mobile robotics\napplications. An evaluation of different strategies for acquisition, updating,\nand final training on the CIFAR-10 dataset shows that incremental network\nupdates with final training on the accumulated acquisition set are essential\nfor best performance, while limiting the amount of required human labeling\nlabor.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 23:56:51 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""], ["Corke", "Peter", ""]]}, {"id": "1703.07475", "submitter": "Chunhui Liu", "authors": "Chunhui Liu, and Yueyu Hu, and Yanghao Li, and Sijie Song, and Jiaying\n  Liu", "title": "PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action\n  Understanding", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that many 3D human activity benchmarks being proposed, most\nexisting action datasets focus on the action recognition tasks for the\nsegmented videos. There is a lack of standard large-scale benchmarks,\nespecially for current popular data-hungry deep learning based methods. In this\npaper, we introduce a new large scale benchmark (PKU-MMD) for continuous\nmulti-modality 3D human action understanding and cover a wide range of complex\nhuman activities with well annotated information. PKU-MMD contains 1076 long\nvideo sequences in 51 action categories, performed by 66 subjects in three\ncamera views. It contains almost 20,000 action instances and 5.4 million frames\nin total. Our dataset also provides multi-modality data sources, including RGB,\ndepth, Infrared Radiation and Skeleton. With different modalities, we conduct\nextensive experiments on our dataset in terms of two scenarios and evaluate\ndifferent methods by various metrics, including a new proposed evaluation\nprotocol 2D-AP. We believe this large-scale dataset will benefit future\nresearches on action detection for the community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 00:22:49 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 01:01:29 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Liu", "Chunhui", ""], ["Hu", "Yueyu", ""], ["Li", "Yanghao", ""], ["Song", "Sijie", ""], ["Liu", "Jiaying", ""]]}, {"id": "1703.07478", "submitter": "S. Alireza Golestaneh", "authors": "S. Alireza Golestaneh, Lina J. Karam", "title": "Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted\n  Transform Coefficients of Gradient Magnitudes", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of spatially-varying blur without having any information about\nthe blur type is a challenging task. In this paper, we propose a novel\neffective approach to address the blur detection problem from a single image\nwithout requiring any knowledge about the blur type, level, or camera settings.\nOur approach computes blur detection maps based on a novel High-frequency\nmultiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The\nevaluations of the proposed approach on a diverse set of blurry images with\ndifferent blur types, levels, and contents demonstrate that the proposed\nalgorithm performs favorably against the state-of-the-art methods qualitatively\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 00:44:26 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 21:37:40 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 18:36:34 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Golestaneh", "S. Alireza", ""], ["Karam", "Lina J.", ""]]}, {"id": "1703.07479", "submitter": "Sandra Avila", "authors": "Afonso Menegola, Michel Fornaciali, Ramon Pires, Fl\\'avia Vasques\n  Bittencourt, Sandra Avila, Eduardo Valle", "title": "Knowledge Transfer for Melanoma Screening with Deep Learning", "comments": "4 pages", "journal-ref": null, "doi": "10.1109/ISBI.2017.7950523", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge transfer impacts the performance of deep learning -- the state of\nthe art for image classification tasks, including automated melanoma screening.\nDeep learning's greed for large amounts of training data poses a challenge for\nmedical tasks, which we can alleviate by recycling knowledge from models\ntrained on different tasks, in a scheme called transfer learning. Although much\nof the best art on automated melanoma screening employs some form of transfer\nlearning, a systematic evaluation was missing. Here we investigate the presence\nof transfer, from which task the transfer is sourced, and the application of\nfine tuning (i.e., retraining of the deep learning model after transfer). We\nalso test the impact of picking deeper (and more expensive) models. Our results\nfavor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an\nAUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 00:51:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Menegola", "Afonso", ""], ["Fornaciali", "Michel", ""], ["Pires", "Ramon", ""], ["Bittencourt", "Fl\u00e1via Vasques", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1703.07511", "submitter": "Fujun Luan", "authors": "Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala", "title": "Deep Photo Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a deep-learning approach to photographic style transfer\nthat handles a large variety of image content while faithfully transferring the\nreference style. Our approach builds upon the recent work on painterly transfer\nthat separates style from the content of an image by considering different\nlayers of a neural network. However, as is, this approach is not suitable for\nphotorealistic style transfer. Even when both the input and reference images\nare photographs, the output still exhibits distortions reminiscent of a\npainting. Our contribution is to constrain the transformation from the input to\nthe output to be locally affine in colorspace, and to express this constraint\nas a custom fully differentiable energy term. We show that this approach\nsuccessfully suppresses distortion and yields satisfying photorealistic style\ntransfers in a broad variety of scenarios, including transfer of the time of\nday, weather, season, and artistic edits.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 04:21:41 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 07:57:03 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 03:53:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Luan", "Fujun", ""], ["Paris", "Sylvain", ""], ["Shechtman", "Eli", ""], ["Bala", "Kavita", ""]]}, {"id": "1703.07514", "submitter": "Simon Niklaus", "authors": "Simon Niklaus and Long Mai and Feng Liu", "title": "Video Frame Interpolation via Adaptive Convolution", "comments": "CVPR 2017, http://graphics.cs.pdx.edu/project/adaconv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation typically involves two steps: motion estimation and\npixel synthesis. Such a two-step approach heavily depends on the quality of\nmotion estimation. This paper presents a robust video frame interpolation\nmethod that combines these two steps into a single process. Specifically, our\nmethod considers pixel synthesis for the interpolated frame as local\nconvolution over two input frames. The convolution kernel captures both the\nlocal motion between the input frames and the coefficients for pixel synthesis.\nOur method employs a deep fully convolutional neural network to estimate a\nspatially-adaptive convolution kernel for each pixel. This deep neural network\ncan be directly trained end to end using widely available video data without\nany difficult-to-obtain ground-truth data like optical flow. Our experiments\nshow that the formulation of video interpolation as a single convolution\nprocess allows our method to gracefully handle challenges like occlusion, blur,\nand abrupt brightness change and enables high-quality video frame\ninterpolation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 04:31:38 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Liu", "Feng", ""]]}, {"id": "1703.07519", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Wei Liu, Charu Aggarwal, Thomas Huang", "title": "Joint Intermodal and Intramodal Label Transfers for Extremely Rare or\n  Unseen Classes", "comments": "The paper has been accepted by IEEE Transactions on Pattern Analysis\n  and Machine Intelligence. It will apear in a future issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a label transfer model from texts to images for\nimage classification tasks. The problem of image classification is often much\nmore challenging than text classification. On one hand, labeled text data is\nmore widely available than the labeled images for classification tasks. On the\nother hand, text data tends to have natural semantic interpretability, and they\nare often more directly related to class labels. On the contrary, the image\nfeatures are not directly related to concepts inherent in class labels. One of\nour goals in this paper is to develop a model for revealing the functional\nrelationships between text and image features as to directly transfer\nintermodal and intramodal labels to annotate the images. This is implemented by\nlearning a transfer function as a bridge to propagate the labels between two\nmultimodal spaces. However, the intermodal label transfers could be undermined\nby blindly transferring the labels of noisy texts to annotate images. To\nmitigate this problem, we present an intramodal label transfer process, which\ncomplements the intermodal label transfer by transferring the image labels\ninstead when relevant text is absent from the source corpus. In addition, we\ngeneralize the inter-modal label transfer to zero-shot learning scenario where\nthere are only text examples available to label unseen classes of images\nwithout any positive image examples. We evaluate our algorithm on an image\nclassification task and show the effectiveness with respect to the other\ncompared algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 04:40:51 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Liu", "Wei", ""], ["Aggarwal", "Charu", ""], ["Huang", "Thomas", ""]]}, {"id": "1703.07523", "submitter": "QiKui Zhu", "authors": "Qikui Zhu and Bo Du and Baris Turkbey and Peter L . Choyke and Pingkun\n  Yan", "title": "Deeply-Supervised CNN for Prostate Segmentation", "comments": "Due to a crucial sign error in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate segmentation from Magnetic Resonance (MR) images plays an important\nrole in image guided interven- tion. However, the lack of clear boundary\nspecifically at the apex and base, and huge variation of shape and texture\nbetween the images from different patients make the task very challenging. To\novercome these problems, in this paper, we propose a deeply supervised\nconvolutional neural network (CNN) utilizing the convolutional information to\naccurately segment the prostate from MR images. The proposed model can\neffectively detect the prostate region with additional deeply supervised layers\ncompared with other approaches. Since some information will be abandoned after\nconvolution, it is necessary to pass the features extracted from early stages\nto later stages. The experimental results show that significant segmentation\naccuracy improvement has been achieved by our proposed method compared to other\nreported approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 04:48:36 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 13:56:01 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 13:12:39 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Zhu", "Qikui", ""], ["Du", "Bo", ""], ["Turkbey", "Baris", ""], ["Choyke", "Peter L .", ""], ["Yan", "Pingkun", ""]]}, {"id": "1703.07570", "submitter": "Florian Chabot", "authors": "Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, C\\'eline\n  Teuli\\`ere, Thierry Chateau", "title": "Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D\n  vehicle analysis from monocular image", "comments": "CVPR 2017 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach, called Deep MANTA (Deep\nMany-Tasks), for many-task vehicle analysis from a given image. A robust\nconvolutional network is introduced for simultaneous vehicle detection, part\nlocalization, visibility characterization and 3D dimension estimation. Its\narchitecture is based on a new coarse-to-fine object proposal that boosts the\nvehicle detection. Moreover, the Deep MANTA network is able to localize vehicle\nparts even if these parts are not visible. In the inference, the network's\noutputs are used by a real time robust pose estimation algorithm for fine\norientation estimation and 3D vehicle localization. We show in experiments that\nour method outperforms monocular state-of-the-art approaches on vehicle\ndetection, orientation and 3D location tasks on the very challenging KITTI\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 09:03:25 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Chabot", "Florian", ""], ["Chaouch", "Mohamed", ""], ["Rabarisoa", "Jaonary", ""], ["Teuli\u00e8re", "C\u00e9line", ""], ["Chateau", "Thierry", ""]]}, {"id": "1703.07579", "submitter": "Zhongwen Xu", "authors": "Fan Wu, Zhongwen Xu, Yi Yang", "title": "An End-to-End Approach to Natural Language Object Retrieval via\n  Context-Aware Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end approach to the natural language object retrieval\ntask, which localizes an object within an image according to a natural language\ndescription, i.e., referring expression. Previous works divide this problem\ninto two independent stages: first, compute region proposals from the image\nwithout the exploration of the language description; second, score the object\nproposals with regard to the referring expression and choose the top-ranked\nproposals. The object proposals are generated independently from the referring\nexpression, which makes the proposal generation redundant and even irrelevant\nto the referred object. In this work, we train an agent with deep reinforcement\nlearning, which learns to move and reshape a bounding box to localize the\nobject according to the referring expression. We incorporate both the spatial\nand temporal context information into the training procedure. By simultaneously\nexploiting local visual information, the spatial and temporal context and the\nreferring language a priori, the agent selects an appropriate action to take at\neach time. A special action is defined to indicate when the agent finds the\nreferred object, and terminate the procedure. We evaluate our model on various\ndatasets, and our algorithm significantly outperforms the compared algorithms.\nNotably, the accuracy improvement of our method over the recent method GroundeR\nand SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 09:25:49 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wu", "Fan", ""], ["Xu", "Zhongwen", ""], ["Yang", "Yi", ""]]}, {"id": "1703.07595", "submitter": "Harish Katti", "authors": "Harish Katti, S. P. Arun", "title": "Can you tell where in India I am from? Comparing humans and computers on\n  fine-grained race face classification", "comments": "9 pages, 5 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faces form the basis for a rich variety of judgments in humans, yet the\nunderlying features remain poorly understood. Although fine-grained\ndistinctions within a race might more strongly constrain possible facial\nfeatures used by humans than in case of coarse categories such as race or\ngender, such fine grained distinctions are relatively less studied.\nFine-grained race classification is also interesting because even humans may\nnot be perfectly accurate on these tasks. This allows us to compare errors made\nby humans and machines, in contrast to standard object detection tasks where\nhuman performance is nearly perfect. We have developed a novel face database of\nclose to 1650 diverse Indian faces labeled for fine-grained race (South vs\nNorth India) as well as for age, weight, height and gender. We then asked close\nto 130 human subjects who were instructed to categorize each face as belonging\ntoa Northern or Southern state in India. We then compared human performance on\nthis task with that of computational models trained on the ground-truth labels.\nOur main results are as follows: (1) Humans are highly consistent (average\naccuracy: 63.6%), with some faces being consistently classified with > 90%\naccuracy and others consistently misclassified with < 30% accuracy; (2) Models\ntrained on ground-truth labels showed slightly worse performance (average\naccuracy: 62%) but showed higher accuracy (72.2%) on faces classified with >\n80% accuracy by humans. This was true for models trained on simple spatial and\nintensity measurements extracted from faces as well as deep neural networks\ntrained on race or gender classification; (3) Using overcomplete banks of\nfeatures derived from each face part, we found that mouth shape was the single\nlargest contributor towards fine-grained race classification, whereas distances\nbetween face parts was the strongest predictor of gender.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 10:35:58 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 06:39:41 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Katti", "Harish", ""], ["Arun", "S. P.", ""]]}, {"id": "1703.07645", "submitter": "Tomas Wilkinson", "authors": "Tomas Wilkinson, Jonas Lindstr\\\"om, Anders Brun", "title": "Neural Ctrl-F: Segmentation-free Query-by-String Word Spotting in\n  Handwritten Manuscript Collections", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we approach the problem of segmentation-free query-by-string\nword spotting for handwritten documents. In other words, we use methods\ninspired from computer vision and machine learning to search for words in large\ncollections of digitized manuscripts. In particular, we are interested in\nhistorical handwritten texts, which are often far more challenging than modern\nprinted documents. This task is important, as it provides people with a way to\nquickly find what they are looking for in large collections that are tedious\nand difficult to read manually. To this end, we introduce an end-to-end\ntrainable model based on deep neural networks that we call Ctrl-F-Net. Given a\nfull manuscript page, the model simultaneously generates region proposals, and\nembeds these into a distributed word embedding space, where searches are\nperformed. We evaluate the model on common benchmarks for handwritten word\nspotting, outperforming the previous state-of-the-art segmentation-free\napproaches by a large margin, and in some cases even segmentation-based\napproaches. One interesting real-life application of our approach is to help\nhistorians to find and count specific words in court records that are related\nto women's sustenance activities and division of labor. We provide promising\npreliminary experiments that validate our method on this task.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 13:35:49 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 13:19:02 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Wilkinson", "Tomas", ""], ["Lindstr\u00f6m", "Jonas", ""], ["Brun", "Anders", ""]]}, {"id": "1703.07655", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Jason M. Allred, Shriram Ramanathan and Kaushik\n  Roy", "title": "ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking\n  Neural Networks", "comments": "14 pages, 14 figures", "journal-ref": "IEEE Journal on Emerging and Selected Topics in Circuits and\n  Systems (Volume: 8, Issue: 1, March 2018)", "doi": "10.1109/JETCAS.2017.2769684", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental feature of learning in animals is the \"ability to forget\" that\nallows an organism to perceive, model and make decisions from disparate streams\nof information and adapt to changing environments. Against this backdrop, we\npresent a novel unsupervised learning mechanism ASP (Adaptive Synaptic\nPlasticity) for improved recognition with Spiking Neural Networks (SNNs) for\nreal time on-line learning in a dynamic environment. We incorporate an adaptive\nweight decay mechanism with the traditional Spike Timing Dependent Plasticity\n(STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic\nweights is modulated based on the temporal correlation between the spiking\npatterns of the pre- and post-synaptic neurons. This mechanism helps in gradual\nforgetting of insignificant data while retaining significant, yet old,\ninformation. ASP, thus, maintains a balance between forgetting and immediate\nlearning to construct a stable-plastic self-adaptive SNN for continuously\nchanging inputs. We demonstrate that the proposed learning methodology\naddresses catastrophic forgetting while yielding significantly improved\naccuracy over the conventional STDP learning method for digit recognition\napplications. Additionally, we observe that the proposed learning model\nautomatically encodes selective attention towards relevant features in the\ninput data while eliminating the influence of background noise (or denoising)\nfurther improving the robustness of the ASP learning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 13:48:47 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 20:17:33 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Allred", "Jason M.", ""], ["Ramanathan", "Shriram", ""], ["Roy", "Kaushik", ""]]}, {"id": "1703.07684", "submitter": "Camille Couprie", "authors": "Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek, Yann\n  LeCun", "title": "Predicting Deeper into the Future of Semantic Segmentation", "comments": "Accepted to ICCV 2017. Supplementary material available on the\n  authors' webpages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict and therefore to anticipate the future is an important\nattribute of intelligence. It is also of utmost importance in real-time\nsystems, e.g. in robotics or autonomous driving, which depend on visual scene\nunderstanding for decision making. While prediction of the raw RGB pixel values\nin future video frames has been studied in previous work, here we introduce the\nnovel task of predicting semantic segmentations of future frames. Given a\nsequence of video frames, our goal is to predict segmentation maps of not yet\nobserved video frames that lie up to a second or further in the future. We\ndevelop an autoregressive convolutional neural network that learns to\niteratively generate multiple frames. Our results on the Cityscapes dataset\nshow that directly predicting future segmentations is substantially better than\npredicting and then segmenting future RGB frames. Prediction results up to half\na second in the future are visually convincing and are much more accurate than\nthose of a baseline based on warping semantic segmentations using optical flow.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 14:45:15 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 13:54:24 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 10:02:36 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Luc", "Pauline", ""], ["Neverova", "Natalia", ""], ["Couprie", "Camille", ""], ["Verbeek", "Jakob", ""], ["LeCun", "Yann", ""]]}, {"id": "1703.07715", "submitter": "Thijs Kooi", "authors": "Thijs Kooi, Nico Karssemeijer", "title": "Classifying Symmetrical Differences and Temporal Change in Mammography\n  Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the addition of symmetry and temporal context information to a\ndeep Convolutional Neural Network (CNN) with the purpose of detecting malignant\nsoft tissue lesions in mammography. We employ a simple linear mapping that\ntakes the location of a mass candidate and maps it to either the contra-lateral\nor prior mammogram and Regions Of Interest (ROI) are extracted around each\nlocation. We subsequently explore two different architectures (1) a fusion\nmodel employing two datastreams were both ROIs are fed to the network during\ntraining and testing and (2) a stage-wise approach where a single ROI CNN is\ntrained on the primary image and subsequently used as feature extractor for\nboth primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree\n(GBT) classifier is then trained on the concatenation of these features and\nused to classify the joint representation. Results shown a significant increase\nin performance using the first architecture and symmetry information, but only\nmarginal gains in performance using temporal data and the other setting. We\nfeel results are promising and can greatly be improved when more temporal data\nbecomes available.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 15:46:49 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:11:36 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kooi", "Thijs", ""], ["Karssemeijer", "Nico", ""]]}, {"id": "1703.07737", "submitter": "Lucas Beyer", "authors": "Alexander Hermans, and Lucas Beyer, and Bastian Leibe", "title": "In Defense of the Triplet Loss for Person Re-Identification", "comments": "Lucas Beyer and Alexander Hermans contributed equally. Updates: Minor\n  fixes, new SOTA comparisons, add CUHK03 results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the field of computer vision has gone through a\nrevolution fueled mainly by the advent of large datasets and the adoption of\ndeep convolutional neural networks for end-to-end learning. The person\nre-identification subfield is no exception to this. Unfortunately, a prevailing\nbelief in the community seems to be that the triplet loss is inferior to using\nsurrogate losses (classification, verification) followed by a separate metric\nlearning step. We show that, for models trained from scratch as well as\npretrained ones, using a variant of the triplet loss to perform end-to-end deep\nmetric learning outperforms most other published methods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:34:29 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 13:39:01 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 10:50:01 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 15:35:07 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Hermans", "Alexander", ""], ["Beyer", "Lucas", ""], ["Leibe", "Bastian", ""]]}, {"id": "1703.07814", "submitter": "Abir Das", "authors": "Huijuan Xu, Abir Das, Kate Saenko", "title": "R-C3D: Region Convolutional 3D Network for Temporal Activity Detection", "comments": "ICCV 2017 Camera Ready Version", "journal-ref": "Proceedings of the International Conference on Computer Vision\n  (ICCV), 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of activity detection in continuous, untrimmed video\nstreams. This is a difficult task that requires extracting meaningful\nspatio-temporal features to capture activities, accurately localizing the start\nand end times of each activity. We introduce a new model, Region Convolutional\n3D Network (R-C3D), which encodes the video streams using a three-dimensional\nfully convolutional network, then generates candidate temporal regions\ncontaining activities, and finally classifies selected regions into specific\nactivities. Computation is saved due to the sharing of convolutional features\nbetween the proposal and the classification pipelines. The entire model is\ntrained end-to-end with jointly optimized localization and classification\nlosses. R-C3D is faster than existing methods (569 frames per second on a\nsingle Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.\nWe further demonstrate that our model is a general activity detection framework\nthat does not rely on assumptions about particular dataset properties by\nevaluating our approach on ActivityNet and Charades. Our code is available at\nhttp://ai.bu.edu/r-c3d/.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 18:49:05 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 22:37:54 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Xu", "Huijuan", ""], ["Das", "Abir", ""], ["Saenko", "Kate", ""]]}, {"id": "1703.07815", "submitter": "Chen Chen", "authors": "Yicong Tian and Chen Chen and Mubarak Shah", "title": "Cross-View Image Matching for Geo-localization in Urban Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of cross-view image geo-localization.\nSpecifically, we aim to estimate the GPS location of a query street view image\nby finding the matching images in a reference database of geo-tagged bird's eye\nview images, or vice versa. To this end, we present a new framework for\ncross-view image geo-localization by taking advantage of the tremendous success\nof deep convolutional neural networks (CNNs) in image classification and object\ndetection. First, we employ the Faster R-CNN to detect buildings in the query\nand reference images. Next, for each building in the query image, we retrieve\nthe $k$ nearest neighbors from the reference buildings using a Siamese network\ntrained on both positive matching image pairs and negative pairs. To find the\ncorrect NN for each query building, we develop an efficient multiple nearest\nneighbors matching method based on dominant sets. We evaluate the proposed\nframework on a new dataset that consists of pairs of street view and bird's eye\nview images. Experimental results show that the proposed method achieves better\ngeo-localization accuracy than other approaches and is able to generalize to\nimages at unseen locations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 18:51:51 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Tian", "Yicong", ""], ["Chen", "Chen", ""], ["Shah", "Mubarak", ""]]}, {"id": "1703.07834", "submitter": "Aaron Jackson", "authors": "Aaron S. Jackson, Adrian Bulat, Vasileios Argyriou, Georgios\n  Tzimiropoulos", "title": "Large Pose 3D Face Reconstruction from a Single Image via Direct\n  Volumetric CNN Regression", "comments": "10 pages, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction is a fundamental Computer Vision problem of\nextraordinary difficulty. Current systems often assume the availability of\nmultiple facial images (sometimes from the same subject) as input, and must\naddress a number of methodological challenges such as establishing dense\ncorrespondences across large facial poses, expressions, and non-uniform\nillumination. In general these methods require complex and inefficient\npipelines for model building and fitting. In this work, we propose to address\nmany of these limitations by training a Convolutional Neural Network (CNN) on\nan appropriate dataset consisting of 2D images and 3D facial models or scans.\nOur CNN works with just a single 2D facial image, does not require accurate\nalignment nor establishes dense correspondence between images, works for\narbitrary facial poses and expressions, and can be used to reconstruct the\nwhole 3D facial geometry (including the non-visible parts of the face)\nbypassing the construction (during training) and fitting (during testing) of a\n3D Morphable Model. We achieve this via a simple CNN architecture that performs\ndirect regression of a volumetric representation of the 3D facial geometry from\na single 2D image. We also demonstrate how the related task of facial landmark\nlocalization can be incorporated into the proposed framework and help improve\nreconstruction quality, especially for the cases of large poses and facial\nexpressions. Testing code will be made available online, along with pre-trained\nmodels http://aaronsplace.co.uk/papers/jackson2017recon\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 20:00:15 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 09:10:08 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Jackson", "Aaron S.", ""], ["Bulat", "Adrian", ""], ["Argyriou", "Vasileios", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1703.07886", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou", "title": "Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling", "comments": "Accepted for publication at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning and component analysis are part of one of the most\nwell-studied and active research fields, at the intersection of signal and\nimage processing, computer vision, and statistical machine learning. In\ndictionary learning, the current methods of choice are arguably K-SVD and its\nvariants, which learn a dictionary (i.e., a decomposition) for sparse coding\nvia Singular Value Decomposition. In robust component analysis, leading methods\nderive from Principal Component Pursuit (PCP), which recovers a low-rank matrix\nfrom sparse corruptions of unknown magnitude and support. However, K-SVD is\nsensitive to the presence of noise and outliers in the training set.\nAdditionally, PCP does not provide a dictionary that respects the structure of\nthe data (e.g., images), and requires expensive SVD computations when solved by\nconvex relaxation. In this paper, we introduce a new robust decomposition of\nimages by combining ideas from sparse dictionary learning and PCP. We propose a\nnovel Kronecker-decomposable component analysis which is robust to gross\ncorruption, can be used for low-rank modeling, and leverages separability to\nsolve significantly smaller problems. We design an efficient learning algorithm\nby drawing links with a restricted form of tensor factorization. The\neffectiveness of the proposed approach is demonstrated on real-world\napplications, namely background subtraction and image denoising, by performing\na thorough comparison with the current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 23:35:51 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:50:13 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Bahri", "Mehdi", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1703.07910", "submitter": "Renlong Hang", "authors": "Qingshan Liu, Feng Zhou, Renlong Hang and Xiaotong Yuan", "title": "Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning\n  for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep learning framework named\nbidirectional-convolutional long short term memory (Bi-CLSTM) network to\nautomatically learn the spectral-spatial feature from hyperspectral images\n(HSIs). In the network, the issue of spectral feature extraction is considered\nas a sequence learning problem, and a recurrent connection operator across the\nspectral domain is used to address it. Meanwhile, inspired from the widely used\nconvolutional neural network (CNN), a convolution operator across the spatial\ndomain is incorporated into the network to extract the spatial feature.\nBesides, to sufficiently capture the spectral information, a bidirectional\nrecurrent connection is proposed. In the classification phase, the learned\nfeatures are concatenated into a vector and fed to a softmax classifier via a\nfully-connected operator. To validate the effectiveness of the proposed\nBi-CLSTM framework, we compare it with several state-of-the-art methods,\nincluding the CNN framework, on three widely used HSIs. The obtained results\nshow that Bi-CLSTM can improve the classification performance as compared to\nother methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 02:50:32 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Liu", "Qingshan", ""], ["Zhou", "Feng", ""], ["Hang", "Renlong", ""], ["Yuan", "Xiaotong", ""]]}, {"id": "1703.07915", "submitter": "Dhagash Mehta", "authors": "Andrew J. Ballard, Ritankar Das, Stefano Martiniani, Dhagash Mehta,\n  Levent Sagun, Jacob D. Stevenson, David J. Wales", "title": "Perspective: Energy Landscapes for Machine Learning", "comments": "41 pages, 25 figures. Accepted for publication in Physical Chemistry\n  Chemical Physics, 2017", "journal-ref": null, "doi": "10.1039/C7CP01108C", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV cs.LG hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are being increasingly used as flexible\nnon-linear fitting and prediction tools in the physical sciences. Fitting\nfunctions that exhibit multiple solutions as local minima can be analysed in\nterms of the corresponding machine learning landscape. Methods to explore and\nvisualise molecular potential energy landscapes can be applied to these machine\nlearning landscapes to gain new insight into the solution space involved in\ntraining and the nature of the corresponding predictions. In particular, we can\ndefine quantities analogous to molecular structure, thermodynamics, and\nkinetics, and relate these emergent properties to the structure of the\nunderlying landscape. This Perspective aims to describe these analogies with\nexamples from recent applications, and suggest avenues for new\ninterdisciplinary research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 03:17:14 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ballard", "Andrew J.", ""], ["Das", "Ritankar", ""], ["Martiniani", "Stefano", ""], ["Mehta", "Dhagash", ""], ["Sagun", "Levent", ""], ["Stevenson", "Jacob D.", ""], ["Wales", "David J.", ""]]}, {"id": "1703.07920", "submitter": "Kaori Abe", "authors": "Kaori Abe, Teppei Suzuki, Shunya Ueta, Akio Nakamura, Yutaka Satoh and\n  Hirokatsu Kataoka", "title": "Changing Fashion Cultures", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel concept that analyzes and visualizes worldwide\nfashion trends. Our goal is to reveal cutting-edge fashion trends without\ndisplaying an ordinary fashion style. To achieve the fashion-based analysis, we\ncreated a new fashion culture database (FCDB), which consists of 76 million\ngeo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of\nmixed fashion styles,the paper also proposes an unsupervised fashion trend\ndescriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal\nanalysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD\neffectively emphasizes consecutive features between two different times. In\nexperiments, we clearly show the analysis of fashion trends and fashion-based\ncity similarity. As the result of large-scale data collection and an\nunsupervised analyzer, the proposed approach achieves world-level fashion\nvisualization in a time series. The code, model, and FCDB will be publicly\navailable after the construction of the project page.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 03:48:08 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Abe", "Kaori", ""], ["Suzuki", "Teppei", ""], ["Ueta", "Shunya", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1703.07928", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Arpit Jain, Ser Nam Lim", "title": "Self corrective Perturbations for Semantic Segmentation and\n  Classification", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been a subject of great importance over\nthe past decade and great strides have been made in their utility for producing\nstate of the art performance in many computer vision problems. However, the\nbehavior of deep networks is yet to be fully understood and is still an active\narea of research. In this work, we present an intriguing behavior: pre-trained\nCNNs can be made to improve their predictions by structurally perturbing the\ninput. We observe that these perturbations - referred as Guided Perturbations -\nenable a trained network to improve its prediction performance without any\nlearning or change in network weights. We perform various ablative experiments\nto understand how these perturbations affect the local context and feature\nrepresentations. Furthermore, we demonstrate that this idea can improve\nperformance of several existing approaches on semantic segmentation and scene\nlabeling tasks on the PASCAL VOC dataset and supervised classification tasks on\nMNIST and CIFAR10 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 04:25:48 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 15:42:06 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Jain", "Arpit", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1703.07938", "submitter": "Pengpeng Liang", "authors": "Pengpeng Liang, Yifan Wu, Hu Lu, Liming Wang, Chunyuan Liao, Haibin\n  Ling", "title": "Planar Object Tracking in the Wild: A Benchmark", "comments": "Accepted by ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar object tracking is an actively studied problem in vision-based robotic\napplications. While several benchmarks have been constructed for evaluating\nstate-of-the-art algorithms, there is a lack of video sequences captured in the\nwild rather than in constrained laboratory environment. In this paper, we\npresent a carefully designed planar object tracking benchmark containing 210\nvideos of 30 planar objects sampled in the natural environment. In particular,\nfor each object, we shoot seven videos involving various challenging factors,\nnamely scale change, rotation, perspective distortion, motion blur, occlusion,\nout-of-view, and unconstrained. The ground truth is carefully annotated\nsemi-manually to ensure the quality. Moreover, eleven state-of-the-art\nalgorithms are evaluated on the benchmark using two evaluation metrics, with\ndetailed analysis provided for the evaluation results. We expect the proposed\nbenchmark to benefit future studies on planar object tracking.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 05:21:24 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 06:54:43 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liang", "Pengpeng", ""], ["Wu", "Yifan", ""], ["Lu", "Hu", ""], ["Wang", "Liming", ""], ["Liao", "Chunyuan", ""], ["Ling", "Haibin", ""]]}, {"id": "1703.07939", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille", "title": "Recurrent Multimodal Interaction for Referring Image Segmentation", "comments": "To appear in ICCV 2017. See http://www.cs.jhu.edu/~cxliu/ for code\n  and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in the problem of image segmentation given\nnatural language descriptions, i.e. referring expressions. Existing works\ntackle this problem by first modeling images and sentences independently and\nthen segment images by combining these two types of representations. We argue\nthat learning word-to-image interaction is more native in the sense of jointly\nmodeling two modalities for the image segmentation task, and we propose\nconvolutional multimodal LSTM to encode the sequential interactions between\nindividual words, visual information, and spatial information. We show that our\nproposed model outperforms the baseline model on benchmark datasets. In\naddition, we analyze the intermediate output of the proposed multimodal LSTM\napproach and empirically explain how this approach enforces a more effective\nword-to-image interaction.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 05:22:22 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 21:53:15 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Liu", "Chenxi", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jimei", ""], ["Lu", "Xin", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.07957", "submitter": "Yohann Salaun", "authors": "Yohann Salaun, Renaud Marlet, and Pascal Monasse", "title": "Robust SfM with Little Image Overlap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usual Structure-from-Motion (SfM) techniques require at least trifocal\noverlaps to calibrate cameras and reconstruct a scene. We consider here\nscenarios of reduced image sets with little overlap, possibly as low as two\nimages at most seeing the same part of the scene. We propose a new method,\nbased on line coplanarity hypotheses, for estimating the relative scale of two\nindependent bifocal calibrations sharing a camera, without the need of any\ntrifocal information or Manhattan-world assumption. We use it to compute SfM in\na chain of up-to-scale relative motions. For accuracy, we however also make use\nof trifocal information for line and/or point features, when present, relaxing\nusual trifocal constraints. For robustness to wrong assumptions and mismatches,\nwe embed all constraints in a parameterless RANSAC-like approach. Experiments\nshow that we can calibrate datasets that previously could not, and that this\nwider applicability does not come at the cost of inaccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 07:52:31 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 09:57:56 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Salaun", "Yohann", ""], ["Marlet", "Renaud", ""], ["Monasse", "Pascal", ""]]}, {"id": "1703.07971", "submitter": "Iaroslav Melekhov", "authors": "Iaroslav Melekhov, Juha Ylioinas, Juho Kannala and Esa Rahtu", "title": "Image-based Localization using Hourglass Networks", "comments": "Camera-ready version for ICCVW 2017 (fixed glitches in abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an encoder-decoder convolutional neural network\n(CNN) architecture for estimating camera pose (orientation and location) from a\nsingle RGB-image. The architecture has a hourglass shape consisting of a chain\nof convolution and up-convolution layers followed by a regression part. The\nup-convolution layers are introduced to preserve the fine-grained information\nof the input image. Following the common practice, we train our model in\nend-to-end manner utilizing transfer learning from large scale classification\ndata. The experiments demonstrate the performance of the approach on data\nexhibiting different lighting conditions, reflections, and motion blur. The\nresults indicate a clear improvement over the previous state-of-the-art even\nwhen compared to methods that utilize sequence of test frames instead of a\nsingle frame.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 09:06:13 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 09:18:45 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 06:10:26 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Melekhov", "Iaroslav", ""], ["Ylioinas", "Juha", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1703.07980", "submitter": "Fengfu Li", "authors": "Fengfu Li, Hong Qiao, Bo Zhang, Xuanyang Xi", "title": "Discriminatively Boosted Image Clustering with Fully Convolutional\n  Auto-Encoders", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image clustering methods take a two-step approach, feature\nlearning and clustering, sequentially. However, recent research results\ndemonstrated that combining the separated phases in a unified framework and\ntraining them jointly can achieve a better performance. In this paper, we first\nintroduce fully convolutional auto-encoders for image feature learning and then\npropose a unified clustering framework to learn image representations and\ncluster centers jointly based on a fully convolutional auto-encoder and soft\n$k$-means scores. At initial stages of the learning procedure, the\nrepresentations extracted from the auto-encoder may not be very discriminative\nfor latter clustering. We address this issue by adopting a boosted\ndiscriminative distribution, where high score assignments are highlighted and\nlow score ones are de-emphasized. With the gradually boosted discrimination,\nclustering assignment scores are discriminated and cluster purities are\nenlarged. Experiments on several vision benchmark datasets show that our\nmethods can achieve a state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 09:49:37 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Li", "Fengfu", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""], ["Xi", "Xuanyang", ""]]}, {"id": "1703.08000", "submitter": "Holger Caesar", "authors": "Miaojing Shi, Holger Caesar and Vittorio Ferrari", "title": "Weakly Supervised Object Localization Using Things and Stuff Transfer", "comments": "ICCV 2017 camera-ready including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to help weakly supervised object localization for classes where\nlocation annotations are not available, by transferring things and stuff\nknowledge from a source set with available annotations. The source and target\nclasses might share similar appearance (e.g. bear fur is similar to cat fur) or\nappear against similar background (e.g. horse and sheep appear against grass).\nTo exploit this, we acquire three types of knowledge from the source set: a\nsegmentation model trained on both thing and stuff classes; similarity\nrelations between target and source classes; and co-occurrence relations\nbetween thing and stuff classes in the source. The segmentation model is used\nto generate thing and stuff segmentation maps on a target image, while the\nclass similarity and co-occurrence knowledge help refining them. We then\nincorporate these maps as new cues into a multiple instance learning framework\n(MIL), propagating the transferred knowledge from the pixel level to the object\nproposal level. In extensive experiments, we conduct our transfer from the\nPASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007\ndatasets (targets). We evaluate our transfer across widely different thing\nclasses, including some that are not similar in appearance, but appear against\nsimilar background. The results demonstrate significant improvement over\nstandard MIL, and we outperform the state-of-the-art in the transfer setting.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:01:27 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 14:00:41 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Shi", "Miaojing", ""], ["Caesar", "Holger", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1703.08001", "submitter": "Martin Benning", "authors": "Martin Benning, Michael M\\\"oller, Raz Z. Nossek, Martin Burger, Daniel\n  Cremers, Guy Gilboa and Carola-Bibiane Sch\\\"onlieb", "title": "Nonlinear Spectral Image Fusion", "comments": "13 pages, 9 figures, submitted to SSVM conference proceedings 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate that the framework of nonlinear spectral\ndecompositions based on total variation (TV) regularization is very well suited\nfor image fusion as well as more general image manipulation tasks. The\nwell-localized and edge-preserving spectral TV decomposition allows to select\nfrequencies of a certain image to transfer particular features, such as\nwrinkles in a face, from one image to another. We illustrate the effectiveness\nof the proposed approach in several numerical experiments, including a\ncomparison to the competing techniques of Poisson image editing, linear\nosmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the\nproposed spectral TV image decomposition framework is a valuable tool for semi-\nand fully-automatic image editing and fusion.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:02:42 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Benning", "Martin", ""], ["M\u00f6ller", "Michael", ""], ["Nossek", "Raz Z.", ""], ["Burger", "Martin", ""], ["Cremers", "Daniel", ""], ["Gilboa", "Guy", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1703.08013", "submitter": "Mao Tan", "authors": "Mao Tan, Si-Ping Yuan, Yong-Xin Su", "title": "Content-based similar document image retrieval using fusion of CNN\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid increase of digitized document give birth to high demand of document\nimage retrieval. While conventional document image retrieval approaches depend\non complex OCR-based text recognition and text similarity detection, this paper\nproposes a new content-based approach, in which more attention is paid to\nfeatures extraction and fusion. In the proposed approach, multiple features of\ndocument images are extracted by different CNN models. After that, the\nextracted CNN features are reduced and fused into weighted average feature.\nFinally, the document images are ranked based on feature similarity to a\nprovided query image. Experimental procedure is performed on a group of\ndocument images that transformed from academic papers, which contain both\nEnglish and Chinese document, the results show that the proposed approach has\ngood ability to retrieve document images with similar text content, and the\nfusion of CNN features can effectively improve the retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:35:27 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 09:30:41 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 00:34:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tan", "Mao", ""], ["Yuan", "Si-Ping", ""], ["Su", "Yong-Xin", ""]]}, {"id": "1703.08014", "submitter": "Timo von Marcard", "authors": "Timo von Marcard, Bodo Rosenhahn, Michael J. Black, Gerard Pons-Moll", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse\n  IMUs", "comments": "12 pages, Accepted at Eurographics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of making human motion capture in the wild more\npractical by using a small set of inertial sensors attached to the body. Since\nthe problem is heavily under-constrained, previous methods either use a large\nnumber of sensors, which is intrusive, or they require additional video input.\nWe take a different approach and constrain the problem by: (i) making use of a\nrealistic statistical body model that includes anthropometric constraints and\n(ii) using a joint optimization framework to fit the model to orientation and\nacceleration measurements over multiple frames. The resulting tracker Sparse\nInertial Poser (SIP) enables 3D human pose estimation using only 6 sensors\n(attached to the wrists, lower legs, back and head) and works for arbitrary\nhuman motions. Experiments on the recently released TNT15 dataset show that,\nusing the same number of sensors, SIP achieves higher accuracy than the dataset\nbaseline without using any video data. We further demonstrate the effectiveness\nof SIP on newly recorded challenging motions in outdoor scenarios such as\nclimbing or jumping over a wall.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:35:41 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 08:24:07 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["von Marcard", "Timo", ""], ["Rosenhahn", "Bodo", ""], ["Black", "Michael J.", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1703.08025", "submitter": "Yunzhen Zhao", "authors": "Yunzhen Zhao and Yuxin Peng", "title": "Saliency-guided video classification via adaptively weighted learning", "comments": "6 pages, 1 figure, accepted by ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification is productive in many practical applications, and the\nrecent deep learning has greatly improved its accuracy. However, existing works\noften model video frames indiscriminately, but from the view of motion, video\nframes can be decomposed into salient and non-salient areas naturally. Salient\nand non-salient areas should be modeled with different networks, for the former\npresent both appearance and motion information, and the latter present static\nbackground information. To address this problem, in this paper, video saliency\nis predicted by optical flow without supervision firstly. Then two streams of\n3D CNN are trained individually for raw frames and optical flow on salient\nareas, and another 2D CNN is trained for raw frames on non-salient areas. For\nthe reason that these three streams play different roles for each class, the\nweights of each stream are adaptively learned for each class. Experimental\nresults show that saliency-guided modeling and adaptively weighted learning can\nreinforce each other, and we achieve the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:02:21 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 02:43:13 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Zhao", "Yunzhen", ""], ["Peng", "Yuxin", ""]]}, {"id": "1703.08033", "submitter": "Akshay Mehotra", "authors": "Akshay Mehrotra, Ambedkar Dukkipati", "title": "Generative Adversarial Residual Pairwise Networks for One Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve unprecedented performance levels over many tasks\nand scale well with large quantities of data, but performance in the low-data\nregime and tasks like one shot learning still lags behind. While recent work\nsuggests many hypotheses from better optimization to more complicated network\nstructures, in this work we hypothesize that having a learnable and more\nexpressive similarity objective is an essential missing component. Towards\novercoming that, we propose a network design inspired by deep residual networks\nthat allows the efficient computation of this more expressive pairwise\nsimilarity objective. Further, we argue that regularization is key in learning\nwith small amounts of data, and propose an additional generator network based\non the Generative Adversarial Networks where the discriminator is our residual\npairwise network. This provides a strong regularizer by leveraging the\ngenerated data samples. The proposed model can generate plausible variations of\nexemplars over unseen classes and outperforms strong discriminative baselines\nfor few shot classification tasks. Notably, our residual pairwise network\ndesign outperforms previous state-of-theart on the challenging mini-Imagenet\ndataset for one shot learning by getting over 55% accuracy for the 5-way\nclassification task over unseen classes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:19:09 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Mehrotra", "Akshay", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1703.08050", "submitter": "Peihua Li", "authors": "Peihua Li and Jiangtao Xie and Qilong Wang and Wangmeng Zuo", "title": "Is Second-order Information Helpful for Large-scale Visual Recognition?", "comments": "accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By stacking layers of convolution and nonlinearity, convolutional networks\n(ConvNets) effectively learn from low-level to high-level features and\ndiscriminative representations. Since the end goal of large-scale recognition\nis to delineate complex boundaries of thousands of classes, adequate\nexploration of feature distributions is important for realizing full potentials\nof ConvNets. However, state-of-the-art works concentrate only on deeper or\nwider architecture design, while rarely exploring feature statistics higher\nthan first-order. We take a step towards addressing this problem. Our method\nconsists in covariance pooling, instead of the most commonly used first-order\npooling, of high-level convolutional features. The main challenges involved are\nrobust covariance estimation given a small sample of large-dimensional features\nand usage of the manifold structure of covariance matrices. To address these\nchallenges, we present a Matrix Power Normalized Covariance (MPN-COV) method.\nWe develop forward and backward propagation formulas regarding the nonlinear\nmatrix functions such that MPN-COV can be trained end-to-end. In addition, we\nanalyze both qualitatively and quantitatively its advantage over the well-known\nLog-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV\nwe achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16,\nrespectively; integration of MPN-COV into 50-layer ResNet outperforms\nResNet-101 and is comparable to ResNet-152. The source code will be available\non the project page: http://www.peihuali.org/MPN-COV\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:55:34 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 11:54:19 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 22:24:31 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Li", "Peihua", ""], ["Xie", "Jiangtao", ""], ["Wang", "Qilong", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1703.08089", "submitter": "Alexander Richard", "authors": "Alexander Richard (1), Juergen Gall (1) ((1) University of Bonn)", "title": "A Bag-of-Words Equivalent Recurrent Neural Network for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional bag-of-words approach has found a wide range of applications\nin computer vision. The standard pipeline consists of a generation of a visual\nvocabulary, a quantization of the features into histograms of visual words, and\na classification step for which usually a support vector machine in combination\nwith a non-linear kernel is used. Given large amounts of data, however, the\nmodel suffers from a lack of discriminative power. This applies particularly\nfor action recognition, where the vast amount of video features needs to be\nsubsampled for unsupervised visual vocabulary generation. Moreover, the kernel\ncomputation can be very expensive on large datasets. In this work, we propose a\nrecurrent neural network that is equivalent to the traditional bag-of-words\napproach but enables for the application of discriminative training. The model\nfurther allows to incorporate the kernel computation into the neural network\ndirectly, solving the complexity issue and allowing to represent the complete\nclassification system within a single network. We evaluate our method on four\nrecent action recognition benchmarks and show that the conventional model as\nwell as sparse coding methods are outperformed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 14:46:46 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Richard", "Alexander", "", "University of Bonn"], ["Gall", "Juergen", "", "University of Bonn"]]}, {"id": "1703.08119", "submitter": "Samuel Dodge", "authors": "Samuel Dodge and Lina Karam", "title": "Quality Resilient Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deep neural networks for classification of images with quality\ndistortions. We first show that networks fine-tuned on distorted data greatly\noutperform the original networks when tested on distorted data. However,\nfine-tuned networks perform poorly on quality distortions that they have not\nbeen trained for. We propose a mixture of experts ensemble method that is\nrobust to different types of distortions. The \"experts\" in our model are\ntrained on a particular type of distortion. The output of the model is a\nweighted sum of the expert models, where the weights are determined by a\nseparate gating network. The gating network is trained to predict optimal\nweights for a particular distortion type and level. During testing, the network\nis blind to the distortion level and type, yet can still assign appropriate\nweights to the expert models. We additionally investigate weight sharing\nmethods for the mixture model and show that improved performance can be\nachieved with a large reduction in the number of unique network parameters.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 15:56:33 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Dodge", "Samuel", ""], ["Karam", "Lina", ""]]}, {"id": "1703.08120", "submitter": "Abhijit Sharang", "authors": "Abhijit Sharang, Eric Lau", "title": "Recurrent and Contextual Models for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a series of recurrent and contextual neural network models for\nmultiple choice visual question answering on the Visual7W dataset. Motivated by\ndivergent trends in model complexities in the literature, we explore the\nbalance between model expressiveness and simplicity by studying incrementally\nmore complex architectures. We start with LSTM-encoding of input questions and\nanswers; build on this with context generation by LSTM-encodings of neural\nimage and question representations and attention over images; and evaluate the\ndiversity and predictive power of our models and the ensemble thereof. All\nmodels are evaluated against a simple baseline inspired by the current\nstate-of-the-art, consisting of involving simple concatenation of bag-of-words\nand CNN representations for the text and images, respectively. Generally, we\nobserve marked variation in image-reasoning performance between our models not\nobvious from their overall performance, as well as evidence of dataset bias.\nOur standalone models achieve accuracies up to $64.6\\%$, while the ensemble of\nall models achieves the best accuracy of $66.67\\%$, within $0.5\\%$ of the\ncurrent state-of-the-art for Visual7W.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 15:57:23 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Sharang", "Abhijit", ""], ["Lau", "Eric", ""]]}, {"id": "1703.08132", "submitter": "Alexander Richard", "authors": "Alexander Richard, Hilde Kuehne, Juergen Gall", "title": "Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for weakly supervised learning of human actions. Given\na set of videos and an ordered list of the occurring actions, the goal is to\ninfer start and end frames of the related action classes within the video and\nto train the respective action classifiers without any need for hand labeled\nframe boundaries. To address this task, we propose a combination of a\ndiscriminative representation of subactions, modeled by a recurrent neural\nnetwork, and a coarse probabilistic model to allow for a temporal alignment and\ninference over long sequences. While this system alone already generates good\nresults, we show that the performance can be further improved by approximating\nthe number of subactions to the characteristics of the different action\nclasses. To this end, we adapt the number of subaction classes by iterating\nrealignment and reestimation during training. The proposed system is evaluated\non two benchmark datasets, the Breakfast and the Hollywood extended dataset,\nshowing a competitive performance on various weak learning tasks such as\ntemporal action segmentation and action alignment.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 16:31:48 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 12:12:37 GMT"}, {"version": "v3", "created": "Mon, 9 Oct 2017 10:10:07 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Richard", "Alexander", ""], ["Kuehne", "Hilde", ""], ["Gall", "Juergen", ""]]}, {"id": "1703.08136", "submitter": "Herman Kamper", "authors": "Herman Kamper, Shane Settle, Gregory Shakhnarovich, Karen Livescu", "title": "Visually grounded learning of keyword prediction from untranscribed\n  speech", "comments": "5 pages, 3 figures, 5 tables; small updates, added link to code;\n  accepted to Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During language acquisition, infants have the benefit of visual cues to\nground spoken language. Robots similarly have access to audio and visual\nsensors. Recent work has shown that images and spoken captions can be mapped\ninto a meaningful common space, allowing images to be retrieved using speech\nand vice versa. In this setting of images paired with untranscribed spoken\ncaptions, we consider whether computer vision systems can be used to obtain\ntextual labels for the speech. Concretely, we use an image-to-words multi-label\nvisual classifier to tag images with soft textual labels, and then train a\nneural network to map from the speech to these soft targets. We show that the\nresulting speech system is able to predict which words occur in an\nutterance---acting as a spoken bag-of-words classifier---without seeing any\nparallel speech and text. We find that the model often confuses semantically\nrelated words, e.g. \"man\" and \"person\", making it even more effective as a\nsemantic keyword spotter.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 16:46:00 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 20:49:15 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Kamper", "Herman", ""], ["Settle", "Shane", ""], ["Shakhnarovich", "Gregory", ""], ["Livescu", "Karen", ""]]}, {"id": "1703.08173", "submitter": "Yudong Liang", "authors": "Yudong Liang, Ze Yang, Kai Zhang, Yihui He, Jinjun Wang and Nanning\n  Zheng", "title": "Single Image Super-resolution via a Lightweight Residual Convolutional\n  Neural Network", "comments": "Extentions of mmm 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed great success of convolutional neural network\n(CNN) for various problems both in low and high level visions. Especially\nnoteworthy is the residual network which was originally proposed to handle\nhigh-level vision problems and enjoys several merits. This paper aims to extend\nthe merits of residual network, such as skip connection induced fast training,\nfor a typical low-level vision problem, i.e., single image super-resolution. In\ngeneral, the two main challenges of existing deep CNN for supper-resolution lie\nin the gradient exploding/vanishing problem and large numbers of parameters or\ncomputational cost as CNN goes deeper. Correspondingly, the skip connections or\nidentity mapping shortcuts are utilized to avoid gradient exploding/vanishing\nproblem. In addition, the skip connections have naturally centered the\nactivation which led to better performance. To tackle with the second problem,\na lightweight CNN architecture which has carefully designed width, depth and\nskip connections was proposed. In particular, a strategy of gradually varying\nthe shape of network has been proposed for residual network. Different residual\narchitectures for image super-resolution have also been compared. Experimental\nresults have demonstrated that the proposed CNN model can not only achieve\nstate-of-the-art PSNR and SSIM results for single image super-resolution but\nalso produce visually pleasant results. This paper has extended the mmm 2017\noral conference paper with a considerable new analyses and more experiments\nespecially from the perspective of centering activations and ensemble behaviors\nof residual network.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 01:51:14 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 13:43:45 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Liang", "Yudong", ""], ["Yang", "Ze", ""], ["Zhang", "Kai", ""], ["He", "Yihui", ""], ["Wang", "Jinjun", ""], ["Zheng", "Nanning", ""]]}, {"id": "1703.08238", "submitter": "Mohammad Saad Billah", "authors": "Mohammad Saad Billah and Tahmida Binte Mahmud", "title": "Semi-Automatic Segmentation and Ultrasonic Characterization of Solid\n  Breast Lesions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of breast lesions is an essential prerequisite to detect\nbreast cancer in an early stage. Automatic segmentation makes this\ncategorization method robust by freeing it from subjectivity and human error.\nBoth spectral and morphometric features are successfully used for\ndifferentiating between benign and malignant breast lesions. In this thesis, we\nused empirical mode decomposition method for semi-automatic segmentation.\nSonographic features like ehcogenicity, heterogeneity, FNPA, margin definition,\nHurst coefficient, compactness, roundness, aspect ratio, convexity, solidity,\nform factor were calculated to be used as our characterization parameters. All\nof these parameters did not give desired comparative results. But some of them\nnamely echogenicity, heterogeneity, margin definition, aspect ratio and\nconvexity gave good results and were used for characterization.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 21:25:48 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Billah", "Mohammad Saad", ""], ["Mahmud", "Tahmida Binte", ""]]}, {"id": "1703.08245", "submitter": "Martin Schrimpf", "authors": "Nicholas Cheney, Martin Schrimpf, Gabriel Kreiman", "title": "On the Robustness of Convolutional Neural Networks to Internal\n  Architecture and Weight Perturbations", "comments": "under review at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are generally regarded as robust function\napproximators. So far, this intuition is based on perturbations to external\nstimuli such as the images to be classified. Here we explore the robustness of\nconvolutional neural networks to perturbations to the internal weights and\narchitecture of the network itself. We show that convolutional networks are\nsurprisingly robust to a number of internal perturbations in the higher\nconvolutional layers but the bottom convolutional layers are much more fragile.\nFor instance, Alexnet shows less than a 30% decrease in classification\nperformance when randomly removing over 70% of weight connections in the top\nconvolutional or dense layers but performance is almost at chance with the same\nperturbation in the first convolutional layer. Finally, we suggest further\ninvestigations which could continue to inform the robustness of convolutional\nnetworks to internal perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 22:25:05 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Cheney", "Nicholas", ""], ["Schrimpf", "Martin", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1703.08274", "submitter": "Cuiling Lan", "authors": "Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue,\n  Nanning Zheng", "title": "View Adaptive Recurrent Neural Networks for High Performance Human\n  Action Recognition from Skeleton Data", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has recently attracted increasing\nattention due to the popularity of 3D skeleton data. One main challenge lies in\nthe large view variations in captured human actions. We propose a novel view\nadaptation scheme to automatically regulate observation viewpoints during the\noccurrence of an action. Rather than re-positioning the skeletons based on a\nhuman defined prior criterion, we design a view adaptive recurrent neural\nnetwork (RNN) with LSTM architecture, which enables the network itself to adapt\nto the most suitable observation viewpoints from end to end. Extensive\nexperiment analyses show that the proposed view adaptive RNN model strives to\n(1) transform the skeletons of various views to much more consistent viewpoints\nand (2) maintain the continuity of the action rather than transforming every\nframe to the same position with the same body orientation. Our model achieves\nsignificant improvement over the state-of-the-art approaches on three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 03:01:29 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 10:21:13 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Zhang", "Pengfei", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Xue", "Jianru", ""], ["Zheng", "Nanning", ""]]}, {"id": "1703.08289", "submitter": "Wenhao He", "authors": "Wenhao He, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu", "title": "Deep Direct Regression for Multi-Oriented Scene Text Detection", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first provide a new perspective to divide existing high\nperformance object detection methods into direct and indirect regressions.\nDirect regression performs boundary regression by predicting the offsets from a\ngiven point, while indirect regression predicts the offsets from some bounding\nbox proposals. Then we analyze the drawbacks of the indirect regression, which\nthe recent state-of-the-art detection structures like Faster-RCNN and SSD\nfollows, for multi-oriented scene text detection, and point out the potential\nsuperiority of direct regression. To verify this point of view, we propose a\ndeep direct regression based method for multi-oriented scene text detection.\nOur detection framework is simple and effective with a fully convolutional\nnetwork and one-step post processing. The fully convolutional network is\noptimized in an end-to-end way and has bi-task outputs where one is pixel-wise\nclassification between text and non-text, and the other is direct regression to\ndetermine the vertex coordinates of quadrilateral text boundaries. The proposed\nmethod is particularly beneficial for localizing incidental scene texts. On the\nICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure\nof 81%, which is a new state-of-the-art and significantly outperforms previous\napproaches. On other standard datasets with focused scene texts, our method\nalso reaches the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 05:54:11 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["He", "Wenhao", ""], ["Zhang", "Xu-Yao", ""], ["Yin", "Fei", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1703.08338", "submitter": "Michael Wray", "authors": "Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas and Dima Damen", "title": "Improving Classification by Improving Labelling: Introducing\n  Probabilistic Multi-Label Object Interaction Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deviates from easy-to-define class boundaries for object\ninteractions. For the task of object interaction recognition, often captured\nusing an egocentric view, we show that semantic ambiguities in verbs and\nrecognising sub-interactions along with concurrent interactions result in\nlegitimate class overlaps (Figure 1). We thus aim to model the mapping between\nobservations and interaction classes, as well as class overlaps, towards a\nprobabilistic multi-label classifier that emulates human annotators. Given a\nvideo segment containing an object interaction, we model the probability for a\nverb, out of a list of possible verbs, to be used to annotate that interaction.\nThe proba- bility is learnt from crowdsourced annotations, and is tested on two\npublic datasets, comprising 1405 video sequences for which we provide\nannotations on 90 verbs. We outper- form conventional single-label\nclassification by 11% and 6% on the two datasets respectively, and show that\nlearning from annotation probabilities outperforms majority voting and enables\ndiscovery of co-occurring labels.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 10:11:03 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 16:29:22 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Wray", "Michael", ""], ["Moltisanti", "Davide", ""], ["Mayol-Cuevas", "Walterio", ""], ["Damen", "Dima", ""]]}, {"id": "1703.08359", "submitter": "Song Bai", "authors": "Song Bai, Xiang Bai, Qi Tian", "title": "Scalable Person Re-identification on Supervised Smoothed Manifold", "comments": "Accepted as spotlight by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification algorithms either extract robust\nvisual features or learn discriminative metrics for person images. However, the\nunderlying manifold which those images reside on is rarely investigated. That\nraises a problem that the learned metric is not smooth with respect to the\nlocal geometry structure of the data manifold.\n  In this paper, we study person re-identification with manifold-based affinity\nlearning, which did not receive enough attention from this area. An\nunconventional manifold-preserving algorithm is proposed, which can 1) make the\nbest use of supervision from training data, whose label information is given as\npairwise constraints; 2) scale up to large repositories with low on-line time\ncomplexity; and 3) be plunged into most existing algorithms, serving as a\ngeneric postprocessing procedure to further boost the identification\naccuracies. Extensive experimental results on five popular person\nre-identification benchmarks consistently demonstrate the effectiveness of our\nmethod. Especially, on the largest CUHK03 and Market-1501, our method\noutperforms the state-of-the-art alternatives by a large margin with high\nefficiency, which is more appropriate for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:17:00 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Bai", "Song", ""], ["Bai", "Xiang", ""], ["Tian", "Qi", ""]]}, {"id": "1703.08366", "submitter": "Mohamed Moustafa", "authors": "Hussein Adly and Mohamed Moustafa", "title": "A Hybrid Deep Learning Approach for Texture Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture classification is a problem that has various applications such as\nremote sensing and forest species recognition. Solutions tend to be custom fit\nto the dataset used but fails to generalize. The Convolutional Neural Network\n(CNN) in combination with Support Vector Machine (SVM) form a robust selection\nbetween powerful invariant feature extractor and accurate classifier. The\nfusion of experts provides stability in classification rates among different\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:39:26 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Adly", "Hussein", ""], ["Moustafa", "Mohamed", ""]]}, {"id": "1703.08378", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Muxin Sun, Wei Wang, Feilong Wang", "title": "Feature Fusion using Extended Jaccard Graph and Stochastic Gradient\n  Descent for Robot", "comments": "Assembly Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot vision is a fundamental device for human-robot interaction and robot\ncomplex tasks. In this paper, we use Kinect and propose a feature graph fusion\n(FGF) for robot recognition. Our feature fusion utilizes RGB and depth\ninformation to construct fused feature from Kinect. FGF involves multi-Jaccard\nsimilarity to compute a robust graph and utilize word embedding method to\nenhance the recognition results. We also collect DUT RGB-D face dataset and a\nbenchmark datset to evaluate the effectiveness and efficiency of our method.\nThe experimental results illustrate FGF is robust and effective to face and\nobject datasets in robot applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:58:14 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Liu", "Shenglan", ""], ["Sun", "Muxin", ""], ["Wang", "Wei", ""], ["Wang", "Feilong", ""]]}, {"id": "1703.08388", "submitter": "Md Abul Hasnat", "authors": "Abul Hasnat, Julien Bohn\\'e, Jonathan Milgram, St\\'ephane Gentric, and\n  Liming Chen", "title": "DeepVisage: Making face recognition simple yet with powerful\n  generalization skills", "comments": "Second version (12 pages), under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) methods report significant performance by adopting the\nconvolutional neural network (CNN) based learning methods. Although CNNs are\nmostly trained by optimizing the softmax loss, the recent trend shows an\nimprovement of accuracy with different strategies, such as task-specific CNN\nlearning with different loss functions, fine-tuning on target dataset, metric\nlearning and concatenating features from multiple CNNs. Incorporating these\ntasks obviously requires additional efforts. Moreover, it demotivates the\ndiscovery of efficient CNN models for FR which are trained only with identity\nlabels. We focus on this fact and propose an easily trainable and single CNN\nbased FR method. Our CNN model exploits the residual learning framework.\nAdditionally, it uses normalized features to compute the loss. Our extensive\nexperiments show excellent generalization on different datasets. We obtain very\ncompetitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and\nCACD datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 12:41:38 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 11:37:21 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Hasnat", "Abul", ""], ["Bohn\u00e9", "Julien", ""], ["Milgram", "Jonathan", ""], ["Gentric", "St\u00e9phane", ""], ["Chen", "Liming", ""]]}, {"id": "1703.08448", "submitter": "Yunchao Wei", "authors": "Yunchao Wei and Jiashi Feng and Xiaodan Liang and Ming-Ming Cheng and\n  Yao Zhao and Shuicheng Yan", "title": "Object Region Mining with Adversarial Erasing: A Simple Classification\n  to Semantic Segmentation Approach", "comments": "Accepted to appear in CVPR 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 15:05:38 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 07:23:05 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 01:19:51 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wei", "Yunchao", ""], ["Feng", "Jiashi", ""], ["Liang", "Xiaodan", ""], ["Cheng", "Ming-Ming", ""], ["Zhao", "Yao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1703.08472", "submitter": "Syed Anwar", "authors": "Adnan Qayyum, Syed Muhammad Anwar, Muhammad Awais, Muhammad Majid", "title": "Medical Image Retrieval using Deep Convolutional Neural Network", "comments": "Submitted to Neurocomputing", "journal-ref": "Neurocomputing 2017", "doi": "10.1016/j.neucom.2017.05.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a widespread use of digital imaging data in hospitals, the size of\nmedical image repositories is increasing rapidly. This causes difficulty in\nmanaging and querying these large databases leading to the need of content\nbased medical image retrieval (CBMIR) systems. A major challenge in CBMIR\nsystems is the semantic gap that exists between the low level visual\ninformation captured by imaging devices and high level semantic information\nperceived by human. The efficacy of such systems is more crucial in terms of\nfeature representations that can characterize the high-level information\ncompletely. In this paper, we propose a framework of deep learning for CBMIR\nsystem by using deep Convolutional Neural Network (CNN) that is trained for\nclassification of medical images. An intermodal dataset that contains twenty\nfour classes and five modalities is used to train the network. The learned\nfeatures and the classification results are used to retrieve medical images.\nFor retrieval, best results are achieved when class based predictions are used.\nAn average classification accuracy of 99.77% and a mean average precision of\n0.69 is achieved for retrieval task. The proposed method is best suited to\nretrieve multimodal medical images for different body organs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 15:41:01 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Qayyum", "Adnan", ""], ["Anwar", "Syed Muhammad", ""], ["Awais", "Muhammad", ""], ["Majid", "Muhammad", ""]]}, {"id": "1703.08492", "submitter": "Nouman Ali", "authors": "Nouman Ali, Danish Ali Mazhar, Zeshan Iqbal, Rehan Ashraf, Jawad\n  Ahmed, Farrukh Zeeshan Khan", "title": "Content-Based Image Retrieval Based on Late Fusion of Binary and Local\n  Descriptors", "comments": null, "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), Volume 14, Issue 11, Publication date 2016/11", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce\nthe semantic gaps between low-level features and high-level semantic concepts.\nIn CBIR, the images are represented in the feature space and the performance of\nCBIR depends on the type of selected feature representation. Late fusion also\nknown as visual words integration is applied to enhance the performance of\nimage retrieval. The recent advances in image retrieval diverted the focus of\nresearch towards the use of binary descriptors as they are reported\ncomputationally efficient. In this paper, we aim to investigate the late fusion\nof Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT).\nThe late fusion of binary and local descriptor is selected because among binary\ndescriptors, FREAK has shown good results in classification-based problems\nwhile SIFT is robust to translation, scaling, rotation and small distortions.\nThe late fusion of FREAK and SIFT integrates the performance of both feature\ndescriptors for an effective image retrieval. Experimental results and\ncomparisons show that the proposed late fusion enhances the performances of\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:27:57 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ali", "Nouman", ""], ["Mazhar", "Danish Ali", ""], ["Iqbal", "Zeshan", ""], ["Ashraf", "Rehan", ""], ["Ahmed", "Jawad", ""], ["Khan", "Farrukh Zeeshan", ""]]}, {"id": "1703.08493", "submitter": "Wei Shen", "authors": "Wei Shen, Bin Wang, Yuan Jiang, Yan Wang, Alan Yuille", "title": "Multi-stage Multi-recursive-input Fully Convolutional Networks for\n  Neuronal Boundary Detection", "comments": "Accepted by ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of connectomics, neuroscientists seek to identify cortical\nconnectivity comprehensively. Neuronal boundary detection from the Electron\nMicroscopy (EM) images is often done to assist the automatic reconstruction of\nneuronal circuit. But the segmentation of EM images is a challenging problem,\nas it requires the detector to be able to detect both filament-like thin and\nblob-like thick membrane, while suppressing the ambiguous intracellular\nstructure. In this paper, we propose multi-stage multi-recursive-input fully\nconvolutional networks to address this problem. The multiple recursive inputs\nfor one stage, i.e., the multiple side outputs with different receptive field\nsizes learned from the lower stage, provide multi-scale contextual boundary\ninformation for the consecutive learning. This design is\nbiologically-plausible, as it likes a human visual system to compare different\npossible segmentation solutions to address the ambiguous boundary issue. Our\nmulti-stage networks are trained end-to-end. It achieves promising results on\ntwo public available EM segmentation datasets, the mouse piriform cortex\ndataset and the ISBI 2012 EM dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:28:57 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 16:10:54 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Shen", "Wei", ""], ["Wang", "Bin", ""], ["Jiang", "Yuan", ""], ["Wang", "Yan", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.08497", "submitter": "Stavros Petridis", "authors": "Zukang Liao, Stavros Petridis, Maja Pantic", "title": "Local Deep Neural Networks for Age and Gender Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local deep neural networks have been recently introduced for gender\nrecognition. Although, they achieve very good performance they are very\ncomputationally expensive to train. In this work, we introduce a simplified\nversion of local deep neural networks which significantly reduces the training\ntime. Instead of using hundreds of patches per image, as suggested by the\noriginal method, we propose to use 9 overlapping patches per image which cover\nthe entire face region. This results in a much reduced training time, since\njust 9 patches are extracted per image instead of hundreds, at the expense of a\nslightly reduced performance. We tested the proposed modified local deep neural\nnetworks approach on the LFW and Adience databases for the task of gender and\nage classification. For both tasks and both databases the performance is up to\n1% lower compared to the original version of the algorithm. We have also\ninvestigated which patches are more discriminative for age and gender\nclassification. It turns out that the mouth and eyes regions are useful for age\nclassification, whereas just the eye region is useful for gender\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:41:19 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Liao", "Zukang", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1703.08516", "submitter": "Martin Valli\\`eres", "authors": "Martin Valli\\`eres (1), Emily Kay-Rivest (2), L\\'eo Jean Perrin (3),\n  Xavier Liem (4), Christophe Furstoss (5), Hugo J. W. L. Aerts (6), Nader\n  Khaouam (5), Phuc Felix Nguyen-Tan (4), Chang-Shu Wang (3), Khalil Sultanem\n  (2), Jan Seuntjens (1), Issam El Naqa (7) ((1) Medical Physics Unit, McGill\n  University, Montr\\'eal, Canada, (2) Radiation Oncology Division, H\\^opital\n  g\\'en\\'eral juif, Montr\\'eal, Canada, (3) Department of Radiation Oncology,\n  Centre hospitalier universitaire de Sherbrooke, Montr\\'eal, Canada, (4)\n  Department of Radiation Oncology, Centre hospitalier de l'Universit\\'e de\n  Montr\\'eal, Montr\\'eal, Canada, (5) Department of Radiation Oncology,\n  H\\^opital Maisonneuve-Rosemont, Montr\\'eal, Canada, (6) Departments of\n  Radiation Oncology & Radiology, Dana-Farber Cancer Institute, Boston, USA,\n  (7) Department of Radiation Oncology, Physics Division, University of\n  Michigan, Ann Arbor, USA)", "title": "Radiomics strategies for risk assessment of tumour failure in\n  head-and-neck cancer", "comments": "(1) Paper: 33 pages, 4 figures, 1 table; (2) SUPP info: 41 pages, 7\n  figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative extraction of high-dimensional mineable data from medical images\nis a process known as radiomics. Radiomics is foreseen as an essential\nprognostic tool for cancer risk assessment and the quantification of\nintratumoural heterogeneity. In this work, 1615 radiomic features (quantifying\ntumour image intensity, shape, texture) extracted from pre-treatment FDG-PET\nand CT images of 300 patients from four different cohorts were analyzed for the\nrisk assessment of locoregional recurrences (LR) and distant metastases (DM) in\nhead-and-neck cancer. Prediction models combining radiomic and clinical\nvariables were constructed via random forests and imbalance-adjustment\nstrategies using two of the four cohorts. Independent validation of the\nprediction and prognostic performance of the models was carried out on the\nother two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88).\nFurthermore, the results obtained via Kaplan-Meier analysis demonstrated the\npotential of radiomics for assessing the risk of specific tumour outcomes using\nmultiple stratification groups. This could have important clinical impact,\nnotably by allowing for a better personalization of chemo-radiation treatments\nfor head-and-neck cancer patients from different risk groups.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 17:14:58 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Valli\u00e8res", "Martin", ""], ["Kay-Rivest", "Emily", ""], ["Perrin", "L\u00e9o Jean", ""], ["Liem", "Xavier", ""], ["Furstoss", "Christophe", ""], ["Aerts", "Hugo J. W. L.", ""], ["Khaouam", "Nader", ""], ["Nguyen-Tan", "Phuc Felix", ""], ["Wang", "Chang-Shu", ""], ["Sultanem", "Khalil", ""], ["Seuntjens", "Jan", ""], ["Naqa", "Issam El", ""]]}, {"id": "1703.08580", "submitter": "Vittal Premachandran", "authors": "Daniil Pakhomov and Vittal Premachandran and Max Allan and Mahdi\n  Azizian and Nassir Navab", "title": "Deep Residual Learning for Instrument Segmentation in Robotic Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection, tracking, and pose estimation of surgical instruments are crucial\ntasks for computer assistance during minimally invasive robotic surgery. In the\nmajority of cases, the first step is the automatic segmentation of surgical\ntools. Prior work has focused on binary segmentation, where the objective is to\nlabel every pixel in an image as tool or background. We improve upon previous\nwork in two major ways. First, we leverage recent techniques such as deep\nresidual learning and dilated convolutions to advance binary-segmentation\nperformance. Second, we extend the approach to multi-class segmentation, which\nlets us segment different parts of the tool, in addition to background. We\ndemonstrate the performance of this method on the MICCAI Endoscopic Vision\nChallenge Robotic Instruments dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 19:43:20 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Pakhomov", "Daniil", ""], ["Premachandran", "Vittal", ""], ["Allan", "Max", ""], ["Azizian", "Mahdi", ""], ["Navab", "Nassir", ""]]}, {"id": "1703.08603", "submitter": "Cihang Xie", "authors": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan\n  Yuille", "title": "Adversarial Examples for Semantic Segmentation and Object Detection", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been well demonstrated that adversarial examples, i.e., natural images\nwith visually imperceptible perturbations added, generally exist for deep\nnetworks to fail on image classification. In this paper, we extend adversarial\nexamples to semantic segmentation and object detection which are much more\ndifficult. Our observation is that both segmentation and detection are based on\nclassifying multiple targets on an image (e.g., the basic target is a pixel or\na receptive field in segmentation, and an object proposal in detection), which\ninspires us to optimize a loss function over a set of pixels/proposals for\ngenerating adversarial perturbations. Based on this idea, we propose a novel\nalgorithm named Dense Adversary Generation (DAG), which generates a large\nfamily of adversarial examples, and applies to a wide range of state-of-the-art\ndeep networks for segmentation and detection. We also find that the adversarial\nperturbations can be transferred across networks with different training data,\nbased on different architectures, and even for different recognition tasks. In\nparticular, the transferability across networks with the same architecture is\nmore significant than in other cases. Besides, summing up heterogeneous\nperturbations often leads to better transfer performance, which provides an\neffective method of black-box adversarial attack.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 21:26:16 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 22:26:57 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 17:27:17 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Xie", "Cihang", ""], ["Wang", "Jianyu", ""], ["Zhang", "Zhishuai", ""], ["Zhou", "Yuyin", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1703.08617", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Ngan le, Marios\n  Savvides", "title": "Temporal Non-Volume Preserving Approach to Facial Age-Progression and\n  Age-Invariant Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the long-term facial aging process is extremely challenging due to\nthe presence of large and non-linear variations during the face development\nstages. In order to efficiently address the problem, this work first decomposes\nthe aging process into multiple short-term stages. Then, a novel generative\nprobabilistic model, named Temporal Non-Volume Preserving (TNVP)\ntransformation, is presented to model the facial aging process at each stage.\nUnlike Generative Adversarial Networks (GANs), which requires an empirical\nbalance threshold, and Restricted Boltzmann Machines (RBM), an intractable\nmodel, our proposed TNVP approach guarantees a tractable density function,\nexact inference and evaluation for embedding the feature transformations\nbetween faces in consecutive stages. Our model shows its advantages not only in\ncapturing the non-linear age related variance in each stage but also producing\na smooth synthesis in age progression across faces. Our approach can model any\nface in the wild provided with only four basic landmark points. Moreover, the\nstructure can be transformed into a deep convolutional network while keeping\nthe advantages of probabilistic models with tractable log-likelihood density\nestimation. Our method is evaluated in both terms of synthesizing\nage-progressed faces and cross-age face verification and consistently shows the\nstate-of-the-art results in various face aging databases, i.e. FG-NET, MORPH,\nAginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A\nlarge-scale face verification on Megaface challenge 1 is also performed to\nfurther show the advantages of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 22:43:05 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Quach", "Kha Gia", ""], ["Luu", "Khoa", ""], ["le", "T. Hoang Ngan", ""], ["Savvides", "Marios", ""]]}, {"id": "1703.08628", "submitter": "Stavros Tsogkas", "authors": "Stavros Tsogkas, Sven Dickinson", "title": "AMAT: Medial Axis Transform for Natural Images", "comments": "10 pages (including references), 5 figures, accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Appearance-MAT (AMAT), a generalization of the medial axis\ntransform for natural images, that is framed as a weighted geometric set cover\nproblem. We make the following contributions: i) we extend previous medial\npoint detection methods for color images, by associating each medial point with\na local scale; ii) inspired by the invertibility property of the binary MAT, we\nalso associate each medial point with a local encoding that allows us to invert\nthe AMAT, reconstructing the input image; iii) we describe a clustering scheme\nthat takes advantage of the additional scale and appearance information to\ngroup individual points into medial branches, providing a shape decomposition\nof the underlying image regions. In our experiments, we show state-of-the-art\nperformance in medial point detection on Berkeley Medial AXes (BMAX500), a new\ndataset of medial axes based on the BSDS500 database, and good generalization\non the SK506 and WH-SYMMAX datasets. We also measure the quality of\nreconstructed images from BMAX500, obtained by inverting their computed AMAT.\nOur approach delivers significantly better reconstruction quality with respect\nto three baselines, using just 10% of the image pixels. Our code and\nannotations are available at https://github.com/tsogkas/amat .\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 23:50:52 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 23:21:18 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Tsogkas", "Stavros", ""], ["Dickinson", "Sven", ""]]}, {"id": "1703.08651", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Junshi Huang, Yi Yang, Shuicheng Yan", "title": "More is Less: A More Complicated Network with Less Inference Complexity", "comments": "This paper has been accepted by the IEEE CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel and general network structure towards\naccelerating the inference process of convolutional neural networks, which is\nmore complicated in network structure yet with less inference complexity. The\ncore idea is to equip each original convolutional layer with another low-cost\ncollaborative layer (LCCL), and the element-wise multiplication of the ReLU\noutputs of these two parallel layers produces the layer-wise output. The\ncombined layer is potentially more discriminative than the original\nconvolutional layer, and its inference is faster for two reasons: 1) the zero\ncells of the LCCL feature maps will remain zero after element-wise\nmultiplication, and thus it is safe to skip the calculation of the\ncorresponding high-cost convolution in the original convolutional layer, 2)\nLCCL is very fast if it is implemented as a 1*1 convolution or only a single\nfilter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100\nand ILSCRC-2012 benchmarks show that our proposed network structure can\naccelerate the inference process by 32\\% on average with negligible performance\ndrop.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 05:51:42 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 07:56:20 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Dong", "Xuanyi", ""], ["Huang", "Junshi", ""], ["Yang", "Yi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1703.08653", "submitter": "Anthony Rhodes", "authors": "Anthony D. Rhodes, Jordan Witte, Melanie Mitchell, Bruno Jedynak", "title": "Gaussian Processes with Context-Supported Priors for Active Object\n  Localization", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise an algorithm using a Bayesian optimization framework in conjunction\nwith contextual visual data for the efficient localization of objects in still\nimages. Recent research has demonstrated substantial progress in object\nlocalization and related tasks for computer vision. However, many current\nstate-of-the-art object localization procedures still suffer from inaccuracy\nand inefficiency, in addition to failing to provide a principled and\ninterpretable system amenable to high-level vision tasks. We address these\nissues with the current research.\n  Our method encompasses an active search procedure that uses contextual data\nto generate initial bounding-box proposals for a target object. We train a\nconvolutional neural network to approximate an offset distance from the target\nobject. Next, we use a Gaussian Process to model this offset response signal\nover the search space of the target. We then employ a Bayesian active search\nfor accurate localization of the target.\n  In experiments, we compare our approach to a state-of-theart bounding-box\nregression method for a challenging pedestrian localization task. Our method\nexhibits a substantial improvement over this baseline regression method.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 06:18:38 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 14:25:09 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 06:47:26 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Rhodes", "Anthony D.", ""], ["Witte", "Jordan", ""], ["Mitchell", "Melanie", ""], ["Jedynak", "Bruno", ""]]}, {"id": "1703.08697", "submitter": "Amir Ghaderi", "authors": "Amir Ghaderi, Srujana Gattupalli, Dylan Ebert, Ali Sharifara, Vassilis\n  Athitsos, Fillia Makedon", "title": "Improving the Accuracy of the CogniLearn System for Cognitive Behavior\n  Assessment", "comments": null, "journal-ref": null, "doi": "10.1145/3056540.3064942", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTKS is a game-like cognitive assessment method, designed for children\nbetween four and eight years of age. During the HTKS assessment, a child\nresponds to a sequence of requests, such as \"touch your head\" or \"touch your\ntoes\". The cognitive challenge stems from the fact that the children are\ninstructed to interpret these requests not literally, but by touching a\ndifferent body part than the one stated. In prior work, we have developed the\nCogniLearn system, that captures data from subjects performing the HTKS game,\nand analyzes the motion of the subjects. In this paper we propose some specific\nimprovements that make the motion analysis module more accurate. As a result of\nthese improvements, the accuracy in recognizing cases where subjects touch\ntheir toes has gone from 76.46% in our previous work to 97.19% in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 14:36:12 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ghaderi", "Amir", ""], ["Gattupalli", "Srujana", ""], ["Ebert", "Dylan", ""], ["Sharifara", "Ali", ""], ["Athitsos", "Vassilis", ""], ["Makedon", "Fillia", ""]]}, {"id": "1703.08710", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Genevieve Boucher and Craig A. Glastonbury and\n  Henry Z. Lo and Yoshua Bengio", "title": "Count-ception: Counting by Fully Convolutional Redundant Counting", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting objects in digital images is a process that should be replaced by\nmachines. This tedious task is time consuming and prone to errors due to\nfatigue of human annotators. The goal is to have a system that takes as input\nan image and returns a count of the objects inside and justification for the\nprediction in the form of object localization. We repose a problem, originally\nposed by Lempitsky and Zisserman, to instead predict a count map which contains\nredundant counts based on the receptive field of a smaller regression network.\nThe regression network predicts a count of the objects that exist inside this\nframe. By processing the image in a fully convolutional way each pixel is going\nto be accounted for some number of times, the number of windows which include\nit, which is the size of each window, (i.e., 32x32 = 1024). To recover the true\ncount we take the average over the redundant predictions. Our contribution is\nredundant counting instead of predicting a density map in order to average over\nerrors. We also propose a novel deep neural network architecture adapted from\nthe Inception family of networks called the Count-ception network. Together our\napproach results in a 20% relative improvement (2.9 to 2.3 MAE) over the state\nof the art method by Xie, Noble, and Zisserman in 2016.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 16:49:03 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 17:36:30 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Boucher", "Genevieve", ""], ["Glastonbury", "Craig A.", ""], ["Lo", "Henry Z.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1703.08738", "submitter": "Long Zhao", "authors": "Long Zhao, Fangda Han, Xi Peng, Xun Zhang, Mubbasir Kapadia, Vladimir\n  Pavlovic, Dimitris N. Metaxas", "title": "Cartoonish sketch-based face editing in videos using identity\n  deformation transfer", "comments": "In Computers & Graphics, 2019. (12 pages, 10 figures)", "journal-ref": null, "doi": "10.1016/j.cag.2019.01.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of using hand-drawn sketches to create exaggerated\ndeformations to faces in videos, such as enlarging the shape or modifying the\nposition of eyes or mouth. This task is formulated as a 3D face model\nreconstruction and deformation problem. We first recover the facial identity\nand expressions from the video by fitting a face morphable model for each\nframe. At the same time, user's editing intention is recognized from input\nsketches as a set of facial modifications. Then a novel identity deformation\nalgorithm is proposed to transfer these facial deformations from 2D space to\nthe 3D facial identity directly while preserving the facial expressions. After\nan optional stage for further refining the 3D face model, these changes are\npropagated to the whole video with the modified identity. Both the user study\nand experimental results demonstrate that our sketching framework can help\nusers effectively edit facial identities in videos, while high consistency and\nfidelity are ensured at the same time.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 20:33:45 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 10:36:08 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 04:05:45 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Zhao", "Long", ""], ["Han", "Fangda", ""], ["Peng", "Xi", ""], ["Zhang", "Xun", ""], ["Kapadia", "Mubbasir", ""], ["Pavlovic", "Vladimir", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1703.08764", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Guosheng Lin, Ruizhi Qiao, Chunhua Shen", "title": "Structured Learning of Tree Potentials in CRF for Image Segmentation", "comments": "10 pages. Appearing in IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to image segmentation, which exploits the\nadvantages of both conditional random fields (CRFs) and decision trees. In the\nliterature, the potential functions of CRFs are mostly defined as a linear\ncombination of some pre-defined parametric models, and then methods like\nstructured support vector machines (SSVMs) are applied to learn those linear\ncoefficients. We instead formulate the unary and pairwise potentials as\nnonparametric forests---ensembles of decision trees, and learn the ensemble\nparameters and the trees in a unified optimization problem within the\nlarge-margin framework. In this fashion, we easily achieve nonlinear learning\nof potential functions on both unary and pairwise terms in CRFs. Moreover, we\nlearn class-wise decision trees for each object that appears in the image. Due\nto the rich structure and flexibility of decision trees, our approach is\npowerful in modelling complex data likelihoods and label relationships. The\nresulting optimization problem is very challenging because it can have\nexponentially many variables and constraints. We show that this challenging\noptimization can be efficiently solved by combining a modified column\ngeneration and cutting-planes techniques. Experimental results on both binary\n(Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC\n2012) segmentation datasets demonstrate the power of the learned nonlinear\nnonparametric potentials.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 04:15:10 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Liu", "Fayao", ""], ["Lin", "Guosheng", ""], ["Qiao", "Ruizhi", ""], ["Shen", "Chunhua", ""]]}, {"id": "1703.08769", "submitter": "Hang Zhao", "authors": "Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, Antonio Torralba", "title": "Open Vocabulary Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing arbitrary objects in the wild has been a challenging problem due\nto the limitations of existing classification models and datasets. In this\npaper, we propose a new task that aims at parsing scenes with a large and open\nvocabulary, and several evaluation metrics are explored for this problem. Our\nproposed approach to this problem is a joint image pixel and word concept\nembeddings framework, where word concepts are connected by semantic relations.\nWe validate the open vocabulary prediction ability of our framework on ADE20K\ndataset which covers a wide variety of scenes and objects. We further explore\nthe trained joint embedding space to show its interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 05:44:56 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 18:28:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Zhao", "Hang", ""], ["Puig", "Xavier", ""], ["Zhou", "Bolei", ""], ["Fidler", "Sanja", ""], ["Torralba", "Antonio", ""]]}, {"id": "1703.08770", "submitter": "Wei Dai", "authors": "Wei Dai, Joseph Doyle, Xiaodan Liang, Hao Zhang, Nanqing Dong, Yuan\n  Li, Eric P. Xing", "title": "SCAN: Structure Correcting Adversarial Network for Organ Segmentation in\n  Chest X-rays", "comments": "10 pages, 7 figures, submitted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray (CXR) is one of the most commonly prescribed medical imaging\nprocedures, often with over 2-10x more scans than other imaging modalities such\nas MRI, CT scan, and PET scans. These voluminous CXR scans place significant\nworkloads on radiologists and medical practitioners. Organ segmentation is a\ncrucial step to obtain effective computer-aided detection on CXR. In this work,\nwe propose Structure Correcting Adversarial Network (SCAN) to segment lung\nfields and the heart in CXR images. SCAN incorporates a critic network to\nimpose on the convolutional segmentation network the structural regularities\nemerging from human physiology. During training, the critic network learns to\ndiscriminate between the ground truth organ annotations from the masks\nsynthesized by the segmentation network. Through this adversarial process the\ncritic network learns the higher order structures and guides the segmentation\nmodel to achieve realistic segmentation outcomes. Extensive experiments show\nthat our method produces highly accurate and natural segmentation. Using only\nvery limited training data available, our model reaches human-level performance\nwithout relying on any existing trained model or dataset. Our method also\ngeneralizes well to CXR images from a different patient population and disease\nprofiles, surpassing the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 05:48:38 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 16:18:36 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Dai", "Wei", ""], ["Doyle", "Joseph", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Hao", ""], ["Dong", "Nanqing", ""], ["Li", "Yuan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.08772", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Xudong Shi and Yu Sun and Li Cheng", "title": "Multivariate Regression with Gross Errors on Manifold-valued Data", "comments": "14 pages, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the topic of multivariate regression on manifold-valued output,\nthat is, for a multivariate observation, its output response lies on a\nmanifold. Moreover, we propose a new regression model to deal with the presence\nof grossly corrupted manifold-valued responses, a bottleneck issue commonly\nencountered in practical scenarios. Our model first takes a correction step on\nthe grossly corrupted responses via geodesic curves on the manifold, and then\nperforms multivariate linear regression on the corrected data. This results in\na nonconvex and nonsmooth optimization problem on manifolds. To this end, we\npropose a dedicated approach named PALMR, by utilizing and extending the\nproximal alternating linearized minimization techniques. Theoretically, we\ninvestigate its convergence property, where it is shown to converge to a\ncritical point under mild conditions. Empirically, we test our model on both\nsynthetic and real diffusion tensor imaging data, and show that our model\noutperforms other multivariate regression models when manifold-valued responses\ncontain gross errors, and is effective in identifying gross errors.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 05:53:39 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:27:35 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Shi", "Xudong", ""], ["Sun", "Yu", ""], ["Cheng", "Li", ""]]}, {"id": "1703.08774", "submitter": "Melody Guan", "authors": "Melody Y. Guan, Varun Gulshan, Andrew M. Dai, Geoffrey E. Hinton", "title": "Who Said What: Modeling Individual Labelers Improves Classification", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data are often labeled by many different experts with each expert only\nlabeling a small fraction of the data and each data point being labeled by\nseveral experts. This reduces the workload on individual experts and also gives\na better estimate of the unobserved ground truth. When experts disagree, the\nstandard approaches are to treat the majority opinion as the correct label or\nto model the correct label as a distribution. These approaches, however, do not\nmake any use of potentially valuable information about which expert produced\nwhich label. To make use of this extra information, we propose modeling the\nexperts individually and then learning averaging weights for combining them,\npossibly in sample-specific ways. This allows us to give more weight to more\nreliable experts and take advantage of the unique strengths of individual\nexperts at classifying certain types of data. Here we show that our approach\nleads to improvements in computer-aided diagnosis of diabetic retinopathy. We\nalso show that our method performs better than competing algorithms by Welinder\nand Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative\napproach for dealing with the myriad real-world settings that use expert\nopinions to define labels for training.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 06:34:45 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 21:46:22 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Guan", "Melody Y.", ""], ["Gulshan", "Varun", ""], ["Dai", "Andrew M.", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1703.08836", "submitter": "Silvano Galliani", "authors": "Wilfried Hartmann, Silvano Galliani, Michal Havlena, Luc Van Gool,\n  Konrad Schindler", "title": "Learned Multi-Patch Similarity", "comments": "10 pages, 7 figures, Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a depth map from multiple views of a scene is a fundamental task\nin computer vision. As soon as more than two viewpoints are available, one\nfaces the very basic question how to measure similarity across >2 image\npatches. Surprisingly, no direct solution exists, instead it is common to fall\nback to more or less robust averaging of two-view similarities. Encouraged by\nthe success of machine learning, and in particular convolutional neural\nnetworks, we propose to learn a matching function which directly maps multiple\nimage patches to a scalar similarity score. Experiments on several multi-view\ndatasets demonstrate that this approach has advantages over methods based on\npairwise patch similarity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:17:55 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 13:10:39 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Hartmann", "Wilfried", ""], ["Galliani", "Silvano", ""], ["Havlena", "Michal", ""], ["Van Gool", "Luc", ""], ["Schindler", "Konrad", ""]]}, {"id": "1703.08837", "submitter": "Wei-Shi Zheng", "authors": "Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, Jian-Huang Lai", "title": "Person Re-Identification by Camera Correlation Aware Feature\n  Augmentation", "comments": "To Appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2017", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2666805", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of person re-identification (re-id) is to match individual\nimages of the same person captured by different non-overlapping camera views\nagainst significant and unknown cross-view feature distortion. While a large\nnumber of distance metric/subspace learning models have been developed for\nre-id, the cross-view transformations they learned are view-generic and thus\npotentially less effective in quantifying the feature distortion inherent to\neach camera view. Learning view-specific feature transformations for re-id\n(i.e., view-specific re-id), an under-studied approach, becomes an alternative\nresort for this problem. In this work, we formulate a novel view-specific\nperson re-identification framework from the feature augmentation point of view,\ncalled Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically,\nCRAFT performs cross-view adaptation by automatically measuring camera\ncorrelation from cross-view visual data distribution and adaptively conducting\nfeature augmentation to transform the original features into a new adaptive\nspace. Through our augmentation framework, view-generic learning algorithms can\nbe readily generalized to learn and optimize view-specific sub-models whilst\nsimultaneously modelling view-generic discrimination information. Therefore,\nour framework not only inherits the strength of view-generic model learning but\nalso provides an effective way to take into account view specific\ncharacteristics. Our CRAFT framework can be extended to jointly learn\nview-specific feature transformations for person re-id across a large network\nwith more than two cameras, a largely under-investigated but realistic re-id\nsetting. Additionally, we present a domain-generic deep person appearance\nrepresentation which is designed particularly to be towards view invariant for\nfacilitating cross-view adaptation by CRAFT.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:18:48 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chen", "Ying-Cong", ""], ["Zhu", "Xiatian", ""], ["Zheng", "Wei-Shi", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "1703.08840", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Jiaming Song, Stefano Ermon", "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations", "comments": "14 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of imitation learning is to mimic expert behavior without access to\nan explicit reward signal. Expert demonstrations provided by humans, however,\noften show significant variability due to latent factors that are typically not\nexplicitly modeled. In this paper, we propose a new algorithm that can infer\nthe latent structure of expert demonstrations in an unsupervised way. Our\nmethod, built on top of Generative Adversarial Imitation Learning, can not only\nimitate complex behaviors, but also learn interpretable and meaningful\nrepresentations of complex behavioral data, including visual demonstrations. In\nthe driving domain, we show that a model learned from human demonstrations is\nable to both accurately reproduce a variety of behaviors and accurately\nanticipate human actions using raw visual inputs. Compared with various\nbaselines, our method can better capture the latent structure underlying expert\ndemonstrations, often recovering semantically meaningful factors of variation\nin the data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:20:36 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 21:51:21 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Li", "Yunzhu", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1703.08866", "submitter": "Lingni Ma", "authors": "Lingni Ma and J\\\"org St\\\"uckler and Christian Kerl and Daniel Cremers", "title": "Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D\n  Cameras", "comments": "the 2017 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual scene understanding is an important capability that enables robots to\npurposefully act in their environment. In this paper, we propose a novel\napproach to object-class segmentation from multiple RGB-D views using deep\nlearning. We train a deep neural network to predict object-class semantics that\nis consistent from several view points in a semi-supervised way. At test time,\nthe semantics predictions of our network can be fused more consistently in\nsemantic keyframe maps than predictions of a network trained on individual\nviews. We base our network architecture on a recent single-view deep learning\napproach to RGB and depth fusion for semantic object-class segmentation and\nenhance it with multi-scale loss minimization. We obtain the camera trajectory\nusing RGB-D SLAM and warp the predictions of RGB-D images into ground-truth\nannotated frames in order to enforce multi-view consistency during training. At\ntest time, predictions from multiple views are fused into keyframes. We propose\nand analyze several methods for enforcing multi-view consistency during\ntraining and testing. We evaluate the benefit of multi-view consistency\ntraining and demonstrate that pooling of deep features and fusion over multiple\nviews outperforms single-view baselines on the NYUDv2 benchmark for semantic\nsegmentation. Our end-to-end trained network achieves state-of-the-art\nperformance on the NYUDv2 dataset in single-view segmentation as well as\nmulti-view semantic fusion.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 20:28:02 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 19:01:11 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Ma", "Lingni", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Kerl", "Christian", ""], ["Cremers", "Daniel", ""]]}, {"id": "1703.08893", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Xi Li, Jichang Guo, Zhongfei Zhang, Haibin Ling,\n  Fei Wu", "title": "Transductive Zero-Shot Learning with a Self-training dictionary approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem in computer vision, zero-shot\nlearning (ZSL) aims at automatically recognizing the instances from unseen\nobject classes without training data. To address this problem, ZSL is usually\ncarried out in the following two aspects: 1) capturing the domain distribution\nconnections between seen classes data and unseen classes data; and 2) modeling\nthe semantic interactions between the image feature space and the label\nembedding space. Motivated by these observations, we propose a bidirectional\nmapping based semantic relationship modeling scheme that seeks for crossmodal\nknowledge transfer by simultaneously projecting the image features and label\nembeddings into a common latent space. Namely, we have a bidirectional\nconnection relationship that takes place from the image feature space to the\nlatent space as well as from the label embedding space to the latent space. To\ndeal with the domain shift problem, we further present a transductive learning\napproach that formulates the class prediction problem in an iterative refining\nprocess, where the object classification capacity is progressively reinforced\nthrough bootstrapping-based model updating over highly reliable instances.\nExperimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate\nthe effectiveness of the proposed approach against the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 01:36:38 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Yu", "Yunlong", ""], ["Ji", "Zhong", ""], ["Li", "Xi", ""], ["Guo", "Jichang", ""], ["Zhang", "Zhongfei", ""], ["Ling", "Haibin", ""], ["Wu", "Fei", ""]]}, {"id": "1703.08897", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Jichang Guo, and Yanwei Pang", "title": "Transductive Zero-Shot Learning with Adaptive Structural Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) endows the computer vision system with the\ninferential capability to recognize instances of a new category that has never\nseen before. Two fundamental challenges in it are visual-semantic embedding and\ndomain adaptation in cross-modality learning and unseen class prediction steps,\nrespectively. To address both challenges, this paper presents two corresponding\nmethods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective\nStrategy (SPASS), respectively. Specifically, ASTE formulates the\nvisualsemantic interactions in a latent structural SVM framework to adaptively\nadjust the slack variables to embody the different reliableness among training\ninstances. In this way, the reliable instances are imposed with small\npunishments, wheras the less reliable instances are imposed with more severe\npunishments. Thus, it ensures a more discriminative embedding. On the other\nhand, SPASS offers a framework to alleviate the domain shift problem in ZSL,\nwhich exploits the unseen data in an easy to hard fashion. Particularly, SPASS\nborrows the idea from selfpaced learning by iteratively selecting the unseen\ninstances from reliable to less reliable to gradually adapt the knowledge from\nthe seen domain to the unseen domain. Subsequently, by combining SPASS and\nASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively\nreinforce the classification capacity. Extensive experiments on three benchmark\ndatasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and\nTASTE. Furthermore, we also propose a fast training (FT) strategy to improve\nthe efficiency of most of existing ZSL methods. The FT strategy is surprisingly\nsimple and general enough, which can speed up the training time of most\nexisting methods by 4~300 times while holding the previous performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 01:44:41 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Yu", "Yunlong", ""], ["Ji", "Zhong", ""], ["Guo", "Jichang", ""], ["Pang", "Yanwei", ""]]}, {"id": "1703.08912", "submitter": "Jing Lou", "authors": "Jing Lou, Huan Wang, Longtao Chen, Fenglei Xu, Qingyuan Xia, Wei Zhu,\n  Mingwu Ren", "title": "Exploiting Color Name Space for Salient Object Detection", "comments": "http://www.loujing.com/cns-sod/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will investigate the contribution of color names for the\ntask of salient object detection. An input image is first converted to color\nname space, which is consisted of 11 probabilistic channels. By exploiting a\nsurroundedness cue, we obtain a saliency map through a linear combination of a\nset of sequential attention maps. To overcome the limitation of only using the\nsurroundedness cue, two global cues with respect to color names are invoked to\nguide the computation of a weighted saliency map. Finally, we integrate the\nabove two saliency maps into a unified framework to generate the final result.\nIn addition, an improved post-processing procedure is introduced to effectively\nsuppress image backgrounds while uniformly highlight salient objects.\nExperimental results show that the proposed model produces more accurate\nsaliency maps and performs well against twenty-one saliency models in terms of\nthree evaluation metrics on three public data sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 03:08:58 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 18:56:36 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lou", "Jing", ""], ["Wang", "Huan", ""], ["Chen", "Longtao", ""], ["Xu", "Fenglei", ""], ["Xia", "Qingyuan", ""], ["Zhu", "Wei", ""], ["Ren", "Mingwu", ""]]}, {"id": "1703.08917", "submitter": "Youn Jin Chung", "authors": "Younjin Chung, Joachim Gudmundsson, Masahiro Takatsuka", "title": "A Visual Measure of Changes to Weighted Self-Organizing Map Patterns", "comments": "8 pages, 3 figures, conference, llncs style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating output changes by input changes is the main task in causal\nanalysis. In previous work, input and output Self-Organizing Maps (SOMs) were\nassociated for causal analysis of multivariate and nonlinear data. Based on the\nassociation, a weight distribution of the output conditional on a given input\nwas obtained over the output map space. Such a weighted SOM pattern of the\noutput changes when the input changes. In order to analyze the change, it is\nimportant to measure the difference of the patterns. Many methods have been\nproposed for the dissimilarity measure of patterns. However, it remains a major\nchallenge when attempting to measure how the patterns change. In this paper, we\npropose a visualization approach that simplifies the comparison of the\ndifference in terms of the pattern property. Using this approach, the change\ncan be analyzed by integrating colors and star glyph shapes representing the\nproperty dissimilarity. Ecological data is used to demonstrate the usefulness\nof our approach and the experimental results show that our approach provides\nthe change information effectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 03:46:58 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chung", "Younjin", ""], ["Gudmundsson", "Joachim", ""], ["Takatsuka", "Masahiro", ""]]}, {"id": "1703.08919", "submitter": "Kun He", "authors": "Fatih Cakir, Kun He, Sarah Adel Bargal, Stan Sclaroff", "title": "MIHash: Online Hashing with Mutual Information", "comments": "International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based hashing methods are widely used for nearest neighbor\nretrieval, and recently, online hashing methods have demonstrated good\nperformance-complexity trade-offs by learning hash functions from streaming\ndata. In this paper, we first address a key challenge for online hashing: the\nbinary codes for indexed data must be recomputed to keep pace with updates to\nthe hash functions. We propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information, and use it\nsuccessfully as a criterion to eliminate unnecessary hash table updates. Next,\nwe also show how to optimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method, MIHash, that can be\nused in both online and batch settings. Experiments on image retrieval\nbenchmarks (including a 2.5M image dataset) confirm the effectiveness of our\nformulation, both in reducing hash table recomputations and in learning\nhigh-quality hash functions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 03:50:51 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 23:09:59 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cakir", "Fatih", ""], ["He", "Kun", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1703.08961", "submitter": "Eugene Belilovsky", "authors": "Edouard Oyallon (DI-ENS), Eugene Belilovsky (CVN, GALEN), Sergey\n  Zagoruyko (ENPC)", "title": "Scaling the Scattering Transform: Deep Hybrid Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the scattering network as a generic and fixed ini-tialization of the\nfirst layers of a supervised hybrid deep network. We show that early layers do\nnot necessarily need to be learned, providing the best results to-date with\npre-defined representations while being competitive with Deep CNNs. Using a\nshallow cascade of 1 x 1 convolutions, which encodes scattering coefficients\nthat correspond to spatial windows of very small sizes, permits to obtain\nAlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local\nencoding explicitly learns invariance w.r.t. rotations. Combining scattering\nnetworks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on\nimagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing\nonly 10 layers. We also find that hybrid architectures can yield excellent\nperformance in the small sample regime, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. We\ndemonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 07:49:43 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 06:13:22 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Oyallon", "Edouard", "", "DI-ENS"], ["Belilovsky", "Eugene", "", "CVN, GALEN"], ["Zagoruyko", "Sergey", "", "ENPC"]]}, {"id": "1703.08966", "submitter": "Edgar Simo-Serra", "authors": "Edgar Simo-Serra, Satoshi Iizuka, Hiroshi Ishikawa", "title": "Mastering Sketching: Adversarial Augmentation for Structured Prediction", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integral framework for training sketch simplification networks\nthat convert challenging rough sketches into clean line drawings. Our approach\naugments a simplification network with a discriminator network, training both\nnetworks jointly so that the discriminator network discerns whether a line\ndrawing is a real training data or the output of the simplification network,\nwhich in turn tries to fool it. This approach has two major advantages. First,\nbecause the discriminator network learns the structure in line drawings, it\nencourages the output sketches of the simplification network to be more similar\nin appearance to the training sketches. Second, we can also train the\nsimplification network with additional unsupervised data, using the\ndiscriminator network as a substitute teacher. Thus, by adding only rough\nsketches without simplified line drawings, or only line drawings without the\noriginal rough sketches, we can improve the quality of the sketch\nsimplification. We show how our framework can be used to train models that\nsignificantly outperform the state of the art in the sketch simplification\ntask, despite using the same architecture for inference. We additionally\npresent an approach to optimize for a single image, which improves accuracy at\nthe cost of additional computation time. Finally, we show that, using the same\nframework, it is possible to train the network to perform the inverse problem,\ni.e., convert simple line sketches into pencil drawings, which is not possible\nusing the standard mean squared error loss. We validate our framework with two\nuser tests, where our approach is preferred to the state of the art in sketch\nsimplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to\n5.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 08:23:47 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Simo-Serra", "Edgar", ""], ["Iizuka", "Satoshi", ""], ["Ishikawa", "Hiroshi", ""]]}, {"id": "1703.08987", "submitter": "Luca Caltagirone", "authors": "Luca Caltagirone, Mauro Bellone, Lennart Svensson, Mattias Wahde", "title": "LIDAR-based Driving Path Generation Using Fully Convolutional Neural\n  Networks", "comments": "Changed title, formerly \"Simultaneous Perception and Path Generation\n  Using Fully Convolutional Neural Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel learning-based approach has been developed to generate\ndriving paths by integrating LIDAR point clouds, GPS-IMU information, and\nGoogle driving directions. The system is based on a fully convolutional neural\nnetwork that jointly learns to carry out perception and path generation from\nreal-world driving sequences and that is trained using automatically generated\ntraining examples. Several combinations of input data were tested in order to\nassess the performance gain provided by specific information modalities. The\nfully convolutional neural network trained using all the available sensors\ntogether with driving directions achieved the best MaxF score of 88.13% when\nconsidering a region of interest of 60x60 meters. By considering a smaller\nregion of interest, the agreement between predicted paths and ground-truth\nincreased to 92.60%. The positive results obtained in this work indicate that\nthe proposed system may help fill the gap between low-level scene parsing and\nbehavior-reflex approaches by generating outputs that are close to vehicle\ncontrol and at the same time human-interpretable.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 09:51:55 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 13:00:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Caltagirone", "Luca", ""], ["Bellone", "Mauro", ""], ["Svensson", "Lennart", ""], ["Wahde", "Mattias", ""]]}, {"id": "1703.09026", "submitter": "Davide Moltisanti", "authors": "Davide Moltisanti, Michael Wray, Walterio Mayol-Cuevas, Dima Damen", "title": "Trespassing the Boundaries: Labeling Temporal Bounds for Object\n  Interactions in Egocentric Video", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual annotations of temporal bounds for object interactions (i.e. start and\nend times) are typical training input to recognition, localization and\ndetection algorithms. For three publicly available egocentric datasets, we\nuncover inconsistencies in ground truth temporal bounds within and across\nannotators and datasets. We systematically assess the robustness of\nstate-of-the-art approaches to changes in labeled temporal bounds, for object\ninteraction recognition. As boundaries are trespassed, a drop of up to 10% is\nobserved for both Improved Dense Trajectories and Two-Stream Convolutional\nNeural Network.\n  We demonstrate that such disagreement stems from a limited understanding of\nthe distinct phases of an action, and propose annotating based on the Rubicon\nBoundaries, inspired by a similarly named cognitive model, for consistent\ntemporal bounds of object interactions. Evaluated on a public dataset, we\nreport a 4% increase in overall accuracy, and an increase in accuracy for 55%\nof classes when Rubicon Boundaries are used for temporal annotations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 12:14:07 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:13:23 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Moltisanti", "Davide", ""], ["Wray", "Michael", ""], ["Mayol-Cuevas", "Walterio", ""], ["Damen", "Dima", ""]]}, {"id": "1703.09039", "submitter": "Vivienne Sze", "authors": "Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel Emer", "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey", "comments": "Based on tutorial on DNN Hardware at eyeriss.mit.edu/tutorial.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are currently widely used for many artificial\nintelligence (AI) applications including computer vision, speech recognition,\nand robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it\ncomes at the cost of high computational complexity. Accordingly, techniques\nthat enable efficient processing of DNNs to improve energy efficiency and\nthroughput without sacrificing application accuracy or increasing hardware cost\nare critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the\nrecent advances towards the goal of enabling efficient processing of DNNs.\nSpecifically, it will provide an overview of DNNs, discuss various hardware\nplatforms and architectures that support DNNs, and highlight key trends in\nreducing the computation cost of DNNs either solely via hardware design changes\nor via joint hardware design and DNN algorithm changes. It will also summarize\nvarious development resources that enable researchers and practitioners to\nquickly get started in this field, and highlight important benchmarking metrics\nand design considerations that should be used for evaluating the rapidly\ngrowing number of DNN hardware designs, optionally including algorithmic\nco-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article:\nunderstand the key design considerations for DNNs; be able to evaluate\ndifferent DNN hardware implementations with benchmarks and comparison metrics;\nunderstand the trade-offs between various hardware architectures and platforms;\nbe able to evaluate the utility of various DNN design techniques for efficient\nprocessing; and understand recent implementation trends and opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 12:42:13 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 14:47:12 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Sze", "Vivienne", ""], ["Chen", "Yu-Hsin", ""], ["Yang", "Tien-Ju", ""], ["Emer", "Joel", ""]]}, {"id": "1703.09076", "submitter": "Yunho Jeon", "authors": "Yunho Jeon, Junmo Kim", "title": "Active Convolution: Learning the Shape of Convolution for Image\n  Classification", "comments": "Accepted to appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has achieved great success in many computer\nvision applications. Convolutional neural networks (CNNs) have lately emerged\nas a major approach to image classification. Most research on CNNs thus far has\nfocused on developing architectures such as the Inception and residual\nnetworks. The convolution layer is the core of the CNN, but few studies have\naddressed the convolution unit itself. In this paper, we introduce a\nconvolution unit called the active convolution unit (ACU). A new convolution\nhas no fixed shape, because of which we can define any form of convolution. Its\nshape can be learned through backpropagation during training. Our proposed unit\nhas a few advantages. First, the ACU is a generalization of convolution; it can\ndefine not only all conventional convolutions, but also convolutions with\nfractional pixel coordinates. We can freely change the shape of the\nconvolution, which provides greater freedom to form CNN structures. Second, the\nshape of the convolution is learned while training and there is no need to tune\nit by hand. Third, the ACU can learn better than a conventional unit, where we\nobtained the improvement simply by changing the conventional convolution to an\nACU. We tested our proposed method on plain and residual networks, and the\nresults showed significant improvement using our method on various datasets and\narchitectures in comparison with the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:44:26 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Jeon", "Yunho", ""], ["Kim", "Junmo", ""]]}, {"id": "1703.09137", "submitter": "Marc Tanti", "authors": "Marc Tanti (1), Albert Gatt (1), Kenneth P. Camilleri (1) ((1)\n  University of Malta)", "title": "Where to put the Image in an Image Caption Generator", "comments": "Accepted in JNLE Special Issue: Language for Images (24.3) (expanded\n  with content that was removed from journal paper in order to reduce number of\n  pages), 28 pages, 5 figures, 6 tables", "journal-ref": null, "doi": "10.1017/S1351324918000098", "report-no": null, "categories": "cs.NE cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN -- conditioning the language model by\n`injecting' image features -- or in a layer following the RNN -- conditioning\nthe language model by `merging' image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN's\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:13:49 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 08:56:53 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1703.09145", "submitter": "Yuguang Liu", "authors": "Yuguang Liu, Martin D. Levine", "title": "Multi-Path Region-Based Convolutional Neural Network for Accurate\n  Detection of Unconstrained \"Hard Faces\"", "comments": "11 pages, 7 figures, to be presented at CRV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale variations still pose a challenge in unconstrained face\ndetection. To the best of our knowledge, no current face detection algorithm\ncan detect a face as large as 800 x 800 pixels while simultaneously detecting\nanother one as small as 8 x 8 pixels within a single image with equally high\naccuracy. We propose a two-stage cascaded face detection framework, Multi-Path\nRegion-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a\ndeep neural network with a classic learning strategy, to tackle this challenge.\nThe first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes\nfaces at three different scales. It simultaneously utilizes three parallel\noutputs of the convolutional feature maps to predict multi-scale candidate face\nregions. The \"atrous\" convolution trick (convolution with up-sampled filters)\nand a newly proposed sampling layer for \"hard\" examples are embedded in MP-RPN\nto further boost its performance. The second stage is a Boosted Forests\nclassifier, which utilizes deep facial features pooled from inside the\ncandidate face regions as well as deep contextual features pooled from a larger\nregion surrounding the candidate face regions. This step is included to further\nremove hard negative samples. Experiments show that this approach achieves\nstate-of-the-art face detection performance on the WIDER FACE dataset \"hard\"\npartition, outperforming the former best result by 9.6% for the Average\nPrecision.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:31:00 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Liu", "Yuguang", ""], ["Levine", "Martin D.", ""]]}, {"id": "1703.09157", "submitter": "Yimian Dai", "authors": "Yimian Dai and Yiquan Wu", "title": "Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local\n  Priors for Single-Frame Small Target Detection", "comments": "Submitted to IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing, 16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art methods have been proposed for infrared small target\ndetection. They work well on the images with homogeneous backgrounds and\nhigh-contrast targets. However, when facing highly heterogeneous backgrounds,\nthey would not perform very well, mainly due to: 1) the existence of strong\nedges and other interfering components, 2) not utilizing the priors fully.\nInspired by this, we propose a novel method to exploit both local and non-local\npriors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT)\nmodel to represent the image and preserve its spatial correlations. Exploiting\nthe target sparse prior and background non-local self-correlation prior, the\ntarget-background separation is modeled as a robust low-rank tensor recovery\nproblem. Moreover, with the help of the structure tensor and reweighted idea,\nwe design an entry-wise local-structure-adaptive and sparsity enhancing weight\nto replace the globally constant weighting parameter. The decomposition could\nbe achieved via the element-wise reweighted higher-order robust principal\ncomponent analysis with an additional convergence condition according to the\npractical situation of target detection. Extensive experiments demonstrate that\nour model outperforms the other state-of-the-arts, in particular for the images\nwith very dim targets and heavy clutters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:57:27 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Dai", "Yimian", ""], ["Wu", "Yiquan", ""]]}, {"id": "1703.09161", "submitter": "Lukas F. Lang", "authors": "Lukas F. Lang", "title": "A Dynamic Programming Solution to Bounded Dejittering Problems", "comments": "The final publication is available at link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic programming solution to image dejittering problems with\nbounded displacements and obtain efficient algorithms for the removal of line\njitter, line pixel jitter, and pixel jitter.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:09:20 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Lang", "Lukas F.", ""]]}, {"id": "1703.09167", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Lee Friedman, Oleg Komogortsev", "title": "A Study on the Extraction and Analysis of a Large Set of Eye Movement\n  Features during Reading", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a study on the extraction and analysis of a set of 101\ncategories of eye movement features from three types of eye movement events:\nfixations, saccades, and post-saccadic oscillations. The eye movements were\nrecorded during a reading task. For the categories of features with multiple\ninstances in a recording we extract corresponding feature subtypes by\ncalculating descriptive statistics on the distributions of these instances. A\nunified framework of detailed descriptions and mathematical formulas are\nprovided for the extraction of the feature set. The analysis of feature values\nis performed using a large database of eye movement recordings from a normative\npopulation of 298 subjects. We demonstrate the central tendency and overall\nvariability of feature values over the experimental population, and more\nimportantly, we quantify the test-retest reliability (repeatability) of each\nseparate feature. The described methods and analysis can provide valuable tools\nin fields exploring the eye movements, such as in behavioral studies, attention\nand cognition research, medical research, biometric recognition, and\nhuman-computer interaction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:21:26 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Rigas", "Ioannis", ""], ["Friedman", "Lee", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "1703.09179", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Gy\\\"orgy Fazekas, Mark Sandler and Kyunghyun Cho", "title": "Transfer learning for music classification and regression tasks", "comments": "18th International Society of Music Information Retrieval (ISMIR)\n  Conference, Suzhou, China, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a transfer learning approach for music\nclassification and regression tasks. We propose to use a pre-trained convnet\nfeature, a concatenated feature vector using the activations of feature maps of\nmultiple layers in a trained convolutional network. We show how this convnet\nfeature can serve as general-purpose music representation. In the experiments,\na convnet is trained for music tagging and then transferred to other\nmusic-related classification and regression tasks. The convnet feature\noutperforms the baseline MFCC feature in all the considered tasks and several\nprevious approaches that are aggregating MFCCs as well as low- and high-level\nmusic features.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:48:03 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 15:58:38 GMT"}, {"version": "v3", "created": "Sat, 15 Jul 2017 13:36:05 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 16:20:26 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Sandler", "Mark", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1703.09199", "submitter": "Christopher Bridge", "authors": "Christopher P. Bridge", "title": "Introduction To The Monogenic Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The monogenic signal is an image analysis methodology that was introduced by\nFelsberg and Sommer in 2001 and has been employed for a variety of purposes in\nimage processing and computer vision research. In particular, it has been found\nto be useful in the analysis of ultrasound imagery in several research\nscenarios mostly in work done within the BioMedIA lab at Oxford. However, the\nliterature on the monogenic signal can be difficult to penetrate due to the\nlack of a single resource to explain the various principles from basics. The\npurpose of this document is therefore to introduce the principles, purpose,\napplications, and limitations of the methodology. It assumes some background\nknowledge from the fields of image and signal processing, in particular a good\nknowledge of Fourier transforms as applied to signals and images. We will not\nattempt to provide a thorough math- ematical description or derivation of the\nmonogenic signal, but rather focus on developing an intuition for understanding\nand using the methodology and refer the reader elsewhere for a more\nmathematical treatment.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:36:33 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Bridge", "Christopher P.", ""]]}, {"id": "1703.09200", "submitter": "Yuanhan Mo", "authors": "Yuanhan Mo, Fangde Liu, Douglas McIlwraith, Guang Yang, Jingqing\n  Zhang, Taigang He, Yike Guo", "title": "The Deep Poincar\\'e Map: A Novel Approach for Left Ventricle\n  Segmentation", "comments": "MICCAI 2018 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise segmentation of the left ventricle (LV) within cardiac MRI images is\na prerequisite for the quantitative measurement of heart function. However,\nthis task is challenging due to the limited availability of labeled data and\nmotion artifacts from cardiac imaging. In this work, we present an iterative\nsegmentation algorithm for LV delineation. By coupling deep learning with a\nnovel dynamic-based labeling scheme, we present a new methodology where a\npolicy model is learned to guide an agent to travel over the the image, tracing\nout a boundary of the ROI -- using the magnitude difference of the Poincar\\'e\nmap as a stopping criterion. Our method is evaluated on two datasets, namely\nthe Sunnybrook Cardiac Dataset (SCD) and data from the STACOM 2011 LV\nsegmentation challenge. Our method outperforms the previous research over many\nmetrics. In order to demonstrate the transferability of our method we present\nencouraging results over the STACOM 2011 data, when using a model trained on\nthe SCD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:37:33 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 11:10:09 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Mo", "Yuanhan", ""], ["Liu", "Fangde", ""], ["McIlwraith", "Douglas", ""], ["Yang", "Guang", ""], ["Zhang", "Jingqing", ""], ["He", "Taigang", ""], ["Guo", "Yike", ""]]}, {"id": "1703.09210", "submitter": "Dongdong Chen", "authors": "Dongdong Chen and Lu Yuan and Jing Liao and Nenghai Yu and Gang Hua", "title": "StyleBank: An Explicit Representation for Neural Image Style Transfer", "comments": "Accepted by CVPR 2017, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose StyleBank, which is composed of multiple convolution filter banks\nand each filter bank explicitly represents one style, for neural image style\ntransfer. To transfer an image to a specific style, the corresponding filter\nbank is operated on top of the intermediate feature embedding produced by a\nsingle auto-encoder. The StyleBank and the auto-encoder are jointly learnt,\nwhere the learning is conducted in such a way that the auto-encoder does not\nencode any style information thanks to the flexibility introduced by the\nexplicit filter bank representation. It also enables us to conduct incremental\nlearning to add a new image style by learning a new filter bank while holding\nthe auto-encoder fixed. The explicit style representation along with the\nflexible network design enables us to fuse styles at not only the image level,\nbut also the region level. Our method is the first style transfer network that\nlinks back to traditional texton mapping methods, and hence provides new\nunderstanding on neural style transfer. Our method is easy to train, runs in\nreal-time, and produces results that qualitatively better or at least\ncomparable to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:52:18 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 09:10:10 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Liao", "Jing", ""], ["Yu", "Nenghai", ""], ["Hua", "Gang", ""]]}, {"id": "1703.09211", "submitter": "Dongdong Chen", "authors": "Dongdong Chen and Jing Liao and Lu Yuan and Nenghai Yu and Gang Hua", "title": "Coherent Online Video Style Transfer", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a feed-forward network for fast neural style transfer of images is\nproven to be successful. However, the naive extension to process video frame by\nframe is prone to producing flickering results. We propose the first end-to-end\nnetwork for online video style transfer, which generates temporally coherent\nstylized video sequences in near real-time. Two key ideas include an efficient\nnetwork by incorporating short-term coherence, and propagating short-term\ncoherence to long-term, which ensures the consistency over larger period of\ntime. Our network can incorporate different image stylization networks. We show\nthat the proposed method clearly outperforms the per-frame baseline both\nqualitatively and quantitatively. Moreover, it can achieve visually comparable\ncoherence to optimization-based video style transfer, but is three orders of\nmagnitudes faster in runtime.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:52:55 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 09:04:15 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Yuan", "Lu", ""], ["Yu", "Nenghai", ""], ["Hua", "Gang", ""]]}, {"id": "1703.09245", "submitter": "Lei Xiao", "authors": "Lei Xiao, Felix Heide, Wolfgang Heidrich, Bernhard Sch\\\"olkopf,\n  Michael Hirsch", "title": "Discriminative Transfer Learning for General Image Restoration", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2831925", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several discriminative learning approaches have been proposed for\neffective image restoration, achieving convincing trade-off between image\nquality and computational efficiency. However, these methods require separate\ntraining for each restoration task (e.g., denoising, deblurring, demosaicing)\nand problem condition (e.g., noise level of input images). This makes it\ntime-consuming and difficult to encompass all tasks and conditions during\ntraining. In this paper, we propose a discriminative transfer learning method\nthat incorporates formal proximal optimization and discriminative learning for\ngeneral image restoration. The method requires a single-pass training and\nallows for reuse across various problems and conditions while achieving an\nefficiency comparable to previous discriminative approaches. Furthermore, after\nbeing trained, our model can be easily transferred to new likelihood terms to\nsolve untrained tasks, or be combined with existing priors to further improve\nimage restoration quality.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 18:09:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Xiao", "Lei", ""], ["Heide", "Felix", ""], ["Heidrich", "Wolfgang", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1703.09296", "submitter": "Jiri Hladuvka", "authors": "Ji\\v{r}\\'i Hlad\\r{u}vka, Bui Thi Mai Phuong, Richard Ljuhar, Davul\n  Ljuhar, Ana M Rodrigues, Jaime C Branco, Helena Canh\\~ao", "title": "Femoral ROIs and Entropy for Texture-based Detection of Osteoarthritis\n  from High-Resolution Knee Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between knee osteoarthritis progression and changes in\ntibial bone structure has long been recognized and various texture descriptors\nhave been proposed to detect early osteoarthritis (OA) from radiographs. This\nwork aims to investigate (1) femoral textures as an OA indicator and (2) the\npotential of entropy as a computationally efficient alternative to established\ntexture descriptors.\n  We design a robust semi-automatically placed layout for regions of interest\n(ROI), compute the Hurst coefficient and the entropy in each ROI, and employ\nstatistical and machine learning methods to evaluate feature combinations.\n  Based on 153 high-resolution radiographs, our results identify medial femur\nas an effective univariate descriptor, with significance comparable to medial\ntibia. Entropy is shown to contribute to classification performance. A linear\nfive-feature classifier combining femur, entropic and standard texture\ndescriptors, achieves AUC of 0.85, outperforming the state-of-the-art by\nroughly 0.1.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 20:20:50 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Hlad\u016fvka", "Ji\u0159\u00ed", ""], ["Phuong", "Bui Thi Mai", ""], ["Ljuhar", "Richard", ""], ["Ljuhar", "Davul", ""], ["Rodrigues", "Ana M", ""], ["Branco", "Jaime C", ""], ["Canh\u00e3o", "Helena", ""]]}, {"id": "1703.09342", "submitter": "Xiao-Yang Liu", "authors": "Fei Jiang, Xiao-Yang Liu, Hongtao Lu, Ruimin Shen", "title": "Graph Regularized Tensor Sparse Coding for Image Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding (SC) is an unsupervised learning scheme that has received an\nincreasing amount of interests in recent years. However, conventional SC\nvectorizes the input images, which destructs the intrinsic spatial structures\nof the images. In this paper, we propose a novel graph regularized tensor\nsparse coding (GTSC) for image representation. GTSC preserves the local\nproximity of elementary structures in the image by adopting the newly proposed\ntubal-tensor representation. Simultaneously, it considers the intrinsic\ngeometric properties by imposing graph regularization that has been\nsuccessfully applied to uncover the geometric distribution for the image data.\nMoreover, the returned sparse representations by GTSC have better physical\nexplanations as the key operation (i.e., circular convolution) in the\ntubal-tensor model preserves the shifting invariance property. Experimental\nresults on image clustering demonstrate the effectiveness of the proposed\nscheme.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 23:34:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Jiang", "Fei", ""], ["Liu", "Xiao-Yang", ""], ["Lu", "Hongtao", ""], ["Shen", "Ruimin", ""]]}, {"id": "1703.09370", "submitter": "Thomas Ploetz", "authors": "Yu Guan and Thomas Ploetz", "title": "Ensembles of Deep LSTM Learners for Activity Recognition using Wearables", "comments": "accepted for publication in ACM IMWUT (Ubicomp) 2017", "journal-ref": null, "doi": "10.1145/3090076", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning (DL) methods have been introduced very successfully\ninto human activity recognition (HAR) scenarios in ubiquitous and wearable\ncomputing. Especially the prospect of overcoming the need for manual feature\ndesign combined with superior classification capabilities render deep neural\nnetworks very attractive for real-life HAR application. Even though DL-based\napproaches now outperform the state-of-the-art in a number of recognitions\ntasks of the field, yet substantial challenges remain. Most prominently, issues\nwith real-life datasets, typically including imbalanced datasets and\nproblematic data quality, still limit the effectiveness of activity recognition\nusing wearables. In this paper we tackle such challenges through Ensembles of\ndeep Long Short Term Memory (LSTM) networks. We have developed modified\ntraining procedures for LSTM networks and combine sets of diverse LSTM learners\ninto classifier collectives. We demonstrate, both formally and empirically,\nthat Ensembles of deep LSTM learners outperform the individual LSTM networks.\nThrough an extensive experimental evaluation on three standard benchmarks\n(Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition\ncapabilities of our approach and its potential for real-life applications of\nhuman activity recognition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 02:00:47 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Guan", "Yu", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1703.09379", "submitter": "Chunhua Shen", "authors": "Wei Liu, Xiaogang Chen, Chunhua Shen, Jingyi Yu, Qiang Wu, Jie Yang", "title": "Robust Guided Image Filtering", "comments": "This paper is an extension of our previous work at arXiv:1512.08103\n  and arXiv:1506.05187", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of using one image to guide the filtering process of another one\nis called Guided Image Filtering (GIF). The main challenge of GIF is the\nstructure inconsistency between the guidance image and the target image.\nBesides, noise in the target image is also a challenging issue especially when\nit is heavy. In this paper, we propose a general framework for Robust Guided\nImage Filtering (RGIF), which contains a data term and a smoothness term, to\nsolve the two issues mentioned above. The data term makes our model\nsimultaneously denoise the target image and perform GIF which is robust against\nthe heavy noise. The smoothness term is able to make use of the property of\nboth the guidance image and the target image which is robust against the\nstructure inconsistency. While the resulting model is highly non-convex, it can\nbe solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in\nan efficient manner. For challenging applications such as guided depth map\nupsampling, we further develop a data-driven parameter optimization scheme to\nproperly determine the parameter in our model. This optimization scheme can\nhelp to preserve small structures and sharp depth edges even for a large\nupsampling factor (8x for example). Moreover, the specially designed structure\nof the data term and the smoothness term makes our model perform well in\nedge-preserving smoothing for single-image tasks (i.e., the guidance image is\nthe target image itself). This paper is an extension of our previous work [1],\n[2].\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 02:59:15 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Xiaogang", ""], ["Shen", "Chunhua", ""], ["Yu", "Jingyi", ""], ["Wu", "Qiang", ""], ["Yang", "Jie", ""]]}, {"id": "1703.09387", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja, Ian Fischer", "title": "Adversarial Transformation Networks: Learning to Generate Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple different approaches of generating adversarial examples have been\nproposed to attack deep neural networks. These approaches involve either\ndirectly computing gradients with respect to the image pixels, or directly\nsolving an optimization on the image pixels. In this work, we present a\nfundamentally new method for generating adversarial examples that is fast to\nexecute and provides exceptional diversity of output. We efficiently train\nfeed-forward neural networks in a self-supervised manner to generate\nadversarial examples against a target network or set of networks. We call such\na network an Adversarial Transformation Network (ATN). ATNs are trained to\ngenerate adversarial examples that minimally modify the classifier's outputs\ngiven the original input, while constraining the new classification to match an\nadversarial target class. We present methods to train ATNs and analyze their\neffectiveness targeting a variety of MNIST classifiers as well as the latest\nstate-of-the-art ImageNet classifier Inception ResNet v2.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 03:24:33 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Baluja", "Shumeet", ""], ["Fischer", "Ian", ""]]}, {"id": "1703.09393", "submitter": "Shohei Kumagai", "authors": "Shohei Kumagai, Kazuhiro Hotta, Takio Kurita", "title": "Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to\n  Specific Appearance for Crowd Counting", "comments": "8pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a crowd counting method. Crowd counting is difficult\nbecause of large appearance changes of a target which caused by density and\nscale changes. Conventional crowd counting methods generally utilize one\npredictor (e,g., regression and multi-class classifier). However, such only one\npredictor can not count targets with large appearance changes well. In this\npaper, we propose to predict the number of targets using multiple CNNs\nspecialized to a specific appearance, and those CNNs are adaptively selected\naccording to the appearance of a test image. By integrating the selected CNNs,\nthe proposed method has the robustness to large appearance changes. In\nexperiments, we confirm that the proposed method can count crowd with lower\ncounting error than a CNN and integration of CNNs with fixed weights. Moreover,\nwe confirm that each predictor automatically specialized to a specific\nappearance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 03:41:44 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Kumagai", "Shohei", ""], ["Hotta", "Kazuhiro", ""], ["Kurita", "Takio", ""]]}, {"id": "1703.09436", "submitter": "Ricardo Marcacini", "authors": "Rodrigo M. Ferreira and Ricardo M. Marcacini", "title": "Evaluation of Classifiers for Image Segmentation: Applications for\n  Eucalypt Forest Inventory", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of counting eucalyptus trees from aerial images collected by\nunmanned aerial vehicles (UAVs) has been frequently explored by techniques of\nestimation of the basal area, i.e, by determining the expected number of trees\nbased on sampling techniques. An alternative is the use of machine learning to\nidentify patterns that represent a tree unit, and then search for the\noccurrence of these patterns throughout the image. This strategy depends on a\nsupervised image segmentation step to define predefined interest regions. Thus,\nit is possible to automate the counting of eucalyptus trees in these images,\nthereby increasing the efficiency of the eucalyptus forest inventory\nmanagement. In this paper, we evaluated 20 different classifiers for the image\nsegmentation task. A real sample was used to analyze the counting trees task\nconsidering a practical environment. The results show that it possible to\nautomate this task with 0.7% counting error, in particular, by using strategies\nbased on a combination of classifiers. Moreover, we present some performance\nconsiderations about each classifier that can be useful as a basis for\ndecision-making in future tasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 07:43:42 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Ferreira", "Rodrigo M.", ""], ["Marcacini", "Ricardo M.", ""]]}, {"id": "1703.09438", "submitter": "Maxim Tatarchenko", "authors": "Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox", "title": "Octree Generating Networks: Efficient Convolutional Architectures for\n  High-resolution 3D Outputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep convolutional decoder architecture that can generate\nvolumetric 3D outputs in a compute- and memory-efficient manner by using an\noctree representation. The network learns to predict both the structure of the\noctree, and the occupancy values of individual cells. This makes it a\nparticularly valuable technique for generating 3D shapes. In contrast to\nstandard decoders acting on regular voxel grids, the architecture does not have\ncubic complexity. This allows representing much higher resolution outputs with\na limited memory budget. We demonstrate this in several application domains,\nincluding 3D convolutional autoencoders, generation of objects and whole scenes\nfrom high-level representations, and shape from a single image.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 07:45:59 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 09:19:18 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 18:07:33 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Tatarchenko", "Maxim", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1703.09470", "submitter": "Silvano Galliani", "authors": "Silvano Galliani, Charis Lanaras, Dimitrios Marmanis, Emmanuel\n  Baltsavias, Konrad Schindler", "title": "Learned Spectral Super-Resolution", "comments": "Submitted to ICCV 2017 (10 pages, 8 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for blind, single-image spectral super-resolution.\nWhile conventional super-resolution aims to increase the spatial resolution of\nan input image, our goal is to spectrally enhance the input, i.e., generate an\nimage with the same spatial resolution, but a greatly increased number of\nnarrow (hyper-spectral) wave-length bands. Just like the spatial statistics of\nnatural images has rich structure, which one can exploit as prior to predict\nhigh-frequency content from a low resolution image, the same is also true in\nthe spectral domain: the materials and lighting conditions of the observed\nworld induce structure in the spectrum of wavelengths observed at a given\npixel. Surprisingly, very little work exists that attempts to use this\ndiagnosis and achieve blind spectral super-resolution from single images. We\nstart from the conjecture that, just like in the spatial domain, we can learn\nthe statistics of natural image spectra, and with its help generate finely\nresolved hyper-spectral images from RGB input. Technically, we follow the\ncurrent best practice and implement a convolutional neural network (CNN), which\nis trained to carry out the end-to-end mapping from an entire RGB image to the\ncorresponding hyperspectral image of equal size. We demonstrate spectral\nsuper-resolution both for conventional RGB images and for multi-spectral\nsatellite data, outperforming the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:17:38 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Galliani", "Silvano", ""], ["Lanaras", "Charis", ""], ["Marmanis", "Dimitrios", ""], ["Baltsavias", "Emmanuel", ""], ["Schindler", "Konrad", ""]]}, {"id": "1703.09471", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Mario Fritz, Bernt Schiele", "title": "Adversarial Image Perturbation for Privacy Protection -- A Game Theory\n  Perspective", "comments": "To appear at ICCV'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users like sharing personal photos with others through social media. At the\nsame time, they might want to make automatic identification in such photos\ndifficult or even impossible. Classic obfuscation methods such as blurring are\nnot only unpleasant but also not as effective as one would expect. Recent\nstudies on adversarial image perturbations (AIP) suggest that it is possible to\nconfuse recognition systems effectively without unpleasant artifacts. However,\nin the presence of counter measures against AIPs, it is unclear how effective\nAIP would be in particular when the choice of counter measure is unknown. Game\ntheory provides tools for studying the interaction between agents with\nuncertainties in the strategies. We introduce a general game theoretical\nframework for the user-recogniser dynamics, and present a case study that\ninvolves current state of the art AIP and person recognition techniques. We\nderive the optimal strategy for the user that assures an upper bound on the\nrecognition rate independent of the recogniser's counter measure. Code is\navailable at https://goo.gl/hgvbNK.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:17:47 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 10:01:43 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Oh", "Seong Joon", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1703.09474", "submitter": "Ancong Wu", "authors": "Ancong Wu, Wei-Shi Zheng, Jianhuang Lai", "title": "Robust Depth-based Person Re-identification", "comments": "IEEE Transactions on Image Processing Early Access", "journal-ref": null, "doi": "10.1109/TIP.2017.2675201", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) aims to match people across non-overlapping\ncamera views. So far the RGB-based appearance is widely used in most existing\nworks. However, when people appeared in extreme illumination or changed\nclothes, the RGB appearance-based re-id methods tended to fail. To overcome\nthis problem, we propose to exploit depth information to provide more invariant\nbody shape and skeleton information regardless of illumination and color\nchange. More specifically, we exploit depth voxel covariance descriptor and\nfurther propose a locally rotation invariant depth shape descriptor called\nEigen-depth feature to describe pedestrian body shape. We prove that the\ndistance between any two covariance matrices on the Riemannian manifold is\nequivalent to the Euclidean distance between the corresponding Eigen-depth\nfeatures. Furthermore, we propose a kernelized implicit feature transfer scheme\nto estimate Eigen-depth feature implicitly from RGB image when depth\ninformation is not available. We find that combining the estimated depth\nfeatures with RGB-based appearance features can sometimes help to better reduce\nvisual ambiguities of appearance features caused by illumination and similar\nclothes. The effectiveness of our models was validated on publicly available\ndepth pedestrian datasets as compared to related methods for person\nre-identification.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:26:54 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Wu", "Ancong", ""], ["Zheng", "Wei-Shi", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1703.09499", "submitter": "Yangyang Li", "authors": "Yangyang Li and Ruqian Lu", "title": "Locality preserving projection on SPD matrix Lie group: algorithm and\n  analysis", "comments": "15 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive definite (SPD) matrices used as feature descriptors in\nimage recognition are usually high dimensional. Traditional manifold learning\nis only applicable for reducing the dimension of high-dimensional vector-form\ndata. For high-dimensional SPD matrices, directly using manifold learning\nalgorithms to reduce the dimension of matrix-form data is impossible. The SPD\nmatrix must first be transformed into a long vector, and then the dimension of\nthis vector must be reduced. However, this approach breaks the spatial\nstructure of the SPD matrix space. To overcome this limitation, we propose a\nnew dimension reduction algorithm on SPD matrix space to transform\nhigh-dimensional SPD matrices into low-dimensional SPD matrices. Our work is\nbased on the fact that the set of all SPD matrices with the same size has a Lie\ngroup structure, and we aim to transform the manifold learning to the SPD\nmatrix Lie group. We use the basic idea of the manifold learning algorithm\ncalled locality preserving projection (LPP) to construct the corresponding\nLaplacian matrix on the SPD matrix Lie group. Thus, we call our approach\nLie-LPP to emphasize its Lie group character. We present a detailed algorithm\nanalysis and show through experiments that Lie-LPP achieves effective results\non human action recognition and human face recognition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 10:38:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:47:32 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Li", "Yangyang", ""], ["Lu", "Ruqian", ""]]}, {"id": "1703.09507", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Carlos D. Castillo and Rama Chellappa", "title": "L2-constrained Softmax Loss for Discriminative Face Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the performance of face verification systems has\nsignificantly improved using deep convolutional neural networks (DCNNs). A\ntypical pipeline for face verification includes training a deep network for\nsubject classification with softmax loss, using the penultimate layer output as\nthe feature descriptor, and generating a cosine similarity score given a pair\nof face images. The softmax loss function does not optimize the features to\nhave higher similarity score for positive pairs and lower similarity score for\nnegative pairs, which leads to a performance gap. In this paper, we add an\nL2-constraint to the feature descriptors which restricts them to lie on a\nhypersphere of a fixed radius. This module can be easily implemented using\nexisting deep learning frameworks. We show that integrating this simple step in\nthe training pipeline significantly boosts the performance of face\nverification. Specifically, we achieve state-of-the-art results on the\nchallenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept\nRate 0.0001 on the face verification protocol. Additionally, we achieve\nstate-of-the-art performance on LFW dataset with an accuracy of 99.78%, and\ncompeting performance on YTF dataset with accuracy of 96.08%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 11:19:50 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 21:30:51 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 18:58:18 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1703.09529", "submitter": "Abel Gonzalez-Garcia", "authors": "Abel Gonzalez-Garcia, Davide Modolo, Vittorio Ferrari", "title": "Objects as context for detecting their semantic parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semantic part detection approach that effectively leverages\nobject information.We use the object appearance and its class as indicators of\nwhat parts to expect. We also model the expected relative location of parts\ninside the objects based on their appearance. We achieve this with a new\nnetwork module, called OffsetNet, that efficiently predicts a variable number\nof part locations within a given object. Our model incorporates all these cues\nto detect parts in the context of their objects. This leads to considerably\nhigher performance for the challenging task of part detection compared to using\npart appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to\nother part detection methods on both PASCAL-Part and CUB200-2011 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:11:28 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 18:23:01 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 13:40:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gonzalez-Garcia", "Abel", ""], ["Modolo", "Davide", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1703.09550", "submitter": "Benjamin Kiessling", "authors": "Maxim Romanov, Matthew Thomas Miller, Sarah Bowen Savant, Benjamin\n  Kiessling", "title": "Important New Developments in Arabographic Optical Character Recognition\n  (OCR)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OpenITI team has achieved Optical Character Recognition (OCR) accuracy\nrates for classical Arabic-script texts in the high nineties. These numbers are\nbased on our tests of seven different Arabic-script texts of varying quality\nand typefaces, totaling over 7,000 lines. These accuracy rates not only\nrepresent a distinct improvement over the actual accuracy rates of the various\nproprietary OCR options for classical Arabic-script texts, but, equally\nimportant, they are produced using an open-source OCR software, thus enabling\nus to make this Arabic-script OCR technology freely available to the broader\nIslamic, Persian, and Arabic Studies communities.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:42:58 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Romanov", "Maxim", ""], ["Miller", "Matthew Thomas", ""], ["Savant", "Sarah Bowen", ""], ["Kiessling", "Benjamin", ""]]}, {"id": "1703.09554", "submitter": "Anna Khoreva", "authors": "Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox, Bernt Schiele", "title": "Lucid Data Dreaming for Video Object Segmentation", "comments": "Accepted in International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks reach top quality in pixel-level video object\nsegmentation but require a large amount of training data (1k~100k) to deliver\nsuch results. We propose a new training strategy which achieves\nstate-of-the-art results across three evaluation datasets while using 20x~1000x\nless annotated data than competing methods. Our approach is suitable for both\nsingle and multiple object segmentation. Instead of using large training sets\nhoping to generalize across domains, we generate in-domain training data using\nthe provided annotation on the first frame of each video to synthesize (\"lucid\ndream\") plausible future video frames. In-domain per-video training data allows\nus to train high quality appearance- and motion-based models, as well as tune\nthe post-processing stage. This approach allows to reach competitive results\neven when training from only a single annotated frame, without ImageNet\npre-training. Our results indicate that using a larger training set is not\nautomatically better, and that for the video object segmentation task a smaller\ntraining set that is closer to the target domain is more effective. This\nchanges the mindset regarding how many training samples and general\n\"objectness\" knowledge are required for the video object segmentation task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:56:40 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 12:50:09 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 13:38:20 GMT"}, {"version": "v4", "created": "Sun, 3 Feb 2019 16:34:23 GMT"}, {"version": "v5", "created": "Wed, 13 Mar 2019 19:55:04 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Khoreva", "Anna", ""], ["Benenson", "Rodrigo", ""], ["Ilg", "Eddy", ""], ["Brox", "Thomas", ""], ["Schiele", "Bernt", ""]]}, {"id": "1703.09625", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Tae-Kyun Kim", "title": "Learning and Refining of Privileged Information-based RNNs for Action\n  Recognition from Depth Sequences", "comments": "conference cvpr 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing RNN-based approaches for action recognition from depth sequences\nrequire either skeleton joints or hand-crafted depth features as inputs. An\nend-to-end manner, mapping from raw depth maps to action classes, is\nnon-trivial to design due to the fact that: 1) single channel map lacks texture\nthus weakens the discriminative power; 2) relatively small set of depth\ntraining data. To address these challenges, we propose to learn an RNN driven\nby privileged information (PI) in three-steps: An encoder is pre-trained to\nlearn a joint embedding of depth appearance and PI (i.e. skeleton joints). The\nlearned embedding layers are then tuned in the learning step, aiming to\noptimize the network by exploiting PI in a form of multi-task loss. However,\nexploiting PI as a secondary task provides little help to improve the\nperformance of a primary task (i.e. classification) due to the gap between\nthem. Finally, a bridging matrix is defined to connect two tasks by discovering\nlatent PI in the refining step. Our PI-based classification loss maintains a\nconsistency between latent PI and predicted distribution. The latent PI and\nnetwork are iteratively estimated and updated in an expectation-maximization\nprocedure. The proposed learning process provides greater discriminative power\nto model subtle depth difference, while helping avoid overfitting the scarcer\ntraining data. Our experiments show significant performance gains over\nstate-of-the-art methods on three public benchmark datasets and our newly\ncollected Blanket dataset.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 15:08:37 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 15:30:03 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 09:34:43 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 11:49:02 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1703.09684", "submitter": "Kushal Kafle", "authors": "Kushal Kafle and Christopher Kanan", "title": "An Analysis of Visual Question Answering Algorithms", "comments": "To appear in ICCV 2017. Visit http://kushalkafle.com/projects/tdiuc\n  to download the dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual question answering (VQA), an algorithm must answer text-based\nquestions about images. While multiple datasets for VQA have been created since\nlate 2014, they all have flaws in both their content and the way algorithms are\nevaluated on them. As a result, evaluation scores are inflated and\npredominantly determined by answering easier questions, making it difficult to\ncompare different methods. In this paper, we analyze existing VQA algorithms\nusing a new dataset. It contains over 1.6 million questions organized into 12\ndifferent categories. We also introduce questions that are meaningless for a\ngiven image to force a VQA system to reason about image content. We propose new\nevaluation schemes that compensate for over-represented question-types and make\nit easier to study the strengths and weaknesses of algorithms. We analyze the\nperformance of both baseline and state-of-the-art VQA models, including\nmulti-modal compact bilinear pooling (MCB), neural module networks, and\nrecurrent answering units. Our experiments establish how attention helps\ncertain categories more than others, determine which models work better than\nothers, and explain how simple models (e.g. MLP) can surpass more complex\nmodels (MCB) by simply learning to answer large, easy question categories.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 17:48:07 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 18:56:45 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "1703.09690", "submitter": "Xiao-Yang Liu", "authors": "Fei Jiang, Xiao-Yang Liu, Hongtao Lu, Ruimin Shen", "title": "Efficient Two-Dimensional Sparse Coding Using Tensor-Linear Combination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding (SC) is an automatic feature extraction and selection technique\nthat is widely used in unsupervised learning. However, conventional SC\nvectorizes the input images, which breaks apart the local proximity of pixels\nand destructs the elementary object structures of images. In this paper, we\npropose a novel two-dimensional sparse coding (2DSC) scheme that represents the\ninput images as the tensor-linear combinations under a novel algebraic\nframework. 2DSC learns much more concise dictionaries because it uses the\ncircular convolution operator, since the shifted versions of atoms learned by\nconventional SC are treated as the same ones. We apply 2DSC to natural images\nand demonstrate that 2DSC returns meaningful dictionaries for large patches.\nMoreover, for mutli-spectral images denoising, the proposed 2DSC reduces\ncomputational costs with competitive performance in comparison with the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 17:54:39 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Jiang", "Fei", ""], ["Liu", "Xiao-Yang", ""], ["Lu", "Hongtao", ""], ["Shen", "Ruimin", ""]]}, {"id": "1703.09695", "submitter": "Nasim Souly", "authors": "Nasim Souly, Concetto Spampinato and Mubarak Shah", "title": "Semi and Weakly Supervised Semantic Segmentation Using Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has been a long standing challenging task in computer\nvision. It aims at assigning a label to each image pixel and needs significant\nnumber of pixellevel annotated data, which is often unavailable. To address\nthis lack, in this paper, we leverage, on one hand, massive amount of available\nunlabeled or weakly labeled data, and on the other hand, non-real images\ncreated through Generative Adversarial Networks. In particular, we propose a\nsemi-supervised framework ,based on Generative Adversarial Networks (GANs),\nwhich consists of a generator network to provide extra training examples to a\nmulti-class classifier, acting as discriminator in the GAN framework, that\nassigns sample a label y from the K possible classes or marks it as a fake\nsample (extra class). The underlying idea is that adding large fake visual data\nforces real samples to be close in the feature space, enabling a bottom-up\nclustering process, which, in turn, improves multiclass pixel classification.\nTo ensure higher quality of generated images for GANs with consequent improved\npixel classification, we extend the above framework by adding weakly annotated\ndata, i.e., we provide class level information to the generator. We tested our\napproaches on several challenging benchmarking visual datasets, i.e. PASCAL,\nSiftFLow, Stanford and CamVid, achieving competitive performance also compared\nto state-of-the-art semantic segmentation method\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 17:57:21 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Souly", "Nasim", ""], ["Spampinato", "Concetto", ""], ["Shah", "Mubarak", ""]]}, {"id": "1703.09725", "submitter": "Tavi Halperin", "authors": "Tavi Halperin and Michael Werman", "title": "An Epipolar Line from a Single Pixel", "comments": "WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the epipolar geometry from feature points between cameras with very\ndifferent viewpoints is often error prone, as an object's appearance can vary\ngreatly between images. For such cases, it has been shown that using motion\nextracted from video can achieve much better results than using a static image.\nThis paper extends these earlier works based on the scene dynamics. In this\npaper we propose a new method to compute the epipolar geometry from a video\nstream, by exploiting the following observation: For a pixel p in Image A, all\npixels corresponding to p in Image B are on the same epipolar line.\nEquivalently, the image of the line going through camera A's center and p is an\nepipolar line in B. Therefore, when cameras A and B are synchronized, the\nmomentary images of two objects projecting to the same pixel, p, in camera A at\ntimes t1 and t2, lie on an epipolar line in camera B. Based on this observation\nwe achieve fast and precise computation of epipolar lines. Calibrating cameras\nbased on our method of finding epipolar lines is much faster and more robust\nthan previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:01:05 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 12:24:43 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 17:48:59 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Halperin", "Tavi", ""], ["Werman", "Michael", ""]]}, {"id": "1703.09744", "submitter": "Shun Yang", "authors": "Shun Yang, Wenshuo Wang, Chang Liu, Kevin Deng and J. Karl Hedrick", "title": "Feature Analysis and Selection for Training an End-to-End Autonomous\n  Vehicle Controller Using the Deep Learning Approach", "comments": "6 pages, 11 figures, 3 tables, accepted by 2017 IEEE Intelligent\n  Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based approaches have been widely used for training controllers\nfor autonomous vehicles due to their powerful ability to approximate nonlinear\nfunctions or policies. However, the training process usually requires large\nlabeled data sets and takes a lot of time. In this paper, we analyze the\ninfluences of features on the performance of controllers trained using the\nconvolutional neural networks (CNNs), which gives a guideline of feature\nselection to reduce computation cost. We collect a large set of data using The\nOpen Racing Car Simulator (TORCS) and classify the image features into three\ncategories (sky-related, roadside-related, and road-related features).We then\ndesign two experimental frameworks to investigate the importance of each single\nfeature for training a CNN controller.The first framework uses the training\ndata with all three features included to train a controller, which is then\ntested with data that has one feature removed to evaluate the feature's\neffects. The second framework is trained with the data that has one feature\nexcluded, while all three features are included in the test data. Different\ndriving scenarios are selected to test and analyze the trained controllers\nusing the two experimental frameworks. The experiment results show that (1) the\nroad-related features are indispensable for training the controller, (2) the\nroadside-related features are useful to improve the generalizability of the\ncontroller to scenarios with complicated roadside information, and (3) the\nsky-related features have limited contribution to train an end-to-end\nautonomous vehicle controller.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:52:38 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Yang", "Shun", ""], ["Wang", "Wenshuo", ""], ["Liu", "Chang", ""], ["Deng", "Kevin", ""], ["Hedrick", "J. Karl", ""]]}, {"id": "1703.09746", "submitter": "Wei Wen", "authors": "Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li", "title": "Coordinating Filters for Faster Deep Neural Networks", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large-scale Deep Neural Networks (DNNs) have achieved remarkable\nsuccesses in a large variety of computer vision tasks. However, the high\ncomputation intensity of DNNs makes it challenging to deploy these models on\nresource-limited systems. Some studies used low-rank approaches that\napproximate the filters by low-rank basis to accelerate the testing. Those\nworks directly decomposed the pre-trained DNNs by Low-Rank Approximations\n(LRA). How to train DNNs toward lower-rank space for more efficient DNNs,\nhowever, remains as an open area. To solve the issue, in this work, we propose\nForce Regularization, which uses attractive forces to enforce filters so as to\ncoordinate more weight information into lower-rank space. We mathematically and\nempirically verify that after applying our technique, standard LRA methods can\nreconstruct filters using much lower basis and thus result in faster DNNs. The\neffectiveness of our approach is comprehensively evaluated in ResNets, AlexNet,\nand GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup\non modern GPU without accuracy loss and 4.05x speedup on CPU by paying small\naccuracy degradation. Moreover, Force Regularization better initializes the\nlow-rank DNNs such that the fine-tuning can converge faster toward higher\naccuracy. The obtained lower-rank DNNs can be further sparsified, proving that\nForce Regularization can be integrated with state-of-the-art sparsity-based\nacceleration methods. Source code is available in\nhttps://github.com/wenwei202/caffe\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:55:05 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 21:35:12 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 17:54:31 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wen", "Wei", ""], ["Xu", "Cong", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1703.09771", "submitter": "Mathieu Garon", "authors": "Mathieu Garon, Jean-Fran\\c{c}ois Lalonde", "title": "Deep 6-DOF Tracking", "comments": "9 pages, 9 figures, ISMAR 2017, TVCG special edition Website:\n  http://vision.gel.ulaval.ca/~jflalonde/projects/deepTracking/index.html", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2017", "doi": "10.1109/TVCG.2017.2734599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a temporal 6-DOF tracking method which leverages deep learning to\nachieve state-of-the-art performance on challenging datasets of real world\ncapture. Our method is both more accurate and more robust to occlusions than\nthe existing best performing approaches while maintaining real-time\nperformance. To assess its efficacy, we evaluate our approach on several\nchallenging RGBD sequences of real objects in a variety of conditions. Notably,\nwe systematically evaluate robustness to occlusions through a series of\nsequences where the object to be tracked is increasingly occluded. Finally, our\napproach is purely data-driven and does not require any hand-designed features:\nrobust tracking is automatically learned from data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 19:55:19 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 19:45:22 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Garon", "Mathieu", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1703.09778", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Jarno Nikkanen, Moncef Gabbouj", "title": "INTEL-TUT Dataset for Camera Invariant Color Constancy Research", "comments": "Download Link for the Dataset:\n  https://etsin.avointiede.fi/dataset/urn-nbn-fi-csc-kata20170321084219004008\n  Submission Info: Submitted to IEEE TIP", "journal-ref": "Published in: IEEE Transactions on Image Processing ( Volume: 27,\n  Issue: 2, Feb. 2018 )", "doi": "10.1109/TIP.2017.2764264", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a novel dataset designed for camera invariant color\nconstancy research. Camera invariance corresponds to the robustness of an\nalgorithm's performance when run on images of the same scene taken by different\ncameras. Accordingly, images in the database correspond to several lab and\nfield scenes each of which are captured by three different cameras with minimal\nregistration errors. The lab scenes are also captured under five different\nilluminations. The spectral responses of cameras and the spectral power\ndistributions of the lab light sources are also provided, as they may prove\nbeneficial for training future algorithms to achieve color constancy. For a\nfair evaluation of future methods, we provide guidelines for supervised methods\nwith indicated training, validation and testing partitions. Accordingly, we\nevaluate a recently proposed convolutional neural network based color constancy\nalgorithm as a baseline for future research. As a side contribution, this\ndataset also includes images taken by a mobile camera with color shading\ncorrected and uncorrected results. This allows research on the effect of color\nshading as well.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 13:07:45 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 07:29:34 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Aytekin", "Caglar", ""], ["Nikkanen", "Jarno", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1703.09779", "submitter": "Kamel Abdelouahab Kamel Eddine ABDELOUAHAB", "authors": "Kamel Abdelouahab, Cedric Bourrasset, Maxime Pelcat, Fran\\c{c}ois\n  Berry, Jean-Charles Quinton, Jocelyn Serot", "title": "A Holistic Approach for Optimizing DSP Block Utilization of a CNN\n  implementation on FPGA", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": "10.1145/2967413.2967430", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are becoming the de-facto standard models for image\nunderstanding, and more generally for computer vision tasks. As they involve\nhighly parallelizable computations, CNN are well suited to current fine grain\nprogrammable logic devices. Thus, multiple CNN accelerators have been\nsuccessfully implemented on FPGAs. Unfortunately, FPGA resources such as logic\nelements or DSP units remain limited. This work presents a holistic method\nrelying on approximate computing and design space exploration to optimize the\nDSP block utilization of a CNN implementation on an FPGA. This method was\ntested when implementing a reconfigurable OCR convolutional neural network on\nan Altera Stratix V device and varying both data representation and CNN\ntopology in order to find the best combination in terms of DSP block\nutilization and classification accuracy. This exploration generated dataflow\narchitectures of 76 CNN topologies with 5 different fixed point representation.\nMost efficient implementation performs 883 classifications/sec at 256 x 256\nresolution using 8% of the available DSP blocks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:41:37 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Bourrasset", "Cedric", ""], ["Pelcat", "Maxime", ""], ["Berry", "Fran\u00e7ois", ""], ["Quinton", "Jean-Charles", ""], ["Serot", "Jocelyn", ""]]}, {"id": "1703.09783", "submitter": "Rui Zhao", "authors": "Rui Zhao, Haider Ali, Patrick van der Smagt", "title": "Two-Stream RNN/CNN for Action Recognition in 3D Videos", "comments": "Published in 2017 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": "10.1109/IROS.2017.8206288", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of actions from video sequences has many applications in\nhealth monitoring, assisted living, surveillance, and smart homes. Despite\nadvances in sensing, in particular related to 3D video, the methodologies to\nprocess the data are still subject to research. We demonstrate superior results\nby a system which combines recurrent neural networks with convolutional neural\nnetworks in a voting approach. The gated-recurrent-unit-based neural networks\nare particularly well-suited to distinguish actions based on long-term\ninformation from optical tracking data; the 3D-CNNs focus more on detailed,\nrecent information from video data. The resulting features are merged in an SVM\nwhich then classifies the movement. In this architecture, our method improves\nrecognition rates of state-of-the-art methods by 14% on standard data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 22:29:56 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 16:16:31 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Zhao", "Rui", ""], ["Ali", "Haider", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1703.09784", "submitter": "Yanhai Gan", "authors": "Yanhai Gan, Huifang Chi, Ying Gao, Jun Liu, Guoqiang Zhong, Junyu Dong", "title": "Perception Driven Texture Generation", "comments": "7 pages, 4 figures, icme2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel task of generating texture images from\nperceptual descriptions. Previous work on texture generation focused on either\nsynthesis from examples or generation from procedural models. Generating\ntextures from perceptual attributes have not been well studied yet. Meanwhile,\nperceptual attributes, such as directionality, regularity and roughness are\nimportant factors for human observers to describe a texture. In this paper, we\npropose a joint deep network model that combines adversarial training and\nperceptual feature regression for texture generation, while only random noise\nand user-defined perceptual attributes are required as input. In this model, a\npreliminary trained convolutional neural network is essentially integrated with\nthe adversarial framework, which can drive the generated textures to possess\ngiven perceptual attributes. An important aspect of the proposed model is that,\nif we change one of the input perceptual features, the corresponding appearance\nof the generated textures will also be changed. We design several experiments\nto validate the effectiveness of the proposed method. The results show that the\nproposed method can produce high quality texture images with desired perceptual\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 01:25:30 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Gan", "Yanhai", ""], ["Chi", "Huifang", ""], ["Gao", "Ying", ""], ["Liu", "Jun", ""], ["Zhong", "Guoqiang", ""], ["Dong", "Junyu", ""]]}, {"id": "1703.09788", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Chenliang Xu and Jason J. Corso", "title": "Towards Automatic Learning of Procedures from Web Instructional Videos", "comments": "AAAI 2018 Camera-ready version. See http://youcook2.eecs.umich.edu\n  for YouCook2 dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential for agents, whether embodied or software, to learn by observing\nother agents performing procedures involving objects and actions is rich.\nCurrent research on automatic procedure learning heavily relies on action\nlabels or video subtitles, even during the evaluation phase, which makes them\ninfeasible in real-world scenarios. This leads to our question: can the\nhuman-consensus structure of a procedure be learned from a large set of long,\nunconstrained videos (e.g., instructional videos from YouTube) with only visual\nevidence? To answer this question, we introduce the problem of procedure\nsegmentation--to segment a video procedure into category-independent procedure\nsegments. Given that no large-scale dataset is available for this problem, we\ncollect a large-scale procedure segmentation dataset with procedure segments\ntemporally localized and described; we use cooking videos and name the dataset\nYouCook2. We propose a segment-level recurrent network for generating procedure\nsegments by modeling the dependencies across segments. The generated segments\ncan be used as pre-processing for other tasks, such as dense video captioning\nand event parsing. We show in our experiments that the proposed model\noutperforms competitive baselines in procedure segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 20:28:52 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 12:40:06 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 20:37:43 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhou", "Luowei", ""], ["Xu", "Chenliang", ""], ["Corso", "Jason J.", ""]]}, {"id": "1703.09793", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao and Radha Poovendran", "title": "Deceiving Google's Cloud Video Intelligence API Built for Summarizing\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid progress of the techniques for image classification, video\nannotation has remained a challenging task. Automated video annotation would be\na breakthrough technology, enabling users to search within the videos.\nRecently, Google introduced the Cloud Video Intelligence API for video\nanalysis. As per the website, the system can be used to \"separate signal from\nnoise, by retrieving relevant information at the video, shot or per frame\"\nlevel. A demonstration website has been also launched, which allows anyone to\nselect a video for annotation. The API then detects the video labels (objects\nwithin the video) as well as shot labels (description of the video events over\ntime). In this paper, we examine the usability of the Google's Cloud Video\nIntelligence API in adversarial environments. In particular, we investigate\nwhether an adversary can subtly manipulate a video in such a way that the API\nwill return only the adversary-desired labels. For this, we select an image,\nwhich is different from the video content, and insert it, periodically and at a\nvery low rate, into the video. We found that if we insert one image every two\nseconds, the API is deceived into annotating the video as if it only contained\nthe inserted image. Note that the modification to the video is hardly\nnoticeable as, for instance, for a typical frame rate of 25, we insert only one\nimage per 50 video frames. We also found that, by inserting one image per\nsecond, all the shot labels returned by the API are related to the inserted\nimage. We perform the experiments on the sample videos provided by the API\ndemonstration website and show that our attack is successful with different\nvideos and images.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 20:52:43 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 05:25:36 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Poovendran", "Radha", ""]]}, {"id": "1703.09833", "submitter": "Qianli Liao", "authors": "Qianli Liao and Tomaso Poggio", "title": "Theory II: Landscape of the Empirical Risk in Deep Learning", "comments": "Merged figures to make the main text more compact. Moved some similar\n  figures to the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous theoretical work on deep learning and neural network optimization\ntend to focus on avoiding saddle points and local minima. However, the\npractical observation is that, at least in the case of the most successful Deep\nConvolutional Neural Networks (DCNNs), practitioners can always increase the\nnetwork size to fit the training data (an extreme example would be [1]). The\nmost successful DCNNs such as VGG and ResNets are best used with a degree of\n\"overparametrization\". In this work, we characterize with a mix of theory and\nexperiments, the landscape of the empirical risk of overparametrized DCNNs. We\nfirst prove in the regression framework the existence of a large number of\ndegenerate global minimizers with zero empirical error (modulo inconsistent\nequations). The argument that relies on the use of Bezout theorem is rigorous\nwhen the RELUs are replaced by a polynomial nonlinearity (which empirically\nworks as well). As described in our Theory III [2] paper, the same minimizers\nare degenerate and thus very likely to be found by SGD that will furthermore\nselect with higher probability the most robust zero-minimizer. We further\nexperimentally explored and visualized the landscape of empirical risk of a\nDCNN on CIFAR-10 during the entire training process and especially the global\nminima. Finally, based on our theoretical and experimental results, we propose\nan intuitive model of the landscape of DCNN's empirical loss surface, which\nmight not be as complicated as people commonly believe.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 22:47:04 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 09:33:35 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Liao", "Qianli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1703.09856", "submitter": "Joseph Antony A", "authors": "Joseph Antony, Kevin McGuinness, Kieran Moran and Noel E O'Connor", "title": "Automatic Detection of Knee Joints and Quantification of Knee\n  Osteoarthritis Severity using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to automatically quantify the severity\nof knee OA using X-ray images. Automatically quantifying knee OA severity\ninvolves two steps: first, automatically localizing the knee joints; next,\nclassifying the localized knee joint images. We introduce a new approach to\nautomatically detect the knee joints using a fully convolutional neural network\n(FCN). We train convolutional neural networks (CNN) from scratch to\nautomatically quantify the knee OA severity optimizing a weighted ratio of two\nloss functions: categorical cross-entropy and mean-squared loss. This joint\ntraining further improves the overall quantification of knee OA severity, with\nthe added benefit of naturally producing simultaneous multi-class\nclassification and regression outputs. Two public datasets are used to evaluate\nour approach, the Osteoarthritis Initiative (OAI) and the Multicenter\nOsteoarthritis Study (MOST), with extremely promising results that outperform\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 01:29:32 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Moran", "Kieran", ""], ["O'Connor", "Noel E", ""]]}, {"id": "1703.09859", "submitter": "Ryan Szeto", "authors": "Ryan Szeto and Jason J. Corso", "title": "Click Here: Human-Localized Keypoints as Guidance for Viewpoint\n  Estimation", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and address a human-in-the-loop variant of the monocular\nviewpoint estimation task in which the location and class of one semantic\nobject keypoint is available at test time. In order to leverage the keypoint\ninformation, we devise a Convolutional Neural Network called Click-Here CNN\n(CH-CNN) that integrates the keypoint information with activations from the\nlayers that process the image. It transforms the keypoint information into a 2D\nmap that can be used to weigh features from certain parts of the image more\nheavily. The weighted sum of these spatial features is combined with global\nimage features to provide relevant information to the prediction layers. To\ntrain our network, we collect a novel dataset of 3D keypoint annotations on\nthousands of CAD models, and synthetically render millions of images with 2D\nkeypoint information. On test instances from PASCAL 3D+, our model achieves a\nmean class accuracy of 90.7%, whereas the state-of-the-art baseline only\nobtains 85.7% mean class accuracy, justifying our argument for\nhuman-in-the-loop inference.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 02:08:08 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 23:01:39 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Szeto", "Ryan", ""], ["Corso", "Jason J.", ""]]}, {"id": "1703.09880", "submitter": "Arvind Balachandrasekaran", "authors": "Arvind Balachandrasekaran and Mathews Jacob", "title": "Novel Structured Low-rank algorithm to recover spatially smooth\n  exponential image time series", "comments": "4 pages, 3 figures, accepted at ISBI 2017, Melbourne, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structured low rank matrix completion algorithm to recover a\ntime series of images consisting of linear combination of exponential\nparameters at every pixel, from under-sampled Fourier measurements. The spatial\nsmoothness of these parameters is exploited along with the exponential\nstructure of the time series at every pixel, to derive an annihilation relation\nin the $k-t$ domain. This annihilation relation translates into a structured\nlow rank matrix formed from the $k-t$ samples. We demonstrate the algorithm in\nthe parameter mapping setting and show significant improvement over state of\nthe art methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 04:12:10 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Balachandrasekaran", "Arvind", ""], ["Jacob", "Mathews", ""]]}, {"id": "1703.09891", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Zhiwei Deng, Guang-Tong Zhou, Fei Sha, Greg Mori", "title": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation", "comments": "Pre-prints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires a detailed labeling of image pixels by object\ncategory. Information derived from local image patches is necessary to describe\nthe detailed shape of individual objects. However, this information is\nambiguous and can result in noisy labels. Global inference of image content can\ninstead capture the general semantic concepts present. We advocate that\nholistic inference of image concepts provides valuable information for detailed\npixel labeling. We propose a generic framework to leverage holistic information\nin the form of a LabelBank for pixel-level segmentation.\n  We show the ability of our framework to improve semantic segmentation\nperformance in a variety of settings. We learn models for extracting a holistic\nLabelBank from visual cues, attributes, and/or textual descriptions. We\ndemonstrate improvements in semantic segmentation accuracy on standard datasets\nacross a range of state-of-the-art segmentation architectures and holistic\ninference approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 05:58:21 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Hu", "Hexiang", ""], ["Deng", "Zhiwei", ""], ["Zhou", "Guang-Tong", ""], ["Sha", "Fei", ""], ["Mori", "Greg", ""]]}, {"id": "1703.09911", "submitter": "Shiyu Chen", "authors": "Shiyu Chen, Shangfei Wang, Tanfang Chen, Xiaoxiao Shi", "title": "Learning with Privileged Information for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for learning multi-label\nclassifiers with the help of privileged information. Specifically, we use\nsimilarity constraints to capture the relationship between available\ninformation and privileged information, and use ranking constraints to capture\nthe dependencies among multiple labels. By integrating similarity constraints\nand ranking constraints into the learning process of classifiers, the\nprivileged information and the dependencies among multiple labels are exploited\nto construct better classifiers during training. A maximum margin classifier is\nadopted, and an efficient learning algorithm of the proposed method is also\ndeveloped. We evaluate the proposed method on two applications: multiple object\nrecognition from images with the help of implicit information about object\nimportance conveyed by the list of manually annotated image tags; and multiple\nfacial action unit detection from low-resolution images augmented by\nhigh-resolution images. Experimental results demonstrate that the proposed\nmethod can effectively take full advantage of privileged information and\ndependencies among multiple labels for better object recognition and better\nfacial action unit detection.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 07:17:52 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Chen", "Shiyu", ""], ["Wang", "Shangfei", ""], ["Chen", "Tanfang", ""], ["Shi", "Xiaoxiao", ""]]}, {"id": "1703.09912", "submitter": "Rick Chang", "authors": "J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya\n  Kumar, Aswin C. Sankaranarayanan", "title": "One Network to Solve Them All --- Solving Linear Inverse Problems using\n  Deep Projection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning methods have achieved state-of-the-art performance in\nmany challenging inverse problems like image inpainting and super-resolution,\nthey invariably involve problem-specific training of the networks. Under this\napproach, different problems require different networks. In scenarios where we\nneed to solve a wide variety of problems, e.g., on a mobile camera, it is\ninefficient and costly to use these specially-trained networks. On the other\nhand, traditional methods using signal priors can be used in all linear inverse\nproblems but often have worse performance on challenging tasks. In this work,\nwe provide a middle ground between the two kinds of methods --- we propose a\ngeneral framework to train a single deep neural network that solves arbitrary\nlinear inverse problems. The proposed network acts as a proximal operator for\nan optimization algorithm and projects non-image signals onto the set of\nnatural images defined by the decision boundary of a classifier. In our\nexperiments, the proposed framework demonstrates superior performance over\ntraditional methods using a wavelet sparsity prior and achieves comparable\nperformance of specially-trained networks on tasks including compressive\nsensing and pixel-wise inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 07:20:10 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Chang", "J. H. Rick", ""], ["Li", "Chun-Liang", ""], ["Poczos", "Barnabas", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1703.09913", "submitter": "Hazel Doughty", "authors": "Hazel Doughty, Dima Damen, Walterio Mayol-Cuevas", "title": "Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for assessing skill from video, applicable to a variety\nof tasks, ranging from surgery to drawing and rolling pizza dough. We formulate\nthe problem as pairwise (who's better?) and overall (who's best?) ranking of\nvideo collections, using supervised deep ranking. We propose a novel loss\nfunction that learns discriminative features when a pair of videos exhibit\nvariance in skill, and learns shared features when a pair of videos exhibit\ncomparable skill levels. Results demonstrate our method is applicable across\ntasks, with the percentage of correctly ordered pairs of videos ranging from\n70% to 83% for four datasets. We demonstrate the robustness of our approach via\nsensitivity analysis of its parameters. We see this work as effort toward the\nautomated organization of how-to video collections and overall, generic skill\ndetermination in video.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 07:25:33 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:18:54 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Doughty", "Hazel", ""], ["Damen", "Dima", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1703.09916", "submitter": "Zhengtao Wang", "authors": "Zhengtao Wang, Ce Zhu, Zhiqiang Xia, Qi Guo, Yipeng Liu", "title": "Towards thinner convolutional neural networks through Gradually Global\n  Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep network pruning is an effective method to reduce the storage and\ncomputation cost of deep neural networks when applying them to resource-limited\ndevices. Among many pruning granularities, neuron level pruning will remove\nredundant neurons and filters in the model and result in thinner networks. In\nthis paper, we propose a gradually global pruning scheme for neuron level\npruning. In each pruning step, a small percent of neurons were selected and\ndropped across all layers in the model. We also propose a simple method to\neliminate the biases in evaluating the importance of neurons to make the scheme\nfeasible. Compared with layer-wise pruning scheme, our scheme avoid the\ndifficulty in determining the redundancy in each layer and is more effective\nfor deep networks. Our scheme would automatically find a thinner sub-network in\noriginal network under a given performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 07:31:46 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Wang", "Zhengtao", ""], ["Zhu", "Ce", ""], ["Xia", "Zhiqiang", ""], ["Guo", "Qi", ""], ["Liu", "Yipeng", ""]]}, {"id": "1703.09928", "submitter": "Qiong Zeng", "authors": "Qiong Zeng and Baoquan Chen and Yanir Kleiman and Daniel Cohen-Or and\n  Yangyan Li", "title": "Bundle Optimization for Multi-aspect Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding semantic similarity among images is the core of a wide range of\ncomputer vision applications. An important step towards this goal is to collect\nand learn human perceptions. Interestingly, the semantic context of images is\noften ambiguous as images can be perceived with emphasis on different aspects,\nwhich may be contradictory to each other.\n  In this paper, we present a method for learning the semantic similarity among\nimages, inferring their latent aspects and embedding them into multi-spaces\ncorresponding to their semantic aspects.\n  We consider the multi-embedding problem as an optimization function that\nevaluates the embedded distances with respect to the qualitative clustering\nqueries. The key idea of our approach is to collect and embed qualitative\nmeasures that share the same aspects in bundles. To ensure similarity aspect\nsharing among multiple measures, image classification queries are presented to,\nand solved by users. The collected image clusters are then converted into\nbundles of tuples, which are fed into our bundle optimization algorithm that\njointly infers the aspect similarity and multi-aspect embedding. Extensive\nexperimental results show that our approach significantly outperforms\nstate-of-the-art multi-embedding approaches on various datasets, and scales\nwell for large multi-aspect similarity measures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 08:29:55 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 06:59:19 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 03:16:06 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Zeng", "Qiong", ""], ["Chen", "Baoquan", ""], ["Kleiman", "Yanir", ""], ["Cohen-Or", "Daniel", ""], ["Li", "Yangyan", ""]]}, {"id": "1703.09933", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Nicola Strisciuglio, Nicolai Petkov, Petia Radeva", "title": "Sentiment Recognition in Egocentric Photostreams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelogging is a process of collecting rich source of information about daily\nlife of people. In this paper, we introduce the problem of sentiment analysis\nin egocentric events focusing on the moments that compose the images recalling\npositive, neutral or negative feelings to the observer. We propose a method for\nthe classification of the sentiments in egocentric pictures based on global and\nsemantic image features extracted by Convolutional Neural Networks. We carried\nout experiments on an egocentric dataset, which we organized in 3 classes on\nthe basis of the sentiment that is recalled to the user (positive, negative or\nneutral).\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 08:38:32 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Talavera", "Estefania", ""], ["Strisciuglio", "Nicola", ""], ["Petkov", "Nicolai", ""], ["Radeva", "Petia", ""]]}, {"id": "1703.09964", "submitter": "Siavash Arjomand Bigdeli", "authors": "Siavash Arjomand Bigdeli and Matthias Zwicker", "title": "Image Restoration using Autoencoding Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage denoising autoencoder networks as priors to address\nimage restoration problems. We build on the key observation that the output of\nan optimal denoising autoencoder is a local mean of the true data density, and\nthe autoencoder error (the difference between the output and input of the\ntrained autoencoder) is a mean shift vector. We use the magnitude of this mean\nshift vector, that is, the distance to the local mean, as the negative log\nlikelihood of our natural image prior. For image restoration, we maximize the\nlikelihood using gradient descent by backpropagating the autoencoder error. A\nkey advantage of our approach is that we do not need to train separate networks\nfor different image restoration tasks, such as non-blind deconvolution with\ndifferent kernels, or super-resolution at different magnification factors. We\ndemonstrate state of the art results for non-blind deconvolution and\nsuper-resolution using the same autoencoding prior.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 10:51:49 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Bigdeli", "Siavash Arjomand", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1703.09971", "submitter": "Darryl D. Holm", "authors": "Alexis Arnaudon, Darryl D. Holm, Stefan Sommer", "title": "A Geometric Framework for Stochastic Shape Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s10208-018-9394-z", "report-no": null, "categories": "cs.CV math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stochastic model of diffeomorphisms, whose action on a variety\nof data types descends to stochastic evolution of shapes, images and landmarks.\nThe stochasticity is introduced in the vector field which transports the data\nin the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework for\nshape analysis and image registration. The stochasticity thereby models errors\nor uncertainties of the flow in following the prescribed deformation velocity.\nThe approach is illustrated in the example of finite dimensional landmark\nmanifolds, whose stochastic evolution is studied both via the Fokker-Planck\nequation and by numerical simulations. We derive two approaches for inferring\nparameters of the stochastic model from landmark configurations observed at\ndiscrete time points. The first of the two approaches matches moments of the\nFokker-Planck equation to sample moments of the data, while the second approach\nemploys an Expectation-Maximisation based algorithm using a Monte Carlo bridge\nsampling scheme to optimise the data likelihood. We derive and numerically test\nthe ability of the two approaches to infer the spatial correlation length of\nthe underlying noise.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 11:08:00 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 10:28:10 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 19:12:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Arnaudon", "Alexis", ""], ["Holm", "Darryl D.", ""], ["Sommer", "Stefan", ""]]}, {"id": "1703.09983", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Yu-Gang Jiang and Dequan Wang and Xiangyang Xue", "title": "Iterative Object and Part Transfer for Fine-Grained Recognition", "comments": "To appear in ICME 2017 as an oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of fine-grained recognition is to identify sub-ordinate categories in\nimages like different species of birds. Existing works have confirmed that, in\norder to capture the subtle differences across the categories, automatic\nlocalization of objects and parts is critical. Most approaches for object and\npart localization relied on the bottom-up pipeline, where thousands of region\nproposals are generated and then filtered by pre-trained object/part models.\nThis is computationally expensive and not scalable once the number of\nobjects/parts becomes large. In this paper, we propose a nonparametric\ndata-driven method for object and part localization. Given an unlabeled test\nimage, our approach transfers annotations from a few similar images retrieved\nin the training set. In particular, we propose an iterative transfer strategy\nthat gradually refine the predicted bounding boxes. Based on the located\nobjects and parts, deep convolutional features are extracted for recognition.\nWe evaluate our approach on the widely-used CUB200-2011 dataset and a new and\nlarge dataset called Birdsnap. On both datasets, we achieve better results than\nmany state-of-the-art approaches, including a few using oracle (manually\nannotated) bounding boxes in the test images.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 11:50:34 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Jiang", "Yu-Gang", ""], ["Wang", "Dequan", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1703.09990", "submitter": "Saeed Reza Kheradpisheh", "authors": "Matin N. Ashtiani, Saeed Reza Kheradpisheh, Timoth\\'ee Masquelier,\n  Mohammad Ganjtabesh", "title": "Object categorization in finer levels requires higher spatial\n  frequencies, and therefore takes longer", "comments": null, "journal-ref": "Frontiers in Psychology 2017", "doi": "10.3389/fpsyg.2017.01261", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system contains a hierarchical sequence of modules that take\npart in visual perception at different levels of abstraction, i.e.,\nsuperordinate, basic, and subordinate levels. One important question is to\nidentify the \"entry\" level at which the visual representation is commenced in\nthe process of object recognition. For a long time, it was believed that the\nbasic level had advantage over two others; a claim that has been challenged\nrecently. Here we used a series of psychophysics experiments, based on a rapid\npresentation paradigm, as well as two computational models, with bandpass\nfiltered images to study the processing order of the categorization levels. In\nthese experiments, we investigated the type of visual information required for\ncategorizing objects in each level by varying the spatial frequency bands of\nthe input image. The results of our psychophysics experiments and computational\nmodels are consistent. They indicate that the different spatial frequency\ninformation had different effects on object categorization in each level. In\nthe absence of high frequency information, subordinate and basic level\ncategorization are performed inaccurately, while superordinate level is\nperformed well. This means that, low frequency information is sufficient for\nsuperordinate level, but not for the basic and subordinate levels. These finer\nlevels require high frequency information, which appears to take longer to be\nprocessed, leading to longer reaction times. Finally, to avoid the ceiling\neffect, we evaluated the robustness of the results by adding different amounts\nof noise to the input images and repeating the experiments. As expected, the\ncategorization accuracy decreased and the reaction time increased\nsignificantly, but the trends were the same.This shows that our results are not\ndue to a ceiling effect.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 12:03:21 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Ashtiani", "Matin N.", ""], ["Kheradpisheh", "Saeed Reza", ""], ["Masquelier", "Timoth\u00e9e", ""], ["Ganjtabesh", "Mohammad", ""]]}, {"id": "1703.10025", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei", "title": "Flow-Guided Feature Aggregation for Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending state-of-the-art object detectors from image to video is\nchallenging. The accuracy of detection suffers from degenerated object\nappearances in videos, e.g., motion blur, video defocus, rare poses, etc.\nExisting work attempts to exploit temporal information on box level, but such\nmethods are not trained end-to-end. We present flow-guided feature aggregation,\nan accurate and end-to-end learning framework for video object detection. It\nleverages temporal coherence on feature level instead. It improves the\nper-frame features by aggregation of nearby features along the motion paths,\nand thus improves the video recognition accuracy. Our method significantly\nimproves upon strong single-frame baselines in ImageNet VID, especially for\nmore challenging fast moving objects. Our framework is principled, and on par\nwith the best engineered systems winning the ImageNet VID challenges 2016,\nwithout additional bells-and-whistles. The proposed method, together with Deep\nFeature Flow, powered the winning entry of ImageNet VID challenges 2017. The\ncode is available at\nhttps://github.com/msracver/Flow-Guided-Feature-Aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 13:21:28 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 12:30:38 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Zhu", "Xizhou", ""], ["Wang", "Yujie", ""], ["Dai", "Jifeng", ""], ["Yuan", "Lu", ""], ["Wei", "Yichen", ""]]}, {"id": "1703.10106", "submitter": "Fabien Baradel", "authors": "Fabien Baradel, Christian Wolf, Julien Mille", "title": "Pose-conditioned Spatio-Temporal Attention for Human Action Recognition", "comments": "10 pages, project page:\n  https://fabienbaradel.github.io/pose_rgb_attention_human_action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address human action recognition from multi-modal video data involving\narticulated pose and RGB frames and propose a two-stream approach. The pose\nstream is processed with a convolutional model taking as input a 3D tensor\nholding data from a sub-sequence. A specific joint ordering, which respects the\ntopology of the human body, ensures that different convolutional layers\ncorrespond to meaningful levels of abstraction. The raw RGB stream is handled\nby a spatio-temporal soft-attention mechanism conditioned on features from the\npose network. An LSTM network receives input from a set of image locations at\neach instant. A trainable glimpse sensor extracts features on a set of\npredefined locations specified by the pose stream, namely the 4 hands of the\ntwo people involved in the activity. Appearance features give important cues on\nhand motion and on objects held in each hand. We show that it is of high\ninterest to shift the attention to different hands at different time steps\ndepending on the activity itself. Finally a temporal attention mechanism learns\nhow to fuse LSTM features over time. We evaluate the method on 3 datasets.\nState-of-the-art results are achieved on the largest dataset for human activity\nrecognition, namely NTU-RGB+D, as well as on the SBU Kinect Interaction\ndataset. Performance close to state-of-the-art is achieved on the smaller MSR\nDaily Activity 3D dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 15:48:30 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 02:04:08 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Baradel", "Fabien", ""], ["Wolf", "Christian", ""], ["Mille", "Julien", ""]]}, {"id": "1703.10114", "submitter": "Nick Johnston", "authors": "Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh\n  Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, George Toderici", "title": "Improved Lossy Image Compression with Priming and Spatially Adaptive Bit\n  Rates for Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for lossy image compression based on recurrent,\nconvolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000,\nand JPEG as measured by MS-SSIM. We introduce three improvements over previous\nresearch that lead to this state-of-the-art result. First, we show that\ntraining with a pixel-wise loss weighted by SSIM increases reconstruction\nquality according to several metrics. Second, we modify the recurrent\narchitecture to improve spatial diffusion, which allows the network to more\neffectively capture and propagate image information through the network's\nhidden state. Finally, in addition to lossless entropy coding, we use a\nspatially adaptive bit allocation algorithm to more efficiently use the limited\nnumber of bits to encode visually complex image regions. We evaluate our method\non the Kodak and Tecnick image sets and compare against standard codecs as well\nrecently published methods based on deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:12:12 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Johnston", "Nick", ""], ["Vincent", "Damien", ""], ["Minnen", "David", ""], ["Covell", "Michele", ""], ["Singh", "Saurabh", ""], ["Chinen", "Troy", ""], ["Hwang", "Sung Jin", ""], ["Shor", "Joel", ""], ["Toderici", "George", ""]]}, {"id": "1703.10125", "submitter": "Mo Shan", "authors": "Mo Shan, Fei Wang, Feng Lin, Zhi Gao, Ya Z. Tang, Ben M. Chen", "title": "Google Map Aided Visual Navigation for UAVs in GPS-denied Environment", "comments": "Published in ROBIO 2015, Zhuhai, China. Fixed a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Google Map aided UAV navigation in GPS-denied\nenvironment. Geo-referenced navigation provides drift-free localization and\ndoes not require loop closures. The UAV position is initialized via\ncorrelation, which is simple and efficient. We then use optical flow to predict\nits position in subsequent frames. During pose tracking, we obtain inter-frame\ntranslation either by motion field or homography decomposition, and we use HOG\nfeatures for registration on Google Map. We employ particle filter to conduct a\ncoarse to fine search to localize the UAV. Offline test using aerial images\ncollected by our quadrotor platform shows promising results as our approach\neliminates the drift in dead-reckoning, and the small localization error\nindicates the superiority of our approach as a supplement to GPS.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:34:25 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Shan", "Mo", ""], ["Wang", "Fei", ""], ["Lin", "Feng", ""], ["Gao", "Zhi", ""], ["Tang", "Ya Z.", ""], ["Chen", "Ben M.", ""]]}, {"id": "1703.10131", "submitter": "Elad Richardson", "authors": "Matan Sela, Elad Richardson, Ron Kimmel", "title": "Unrestricted Facial Geometry Reconstruction Using Image-to-Image\n  Translation", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that neural networks can recover the geometric\nstructure of a face from a single given image. A common denominator of most\nexisting face geometry reconstruction methods is the restriction of the\nsolution space to some low-dimensional subspace. While such a model\nsignificantly simplifies the reconstruction problem, it is inherently limited\nin its expressiveness. As an alternative, we propose an Image-to-Image\ntranslation network that jointly maps the input image to a depth image and a\nfacial correspondence map. This explicit pixel-based mapping can then be\nutilized to provide high quality reconstructions of diverse faces under extreme\nexpressions, using a purely geometric refinement process. In the spirit of\nrecent approaches, the network is trained only with synthetic data, and is then\nevaluated on in-the-wild facial images. Both qualitative and quantitative\nanalyses demonstrate the accuracy and the robustness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:52:14 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 10:33:33 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Sela", "Matan", ""], ["Richardson", "Elad", ""], ["Kimmel", "Ron", ""]]}, {"id": "1703.10155", "submitter": "Jianmin Bao", "authors": "Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua", "title": "CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training", "comments": "to appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present variational generative adversarial networks, a general learning\nframework that combines a variational auto-encoder with a generative\nadversarial network, for synthesizing images in fine-grained categories, such\nas faces of a specific person or objects in a category. Our approach models an\nimage as a composition of label and latent attributes in a probabilistic model.\nBy varying the fine-grained category label fed into the resulting generative\nmodel, we can generate images in a specific category with randomly drawn values\non a latent attribute vector. Our approach has two novel aspects. First, we\nadopt a cross entropy loss for the discriminative and classifier network, but a\nmean discrepancy objective for the generative network. This kind of asymmetric\nloss function makes the GAN training more stable. Second, we adopt an encoder\nnetwork to learn the relationship between the latent space and the real image\nspace, and use pairwise feature matching to keep the structure of generated\nimages. We experiment with natural images of faces, flowers, and birds, and\ndemonstrate that the proposed models are capable of generating realistic and\ndiverse samples with fine-grained category labels. We further show that our\nmodels can be applied to other tasks, such as image inpainting,\nsuper-resolution, and data augmentation for training better face recognition\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 17:49:48 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 15:19:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Bao", "Jianmin", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Li", "Houqiang", ""], ["Hua", "Gang", ""]]}, {"id": "1703.10196", "submitter": "Edward Boyda", "authors": "Edward Boyda, Colin McCormick, and Dan Hammer", "title": "Detecting Human Interventions on the Landscape: KAZE Features, Poisson\n  Point Processes, and a Construction Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm capable of identifying a wide variety of\nhuman-induced change on the surface of the planet by analyzing matches between\nlocal features in time-sequenced remote sensing imagery. We evaluate feature\nsets, match protocols, and the statistical modeling of feature matches. With\napplication of KAZE features, k-nearest-neighbor descriptor matching, and\ngeometric proximity and bi-directional match consistency checks, average match\nrates increase more than two-fold over the previous standard. In testing our\nplatform, we developed a small, labeled benchmark dataset expressing\nlarge-scale residential, industrial, and civic construction, along with null\ninstances, in California between the years 2010 and 2012. On the benchmark set,\nour algorithm makes precise, accurate change proposals on two-thirds of scenes.\nFurther, the detection threshold can be tuned so that all or almost all\nproposed detections are true positives.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 18:56:32 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Boyda", "Edward", ""], ["McCormick", "Colin", ""], ["Hammer", "Dan", ""]]}, {"id": "1703.10200", "submitter": "Jinsong Zhang", "authors": "Jinsong Zhang, Jean-Fran\\c{c}ois Lalonde", "title": "Learning High Dynamic Range from Outdoor Panoramas", "comments": "8 pages + 2 pages of citations, 10 figures. Accepted as an oral paper\n  at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor lighting has extremely high dynamic range. This makes the process of\ncapturing outdoor environment maps notoriously challenging since special\nequipment must be used. In this work, we propose an alternative approach. We\nfirst capture lighting with a regular, LDR omnidirectional camera, and aim to\nrecover the HDR after the fact via a novel, learning-based inverse tonemapping\nmethod. We propose a deep autoencoder framework which regresses linear, high\ndynamic range data from non-linear, saturated, low dynamic range panoramas. We\nvalidate our method through a wide set of experiments on synthetic data, as\nwell as on a novel dataset of real photographs with ground truth. Our approach\nfinds applications in a variety of settings, ranging from outdoor light capture\nto image matching.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 19:10:27 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 14:04:10 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 18:23:11 GMT"}, {"version": "v4", "created": "Tue, 7 Nov 2017 15:13:06 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Zhang", "Jinsong", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1703.10217", "submitter": "Mehmet Solmaz", "authors": "Ali Y. Mutlu, Volkan K{\\i}l{\\i}\\c{c}, Gizem K. \\\"Ozdemir, Abdullah\n  Bayram, Nesrin Horzum, Mehmet E. Solmaz", "title": "Smartphone Based Colorimetric Detection via Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1039/C7AN00741H", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the application of machine learning to smartphone based\ncolorimetric detection of pH values. The strip images were used as the training\nset for Least Squares-Support Vector Machine (LS-SVM) classifier algorithms\nthat were able to successfully classify the distinct pH values. The difference\nin the obtained image formats was found not to significantly affect the\nperformance of the proposed machine learning approach. Moreover, the influence\nof the illumination conditions on the perceived color of pH strips was\ninvestigated and further experiments were carried out to study effect of color\nchange on the learning model. Test results on JPEG, RAW and RAW-corrected image\nformats captured in different lighting conditions lead to perfect\nclassification accuracy, sensitivity and specificity, which proves that the\ncolorimetric detection using machine learning based systems is able to adapt to\nvarious experimental conditions and is a great candidate for smartphone based\nsensing in paper-based colorimetric assays.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 13:38:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mutlu", "Ali Y.", ""], ["K\u0131l\u0131\u00e7", "Volkan", ""], ["\u00d6zdemir", "Gizem K.", ""], ["Bayram", "Abdullah", ""], ["Horzum", "Nesrin", ""], ["Solmaz", "Mehmet E.", ""]]}, {"id": "1703.10239", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Roozbeh Mottaghi, Ali Farhadi", "title": "SeGAN: Segmenting and Generating the Invisible", "comments": "Accepted to CVPR18 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects often occlude each other in scenes; Inferring their appearance beyond\ntheir visible parts plays an important role in scene understanding, depth\nestimation, object interaction and manipulation. In this paper, we study the\nchallenging problem of completing the appearance of occluded objects. Doing so\nrequires knowing which pixels to paint (segmenting the invisible parts of\nobjects) and what color to paint them (generating the invisible parts). Our\nproposed novel solution, SeGAN, jointly optimizes for both segmentation and\ngeneration of the invisible parts of objects. Our experimental results show\nthat: (a) SeGAN can learn to generate the appearance of the occluded parts of\nobjects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the\ninvisible parts of objects; (c) trained on synthetic photo realistic images,\nSeGAN can reliably segment natural images; (d) by reasoning about occluder\noccludee relations, our method can infer depth layering.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 20:34:20 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 03:09:28 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 20:16:33 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Ehsani", "Kiana", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1703.10277", "submitter": "Alireza Fathi", "authors": "Alireza Fathi, Zbigniew Wojna, Vivek Rathod, Peng Wang, Hyun Oh Song,\n  Sergio Guadarrama, Kevin P. Murphy", "title": "Semantic Instance Segmentation via Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for semantic instance segmentation, by first\ncomputing how likely two pixels are to belong to the same object, and then by\ngrouping similar pixels together. Our similarity metric is based on a deep,\nfully convolutional embedding model. Our grouping method is based on selecting\nall points that are sufficiently similar to a set of \"seed points\", chosen from\na deep, fully convolutional scoring model. We show competitive results on the\nPascal VOC instance segmentation benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 00:39:21 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fathi", "Alireza", ""], ["Wojna", "Zbigniew", ""], ["Rathod", "Vivek", ""], ["Wang", "Peng", ""], ["Song", "Hyun Oh", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin P.", ""]]}, {"id": "1703.10295", "submitter": "Lachlan Tychsen-Smith", "authors": "Lachlan Tychsen-Smith and Lars Petersson", "title": "DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling", "comments": "8 pages, ICCV2017 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the object detection from imagery problem as estimating a very\nlarge but extremely sparse bounding box dependent probability distribution.\nSubsequently we identify a sparse distribution estimation scheme, Directed\nSparse Sampling, and employ it in a single end-to-end CNN based detection\nmodel. This methodology extends and formalizes previous state-of-the-art\ndetection models with an additional emphasis on high evaluation rates and\nreduced manual engineering. We introduce two novelties, a corner based\nregion-of-interest estimator and a deconvolution based CNN model. The resulting\nmodel is scene adaptive, does not require manually defined reference bounding\nboxes and produces highly competitive results on MSCOCO, Pascal VOC 2007 and\nPascal VOC 2012 with real-time evaluation rates. Further analysis suggests our\nmodel performs particularly well when finegrained object localization is\ndesirable. We argue that this advantage stems from the significantly larger set\nof available regions-of-interest relative to other methods. Source-code is\navailable from: https://github.com/lachlants/denet\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 02:50:54 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 02:20:02 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 02:46:05 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Tychsen-Smith", "Lachlan", ""], ["Petersson", "Lars", ""]]}, {"id": "1703.10304", "submitter": "Lei Fan", "authors": "Lei Fan, Ziyu Pan, Long Chen and Kai Huang", "title": "Planecell: Representing the 3D Space with Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction based on the stereo camera has received considerable attention\nrecently, but two particular challenges still remain. The first concerns the\nneed to aggregate similar pixels in an effective approach, and the second is to\nmaintain as much of the available information as possible while ensuring\nsufficient accuracy. To overcome these issues, we propose a new 3D\nrepresentation method, namely, planecell, that extracts planarity from the\ndepth-assisted image segmentation and then projects these depth planes into the\n3D world. An energy function formulated from Conditional Random Field that\ngeneralizes the planar relationships is maximized to merge coplanar segments.\nWe evaluate our method with a variety of reconstruction baselines on both KITTI\nand Middlebury datasets, and the results indicate the superiorities compared to\nother 3D space representation methods in accuracy, memory requirements and\nfurther applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 03:58:05 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fan", "Lei", ""], ["Pan", "Ziyu", ""], ["Chen", "Long", ""], ["Huang", "Kai", ""]]}, {"id": "1703.10332", "submitter": "Zhichao Li", "authors": "Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, Wei Xu", "title": "Dynamic Computational Time for Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic computational time model to accelerate the average\nprocessing time for recurrent visual attention (RAM). Rather than attention\nwith a fixed number of steps for each input image, the model learns to decide\nwhen to stop on the fly. To achieve this, we add an additional continue/stop\naction per time step to RAM and use reinforcement learning to learn both the\noptimal attention policy and stopping policy. The modification is simple but\ncould dramatically save the average computational time while keeping the same\nrecognition performance as RAM. Experimental results on CUB-200-2011 and\nStanford Cars dataset demonstrate the dynamic computational model can work\neffectively for fine-grained image recognition.The source code of this paper\ncan be obtained from https://github.com/baidu-research/DT-RAM\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 06:55:02 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 02:27:05 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 00:59:49 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Li", "Zhichao", ""], ["Yang", "Yi", ""], ["Liu", "Xiao", ""], ["Zhou", "Feng", ""], ["Wen", "Shilei", ""], ["Xu", "Wei", ""]]}, {"id": "1703.10476", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz,\n  Bernt Schiele", "title": "Speaking the Same Language: Matching Machine to Human Captions by\n  Adversarial Training", "comments": "16 pages, Published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While strong progress has been made in image captioning over the last years,\nmachine and human captions are still quite distinct. A closer look reveals that\nthis is due to the deficiencies in the generated word distribution, vocabulary\nsize, and strong bias in the generators towards frequent captions. Furthermore,\nhumans -- rightfully so -- generate multiple, diverse captions, due to the\ninherent ambiguity in the captioning task which is not considered in today's\nsystems.\n  To address these challenges, we change the training objective of the caption\ngenerator from reproducing groundtruth captions to generating a set of captions\nthat is indistinguishable from human generated captions. Instead of\nhandcrafting such a learning target, we employ adversarial training in\ncombination with an approximate Gumbel sampler to implicitly match the\ngenerated distribution to the human one. While our method achieves comparable\nperformance to the state-of-the-art in terms of the correctness of the\ncaptions, we generate a set of diverse captions, that are significantly less\nbiased and match the word statistics better in several aspects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 13:54:51 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 15:43:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Shetty", "Rakshith", ""], ["Rohrbach", "Marcus", ""], ["Hendricks", "Lisa Anne", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1703.10480", "submitter": "Jose Dolz", "authors": "Jose Dolz, Nicolas Reyns, Nacim Betrouni, Dris Kharroubi, Mathilde\n  Quidet, Laurent Massoptier, Maximilien Vermandel", "title": "A deep learning classification scheme based on augmented-enhanced\n  features to segment organs at risk on the optic region in brain cancer\n  patients", "comments": "Submitted to the Journal of Physics in Biology and Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiation therapy has emerged as one of the preferred techniques to treat\nbrain cancer patients. During treatment, a very high dose of radiation is\ndelivered to a very narrow area. Prescribed radiation therapy for brain cancer\nrequires precisely defining the target treatment area, as well as delineating\nvital brain structures which must be spared from radiotoxicity. Nevertheless,\ndelineation task is usually still manually performed, which is inefficient and\noperator-dependent. Several attempts of automatizing this process have\nreported. however, marginal results when analyzing organs in the optic region.\nIn this work we present a deep learning classification scheme based on\naugmented-enhanced features to automatically segment organs at risk (OARs) in\nthe optic region -optic nerves, optic chiasm, pituitary gland and pituitary\nstalk-. Fifteen MR images with various types of brain tumors were\nretrospectively collected to undergo manual and automatic segmentation. Mean\nDice Similarity coefficients around 0.80 were reported. Incorporation of\nproposed features yielded to improvements on the segmentation. Compared with\nsupport vector machines, our method achieved better performance with less\nvariation on the results, as well as a considerably reduction on the\nclassification time. Performance of the proposed approach was also evaluated\nwith respect to manual contours. In this case, results obtained from the\nautomatic contours mostly lie on the variability of the observers, showing no\nsignificant differences with respect to them. These results suggest therefore\nthat the proposed system is more accurate than other presented approaches, up\nto date, to segment these structures. The speed, reproducibility, and\nrobustness of the process make the proposed deep learning-based classification\nsystem a valuable tool for assisting in the delineation task of small OARs in\nbrain cancer.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 14:09:53 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 12:34:07 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Dolz", "Jose", ""], ["Reyns", "Nicolas", ""], ["Betrouni", "Nacim", ""], ["Kharroubi", "Dris", ""], ["Quidet", "Mathilde", ""], ["Massoptier", "Laurent", ""], ["Vermandel", "Maximilien", ""]]}, {"id": "1703.10501", "submitter": "Grigorios Kalliatakis M.A.", "authors": "Grigorios Kalliatakis, Shoaib Ehsan, and Klaus D. McDonald-Maier", "title": "A Paradigm Shift: Detecting Human Rights Violations Through Web Images", "comments": "Position paper, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing presence of devices carrying digital cameras, such as mobile\nphones and tablets, combined with ever improving internet networks have enabled\nordinary citizens, victims of human rights abuse, and participants in armed\nconflicts, protests, and disaster situations to capture and share via social\nmedia networks images and videos of specific events. This paper discusses the\npotential of images in human rights context including the opportunities and\nchallenges they present. This study demonstrates that real-world images have\nthe capacity to contribute complementary data to operational human rights\nmonitoring efforts when combined with novel computer vision approaches. The\nanalysis is concluded by arguing that if images are to be used effectively to\ndetect and identify human rights violations by rights advocates, greater\nattention to gathering task-specific visual concepts from large-scale web\nimages is required.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 14:53:55 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Ehsan", "Shoaib", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1703.10530", "submitter": "Hossam Isack", "authors": "Hossam Isack, Olga Veksler, Ipek Oguz, Milan Sonka and Yuri Boykov", "title": "Efficient optimization for Hierarchically-structured Interacting\n  Segments (HINTS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective optimization algorithm for a general hierarchical\nsegmentation model with geometric interactions between segments. Any given tree\ncan specify a partial order over object labels defining a hierarchy. It is\nwell-established that segment interactions, such as inclusion/exclusion and\nmargin constraints, make the model significantly more discriminant. However,\nexisting optimization methods do not allow full use of such models. Generic\n-expansion results in weak local minima, while common binary multi-layered\nformulations lead to non-submodularity, complex high-order potentials, or polar\ndomain unwrapping and shape biases. In practice, applying these methods to\narbitrary trees does not work except for simple cases. Our main contribution is\nan optimization method for the Hierarchically-structured Interacting Segments\n(HINTS) model with arbitrary trees. Our Path-Moves algorithm is based on\nmulti-label MRF formulation and can be seen as a combination of well-known\na-expansion and Ishikawa techniques. We show state-of-the-art biomedical\nsegmentation for many diverse examples of complex trees.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 15:32:29 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Isack", "Hossam", ""], ["Veksler", "Olga", ""], ["Oguz", "Ipek", ""], ["Sonka", "Milan", ""], ["Boykov", "Yuri", ""]]}, {"id": "1703.10553", "submitter": "Mu Li", "authors": "Mu Li, Wangmeng Zuo, Shuhang Gu, Debin Zhao, David Zhang", "title": "Learning Convolutional Networks for Content-weighted Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression is generally formulated as a joint rate-distortion\noptimization to learn encoder, quantizer, and decoder. However, the quantizer\nis non-differentiable, and discrete entropy estimation usually is required for\nrate control. These make it very challenging to develop a convolutional network\n(CNN)-based image compression system. In this paper, motivated by that the\nlocal information content is spatially variant in an image, we suggest that the\nbit rate of the different parts of the image should be adapted to local\ncontent. And the content aware bit rate is allocated under the guidance of a\ncontent-weighted importance map. Thus, the sum of the importance map can serve\nas a continuous alternative of discrete entropy estimation to control\ncompression rate. And binarizer is adopted to quantize the output of encoder\ndue to the binarization scheme is also directly defined by the importance map.\nFurthermore, a proxy function is introduced for binary operation in backward\npropagation to make it differentiable. Therefore, the encoder, decoder,\nbinarizer and importance map can be jointly optimized in an end-to-end manner\nby using a subset of the ImageNet database. In low bit rate image compression,\nexperiments show that our system significantly outperforms JPEG and JPEG 2000\nby structural similarity (SSIM) index, and can produce the much better visual\nresult with sharp edges, rich textures, and fewer artifacts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 16:21:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:23:26 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Li", "Mu", ""], ["Zuo", "Wangmeng", ""], ["Gu", "Shuhang", ""], ["Zhao", "Debin", ""], ["Zhang", "David", ""]]}, {"id": "1703.10571", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov and Robert Ross and John Kelleher", "title": "Bootstrapping Labelled Dataset Construction for Cow Tracking and\n  Behavior Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to the long-term tracking of an object\nin a challenging environment. The object is a cow and the environment is an\nenclosure in a cowshed. Some of the key challenges in this domain are a\ncluttered background, low contrast and high similarity between moving objects\nwhich greatly reduces the efficiency of most existing approaches, including\nthose based on background subtraction. Our approach is split into object\nlocalization, instance segmentation, learning and tracking stages. Our solution\nis compared to a range of semi-supervised object tracking algorithms and we\nshow that the performance is strong and well suited to subsequent analysis. We\npresent our solution as a first step towards broader tracking and behavior\nmonitoring for cows in precision agriculture with the ultimate objective of\nearly detection of lameness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:09:39 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Ter-Sarkisov", "Aram", ""], ["Ross", "Robert", ""], ["Kelleher", "John", ""]]}, {"id": "1703.10580", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Michael Zollh\\\"ofer, Hyeongwoo Kim, Pablo Garrido,\n  Florian Bernard, Patrick P\\'erez, Christian Theobalt", "title": "MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised\n  Monocular Reconstruction", "comments": "International Conference on Computer Vision (ICCV) 2017 (Oral), 13\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel model-based deep convolutional autoencoder\nthat addresses the highly challenging problem of reconstructing a 3D human face\nfrom a single in-the-wild color image. To this end, we combine a convolutional\nencoder network with an expert-designed generative model that serves as\ndecoder. The core innovation is our new differentiable parametric decoder that\nencapsulates image formation analytically based on a generative model. Our\ndecoder takes as input a code vector with exactly defined semantic meaning that\nencodes detailed face pose, shape, expression, skin reflectance and scene\nillumination. Due to this new way of combining CNN-based with model-based face\nreconstruction, the CNN-based encoder learns to extract semantically meaningful\nparameters from a single monocular input image. For the first time, a CNN\nencoder and an expert-designed generative model can be trained end-to-end in an\nunsupervised manner, which renders training on very large (unlabeled) real\nworld data feasible. The obtained reconstructions compare favorably to current\nstate-of-the-art approaches in terms of quality and richness of representation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:29:42 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 20:38:13 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Kim", "Hyeongwoo", ""], ["Garrido", "Pablo", ""], ["Bernard", "Florian", ""], ["P\u00e9rez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "1703.10584", "submitter": "Eduardo Ruiz", "authors": "Eduardo Ruiz, Walterio Mayol-Cuevas", "title": "Geometric Affordances from a Single Example via the Interaction Tensor", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops and evaluates a new tensor field representation to\nexpress the geometric affordance of one object over another. We expand the well\nknown bisector surface representation to one that is weight-driven and that\nretains the provenance of surface points with directional vectors. We also\nincorporate the notion of affordance keypoints which allow for faster decisions\nat a point of query and with a compact and straightforward descriptor. Using a\nsingle interaction example, we are able to generalize to previously-unseen\nscenarios; both synthetic and also real scenes captured with RGBD sensors. We\nshow how our interaction tensor allows for significantly better performance\nover alternative formulations. Evaluations also include crowdsourcing\ncomparisons that confirm the validity of our affordance proposals, which agree\non average 84% of the time with human judgments, and which is 20-40% better\nthan the baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:32:02 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Ruiz", "Eduardo", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1703.10593", "submitter": "Jun-Yan Zhu", "authors": "Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros", "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial\n  Networks", "comments": "An extended version of our ICCV 2017 paper, v7 fixed the typos and\n  updated the implementation details. Code and data:\n  https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:44:17 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 01:19:36 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 01:37:05 GMT"}, {"version": "v4", "created": "Mon, 19 Feb 2018 06:27:55 GMT"}, {"version": "v5", "created": "Thu, 30 Aug 2018 06:48:43 GMT"}, {"version": "v6", "created": "Thu, 15 Nov 2018 14:38:20 GMT"}, {"version": "v7", "created": "Mon, 24 Aug 2020 16:51:03 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Park", "Taesung", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1703.10631", "submitter": "Jinkyu Kim", "authors": "Jinkyu Kim and John Canny", "title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural perception and control networks are likely to be a key component\nof self-driving vehicles. These models need to be explainable - they should\nprovide easy-to-interpret rationales for their behavior - so that passengers,\ninsurance companies, law enforcement, developers etc., can understand what\ntriggered a particular behavior. Here we explore the use of visual\nexplanations. These explanations take the form of real-time highlighted regions\nof an image that causally influence the network's output (steering control).\nOur approach is two-stage. In the first stage, we use a visual attention model\nto train a convolution network end-to-end from images to steering angle. The\nattention model highlights image regions that potentially influence the\nnetwork's output. Some of these are true influences, but some are spurious. We\nthen apply a causal filtering step to determine which input regions actually\ninfluence the output. This produces more succinct visual explanations and more\naccurately exposes the network's behavior. We demonstrate the effectiveness of\nour model on three datasets totaling 16 hours of driving. We first show that\ntraining with attention does not degrade the performance of the end-to-end\nnetwork. Then we show that the network causally cues on a variety of features\nthat are used by humans while driving.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 18:37:49 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Kim", "Jinkyu", ""], ["Canny", "John", ""]]}, {"id": "1703.10645", "submitter": "Igor Fedorov", "authors": "Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen", "title": "Relevance Subject Machine: A Novel Person Re-identification Framework", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 19:21:55 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Fedorov", "Igor", ""], ["Giri", "Ritwik", ""], ["Rao", "Bhaskar D.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1703.10660", "submitter": "Tribhuvanesh Orekondy", "authors": "Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz", "title": "Towards a Visual Privacy Advisor: Understanding and Predicting Privacy\n  Risks in Images", "comments": "ICCV 2017. Project page: https://tribhuvanesh.github.io/vpa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing number of users sharing information online, privacy\nimplications entailing such actions are a major concern. For explicit content,\nsuch as user profile or GPS data, devices (e.g. mobile phones) as well as web\nservices (e.g. Facebook) offer to set privacy settings in order to enforce the\nusers' privacy preferences. We propose the first approach that extends this\nconcept to image content in the spirit of a Visual Privacy Advisor. First, we\ncategorize personal information in images into 68 image attributes and collect\na dataset, which allows us to train models that predict such information\ndirectly from images. Second, we run a user study to understand the privacy\npreferences of different users w.r.t. such attributes. Third, we propose models\nthat predict user specific privacy score from images in order to enforce the\nusers' privacy preferences. Our model is trained to predict the user specific\nprivacy risk and even outperforms the judgment of the users, who often fail to\nfollow their own privacy preferences on image data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 20:18:08 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 08:35:35 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Orekondy", "Tribhuvanesh", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1703.10664", "submitter": "Chen Chen", "authors": "Rui Hou, Chen Chen, Mubarak Shah", "title": "Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been demonstrated to achieve excellent results for image\nclassification and object detection. However, the impact of deep learning on\nvideo analysis (e.g. action detection and recognition) has been limited due to\ncomplexity of video data and lack of annotations. Previous convolutional neural\nnetworks (CNN) based video action detection approaches usually consist of two\nmajor steps: frame-level action proposal detection and association of proposals\nacross frames. Also, these methods employ two-stream CNN framework to handle\nspatial and temporal feature separately. In this paper, we propose an\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\naction detection in videos. The proposed architecture is a unified network that\nis able to recognize and localize action based on 3D convolution features. A\nvideo is first divided into equal length clips and for each clip a set of tube\nproposals are generated next based on 3D Convolutional Network (ConvNet)\nfeatures. Finally, the tube proposals of different clips are linked together\nemploying network flow and spatio-temporal action detection is performed using\nthese linked video proposals. Extensive experiments on several video datasets\ndemonstrate the superior performance of T-CNN for classifying and localizing\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 20:28:31 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 01:32:43 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 04:52:02 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Hou", "Rui", ""], ["Chen", "Chen", ""], ["Shah", "Mubarak", ""]]}, {"id": "1703.10667", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Min-Hung Chen, Zsolt Kira, Ghassan AlRegib", "title": "TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for\n  Activity Recognition", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent two-stream deep Convolutional Neural Networks (ConvNets) have made\nsignificant progress in recognizing human actions in videos. Despite their\nsuccess, methods extending the basic two-stream ConvNet have not systematically\nexplored possible network architectures to further exploit spatiotemporal\ndynamics within video sequences. Further, such networks often use different\nbaseline two-stream networks. Therefore, the differences and the distinguishing\nfactors between various methods using Recurrent Neural Networks (RNN) or\nconvolutional networks on temporally-constructed feature vectors\n(Temporal-ConvNet) are unclear. In this work, we first demonstrate a strong\nbaseline two-stream ConvNet using ResNet-101. We use this baseline to\nthoroughly examine the use of both RNNs and Temporal-ConvNets for extracting\nspatiotemporal information. Building upon our experimental results, we then\npropose and investigate two different networks to further integrate\nspatiotemporal information: 1) temporal segment RNN and 2) Inception-style\nTemporal-ConvNet. We demonstrate that using both RNNs (using LSTMs) and\nTemporal-ConvNets on spatiotemporal feature matrices are able to exploit\nspatiotemporal dynamics to improve the overall performance. However, each of\nthese methods require proper care to achieve state-of-the-art performance; for\nexample, LSTMs require pre-segmented data or else they cannot fully exploit\ntemporal information. Our analysis identifies specific limitations for each\nmethod that could form the basis of future work. Our experimental results on\nUCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and\n69.0%, respectively, without requiring extensive temporal augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 20:45:00 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Chen", "Min-Hung", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1703.10701", "submitter": "Nicola Rieke", "authors": "Iro Laina, Nicola Rieke, Christian Rupprecht, Josu\\'e Page Vizca\\'ino,\n  Abouzar Eslami, Federico Tombari, and Nassir Navab", "title": "Concurrent Segmentation and Localization for Tracking of Surgical\n  Instruments", "comments": "I. Laina and N. Rieke contributed equally to this work. Accepted to\n  MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time instrument tracking is a crucial requirement for various\ncomputer-assisted interventions. In order to overcome problems such as specular\nreflections and motion blur, we propose a novel method that takes advantage of\nthe interdependency between localization and segmentation of the surgical tool.\nIn particular, we reformulate the 2D instrument pose estimation as heatmap\nregression and thereby enable a concurrent, robust and near real-time\nregression of both tasks via deep learning. As demonstrated by our experimental\nresults, this modeling leads to a significantly improved performance than\ndirectly regressing the tool position and allows our method to outperform the\nstate of the art on a Retinal Microsurgery benchmark and the MICCAI EndoVis\nChallenge 2015.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 22:37:38 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 13:05:21 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Laina", "Iro", ""], ["Rieke", "Nicola", ""], ["Rupprecht", "Christian", ""], ["Vizca\u00edno", "Josu\u00e9 Page", ""], ["Eslami", "Abouzar", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""]]}, {"id": "1703.10714", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Matthias Hernandez, Jongmoo Choi, Gerard Medioni", "title": "Deep 3D Face Identification", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D face recognition algorithm using a deep convolutional\nneural network (DCNN) and a 3D augmentation technique. The performance of 2D\nface recognition algorithms has significantly increased by leveraging the\nrepresentational power of deep neural networks and the use of large-scale\nlabeled training data. As opposed to 2D face recognition, training\ndiscriminative deep features for 3D face recognition is very difficult due to\nthe lack of large-scale 3D face datasets. In this paper, we show that transfer\nlearning from a CNN trained on 2D face images can effectively work for 3D face\nrecognition by fine-tuning the CNN with a relatively small number of 3D facial\nscans. We also propose a 3D face augmentation technique which synthesizes a\nnumber of different facial expressions from a single 3D face scan. Our proposed\nmethod shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC\ndatasets, without using hand-crafted features. The 3D identification using our\ndeep features also scales well for large databases.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 23:49:23 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Kim", "Donghyun", ""], ["Hernandez", "Matthias", ""], ["Choi", "Jongmoo", ""], ["Medioni", "Gerard", ""]]}, {"id": "1703.10729", "submitter": "Qixing Zhang", "authors": "Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Jinjun Wang", "title": "Deep Domain Adaptation Based Video Smoke Detection using Synthetic Smoke\n  Images", "comments": "The manuscript approved by all authors is our original work, and has\n  submitted to Fire Safety Journal for peer review previously. There are 4516\n  words, 8 figures and 2 tables in this manuscript", "journal-ref": "Fire Safety Journal 93C (2017) pp. 53-59", "doi": "10.1016/j.firesaf.2017.08.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a deep domain adaptation based method for video smoke\ndetection is proposed to extract a powerful feature representation of smoke.\nDue to the smoke image samples limited in scale and diversity for deep CNN\ntraining, we systematically produced adequate synthetic smoke images with a\nwide variation in the smoke shape, background and lighting conditions.\nConsidering that the appearance gap (dataset bias) between synthetic and real\nsmoke images degrades significantly the performance of the trained model on the\ntest set composed fully of real images, we build deep architectures based on\ndomain adaptation to confuse the distributions of features extracted from\nsynthetic and real smoke images. This approach expands the domain-invariant\nfeature space for smoke image samples. With their approximate feature\ndistribution off non-smoke images, the recognition rate of the trained model is\nimproved significantly compared to the model trained directly on mixed dataset\nof synthetic and real images. Experimentally, several deep architectures with\ndifferent design choices are applied to the smoke detector. The ultimate\nframework can get a satisfactory result on the test set. We believe that our\napproach is a start in the direction of utilizing deep neural networks enhanced\nwith synthetic smoke images for video smoke detection.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 01:42:46 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Xu", "Gao", ""], ["Zhang", "Yongming", ""], ["Zhang", "Qixing", ""], ["Lin", "Gaohua", ""], ["Wang", "Jinjun", ""]]}, {"id": "1703.10730", "submitter": "Donghoon Lee", "authors": "Donghoon Lee, Sangdoo Yun, Sungjoon Choi, Hwiyeon Yoo, Ming-Hsuan\n  Yang, and Songhwai Oh", "title": "Unsupervised Holistic Image Generation from Key Local Patches", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem of generating an image based on a small number of\nkey local patches without any geometric prior. In this work, key local patches\nare defined as informative regions of the target object or scene. This is a\nchallenging problem since it requires generating realistic images and\npredicting locations of parts at the same time. We construct adversarial\nnetworks to tackle this problem. A generator network generates a fake image as\nwell as a mask based on the encoder-decoder framework. On the other hand, a\ndiscriminator network aims to detect fake images. The network is trained with\nthree losses to consider spatial, appearance, and adversarial information. The\nspatial loss determines whether the locations of predicted parts are correct.\nInput patches are restored in the output image without much modification due to\nthe appearance loss. The adversarial loss ensures output images are realistic.\nThe proposed network is trained without supervisory signals since no labels of\nkey parts are required. Experimental results on six datasets demonstrate that\nthe proposed algorithm performs favorably on challenging objects and scenes.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 01:43:06 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 00:38:12 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Lee", "Donghoon", ""], ["Yun", "Sangdoo", ""], ["Choi", "Sungjoon", ""], ["Yoo", "Hwiyeon", ""], ["Yang", "Ming-Hsuan", ""], ["Oh", "Songhwai", ""]]}, {"id": "1703.10756", "submitter": "Lalith Srikanth Chintalapati", "authors": "Lalith Srikanth Chintalapati, Raghunatha Sarma Rachakonda", "title": "Novel Framework for Spectral Clustering using Topological Node\n  Features(TNF)", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has gained importance in recent years due to its ability\nto cluster complex data as it requires only pairwise similarity among data\npoints with its ease of implementation. The central point in spectral\nclustering is the process of capturing pair-wise similarity. In the literature,\nmany research techniques have been proposed for effective construction of\naffinity matrix with suitable pair- wise similarity. In this paper a general\nframework for capturing pairwise affinity using local features such as density,\nproximity and structural similarity is been proposed. Topological Node Features\nare exploited to define the notion of density and local structure. These local\nfeatures are incorporated into the construction of the affinity matrix.\nExperimental results, on widely used datasets such as synthetic shape datasets,\nUCI real datasets and MNIST handwritten datasets show that the proposed\nframework outperforms standard spectral clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 04:56:46 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 17:00:22 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Chintalapati", "Lalith Srikanth", ""], ["Rachakonda", "Raghunatha Sarma", ""]]}, {"id": "1703.10757", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Jianbo Yang", "title": "Diabetic Retinopathy Detection via Deep Convolutional Networks for\n  Discriminative Localization and Visual Explanation", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a deep learning method for interpretable diabetic retinopathy\n(DR) detection. The visual-interpretable feature of the proposed method is\nachieved by adding the regression activation map (RAM) after the global\naveraging pooling layer of the convolutional networks (CNN). With RAM, the\nproposed model can localize the discriminative regions of an retina image to\nshow the specific region of interest in terms of its severity level. We believe\nthis advantage of the proposed deep learning model is highly desired for DR\ndetection because in practice, users are not only interested with high\nprediction performance, but also keen to understand the insights of DR\ndetection and why the adopted learning model works. In the experiments\nconducted on a large scale of retina image dataset, we show that the proposed\nCNN model can achieve high performance on DR detection compared with the\nstate-of-the-art while achieving the merits of providing the RAM to highlight\nthe salient regions of the input image.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 05:10:56 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 16:44:23 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 22:05:07 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wang", "Zhiguang", ""], ["Yang", "Jianbo", ""]]}, {"id": "1703.10764", "submitter": "Min Yang", "authors": "Min Yang, Yuwei Wu, and Yunde Jia", "title": "A Hybrid Data Association Framework for Robust Online Multi-Object\n  Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2745103", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global optimization algorithms have shown impressive performance in\ndata-association based multi-object tracking, but handling online data remains\na difficult hurdle to overcome. In this paper, we present a hybrid data\nassociation framework with a min-cost multi-commodity network flow for robust\nonline multi-object tracking. We build local target-specific models interleaved\nwith global optimization of the optimal data association over multiple video\nframes. More specifically, in the min-cost multi-commodity network flow, the\ntarget-specific similarities are online learned to enforce the local\nconsistency for reducing the complexity of the global data association.\nMeanwhile, the global data association taking multiple video frames into\naccount alleviates irrecoverable errors caused by the local data association\nbetween adjacent frames. To ensure the efficiency of online tracking, we give\nan efficient near-optimal solution to the proposed min-cost multi-commodity\nflow problem, and provide the empirical proof of its sub-optimality. The\ncomprehensive experiments on real data demonstrate the superior tracking\nperformance of our approach in various challenging situations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 05:49:33 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Min", ""], ["Wu", "Yuwei", ""], ["Jia", "Yunde", ""]]}, {"id": "1703.10798", "submitter": "Wei-Sheng Lai", "authors": "Wei-Sheng Lai, Yujia Huang, Neel Joshi, Chris Buehler, Ming-Hsuan Yang\n  and Sing Bing Kang", "title": "Semantic-driven Generation of Hyperlapse from $360^\\circ$ Video", "comments": "This work is accepted in Transactions on Visualization and Computer\n  Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 08:42:00 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 04:34:12 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 01:19:07 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 00:05:17 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lai", "Wei-Sheng", ""], ["Huang", "Yujia", ""], ["Joshi", "Neel", ""], ["Buehler", "Chris", ""], ["Yang", "Ming-Hsuan", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1703.10818", "submitter": "Liying Chi", "authors": "Liying Chi, Hongxin Zhang and Mingxiu Chen", "title": "End-To-End Face Detection and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of face detection and recognition methods have been proposed and got\ndelightful results in decades. Common face recognition pipeline consists of: 1)\nface detection, 2) face alignment, 3) feature extraction, 4) similarity\ncalculation, which are separated and independent from each other. The separated\nface analyzing stages lead the model redundant calculation and are hard for\nend-to-end training. In this paper, we proposed a novel end-to-end trainable\nconvolutional network framework for face detection and recognition, in which a\ngeometric transformation matrix was directly learned to align the faces,\ninstead of predicting the facial landmarks. In training stage, our single CNN\nmodel is supervised only by face bounding boxes and personal identities, which\nare publicly available from WIDER FACE \\cite{Yang2016} dataset and\nCASIA-WebFace \\cite{Yi2014} dataset. Tested on Face Detection Dataset and\nBenchmark (FDDB) \\cite{Jain2010} dataset and Labeled Face in the Wild (LFW)\n\\cite{Huang2007} dataset, we have achieved 89.24\\% recall for face detection\ntask and 98.63\\% verification accuracy for face recognition task\nsimultaneously, which are comparable to state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 09:48:32 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Chi", "Liying", ""], ["Zhang", "Hongxin", ""], ["Chen", "Mingxiu", ""]]}, {"id": "1703.10881", "submitter": "Fabio Maria Carlucci", "authors": "F. M. Carlucci, P. Russo and B. Caputo", "title": "(DE)^2 CO: Deep Depth Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to classify objects is fundamental for robots. Besides knowledge\nabout their visual appearance, captured by the RGB channel, robots heavily need\nalso depth information to make sense of the world. While the use of deep\nnetworks on RGB robot images has benefited from the plethora of results\nobtained on databases like ImageNet, using convnets on depth images requires\nmapping them into three dimensional channels. This transfer learning procedure\nmakes them processable by pre-trained deep architectures. Current mappings are\nbased on heuristic assumptions over preprocessing steps and on what depth\nproperties should be most preserved, resulting often in cumbersome data\nvisualizations, and in sub-optimal performance in terms of generality and\nrecognition results. Here we take an alternative route and we attempt instead\nto learn an optimal colorization mapping for any given pre-trained\narchitecture, using as training data a reference RGB-D database. We propose a\ndeep network architecture, exploiting the residual paradigm, that learns how to\nmap depth data to three channel images. A qualitative analysis of the images\nobtained with this approach clearly indicates that learning the optimal mapping\npreserves the richness of depth information better than current hand-crafted\napproaches. Experiments on the Washington, JHUIT-50 and BigBIRD public\nbenchmark databases, using CaffeNet, VGG16, GoogleNet, and ResNet50 clearly\nshowcase the power of our approach, with gains in performance of up to 16%\ncompared to state of the art competitors on the depth channel only, leading to\ntop performances when dealing with RGB-D data\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 12:30:30 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 22:04:47 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 14:06:03 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Carlucci", "F. M.", ""], ["Russo", "P.", ""], ["Caputo", "B.", ""]]}, {"id": "1703.10889", "submitter": "Yudong Liang", "authors": "Yudong Liang, Radu Timofte, Jinjun Wang, Yihong Gong and Nanning Zheng", "title": "Single Image Super Resolution - When Model Adaptation Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years impressive advances were made for single image\nsuper-resolution. Deep learning is behind a big part of this success. Deep(er)\narchitecture design and external priors modeling are the key ingredients. The\ninternal contents of the low resolution input image is neglected with deep\nmodeling despite the earlier works showing the power of using such internal\npriors. In this paper we propose a novel deep convolutional neural network\ncarefully designed for robustness and efficiency at both learning and testing.\nMoreover, we propose a couple of model adaptation strategies to the internal\ncontents of the low resolution input image and analyze their strong points and\nweaknesses. By trading runtime and using internal priors we achieve 0.1 up to\n0.3dB PSNR improvements over best reported results on standard datasets. Our\nadaptation especially favors images with repetitive structures or under large\nresolutions. Moreover, it can be combined with other simple techniques, such as\nback-projection or enhanced prediction, for further improvements.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 13:20:19 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Liang", "Yudong", ""], ["Timofte", "Radu", ""], ["Wang", "Jinjun", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1703.10896", "submitter": "Mahdi Rad", "authors": "Mahdi Rad, Vincent Lepetit", "title": "BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for\n  Predicting the 3D Poses of Challenging Objects without Using Depth", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for 3D object detection and pose estimation from\ncolor images only. We first use segmentation to detect the objects of interest\nin 2D even in presence of partial occlusions and cluttered background. By\ncontrast with recent patch-based methods, we rely on a \"holistic\" approach: We\napply to the detected objects a Convolutional Neural Network (CNN) trained to\npredict their 3D poses in the form of 2D projections of the corners of their 3D\nbounding boxes. This, however, is not sufficient for handling objects from the\nrecent T-LESS dataset: These objects exhibit an axis of rotational symmetry,\nand the similarity of two images of such an object under two different poses\nmakes training the CNN challenging. We solve this problem by restricting the\nrange of poses used for training, and by introducing a classifier to identify\nthe range of a pose at run-time before estimating it. We also use an optional\nadditional step that refines the predicted poses. We improve the\nstate-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly\nregistered RGB frames. We are also the first to report results on the Occlusion\ndataset using color images only. We obtain 54% of frames passing the Pose 6D\ncriterion on average on several sequences of the T-LESS dataset, compared to\nthe 67% of the state-of-the-art on the same sequences which uses both color and\ndepth. The full approach is also scalable, as a single network can be trained\nfor multiple objects simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 13:56:35 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 16:08:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rad", "Mahdi", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1703.10898", "submitter": "Jie Song", "authors": "Jie Song, Limin Wang, Luc Van Gool, Otmar Hilliges", "title": "Thin-Slicing Network: A Deep Structured Model for Pose Estimation in\n  Videos", "comments": "Preliminary version to appear in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep ConvNets have been shown to be effective for the task of human pose\nestimation from single images. However, several challenging issues arise in the\nvideo-based case such as self-occlusion, motion blur, and uncommon poses with\nfew or no examples in training data sets. Temporal information can provide\nadditional cues about the location of body joints and help to alleviate these\nissues. In this paper, we propose a deep structured model to estimate a\nsequence of human poses in unconstrained videos. This model can be efficiently\ntrained in an end-to-end manner and is capable of representing appearance of\nbody joints and their spatio-temporal relationships simultaneously. Domain\nknowledge about the human body is explicitly incorporated into the network\nproviding effective priors to regularize the skeletal structure and to enforce\ntemporal consistency. The proposed end-to-end architecture is evaluated on two\nwidely used benchmarks (Penn Action dataset and JHMDB dataset) for video-based\npose estimation. Our approach significantly outperforms the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 13:59:31 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Song", "Jie", ""], ["Wang", "Limin", ""], ["Van Gool", "Luc", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1703.10901", "submitter": "Simion-Vlad Bogolin", "authors": "Ioana Croitoru (1), Simion-Vlad Bogolin (1), Marius Leordeanu (1 and\n  2) ((1) Institute of Mathematics of the Romanian Academy, (2) University\n  \"Politehnica\" of Bucharest)", "title": "Unsupervised learning from video to detect foreground objects in single\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning from visual data is one of the most difficult\nchallenges in computer vision, being a fundamental task for understanding how\nvisual recognition works. From a practical point of view, learning from\nunsupervised visual input has an immense practical value, as very large\nquantities of unlabeled videos can be collected at low cost. In this paper, we\naddress the task of unsupervised learning to detect and segment foreground\nobjects in single images. We achieve our goal by training a student pathway,\nconsisting of a deep neural network. It learns to predict from a single input\nimage (a video frame) the output for that particular frame, of a teacher\npathway that performs unsupervised object discovery in video. Our approach is\ndifferent from the published literature that performs unsupervised discovery in\nvideos or in collections of images at test time. We move the unsupervised\ndiscovery phase during the training stage, while at test time we apply the\nstandard feed-forward processing along the student pathway. This has a dual\nbenefit: firstly, it allows in principle unlimited possibilities of learning\nand generalization during training, while remaining very fast at testing.\nSecondly, the student not only becomes able to detect in single images\nsignificantly better than its unsupervised video discovery teacher, but it also\nachieves state of the art results on two important current benchmarks, YouTube\nObjects and Object Discovery datasets. Moreover, at test time, our system is at\nleast two orders of magnitude faster than other previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 14:05:13 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Croitoru", "Ioana", "", "1 and\n  2"], ["Bogolin", "Simion-Vlad", "", "1 and\n  2"], ["Leordeanu", "Marius", "", "1 and\n  2"]]}, {"id": "1703.10902", "submitter": "Xiao Yang", "authors": "Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer", "title": "Fast Predictive Multimodal Image Registration", "comments": "Accepted as a conference paper for ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep encoder-decoder architecture for image deformation\nprediction from multimodal images. Specifically, we design an image-patch-based\ndeep network that jointly (i) learns an image similarity measure and (ii) the\nrelationship between image patches and deformation parameters. While our method\ncan be applied to general image registration formulations, we focus on the\nLarge Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By\npredicting the initial momentum of the shooting formulation of LDDMM, we\npreserve its mathematical properties and drastically reduce the computation\ntime, compared to optimization-based approaches. Furthermore, we create a\nBayesian probabilistic version of the network that allows evaluation of\nregistration uncertainty via sampling of the network at test time. We evaluate\nour method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our\nexperiments show that our method generates accurate predictions and that\nlearning the similarity measure leads to more consistent registrations than\nrelying on generic multimodal image similarity measures, such as mutual\ninformation. Our approach is an order of magnitude faster than\noptimization-based LDDMM.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 14:05:57 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Yang", "Xiao", ""], ["Kwitt", "Roland", ""], ["Styner", "Martin", ""], ["Niethammer", "Marc", ""]]}, {"id": "1703.10908", "submitter": "Xiao Yang", "authors": "Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer", "title": "Quicksilver: Fast Predictive Image Registration - a Deep Learning\n  Approach", "comments": "Add new discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Quicksilver, a fast deformable image registration\nmethod. Quicksilver registration for image-pairs works by patch-wise prediction\nof a deformation model based directly on image appearance. A deep\nencoder-decoder network is used as the prediction model. While the prediction\nstrategy is general, we focus on predictions for the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the\nmomentum-parameterization of LDDMM, which facilitates a patch-wise prediction\nstrategy while maintaining the theoretical properties of LDDMM, such as\nguaranteed diffeomorphic mappings for sufficiently strong regularization. We\nalso provide a probabilistic version of our prediction network which can be\nsampled during the testing time to calculate uncertainties in the predicted\ndeformations. Finally, we introduce a new correction network which greatly\nincreases the prediction accuracy of an already existing prediction network. We\nshow experimental results for uni-modal atlas-to-image as well as uni- / multi-\nmodal image-to-image registrations. These experiments demonstrate that our\nmethod accurately predicts registrations obtained by numerical optimization, is\nvery fast, achieves state-of-the-art registration results on four standard\nvalidation datasets, and can jointly learn an image similarity measure.\nQuicksilver is freely available as an open-source software.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 14:13:55 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 20:38:02 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 14:11:02 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 18:40:48 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Yang", "Xiao", ""], ["Kwitt", "Roland", ""], ["Styner", "Martin", ""], ["Niethammer", "Marc", ""]]}, {"id": "1703.10956", "submitter": "Michael Zollh\\\"ofer", "authors": "Hyeongwoo Kim, Michael Zollh\\\"ofer, Ayush Tewari, Justus Thies,\n  Christian Richardt, Christian Theobalt", "title": "InverseFaceNet: Deep Monocular Inverse Face Rendering", "comments": "CVPR 2018 (poster) 10 pages (+5 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce InverseFaceNet, a deep convolutional inverse rendering framework\nfor faces that jointly estimates facial pose, shape, expression, reflectance\nand illumination from a single input image. By estimating all parameters from\njust a single image, advanced editing possibilities on a single face image,\nsuch as appearance editing and relighting, become feasible in real time. Most\nprevious learning-based face reconstruction approaches do not jointly recover\nall dimensions, or are severely limited in terms of visual quality. In\ncontrast, we propose to recover high-quality facial pose, shape, expression,\nreflectance and illumination using a deep neural network that is trained using\na large, synthetically created training corpus. Our approach builds on a novel\nloss function that measures model-space similarity directly in parameter space\nand significantly improves reconstruction accuracy. We further propose a\nself-supervised bootstrapping process in the network training loop, which\niteratively updates the synthetic training corpus to better reflect the\ndistribution of real-world imagery. We demonstrate that this strategy\noutperforms completely synthetically trained networks. Finally, we show\nhigh-quality reconstructions and compare our approach to several\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:47:27 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 04:31:57 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Zollh\u00f6fer", "Michael", ""], ["Tewari", "Ayush", ""], ["Thies", "Justus", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}]