[{"id": "1912.00036", "submitter": "Angela Dai", "authors": "Angela Dai, Christian Diller, Matthias Nie{\\ss}ner", "title": "SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene\n  Completion of RGB-D Scans", "comments": "CVPR 2020; Video: https://youtu.be/rN6D3QmMNuU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that converts partial and noisy RGB-D scans into\nhigh-quality 3D scene reconstructions by inferring unobserved scene geometry.\nOur approach is fully self-supervised and can hence be trained solely on\nreal-world, incomplete scans. To achieve self-supervision, we remove frames\nfrom a given (incomplete) 3D scan in order to make it even more incomplete;\nself-supervision is then formulated by correlating the two levels of\npartialness of the same scan while masking out regions that have never been\nobserved. Through generalization across a large training set, we can then\npredict 3D scene completion without ever seeing any 3D scan of entirely\ncomplete geometry. Combined with a new 3D sparse generative neural network\narchitecture, our method is able to predict highly-detailed surfaces in a\ncoarse-to-fine hierarchical fashion, generating 3D scenes at 2cm resolution,\nmore than twice the resolution of existing state-of-the-art methods as well as\noutperforming them by a significant margin in reconstruction quality.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:00:14 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 18:57:00 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Dai", "Angela", ""], ["Diller", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.00042", "submitter": "Christina Winkler", "authors": "Christina Winkler, Daniel Worrall, Emiel Hoogeboom, Max Welling", "title": "Learning Likelihoods with Conditional Normalizing Flows", "comments": "18 pages, 8 Tables, 9 Figures, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing Flows (NFs) are able to model complicated distributions p(y) with\nstrong inter-dimensional correlations and high multimodality by transforming a\nsimple base density p(z) through an invertible neural network under the change\nof variables formula. Such behavior is desirable in multivariate structured\nprediction tasks, where handcrafted per-pixel loss-based methods inadequately\ncapture strong correlations between output dimensions. We present a study of\nconditional normalizing flows (CNFs), a class of NFs where the base density to\noutput space mapping is conditioned on an input x, to model conditional\ndensities p(y|x). CNFs are efficient in sampling and inference, they can be\ntrained with a likelihood-based objective, and CNFs, being generative flows, do\nnot suffer from mode collapse or training instabilities. We provide an\neffective method to train continuous CNFs for binary problems and in\nparticular, we apply these CNFs to super-resolution and vessel segmentation\ntasks demonstrating competitive performance on standard benchmark datasets in\nterms of likelihood and conventional metrics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:17:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Winkler", "Christina", ""], ["Worrall", "Daniel", ""], ["Hoogeboom", "Emiel", ""], ["Welling", "Max", ""]]}, {"id": "1912.00049", "submitter": "Maksym Andriushchenko", "authors": "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, Matthias\n  Hein", "title": "Square Attack: a query-efficient black-box adversarial attack via random\n  search", "comments": "Accepted at ECCV 2020; added imperceptible perturbations, analysis of\n  examples that require more queries, results on dilated CNNs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Square Attack, a score-based black-box $l_2$- and\n$l_\\infty$-adversarial attack that does not rely on local gradient information\nand thus is not affected by gradient masking. Square Attack is based on a\nrandomized search scheme which selects localized square-shaped updates at\nrandom positions so that at each iteration the perturbation is situated\napproximately at the boundary of the feasible set. Our method is significantly\nmore query efficient and achieves a higher success rate compared to the\nstate-of-the-art methods, especially in the untargeted setting. In particular,\non ImageNet we improve the average query efficiency in the untargeted setting\nfor various deep networks by a factor of at least $1.8$ and up to $3$ compared\nto the recent state-of-the-art $l_\\infty$-attack of Al-Dujaili & O'Reilly.\nMoreover, although our attack is black-box, it can also outperform\ngradient-based white-box attacks on the standard benchmarks achieving a new\nstate-of-the-art in terms of the success rate. The code of our attack is\navailable at https://github.com/max-andr/square-attack.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:29:32 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:30:48 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 07:53:10 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Andriushchenko", "Maksym", ""], ["Croce", "Francesco", ""], ["Flammarion", "Nicolas", ""], ["Hein", "Matthias", ""]]}, {"id": "1912.00070", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi, Poojan Oza, Rajeev Yasarla, Vishal M. Patel", "title": "Prior-based Domain Adaptive Object Detection for Hazy and Rainy\n  Conditions", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adverse weather conditions such as haze and rain corrupt the quality of\ncaptured images, which cause detection networks trained on clean images to\nperform poorly on these images. To address this issue, we propose an\nunsupervised prior-based domain adversarial object detection framework for\nadapting the detectors to hazy and rainy conditions. In particular, we use\nweather-specific prior knowledge obtained using the principles of image\nformation to define a novel prior-adversarial loss. The prior-adversarial loss\nused to train the adaptation process aims to reduce the weather-specific\ninformation in the features, thereby mitigating the effects of weather on the\ndetection performance. Additionally, we introduce a set of residual feature\nrecovery blocks in the object detection pipeline to de-distort the feature\nspace, resulting in further improvements. Evaluations performed on various\ndatasets (Foggy-Cityscapes, Rainy-Cityscapes, RTTS and UFDD) for rainy and hazy\nconditions demonstrates the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:09:13 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 00:51:06 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 17:35:19 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Oza", "Poojan", ""], ["Yasarla", "Rajeev", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1912.00076", "submitter": "Zicong Fan", "authors": "Zicong Fan, Si Yi Meng, Leonid Sigal, James J. Little", "title": "OptiBox: Breaking the Limits of Proposals for Visual Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of language grounding has attracted much attention in recent\nyears due to its pivotal role in more general image-lingual high level\nreasoning tasks (e.g., image captioning, VQA). Despite the tremendous progress\nin visual grounding, the performance of most approaches has been hindered by\nthe quality of bounding box proposals obtained in the early stages of all\nrecent pipelines. To address this limitation, we propose a general progressive\nquery-guided bounding box refinement architecture (OptiBox) that leverages\nglobal image encoding for added context. We apply this architecture in the\ncontext of the GroundeR model, first introduced in 2016, which has a number of\nunique and appealing properties, such as the ability to learn in the\nsemi-supervised setting by leveraging cyclic language-reconstruction. Using\nGroundeR + OptiBox and a simple semantic language reconstruction loss that we\npropose, we achieve state-of-the-art grounding performance in the supervised\nsetting on Flickr30k Entities dataset. More importantly, we are able to surpass\nmany recent fully supervised models with only 50% of training data and perform\ncompetitively with as low as 3%.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:46:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fan", "Zicong", ""], ["Meng", "Si Yi", ""], ["Sigal", "Leonid", ""], ["Little", "James J.", ""]]}, {"id": "1912.00086", "submitter": "Chi Zhang", "authors": "Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, Hongjing Lu, Song-Chun\n  Zhu", "title": "Learning Perceptual Inference by Contrasting", "comments": "NeurIPS 2019 spotlight. Project page:\n  http://wellyzhang.github.io/project/copinet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Thinking in pictures,\" [1] i.e., spatial-temporal reasoning, effortless and\ninstantaneous for humans, is believed to be a significant ability to perform\nlogical induction and a crucial factor in the intellectual history of\ntechnology development. Modern Artificial Intelligence (AI), fueled by massive\ndatasets, deeper models, and mighty computation, has come to a stage where\n(super-)human-level performances are observed in certain specific tasks.\nHowever, current AI's ability in \"thinking in pictures\" is still far lacking\nbehind. In this work, we study how to improve machines' reasoning ability on\none challenging task of this kind: Raven's Progressive Matrices (RPM).\nSpecifically, we borrow the very idea of \"contrast effects\" from the field of\npsychology, cognition, and education to design and train a\npermutation-invariant model. Inspired by cognitive studies, we equip our model\nwith a simple inference module that is jointly trained with the perception\nbackbone. Combining all the elements, we propose the Contrastive Perceptual\nInference network (CoPINet) and empirically demonstrate that CoPINet sets the\nnew state-of-the-art for permutation-invariant models on two major datasets. We\nconclude that spatial-temporal reasoning depends on envisaging the\npossibilities consistent with the relations between objects and can be solved\nfrom pixel-level inputs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 23:02:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Baoxiong", ""], ["Gao", "Feng", ""], ["Zhu", "Yixin", ""], ["Lu", "Hongjing", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1912.00096", "submitter": "Jianing Qian", "authors": "Peng Yin, Jianing Qian, Yibo Cao, David Held, Howie Choset", "title": "FusionMapping: Learning Depth Prediction with Monocular Images and 2D\n  Laser Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring accurate three-dimensional depth information conventionally\nrequires expensive multibeam LiDAR devices. Recently, researchers have\ndeveloped a less expensive option by predicting depth information from\ntwo-dimensional color imagery. However, there still exists a substantial gap in\naccuracy between depth information estimated from two-dimensional images and\nreal LiDAR point-cloud. In this paper, we introduce a fusion-based depth\nprediction method, called FusionMapping. This is the first method that fuses\ncolored imagery and two-dimensional laser scan to estimate depth in-formation.\nMore specifically, we propose an autoencoder-based depth prediction network and\na novel point-cloud refinement network for depth estimation. We analyze the\nperformance of our FusionMapping approach on the KITTI LiDAR odometry dataset\nand an indoor mobile robot system. The results show that our introduced\napproach estimates depth with better accuracy when compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 23:41:45 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Yin", "Peng", ""], ["Qian", "Jianing", ""], ["Cao", "Yibo", ""], ["Held", "David", ""], ["Choset", "Howie", ""]]}, {"id": "1912.00124", "submitter": "Jihyeon Lee", "authors": "Jihyeon Lee, Sho Arora", "title": "A Free Lunch in Generating Datasets: Building a VQG and VQA System with\n  Attention and Humans in the Loop", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their importance in training artificial intelligence systems, large\ndatasets remain challenging to acquire. For example, the ImageNet dataset\nrequired fourteen million labels of basic human knowledge, such as whether an\nimage contains a chair. Unfortunately, this knowledge is so simple that it is\ntedious for human annotators but also tacit enough such that they are\nnecessary. However, human collaborative efforts for tasks like labeling massive\namounts of data are costly, inconsistent, and prone to failure, and this method\ndoes not resolve the issue of the resulting dataset being static in nature.\nWhat if we asked people questions they want to answer and collected their\nresponses as data? This would mean we could gather data at a much lower cost,\nand expanding a dataset would simply become a matter of asking more questions.\nWe focus on the task of Visual Question Answering (VQA) and propose a system\nthat uses Visual Question Generation (VQG) to produce questions, asks them to\nsocial media users, and collects their responses. We present two models that\ncan then parse clean answers from the noisy human responses significantly\nbetter than our baselines, with the goal of eventually incorporating the\nanswers into a Visual Question Answering (VQA) dataset. By demonstrating how\nour system can collect large amounts of data at little to no cost, we envision\nsimilar systems being used to improve performance on other tasks in the future.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:45:17 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 17:52:03 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lee", "Jihyeon", ""], ["Arora", "Sho", ""]]}, {"id": "1912.00138", "submitter": "Yannick Zoetgnande", "authors": "Yannick Wend Kuni Zoetgnande, Geoffroy Cormier, Alain-J\\'er\\^ome\n  Foug\\`eres, Jean-Louis Dillenseger", "title": "Sub-pixel matching method for low-resolution thermal stereo images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a localization and tracking application, we developed a\nstereo vision system based on cheap low-resolution 80x60 pixels thermal\ncameras. We proposed a threefold sub-pixel stereo matching framework (called ST\nfor Subpixel Thermal): 1) robust features extraction method based on phase\ncongruency, 2) rough matching of these features in pixel precision, and 3)\nrefined matching in sub-pixel accuracy based on local phase coherence. We\nperformed experiments on our very low-resolution thermal images (acquired using\na stereo system we manufactured) as for high-resolution images from a benchmark\ndataset. Even if phase congruency computation time is high, it was able to\nextract two times more features than state-of-the-art methods such as ORB or\nSURF. We proposed a modified version of the phase correlation applied in the\nphase congruency feature space for sub-pixel matching. Using simulated stereo,\nwe investigated how the phase congruency threshold and the sub-image size of\nsub-pixel matching can influence the accuracy. We then proved that given our\nstereo setup and the resolution of our images, being wrong of 1 pixel leads to\na 500 mm error in the Z position of the point. Finally, we showed that our\nmethod could extract four times more matches than a baseline method ORB +\nOpenCV KNN matching on low-resolution images. Moreover, our matches were more\nrobust. More precisely, when projecting points of a standing person, ST got a\nstandard deviation of 300 mm when ORB + OpenCV KNN gave more than 1000 mm.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:09:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zoetgnande", "Yannick Wend Kuni", ""], ["Cormier", "Geoffroy", ""], ["Foug\u00e8res", "Alain-J\u00e9r\u00f4me", ""], ["Dillenseger", "Jean-Louis", ""]]}, {"id": "1912.00144", "submitter": "Huangxing Lin", "authors": "Huangxing Lin, Weihong Zeng, Xinghao Ding, Yue Huang, Chenxi Huang and\n  John Paisley", "title": "Learning Rate Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a deep neural network is highly dependent on its training,\nand finding better local optimal solutions is the goal of many optimization\nalgorithms. However, existing optimization algorithms show a preference for\ndescent paths that converge slowly and do not seek to avoid bad local optima.\nIn this work, we propose Learning Rate Dropout (LRD), a simple gradient descent\ntechnique for training related to coordinate descent. LRD empirically aids the\noptimizer to actively explore in the parameter space by randomly setting some\nlearning rates to zero; at each iteration, only parameters whose learning rate\nis not 0 are updated. As the learning rate of different parameters is dropped,\nthe optimizer will sample a new loss descent path for the current update. The\nuncertainty of the descent path helps the model avoid saddle points and bad\nlocal minima. Experiments show that LRD is surprisingly effective in\naccelerating training while preventing overfitting.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:58:40 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 08:43:16 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lin", "Huangxing", ""], ["Zeng", "Weihong", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Huang", "Chenxi", ""], ["Paisley", "John", ""]]}, {"id": "1912.00145", "submitter": "Biao Zhang", "authors": "Biao Zhang, Peter Wonka", "title": "Point Cloud Instance Segmentation using Probabilistic Embeddings", "comments": "Accepted by CVPR 2021. Project:\n  http://1zb.github.io/publication/prob-embed/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new framework for point cloud instance\nsegmentation. Our framework has two steps: an embedding step and a clustering\nstep. In the embedding step, our main contribution is to propose a\nprobabilistic embedding space for point cloud embedding. Specifically, each\npoint is represented as a tri-variate normal distribution. In the clustering\nstep, we propose a novel loss function, which benefits both the semantic\nsegmentation and the clustering. Our experimental results show important\nimprovements to the SOTA, i.e., 3.1% increased average per-category mAP on the\nPartNet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:04:09 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:03:48 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Biao", ""], ["Wonka", "Peter", ""]]}, {"id": "1912.00157", "submitter": "Shady Abu Hussein", "authors": "Shady Abu Hussein, Tom Tirer, and Raja Giryes", "title": "Correction Filter for Single Image Super-Resolution: Robustifying\n  Off-the-Shelf Deep Super-Resolvers", "comments": "Accepted to CVPR 2020 (Oral). Code is available at\n  https://github.com/shadyabh/Correction-Filter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The single image super-resolution task is one of the most examined inverse\nproblems in the past decade. In the recent years, Deep Neural Networks (DNNs)\nhave shown superior performance over alternative methods when the acquisition\nprocess uses a fixed known downsampling kernel-typically a bicubic kernel.\nHowever, several recent works have shown that in practical scenarios, where the\ntest data mismatch the training data (e.g. when the downsampling kernel is not\nthe bicubic kernel or is not available at training), the leading DNN methods\nsuffer from a huge performance drop. Inspired by the literature on generalized\nsampling, in this work we propose a method for improving the performance of\nDNNs that have been trained with a fixed kernel on observations acquired by\nother kernels. For a known kernel, we design a closed-form correction filter\nthat modifies the low-resolution image to match one which is obtained by\nanother kernel (e.g. bicubic), and thus improves the results of existing\npre-trained DNNs. For an unknown kernel, we extend this idea and propose an\nalgorithm for blind estimation of the required correction filter. We show that\nour approach outperforms other super-resolution methods, which are designed for\ngeneral downsampling kernels.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 08:04:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 13:27:27 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Hussein", "Shady Abu", ""], ["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1912.00177", "submitter": "Jeffrey Hawke", "authors": "Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele\n  Reda, Nikolay Nikolov, Przemyslaw Mazur, Sean Micklethwaite, Nicolas\n  Griffiths, Amar Shah, Alex Kendall", "title": "Urban Driving with Conditional Imitation Learning", "comments": "Under submission; added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand-crafting generalised decision-making rules for real-world urban\nautonomous driving is hard. Alternatively, learning behaviour from\neasy-to-collect human driving demonstrations is appealing. Prior work has\nstudied imitation learning (IL) for autonomous driving with a number of\nlimitations. Examples include only performing lane-following rather than\nfollowing a user-defined route, only using a single camera view or heavily\ncropped frames lacking state observability, only lateral (steering) control,\nbut not longitudinal (speed) control and a lack of interaction with traffic.\nImportantly, the majority of such systems have been primarily evaluated in\nsimulation - a simple domain, which lacks real-world complexities. Motivated by\nthese challenges, we focus on learning representations of semantics, geometry\nand motion with computer vision for IL from human driving demonstrations. As\nour main contribution, we present an end-to-end conditional imitation learning\napproach, combining both lateral and longitudinal control on a real vehicle for\nfollowing urban routes with simple traffic. We address inherent dataset bias by\ndata balancing, training our final policy on approximately 30 hours of\ndemonstrations gathered over six months. We evaluate our method on an\nautonomous vehicle by driving 35km of novel routes in European urban streets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 10:24:45 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 18:17:45 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Hawke", "Jeffrey", ""], ["Shen", "Richard", ""], ["Gurau", "Corina", ""], ["Sharma", "Siddharth", ""], ["Reda", "Daniele", ""], ["Nikolov", "Nikolay", ""], ["Mazur", "Przemyslaw", ""], ["Micklethwaite", "Sean", ""], ["Griffiths", "Nicolas", ""], ["Shah", "Amar", ""], ["Kendall", "Alex", ""]]}, {"id": "1912.00195", "submitter": "Guohao Li", "authors": "Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias M\\\"uller, Ali\n  Thabet, Bernard Ghanem", "title": "SGAS: Sequential Greedy Architecture Search", "comments": "Accepted at CVPR'2020. Project website:\n  https://www.deepgcns.org/auto/sgas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architecture design has become a crucial component of successful deep\nlearning. Recent progress in automatic neural architecture search (NAS) shows a\nlot of promise. However, discovered architectures often fail to generalize in\nthe final evaluation. Architectures with a higher validation accuracy during\nthe search phase may perform worse in the evaluation. Aiming to alleviate this\ncommon issue, we introduce sequential greedy architecture search (SGAS), an\nefficient method for neural architecture search. By dividing the search\nprocedure into sub-problems, SGAS chooses and prunes candidate operations in a\ngreedy fashion. We apply SGAS to search architectures for Convolutional Neural\nNetworks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments\nshow that SGAS is able to find state-of-the-art architectures for tasks such as\nimage classification, point cloud classification and node classification in\nprotein-protein interaction graphs with minimal computational cost. Please\nvisit https://www.deepgcns.org/auto/sgas for more information about SGAS.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 12:39:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 12:55:03 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Li", "Guohao", ""], ["Qian", "Guocheng", ""], ["Delgadillo", "Itzel C.", ""], ["M\u00fcller", "Matthias", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1912.00200", "submitter": "Abdullah Salama", "authors": "Abdullah Salama, Oleksiy Ostapenko, Tassilo Klein, Moin Nabi", "title": "Pruning at a Glance: Global Neural Pruning for Model Compression", "comments": "Extended version of the ICASSP paper\n  (https://ieeexplore.ieee.org/document/8683224)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning models have become the dominant approach in several areas due\nto their high performance. Unfortunately, the size and hence computational\nrequirements of operating such models can be considerably high. Therefore, this\nconstitutes a limitation for deployment on memory and battery constrained\ndevices such as mobile phones or embedded systems. To address these\nlimitations, we propose a novel and simple pruning method that compresses\nneural networks by removing entire filters and neurons according to a global\nthreshold across the network without any pre-calculation of layer sensitivity.\nThe resulting model is compact, non-sparse, with the same accuracy as the\nnon-compressed model, and most importantly requires no special infrastructure\nfor deployment. We prove the viability of our method by producing highly\ncompressed models, namely VGG-16, ResNet-56, and ResNet-110 respectively on\nCIFAR10 without losing any performance compared to the baseline, as well as\nResNet-34 and ResNet-50 on ImageNet without a significant loss of accuracy. We\nalso provide a well-retrained 30% compressed ResNet-50 that slightly surpasses\nthe base model accuracy. Additionally, compressing more than 56% and 97% of\nAlexNet and LeNet-5 respectively. Interestingly, the resulted models' pruning\npatterns are highly similar to the other methods using layer sensitivity\npre-calculation step. Our method does not only exhibit good performance but\nwhat is more also easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 13:17:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 09:44:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Salama", "Abdullah", ""], ["Ostapenko", "Oleksiy", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1912.00201", "submitter": "Jialin Peng", "authors": "Zhimin Yuan, Jiajin Yi, Zhengrong Luo, Zhongdao Jia, Jialin Peng", "title": "EM-NET: Centerline-Aware Mitochondria Segmentation in EM Images via\n  Hierarchical View-Ensemble Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep encoder-decoder networks have achieved astonishing performance\nfor mitochondria segmentation from electron microscopy (EM) images, they still\nproduce coarse segmentations with lots of discontinuities and false positives.\nBesides, the need for labor intensive annotations of large 3D dataset and huge\nmemory overhead by 3D models are also major limitations. To address these\nproblems, we introduce a multi-task network named EM-Net, which includes an\nauxiliary centerline detection task to account for shape information of\nmitochondria represented by centerline. Therefore, the centerline detection\nsub-network is able to enhance the accuracy and robustness of segmentation\ntask, especially when only a small set of annotated data are available. To\nachieve a light-weight 3D network, we introduce a novel hierarchical\nview-ensemble convolution module to reduce number of parameters, and facilitate\nmulti-view information aggregation.Validations on public benchmark showed\nstate-of-the-art performance by EM-Net. Even with significantly reduced\ntraining data, our method still showed quite promising results.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 13:25:15 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 08:42:27 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 04:04:56 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Yuan", "Zhimin", ""], ["Yi", "Jiajin", ""], ["Luo", "Zhengrong", ""], ["Jia", "Zhongdao", ""], ["Peng", "Jialin", ""]]}, {"id": "1912.00202", "submitter": "Mingtao Feng", "authors": "Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Liang Zhang and\n  Ajmal Mian", "title": "Relation Graph Network for 3D Object Detection in Point Clouds", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have emerged as a powerful strategy for\nmost object detection tasks on 2D images. However, their power has not been\nfully realised for detecting 3D objects in point clouds directly without\nconverting them to regular grids. Existing state-of-art 3D object detection\nmethods aim to recognize 3D objects individually without exploiting their\nrelationships during learning or inference. In this paper, we first propose a\nstrategy that associates the predictions of direction vectors and pseudo\ngeometric centers together leading to a win-win solution for 3D bounding box\ncandidates regression. Secondly, we propose point attention pooling to extract\nuniform appearance features for each 3D object proposal, benefiting from the\nlearned direction features, semantic features and spatial coordinates of the\nobject points. Finally, the appearance features are used together with the\nposition features to build 3D object-object relationship graphs for all\nproposals to model their co-existence. We explore the effect of relation graphs\non proposals' appearance features enhancement under supervised and unsupervised\nsettings. The proposed relation graph network consists of a 3D object proposal\ngeneration module and a 3D relation module, makes it an end-to-end trainable\nnetwork for detecting 3D object in point clouds. Experiments on challenging\nbenchmarks ( SunRGB-Dand ScanNet datasets ) of 3D point clouds show that our\nalgorithm can perform better than the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 13:31:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Feng", "Mingtao", ""], ["Gilani", "Syed Zulqarnain", ""], ["Wang", "Yaonan", ""], ["Zhang", "Liang", ""], ["Mian", "Ajmal", ""]]}, {"id": "1912.00215", "submitter": "Cinjon Resnick", "authors": "Cinjon Resnick, Zeping Zhan, Joan Bruna", "title": "Probing the State of the Art: A Critical Look at Visual Representation\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised research improved greatly over the past half decade, with\nmuch of the growth being driven by objectives that are hard to quantitatively\ncompare. These techniques include colorization, cyclical consistency, and\nnoise-contrastive estimation from image patches. Consequently, the field has\nsettled on a handful of measurements that depend on linear probes to adjudicate\nwhich approaches are the best. Our first contribution is to show that this test\nis insufficient and that models which perform poorly (strongly) on linear\nclassification can perform strongly (weakly) on more involved tasks like\ntemporal activity localization. Our second contribution is to analyze the\ncapabilities of five different representations. And our third contribution is a\nmuch needed new dataset for temporal activity localization.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 15:05:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Resnick", "Cinjon", ""], ["Zhan", "Zeping", ""], ["Bruna", "Joan", ""]]}, {"id": "1912.00233", "submitter": "Mir Assadullah", "authors": "Mir Muhammad Abdullah, Mir Muhammad Abdur Rahman, Mir Mohammed\n  Assadullah", "title": "Convolutional neural networks model improvements using demographics and\n  image processing filters on chest x-rays", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The purpose of this study was to observe change in accuracies of\nconvolutional neural networks (CNN) models (ratio of correct classifications to\ntotal predictions) on thoracic radiological images by creating different binary\nclassification models based on age, gender, and image pre-processing filters on\n14 pathologies.\n  Methodology: This is a quantitative research exploring variation in CNN model\naccuracies. Radiological thoracic images were divided by age and gender and\npre-processed by various image processing filters.\n  Findings: We found partial support for enhancement to model accuracies by\nsegregating modeling images by age and gender and applying image processing\nfilters even though image processing filters are sometimes thought of as\ninformation filters.\n  Research limitations: This study may be biased because it is based on\nradiological images by another research that tagged the images using an\nautomated process that was not checked by a human.\n  Practical implications: Researchers may want to focus on creating models\nsegregated by demographics and pre-process the modeling images using image\nprocessing filters. Practitioners developing assistive technologies for\nthoracic diagnoses may benefit from incorporating demographics and employing\nmultiple models simultaneously with varying statistical likelihood.\n  Originality/value: This study uses demographics in model creation and\nutilizes image processing filters to improve model performance.\n  Keywords: Convolutional Neural Network (CNN), Chest X-Ray, ChestX-ray14,\nLung, Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema,\nInfiltration, Mass, Nodule, Pleural Thickening, Pneumonia, Pneumathorax\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 17:00:21 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Abdullah", "Mir Muhammad", ""], ["Rahman", "Mir Muhammad Abdur", ""], ["Assadullah", "Mir Mohammed", ""]]}, {"id": "1912.00254", "submitter": "Amnon Geifman", "authors": "Amnon Geifman, Yoni Kasten, Meirav Galun and Ronen Basri", "title": "Averaging Essential and Fundamental Matrices in Collinear Camera\n  Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global methods to Structure from Motion have gained popularity in recent\nyears. A significant drawback of global methods is their sensitivity to\ncollinear camera settings. In this paper, we introduce an analysis and\nalgorithms for averaging bifocal tensors (essential or fundamental matrices)\nwhen either subsets or all of the camera centers are collinear.\n  We provide a complete spectral characterization of bifocal tensors in\ncollinear scenarios and further propose two averaging algorithms. The first\nalgorithm uses rank constrained minimization to recover camera matrices in\nfully collinear settings. The second algorithm enriches the set of possibly\nmixed collinear and non-collinear cameras with additional, \"virtual cameras,\"\nwhich are placed in general position, enabling the application of existing\naveraging methods to the enriched set of bifocal tensors. Our algorithms are\nshown to achieve state of the art results on various benchmarks that include\nautonomous car datasets and unordered image collections in both calibrated and\nunclibrated settings.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 19:24:13 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 09:33:28 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 08:41:37 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Geifman", "Amnon", ""], ["Kasten", "Yoni", ""], ["Galun", "Meirav", ""], ["Basri", "Ronen", ""]]}, {"id": "1912.00262", "submitter": "Ava Soleimany", "authors": "Ava P. Soleimany, Harini Suresh, Jose Javier Gonzalez Ortiz, Divya\n  Shanmugam, Nil Gural, John Guttag, Sangeeta N. Bhatia", "title": "Image segmentation of liver stage malaria infection with spatial\n  uncertainty sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global eradication of malaria depends on the development of drugs effective\nagainst the silent, yet obligate liver stage of the disease. The gold standard\nin drug development remains microscopic imaging of liver stage parasites in in\nvitro cell culture models. Image analysis presents a major bottleneck in this\npipeline since the parasite has significant variability in size, shape, and\ndensity in these models. As with other highly variable datasets, traditional\nsegmentation models have poor generalizability as they rely on hand-crafted\nfeatures; thus, manual annotation of liver stage malaria images remains\nstandard. To address this need, we develop a convolutional neural network\narchitecture that utilizes spatial dropout sampling for parasite segmentation\nand epistemic uncertainty estimation in images of liver stage malaria. Our\npipeline produces high-precision segmentations nearly identical to expert\nannotations, generalizes well on a diverse dataset of liver stage malaria\nparasites, and promotes independence between learned feature maps to model the\nuncertainty of generated predictions.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 20:57:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Soleimany", "Ava P.", ""], ["Suresh", "Harini", ""], ["Ortiz", "Jose Javier Gonzalez", ""], ["Shanmugam", "Divya", ""], ["Gural", "Nil", ""], ["Guttag", "John", ""], ["Bhatia", "Sangeeta N.", ""]]}, {"id": "1912.00271", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi, Hang Su, Mohammed Bennamoun,\n  David Zhang", "title": "Biometrics Recognition Using Deep Learning: A Survey", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models have been very successful in achieving\nstate-of-the-art results in many of the computer vision, speech recognition,\nand natural language processing tasks in the last few years. These models seem\na natural fit for handling the ever-increasing scale of biometric recognition\nproblems, from cellphone authentication to airport security systems. Deep\nlearning-based models have increasingly been leveraged to improve the accuracy\nof different biometric recognition systems in recent years. In this work, we\nprovide a comprehensive survey of more than 120 promising works on biometric\nrecognition (including face, fingerprint, iris, palmprint, ear, voice,\nsignature, and gait recognition), which deploy deep learning models, and show\ntheir strengths and potentials in different applications. For each biometric,\nwe first introduce the available datasets that are widely used in the\nliterature and their characteristics. We will then talk about several promising\ndeep learning works developed for that biometric, and show their performance on\npopular public benchmarks. We will also discuss some of the main challenges\nwhile using these models for biometric recognition, and possible future\ndirections to which research in this area is headed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 22:00:57 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 14:59:36 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 19:24:49 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""], ["Su", "Hang", ""], ["Bennamoun", "Mohammed", ""], ["Zhang", "David", ""]]}, {"id": "1912.00280", "submitter": "Minghua Liu", "authors": "Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, Shi-Min Hu", "title": "Morphing and Sampling Network for Dense Point Cloud Completion", "comments": "8pages, 7 figures, AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud completion, the task of inferring the complete geometric shape\nfrom a partial point cloud, has been attracting attention in the community. For\nacquiring high-fidelity dense point clouds and avoiding uneven distribution,\nblurred details, or structural loss of existing methods' results, we propose a\nnovel approach to complete the partial point cloud in two stages. Specifically,\nin the first stage, the approach predicts a complete but coarse-grained point\ncloud with a collection of parametric surface elements. Then, in the second\nstage, it merges the coarse-grained prediction with the input point cloud by a\nnovel sampling algorithm. Our method utilizes a joint loss function to guide\nthe distribution of the points. Extensive experiments verify the effectiveness\nof our method and demonstrate that it outperforms the existing methods in both\nthe Earth Mover's Distance (EMD) and the Chamfer Distance (CD).\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 22:52:54 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Liu", "Minghua", ""], ["Sheng", "Lu", ""], ["Yang", "Sheng", ""], ["Shao", "Jing", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1912.00289", "submitter": "Edward Kim", "authors": "Edward Kim, Divya Gopinath, Corina Pasareanu, Sanjit Seshia", "title": "A Programmatic and Semantic Approach to Explaining and DebuggingNeural\n  Network Based Object Detectors", "comments": null, "journal-ref": "CVPR (2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Even as deep neural networks have become very effective for tasks in vision\nand perception, it remains difficult to explain and debug their behavior. In\nthis paper, we present a programmatic and semantic approach to explaining,\nunderstanding, and debugging the correct and incorrect behaviors of a neural\nnetwork-based perception system. Our approach is semantic in that it employs a\nhigh-level representation of the distribution of environment scenarios that the\ndetector is intended to work on. It is programmatic in that scenario\nrepresentation is a program in a domain-specific probabilistic programming\nlanguage which can be used to generate synthetic data to test a given\nperception module. Our framework assesses the performance of a perception\nmodule to identify correct and incorrect detections, extracts rules from those\nresults that semantically characterizes the correct and incorrect scenarios,\nand then specializes the probabilistic program with those rules in order to\nmore precisely characterize the scenarios in which the perception module\noperates correctly or not. We demonstrate our results using the SCENIC\nprobabilistic programming language and a neural network-based object detector.\nOur experiments show that it is possible to automatically generate compact\nrules that significantly increase the correct detection rate (or conversely the\nincorrect detection rate) of the network and can thus help with understanding\nand debugging its behavior.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:07:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 07:48:40 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Kim", "Edward", ""], ["Gopinath", "Divya", ""], ["Pasareanu", "Corina", ""], ["Seshia", "Sanjit", ""]]}, {"id": "1912.00296", "submitter": "Prabu Ravindran", "authors": "Prabu Ravindran, Emmanuel Ebanyenle, Alberta Asi Ebeheakey, Kofi Bonsu\n  Abban, Ophilious Lambog, Richard Soares, Adriana Costa and Alex C.\n  Wiedenhoeft", "title": "Image Based Identification of Ghanaian Timbers Using the XyloTron:\n  Opportunities, Risks and Challenges", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems for wood identification have the potential to empower\nboth producer and consumer countries to combat illegal logging if they can be\ndeployed effectively in the field. In this work, carried out as part of an\nactive international partnership with the support of UNIDO, we constructed and\ncurated a field-relevant image data set to train a classifier for wood\nidentification of $15$ commercial Ghanaian woods using the XyloTron system. We\ntested model performance in the laboratory, and then collected real-world field\nperformance data across multiple sites using multiple XyloTron devices. We\npresent efficacies of the trained model in the laboratory and in the field,\ndiscuss practical implications and challenges of deploying machine learning\nwood identification models, and conclude that field testing is a necessary step\n- and should be considered the gold-standard - for validating computer vision\nwood identification systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 01:05:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ravindran", "Prabu", ""], ["Ebanyenle", "Emmanuel", ""], ["Ebeheakey", "Alberta Asi", ""], ["Abban", "Kofi Bonsu", ""], ["Lambog", "Ophilious", ""], ["Soares", "Richard", ""], ["Costa", "Adriana", ""], ["Wiedenhoeft", "Alex C.", ""]]}, {"id": "1912.00308", "submitter": "Yiyi Zhang", "authors": "Yiyi Zhang, Li Niu, Ziqi Pan, Meichao Luo, Jianfu Zhang, Dawei Cheng,\n  Liqing Zhang", "title": "Exploiting Motion Information from Unlabeled Videos for Static Image\n  Action Recognition", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static image action recognition, which aims to recognize action based on a\nsingle image, usually relies on expensive human labeling effort such as\nadequate labeled action images and large-scale labeled image dataset. In\ncontrast, abundant unlabeled videos can be economically obtained. Therefore,\nseveral works have explored using unlabeled videos to facilitate image action\nrecognition, which can be categorized into the following two groups: (a)\nenhance visual representations of action images with a designed proxy task on\nunlabeled videos, which falls into the scope of self-supervised learning; (b)\ngenerate auxiliary representations for action images with the generator learned\nfrom unlabeled videos. In this paper, we integrate the above two strategies in\na unified framework, which consists of Visual Representation Enhancement (VRE)\nmodule and Motion Representation Augmentation (MRA) module. Specifically, the\nVRE module includes a proxy task which imposes pseudo motion label constraint\nand temporal coherence constraint on unlabeled videos, while the MRA module\ncould predict the motion information of a static action image by exploiting\nunlabeled videos. We demonstrate the superiority of our framework based on four\nbenchmark human action datasets with limited labeled data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 03:01:10 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Yiyi", ""], ["Niu", "Li", ""], ["Pan", "Ziqi", ""], ["Luo", "Meichao", ""], ["Zhang", "Jianfu", ""], ["Cheng", "Dawei", ""], ["Zhang", "Liqing", ""]]}, {"id": "1912.00311", "submitter": "Joel Ruben Antony Moniz", "authors": "Sarthak Garg, Joel Ruben Antony Moniz, Anshu Aviral, Priyatham\n  Bollimpalli", "title": "Learning to Relate from Captions and Bounding Boxes", "comments": "ACL 2019", "journal-ref": null, "doi": "10.18653/v1/P19-1660", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel approach that predicts the relationships\nbetween various entities in an image in a weakly supervised manner by relying\non image captions and object bounding box annotations as the sole source of\nsupervision. Our proposed approach uses a top-down attention mechanism to align\nentities in captions to objects in the image, and then leverage the syntactic\nstructure of the captions to align the relations. We use these alignments to\ntrain a relation classification network, thereby obtaining both grounded\ncaptions and dense relationships. We demonstrate the effectiveness of our model\non the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of\n25% on the relationships present in the image. We also show that the model\nsuccessfully predicts relations that are not present in the corresponding\ncaptions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 03:30:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Garg", "Sarthak", ""], ["Moniz", "Joel Ruben Antony", ""], ["Aviral", "Anshu", ""], ["Bollimpalli", "Priyatham", ""]]}, {"id": "1912.00320", "submitter": "Dongrui Wu", "authors": "Wen Zhang and Dongrui Wu", "title": "Discriminative Joint Probability Maximum Mean Discrepancy (DJP-MMD) for\n  Domain Adaptation", "comments": "Int'l Joint Conf. on Neural Networks (IJCNN), Glasgow, UK, July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum mean discrepancy (MMD) has been widely adopted in domain adaptation\nto measure the discrepancy between the source and target domain distributions.\nMany existing domain adaptation approaches are based on the joint MMD, which is\ncomputed as the (weighted) sum of the marginal distribution discrepancy and the\nconditional distribution discrepancy; however, a more natural metric may be\ntheir joint probability distribution discrepancy. Additionally, most metrics\nonly aim to increase the transferability between domains, but ignores the\ndiscriminability between different classes, which may result in insufficient\nclassification performance. To address these issues, discriminative joint\nprobability MMD (DJP-MMD) is proposed in this paper to replace the\nfrequently-used joint MMD in domain adaptation. It has two desirable\nproperties: 1) it provides a new theoretical basis for computing the\ndistribution discrepancy, which is simpler and more accurate; 2) it increases\nthe transferability and discriminability simultaneously. We validate its\nperformance by embedding it into a joint probability domain adaptation\nframework. Experiments on six image classification datasets demonstrated that\nthe proposed DJP-MMD can outperform traditional MMDs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:52:41 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:47:44 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 08:04:37 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 15:13:57 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Wen", ""], ["Wu", "Dongrui", ""]]}, {"id": "1912.00336", "submitter": "Lisai Zhang", "authors": "Lisai Zhang, Qingcai Chen, Dongfang Li, Buzhou Tang", "title": "Semi-supervised Visual Feature Integration for Pre-trained Language\n  Models", "comments": "12 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating visual features has been proved useful for natural language\nunderstanding tasks. Nevertheless, in most existing multimodal language models,\nthe alignment of visual and textual data is expensive. In this paper, we\npropose a novel semi-supervised visual integration framework for pre-trained\nlanguage models. In the framework, the visual features are obtained through a\nvisualization and fusion mechanism. The uniqueness includes: 1) the integration\nis conducted via a semi-supervised approach, which does not require aligned\nimages for every sentences 2) the visual features are integrated as an external\ncomponent and can be directly used by pre-trained language models. To verify\nthe efficacy of the proposed framework, we conduct the experiments on both\nnatural language inference and reading comprehension tasks. The results\ndemonstrate that our mechanism brings improvement to two strong baseline\nmodels. Considering that our framework only requires an image database, and no\nnot requires further alignments, it provides an efficient and feasible way for\nmultimodal language learning.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 06:53:23 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 02:59:54 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Lisai", ""], ["Chen", "Qingcai", ""], ["Li", "Dongfang", ""], ["Tang", "Buzhou", ""]]}, {"id": "1912.00367", "submitter": "Shir Gur", "authors": "Shir Gur, Tal Shaharabany, Lior Wolf", "title": "End to End Trainable Active Contours via Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image segmentation method that iteratively evolves a polygon.\nAt each iteration, the vertices of the polygon are displaced based on the local\nvalue of a 2D shift map that is inferred from the input image via an\nencoder-decoder architecture. The main training loss that is used is the\ndifference between the polygon shape and the ground truth segmentation mask.\nThe network employs a neural renderer to create the polygon from its vertices,\nmaking the process fully differentiable. We demonstrate that our method\noutperforms the state of the art segmentation networks and deep active contour\nsolutions in a variety of benchmarks, including medical imaging and aerial\nimages. Our code is available at https://github.com/shirgur/ACDRNet.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 09:27:22 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gur", "Shir", ""], ["Shaharabany", "Tal", ""], ["Wolf", "Lior", ""]]}, {"id": "1912.00377", "submitter": "Min Ren", "authors": "Min Ren, Yunlong Wang, Zhenan Sun, Tieniu Tan", "title": "Dynamic Graph Representation for Partially Occluded Biometrics", "comments": "Accepted by AAAI2020, 9 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization ability of Convolutional neural networks (CNNs) for\nbiometrics drops greatly due to the adverse effects of various occlusions. To\nthis end, we propose a novel unified framework integrated the merits of both\nCNNs and graphical models to learn dynamic graph representations for occlusion\nproblems in biometrics, called Dynamic Graph Representation (DGR).\nConvolutional features onto certain regions are re-crafted by a graph generator\nto establish the connections among the spatial parts of biometrics and build\nFeature Graphs based on these node representations. Each node of Feature Graphs\ncorresponds to a specific part of the input image and the edges express the\nspatial relationships between parts. By analyzing the similarities between the\nnodes, the framework is able to adaptively remove the nodes representing the\noccluded parts. During dynamic graph matching, we propose a novel strategy to\nmeasure the distances of both nodes and adjacent matrixes. In this way, the\nproposed method is more convincing than CNNs-based methods because the dynamic\ngraph method implies a more illustrative and reasonable inference of the\nbiometrics decision. Experiments conducted on iris and face demonstrate the\nsuperiority of the proposed framework, which boosts the accuracy of occluded\nbiometrics recognition by a large margin comparing with baseline methods.The\ncode is avaliable at\nhttps://github.com/RenMin1991/Dyamic\\_Graph\\_Representation\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 10:27:11 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 11:06:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ren", "Min", ""], ["Wang", "Yunlong", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1912.00381", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz", "title": "Gate-Shift Networks for Video Action Recognition", "comments": "CVPR20 camera ready version. Code and models available at\n  https://github.com/swathikirans/GSM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep 3D CNNs for video action recognition are designed to learn powerful\nrepresentations in the joint spatio-temporal feature space. In practice\nhowever, because of the large number of parameters and computations involved,\nthey may under-perform in the lack of sufficiently large datasets for training\nthem at scale. In this paper we introduce spatial gating in spatial-temporal\ndecomposition of 3D kernels. We implement this concept with Gate-Shift Module\n(GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient\nspatio-temporal feature extractor. With GSM plugged in, a 2D-CNN learns to\nadaptively route features through time and combine them, at almost no\nadditional parameters and computational overhead. We perform an extensive\nevaluation of the proposed module to study its effectiveness in video action\nrecognition, achieving state-of-the-art results on Something Something-V1 and\nDiving48 datasets, and obtaining competitive results on EPIC-Kitchens with far\nless model complexity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 10:49:11 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 19:23:27 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "1912.00382", "submitter": "Min Ren", "authors": "Min Ren, Caiyong Wang, Yunlong Wang, Zhenan Sun, Tieniu Tan", "title": "Alignment Free and Distortion Robust Iris Recognition", "comments": "Accepted by ICB2019, 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition is a reliable personal identification method but there is\nstill much room to improve its accuracy especially in less-constrained\nsituations. For example, free movement of head pose may cause large rotation\ndifference between iris images. And illumination variations may cause irregular\ndistortion of iris texture. To match intra-class iris images with head rotation\nrobustly, the existing solutions usually need a precise alignment operation by\nexhaustive search within a determined range in iris image preprosessing or\nbrute force searching the minimum Hamming distance in iris feature matching. In\nthe wild, iris rotation is of much greater uncertainty than that in constrained\nsituations and exhaustive search within a determined range is impracticable.\nThis paper presents a unified feature-level solution to both alignment free and\ndistortion robust iris recognition in the wild. A new deep learning based\nmethod named Alignment Free Iris Network (AFINet) is proposed, which uses a\ntrainable VLAD (Vector of Locally Aggregated Descriptors) encoder called\nNetVLAD to decouple the correlations between local representations and their\nspatial positions. And deformable convolution is used to overcome iris texture\ndistortion by dense adaptive sampling. The results of extensive experiments on\nthree public iris image databases and the simulated degradation databases show\nthat AFINet significantly outperforms state-of-art iris recognition methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 11:03:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ren", "Min", ""], ["Wang", "Caiyong", ""], ["Wang", "Yunlong", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1912.00384", "submitter": "Zhaohui Yang", "authors": "Zhaohui Yang, Miaojing Shi, Chao Xu, Vittorio Ferrari, Yannis Avrithis", "title": "Training Object Detectors from Few Weakly-Labeled and Many Unlabeled\n  Images", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object detection attempts to limit the amount of\nsupervision by dispensing the need for bounding boxes, but still assumes\nimage-level labels on the entire training set. In this work, we study the\nproblem of training an object detector from one or few images with image-level\nlabels and a larger set of completely unlabeled images. This is an extreme case\nof semi-supervised learning where the labeled data are not enough to bootstrap\nthe learning of a detector. Our solution is to train a weakly-supervised\nstudent detector model from image-level pseudo-labels generated on the\nunlabeled set by a teacher classifier model, bootstrapped by region-level\nsimilarities to labeled images. Building upon the recent representative\nweakly-supervised pipeline PCL, our method can use more unlabeled images to\nachieve performance competitive or superior to many recent weakly-supervised\ndetection solutions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 11:09:48 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 12:49:49 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 10:18:58 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 16:29:10 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 13:50:11 GMT"}, {"version": "v6", "created": "Wed, 21 Jul 2021 01:36:48 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yang", "Zhaohui", ""], ["Shi", "Miaojing", ""], ["Xu", "Chao", ""], ["Ferrari", "Vittorio", ""], ["Avrithis", "Yannis", ""]]}, {"id": "1912.00385", "submitter": "Ismail Elezi", "authors": "Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello\n  Pelillo, Laura Leal-Taixe", "title": "The Group Loss for Deep Metric Learning", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2020,\n  includes non-archival supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that \"similar objects\nshould belong to the same group\", the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We show state-of-the-art results on clustering and image\nretrieval on several datasets, and show the potential of our method when\ncombined with other techniques such as ensembles\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 11:09:57 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 20:19:30 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 03:32:58 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 17:28:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Elezi", "Ismail", ""], ["Vascon", "Sebastiano", ""], ["Torcinovich", "Alessandro", ""], ["Pelillo", "Marcello", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1912.00403", "submitter": "Dean Zadok", "authors": "Dean Zadok, Daniel McDuff and Ashish Kapoor", "title": "Modeling Affect-based Intrinsic Rewards for Exploration and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive affect has been linked to increased interest, curiosity and\nsatisfaction in human learning. In reinforcement learning, extrinsic rewards\nare often sparse and difficult to define, intrinsically motivated learning can\nhelp address these challenges. We argue that positive affect is an important\nintrinsic reward that effectively helps drive exploration that is useful in\ngathering experiences. We present a novel approach leveraging a\ntask-independent reward function trained on spontaneous smile behavior that\nreflects the intrinsic reward of positive affect. To evaluate our approach we\ntrained several downstream computer vision tasks on data collected with our\npolicy and several baseline methods. We show that the policy based on our\naffective rewards successfully increases the duration of episodes, the area\nexplored and reduces collisions. The impact is the increased speed of learning\nfor several downstream computer vision tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 13:17:39 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 20:17:06 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 22:05:42 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 21:01:23 GMT"}, {"version": "v5", "created": "Wed, 12 Feb 2020 22:39:15 GMT"}, {"version": "v6", "created": "Mon, 2 Mar 2020 09:17:56 GMT"}, {"version": "v7", "created": "Sun, 4 Apr 2021 09:37:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zadok", "Dean", ""], ["McDuff", "Daniel", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1912.00411", "submitter": "Junlin Yang", "authors": "Junlin Yang, Nicha C. Dvornek, Fan Zhang, Julius Chapiro, MingDe Lin,\n  Aaron Abajian, James S. Duncan", "title": "Hepatocellular Carcinoma Intra-arterial Treatment Response Prediction\n  for Improved Therapeutic Decision-Making", "comments": "Accepted by NeurIPS workshop MED-NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a pipeline to predict treatment response to intra-arterial\ntherapy of patients with Hepatocellular Carcinoma (HCC) for improved\ntherapeutic decision-making. Our graph neural network model seamlessly combines\nheterogeneous inputs of baseline MR scans, pre-treatment clinical information,\nand planned treatment characteristics and has been validated on patients with\nHCC treated by transarterial chemoembolization (TACE). It achieves Accuracy of\n$0.713 \\pm 0.075$, F1 of $0.702 \\pm 0.082$ and AUC of $0.710 \\pm 0.108$. In\naddition, the pipeline incorporates uncertainty estimation to select hard cases\nand most align with the misclassified cases. The proposed pipeline arrives at\nmore informed intra-arterial therapeutic decisions for patients with HCC via\nimproving model accuracy and incorporating uncertainty estimation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:00:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Yang", "Junlin", ""], ["Dvornek", "Nicha C.", ""], ["Zhang", "Fan", ""], ["Chapiro", "Julius", ""], ["Lin", "MingDe", ""], ["Abajian", "Aaron", ""], ["Duncan", "James S.", ""]]}, {"id": "1912.00412", "submitter": "Eli Schwartz", "authors": "Sivan Doveh, Eli Schwartz, Chao Xue, Rogerio Feris, Alex Bronstein,\n  Raja Giryes, Leonid Karlinsky", "title": "MetAdapt: Meta-Learned Task-Adaptive Architecture for Few-Shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-Shot Learning (FSL) is a topic of rapidly growing interest. Typically, in\nFSL a model is trained on a dataset consisting of many small tasks (meta-tasks)\nand learns to adapt to novel tasks that it will encounter during test time.\nThis is also referred to as meta-learning. Another topic closely related to\nmeta-learning with a lot of interest in the community is Neural Architecture\nSearch (NAS), automatically finding optimal architecture instead of engineering\nit manually. In this work, we combine these two aspects of meta-learning. So\nfar, meta-learning FSL methods have focused on optimizing parameters of\npre-defined network architectures, in order to make them easily adaptable to\nnovel tasks. Moreover, it was observed that, in general, larger architectures\nperform better than smaller ones up to a certain saturation point (where they\nstart to degrade due to over-fitting). However, little attention has been given\nto explicitly optimizing the architectures for FSL, nor to an adaptation of the\narchitecture at test time to particular novel tasks. In this work, we propose\nto employ tools inspired by the Differentiable Neural Architecture Search\n(D-NAS) literature in order to optimize the architecture for FSL without\nover-fitting. Additionally, to make the architecture task adaptive, we propose\nthe concept of `MetAdapt Controller' modules. These modules are added to the\nmodel and are meta-trained to predict the optimal network connections for a\ngiven novel task. Using the proposed approach we observe state-of-the-art\nresults on two popular few-shot benchmarks: miniImageNet and FC100.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:04:34 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 09:27:25 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 11:44:12 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Doveh", "Sivan", ""], ["Schwartz", "Eli", ""], ["Xue", "Chao", ""], ["Feris", "Rogerio", ""], ["Bronstein", "Alex", ""], ["Giryes", "Raja", ""], ["Karlinsky", "Leonid", ""]]}, {"id": "1912.00416", "submitter": "Keunhong Park", "authors": "Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox", "title": "LatentFusion: End-to-End Differentiable Reconstruction and Rendering for\n  Unseen Object Pose Estimation", "comments": "CVPR 2020, Project Page:\n  https://keunhong.com/publications/latentfusion/ , Video:\n  https://youtu.be/tlzcq1KYXd8 , Code: https://github.com/NVlabs/latentfusion .\n  We have added experiments for LINEMOD and have updated the experiments on\n  MOPED. We've also added more technical and implementation details to the\n  methods section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current 6D object pose estimation methods usually require a 3D model for each\nobject. These methods also require additional training in order to incorporate\nnew objects. As a result, they are difficult to scale to a large number of\nobjects and cannot be directly applied to unseen objects.\n  We propose a novel framework for 6D pose estimation of unseen objects. We\npresent a network that reconstructs a latent 3D representation of an object\nusing a small number of reference views at inference time. Our network is able\nto render the latent 3D representation from arbitrary views. Using this neural\nrenderer, we directly optimize for pose given an input image. By training our\nnetwork with a large number of 3D shapes for reconstruction and rendering, our\nnetwork generalizes well to unseen objects. We present a new dataset for unseen\nobject pose estimation--MOPED. We evaluate the performance of our method for\nunseen object pose estimation on MOPED as well as the ModelNet and LINEMOD\ndatasets. Our method performs competitively to supervised methods that are\ntrained on those objects. Code and data is available at\nhttps://keunhong.com/publications/latentfusion/.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:32:58 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 07:03:48 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 02:56:30 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Park", "Keunhong", ""], ["Mousavian", "Arsalan", ""], ["Xiang", "Yu", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.00418", "submitter": "Yang Wang", "authors": "Biao Qian, Yang Wang, Zhao Zhang, Richang Hong, Meng Wang, Ling Shao", "title": "Diversifying Inference Path Selection: Moving-Mobile-Network for\n  Landmark Recognition", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have largely benefited computer vision\ntasks. However, the high computational complexity limits their real-world\napplications. To this end, many methods have been proposed for efficient\nnetwork learning, and applications in portable mobile devices. In this paper,\nwe propose a novel \\underline{M}oving-\\underline{M}obile-\\underline{Net}work,\nnamed M$^2$Net, for landmark recognition, equipped each landmark image with\nlocated geographic information. We intuitively find that M$^2$Net can\nessentially promote the diversity of the inference path (selected blocks\nsubset) selection, so as to enhance the recognition accuracy. The above\nintuition is achieved by our proposed reward function with the input of\ngeo-location and landmarks. We also find that the performance of other portable\nnetworks can be improved via our architecture. We construct two landmark image\ndatasets, with each landmark associated with geographic information, over which\nwe conduct extensive experiments to demonstrate that M$^2$Net achieves improved\nrecognition accuracy with comparable complexity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:40:38 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Qian", "Biao", ""], ["Wang", "Yang", ""], ["Zhang", "Zhao", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""], ["Shao", "Ling", ""]]}, {"id": "1912.00420", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Yucheng Tang, Yunqiang Chen, Dashan Gao, Shizhong Han,\n  Shunxing Bao, Smita De, James G. Terry, Jeffrey J. Carr, Richard G. Abramson,\n  Bennett A. Landman", "title": "Stochastic tissue window normalization of deep learning on computed\n  tomography", "comments": null, "journal-ref": "Journal of Medical Imaging 6.4 (2019): 044005", "doi": "10.1117/1.JMI.6.4.044005", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue window filtering has been widely used in deep learning for computed\ntomography (CT) image analyses to improve training performance (e.g., soft\ntissue windows for abdominal CT). However, the effectiveness of tissue window\nnormalization is questionable since the generalizability of the trained model\nmight be further harmed, especially when such models are applied to new cohorts\nwith different CT reconstruction kernels, contrast mechanisms, dynamic\nvariations in the acquisition, and physiological changes. We evaluate the\neffectiveness of both with and without using soft tissue window normalization\non multisite CT cohorts. Moreover, we propose a stochastic tissue window\nnormalization (SWN) method to improve the generalizability of tissue window\nnormalization. Different from the random sampling, the SWN method centers the\nrandomization around the soft tissue window to maintain the specificity for\nabdominal organs. To evaluate the performance of different strategies, 80\ntraining and 453 validation and testing scans from six datasets are employed to\nperform multi-organ segmentation using standard 2D U-Net. The six datasets\ncover the scenarios, where the training and testing scans are from (1) same\nscanner and same population, (2) same CT contrast but different pathology, and\n(3) different CT contrast and pathology. The traditional soft tissue window and\nnonwindowed approaches achieved better performance on (1). The proposed SWN\nachieved general superior performance on (2) and (3) with statistical analyses,\nwhich offers better generalizability for a trained model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:48:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Huo", "Yuankai", ""], ["Tang", "Yucheng", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Han", "Shizhong", ""], ["Bao", "Shunxing", ""], ["De", "Smita", ""], ["Terry", "James G.", ""], ["Carr", "Jeffrey J.", ""], ["Abramson", "Richard G.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1912.00438", "submitter": "Senthil Yogamani", "authors": "Mohamed Ramzy, Hazem Rashed, Ahmad El Sallab and Senthil Yogamani", "title": "RST-MODNet: Real-time Spatio-temporal Moving Object Detection for\n  Autonomous Driving", "comments": "Accepted for presentation at NeurIPS 2019 Workshop on Machine\n  Learning for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving Object Detection (MOD) is a critical task for autonomous vehicles as\nmoving objects represent higher collision risk than static ones. The trajectory\nof the ego-vehicle is planned based on the future states of detected moving\nobjects. It is quite challenging as the ego-motion has to be modelled and\ncompensated to be able to understand the motion of the surrounding objects. In\nthis work, we propose a real-time end-to-end CNN architecture for MOD utilizing\nspatio-temporal context to improve robustness. We construct a novel time-aware\narchitecture exploiting temporal motion information embedded within sequential\nimages in addition to explicit motion maps using optical flow images.We\ndemonstrate the impact of our algorithm on KITTI dataset where we obtain an\nimprovement of 8% relative to the baselines. We compare our algorithm with\nstate-of-the-art methods and achieve competitive results on KITTI-Motion\ndataset in terms of accuracy at three times better run-time. The proposed\nalgorithm runs at 23 fps on a standard desktop GPU targeting deployment on\nembedded platforms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 16:14:59 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ramzy", "Mohamed", ""], ["Rashed", "Hazem", ""], ["Sallab", "Ahmad El", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1912.00439", "submitter": "Christian Sormann", "authors": "Andreas Kuhn (1), Christian Sormann (2), Mattia Rossi (1,3), Oliver\n  Erdler (1), Friedrich Fraundorfer (2) ((1) Sony Europe B.V., (2) Graz\n  University of Technology, (3) \\'Ecole Polytechnique F\\'ed\\'erale de Lausanne)", "title": "DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo\n  Reconstruction", "comments": "changes in V3: re-worked confidence prediction scheme, re-organized\n  text, updated experiments; changes in V2: a reference was updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have the potential to improve the quality of\nimage-based 3D reconstructions. However, the use of DNNs in the context of 3D\nreconstruction from large and high-resolution image datasets is still an open\nchallenge, due to memory and computational constraints. We propose a pipeline\nwhich takes advantage of DNNs to improve the quality of 3D reconstructions\nwhile being able to handle large and high-resolution datasets. In particular,\nwe propose a confidence prediction network explicitly tailored for Multi-View\nStereo (MVS) and we use it for both depth map outlier filtering and depth map\nrefinement within our pipeline, in order to improve the quality of the final 3D\nreconstructions. We train our confidence prediction network on (semi-)dense\nground truth depth maps from publicly available real world MVS datasets. With\nextensive experiments on popular benchmarks, we show that our overall pipeline\ncan produce state-of-the-art 3D reconstructions, both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 16:50:07 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 12:54:34 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 14:47:16 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Kuhn", "Andreas", ""], ["Sormann", "Christian", ""], ["Rossi", "Mattia", ""], ["Erdler", "Oliver", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "1912.00461", "submitter": "Abdullah Hamdi", "authors": "Abdullah Hamdi, Sara Rojas, Ali Thabet, Bernard Ghanem", "title": "AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds", "comments": "Presented at European conference on computer vision (ECCV), 2020. The\n  code is available at https://github.com/ajhamdi/AdvPC", "journal-ref": "ECCV 2020", "doi": "10.1007/978-3-030-58610-2_15", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks, in which\nimperceptible perturbations to their input lead to erroneous network\npredictions. This phenomenon has been extensively studied in the image domain,\nand has only recently been extended to 3D point clouds. In this work, we\npresent novel data-driven adversarial attacks against 3D point cloud networks.\nWe aim to address the following problems in current 3D point cloud adversarial\nattacks: they do not transfer well between different networks, and they are\neasy to defend against via simple statistical methods. To this extent, we\ndevelop a new point cloud attack (dubbed AdvPC) that exploits the input data\ndistribution by adding an adversarial loss, after Auto-Encoder reconstruction,\nto the objective it optimizes. AdvPC leads to perturbations that are resilient\nagainst current defenses, while remaining highly transferable compared to\nstate-of-the-art attacks. We test AdvPC using four popular point cloud\nnetworks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack\nincreases the attack success rate by up to 40% for those transferred to unseen\nnetworks (transferability), while maintaining a high success rate on the\nattacked network. AdvPC also increases the ability to break defenses by up to\n38% as compared to other baselines on the ModelNet40 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:13:23 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 12:16:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hamdi", "Abdullah", ""], ["Rojas", "Sara", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1912.00466", "submitter": "Nupur Kumari", "authors": "Tejus Gupta, Abhishek Sinha, Nupur Kumari, Mayank Singh, Balaji\n  Krishnamurthy", "title": "A Method for Computing Class-wise Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for computing class-specific universal adversarial\nperturbations for deep neural networks. Such perturbations can induce\nmisclassification in a large fraction of images of a specific class. Unlike\nprevious methods that use iterative optimization for computing a universal\nperturbation, the proposed method employs a perturbation that is a linear\nfunction of weights of the neural network and hence can be computed much\nfaster. The method does not require any training data and has no\nhyper-parameters. The attack obtains 34% to 51% fooling rate on\nstate-of-the-art deep neural networks on ImageNet and transfers across models.\nWe also study the characteristics of the decision boundaries learned by\nstandard and adversarially trained models to understand the universal\nadversarial perturbations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:22:14 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gupta", "Tejus", ""], ["Sinha", "Abhishek", ""], ["Kumari", "Nupur", ""], ["Singh", "Mayank", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1912.00497", "submitter": "Himangi Mittal", "authors": "Himangi Mittal, Brian Okorn, David Held", "title": "Just Go with the Flow: Self-Supervised Scene Flow Estimation", "comments": "Accepted at CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When interacting with highly dynamic environments, scene flow allows\nautonomous systems to reason about the non-rigid motion of multiple independent\nobjects. This is of particular interest in the field of autonomous driving, in\nwhich many cars, people, bicycles, and other objects need to be accurately\ntracked. Current state-of-the-art methods require annotated scene flow data\nfrom autonomous driving scenes to train scene flow networks with supervised\nlearning. As an alternative, we present a method of training scene flow that\nuses two self-supervised losses, based on nearest neighbors and cycle\nconsistency. These self-supervised losses allow us to train our method on large\nunlabeled autonomous driving datasets; the resulting method matches current\nstate-of-the-art supervised performance using no real world annotations and\nexceeds state-of-the-art performance when combining our self-supervised\napproach with supervised learning on a smaller labeled dataset.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 20:32:54 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:10:57 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Mittal", "Himangi", ""], ["Okorn", "Brian", ""], ["Held", "David", ""]]}, {"id": "1912.00501", "submitter": "Himangi Mittal", "authors": "Himangi Mittal, Ajith Abraham, Anuja Arora", "title": "Interpreting Context of Images using Scene Graphs", "comments": "To appear in International Conference on Big Data Analytics (BDA2019)\n  (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a visual scene incorporates objects, relationships, and\ncontext. Traditional methods working on an image mostly focus on object\ndetection and fail to capture the relationship between the objects.\nRelationships can give rich semantic information about the objects in a scene.\nThe context can be conducive to comprehending an image since it will help us to\nperceive the relation between the objects and thus, give us a deeper insight\ninto the image. Through this idea, our project delivers a model that focuses on\nfinding the context present in an image by representing the image as a graph,\nwhere the nodes will the objects and edges will be the relation between them.\nThe context is found using the visual and semantic cues which are further\nconcatenated and given to the Support Vector Machines (SVM) to detect the\nrelation between two objects. This presents us with the context of the image\nwhich can be further used in applications such as similar image retrieval,\nimage captioning, or story generation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 21:32:11 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mittal", "Himangi", ""], ["Abraham", "Ajith", ""], ["Arora", "Anuja", ""]]}, {"id": "1912.00515", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose\n  Echevarria, Yun Fu", "title": "Texture Hallucination for Large-Factor Painting Super-Resolution", "comments": "Accepted to ECCV 2020. Supplementary material contains more visual\n  results and is available at\n  http://yulunzhang.com/papers/PaintingSR_supp_arXiv.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to super-resolve digital paintings, synthesizing realistic details\nfrom high-resolution reference painting materials for very large scaling\nfactors (e.g., 8X, 16X). However, previous single image super-resolution (SISR)\nmethods would either lose textural details or introduce unpleasing artifacts.\nOn the other hand, reference-based SR (Ref-SR) methods can transfer textures to\nsome extent, but is still impractical to handle very large factors and keep\nfidelity with original input. To solve these problems, we propose an efficient\nhigh-resolution hallucination network for very large scaling factors with an\nefficient network structure and feature transferring. To transfer more detailed\ntextures, we design a wavelet texture loss, which helps to enhance more\nhigh-frequency components. At the same time, to reduce the smoothing effect\nbrought by the image reconstruction loss, we further relax the reconstruction\nconstraint with a degradation loss which ensures the consistency between\ndownscaled super-resolution results and low-resolution inputs. We also\ncollected a high-resolution (e.g., 4K resolution) painting dataset PaintHD by\nconsidering both physical size and image resolution. We demonstrate the\neffectiveness of our method with extensive experiments on PaintHD by comparing\nwith SISR and Ref-SR state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:46:44 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 03:38:13 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 02:00:40 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 04:03:53 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zhang", "Yulun", ""], ["Zhang", "Zhifei", ""], ["DiVerdi", "Stephen", ""], ["Wang", "Zhaowen", ""], ["Echevarria", "Jose", ""], ["Fu", "Yun", ""]]}, {"id": "1912.00527", "submitter": "Xiru Zhu", "authors": "Xiru Zhu, Fengdi Che (Equal Contribution), Tianzi Yang, Tzuyang Yu,\n  David Meger, Gregory Dudek", "title": "Detecting GAN generated errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite an impressive performance from the latest GAN for generating\nhyper-realistic images, GAN discriminators have difficulty evaluating the\nquality of an individual generated sample. This is because the task of\nevaluating the quality of a generated image differs from deciding if an image\nis real or fake. A generated image could be perfect except in a single area but\nstill be detected as fake. Instead, we propose a novel approach for detecting\nwhere errors occur within a generated image. By collaging real images with\ngenerated images, we compute for each pixel, whether it belongs to the real\ndistribution or generated distribution. Furthermore, we leverage attention to\nmodel long-range dependency; this allows detection of errors which are\nreasonable locally but not holistically. For evaluation, we show that our error\ndetection can act as a quality metric for an individual image, unlike FID and\nIS. We leverage Improved Wasserstein, BigGAN, and StyleGAN to show a ranking\nbased on our metric correlates impressively with FID scores. Our work opens the\ndoor for better understanding of GAN and the ability to select the best samples\nfrom a GAN model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 00:24:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhu", "Xiru", "", "Equal Contribution"], ["Che", "Fengdi", "", "Equal Contribution"], ["Yang", "Tianzi", ""], ["Yu", "Tzuyang", ""], ["Meger", "David", ""], ["Dudek", "Gregory", ""]]}, {"id": "1912.00535", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Li Cheng, Hossein Ghanei-Yakhdan, and\n  Shohreh Kasaei", "title": "Deep Learning for Visual Tracking: A Comprehensive Survey", "comments": "Accepted Manuscript in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3046478", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual target tracking is one of the most sought-after yet challenging\nresearch topics in computer vision. Given the ill-posed nature of the problem\nand its popularity in a broad range of real-world scenarios, a number of\nlarge-scale benchmark datasets have been established, on which considerable\nmethods have been developed and demonstrated with significant progress in\nrecent years -- predominantly by recent deep learning (DL)-based methods. This\nsurvey aims to systematically investigate the current DL-based visual tracking\nmethods, benchmark datasets, and evaluation metrics. It also extensively\nevaluates and analyzes the leading visual tracking methods. First, the\nfundamental characteristics, primary motivations, and contributions of DL-based\nmethods are summarized from nine key aspects of: network architecture, network\nexploitation, network training for visual tracking, network objective, network\noutput, exploitation of correlation filter advantages, aerial-view tracking,\nlong-term tracking, and online tracking. Second, popular visual tracking\nbenchmarks and their respective properties are compared, and their evaluation\nmetrics are summarized. Third, the state-of-the-art DL-based methods are\ncomprehensively examined on a set of well-established benchmarks of OTB2013,\nOTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by\nconducting critical analyses of these state-of-the-art trackers quantitatively\nand qualitatively, their pros and cons under various common scenarios are\ninvestigated. It may serve as a gentle use guide for practitioners to weigh\nwhen and under what conditions to choose which method(s). It also facilitates a\ndiscussion on ongoing issues and sheds light on promising research directions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:05:54 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:05:50 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Cheng", "Li", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1912.00543", "submitter": "Eric Chen", "authors": "Puyang Wang, Eric Z. Chen, Terrence Chen, Vishal M. Patel, Shanhui Sun", "title": "Pyramid Convolutional RNN for MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate MRI image reconstruction from undersampled data is\ncritically important in clinical practice. Compressed sensing based methods are\nwidely used in image reconstruction but the speed is slow due to the iterative\nalgorithms. Deep learning based methods have shown promising advances in recent\nyears. However, recovering the fine details from highly undersampled data is\nstill challenging. In this paper, we introduce a novel deep learning-based\nmethod, Pyramid Convolutional RNN (PC-RNN), to reconstruct the image from\nmultiple scales. We evaluated our model on the fastMRI dataset and the results\nshow that the proposed model achieves significant improvements than other\nmethods and can recover more fine details.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:06:46 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:50:17 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 14:17:57 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 02:18:32 GMT"}, {"version": "v5", "created": "Mon, 27 Apr 2020 18:18:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Puyang", ""], ["Chen", "Eric Z.", ""], ["Chen", "Terrence", ""], ["Patel", "Vishal M.", ""], ["Sun", "Shanhui", ""]]}, {"id": "1912.00574", "submitter": "Zhaoyang Lyu", "authors": "Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, Luca\n  Daniel", "title": "Fastened CROWN: Tightened Neural Network Robustness Certificates", "comments": "Zhaoyang Lyu and Ching-Yun Ko contributed equally, accepted to AAAI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of deep learning applications in real life is accompanied by\nsevere safety concerns. To mitigate this uneasy phenomenon, much research has\nbeen done providing reliable evaluations of the fragility level in different\ndeep neural networks. Apart from devising adversarial attacks, quantifiers that\ncertify safeguarded regions have also been designed in the past five years. The\nsummarizing work of Salman et al. unifies a family of existing verifiers under\na convex relaxation framework. We draw inspiration from such work and further\ndemonstrate the optimality of deterministic CROWN (Zhang et al. 2018) solutions\nin a given linear programming problem under mild constraints. Given this\ntheoretical result, the computationally expensive linear programming based\nmethod is shown to be unnecessary. We then propose an optimization-based\napproach \\textit{FROWN} (\\textbf{F}astened C\\textbf{ROWN}): a general algorithm\nto tighten robustness certificates for neural networks. Extensive experiments\non various networks trained individually verify the effectiveness of FROWN in\nsafeguarding larger robust regions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:54:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Lyu", "Zhaoyang", ""], ["Ko", "Ching-Yun", ""], ["Kong", "Zhifeng", ""], ["Wong", "Ngai", ""], ["Lin", "Dahua", ""], ["Daniel", "Luca", ""]]}, {"id": "1912.00576", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Chhavi Dhiman, Dinesh Kumar Vishwakarma, Paras Aggarwal", "title": "Skeleton based Activity Recognition by Fusing Part-wise Spatio-temporal\n  and Attention Driven Residues", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist a wide range of intra class variations of the same actions and\ninter class similarity among the actions, at the same time, which makes the\naction recognition in videos very challenging. In this paper, we present a\nnovel skeleton-based part-wise Spatiotemporal CNN RIAC Network-based 3D human\naction recognition framework to visualise the action dynamics in part wise\nmanner and utilise each part for action recognition by applying weighted late\nfusion mechanism. Part wise skeleton based motion dynamics helps to highlight\nlocal features of the skeleton which is performed by partitioning the complete\nskeleton in five parts such as Head to Spine, Left Leg, Right Leg, Left Hand,\nRight Hand. The RIAFNet architecture is greatly inspired by the InceptionV4\narchitecture which unified the ResNet and Inception based Spatio-temporal\nfeature representation concept and achieving the highest top-1 accuracy till\ndate. To extract and learn salient features for action recognition, attention\ndriven residues are used which enhance the performance of residual components\nfor effective 3D skeleton-based Spatio-temporal action representation. The\nrobustness of the proposed framework is evaluated by performing extensive\nexperiments on three challenging datasets such as UT Kinect Action 3D, Florence\n3D action Dataset, and MSR Daily Action3D datasets, which consistently\ndemonstrate the superiority of our method\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 04:09:22 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Dhiman", "Chhavi", ""], ["Vishwakarma", "Dinesh Kumar", ""], ["Aggarwal", "Paras", ""]]}, {"id": "1912.00578", "submitter": "Shruti Bhargava", "authors": "Shruti Bhargava, David Forsyth", "title": "Exposing and Correcting the Gender Bias in Image Captioning Datasets and\n  Models", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of image captioning implicitly involves gender identification.\nHowever, due to the gender bias in data, gender identification by an image\ncaptioning model suffers. Also, the gender-activity bias, owing to the\nword-by-word prediction, influences other words in the caption prediction,\nresulting in the well-known problem of label bias. In this work, we investigate\ngender bias in the COCO captioning dataset and show that it engenders not only\nfrom the statistical distribution of genders with contexts but also from the\nflawed annotation by the human annotators. We look at the issues created by\nthis bias in the trained models. We propose a technique to get rid of the bias\nby splitting the task into 2 subtasks: gender-neutral image captioning and\ngender classification. By this decoupling, the gender-context influence can be\neradicated. We train the gender-neutral image captioning model, which gives\ncomparable results to a gendered model even when evaluating against a dataset\nthat possesses a similar bias as the training data. Interestingly, the\npredictions by this model on images with no humans, are also visibly different\nfrom the one trained on gendered captions. We train gender classifiers using\nthe available bounding box and mask-based annotations for the person in the\nimage. This allows us to get rid of the context and focus on the person to\npredict the gender. By substituting the genders into the gender-neutral\ncaptions, we get the final gendered predictions. Our predictions achieve\nsimilar performance to a model trained with gender, and at the same time are\ndevoid of gender bias. Finally, our main result is that on an\nanti-stereotypical dataset, our model outperforms a popular image captioning\nmodel which is trained with gender.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 04:14:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bhargava", "Shruti", ""], ["Forsyth", "David", ""]]}, {"id": "1912.00583", "submitter": "YeongHyeon Park", "authors": "YeongHyeon Park, Won Seok Park, and Yeong Beom Kim", "title": "Anomaly Detection in Particulate Matter Sensor using Hypothesis Pruning\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World Health Organization (WHO) provides the guideline for managing the\nParticulate Matter (PM) level because when the PM level is higher, it threats\nthe human health. For managing PM level, the procedure for measuring PM value\nis needed firstly. We use Tapered Element Oscillating Microbalance (TEOM)-based\nPM measuring sensors because it shows higher cost-effectiveness than Beta\nAttenuation Monitor (BAM)-based sensor. However, TEOM-based sensor has higher\nprobability of malfunctioning than BAM-based sensor. In this paper, we call the\noverall malfunction as an anomaly, and we aim to detect anomalies for the\nmaintenance of PM measuring sensors. We propose a novel architecture for\nsolving the above aim that named as Hypothesis Pruning Generative Adversarial\nNetwork (HP-GAN). We experimentally compare the several anomaly detection\narchitectures to certify ours performing better.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 05:43:20 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 01:30:31 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Park", "YeongHyeon", ""], ["Park", "Won Seok", ""], ["Kim", "Yeong Beom", ""]]}, {"id": "1912.00589", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai,\n  Ying Nian Wu", "title": "Flow Contrastive Estimation of Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a training method to jointly estimate an energy-based\nmodel and a flow-based model, in which the two models are iteratively updated\nbased on a shared adversarial value function. This joint training method has\nthe following traits. (1) The update of the energy-based model is based on\nnoise contrastive estimation, with the flow model serving as a strong noise\ndistribution. (2) The update of the flow model approximately minimizes the\nJensen-Shannon divergence between the flow model and the data distribution. (3)\nUnlike generative adversarial networks (GAN) which estimates an implicit\nprobability distribution defined by a generator model, our method estimates two\nexplicit probabilistic distributions on the data. Using the proposed method we\ndemonstrate a significant improvement on the synthesis quality of the flow\nmodel, and show the effectiveness of unsupervised feature learning by the\nlearned energy-based model. Furthermore, the proposed training method can be\neasily adapted to semi-supervised learning. We achieve competitive results to\nthe state-of-the-art semi-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 06:29:36 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:53:41 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gao", "Ruiqi", ""], ["Nijkamp", "Erik", ""], ["Kingma", "Diederik P.", ""], ["Xu", "Zhen", ""], ["Dai", "Andrew M.", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1912.00596", "submitter": "Samuel Earp", "authors": "Samuel W. F. Earp, Pavit Noinongyao, Justin A. Cairns and Ankush\n  Ganguly", "title": "Face Detection with Feature Pyramids and Landmarks", "comments": "12 pages, 2 figures, whitepaper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate face detection and facial landmark localization are crucial to any\nface recognition system. We present a series of three single-stage RCNNs with\ndifferent sized backbones (MobileNetV2-25, MobileNetV2-100, and ResNet101) and\na six-layer feature pyramid trained exclusively on the WIDER FACE dataset. We\ncompare the face detection and landmark accuracies using eight context module\narchitectures, four proposed by previous research and four modified versions.\nWe find no evidence that any of the proposed architectures significantly\noverperform and postulate that the random initialization of the additional\nlayers is at least of equal importance. To show this we present a model that\nachieves near state-of-the-art performance on WIDER FACE and also provides high\naccuracy landmarks with a simple context module. We also present results using\nMobileNetV2 backbones, which achieve over 90% average precision on the WIDER\nFACE hard validation set while being able to run in real-time. By comparing to\nother authors, we show that our models exceed the state-of-the-art for\nsimilar-sized RCNNs and match the performance of much heavier networks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 06:54:13 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 03:39:25 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Earp", "Samuel W. F.", ""], ["Noinongyao", "Pavit", ""], ["Cairns", "Justin A.", ""], ["Ganguly", "Ankush", ""]]}, {"id": "1912.00597", "submitter": "Qintao Hu", "authors": "Qintao Hu, Lijun Zhou, Xiaoxiao Wang, Yao Mao, Jianlin Zhang, Qixiang\n  Ye", "title": "SPSTracker: Sub-Peak Suppression of Response Map for Robust Object\n  Tracking", "comments": "Accepted as oral paper at AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern visual trackers usually construct online learning models under the\nassumption that the feature response has a Gaussian distribution with\ntarget-centered peak response. Nevertheless, such an assumption is implausible\nwhen there is progressive interference from other targets and/or background\nnoise, which produce sub-peaks on the tracking response map and cause model\ndrift. In this paper, we propose a rectified online learning approach for\nsub-peak response suppression and peak response enforcement and target at\nhandling progressive interference in a systematic way. Our approach, referred\nto as SPSTracker, applies simple-yet-efficient Peak Response Pooling (PRP) to\naggregate and align discriminative features, as well as leveraging a Boundary\nResponse Truncation (BRT) to reduce the variance of feature response. By fusing\nwith multi-scale features, SPSTracker aggregates the response distribution of\nmultiple sub-peaks to a single maximum peak, which enforces the discriminative\ncapability of features for robust object tracking. Experiments on the OTB, NFS\nand VOT2018 benchmarks demonstrate that SPSTrack outperforms the\nstate-of-the-art real-time trackers with significant margins.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:02:32 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 04:11:26 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 14:21:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hu", "Qintao", ""], ["Zhou", "Lijun", ""], ["Wang", "Xiaoxiao", ""], ["Mao", "Yao", ""], ["Zhang", "Jianlin", ""], ["Ye", "Qixiang", ""]]}, {"id": "1912.00606", "submitter": "Sivan Doveh", "authors": "Sivan Doveh, Raja Giryes", "title": "DEGAS: Differentiable Efficient Generator Search", "comments": "appeared at NeurIPS 2019, Meta-Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network architecture search (NAS) achieves state-of-the-art results in\nvarious tasks such as classification and semantic segmentation. Recently, a\nreinforcement learning-based approach has been proposed for Generative\nAdversarial Networks (GANs) search. In this work, we propose an alternative\nstrategy for GAN search by using a method called DEGAS (Differentiable\nEfficient GenerAtor Search), which focuses on efficiently finding the generator\nin the GAN. Our search algorithm is inspired by the differential architecture\nsearch strategy and the Global Latent Optimization (GLO) procedure. This leads\nto both an efficient and stable GAN search. After the generator architecture is\nfound, it can be plugged into any existing framework for GAN training. For\nCTGAN, which we use in this work, the new model outperforms the original\ninception score results by 0.25 for CIFAR-10 and 0.77 for STL. It also gets\nbetter results than the RL based GAN search methods in shorter search time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:30:28 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 12:16:46 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 08:01:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Doveh", "Sivan", ""], ["Giryes", "Raja", ""]]}, {"id": "1912.00622", "submitter": "Xiaohan Yu", "authors": "Xiaohan Yu, Yang Zhao, Yongsheng Gao, Shengwu Xiong, Xiaohui Yuan", "title": "Patchy Image Structure Classification Using Multi-Orientation Region\n  Transform", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exterior contour and interior structure are both vital features for\nclassifying objects. However, most of the existing methods consider exterior\ncontour feature and internal structure feature separately, and thus fail to\nfunction when classifying patchy image structures that have similar contours\nand flexible structures. To address above limitations, this paper proposes a\nnovel Multi-Orientation Region Transform (MORT), which can effectively\ncharacterize both contour and structure features simultaneously, for patchy\nimage structure classification. MORT is performed over multiple orientation\nregions at multiple scales to effectively integrate patchy features, and thus\nenables a better description of the shape in a coarse-to-fine manner. Moreover,\nthe proposed MORT can be extended to combine with the deep convolutional neural\nnetwork techniques, for further enhancement of classification accuracy. Very\nencouraging experimental results on the challenging ultra-fine-grained cultivar\nrecognition task, insect wing recognition task, and large variation butterfly\nrecognition task are obtained, which demonstrate the effectiveness and\nsuperiority of the proposed MORT over the state-of-the-art methods in\nclassifying patchy image structures. Our code and three patchy image structure\ndatasets are available at: https://github.com/XiaohanYu-GU/MReT2019.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:19:30 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Yu", "Xiaohan", ""], ["Zhao", "Yang", ""], ["Gao", "Yongsheng", ""], ["Xiong", "Shengwu", ""], ["Yuan", "Xiaohui", ""]]}, {"id": "1912.00623", "submitter": "Eric Brachmann", "authors": "Aritra Bhowmik, Stefan Gumhold, Carsten Rother, Eric Brachmann", "title": "Reinforced Feature Points: Optimizing Feature Detection and Description\n  for a High-Level Task", "comments": "CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a core problem of computer vision: Detection and description of 2D\nfeature points for image matching. For a long time, hand-crafted designs, like\nthe seminal SIFT algorithm, were unsurpassed in accuracy and efficiency.\nRecently, learned feature detectors emerged that implement detection and\ndescription using neural networks. Training these networks usually resorts to\noptimizing low-level matching scores, often pre-defining sets of image patches\nwhich should or should not match, or which should or should not contain key\npoints. Unfortunately, increased accuracy for these low-level matching scores\ndoes not necessarily translate to better performance in high-level vision\ntasks. We propose a new training methodology which embeds the feature detector\nin a complete vision pipeline, and where the learnable parameters are trained\nin an end-to-end fashion. We overcome the discrete nature of key point\nselection and descriptor matching using principles from reinforcement learning.\nAs an example, we address the task of relative pose estimation between a pair\nof images. We demonstrate that the accuracy of a state-of-the-art\nlearning-based feature detector can be increased when trained for the task it\nis supposed to solve at test time. Our training methodology poses little\nrestrictions on the task to learn, and works for any architecture which\npredicts key point heat maps, and descriptors for key point locations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:23:10 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 08:41:46 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Bhowmik", "Aritra", ""], ["Gumhold", "Stefan", ""], ["Rother", "Carsten", ""], ["Brachmann", "Eric", ""]]}, {"id": "1912.00632", "submitter": "Zi-Ming Liu", "authors": "Ziming Liu, Guangyu Gao, Lin Sun, Li Fang", "title": "IPG-Net: Image Pyramid Guidance Network for Small Object Detection", "comments": "Accepted by CVPR2020 Anti-UVA workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Convolutional Neural Network-based object detection, there is a typical\ndilemma: the spatial information is well kept in the shallow layers which\nunfortunately do not have enough semantic information, while the deep layers\nhave a high semantic concept but lost a lot of spatial information, resulting\nin serious information imbalance. To acquire enough semantic information for\nshallow layers, Feature Pyramid Networks (FPN) is used to build a top-down\npropagated path. In this paper, except for top-down combining of information\nfor shallow layers, we propose a novel network called Image Pyramid Guidance\nNetwork (IPG-Net) to make sure both the spatial information and semantic\ninformation are abundant for each layer. Our IPG-Net has two main parts: the\nimage pyramid guidance transformation module and the image pyramid guidance\nfusion module. Our main idea is to introduce the image pyramid guidance into\nthe backbone stream to solve the information imbalance problem, which\nalleviates the vanishment of the small object features. This IPG transformation\nmodule promises even in the deepest stage of the backbone, there is enough\nspatial information for bounding box regression and classification.\nFurthermore, we designed an effective fusion module to fuse the features from\nthe image pyramid and features from the backbone stream. We have tried to apply\nthis novel network to both one-stage and two-stage detection models, state of\nthe art results are obtained on the most popular benchmark data sets, i.e. MS\nCOCO and Pascal VOC.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:47:12 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 08:51:56 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 09:15:27 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Liu", "Ziming", ""], ["Gao", "Guangyu", ""], ["Sun", "Lin", ""], ["Fang", "Li", ""]]}, {"id": "1912.00649", "submitter": "Jungwoo Pyo", "authors": "Jungwoo Pyo, Joohyun Lee, Youngjune Park, Tien-Cuong Bui, Sang Kyun\n  Cha", "title": "An Attention-Based Speaker Naming Method for Online Adaptation in\n  Non-Fixed Scenarios", "comments": "AAAI 2020 Workshop on Interactive and Conversational Recommendation\n  Systems(WICRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A speaker naming task, which finds and identifies the active speaker in a\ncertain movie or drama scene, is crucial for dealing with high-level video\nanalysis applications such as automatic subtitle labeling and video\nsummarization. Modern approaches have usually exploited biometric features with\na gradient-based method instead of rule-based algorithms. In a certain\nsituation, however, a naive gradient-based method does not work efficiently.\nFor example, when new characters are added to the target identification list,\nthe neural network needs to be frequently retrained to identify new people and\nit causes delays in model preparation. In this paper, we present an\nattention-based method which reduces the model setup time by updating the newly\nadded data via online adaptation without a gradient update process. We\ncomparatively analyzed with three evaluation metrics(accuracy, memory usage,\nsetup time) of the attention-based method and existing gradient-based methods\nunder various controlled settings of speaker naming. Also, we applied existing\nspeaker naming models and the attention-based model to real video to prove that\nour approach shows comparable accuracy to the existing state-of-the-art models\nand even higher accuracy in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:30:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pyo", "Jungwoo", ""], ["Lee", "Joohyun", ""], ["Park", "Youngjune", ""], ["Bui", "Tien-Cuong", ""], ["Cha", "Sang Kyun", ""]]}, {"id": "1912.00660", "submitter": "Yanlin Qian", "authors": "Yanlin Qian and Alan Luke\\v{z}i\\v{c} and Matej Kristan and\n  Joni-Kristian K\\\"am\\\"ar\\\"ainen and Jiri Matas", "title": "DAL -- A Deep Depth-aware Long-term Tracker", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best RGBD trackers provide high accuracy but are slow to run. On the\nother hand, the best RGB trackers are fast but clearly inferior on the RGBD\ndatasets. In this work, we propose a deep depth-aware long-term tracker that\nachieves state-of-the-art RGBD tracking performance and is fast to run. We\nreformulate deep discriminative correlation filter (DCF) to embed the depth\ninformation into deep features. Moreover, the same depth-aware correlation\nfilter is used for target re-detection. Comprehensive evaluations show that the\nproposed tracker achieves state-of-the-art performance on the Princeton RGBD,\nSTC, and the newly-released CDTB benchmarks and runs 20 fps.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:02:40 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Qian", "Yanlin", ""], ["Luke\u017ei\u010d", "Alan", ""], ["Kristan", "Matej", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "1912.00664", "submitter": "Dmitry Slugin", "authors": "Igor Janiszewski, Dmitry Slugin, Vladimir V. Arlazarov", "title": "Training the Convolutional Neural Network with Statistical Dependence of\n  the Response on the Input Data Distortion", "comments": "Submitted and presented at The 12th International Conference on\n  Machine Vision (ICMV 2019). 8 pages, 7 figures, 14 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes an approach to training a convolutional neural network\nusing information on the level of distortion of input data. The learning\nprocess is modified with an additional layer, which is subsequently deleted, so\nthe architecture of the original network does not change. As an example, the\nLeNet5 architecture network with training data based on the MNIST symbols and a\ndistortion model as Gaussian blur with a variable level of distortion is\nconsidered. This approach does not have quality loss of the network and has a\nsignificant error-free zone in responses on the test data which is absent in\nthe traditional approach to training. The responses are statistically dependent\non the level of input image's distortions and there is a presence of a strong\nrelationship between them.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:09:21 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Janiszewski", "Igor", ""], ["Slugin", "Dmitry", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1912.00711", "submitter": "Angel Martinez-Gonzalez", "authors": "Angel Mart\\'inez-Gonz\\'alez, Michael Villamizar, Olivier Can\\'evet and\n  Jean-Marc Odobez", "title": "Efficient Convolutional Neural Networks for Depth-Based Multi-Person\n  Pose Estimation", "comments": "Published in IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO\n  TECHNOLOGY", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2952779", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving robust multi-person 2D body landmark localization and pose\nestimation is essential for human behavior and interaction understanding as\nencountered for instance in HRI settings. Accurate methods have been proposed\nrecently, but they usually rely on rather deep Convolutional Neural Network\n(CNN) architecture, thus requiring large computational and training resources.\nIn this paper, we investigate different architectures and methodologies to\naddress these issues and achieve fast and accurate multi-person 2D pose\nestimation. To foster speed, we propose to work with depth images, whose\nstructure contains sufficient information about body landmarks while being\nsimpler than textured color images and thus potentially requiring less complex\nCNNs for processing. In this context, we make the following contributions. i)\nwe study several CNN architecture designs combining pose machines relying on\nthe cascade of detectors concept with lightweight and efficient CNN structures;\nii) to address the need for large training datasets with high variability, we\nrely on semi-synthetic data combining multi-person synthetic depth data with\nreal sensor backgrounds; iii) we explore domain adaptation techniques to\naddress the performance gap introduced by testing on real depth images; iv) to\nincrease the accuracy of our fast lightweight CNN models, we investigate\nknowledge distillation at several architecture levels which effectively enhance\nperformance. Experiments and results on synthetic and real data highlight the\nimpact of our design choices, providing insights into methods addressing\nstandard issues normally faced in practical applications, and resulting in\narchitectures effectively matching our goal in both performance and speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:18:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mart\u00ednez-Gonz\u00e1lez", "Angel", ""], ["Villamizar", "Michael", ""], ["Can\u00e9vet", "Olivier", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1912.00756", "submitter": "Siming Zheng", "authors": "Siming Zheng, Rahmita Wirza O.K. Rahmat, Fatimah Khalid, Nurul Amelina\n  Nasharuddin", "title": "Learning scale-variant features for robust iris authentication with deep\n  learning based ensemble framework", "comments": "This is the second revision for updating the formation, logical and\n  image captions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, mobile Internet has accelerated the proliferation of smart\nmobile development. The mobile payment, mobile security and privacy protection\nhave become the focus of widespread attention. Iris recognition becomes a\nhigh-security authentication technology in these fields, it is widely used in\ndistinct science fields in biometric authentication fields. The Convolutional\nNeural Network (CNN) is one of the mainstream deep learning approaches for\nimage recognition, whereas its anti-noise ability is weak and needs a certain\namount of memory to train in image classification tasks. Under these conditions\nwe put forward a fine-tuning neural network model based on the Mask R-CNN and\nInception V4 neural network model, which integrates every component in an\noverall system that combines the iris detection, extraction, and recognition\nfunction as an iris recognition system. The proposed framework has the\ncharacteristics of scalability and high availability; it not only can learn\npart-whole relationships of the iris image but also enhancing the robustness of\nthe whole framework. Importantly, the proposed model can be trained using the\ndifferent spectrum of samples, such as Visible Wavelength (VW) and Near\nInfrared (NIR) iris biometric databases. The recognition average accuracy of\n99.10% is achieved while executing in the mobile edge calculation device of the\nJetson Nano.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:25:05 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 15:27:43 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zheng", "Siming", ""], ["Rahmat", "Rahmita Wirza O. K.", ""], ["Khalid", "Fatimah", ""], ["Nasharuddin", "Nurul Amelina", ""]]}, {"id": "1912.00790", "submitter": "Yusuke Sekikawa", "authors": "Yusuke Sekikawa and Teppei Suzuki", "title": "Tabulated MLP for Fast Point Feature Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at a drastic speedup for point-data embeddings at test time, we\npropose a new framework that uses a pair of multi-layer perceptron (MLP) and\nlook-up table (LUT) to transform point-coordinate inputs into high-dimensional\nfeatures. When compared with PointNet's feature embedding part realized by MLP\nthat requires millions of dot products, ours at test time requires no such\nlayers of matrix-vector products but requires only looking up the nearest\nentities followed by interpolation, from the tabulated MLP defined over\ndiscrete inputs on a 3D lattice. We call this framework as \"LUTI-MLP: LUT\nInterpolation MLP\" that provides a way to train end-to-end tabulated MLP\ncoupled to a LUT in a specific manner without the need for any approximation at\ntest time. LUTI-MLP also provides significant speedup for Jacobian computation\nof the embedding function wrt global pose coordinate on Lie algebra\n$\\mathfrak{se}(3)$ at test time, which could be used for point-set registration\nproblems. After extensive architectural analysis using ModelNet40 dataset, we\nconfirmed that our LUTI-MLP even with a small-sized table ($8\\times 8\\times 8$)\nyields performance comparable to that of MLP while achieving significant\nspeedup: $80\\times$ for embedding, $12\\times$ for approximate Jacobian, and\n$860\\times$ for canonical Jacobian.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 06:46:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sekikawa", "Yusuke", ""], ["Suzuki", "Teppei", ""]]}, {"id": "1912.00821", "submitter": "Ali Varamesh", "authors": "Ali Varamesh, Tinne Tuytelaars", "title": "Mixture Dense Regression for Object Detection and Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are well-established learning approaches that, in computer\nvision, have mostly been applied to inverse or ill-defined problems. However,\nthey are general-purpose divide-and-conquer techniques, splitting the input\nspace into relatively homogeneous subsets in a data-driven manner. Not only\nill-defined but also well-defined complex problems should benefit from them. To\nthis end, we devise a framework for spatial regression using mixture density\nnetworks. We realize the framework for object detection and human pose\nestimation. For both tasks, a mixture model yields higher accuracy and divides\nthe input space into interpretable modes. For object detection, mixture\ncomponents focus on object scale, with the distribution of components closely\nfollowing that of ground truth the object scale. This practically alleviates\nthe need for multi-scale testing, providing a superior speed-accuracy\ntrade-off. For human pose estimation, a mixture model divides the data based on\nviewpoint and uncertainty -- namely, front and back views, with back view\nimposing higher uncertainty. We conduct experiments on the MS COCO dataset and\ndo not face any mode collapse.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:31:52 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 01:20:13 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Varamesh", "Ali", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1912.00826", "submitter": "Qiujie Dong", "authors": "Qiujie Dong, Xuedong He, Haiyan Ge, Qin Liu, Aifu Han and Shengzong\n  Zhou", "title": "Improving Model Drift for Robust Object Tracking", "comments": "7 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters show excellent performance in object\ntracking. However, in complex scenes, the apparent characteristics of the\ntracked target are variable, which makes it easy to pollute the model and cause\nthe model drift. In this paper, considering that the secondary peak has a\ngreater impact on the model update, we propose a method for detecting the\nprimary and secondary peaks of the response map. Secondly, a novel confidence\nfunction which uses the adaptive update discriminant mechanism is proposed,\nwhich yield good robustness. Thirdly, we propose a robust tracker with\ncorrelation filters, which uses hand-crafted features and can improve model\ndrift in complex scenes. Finally, in order to cope with the current trackers'\nmulti-feature response merge, we propose a simple exponential adaptive merge\napproach. Extensive experiments are performed on OTB2013, OTB100 and TC128\ndatasets. Our approach performs superiorly against several state-of-the-art\ntrackers while runs at speed in real time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:41:51 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Dong", "Qiujie", ""], ["He", "Xuedong", ""], ["Ge", "Haiyan", ""], ["Liu", "Qin", ""], ["Han", "Aifu", ""], ["Zhou", "Shengzong", ""]]}, {"id": "1912.00830", "submitter": "Olga Taran", "authors": "Slava Voloshynovskiy, Mouad Kondah, Shideh Rezaeifar, Olga Taran,\n  Taras Holotyak and Danilo Jimenez Rezende", "title": "Information bottleneck through variational glasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information bottleneck (IB) principle [1] has become an important element in\ninformation-theoretic analysis of deep models. Many state-of-the-art generative\nmodels of both Variational Autoencoder (VAE) [2; 3] and Generative Adversarial\nNetworks (GAN) [4] families use various bounds on mutual information terms to\nintroduce certain regularization constraints [5; 6; 7; 8; 9; 10]. Accordingly,\nthe main difference between these models consists in add regularization\nconstraints and targeted objectives.\n  In this work, we will consider the IB framework for three classes of models\nthat include supervised, unsupervised and adversarial generative models. We\nwill apply a variational decomposition leading a common structure and allowing\neasily establish connections between these models and analyze underlying\nassumptions.\n  Based on these results, we focus our analysis on unsupervised setup and\nreconsider the VAE family. In particular, we present a new interpretation of\nVAE family based on the IB framework using a direct decomposition of mutual\ninformation terms and show some interesting connections to existing methods\nsuch as VAE [2; 3], beta-VAE [11], AAE [12], InfoVAE [5] and VAE/GAN [13].\nInstead of adding regularization constraints to an evidence lower bound (ELBO)\n[2; 3], which itself is a lower bound, we show that many known methods can be\nconsidered as a product of variational decomposition of mutual information\nterms in the IB framework. The proposed decomposition might also contribute to\nthe interpretability of generative models of both VAE and GAN families and\ncreate a new insights to a generative compression [14; 15; 16; 17]. It can also\nbe of interest for the analysis of novelty detection based on one-class\nclassifiers [18] with the IB based discriminators.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:52:29 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 17:15:52 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Voloshynovskiy", "Slava", ""], ["Kondah", "Mouad", ""], ["Rezaeifar", "Shideh", ""], ["Taran", "Olga", ""], ["Holotyak", "Taras", ""], ["Rezende", "Danilo Jimenez", ""]]}, {"id": "1912.00833", "submitter": "Xiaobo Wang", "authors": "Xiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu, Hailin Shi, Tao Mei", "title": "Mis-classified Vector Guided Softmax Loss for Face Recognition", "comments": "Accepted by AAAI2020 as Oral presentation. arXiv admin note:\n  substantial text overlap with arXiv:1812.11317", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has witnessed significant progress due to the advances of\ndeep convolutional neural networks (CNNs), the central task of which is how to\nimprove the feature discrimination. To this end, several margin-based\n(\\textit{e.g.}, angular, additive and additive angular margins) softmax loss\nfunctions have been proposed to increase the feature margin between different\nclasses. However, despite great achievements have been made, they mainly suffer\nfrom three issues: 1) Obviously, they ignore the importance of informative\nfeatures mining for discriminative learning; 2) They encourage the feature\nmargin only from the ground truth class, without realizing the discriminability\nfrom other non-ground truth classes; 3) The feature margin between different\nclasses is set to be same and fixed, which may not adapt the situations very\nwell. To cope with these issues, this paper develops a novel loss function,\nwhich adaptively emphasizes the mis-classified feature vectors to guide the\ndiscriminative feature learning. Thus we can address all the above issues and\nachieve more discriminative face features. To the best of our knowledge, this\nis the first attempt to inherit the advantages of feature margin and feature\nmining into a unified loss function. Experimental results on several benchmarks\nhave demonstrated the effectiveness of our method over state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 04:05:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Xiaobo", ""], ["Zhang", "Shifeng", ""], ["Wang", "Shuo", ""], ["Fu", "Tianyu", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""]]}, {"id": "1912.00858", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Bingkun Wei, Hongying Liu, Yuanyuan Liu and Jiacheng\n  Zhuo", "title": "Efficient Relaxed Gradient Support Pursuit for Sparsity Constrained\n  Non-convex Optimization", "comments": "7 pages, 3 figures, Appeared at the Data Science Meets Optimization\n  Workshop (DSO) at IJCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale non-convex sparsity-constrained problems have recently gained\nextensive attention. Most existing deterministic optimization methods (e.g.,\nGraSP) are not suitable for large-scale and high-dimensional problems, and thus\nstochastic optimization methods with hard thresholding (e.g., SVRGHT) become\nmore attractive. Inspired by GraSP, this paper proposes a new general relaxed\ngradient support pursuit (RGraSP) framework, in which the sub-algorithm only\nrequires to satisfy a slack descent condition. We also design two specific\nsemi-stochastic gradient hard thresholding algorithms. In particular, our\nalgorithms have much less hard thresholding operations than SVRGHT, and their\naverage per-iteration cost is much lower (i.e., O(d) vs. O(d log(d)) for\nSVRGHT), which leads to faster convergence. Our experimental results on both\nsynthetic and real-world datasets show that our algorithms are superior to the\nstate-of-the-art gradient hard thresholding methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:25:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Shang", "Fanhua", ""], ["Wei", "Bingkun", ""], ["Liu", "Hongying", ""], ["Liu", "Yuanyuan", ""], ["Zhuo", "Jiacheng", ""]]}, {"id": "1912.00869", "submitter": "Chun-Fu (Richard) Chen", "authors": "Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, David Cox", "title": "More Is Less: Learning Efficient Video Representations by Big-Little\n  Network and Depthwise Temporal Aggregation", "comments": "Accepted at NeurIPS 2019, codes and models are available at\n  https://github.com/IBM/bLVNet-TAM", "journal-ref": "Advances in Neural Information Processing Systems (Neurips 2019)", "doi": null, "report-no": "32", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art models for video action recognition are mostly based\non expensive 3D ConvNets. This results in a need for large GPU clusters to\ntrain and evaluate such architectures. To address this problem, we present a\nlightweight and memory-friendly architecture for action recognition that\nperforms on par with or better than current architectures by using only a\nfraction of resources. The proposed architecture is based on a combination of a\ndeep subnet operating on low-resolution frames with a compact subnet operating\non high-resolution frames, allowing for high efficiency and accuracy at the\nsame time. We demonstrate that our approach achieves a reduction by $3\\sim4$\ntimes in FLOPs and $\\sim2$ times in memory usage compared to the baseline. This\nenables training deeper models with more input frames under the same\ncomputational budget. To further obviate the need for large-scale 3D\nconvolutions, a temporal aggregation module is proposed to model temporal\ndependencies in a video at very small additional computational costs. Our\nmodels achieve strong performance on several action recognition benchmarks\nincluding Kinetics, Something-Something and Moments-in-time. The code and\nmodels are available at https://github.com/IBM/bLVNet-TAM.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:35:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fan", "Quanfu", ""], ["Chen", "Chun-Fu", ""], ["Kuehne", "Hilde", ""], ["Pistoia", "Marco", ""], ["Cox", "David", ""]]}, {"id": "1912.00943", "submitter": "Alireza Borjali", "authors": "Alireza Borjali, Antonia F. Chen, Orhun K. Muratoglu, Mohammad A.\n  Morid, Kartik M. Varadarajan", "title": "Detecting mechanical loosening of total hip replacement implant from\n  plain radiograph using deep convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plain radiography is widely used to detect mechanical loosening of total hip\nreplacement (THR) implants. Currently, radiographs are assessed manually by\nmedical professionals, which may be prone to poor inter and intra observer\nreliability and low accuracy. Furthermore, manual detection of mechanical\nloosening of THR implants requires experienced clinicians who might not always\nbe readily available, potentially resulting in delayed diagnosis. In this\nstudy, we present a novel, fully automatic and interpretable approach to detect\nmechanical loosening of THR implants from plain radiographs using deep\nconvolutional neural network (CNN). We trained a CNN on 40 patients\nanteroposterior hip x rays using five fold cross validation and compared its\nperformance with a high volume board certified orthopaedic surgeon (AFC). To\nincrease the confidence in the machine outcome, we also implemented saliency\nmaps to visualize where the CNN looked at to make a diagnosis. CNN outperformed\nthe orthopaedic surgeon in diagnosing mechanical loosening of THR implants\nachieving significantly higher sensitively (0.94) than the orthopaedic surgeon\n(0.53) with the same specificity (0.96). The saliency maps showed that the CNN\nlooked at clinically relevant features to make a diagnosis. Such CNNs can be\nused for automatic radiologic assessment of mechanical loosening of THR\nimplants to supplement the practitioners decision making process, increasing\ntheir diagnostic accuracy, and freeing them to engage in more patient centric\ncare.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:15:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Borjali", "Alireza", ""], ["Chen", "Antonia F.", ""], ["Muratoglu", "Orhun K.", ""], ["Morid", "Mohammad A.", ""], ["Varadarajan", "Kartik M.", ""]]}, {"id": "1912.00951", "submitter": "Sumeet Batra", "authors": "Sumeet Batra, John Klingner and Nikolaus Correll", "title": "Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic\n  Chemistry Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to register individual members of a robotic swarm in an\naugmented reality display while showing relevant information about swarm\ndynamics to the user that would be otherwise hidden. Individual swarm members\nand clusters of the same group are identified by their color, and by blinking\nat a specific time interval that is distinct from the time interval at which\ntheir neighbors blink. We show that this problem is an instance of the graph\ncoloring problem, which can be solved in a distributed manner in O(log(n))\ntime. We demonstrate our approach using a swarm chemistry simulation in which\nrobots simulate individual atoms that form molecules following the rules of\nchemistry. Augmented reality is then used to display information about the\ninternal state of individual swarm members as well as their topological\nrelationship, corresponding to molecular bonds.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:24:10 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Batra", "Sumeet", ""], ["Klingner", "John", ""], ["Correll", "Nikolaus", ""]]}, {"id": "1912.00957", "submitter": "Jessenia Gonzalez Villarreal", "authors": "Jessenia Gonzalez, Debjani Bhowmick, Cesar Beltran, Kris Sankaran,\n  Yoshua Bengio", "title": "Applying Knowledge Transfer for Water Body Segmentation in Peru", "comments": "5 pages, 3 figures, 1 table, NeurIPS 2019 Workshop on Machine\n  Learning for the Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the application of convolutional neural networks for\nsegmenting water bodies in satellite images. We first use a variant of the\nU-Net model to segment rivers and lakes from very high-resolution images from\nPeru. To circumvent the issue of scarce labelled data, we investigate the\napplicability of a knowledge transfer-based model that learns the mapping from\nhigh-resolution labelled images and combines it with the very high-resolution\nmapping so that better segmentation can be achieved. We train this model in a\nsingle process, end-to-end. Our preliminary results show that adding the\ninformation from the available high-resolution images does not help\nout-of-the-box, and in fact worsen results. This leads us to infer that the\nhigh-resolution data could be from a different distribution, and its addition\nleads to increased variance in our results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:41:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gonzalez", "Jessenia", ""], ["Bhowmick", "Debjani", ""], ["Beltran", "Cesar", ""], ["Sankaran", "Kris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1912.00969", "submitter": "Youtian Lin", "authors": "Youtian Lin and Pengming Feng and Jian Guan and Wenwu Wang and\n  Jonathon Chambers", "title": "IENet: Interacting Embranchment One Stage Anchor Free Detector for\n  Orientation Aerial Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in aerial images is a challenging task due to the lack of\nvisible features and variant orientation of objects. Significant progress has\nbeen made recently for predicting targets from aerial images with horizontal\nbounding boxes (HBBs) and oriented bounding boxes (OBBs) using two-stage\ndetectors with region based convolutional neural networks (R-CNN), involving\nobject localization in one stage and object classification in the other.\nHowever, the computational complexity in two-stage detectors is often high,\nespecially for orientational object detection, due to anchor matching and using\nregions of interest (RoI) pooling for feature extraction. In this paper, we\npropose a one-stage anchor free detector for orientational object detection,\nnamely, an interactive embranchment network (IENet), which is built upon a\ndetector with prediction in per-pixel fashion. First, a novel geometric\ntransformation is employed to better represent the oriented object in angle\nprediction, then a branch interactive module with a self-attention mechanism is\ndeveloped to fuse features from classification and box regression branches.\nFinally, we introduce an enhanced intersection over union (IoU) loss for OBB\ndetection, which is computationally more efficient than regular polygon IoU.\nExperiments conducted demonstrate the effectiveness and the superiority of our\nproposed method, as compared with state-of-the-art detectors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:02:30 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:58:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lin", "Youtian", ""], ["Feng", "Pengming", ""], ["Guan", "Jian", ""], ["Wang", "Wenwu", ""], ["Chambers", "Jonathon", ""]]}, {"id": "1912.00979", "submitter": "Yufan Zhou", "authors": "Yufan Zhou, Changyou Chen, Jinhui Xu", "title": "KernelNet: A Data-Dependent Kernel Parameterization for Deep Generative\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with kernels is an important concept in machine learning. Standard\napproaches for kernel methods often use predefined kernels that require careful\nselection of hyperparameters. To mitigate this burden, we propose in this paper\na framework to construct and learn a data-dependent kernel based on random\nfeatures and implicit spectral distributions that are parameterized by deep\nneural networks. The constructed network (called KernelNet) can be applied to\ndeep generative modeling in various scenarios, including two popular learning\nparadigms in deep generative models, MMD-GAN and implicit Variational\nAutoencoder (VAE). We show that our proposed kernel indeed exists in\napplications and is guaranteed to be positive definite. Furthermore, the\ninduced Maximum Mean Discrepancy (MMD) can endow the continuity property in\nweak topology by simple regularization. Extensive experiments indicate that our\nproposed KernelNet consistently achieves better performance compared to related\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:15:43 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 04:45:45 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 02:50:37 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhou", "Yufan", ""], ["Chen", "Changyou", ""], ["Xu", "Jinhui", ""]]}, {"id": "1912.00993", "submitter": "Pierre-Luc Delisle", "authors": "Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers,\n  Herve Lombaert", "title": "Adversarial normalization for multi domain image segmentation", "comments": "Submitted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image normalization is a critical step in medical imaging. This step is often\ndone on a per-dataset basis, preventing current segmentation algorithms from\nthe full potential of exploiting jointly normalized information across multiple\ndatasets. To solve this problem, we propose an adversarial normalization\napproach for image segmentation which learns common normalizing functions\nacross multiple datasets while retaining image realism. The adversarial\ntraining provides an optimal normalizer that improves both the segmentation\naccuracy and the discrimination of unrealistic normalizing functions. Our\ncontribution therefore leverages common imaging information from multiple\ndomains. The optimality of our common normalizer is evaluated by combining\nbrain images from both infants and adults. Results on the challenging iSEG and\nMRBrainS datasets reveal the potential of our adversarial normalization\napproach for segmentation, with Dice improvements of up to 59.6% over the\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:52:45 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 18:43:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Delisle", "Pierre-Luc", ""], ["Anctil-Robitaille", "Benoit", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "1912.00998", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer,\n  Philipp Kr\\\"ahenb\\\"uhl", "title": "A Multigrid Method for Efficiently Training Video Models", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training competitive deep video models is an order of magnitude slower than\ntraining their counterpart image models. Slow training causes long research\ncycles, which hinders progress in video understanding research. Following\nstandard practice for training image models, video model training assumes a\nfixed mini-batch shape: a specific number of clips, frames, and spatial size.\nHowever, what is the optimal shape? High resolution models perform well, but\ntrain slowly. Low resolution models train faster, but they are inaccurate.\nInspired by multigrid methods in numerical optimization, we propose to use\nvariable mini-batch shapes with different spatial-temporal resolutions that are\nvaried according to a schedule. The different shapes arise from resampling the\ntraining data on multiple sampling grids. Training is accelerated by scaling up\nthe mini-batch size and learning rate when shrinking the other dimensions. We\nempirically demonstrate a general and robust grid schedule that yields a\nsignificant out-of-the-box training speedup without a loss in accuracy for\ndifferent models (I3D, non-local, SlowFast), datasets (Kinetics,\nSomething-Something, Charades), and training settings (with and without\npre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed\nmultigrid method trains a ResNet-50 SlowFast network 4.5x faster (wall-clock\ntime, same hardware) while also improving accuracy (+0.8% absolute) on\nKinetics-400 compared to the baseline training method. Code is available\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:59:13 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 03:05:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Feichtenhofer", "Christoph", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1912.01001", "submitter": "Ting Liu", "authors": "Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff,\n  Hartwig Adam, Ting Liu", "title": "View-Invariant Probabilistic Embedding for Human Pose", "comments": "Accepted to ECCV 2020 (Spotlight presentation). Code is available at\n  https://github.com/google-research/google-research/tree/master/poem . Video\n  synchronization results are available at\n  https://drive.google.com/corp/drive/folders/1kTc_UT0Eq0H2ZBgfEoh8qEJMFBouC-Wv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depictions of similar human body configurations can vary with changing\nviewpoints. Using only 2D information, we would like to enable vision\nalgorithms to recognize similarity in human body poses across multiple views.\nThis ability is useful for analyzing body movements and human behaviors in\nimages and videos. In this paper, we propose an approach for learning a compact\nview-invariant embedding space from 2D joint keypoints alone, without\nexplicitly predicting 3D poses. Since 2D poses are projected from 3D space,\nthey have an inherent ambiguity, which is difficult to represent through a\ndeterministic mapping. Hence, we use probabilistic embeddings to model this\ninput uncertainty. Experimental results show that our embedding model achieves\nhigher accuracy when retrieving similar poses across different camera views, in\ncomparison with 2D-to-3D pose lifting models. We also demonstrate the\neffectiveness of applying our embeddings to view-invariant action recognition\nand video alignment. Our code is available at\nhttps://github.com/google-research/google-research/tree/master/poem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:59:56 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 20:25:47 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 18:28:47 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 19:52:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sun", "Jennifer J.", ""], ["Zhao", "Jiaping", ""], ["Chen", "Liang-Chieh", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Liu", "Ting", ""]]}, {"id": "1912.01054", "submitter": "Nicholas Heller", "authors": "Nicholas Heller, Fabian Isensee, Klaus H. Maier-Hein, Xiaoshuai Hou,\n  Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, Guang\n  Yao, Yaozong Gao, Yao Zhang, Yixin Wang, Feng Hou, Jiawei Yang, Guangwei\n  Xiong, Jiang Tian, Cheng Zhong, Jun Ma, Jack Rickman, Joshua Dean, Bethany\n  Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Heather Kaluzniak,\n  Shaneabbas Raza, Joel Rosenberg, Keenan Moore, Edward Walczak, Zachary\n  Rengel, Zach Edgerton, Ranveer Vasdev, Matthew Peterson, Sean McSweeney,\n  Sarah Peterson, Arveen Kalapara, Niranjan Sathianathen, Nikolaos\n  Papanikolopoulos, Christopher Weight", "title": "The state of the art in kidney and kidney tumor segmentation in\n  contrast-enhanced CT imaging: Results of the KiTS19 Challenge", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large body of literature linking anatomic and geometric\ncharacteristics of kidney tumors to perioperative and oncologic outcomes.\nSemantic segmentation of these tumors and their host kidneys is a promising\ntool for quantitatively characterizing these lesions, but its adoption is\nlimited due to the manual effort required to produce high-quality 3D\nsegmentations of these structures. Recently, methods based on deep learning\nhave shown excellent results in automatic 3D segmentation, but they require\nlarge datasets for training, and there remains little consensus on which\nmethods perform best. The 2019 Kidney and Kidney Tumor Segmentation challenge\n(KiTS19) was a competition held in conjunction with the 2019 International\nConference on Medical Image Computing and Computer Assisted Intervention\n(MICCAI) which sought to address these issues and stimulate progress on this\nautomatic segmentation problem. A training set of 210 cross sectional CT images\nwith kidney tumors was publicly released with corresponding semantic\nsegmentation masks. 106 teams from five continents used this data to develop\nautomated systems to predict the true segmentation masks on a test set of 90 CT\nimages for which the corresponding ground truth segmentations were kept\nprivate. These predictions were scored and ranked according to their average So\nrensen-Dice coefficient between the kidney and tumor across all 90 cases. The\nwinning team achieved a Dice of 0.974 for kidney and 0.851 for tumor,\napproaching the inter-annotator performance on kidney (0.983) but falling short\non tumor (0.923). This challenge has now entered an \"open leaderboard\" phase\nwhere it serves as a challenging benchmark in 3D semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:32:53 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 19:38:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Heller", "Nicholas", ""], ["Isensee", "Fabian", ""], ["Maier-Hein", "Klaus H.", ""], ["Hou", "Xiaoshuai", ""], ["Xie", "Chunmei", ""], ["Li", "Fengyi", ""], ["Nan", "Yang", ""], ["Mu", "Guangrui", ""], ["Lin", "Zhiyong", ""], ["Han", "Miofei", ""], ["Yao", "Guang", ""], ["Gao", "Yaozong", ""], ["Zhang", "Yao", ""], ["Wang", "Yixin", ""], ["Hou", "Feng", ""], ["Yang", "Jiawei", ""], ["Xiong", "Guangwei", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Ma", "Jun", ""], ["Rickman", "Jack", ""], ["Dean", "Joshua", ""], ["Stai", "Bethany", ""], ["Tejpaul", "Resha", ""], ["Oestreich", "Makinna", ""], ["Blake", "Paul", ""], ["Kaluzniak", "Heather", ""], ["Raza", "Shaneabbas", ""], ["Rosenberg", "Joel", ""], ["Moore", "Keenan", ""], ["Walczak", "Edward", ""], ["Rengel", "Zachary", ""], ["Edgerton", "Zach", ""], ["Vasdev", "Ranveer", ""], ["Peterson", "Matthew", ""], ["McSweeney", "Sean", ""], ["Peterson", "Sarah", ""], ["Kalapara", "Arveen", ""], ["Sathianathen", "Niranjan", ""], ["Papanikolopoulos", "Nikolaos", ""], ["Weight", "Christopher", ""]]}, {"id": "1912.01059", "submitter": "Patrick Wieschollek", "authors": "Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P.A. Lensch", "title": "GGNN: Graph-based GPU Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT and FAISS started to leverage\nthe massive parallelism offered by GPUs, GPU-based implementations are a\ncrucial resource for today's state-of-the-art ANN methods. While most of these\nmethods allow for faster queries, less emphasis is devoted to accelerate the\nconstruction of the underlying index structures. In this paper, we propose a\nnovel search structure based on nearest neighbor graphs and information\npropagation on graphs. Our method is designed to take advantage of GPU\narchitectures to accelerate the hierarchical building of the index structure\nand for performing the query. Empirical evaluation shows that GGNN\nsignificantly surpasses the state-of-the-art GPU- and CPU-based systems in\nterms of build-time, accuracy and search speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:46:13 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:15:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:49:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Groh", "Fabian", ""], ["Ruppert", "Lukas", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1912.01064", "submitter": "Maani Ghaffari Jadidi", "authors": "Xi Lin, Dingyi Sun, Tzu-Yuan Lin, Ryan M. Eustice, Maani Ghaffari", "title": "A Keyframe-based Continuous Visual SLAM for RGB-D Cameras via\n  Nonparametric Joint Geometric and Appearance Representation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a robust RGB-D SLAM system that performs well in\nscarcely textured and structured environments. We present a novel\nkeyframe-based continuous visual odometry that builds on the recently developed\ncontinuous sensor registration framework. A joint geometric and appearance\nrepresentation is the result of transforming the RGB-D images into functions\nthat live in a Reproducing Kernel Hilbert Space (RKHS). We solve both\nregistration and keyframe selection problems via the inner product structure\navailable in the RKHS. We also extend the proposed keyframe-based odometry\nmethod to a SLAM system using indirect ORB loop-closure constraints. The\nexperimental evaluations using publicly available RGB-D benchmarks show that\nthe developed keyframe selection technique using continuous visual odometry\noutperforms its robust dense (and direct) visual odometry equivalent. In\naddition, the developed SLAM system has better generalization across different\ntraining and validation sequences; it is robust to the lack of texture and\nstructure in the scene; and shows comparable performance with the\nstate-of-the-art SLAM systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:10:21 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lin", "Xi", ""], ["Sun", "Dingyi", ""], ["Lin", "Tzu-Yuan", ""], ["Eustice", "Ryan M.", ""], ["Ghaffari", "Maani", ""]]}, {"id": "1912.01067", "submitter": "Yu Guo", "authors": "Yu Guo, Milos Hasan, Lingqi Yan, Shuang Zhao", "title": "A Bayesian Inference Framework for Procedural Material Parameter\n  Estimation", "comments": "12 pages, 13 figures, Pacific Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural material models have been gaining traction in many applications\nthanks to their flexibility, compactness, and easy editability. We explore the\ninverse rendering problem of procedural material parameter estimation from\nphotographs, presenting a unified view of the problem in a Bayesian framework.\nIn addition to computing point estimates of the parameters by optimization, our\nframework uses a Markov Chain Monte Carlo approach to sample the space of\nplausible material parameters, providing a collection of plausible matches that\na user can choose from, and efficiently handling both discrete and continuous\nmodel parameters. To demonstrate the effectiveness of our framework, we fit\nprocedural models of a range of materials---wall plaster, leather, wood,\nanisotropic brushed metals and layered metallic paints---to both synthetic and\nreal target images.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:19:07 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 21:58:33 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 00:41:02 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 12:07:55 GMT"}, {"version": "v5", "created": "Wed, 4 Nov 2020 01:23:13 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Guo", "Yu", ""], ["Hasan", "Milos", ""], ["Yan", "Lingqi", ""], ["Zhao", "Shuang", ""]]}, {"id": "1912.01100", "submitter": "Vincenzo Lomonaco PhD", "authors": "Lorenzo Pellegrini, Gabriele Graffieti, Vincenzo Lomonaco, Davide\n  Maltoni", "title": "Latent Replay for Real-Time Continual Learning", "comments": "Pre-print v3: 13 pages, 9 figures, 10 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks at the edge on light computational devices,\nembedded systems and robotic platforms is nowadays very challenging. Continual\nlearning techniques, where complex models are incrementally trained on small\nbatches of new data, can make the learning problem tractable even for CPU-only\nembedded devices enabling remarkable levels of adaptiveness and autonomy.\nHowever, a number of practical problems need to be solved: catastrophic\nforgetting before anything else. In this paper we introduce an original\ntechnique named \"Latent Replay\" where, instead of storing a portion of past\ndata in the input space, we store activations volumes at some intermediate\nlayer. This can significantly reduce the computation and storage required by\nnative rehearsal. To keep the representation stable and the stored activations\nvalid we propose to slow-down learning at all the layers below the latent\nreplay one, leaving the layers above free to learn at full pace. In our\nexperiments we show that Latent Replay, combined with existing continual\nlearning techniques, achieves state-of-the-art performance on complex video\nbenchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.\nbatches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly\nreal-time continual learning on the edge through the deployment of the proposed\ntechnique on a smartphone device.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:16:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 09:50:32 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Pellegrini", "Lorenzo", ""], ["Graffieti", "Gabriele", ""], ["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""]]}, {"id": "1912.01101", "submitter": "Aaron Defazio", "authors": "Aaron Defazio", "title": "Offset Sampling Improves Deep Learning based Accelerated MRI\n  Reconstructions by Exploiting Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to accelerated MRI take a matrix of sampled\nFourier-space lines as input and produce a spatial image as output. In this\nwork we show that by careful choice of the offset used in the sampling\nprocedure, the symmetries in k-space can be better exploited, producing higher\nquality reconstructions than given by standard equally-spaced samples or\nrandomized samples motivated by compressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:29:26 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 17:06:58 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Defazio", "Aaron", ""]]}, {"id": "1912.01106", "submitter": "Bo Chen", "authors": "Bo Chen, Golnaz Ghiasi, Hanxiao Liu, Tsung-Yi Lin, Dmitry\n  Kalenichenko, Hartwig Adams, Quoc V. Le", "title": "MnasFPN: Learning Latency-aware Pyramid Architecture for Object\n  Detection on Mobile Devices", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the blooming success of architecture search for vision tasks in\nresource-constrained environments, the design of on-device object detection\narchitectures have mostly been manual. The few automated search efforts are\neither centered around non-mobile-friendly search spaces or not guided by\non-device latency. We propose MnasFPN, a mobile-friendly search space for the\ndetection head, and combine it with latency-aware architecture search to\nproduce efficient object detection models. The learned MnasFPN head, when\npaired with MobileNetV2 body, outperforms MobileNetV3+SSDLite by 1.8 mAP at\nsimilar latency on Pixel. It is also both 1.0 mAP more accurate and 10% faster\nthan NAS-FPNLite. Ablation studies show that the majority of the performance\ngain comes from innovations in the search space. Further explorations reveal an\ninteresting coupling between the search space design and the search algorithm,\nand that the complexity of MnasFPN search space may be at a local optimum.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:42:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 18:22:02 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Chen", "Bo", ""], ["Ghiasi", "Golnaz", ""], ["Liu", "Hanxiao", ""], ["Lin", "Tsung-Yi", ""], ["Kalenichenko", "Dmitry", ""], ["Adams", "Hartwig", ""], ["Le", "Quoc V.", ""]]}, {"id": "1912.01115", "submitter": "Karol Chlasta", "authors": "Karol Chlasta, Krzysztof Wo{\\l}k, Izabela Krejtz", "title": "Automated speech-based screening of depression using deep convolutional\n  neural networks", "comments": "10 pages, 8 figures and 2 tables, HCist 2019 - 8th International\n  Conference on Health and Social Care Information Systems and Technologies\n  (16-18 October 2019, Sousse, Tunisia)", "journal-ref": "Procedia Computer Science 164 (2019) 618-628", "doi": "10.1016/j.procs.2019.12.228", "report-no": null, "categories": "cs.LG cs.CV cs.CY cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and treatment of depression is essential in promoting\nremission, preventing relapse, and reducing the emotional burden of the\ndisease. Current diagnoses are primarily subjective, inconsistent across\nprofessionals, and expensive for individuals who may be in urgent need of help.\nThis paper proposes a novel approach to automated depression detection in\nspeech using convolutional neural network (CNN) and multipart interactive\ntraining. The model was tested using 2568 voice samples obtained from 77\nnon-depressed and 30 depressed individuals. In experiment conducted, data were\napplied to residual CNNs in the form of spectrograms, images auto-generated\nfrom audio samples. The experimental results obtained using different ResNet\narchitectures gave a promising baseline accuracy reaching 77%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:58:40 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Chlasta", "Karol", ""], ["Wo\u0142k", "Krzysztof", ""], ["Krejtz", "Izabela", ""]]}, {"id": "1912.01119", "submitter": "Khaled Jedoui", "authors": "Khaled Jedoui, Ranjay Krishna, Michael Bernstein and Li Fei-Fei", "title": "Deep Bayesian Active Learning for Multiple Correct Outputs", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical active learning strategies are designed for tasks, such as\nclassification, with the assumption that the output space is mutually\nexclusive. The assumption that these tasks always have exactly one correct\nanswer has resulted in the creation of numerous uncertainty-based measurements,\nsuch as entropy and least confidence, which operate over a model's outputs.\nUnfortunately, many real-world vision tasks, like visual question answering and\nimage captioning, have multiple correct answers, causing these measurements to\noverestimate uncertainty and sometimes perform worse than a random sampling\nbaseline. In this paper, we propose a new paradigm that estimates uncertainty\nin the model's internal hidden space instead of the model's output space. We\nspecifically study a manifestation of this problem for visual question answer\ngeneration (VQA), where the aim is not to classify the correct answer but to\nproduce a natural language answer, given an image and a question. Our method\novercomes the paraphrastic nature of language. It requires a semantic space\nthat structures the model's output concepts and that enables the usage of\ntechniques like dropout-based Bayesian uncertainty. We build a visual-semantic\nspace that embeds paraphrases close together for any existing VQA model. We\nempirically show state-of-art active learning results on the task of VQA on two\ndatasets, being 5 times more cost-efficient on Visual Genome and 3 times more\ncost-efficient on VQA 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:09:16 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 06:36:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jedoui", "Khaled", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1912.01127", "submitter": "Tianqi Liu", "authors": "Tianqi Liu and Qizhan Shao", "title": "BERT for Large-scale Video Segment Classification with Test-time\n  Augmentation", "comments": "ICCV 2019 YouTube8M workshop", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our approach to the third YouTube-8M video understanding\ncompetition that challenges par-ticipants to localize video-level labels at\nscale to the pre-cise time in the video where the label actually occurs.\nOurmodel is an ensemble of frame-level models such as GatedNetVLAD and NeXtVLAD\nand various BERT models withtest-time augmentation. We explore multiple ways to\nag-gregate BERT outputs as video representation and variousways to combine\nvisual and audio information. We proposetest-time augmentation as shifting\nvideo frames to one leftor right unit, which adds variety to the predictions\nand em-pirically shows improvement in evaluation metrics. We firstpre-train the\nmodel on the 4M training video-level data, andthen fine-tune the model on 237K\nannotated video segment-level data. We achieve MAP@100K 0.7871 on private\ntest-ing video segment data, which is ranked 9th over 283 teams.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:26:43 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Liu", "Tianqi", ""], ["Shao", "Qizhan", ""]]}, {"id": "1912.01137", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Mixing autoencoder with classifier: conceptual data visualization", "comments": null, "journal-ref": "IEEE Access, vol. 8, no. 1, pp. 105301-105310, 2020", "doi": "10.1109/ACCESS.2020.2999155", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, a neural network that is able to form a low dimensional\ntopological hidden representation is explained. The neural network can be\ntrained as an autoencoder, a classifier or mix of both, and produces different\nlow dimensional topological map for each of them. When it is trained as an\nautoencoder, the inherent topological structure of the data can be visualized,\nwhile when it is trained as a classifier, the topological structure is further\nconstrained by the concept, for example the labels the data, hence the\nvisualization is not only structural but also conceptual. The proposed neural\nnetwork significantly differ from many dimensional reduction models, primarily\nin its ability to execute both supervised and unsupervised dimensional\nreduction. The neural network allows multi perspective visualization of the\ndata, and thus giving more flexibility in data analysis. This paper is\nsupported by preliminary but intuitive visualization experiments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:33:26 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:46:11 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 10:27:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "1912.01148", "submitter": "Antonio Busson", "authors": "Eduardo Betine Bucker, Antonio Jos\\'e Grandson Busson, Ruy Luiz\n  Milidi\\'u, S\\'ergio Colcher, Bruno Pereira Dias, Andr\\'e Bulc\\~ao", "title": "A Deep Convolutional Network for Seismic Shot-Gather Image Quality\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning-based models such as Convolutional Neural Networks, have led to\nsignificant advancements in several areas of computing applications. Seismogram\nquality assurance is a relevant Geophysics task, since in the early stages of\nseismic processing, we are required to identify and fix noisy sail lines. In\nthis work, we introduce a real-world seismogram quality classification dataset\nbased on 6,613 examples, manually labeled by human experts as good, bad or\nugly, according to their noise intensity. This dataset is used to train a CNN\nclassifier for seismic shot-gathers quality prediction. In our empirical\nevaluation, we observe an F1-score of 93.56% in the test set.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 01:48:20 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bucker", "Eduardo Betine", ""], ["Busson", "Antonio Jos\u00e9 Grandson", ""], ["Milidi\u00fa", "Ruy Luiz", ""], ["Colcher", "S\u00e9rgio", ""], ["Dias", "Bruno Pereira", ""], ["Bulc\u00e3o", "Andr\u00e9", ""]]}, {"id": "1912.01158", "submitter": "Huangxing Lin", "authors": "Huangxing Lin, Weihong Zeng, Xinghao Ding, Xueyang Fu, Yue Huang and\n  John Paisley", "title": "Noise2Blur: Online Noise Extraction and Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework called Noise2Blur (N2B) for training robust image\ndenoising models without pre-collected paired noisy/clean images. The training\nof the model requires only some (or even one) noisy images, some random\nunpaired clean images, and noise-free but blurred labels obtained by predefined\nfiltering of the noisy images. The N2B model consists of two parts: a denoising\nnetwork and a noise extraction network. First, the noise extraction network\nlearns to output a noise map using the noise information from the denoising\nnetwork under the guidence of the blurred labels. Then, the noise map is added\nto a clean image to generate a new \"noisy/clean\" image pair. Using the new\nimage pair, the denoising network learns to generate clean and high-quality\nimages from noisy observations. These two networks are trained simultaneously\nand mutually aid each other to learn the mappings of noise to clean/blur.\nExperiments on several denoising tasks show that the denoising performance of\nN2B is close to that of other denoising CNNs trained with pre-collected paired\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:28:56 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 07:33:08 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lin", "Huangxing", ""], ["Zeng", "Weihong", ""], ["Ding", "Xinghao", ""], ["Fu", "Xueyang", ""], ["Huang", "Yue", ""], ["Paisley", "John", ""]]}, {"id": "1912.01176", "submitter": "Canqun Xiang", "authors": "Canqun Xiang, Shishun Tian, Wenbin Zou and Chen Xu", "title": "SAIS: Single-stage Anchor-free Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet efficientinstance segmentation\napproach based on the single-stage anchor-free detector, termed SAIS. In our\napproach, the instancesegmentation task consists of two parallel subtasks which\nre-spectively predict the mask coefficients and the mask prototypes.Then,\ninstance masks are generated by linearly combining theprototypes with the mask\ncoefficients. To enhance the quality ofinstance mask, the information from\nregression and classificationis fused to predict the mask coefficients. In\naddition, center-aware target is designed to preserve the center coordination\nofeach instance, which achieves a stable improvement in instancesegmentation.\nThe experiment on MS COCO shows that SAISachieves the performance of the\nexiting state-of-the-art single-stage methods with a much less memory footpr\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:22:32 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Xiang", "Canqun", ""], ["Tian", "Shishun", ""], ["Zou", "Wenbin", ""], ["Xu", "Chen", ""]]}, {"id": "1912.01180", "submitter": "Yi Zhang", "authors": "Yi Zhang, Xinyue Wei, Weichao Qiu, Zihao Xiao, Gregory D. Hager and\n  Alan Yuille", "title": "RSA: Randomized Simulation as Augmentation for Robust Human Action\n  Recognition", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid growth in datasets for video activity, stable robust\nactivity recognition with neural networks remains challenging. This is in large\npart due to the explosion of possible variation in video -- including lighting\nchanges, object variation, movement variation, and changes in surrounding\ncontext. An alternative is to make use of simulation data, where all of these\nfactors can be artificially controlled. In this paper, we propose the\nRandomized Simulation as Augmentation (RSA) framework which augments real-world\ntraining data with synthetic data to improve the robustness of action\nrecognition networks. We generate large-scale synthetic datasets with\nrandomized nuisance factors. We show that training with such extra data, when\nappropriately constrained, can significantly improve the performance of the\nstate-of-the-art I3D networks or, conversely, reduce the number of labeled real\nvideos needed to achieve good performance. Experiments on two real-world\ndatasets NTU RGB+D and VIRAT demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:45:45 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhang", "Yi", ""], ["Wei", "Xinyue", ""], ["Qiu", "Weichao", ""], ["Xiao", "Zihao", ""], ["Hager", "Gregory D.", ""], ["Yuille", "Alan", ""]]}, {"id": "1912.01181", "submitter": "Xin Ma", "authors": "Xin Ma, Guorong Wu, Won Hwa Kim", "title": "Multi-resolution Graph Neural Network for Identifying Disease-specific\n  Variations in Brain Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution Neural Network (CNN) recently have been adopted in several\nneuroimaging studies for diagnosis capturing disease-specific changes in the\nbrain. While many of these methods are designed to work with images in $\\mathbb\nR^n$ exploiting regular structure of the domain, they are not well-suited to\nanalyze data with irregular structure such as brain connectivity. As there is\nsignificant interest in understanding the altered interactions between\ndifferent brain regions that lead to neuro-disorders, it is important to\ndevelop data-driven methods that work with a population of graph data for\ntraditional prediction tasks. In this regime, we propose a novel CNN-based\nframework with adaptive graph transforms to learn the most disease-relevant\nconnectome feature maps which have the highest discrimination power across\ndiagnostic categories. The backbone of our framework is a multi-resolution\nrepresentation of the graph matrix which is steered by a set of wavelet-like\ngraph transforms. In this context, our supervised graph learning framework\noutperforms conventional graph methods that predict diagnostic label only based\non the underlying individual graph. Our extensive experiments on two real\ndatasets of functional and structural brain networks show that our\nmulti-resolution framework achieves significantly higher accuracy, precision\nand recall in predicting diagnostic labels and identifying disease-specific\nbrain connectivities that are associated with brain disorders such as\nAttention-Deficit/Hyperactivity Disorder (ADHD) and Alzheimer's Disease (AD).\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:46:14 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ma", "Xin", ""], ["Wu", "Guorong", ""], ["Kim", "Won Hwa", ""]]}, {"id": "1912.01196", "submitter": "Sayed Mohammad Mostafavi I.", "authors": "S. Mohammad Mostafavi I., Jonghyun Choi and Kuk-Jin Yoon", "title": "Learning to Super Resolve Intensity Images from Events", "comments": "To appear in CVPR 2020 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An event camera detects per-pixel intensity difference and produces\nasynchronous event stream with low latency, high dynamic range, and low power\nconsumption. As a trade-off, the event camera has low spatial resolution. We\npropose an end-to-end network to reconstruct high resolution, high dynamic\nrange (HDR) images directly from the event stream. We evaluate our algorithm on\nboth simulated and real-world sequences and verify that it captures fine\ndetails of a scene and outperforms the combination of the state-of-the-art\nevent to image algorithms with the state-of-the-art super resolution schemes in\nmany quantitative measures by large margins. We further extend our method by\nusing the active sensor pixel (APS) frames or reconstructing images\niteratively.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:24:06 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 09:15:13 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 23:28:28 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["I.", "S. Mohammad Mostafavi", ""], ["Choi", "Jonghyun", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1912.01197", "submitter": "Zhao Kang", "authors": "Zhao Kang and Xiao Lu and Yiwei Lu and Chong Peng and Zenglin Xu", "title": "Structure Learning with Similarity Preserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging on the underlying low-dimensional structure of data, low-rank and\nsparse modeling approaches have achieved great success in a wide range of\napplications. However, in many applications the data can display structures\nbeyond simply being low-rank or sparse. Fully extracting and exploiting hidden\nstructure information in the data is always desirable and favorable. To reveal\nmore underlying effective manifold structure, in this paper, we explicitly\nmodel the data relation. Specifically, we propose a structure learning\nframework that retains the pairwise similarities between the data points.\nRather than just trying to reconstruct the original data based on\nself-expression, we also manage to reconstruct the kernel matrix, which\nfunctions as similarity preserving. Consequently, this technique is\nparticularly suitable for the class of learning problems that are sensitive to\nsample similarity, e.g., clustering and semisupervised classification. To take\nadvantage of representation power of deep neural network, a deep auto-encoder\narchitecture is further designed to implement our model. Extensive experiments\non benchmark data sets demonstrate that our proposed framework can consistently\nand significantly improve performance on both evaluation tasks. We conclude\nthat the quality of structure learning can be enhanced if similarity\ninformation is incorporated.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:25:08 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Lu", "Yiwei", ""], ["Peng", "Chong", ""], ["Xu", "Zenglin", ""]]}, {"id": "1912.01201", "submitter": "Zhao Kang", "authors": "Juncheng Lv and Zhao Kang and Boyu Wang and Luping Ji and Zenglin Xu", "title": "Multi-view Subspace Clustering via Partition Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering is an important approach to analyze multi-view data in\nan unsupervised way. Among various methods, the multi-view subspace clustering\napproach has gained increasing attention due to its encouraging performance.\nBasically, it integrates multi-view information into graphs, which are then fed\ninto spectral clustering algorithm for final result. However, its performance\nmay degrade due to noises existing in each individual view or inconsistency\nbetween heterogeneous features. Orthogonal to current work, we propose to fuse\nmulti-view information in a partition space, which enhances the robustness of\nMulti-view clustering. Specifically, we generate multiple partitions and\nintegrate them to find the shared partition. The proposed model unifies graph\nlearning, generation of basic partitions, and view weight learning. These three\ncomponents co-evolve towards better quality outputs. We have conducted\ncomprehensive experiments on benchmark datasets and our empirical results\nverify the effectiveness and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:48:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lv", "Juncheng", ""], ["Kang", "Zhao", ""], ["Wang", "Boyu", ""], ["Ji", "Luping", ""], ["Xu", "Zenglin", ""]]}, {"id": "1912.01202", "submitter": "Jie Li", "authors": "Rui Hou, Jie Li, Arjun Bhargava, Allan Raventos, Vitor Guizilini, Chao\n  Fang, Jerome Lynch, Adrien Gaidon", "title": "Real-Time Panoptic Segmentation from Dense Detections", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation is a complex full scene parsing task requiring\nsimultaneous instance and semantic segmentation at high resolution. Current\nstate-of-the-art approaches cannot run in real-time, and simplifying these\narchitectures to improve efficiency severely degrades their accuracy. In this\npaper, we propose a new single-shot panoptic segmentation network that\nleverages dense detections and a global self-attention mechanism to operate in\nreal-time with performance approaching the state of the art. We introduce a\nnovel parameter-free mask construction method that substantially reduces\ncomputational complexity by efficiently reusing information from the object\ndetection and semantic segmentation sub-tasks. The resulting network has a\nsimple data flow that does not require feature map re-sampling or clustering\npost-processing, enabling significant hardware acceleration. Our experiments on\nthe Cityscapes and COCO benchmarks show that our network works at 30 FPS on\n1024x2048 resolution, trading a 3% relative performance degradation from the\ncurrent state of the art for up to 440% faster inference.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:50:02 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 18:05:28 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 23:08:01 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hou", "Rui", ""], ["Li", "Jie", ""], ["Bhargava", "Arjun", ""], ["Raventos", "Allan", ""], ["Guizilini", "Vitor", ""], ["Fang", "Chao", ""], ["Lynch", "Jerome", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1912.01224", "submitter": "Jun Jia", "authors": "Jun Jia and Zhongpai Gao and Kang Chen and Menghan Hu and Guangtao\n  Zhai and Guodong Guo and Xiaokang Yang", "title": "Robust Invisible Hyperlinks in Physical Photographs Based on 3D\n  Rendering Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of multimedia and Internet, people are eager to obtain information\nfrom offline to online. Quick Response (QR) codes and digital watermarks help\nus access information quickly. However, QR codes look ugly and invisible\nwatermarks can be easily broken in physical photographs. Therefore, this paper\nproposes a novel method to embed hyperlinks into natural images, making the\nhyperlinks invisible for human eyes but detectable for mobile devices. Our\nmethod is an end-to-end neural network with an encoder to hide information and\na decoder to recover information. From original images to physical photographs,\ncamera imaging process will introduce a series of distortion such as noise,\nblur, and light. To train a robust decoder against the physical distortion from\nthe real world, a distortion network based on 3D rendering is inserted between\nthe encoder and the decoder to simulate the camera imaging process. Besides, in\norder to maintain the visual attraction of the image with hyperlinks, we\npropose a loss function based on just noticeable difference (JND) to supervise\nthe training of encoder. Experimental results show that our approach\noutperforms the previous method in both simulated and real situations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 07:30:49 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jia", "Jun", ""], ["Gao", "Zhongpai", ""], ["Chen", "Kang", ""], ["Hu", "Menghan", ""], ["Zhai", "Guangtao", ""], ["Guo", "Guodong", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1912.01230", "submitter": "Seokeon Choi", "authors": "Seokeon Choi, Sumin Lee, Youngeun Kim, Taekyung Kim, Changick Kim", "title": "Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared\n  Person Re-Identification", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-infrared person re-identification (VI-ReID) is an important task in\nnight-time surveillance applications, since visible cameras are difficult to\ncapture valid appearance information under poor illumination conditions.\nCompared to traditional person re-identification that handles only the\nintra-modality discrepancy, VI-ReID suffers from additional cross-modality\ndiscrepancy caused by different types of imaging systems. To reduce both intra-\nand cross-modality discrepancies, we propose a Hierarchical Cross-Modality\nDisentanglement (Hi-CMD) method, which automatically disentangles\nID-discriminative factors and ID-excluded factors from visible-thermal images.\nWe only use ID-discriminative factors for robust cross-modality matching\nwithout ID-excluded factors such as pose or illumination. To implement our\napproach, we introduce an ID-preserving person image generation network and a\nhierarchical feature learning module. Our generation network learns the\ndisentangled representation by generating a new cross-modality image with\ndifferent poses and illuminations while preserving a person's identity. At the\nsame time, the feature learning module enables our model to explicitly extract\nthe common ID-discriminative characteristic between visible-infrared images.\nExtensive experimental results demonstrate that our method outperforms the\nstate-of-the-art methods on two VI-ReID datasets. The source code is available\nat: https://github.com/bismex/HiCMD.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 07:46:47 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:23:49 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 20:34:26 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Choi", "Seokeon", ""], ["Lee", "Sumin", ""], ["Kim", "Youngeun", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1912.01237", "submitter": "Hyeong Gwon Hong", "authors": "Hyeong Gwon Hong, Pyunghwan Ahn, and Junmo Kim", "title": "EDAS: Efficient and Differentiable Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferrable neural architecture search can be viewed as a binary\noptimization problem where a single optimal path should be selected among\ncandidate paths in each edge within the repeated cell block of the directed a\ncyclic graph form. Recently, the field of differentiable architecture search\nattempts to relax the search problem continuously using a one-shot network that\ncombines all the candidate paths in search space. However, when the one-shot\nnetwork is pruned to the model in the discrete architecture space by the\nderivation algorithm, performance is significantly degraded to an almost random\nestimator. To reduce the quantization error from the heavy use of relaxation,\nwe only sample a single edge to relax the corresponding variable and clamp\nvariables in the other edges to zero or one. By this method, there is no\nperformance drop after pruning the one-shot network by derivation algorithm,\ndue to the preservation of the discrete nature of optimization variables during\nthe search. Furthermore, the minimization of relaxation degree allows searching\nin a deeper network to discover better performance with remarkable search cost\nreduction (0.125 GPU days) compared to previous methods. By adding several\nregularization methods that help explore within the search space, we could\nobtain the network with notable performances on CIFAR-10, CIFAR-100, and\nImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:13:28 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 10:10:04 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Hong", "Hyeong Gwon", ""], ["Ahn", "Pyunghwan", ""], ["Kim", "Junmo", ""]]}, {"id": "1912.01249", "submitter": "Dvir Ginzburg", "authors": "Dvir Ginzburg, Dan Raviv", "title": "Cyclic Functional Mapping: Self-supervised correspondence between\n  non-isometric deformable shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first utterly self-supervised network for dense correspondence\nmapping between non-isometric shapes. The task of alignment in non-Euclidean\ndomains is one of the most fundamental and crucial problems in computer vision.\nAs 3D scanners can generate highly complex and dense models, the mission of\nfinding dense mappings between those models is vital. The novelty of our\nsolution is based on a cyclic mapping between metric spaces, where the distance\nbetween a pair of points should remain invariant after the full cycle. As the\nsame learnable rules that generate the point-wise descriptors apply in both\ndirections, the network learns invariant structures without any labels while\ncoping with non-isometric deformations. We show here state-of-the-art-results\nby a large margin for a variety of tasks compared to known self-supervised and\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:11:25 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ginzburg", "Dvir", ""], ["Raviv", "Dan", ""]]}, {"id": "1912.01274", "submitter": "Matan Haroush", "authors": "Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry", "title": "The Knowledge Within: Methods for Data-Free Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an extensive amount of research has been focused on compressing and\naccelerating Deep Neural Networks (DNN). So far, high compression rate\nalgorithms require part of the training dataset for a low precision\ncalibration, or a fine-tuning process. However, this requirement is\nunacceptable when the data is unavailable or contains sensitive information, as\nin medical and biometric use-cases. We present three methods for generating\nsynthetic samples from trained models. Then, we demonstrate how these samples\ncan be used to calibrate and fine-tune quantized models without using any real\ndata in the process. Our best performing method has a negligible accuracy\ndegradation compared to the original training set. This method, which leverages\nintrinsic batch normalization layers' statistics of the trained model, can be\nused to evaluate data similarity. Our approach opens a path towards genuine\ndata-free model compression, alleviating the need for training data during\nmodel deployment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:01:51 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 19:00:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Haroush", "Matan", ""], ["Hubara", "Itay", ""], ["Hoffer", "Elad", ""], ["Soudry", "Daniel", ""]]}, {"id": "1912.01293", "submitter": "Ruiqi Wang", "authors": "R.Q. Wang, W.Z. Wang, D.Z. Zhao, G.H. Chen and D.S.Luo", "title": "Scene recognition based on DNN and game theory with its applications in\n  human-robot interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition model based on the DNN and game theory with its\napplications in human-robot interaction is proposed in this paper. The use of\ndeep learning methods in the field of scene recognition is still in its\ninfancy, but has become an important trend in the future. As the innovative\nidea of the paper, we propose the following novelties. (1) In this paper, the\nimage registration problem is transformed into a problem of minimum energy in\nMarkov Random Field to finalize the image pre-processing task. Game theory is\nused to find the optimal. (2) We select neighboring homogeneous sample features\nand the neighboring heterogeneous sample features for the extracted sample\nfeatures to build a triple and modify the traditional neural network to propose\nthe novel DNN for scene understanding. (3) The robot control is well combined\nto guide the robot vision for multiple tasks. The experiment is then conducted\nto validate the overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:54:38 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 14:32:09 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 09:05:28 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 11:03:24 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Wang", "R. Q.", ""], ["Wang", "W. Z.", ""], ["Zhao", "D. Z.", ""], ["Chen", "G. H.", ""], ["Luo", "D. S.", ""]]}, {"id": "1912.01300", "submitter": "Xinyang Jiang", "authors": "Zhihui Zhu, Xinyang Jiang, Feng Zheng, Xiaowei Guo, Feiyue Huang,\n  Weishi Zheng, Xing Sun", "title": "Viewpoint-Aware Loss with Angular Regularization for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although great progress in supervised person re-identification (Re-ID) has\nbeen made recently, due to the viewpoint variation of a person, Re-ID remains a\nmassive visual challenge. Most existing viewpoint-based person Re-ID methods\nproject images from each viewpoint into separated and unrelated sub-feature\nspaces. They only model the identity-level distribution inside an individual\nviewpoint but ignore the underlying relationship between different viewpoints.\nTo address this problem, we propose a novel approach, called\n\\textit{Viewpoint-Aware Loss with Angular Regularization }(\\textbf{VA-reID}).\nInstead of one subspace for each viewpoint, our method projects the feature\nfrom different viewpoints into a unified hypersphere and effectively models the\nfeature distribution on both the identity-level and the viewpoint-level. In\naddition, rather than modeling different viewpoints as hard labels used for\nconventional viewpoint classification, we introduce viewpoint-aware adaptive\nlabel smoothing regularization (VALSR) that assigns the adaptive soft label to\nfeature representation. VALSR can effectively solve the ambiguity of the\nviewpoint cluster label assignment. Extensive experiments on the Market1501 and\nDukeMTMC-reID datasets demonstrated that our method outperforms the\nstate-of-the-art supervised Re-ID methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:10:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhu", "Zhihui", ""], ["Jiang", "Xinyang", ""], ["Zheng", "Feng", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Zheng", "Weishi", ""], ["Sun", "Xing", ""]]}, {"id": "1912.01306", "submitter": "Mireille El Gheche", "authors": "Mattia Rossi, Mireille El Gheche, Andreas Kuhn, Pascal Frossard", "title": "Joint Graph-based Depth Refinement and Normal Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is an essential component in understanding the 3D geometry\nof a scene, with numerous applications in urban and indoor settings. These\nscenes are characterized by a prevalence of human made structures, which in\nmost of the cases, are either inherently piece-wise planar, or can be\napproximated as such. In these settings, we devise a novel depth refinement\nframework that aims at recovering the underlying piece-wise planarity of the\ninverse depth map. We formulate this task as an optimization problem involving\na data fidelity term that minimizes the distance to the input inverse depth\nmap, as well as a regularization that enforces a piece-wise planar solution. As\nfor the regularization term, we model the inverse depth map as a weighted graph\nbetween pixels. The proposed regularization is designed to estimate a plane\nautomatically at each pixel, without any need for an a priori estimation of the\nscene planes, and at the same time it encourages similar pixels to be assigned\nto the same plane. The resulting optimization problem is efficiently solved\nwith ADAM algorithm. Experiments show that our method leads to a significant\nimprovement in depth refinement, both visually and numerically, with respect to\nstate-of-the-art algorithms on Middlebury, KITTI and ETH3D multi-view stereo\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:26:32 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 09:20:12 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rossi", "Mattia", ""], ["Gheche", "Mireille El", ""], ["Kuhn", "Andreas", ""], ["Frossard", "Pascal", ""]]}, {"id": "1912.01320", "submitter": "Arren Glover", "authors": "Arren Glover, Alan B. Stokes, Steve Furber, Chiara Bartolozzi", "title": "ATIS + SpiNNaker: a Fully Event-based Visual Tracking Demonstration", "comments": "Presented at the Unconventional Sensing and Processing for Robotic\n  Visual Perception workshop at the 2018 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems. 2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Asynchronous Time-based Image Sensor (ATIS) and the Spiking Neural\nNetwork Architecture (SpiNNaker) are both neuromorphic technologies that\n\"unconventionally\" use binary spikes to represent information. The ATIS\nproduces spikes to represent the change in light falling on the sensor, and the\nSpiNNaker is a massively parallel computing platform that asynchronously sends\nspikes between cores for processing. In this demonstration we show these two\nhardware used together to perform a visual tracking task. We aim to show the\nhardware and software architecture that integrates the ATIS and SpiNNaker\ntogether in a robot middle-ware that makes processing agnostic to the platform\n(CPU or SpiNNaker). We also aim to describe the algorithm, why it is suitable\nfor the \"unconventional\" sensor and processing platform including the\nadvantages as well as challenges faced.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:46:54 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Glover", "Arren", ""], ["Stokes", "Alan B.", ""], ["Furber", "Steve", ""], ["Bartolozzi", "Chiara", ""]]}, {"id": "1912.01326", "submitter": "Adrien Deli\\`ege Mr", "authors": "Anthony Cioppa, Adrien Deli\\`ege, Silvio Giancola, Bernard Ghanem,\n  Marc Van Droogenbroeck, Rikke Gade, Thomas B. Moeslund", "title": "A Context-Aware Loss Function for Action Spotting in Soccer Videos", "comments": "Accepted for CVPR2020 main conference. This document contains 8 pages\n  + references + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video understanding, action spotting consists in temporally localizing\nhuman-induced events annotated with single timestamps. In this paper, we\npropose a novel loss function that specifically considers the temporal context\nnaturally present around each action, rather than focusing on the single\nannotated frame to spot. We benchmark our loss on a large dataset of soccer\nvideos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We\nshow the generalization capability of our loss for generic activity proposals\nand detection on ActivityNet, by spotting the beginning and the end of each\nactivity. Furthermore, we provide an extended ablation study and display\nchallenging cases for action spotting in soccer videos. Finally, we\nqualitatively illustrate how our loss induces a precise temporal understanding\nof actions and show how such semantic knowledge can be used for automatic\nhighlights generation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:59:55 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 17:08:43 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 13:53:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Cioppa", "Anthony", ""], ["Deli\u00e8ge", "Adrien", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""], ["Van Droogenbroeck", "Marc", ""], ["Gade", "Rikke", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1912.01349", "submitter": "Fengxiang Yang", "authors": "Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing Sun, Hao Cheng,\n  Xiaowei Guo, Feiyue Huang, Rongrong Ji and Shaozi Li", "title": "Asymmetric Co-Teaching for Unsupervised Cross Domain Person\n  Re-Identification", "comments": "Accepted by AAAi 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID), is a challenging task due to the high\nvariance within identity samples and imaging conditions. Although recent\nadvances in deep learning have achieved remarkable accuracy in settled scenes,\ni.e., source domain, few works can generalize well on the unseen target domain.\nOne popular solution is assigning unlabeled target images with pseudo labels by\nclustering, and then retraining the model. However, clustering methods tend to\nintroduce noisy labels and discard low confidence samples as outliers, which\nmay hinder the retraining process and thus limit the generalization ability. In\nthis study, we argue that by explicitly adding a sample filtering procedure\nafter the clustering, the mined examples can be much more efficiently used. To\nthis end, we design an asymmetric co-teaching framework, which resists noisy\nlabels by cooperating two models to select data with possibly clean labels for\neach other. Meanwhile, one of the models receives samples as pure as possible,\nwhile the other takes in samples as diverse as possible. This procedure\nencourages that the selected training samples can be both clean and\nmiscellaneous, and that the two models can promote each other iteratively.\nExtensive experiments show that the proposed framework can consistently benefit\nmost clustering-based methods, and boost the state-of-the-art adaptation\naccuracy. Our code is available at\nhttps://github.com/FlyingRoastDuck/ACT_AAAI20.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:00:34 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Yang", "Fengxiang", ""], ["Li", "Ke", ""], ["Zhong", "Zhun", ""], ["Luo", "Zhiming", ""], ["Sun", "Xing", ""], ["Cheng", "Hao", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Li", "Shaozi", ""]]}, {"id": "1912.01359", "submitter": "Sidney Pontes-Filho", "authors": "Sidney Pontes-Filho, Annelene Gulden Dahl, Stefano Nichele and Gustavo\n  Borges Moreno e Mello", "title": "A deep learning based tool for automatic brain extraction from\n  functional magnetic resonance images in rodents", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing skull artifacts from functional magnetic images (fMRI) is a well\nunderstood and frequently encountered problem. Because the fMRI field has grown\nmostly due to human studies, many new tools were developed to handle human\ndata. Nonetheless, these tools are not equally useful to handle the data\nderived from animal studies, especially from rodents. This represents a major\nproblem to the field because rodent studies generate larger datasets from\nlarger populations, which implies that preprocessing these images manually to\nremove the skull becomes a bottleneck in the data analysis pipeline. In this\nstudy, we address this problem by implementing a neural network based method\nthat uses a U-Net architecture to segment the brain area into a mask and\nremoving the skull and other tissues from the image. We demonstrate several\nstrategies to speed up the process of generating the training dataset using\nwatershedding and several strategies for data augmentation that allowed to\ntrain faster the U-Net to perform the segmentation. Finally, we deployed the\ntrained network freely available.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:35:03 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 00:11:09 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Pontes-Filho", "Sidney", ""], ["Dahl", "Annelene Gulden", ""], ["Nichele", "Stefano", ""], ["Mello", "Gustavo Borges Moreno e", ""]]}, {"id": "1912.01362", "submitter": "Jennifer Maier", "authors": "Jennifer Maier, Luis Carlos Rivera Monroy, Christopher Syben, Yejin\n  Jeon, Jang-Hwan Choi, Mary Elizabeth Hall, Marc Levenston, Garry Gold,\n  Rebecca Fahrig, Andreas Maier", "title": "Multi-Channel Volumetric Neural Network for Knee Cartilage Segmentation\n  in Cone-beam CT", "comments": "6 pages, accepted at BVM 2020", "journal-ref": null, "doi": "10.1007/978-3-658-29267-6_14", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing knee cartilage thickness and strain under load can help to further\nthe understanding of the effects of diseases like Osteoarthritis. A precise\nsegmentation of the cartilage is a necessary prerequisite for this analysis.\nThis segmentation task has mainly been addressed in Magnetic Resonance Imaging,\nand was rarely investigated on contrast-enhanced Computed Tomography, where\ncontrast agent visualizes the border between femoral and tibial cartilage. To\novercome the main drawback of manual segmentation, namely its high time\ninvestment, we propose to use a 3D Convolutional Neural Network for this task.\nThe presented architecture consists of a V-Net with SeLu activation, and a\nTversky loss function. Due to the high imbalance between very few cartilage\npixels and many background pixels, a high false positive rate is to be\nexpected. To reduce this rate, the two largest segmented point clouds are\nextracted using a connected component analysis, since they most likely\nrepresent the medial and lateral tibial cartilage surfaces. The resulting\nsegmentations are compared to manual segmentations, and achieve on average a\nrecall of 0.69, which confirms the feasibility of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:43:46 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Maier", "Jennifer", ""], ["Monroy", "Luis Carlos Rivera", ""], ["Syben", "Christopher", ""], ["Jeon", "Yejin", ""], ["Choi", "Jang-Hwan", ""], ["Hall", "Mary Elizabeth", ""], ["Levenston", "Marc", ""], ["Gold", "Garry", ""], ["Fahrig", "Rebecca", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.01369", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu, Ian Whalen, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman,\n  Wolfgang Banzhaf, Vishnu Naresh Boddeti", "title": "Multi-Objective Evolutionary Design of Deep Convolutional Neural\n  Networks for Image Classification", "comments": "Published in IEEE Transactions on Evolutionary Computation, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early advancements in convolutional neural networks (CNNs) architectures are\nprimarily driven by human expertise and by elaborate design processes.\nRecently, neural architecture search was proposed with the aim of automating\nthe network design process and generating task-dependent architectures. While\nexisting approaches have achieved competitive performance in image\nclassification, they are not well suited to problems where the computational\nbudget is limited for two reasons: (1) the obtained architectures are either\nsolely optimized for classification performance, or only for one deployment\nscenario; (2) the search process requires vast computational resources in most\napproaches. To overcome these limitations, we propose an evolutionary algorithm\nfor searching neural architectures under multiple objectives, such as\nclassification performance and floating-point operations (FLOPs). The proposed\nmethod addresses the first shortcoming by populating a set of architectures to\napproximate the entire Pareto frontier through genetic operations that\nrecombine and modify architectural components progressively. Our approach\nimproves computational efficiency by carefully down-scaling the architectures\nduring the search as well as reinforcing the patterns commonly shared among\npast successful architectures through Bayesian model learning. The integration\nof these two main contributions allows an efficient design of architectures\nthat are competitive and in most cases outperform both manually and\nautomatically designed architectures on benchmark image classification\ndatasets: CIFAR, ImageNet, and human chest X-ray. The flexibility provided from\nsimultaneously obtaining multiple architecture choices for different compute\nrequirements further differentiates our approach from other methods in the\nliterature. Code is available at https://github.com/mikelzc1990/nsganetv1\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:57:25 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 18:38:21 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 13:35:27 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lu", "Zhichao", ""], ["Whalen", "Ian", ""], ["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1912.01372", "submitter": "Jag Mohan Singh", "authors": "Jag Mohan Singh, Raghavendra Ramachandra, Kiran B. Raja, Christoph\n  Busch", "title": "Robust Morph-Detection at Automated Border Control Gate using Deep\n  Decomposed 3D Shape and Diffuse Reflectance", "comments": "This work was accepted in The 15th International Conference on SIGNAL\n  IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is widely employed in Automated Border Control (ABC) gates,\nwhich verify the face image on passport or electronic Machine Readable Travel\nDocument (eMTRD) against the captured image to confirm the identity of the\npassport holder. In this paper, we present a robust morph detection algorithm\nthat is based on differential morph detection. The proposed method decomposes\nthe bona fide image captured from the ABC gate and the digital face image\nextracted from the eMRTD into the diffuse reconstructed image and a quantized\nnormal map. The extracted features are further used to learn a linear\nclassifier (SVM) to detect a morphing attack based on the assessment of\ndifferences between the bona fide image from the ABC gate and the digital face\nimage extracted from the passport. Owing to the availability of multiple\ncameras within an ABC gate, we extend the proposed method to fuse the\nclassification scores to generate the final decision on morph-attack-detection.\nTo validate our proposed algorithm, we create a morph attack database with\noverall 588 images, where bona fide are captured in an indoor lighting\nenvironment with a Canon DSLR Camera with one sample per subject and\ncorrespondingly images from ABC gates. We benchmark our proposed method with\nthe existing state-of-the-art and can state that the new approach significantly\noutperforms previous approaches in the ABC gate scenario.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:02:40 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Singh", "Jag Mohan", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran B.", ""], ["Busch", "Christoph", ""]]}, {"id": "1912.01373", "submitter": "Sungkwon Choo", "authors": "Sungkwon Choo, Wonkyo Seo, Nam Ik Cho", "title": "Automatic Video Object Segmentation via Motion-Appearance-Stream Fusion\n  and Instance-aware Segmentation", "comments": "8+1 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for automatic video object segmentation based on\nthe fusion of motion stream, appearance stream, and instance-aware\nsegmentation. The proposed scheme consists of a two-stream fusion network and\nan instance segmentation network. The two-stream fusion network again consists\nof motion and appearance stream networks, which extract long-term temporal and\nspatial information, respectively. Unlike the existing two-stream fusion\nmethods, the proposed fusion network blends the two streams at the original\nresolution for obtaining accurate segmentation boundary. We develop a recurrent\nbidirectional multiscale structure with skip connection for the stream fusion\nnetwork to extract long-term temporal information. Also, the multiscale\nstructure enables to obtain the original resolution features at the end of the\nnetwork. As a result of two-stream fusion, we have a pixel-level probabilistic\nsegmentation map, which has higher values at the pixels belonging to the\nforeground object. By combining the probability of foreground map and\nobjectness score of instance segmentation mask, we finally obtain foreground\nsegmentation results for video sequences without any user intervention, i.e.,\nwe achieve successful automatic video segmentation. The proposed structure\nshows a state-of-the-art performance for automatic video object segmentation\ntask, and also achieves near semi-supervised performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:02:59 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Choo", "Sungkwon", ""], ["Seo", "Wonkyo", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1912.01394", "submitter": "Elahe Arani", "authors": "Elahe Arani, Shabbir Marzban, Andrei Pata and Bahram Zonooz", "title": "RGPNet: A Real-Time General Purpose Semantic Segmentation", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV, 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time general purpose semantic segmentation architecture,\nRGPNet, which achieves significant performance gain in complex environments.\nRGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor.\nThe adaptor helps preserve and refine the abstract concepts from multiple\nlevels of distributed representations between the encoder and decoder. It also\nfacilitates the gradient flow from deeper layers to shallower layers. Our\nexperiments demonstrate that RGPNet can generate segmentation results in\nreal-time with comparable accuracy to the state-of-the-art non-real-time heavy\nmodels. Moreover, towards green AI, we show that using an optimized\nlabel-relaxation technique with progressive resizing can reduce the training\ntime by up to 60% while preserving the performance. We conclude that RGPNet\nobtains a better speed-accuracy trade-off across multiple datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:24:55 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:19:38 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Arani", "Elahe", ""], ["Marzban", "Shabbir", ""], ["Pata", "Andrei", ""], ["Zonooz", "Bahram", ""]]}, {"id": "1912.01401", "submitter": "Anton Truov Mr.", "authors": "Anton Trusov and Elena Limonova", "title": "The Analysis of Projective Transformation Algorithms for Image\n  Recognition on Mobile Devices", "comments": "ICMV 2019", "journal-ref": null, "doi": "10.1117/12.2559732", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply commonly known methods of non-adaptive interpolation\n(nearest pixel, bilinear, B-spline, bicubic, Hermite spline) and sampling\n(point sampling, supersampling, mip-map pre-filtering, rip-map pre-filtering\nand FAST) to the problem of projective image transformation. We compare their\ncomputational complexity, describe their artifacts and than experimentally\nmeasure their quality and working time on mobile processor with ARM\narchitecture. Those methods were widely developed in the 90s and early 2000s,\nbut were not in an area of active research in resent years due to a lower need\nin computationally efficient algorithms. However, real-time mobile recognition\nsystems, which collect more and more attention, do not only require fast\nprojective transform methods, but also demand high quality images without\nartifacts. As a result, in this work we choose methods appropriate for those\nsystems, which allow to avoid artifacts, while preserving low computational\ncomplexity. Based on the experimental results for our setting they are bilinear\ninterpolation combined with either mip-map pre-filtering or FAST sampling, but\ncould be modified for specific use cases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:27:38 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Trusov", "Anton", ""], ["Limonova", "Elena", ""]]}, {"id": "1912.01408", "submitter": "Jag Mohan Singh", "authors": "Jag Mohan Singh, Sushma Venkatesh, Kiran B. Raja, Raghavendra\n  Ramachandra, Christoph Busch", "title": "Detecting Finger-Vein Presentation Attacks Using 3D Shape & Diffuse\n  Reflectance Decomposition", "comments": "This work was accepted in The 15th International Conference on SIGNAL\n  IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the high biometric performance, finger-vein recognition systems are\nvulnerable to presentation attacks (aka., spoofing attacks). In this paper, we\npresent a new and robust approach for detecting presentation attacks on\nfinger-vein biometric systems exploiting the 3D Shape (normal-map) and material\nproperties (diffuse-map) of the finger. Observing the normal-map and\ndiffuse-map exhibiting enhanced textural differences in comparison with the\noriginal finger-vein image, especially in the presence of varying illumination\nintensity, we propose to employ textural feature-descriptors on both of them\nindependently. The features are subsequently used to compute a separating\nhyper-plane using Support Vector Machine (SVM) classifiers for the features\ncomputed from normal-maps and diffuse-maps independently. Given the scores from\neach classifier for normal-map and diffuse-map, we propose sum-rule based score\nlevel fusion to make detection of such presentation attack more robust. To this\nend, we construct a new database of finger-vein images acquired using a custom\ncapture device with three inbuilt illuminations and validate the applicability\nof the proposed approach. The newly collected database consists of 936 images,\nwhich corresponds to 468 bona fide images and 468 artefact images. We establish\nthe superiority of the proposed approach by benchmarking it with classical\ntextural feature-descriptor applied directly on finger-vein images. The\nproposed approach outperforms the classical approaches by providing the Attack\nPresentation Classification Error Rate (APCER) & Bona fide Presentation\nClassification Error Rate (BPCER) of 0% compared to comparable traditional\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:31:49 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Singh", "Jag Mohan", ""], ["Venkatesh", "Sushma", ""], ["Raja", "Kiran B.", ""], ["Ramachandra", "Raghavendra", ""], ["Busch", "Christoph", ""]]}, {"id": "1912.01438", "submitter": "Zirui Wang", "authors": "Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor Adrian Prisacariu,\n  Min Chen", "title": "FlowNet3D++: Geometric Losses For Deep Scene Flow Estimation", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FlowNet3D++, a deep scene flow estimation network. Inspired by\nclassical methods, FlowNet3D++ incorporates geometric constraints in the form\nof point-to-plane distance and angular alignment between individual vectors in\nthe flow field, into FlowNet3D. We demonstrate that the addition of these\ngeometric loss terms improves the previous state-of-art FlowNet3D accuracy from\n57.85% to 63.43%. To further demonstrate the effectiveness of our geometric\nconstraints, we propose a benchmark for flow estimation on the task of dynamic\n3D reconstruction, thus providing a more holistic and practical measure of\nperformance than the breakdown of individual metrics previously used to\nevaluate scene flow. This is made possible through the contribution of a novel\npipeline to integrate point-based scene flow predictions into a global dense\nvolume. FlowNet3D++ achieves up to a 15.0% reduction in reconstruction error\nover FlowNet3D, and up to a 35.2% improvement over KillingFusion alone. We will\nrelease our scene flow estimation code later.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:53:56 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 11:16:06 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 14:00:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Zirui", ""], ["Li", "Shuda", ""], ["Howard-Jenkins", "Henry", ""], ["Prisacariu", "Victor Adrian", ""], ["Chen", "Min", ""]]}, {"id": "1912.01447", "submitter": "Xu Shen", "authors": "Xu Shen, Xinmei Tian, Anfeng He, Shaoyan Sun, Dacheng Tao", "title": "Transform-Invariant Convolutional Neural Networks for Image\n  Classification and Search", "comments": "Accepted by ACM Multimedia. arXiv admin note: text overlap with\n  arXiv:1911.12682", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved state-of-the-art results\non many visual recognition tasks. However, current CNN models still exhibit a\npoor ability to be invariant to spatial transformations of images. Intuitively,\nwith sufficient layers and parameters, hierarchical combinations of convolution\n(matrix multiplication and non-linear activation) and pooling operations should\nbe able to learn a robust mapping from transformed input images to\ntransform-invariant representations. In this paper, we propose randomly\ntransforming (rotation, scale, and translation) feature maps of CNNs during the\ntraining stage. This prevents complex dependencies of specific rotation, scale,\nand translation levels of training images in CNN models. Rather, each\nconvolutional kernel learns to detect a feature that is generally helpful for\nproducing the transform-invariant answer given the combinatorially large\nvariety of transform levels of its input feature maps. In this way, we do not\nrequire any extra training supervision or modification to the optimization\nprocess and training images. We show that random transformation provides\nsignificant improvements of CNNs on many benchmark tasks, including small-scale\nimage recognition, large-scale image recognition, and image retrieval. The code\nis available at https://github.com/jasonustc/caffe-multigpu/tree/TICNN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:09:21 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["He", "Anfeng", ""], ["Sun", "Shaoyan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1912.01451", "submitter": "Richard Tomsett", "authors": "Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram,\n  Alun Preece", "title": "Sanity Checks for Saliency Metrics", "comments": "Accepted for publication at the Thirty Fourth AAAI conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps are a popular approach to creating post-hoc explanations of\nimage classifier outputs. These methods produce estimates of the relevance of\neach pixel to the classification output score, which can be displayed as a\nsaliency map that highlights important pixels. Despite a proliferation of such\nmethods, little effort has been made to quantify how good these saliency maps\nare at capturing the true relevance of the pixels to the classifier output\n(i.e. their \"fidelity\"). We therefore investigate existing metrics for\nevaluating the fidelity of saliency methods (i.e. saliency metrics). We find\nthat there is little consistency in the literature in how such metrics are\ncalculated, and show that such inconsistencies can have a significant effect on\nthe measured fidelity. Further, we apply measures of reliability developed in\nthe psychometric testing literature to assess the consistency of saliency\nmetrics when applied to individual saliency maps. Our results show that\nsaliency metrics can be statistically unreliable and inconsistent, indicating\nthat comparative rankings between saliency methods generated using such metrics\ncan be untrustworthy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:30:56 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Tomsett", "Richard", ""], ["Harborne", "Dan", ""], ["Chakraborty", "Supriyo", ""], ["Gurram", "Prudhvi", ""], ["Preece", "Alun", ""]]}, {"id": "1912.01452", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Modar Alfadly, Bernard Ghanem, Marcel Worring", "title": "Assessing the Robustness of Visual Question Answering", "comments": "24 pages, 13 figures, International Journal of Computer Vision (IJCV)\n  [under review]. arXiv admin note: substantial text overlap with\n  arXiv:1711.06232, arXiv:1709.04625", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been playing an essential role in the task of\nVisual Question Answering (VQA). Until recently, their accuracy has been the\nmain focus of research. Now there is a trend toward assessing the robustness of\nthese models against adversarial attacks by evaluating the accuracy of these\nmodels under increasing levels of noisiness in the inputs of VQA models. In\nVQA, the attack can target the image and/or the proposed query question, dubbed\nmain question, and yet there is a lack of proper analysis of this aspect of\nVQA. In this work, we propose a new method that uses semantically related\nquestions, dubbed basic questions, acting as noise to evaluate the robustness\nof VQA models. We hypothesize that as the similarity of a basic question to the\nmain question decreases, the level of noise increases. To generate a reasonable\nnoise level for a given main question, we rank a pool of basic questions based\non their similarity with this main question. We cast this ranking problem as a\nLASSO optimization problem. We also propose a novel robustness measure Rscore\nand two large-scale basic question datasets in order to standardize robustness\nanalysis of VQA models. The experimental results demonstrate that the proposed\nevaluation method is able to effectively analyze the robustness of VQA models.\nTo foster the VQA research, we will publish our proposed datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 09:32:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Alfadly", "Modar", ""], ["Ghanem", "Bernard", ""], ["Worring", "Marcel", ""]]}, {"id": "1912.01454", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, Salman Khan, Nick Barnes, Stephen Gould", "title": "Representation Learning on Unit Ball with 3D Roto-Translational\n  Equivariance", "comments": "arXiv admin note: text overlap with arXiv:1901.00616", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is an integral operation that defines how the shape of one\nfunction is modified by another function. This powerful concept forms the basis\nof hierarchical feature learning in deep neural networks. Although performing\nconvolution in Euclidean geometries is fairly straightforward, its extension to\nother topological spaces---such as a sphere ($\\mathbb{S}^2$) or a unit ball\n($\\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel\n`\\emph{volumetric convolution}' operation that can effectively model and\nconvolve arbitrary functions in $\\mathbb{B}^3$. We develop a theoretical\nframework for \\emph{volumetric convolution} based on Zernike polynomials and\nefficiently implement it as a differentiable and an easily pluggable layer in\ndeep networks. By construction, our formulation leads to the derivation of a\nnovel formula to measure the symmetry of a function in $\\mathbb{B}^3$ around an\narbitrary axis, that is useful in function analysis tasks. We demonstrate the\nefficacy of proposed volumetric convolution operation on one viable use case\ni.e., 3D object recognition.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:31:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""], ["Gould", "Stephen", ""]]}, {"id": "1912.01456", "submitter": "Kamran Ali", "authors": "Kamran Ali and Charles E. Hughes", "title": "Facial Expression Representation Learning by Synthesizing Expression\n  Images", "comments": "7 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1909.13135", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations used for Facial Expression Recognition (FER) usually contain\nexpression information along with identity features. In this paper, we propose\na novel Disentangled Expression learning-Generative Adversarial Network\n(DE-GAN) which combines the concept of disentangled representation learning\nwith residue learning to explicitly disentangle facial expression\nrepresentation from identity information. In this method the facial expression\nrepresentation is learned by reconstructing an expression image employing an\nencoder-decoder based generator. Unlike previous works using only expression\nresidual learning for facial expression recognition, our method learns the\ndisentangled expression representation along with the expressive component\nrecorded by the encoder of DE-GAN. In order to improve the quality of\nsynthesized expression images and the effectiveness of the learned disentangled\nexpression representation, expression and identity classification is performed\nby the discriminator of DE-GAN. Experiments performed on widely used datasets\n(CK+, MMI, Oulu-CASIA) show that the proposed technique produces comparable or\nbetter results than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 12:45:27 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ali", "Kamran", ""], ["Hughes", "Charles E.", ""]]}, {"id": "1912.01494", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu", "title": "Supervised and Unsupervised End-to-End Deep Learning for Gene Ontology\n  Classification of Neural In Situ Hybridization Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09663", "journal-ref": "Entropy, Vol. 21, No. 3, pp. 221-238, February 2019", "doi": "10.3390/e21030221", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large datasets of high-resolution mammalian neural images\nhave become available, which has prompted active research on the analysis of\ngene expression data. Traditional image processing methods are typically\napplied for learning functional representations of genes, based on their\nexpressions in these brain images. In this paper, we describe a novel\nend-to-end deep learning-based method for generating compact representations of\nin situ hybridization (ISH) images, which are invariant-to-translation. In\ncontrast to traditional image processing methods, our method relies, instead,\non deep convolutional denoising autoencoders (CDAE) for processing raw pixel\ninputs, and generating the desired compact image representations. We provide an\nin-depth description of our deep learning-based approach, and present extensive\nexperimental results, demonstrating that representations extracted by CDAE can\nhelp learn features of functional gene ontology categories for their\nclassification in a highly accurate manner. Our methods improve the previous\nstate-of-the-art classification rate (Liscovitch, et al.) from an average AUC\nof 0.92 to 0.997, i.e., it achieves 96% reduction in error rate. Furthermore,\nthe representation vectors generated due to our method are more compact in\ncomparison to previous state-of-the-art methods, allowing for a more efficient\nhigh-level representation of images. These results are obtained with\nsignificantly downsampled images in comparison to the original high-resolution\nones, further underscoring the robustness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:20:12 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.01521", "submitter": "Oren Barkan", "authors": "Oren Barkan", "title": "Multiscale Self Attentive Convolutions for Vision and Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self attention mechanisms have become a key building block in many\nstate-of-the-art language understanding models. In this paper, we show that the\nself attention operator can be formulated in terms of 1x1 convolution\noperations. Following this observation, we propose several novel operators:\nFirst, we introduce a 2D version of self attention that is applicable for 2D\nsignals such as images. Second, we present the 1D and 2D Self Attentive\nConvolutions (SAC) operator that generalizes self attention beyond 1x1\nconvolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self\nattention operate on individual words and pixels, SAC operates on m-grams and\nimage patches, respectively. Third, we present a multiscale version of SAC\n(MSAC) which analyzes the input by employing multiple SAC operators that vary\nby filter size, in parallel. Finally, we explain how MSAC can be utilized for\nvision and language modeling, and further harness MSAC to form a cross\nattentive image similarity machinery.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:51:09 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Barkan", "Oren", ""]]}, {"id": "1912.01522", "submitter": "Akhil Meethal", "authors": "Akhil Meethal and Marco Pedersoli and Soufiane Belharbi and Eric\n  Granger", "title": "Convolutional STN for Weakly Supervised Object Localization", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization is a challenging task in which the\nobject of interest should be localized while learning its appearance.\nState-of-the-art methods recycle the architecture of a standard CNN by using\nthe activation maps of the last layer for localizing the object. While this\napproach is simple and works relatively well, object localization relies on\ndifferent features than classification, thus, a specialized localization\nmechanism is required during training to improve performance. In this paper, we\npropose a convolutional, multi-scale spatial localization network that provides\naccurate localization for the object of interest. Experimental results on\nCUB-200-2011 and ImageNet datasets show that our proposed approach provides\ncompetitive performance for weakly supervised localization.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:51:11 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 23:16:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Meethal", "Akhil", ""], ["Pedersoli", "Marco", ""], ["Belharbi", "Soufiane", ""], ["Granger", "Eric", ""]]}, {"id": "1912.01526", "submitter": "Daniele Ravi", "authors": "Daniele Ravi, Stefano B. Blumberg, Silvia Ingala, Frederik Barkhof,\n  Daniel C. Alexander, Neil P. Oxtoby (for the Alzheimer's Disease Neuroimaging\n  Initiative)", "title": "Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations:\n  Application in Ageing and Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and realistic simulation of high-dimensional medical images has\nbecome an important research area relevant to many AI-enabled healthcare\napplications. However, current state-of-the-art approaches lack the ability to\nproduce satisfactory high-resolution and accurate subject-specific images. In\nthis work, we present a deep learning framework, namely 4D-Degenerative\nAdversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,\nlongitudinal MRI scans that mimic subject-specific neurodegeneration in ageing\nand dementia. 4D-DANI-Net is a modular framework based on adversarial training\nand a set of novel spatiotemporal, biologically-informed constraints. To ensure\nefficient training and overcome memory limitations affecting such\nhigh-dimensional problems, we rely on three key technological advances: i) a\nnew 3D training consistency mechanism called Profile Weight Functions (PWFs),\nii) a 3D super-resolution module and iii) a transfer learning strategy to\nfine-tune the system for a given individual. To evaluate our approach, we\ntrained the framework on 9852 T1-weighted MRI scans from 876 participants in\nthe Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate\ntest set of 1283 MRI scans from 170 participants for quantitative and\nqualitative assessment of the personalised time series of synthetic images. We\nperformed three evaluations: i) image quality assessment; ii) quantifying the\naccuracy of regional brain volumes over and above benchmark models; and iii)\nquantifying visual perception of the synthetic images by medical experts.\nOverall, both quantitative and qualitative results show that 4D-DANI-Net\nproduces realistic, low-artefact, personalised time series of synthetic T1 MRI\nthat outperforms benchmark models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:58:14 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 20:24:27 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 18:56:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ravi", "Daniele", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Blumberg", "Stefano B.", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Ingala", "Silvia", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Barkhof", "Frederik", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Alexander", "Daniel C.", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"], ["Oxtoby", "Neil P.", "", "for the Alzheimer's Disease Neuroimaging\n  Initiative"]]}, {"id": "1912.01540", "submitter": "Himalaya Jain", "authors": "Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick P\\'erez and\n  Matthieu Cord", "title": "QUEST: Quantized embedding space for transferring knowledge", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation refers to the process of training a compact student\nnetwork to achieve better accuracy by learning from a high capacity teacher\nnetwork. Most of the existing knowledge distillation methods direct the student\nto follow the teacher by matching the teacher's output, feature maps or their\ndistribution. In this work, we propose a novel way to achieve this goal: by\ndistilling the knowledge through a quantized space. According to our method,\nthe teacher's feature maps are quantized to represent the main visual concepts\nencompassed in the feature maps. The student is then asked to predict the\nquantized representation, which thus forms the task that the student uses to\nlearn from the teacher. Despite its simplicity, we show that our approach is\nable to yield results that improve the state of the art on knowledge\ndistillation. To that end, we provide an extensive evaluation across several\nnetwork architectures and most commonly used benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:38:40 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:34:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jain", "Himalaya", ""], ["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1912.01553", "submitter": "Joel Michelson", "authors": "Joel Michelson, Joshua H. Palmer, Aneesha Dasari, Maithilee Kunda", "title": "Learning Spatially Structured Image Transformations Using Planar Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning image transformations is essential to the idea of mental simulation\nas a method of cognitive inference. We take a connectionist modeling approach,\nusing planar neural networks to learn fundamental imagery transformations, like\ntranslation, rotation, and scaling, from perceptual experiences in the form of\nimage sequences. We investigate how variations in network topology, training\ndata, and image shape, among other factors, affect the efficiency and\neffectiveness of learning visual imagery transformations, including\neffectiveness of transfer to operating on new types of data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:54:35 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 00:46:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Michelson", "Joel", ""], ["Palmer", "Joshua H.", ""], ["Dasari", "Aneesha", ""], ["Kunda", "Maithilee", ""]]}, {"id": "1912.01584", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Ziyun Wang, Kaung Khant, Kostas Daniilidis", "title": "EventGAN: Leveraging Large Scale Image Datasets for Event Cameras", "comments": "10 pages, 5 figures, 2 tables, Code:\n  https://github.com/alexzzhu/EventGAN, Video:\n  https://www.youtube.com/watch?v=Vcm4Iox4H2w", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras provide a number of benefits over traditional cameras, such as\nthe ability to track incredibly fast motions, high dynamic range, and low power\nconsumption. However, their application into computer vision problems, many of\nwhich are primarily dominated by deep learning solutions, has been limited by\nthe lack of labeled training data for events. In this work, we propose a method\nwhich leverages the existing labeled data for images by simulating events from\na pair of temporal image frames, using a convolutional neural network. We train\nthis network on pairs of images and events, using an adversarial discriminator\nloss and a pair of cycle consistency losses. The cycle consistency losses\nutilize a pair of pre-trained self-supervised networks which perform optical\nflow estimation and image reconstruction from events, and constrain our network\nto generate events which result in accurate outputs from both of these\nnetworks. Trained fully end to end, our network learns a generative model for\nevents from images without the need for accurate modeling of the motion in the\nscene, exhibited by modeling based methods, while also implicitly modeling\nevent noise. Using this simulator, we train a pair of downstream networks on\nobject detection and 2D human pose estimation from events, using simulated data\nfrom large scale image datasets, and demonstrate the networks' abilities to\ngeneralize to datasets with real events.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:29:49 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 10:07:36 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Wang", "Ziyun", ""], ["Khant", "Kaung", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1912.01601", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, Larry S. Davis", "title": "LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video\n  Recognition", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents LiteEval, a simple yet effective coarse-to-fine framework\nfor resource efficient video recognition, suitable for both online and offline\nscenarios. Exploiting decent yet computationally efficient features derived at\na coarse scale with a lightweight CNN model, LiteEval dynamically decides\non-the-fly whether to compute more powerful features for incoming video frames\nat a finer scale to obtain more details. This is achieved by a coarse LSTM and\na fine LSTM operating cooperatively, as well as a conditional gating module to\nlearn when to allocate more computation. Extensive experiments are conducted on\ntwo large-scale video benchmarks, FCVID and ActivityNet, and the results\ndemonstrate LiteEval requires substantially less computation while offering\nexcellent classification accuracy for both online and offline predictions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:54:50 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wu", "Zuxuan", ""], ["Xiong", "Caiming", ""], ["Jiang", "Yu-Gang", ""], ["Davis", "Larry S.", ""]]}, {"id": "1912.01643", "submitter": "Alexander Gomez Villa A. Gomez-Villa", "authors": "A. Gomez-Villa, A. Mart\\'in, J. Vazquez-Corral, M. Bertalm\\'io, J.\n  Malo", "title": "Visual Illusions Also Deceive Convolutional Neural Networks: Analysis\n  and Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual illusions allow researchers to devise and test new models of visual\nperception. Here we show that artificial neural networks trained for basic\nvisual tasks in natural images are deceived by brightness and color illusions,\nhaving a response that is qualitatively very similar to the human achromatic\nand chromatic contrast sensitivity functions, and consistent with natural image\nstatistics. We also show that, while these artificial networks are deceived by\nillusions, their response might be significantly different to that of humans.\nOur results suggest that low-level illusions appear in any system that has to\nperform basic visual tasks in natural environments, in line with error\nminimization explanations of visual function, and they also imply a word of\ncaution on using artificial networks to study human vision, as previously\nsuggested in other contexts in the vision science literature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 19:33:30 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Gomez-Villa", "A.", ""], ["Mart\u00edn", "A.", ""], ["Vazquez-Corral", "J.", ""], ["Bertalm\u00edo", "M.", ""], ["Malo", "J.", ""]]}, {"id": "1912.01652", "submitter": "Eric Heiden", "authors": "Eric Heiden, Ziang Liu, Ragesh K. Ramachandran, Gaurav S. Sukhatme", "title": "Physics-based Simulation of Continuous-Wave LIDAR for Localization,\n  Calibration and Tracking", "comments": "Published at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light Detection and Ranging (LIDAR) sensors play an important role in the\nperception stack of autonomous robots, supplying mapping and localization\npipelines with depth measurements of the environment. While their accuracy\noutperforms other types of depth sensors, such as stereo or time-of-flight\ncameras, the accurate modeling of LIDAR sensors requires laborious manual\ncalibration that typically does not take into account the interaction of laser\nlight with different surface types, incidence angles and other phenomena that\nsignificantly influence measurements. In this work, we introduce a physically\nplausible model of a 2D continuous-wave LIDAR that accounts for the\nsurface-light interactions and simulates the measurement process in the Hokuyo\nURG-04LX LIDAR. Through automatic differentiation, we employ gradient-based\noptimization to estimate model parameters from real sensor measurements.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 19:47:01 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 06:30:01 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Heiden", "Eric", ""], ["Liu", "Ziang", ""], ["Ramachandran", "Ragesh K.", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1912.01661", "submitter": "Todd Hylton", "authors": "Michael Hazoglou, Todd Hylton", "title": "Integrating Motion into Vision Models for Better Visual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an improved vision system that learns a model of its\nenvironment using a self-supervised, predictive learning method. The system\nincludes a pan-tilt camera, a foveated visual input, a saccading reflex to\nservo the foveated region to areas high prediction error, input frame\ntransformation synced to the camera motion, and a recursive, hierachical\nmachine learning technique based on the Predictive Vision Model. In earlier\nwork, which did not integrate camera motion into the vision model, prediction\nwas impaired and camera movement suffered from undesired feedback effects. Here\nwe detail the integration of camera motion into the predictive learning system\nand show improved visual prediction and saccadic behavior. From these\nexperiences, we speculate on the integration of additional sensory and motor\nsystems into self-supervised, predictive learning models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 19:56:05 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Hazoglou", "Michael", ""], ["Hylton", "Todd", ""]]}, {"id": "1912.01667", "submitter": "Siddhant Bhambri", "authors": "Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, Arun Balaji Buduru", "title": "A Survey of Black-Box Adversarial Attacks on Computer Vision Models", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has seen tremendous advances in the past few years, which\nhas lead to deep learning models being deployed in varied applications of\nday-to-day life. Attacks on such models using perturbations, particularly in\nreal-life scenarios, pose a severe challenge to their applicability, pushing\nresearch into the direction which aims to enhance the robustness of these\nmodels. After the introduction of these perturbations by Szegedy et al. [1],\nsignificant amount of research has focused on the reliability of such models,\nprimarily in two aspects - white-box, where the adversary has access to the\ntargeted model and related parameters; and the black-box, which resembles a\nreal-life scenario with the adversary having almost no knowledge of the model\nto be attacked. To provide a comprehensive security cover, it is essential to\nidentify, study, and build defenses against such attacks. Hence, in this paper,\nwe propose to present a comprehensive comparative study of various black-box\nadversarial attacks and defense techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:06:49 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 07:33:59 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 09:17:38 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Bhambri", "Siddhant", ""], ["Muku", "Sumanyu", ""], ["Tulasi", "Avinash", ""], ["Buduru", "Arun Balaji", ""]]}, {"id": "1912.01674", "submitter": "Chenhongyi Yang", "authors": "Chenhongyi Yang, Vitaly Ablavsky, Kaihong Wang, Qi Feng and Margrit\n  Betke", "title": "Learning to Separate: Detecting Heavily-Occluded Objects in Urban Scenes", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While visual object detection with deep learning has received much attention\nin the past decade, cases when heavy intra-class occlusions occur have not been\nstudied thoroughly. In this work, we propose a Non-Maximum-Suppression (NMS)\nalgorithm that dramatically improves the detection recall while maintaining\nhigh precision in scenes with heavy occlusions. Our NMS algorithm is derived\nfrom a novel embedding mechanism, in which the semantic and geometric features\nof the detected boxes are jointly exploited. The embedding makes it possible to\ndetermine whether two heavily-overlapping boxes belong to the same object in\nthe physical world. Our approach is particularly useful for car detection and\npedestrian detection in urban scenes where occlusions often happen. We show the\neffectiveness of our approach by creating a model called SG-Det (short for\nSemantics and Geometry Detection) and testing SG-Det on two widely-adopted\ndatasets, KITTI and CityPersons for which it achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:21:21 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 21:26:45 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 13:41:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yang", "Chenhongyi", ""], ["Ablavsky", "Vitaly", ""], ["Wang", "Kaihong", ""], ["Feng", "Qi", ""], ["Betke", "Margrit", ""]]}, {"id": "1912.01691", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli and Agnes Desolneux and Alain Durmus and Bruno\n  Galerne and Arthur Leclaire", "title": "Maximum entropy methods for texture synthesis: theory and practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the rise of convolutional neural network techniques in\nexemplar-based image synthesis. These methods often rely on the minimization of\nsome variational formulation on the image space for which the minimizers are\nassumed to be the solutions of the synthesis problem. In this paper we\ninvestigate, both theoretically and experimentally, another framework to deal\nwith this problem using an alternate sampling/minimization scheme. First, we\nuse results from information geometry to assess that our method yields a\nprobability measure which has maximum entropy under some constraints in\nexpectation. Then, we turn to the analysis of our method and we show, using\nrecent results from the Markov chain literature, that its error can be\nexplicitly bounded with constants which depend polynomially in the dimension\neven in the non-convex setting. This includes the case where the constraints\nare defined via a differentiable neural network. Finally, we present an\nextensive experimental study of the model, including a comparison with\nstate-of-the-art methods and an extension to style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:36:44 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Desolneux", "Agnes", ""], ["Durmus", "Alain", ""], ["Galerne", "Bruno", ""], ["Leclaire", "Arthur", ""]]}, {"id": "1912.01704", "submitter": "Xiaotian Liu", "authors": "Xiaotian Liu, Pengyi Shi, Sarra Alqahtani, Victor Pa\\'ul Pauca, Miles\n  Silman", "title": "Robustness-Driven Exploration with Probabilistic Metric Temporal Logic", "comments": "10 pages, 8 figures, submitted to AAMAS 2020 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform autonomous exploration is essential for unmanned\naerial vehicles (UAV) operating in unstructured or unknown environments where\nit is hard or even impossible to describe the environment beforehand. However,\nalgorithms for autonomous exploration often focus on optimizing time and\ncoverage in a greedy fashion. That type of exploration can collect irrelevant\ndata and wastes time navigating areas with no important information. In this\npaper, we propose a method for exploiting the discovered knowledge about the\nenvironment while exploring it by relying on a theory of robustness based on\nProbabilistic Metric Temporal Logic (P-MTL) as applied to offline verification\nand online control of hybrid systems. By maximizing the satisfaction of the\npredefined P-MTL specifications of the exploration problem, the robustness\nvalues guide the UAV towards areas with more interesting information to gain.\nWe use Markov Chain Monte Carlo to solve the P-MTL constraints. We demonstrate\nthe effectiveness of the proposed approach by simulating autonomous exploration\nover Amazonian rainforest where our approach is used to detect areas occupied\nby illegal Artisanal Small-scale Gold Mining (ASGM) activities. The results\nshow that our approach outperform a greedy exploration approach (Autonomous\nExploration Planner) by 38% in terms of ASGM coverage.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:06:22 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Liu", "Xiaotian", ""], ["Shi", "Pengyi", ""], ["Alqahtani", "Sarra", ""], ["Pauca", "Victor Pa\u00fal", ""], ["Silman", "Miles", ""]]}, {"id": "1912.01707", "submitter": "Charan Dudda Prakash", "authors": "Charan D. Prakash and Lina J. Karam", "title": "It GAN DO Better: GAN-based Detection of Objects on Images with Varying\n  Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose in our novel generative framework the use of\nGenerative Adversarial Networks (GANs) to generate features that provide\nrobustness for object detection on reduced quality images. The proposed\nGAN-based Detection of Objects (GAN-DO) framework is not restricted to any\nparticular architecture and can be generalized to several deep neural network\n(DNN) based architectures. The resulting deep neural network maintains the\nexact architecture as the selected baseline model without adding to the model\nparameter complexity or inference speed. We first evaluate the effect of image\nquality not only on the object classification but also on the object bounding\nbox regression. We then test the models resulting from our proposed GAN-DO\nframework, using two state-of-the-art object detection architectures as the\nbaseline models. We also evaluate the effect of the number of re-trained\nparameters in the generator of GAN-DO on the accuracy of the final trained\nmodel. Performance results provided using GAN-DO on object detection datasets\nestablish an improved robustness to varying image quality and a higher mAP\ncompared to the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:10:07 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Prakash", "Charan D.", ""], ["Karam", "Lina J.", ""]]}, {"id": "1912.01721", "submitter": "Krystian Radlak", "authors": "Krystian Radlak and Lukasz Malinski and Bogdan Smolka", "title": "Deep Learning based Switching Filter for Impulsive Noise Removal in\n  Color Images", "comments": null, "journal-ref": "Sensors 20 (2020) 2782", "doi": "10.3390/s20102782", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise reduction is one the most important and still active research topic in\nlow-level image processing due to its high impact on object detection and scene\nunderstanding for computer vision systems. Recently, we can observe a\nsubstantial increase of interest in the application of deep learning algorithms\nin many computer vision problems due to its impressive capability of automatic\nfeature extraction and classification. These methods have been also\nsuccessfully applied in image denoising, significantly improving the\nperformance, but most of the proposed approaches were designed for Gaussian\nnoise suppression. In this paper, we present a switching filtering design\nintended for impulsive noise removal using deep learning. In the proposed\nmethod, the impulses are identified using a novel deep neural network\narchitecture and noisy pixels are restored using the fast adaptive mean filter.\nThe performed experiments show that the proposed approach is superior to the\nstate-of-the-art filters designed for impulsive noise removal in digital color\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:23:00 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Radlak", "Krystian", ""], ["Malinski", "Lukasz", ""], ["Smolka", "Bogdan", ""]]}, {"id": "1912.01734", "submitter": "Jesse Thomason", "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson\n  Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks", "comments": "Computer Vision and Pattern Recognition (CVPR) 2020 ;\n  https://askforalfred.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike \"Rinse off a mug and place it in the coffee maker.\" and low-level language\ninstructions like \"Walk to the coffee maker on the right.\" ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 23:18:59 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 01:18:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Shridhar", "Mohit", ""], ["Thomason", "Jesse", ""], ["Gordon", "Daniel", ""], ["Bisk", "Yonatan", ""], ["Han", "Winson", ""], ["Mottaghi", "Roozbeh", ""], ["Zettlemoyer", "Luke", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.01756", "submitter": "Fuyang Zhang", "authors": "Fuyang Zhang, Nelson Nauata, Yasutaka Furukawa", "title": "Conv-MPN: Convolutional Message Passing Neural Network for Structured\n  Outdoor Architecture Reconstruction", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel message passing neural (MPN) architecture\nConv-MPN, which reconstructs an outdoor building as a planar graph from a\nsingle RGB image. Conv-MPN is specifically designed for cases where nodes of a\ngraph have explicit spatial embedding. In our problem, nodes correspond to\nbuilding edges in an image. Conv-MPN is different from MPN in that 1) the\nfeature associated with a node is represented as a feature volume instead of a\n1D vector; and 2) convolutions encode messages instead of fully connected\nlayers. Conv-MPN learns to select a true subset of nodes (i.e., building edges)\nto reconstruct a building planar graph. Our qualitative and quantitative\nevaluations over 2,000 buildings show that Conv-MPN makes significant\nimprovements over the existing fully neural solutions. We believe that the\npaper has a potential to open a new line of graph neural network research for\nstructured geometry reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 01:15:51 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 22:57:00 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 19:29:17 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 03:42:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Fuyang", ""], ["Nauata", "Nelson", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1912.01800", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, Salman Khan, Nick Barnes, Stephen Gould", "title": "Spectral-GANs for High-Resolution 3D Point-cloud Generation", "comments": "1 page: Added affiliations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-clouds are a popular choice for vision and graphics tasks due to their\naccurate shape description and direct acquisition from range-scanners. This\ndemands the ability to synthesize and reconstruct high-quality point-clouds.\nCurrent deep generative models for 3D data generally work on simplified\nrepresentations (e.g., voxelized objects) and cannot deal with the inherent\nredundancy and irregularity in point-clouds. A few recent efforts on 3D\npoint-cloud generation offer limited resolution and their complexity grows with\nthe increase in output resolution. In this paper, we develop a principled\napproach to synthesize 3D point-clouds using a spectral-domain Generative\nAdversarial Network (GAN). Our spectral representation is highly structured and\nallows us to disentangle various frequency bands such that the learning task is\nsimplified for a GAN model. As compared to spatial-domain generative\napproaches, our formulation allows us to generate arbitrary number of points\nhigh-resolution point-clouds with minimal computational overhead. Furthermore,\nwe propose a fully differentiable block to transform from {the} spectral to the\nspatial domain and back, thereby allowing us to integrate knowledge from\nwell-established spatial models. We demonstrate that Spectral-GAN performs well\nfor point-cloud generation task. Additionally, it can learn {a} highly\ndiscriminative representation in an unsupervised fashion and can be used to\naccurately reconstruct 3D objects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:11:16 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 08:50:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""], ["Gould", "Stephen", ""]]}, {"id": "1912.01805", "submitter": "Minghao Xu", "authors": "Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian,\n  Wenjun Zhang", "title": "Adversarial Domain Adaptation with Domain Mixup", "comments": "Accepted as oral presentation at 34th AAAI Conference on Artificial\n  Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on domain adaptation reveal the effectiveness of adversarial\nlearning on filling the discrepancy between source and target domains. However,\ntwo common limitations exist in current adversarial-learning-based methods.\nFirst, samples from two domains alone are not sufficient to ensure\ndomain-invariance at most part of latent space. Second, the domain\ndiscriminator involved in these methods can only judge real or fake with the\nguidance of hard label, while it is more reasonable to use soft scores to\nevaluate the generated images or features, i.e., to fully utilize the\ninter-domain information. In this paper, we present adversarial domain\nadaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a\nmore continuous latent space and guides the domain discriminator in judging\nsamples' difference relative to source and target domains. Domain mixup is\njointly conducted on pixel and feature level to improve the robustness of\nmodels. Extensive experiments prove that the proposed approach can achieve\nsuperior performance on tasks with various degrees of domain shift and data\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:45:43 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Xu", "Minghao", ""], ["Zhang", "Jian", ""], ["Ni", "Bingbing", ""], ["Li", "Teng", ""], ["Wang", "Chengjie", ""], ["Tian", "Qi", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1912.01811", "submitter": "Dawei Du", "authors": "Longyin Wen, Dawei Du, Pengfei Zhu, Qinghua Hu, Qilong Wang, Liefeng\n  Bo, Siwei Lyu", "title": "Drone-based Joint Density Map Estimation, Localization and Tracking with\n  Space-Time Multi-Scale Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a space-time multi-scale attention network (STANet) to\nsolve density map estimation, localization and tracking in dense crowds of\nvideo clips captured by drones with arbitrary crowd density, perspective, and\nflight altitude. Our STANet method aggregates multi-scale feature maps in\nsequential frames to exploit the temporal coherency, and then predict the\ndensity maps, localize the targets, and associate them in crowds\nsimultaneously. A coarse-to-fine process is designed to gradually apply the\nattention module on the aggregated multi-scale feature maps to enforce the\nnetwork to exploit the discriminative space-time features for better\nperformance. The whole network is trained in an end-to-end manner with the\nmulti-task loss, formed by three terms, i.e., the density map loss,\nlocalization loss and association loss. The non-maximal suppression followed by\nthe min-cost flow framework is used to generate the trajectories of targets' in\nscenarios. Since existing crowd counting datasets merely focus on crowd\ncounting in static cameras rather than density map estimation, counting and\ntracking in crowds on drones, we have collected a new large-scale drone-based\ndataset, DroneCrowd, formed by 112 video clips with 33,600 high resolution\nframes (i.e., 1920x1080) captured in 70 different scenarios. With intensive\namount of effort, our dataset provides 20,800 people trajectories with 4.8\nmillion head annotations and several video-level attributes in sequences.\nExtensive experiments are conducted on two challenging public datasets, i.e.,\nShanghaitech and UCF-QNRF, and our DroneCrowd, to demonstrate that STANet\nachieves favorable performance against the state-of-the-arts. The datasets and\ncodes can be found at https://github.com/VisDrone.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:04:03 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wen", "Longyin", ""], ["Du", "Dawei", ""], ["Zhu", "Pengfei", ""], ["Hu", "Qinghua", ""], ["Wang", "Qilong", ""], ["Bo", "Liefeng", ""], ["Lyu", "Siwei", ""]]}, {"id": "1912.01816", "submitter": "Eli (Omid) David", "authors": "Evyatar Illouz, Eli David, and Nathan S. Netanyahu", "title": "Handwriting-Based Gender Classification Using End-to-End Deep Neural\n  Networks", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 613-621, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_60", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting-based gender classification is a well-researched problem that has\nbeen approached mainly by traditional machine learning techniques. In this\npaper, we propose a novel deep learning-based approach for this task.\nSpecifically, we present a convolutional neural network (CNN), which performs\nautomatic feature extraction from a given handwritten image, followed by\nclassification of the writer's gender. Also, we introduce a new dataset of\nlabeled handwritten samples, in Hebrew and English, of 405 participants.\nComparing the gender classification accuracy on this dataset against human\nexaminers, our results show that the proposed deep learning-based approach is\nsubstantially more accurate than that of humans.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:31 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Illouz", "Evyatar", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.01824", "submitter": "Yuto Onga", "authors": "Yuto Onga, Shingo Fujiyama, Hayato Arai, Yusuke Chayama, Hitoshi\n  Iyatomi, Kenichi Oishi", "title": "Efficient feature embedding of 3D brain MRI images for content-based\n  image retrieval with deep metric learning", "comments": "To appear in the IEEE BigData 2019 Workshop on Advances in High\n  Dimensional (AdHD) Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing numbers of MRI brain scans, improvements in image resolution, and\nadvancements in MRI acquisition technology are causing significant increases in\nthe demand for and burden on radiologists' efforts in terms of reading and\ninterpreting brain MRIs. Content-based image retrieval (CBIR) is an emerging\ntechnology for reducing this burden by supporting the reading of medical\nimages. High dimensionality is a major challenge in developing a CBIR system\nthat is applicable for 3D brain MRIs. In this study, we propose a system called\ndisease-oriented data concentration with metric learning (DDCML). In DDCML, we\nintroduce deep metric learning to a 3D convolutional autoencoder (CAE). Our\nproposed DDCML scheme achieves a high dimensional compression rate (4096:1)\nwhile preserving the disease-related anatomical features that are important for\nmedical image classification. The low-dimensional representation obtained by\nDDCML improved the clustering performance by 29.1\\% compared to plain 3D-CAE in\nterms of discriminating Alzheimer's disease patients from healthy subjects, and\nsuccessfully reproduced the relationships of the severity of disease categories\nthat were not included in the training.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:59:11 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Onga", "Yuto", ""], ["Fujiyama", "Shingo", ""], ["Arai", "Hayato", ""], ["Chayama", "Yusuke", ""], ["Iyatomi", "Hitoshi", ""], ["Oishi", "Kenichi", ""]]}, {"id": "1912.01834", "submitter": "Weiwei Cai", "authors": "Weiwei Cai, Zhanguo Wei", "title": "PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting", "comments": null, "journal-ref": "in IEEE Access, vol. 8, pp. 48451-48463, 2020", "doi": "10.1109/ACCESS.2020.2979348", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest methods based on deep learning have achieved amazing results\nregarding the complex work of inpainting large missing areas in an image. But\nthis type of method generally attempts to generate one single \"optimal\" result,\nignoring many other plausible results. Considering the uncertainty of the\ninpainting task, one sole result can hardly be regarded as a desired\nregeneration of the missing area. In view of this weakness, which is related to\nthe design of the previous algorithms, we propose a novel deep generative model\nequipped with a brand new style extractor which can extract the style feature\n(latent vector) from the ground truth. Once obtained, the extracted style\nfeature and the ground truth are both input into the generator. We also craft a\nconsistency loss that guides the generated image to approximate the ground\ntruth. After iterations, our generator is able to learn the mapping of styles\ncorresponding to multiple sets of vectors. The proposed model can generate a\nlarge number of results consistent with the context semantics of the image.\nMoreover, we evaluated the effectiveness of our model on three datasets, i.e.,\nCelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting\nmethods, this model is able to offer desirable inpainting results with both\nbetter quality and higher diversity. The code and model will be made available\non https://github.com/vivitsai/PiiGAN.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:44:41 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 07:49:07 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Cai", "Weiwei", ""], ["Wei", "Zhanguo", ""]]}, {"id": "1912.01838", "submitter": "Sreyas Mohan", "authors": "Alejandra Duarte, Chaitra V. Hegde, Aakash Kaku, Sreyas Mohan, Jos\\'e\n  G. Raya", "title": "Knee Cartilage Segmentation Using Diffusion-Weighted MRI", "comments": "Accepted to Medical Imaging Meets NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrity of articular cartilage is a crucial aspect in the early\ndiagnosis of osteoarthritis (OA). Many novel MRI techniques have the potential\nto assess compositional changes of the cartilage extracellular matrix. Among\nthese techniques, diffusion tensor imaging (DTI) of cartilage provides a\nsimultaneous assessment of the two principal components of the solid matrix:\ncollagen structure and proteoglycan concentration. DTI, as for any other\ncompositional MRI technique, require a human expert to perform segmentation\nmanually. The manual segmentation is error-prone and time-consuming ($\\sim$ few\nhours per subject). We use an ensemble of modified U-Nets to automate this\nsegmentation task. We benchmark our model against a human expert test-retest\nsegmentation and conclude that our model is superior for Patellar and Tibial\ncartilage using dice score as the comparison metric. In the end, we do a\nperturbation analysis to understand the sensitivity of our model to the\ndifferent components of our input. We also provide confidence maps for the\npredictions so that radiologists can tweak the model predictions as required.\nThe model has been deployed in practice. In conclusion, cartilage segmentation\non DW-MRI images with modified U-Nets achieves accuracy that outperforms the\nhuman segmenter. Code is available at\nhttps://github.com/aakashrkaku/knee-cartilage-segmentation\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:00:11 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Duarte", "Alejandra", ""], ["Hegde", "Chaitra V.", ""], ["Kaku", "Aakash", ""], ["Mohan", "Sreyas", ""], ["Raya", "Jos\u00e9 G.", ""]]}, {"id": "1912.01839", "submitter": "Yuval Bahat", "authors": "Yuval Bahat and Tomer Michaeli", "title": "Explorable Super Resolution", "comments": null, "journal-ref": "Proceedings .of\n  .the.IEEE/CVF.Conference.on.Computer.Vision.and.Pattern.Recognition. (2020)\n  2716-2725", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution (SR) has seen major performance leaps in recent\nyears. However, existing methods do not allow exploring the infinitely many\nplausible reconstructions that might have given rise to the observed\nlow-resolution (LR) image. These different explanations to the LR image may\ndramatically vary in their textures and fine details, and may often encode\ncompletely different semantic information. In this paper, we introduce the task\nof explorable super resolution. We propose a framework comprising a graphical\nuser interface with a neural network backend, allowing editing the SR output so\nas to explore the abundance of plausible HR explanations to the LR input. At\nthe heart of our method is a novel module that can wrap any existing SR\nnetwork, analytically guaranteeing that its SR outputs would precisely match\nthe LR input, when downsampled. Besides its importance in our setting, this\nmodule is guaranteed to decrease the reconstruction error of any SR network it\nwraps, and can be used to cope with blur kernels that are different from the\none the network was trained for. We illustrate our approach in a variety of use\ncases, ranging from medical imaging and forensics, to graphics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:01:58 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 12:37:36 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 08:29:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bahat", "Yuval", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1912.01842", "submitter": "Aythami Morales", "authors": "Ignacio Serna, Aythami Morales, Julian Fierrez, Manuel Cebrian, Nick\n  Obradovich, Iyad Rahwan", "title": "Algorithmic Discrimination: Formulation and Exploration in Deep\n  Learning-based Face Biometrics", "comments": null, "journal-ref": "AAAI Workshop on Artificial Intelligence Safety (SafeAI), New\n  York, NY, USA, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular face recognition benchmarks assume a distribution of\nsubjects without much attention to their demographic attributes. In this work,\nwe perform a comprehensive discrimination-aware experimentation of deep\nlearning-based face recognition. The main aim of this study is focused on a\nbetter understanding of the feature space generated by deep models, and the\nperformance achieved over different demographic groups. We also propose a\ngeneral formulation of algorithmic discrimination with application to face\nbiometrics. The experiments are conducted over the new DiveFace database\ncomposed of 24K identities from six different demographic groups. Two popular\nface recognition models are considered in the experimental framework: ResNet-50\nand VGG-Face. We experimentally show that demographic groups highly represented\nin popular face databases have led to popular pre-trained deep face models\npresenting strong algorithmic discrimination. That discrimination can be\nobserved both qualitatively at the feature space of the deep models and\nquantitatively in large performance differences when applying those models in\ndifferent demographic groups, e.g. for face biometrics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:08:28 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Serna", "Ignacio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Cebrian", "Manuel", ""], ["Obradovich", "Nick", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1912.01844", "submitter": "Kaidong Li", "authors": "Kaidong Li, Wenchi Ma, Usman Sajid, Yuanwei Wu, Guanghui Wang", "title": "Object Detection with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we present a brief overview of the recent development in\nobject detection using convolutional neural networks (CNN). Several classical\nCNN-based detectors are presented. Some developments are based on the detector\narchitectures, while others are focused on solving certain problems, like model\ndegradation and small-scale object detection. The chapter also presents some\nperformance comparison results of different models on several benchmark\ndatasets. Through the discussion of these models, we hope to give readers a\ngeneral idea about the developments of CNN-based object detection.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:18:54 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Li", "Kaidong", ""], ["Ma", "Wenchi", ""], ["Sajid", "Usman", ""], ["Wu", "Yuanwei", ""], ["Wang", "Guanghui", ""]]}, {"id": "1912.01857", "submitter": "Byungju Kim", "authors": "Byungju Kim, Junmo Kim", "title": "Adjusting Decision Boundary for Class Imbalanced Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of deep neural networks heavily depends on the data distribution. In\nparticular, the networks easily suffer from class imbalance. The trained\nnetworks would recognize the frequent classes better than the infrequent\nclasses. To resolve this problem, existing approaches typically propose novel\nloss functions to obtain better feature embedding. In this paper, we argue that\ndrawing a better decision boundary is as important as learning better features.\nInspired by observations, we investigate how the class imbalance affects the\ndecision boundary and deteriorates the performance. We also investigate the\nfeature distributional discrepancy between training and test time. As a result,\nwe propose a novel, yet simple method for class imbalanced learning. Despite\nits simplicity, our method shows outstanding performance. In particular, the\nexperimental results show that we can significantly improve the network by\nscaling the weight vectors, even without additional training process.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:14:04 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:15:57 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kim", "Byungju", ""], ["Kim", "Junmo", ""]]}, {"id": "1912.01865", "submitter": "Yunjey Choi", "authors": "Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha", "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A good image-to-image translation model should learn a mapping between\ndifferent visual domains while satisfying the following properties: 1)\ndiversity of generated images and 2) scalability over multiple domains.\nExisting methods address either of the issues, having limited diversity or\nmultiple models for all domains. We propose StarGAN v2, a single framework that\ntackles both and shows significantly improved results over the baselines.\nExperiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our\nsuperiority in terms of visual quality, diversity, and scalability. To better\nassess image-to-image translation models, we release AFHQ, high-quality animal\nfaces with large inter- and intra-domain differences. The code, pretrained\nmodels, and dataset can be found at https://github.com/clovaai/stargan-v2.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:42:22 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 07:09:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Choi", "Yunjey", ""], ["Uh", "Youngjung", ""], ["Yoo", "Jaejun", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1912.01870", "submitter": "Abel Gawel", "authors": "Abel Gawel, Hermann Blum, Johannes Pankert, Koen Kr\\\"amer, Luca\n  Bartolomei, Selen Ercan, Farbod Farshidian, Margarita Chli, Fabio Gramazio,\n  Roland Siegwart, Marco Hutter, Timothy Sandy", "title": "A Fully-Integrated Sensing and Control System for High-Accuracy Mobile\n  Robotic Building Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully-integrated sensing and control system which enables mobile\nmanipulator robots to execute building tasks with millimeter-scale accuracy on\nbuilding construction sites. The approach leverages multi-modal sensing\ncapabilities for state estimation, tight integration with digital building\nmodels, and integrated trajectory planning and whole-body motion control. A\nnovel method for high-accuracy localization updates relative to the known\nbuilding structure is proposed. The approach is implemented on a real platform\nand tested under realistic construction conditions. We show that the system can\nachieve sub-cm end-effector positioning accuracy during fully autonomous\noperation using solely on-board sensing.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:01:37 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Gawel", "Abel", ""], ["Blum", "Hermann", ""], ["Pankert", "Johannes", ""], ["Kr\u00e4mer", "Koen", ""], ["Bartolomei", "Luca", ""], ["Ercan", "Selen", ""], ["Farshidian", "Farbod", ""], ["Chli", "Margarita", ""], ["Gramazio", "Fabio", ""], ["Siegwart", "Roland", ""], ["Hutter", "Marco", ""], ["Sandy", "Timothy", ""]]}, {"id": "1912.01875", "submitter": "Yiming He", "authors": "Yiming He, Wei Hu", "title": "3D Hand Pose Estimation via Regularized Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D hand pose estimation from a monocular\nRGB image. While previous methods have shown great success, the structure of\nhands has not been fully exploited, which is critical in pose estimation. To\nthis end, we propose a regularized graph representation learning under a\nconditional adversarial learning framework for 3D hand pose estimation, aiming\nto capture structural inter-dependencies of hand joints. In particular, we\nestimate an initial hand pose from a parametric hand model as a prior of hand\nstructure, which regularizes the inference of the structural deformation in the\nprior pose for accurate graph representation learning via residual graph\nconvolution. To optimize the hand structure further, we propose two\nbone-constrained loss functions, which characterize the morphable structure of\nhand poses explicitly. Also, we introduce an adversarial learning framework\nconditioned on the input image with a multi-source discriminator, which imposes\nthe structural constraints onto the distribution of generated 3D hand poses for\nanthropomorphically valid hand poses. Extensive experiments demonstrate that\nour model sets the new state-of-the-art in 3D hand pose estimation from a\nmonocular image on five standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:13:06 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:57:48 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 13:28:50 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 13:06:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["He", "Yiming", ""], ["Hu", "Wei", ""]]}, {"id": "1912.01879", "submitter": "Halit Murat G\\\"ursu", "authors": "Serkut Ayva\\c{s}{\\i}k, H. Murat G\\\"ursu, Wolfgang Kellerer", "title": "Veni Vidi Dixi: Reliable Wireless Communication with Depth Images", "comments": "Accepted for publication in CoNext 2019 with reproducibility badges.\n  The measurements and the processing codes are available at\n  https://gitlab.lrz.de/lkn_measurements/vvd_measurements for your evaluation", "journal-ref": null, "doi": "10.1145/3359989.3365418", "report-no": null, "categories": "cs.IT cs.CV cs.LG cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upcoming industrial revolution requires deployment of critical wireless\nsensor networks for automation and monitoring purposes. However, the\nreliability of the wireless communication is rendered unpredictable by mobile\nelements in the communication environment such as humans or mobile robots which\nlead to dynamically changing radio environments. Changes in the wireless\nchannel can be monitored with frequent pilot transmission. However, that would\nstress the battery life of sensors. In this work a new wireless channel\nestimation technique, Veni Vidi Dixi, VVD, is proposed. VVD leverages the\nredundant information in depth images obtained from the surveillance cameras in\nthe communication environment and utilizes Convolutional Neural Networks CNNs\nto map the depth images of the communication environment to complex wireless\nchannel estimations. VVD increases the wireless communication reliability\nwithout the need for frequent pilot transmission and with no additional\ncomplexity on the receiver. The proposed method is tested by conducting\nmeasurements in an indoor environment with a single mobile human. Up to authors\nbest knowledge our work is the first to obtain complex wireless channel\nestimation from only depth images without any pilot transmission. The collected\nwireless trace, depth images and codes are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:21:34 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Ayva\u015f\u0131k", "Serkut", ""], ["G\u00fcrsu", "H. Murat", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "1912.01881", "submitter": "Zhengcong Fei", "authors": "Zheng-cong Fei", "title": "Better Understanding Hierarchical Visual Relationship for Image Caption", "comments": "NeurIPS 2019 workshop on New In ML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) has been the dominant image feature\nextractor in computer vision for years. However, it fails to get the\nrelationship between images/objects and their hierarchical interactions which\ncan be helpful for representing and describing an image. In this paper, we\npropose a new design for image caption under a general encoder-decoder\nframework. It takes into account the hierarchical interactions between\ndifferent abstraction levels of visual information in the images and their\nbounding-boxes. Specifically, we present CNN plus Graph Convolutional Network\n(GCN) architecture that novelly integrates both semantic and spatial visual\nrelationships into image encoder. The representations of regions in an image\nand the connections between images are refined by leveraging graph structure\nthrough GCN. With the learned multi-level features, our model capitalizes on\nthe Transformer-based decoder for description generation. We conduct\nexperiments on the COCO image captioning dataset. Evaluations show that our\nproposed model outperforms the previous state-of-the-art models in the task of\nimage caption, leading to a better performance in terms of all evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:26:11 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Fei", "Zheng-cong", ""]]}, {"id": "1912.01884", "submitter": "Mikhail Aliev", "authors": "Ekaterina Panfilova, Mikhail Aliev, Irina Kunina, Vasiliy Postnikov,\n  Dmitry Nikolaev", "title": "A Method of Detecting End-To-End Curves of Limited Curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a method for detecting end-to-end curves of limited\ncurvature like the k-link polylines with bending angle between adjacent\nsegments in a given range. The approximation accuracy is achieved by\nmaximization of the quality function in the image matrix. The method is based\non a dynamic programming scheme constructed over Fast Hough Transform\ncalculation results for image bands. The proposed method asymptotic complexity\nis $O(h \\cdot (w+ \\frac{h}{k}) \\cdot log(\\frac{h}{k}))$, where $h$ and $w$ are\nthe image size, and $k$ is the approximating polyline links number, which is an\nanalogue of the complexity of the fast Fourier transform or the fast Hough\ntransform. We also show the results of the proposed method on synthetic and\nreal data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:36:14 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Panfilova", "Ekaterina", ""], ["Aliev", "Mikhail", ""], ["Kunina", "Irina", ""], ["Postnikov", "Vasiliy", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "1912.01892", "submitter": "Ivan Konovalenko", "authors": "Julia Shemiakina, Ivan Konovalenko, Daniil Tropin, Igor Faradjev", "title": "Fast Projective Image Rectification for Planar Objects with Manhattan\n  Structure", "comments": "Accepted and presented on ICMV conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for metric rectification of planar objects that\npreserves angles and length ratios. An inner structure of an object is assumed\nto follow the laws of Manhattan World i.e. the majority of line segments are\naligned with two orthogonal directions of the object. For that purpose we\nintroduce the method that estimates the position of two vanishing points\ncorresponding to the main object directions. It is based on an original\noptimization function of segments that estimates a vanishing point position.\nFor calculation of the rectification homography with two vanishing points we\npropose a new method based on estimation of the camera rotation so that the\ncamera axis is perpendicular to the object plane. The proposed method can be\napplied for rectification of various objects such as documents or building\nfacades. Also since the camera rotation is estimated the method can be employed\nfor estimation of object orientation (for example, during a surgery with\nradiograph of osteosynthesis implants). The method was evaluated on the\nMIDV-500 dataset containing projectively distorted images of documents with\ncomplex background. According to the experimental results an accuracy of the\nproposed method is better or equal to the-state-of-the-art if the background\noccupies no more than half of the image. Runtime of the method is around 3ms on\ncore i7 3610qm CPU.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 10:59:22 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Shemiakina", "Julia", ""], ["Konovalenko", "Ivan", ""], ["Tropin", "Daniil", ""], ["Faradjev", "Igor", ""]]}, {"id": "1912.01916", "submitter": "Mikhail Aliev", "authors": "Kunina I.A., Aliev M.A., Arlazarov N.V., Polevoy D.V", "title": "A Method of Fluorescent Fibers Detection on Identity Documents under\n  Ultraviolet Light", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of the fluorescent security fibers\ndetection on the images of identity documents captured under ultraviolet light.\nAs an example we use images of the second and third pages of the Russian\npassport and show features that render known methods and approaches based on\nimage binarization non applicable. We propose a solution based on ridge\ndetection in the gray-scale image of the document with preliminary normalized\nbackground. The algorithm was tested on a private dataset consisting of both\nauthentic and model passports. Abandonment of binarization allowed to provide\nreliable and stable functioning of the proposed detector on a target dataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:08:33 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["A.", "Kunina I.", ""], ["A.", "Aliev M.", ""], ["V.", "Arlazarov N.", ""], ["D.", "Polevoy", "V"]]}, {"id": "1912.01923", "submitter": "Mikhail Aliev", "authors": "M.A. Aliev, D.A. Bocharov, I.A. Kunina, and D.P. Nikolaev", "title": "A Low Computational Approach for Price Tag Recognition", "comments": "9 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the task of search, localization and recognition of\nprice zone within a photograph of the price tag. The task is being addressed\nfor the case when image is acquired by small-scale digital camera and\ncalculation device has significant resource constraints. The proposed approach\nis based on Niblack binarization algorithm, analysis and clasterization of\nconnected components in conditions of known price tag geometrical model. The\nalgorithm was tested on a private dataset and has shown high quality.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:17:26 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Aliev", "M. A.", ""], ["Bocharov", "D. A.", ""], ["Kunina", "I. A.", ""], ["Nikolaev", "D. P.", ""]]}, {"id": "1912.01944", "submitter": "Saeideh Ghanbari Azar", "authors": "Saeideh Ghanbari Azar and Hadi Seyedarabi", "title": "Trajectory-Based Recognition of Dynamic Persian Sign Language Using\n  Hidden Markov Model", "comments": null, "journal-ref": null, "doi": "10.1016/j.csl.2019.101053", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language Recognition (SLR) is an important step in facilitating the\ncommunication among deaf people and the rest of society. Existing Persian sign\nlanguage recognition systems are mainly restricted to static signs which are\nnot very useful in everyday communications. In this study, a dynamic Persian\nsign language recognition system is presented. A collection of 1200 videos were\ncaptured from 12 individuals performing 20 dynamic signs with a simple white\nglove. The trajectory of the hands, along with hand shape information were\nextracted from each video using a simple region-growing technique. These\ntime-varying trajectories were then modeled using Hidden Markov Model (HMM)\nwith Gaussian probability density functions as observations. The performance of\nthe system was evaluated in different experimental strategies.\nSigner-independent and signer-dependent experiments were performed on the\nproposed system and the average accuracy of 97.48% was obtained. The\nexperimental results demonstrated that the performance of the system is\nindependent of the subject and it can also perform excellently even with a\nlimited number of training data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:08:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Azar", "Saeideh Ghanbari", ""], ["Seyedarabi", "Hadi", ""]]}, {"id": "1912.01954", "submitter": "Hui Ying", "authors": "Hui Ying, Zhaojin Huang, Shu Liu, Tianjia Shao and Kun Zhou", "title": "EmbedMask: Embedding Coupling for One-stage Instance Segmentation", "comments": "Code is available at github.com/yinghdb/EmbedMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current instance segmentation methods can be categorized into\nsegmentation-based methods that segment first then do clustering, and\nproposal-based methods that detect first then predict masks for each instance\nproposal using repooling. In this work, we propose a one-stage method, named\nEmbedMask, that unifies both methods by taking advantages of them. Like\nproposal-based methods, EmbedMask builds on top of detection models making it\nstrong in detection capability. Meanwhile, EmbedMask applies extra embedding\nmodules to generate embeddings for pixels and proposals, where pixel embeddings\nare guided by proposal embeddings if they belong to the same instance. Through\nthis embedding coupling process, pixels are assigned to the mask of the\nproposal if their embeddings are similar. The pixel-level clustering enables\nEmbedMask to generate high-resolution masks without missing details from\nrepooling, and the existence of proposal embedding simplifies and strengthens\nthe clustering procedure to achieve high speed with higher performance than\nsegmentation-based methods. Without any bells and whistles, EmbedMask achieves\ncomparable performance as Mask R-CNN, which is the representative two-stage\nmethod, and can produce more detailed masks at a higher speed. Code is\navailable at github.com/yinghdb/EmbedMask.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:31:52 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 10:42:25 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ying", "Hui", ""], ["Huang", "Zhaojin", ""], ["Liu", "Shu", ""], ["Shao", "Tianjia", ""], ["Zhou", "Kun", ""]]}, {"id": "1912.01966", "submitter": "Sebastian Guendel", "authors": "Sebastian Guendel and Andreas Maier", "title": "Epoch-wise label attacks for robustness against label noise", "comments": "Accepted at BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current accessibility to large medical datasets for training\nconvolutional neural networks is tremendously high. The associated dataset\nlabels are always considered to be the real \"ground truth\". However, the\nlabeling procedures often seem to be inaccurate and many wrong labels are\nintegrated. This may have fatal consequences on the performance of both\ntraining and evaluation. In this paper, we show the impact of label noise in\nthe training set on a specific medical problem based on chest X-ray images.\nWith a simple one-class problem, the classification of tuberculosis, we measure\nthe performance on a clean evaluation set when training with label-corrupt\ndata. We develop a method to compete with incorrectly labeled data during\ntraining by randomly attacking labels on individual epochs. The network tends\nto be robust when flipping correct labels for a single epoch and initiates a\ngood step to the optimal minimum on the error surface when flipping noisy\nlabels. On a baseline with an AUC (Area under Curve) score of 0.924, the\nperformance drops to 0.809 when 30% of our training data is misclassified. With\nour approach the baseline performance could almost be maintained, the\nperformance raised to 0.918.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:39:21 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 14:47:37 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Guendel", "Sebastian", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.01991", "submitter": "Ishan Misra", "authors": "Ishan Misra, Laurens van der Maaten", "title": "Self-Supervised Learning of Pretext-Invariant Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of self-supervised learning from images is to construct image\nrepresentations that are semantically meaningful via pretext tasks that do not\nrequire semantic annotations for a large training set of images. Many pretext\ntasks lead to representations that are covariant with image transformations. We\nargue that, instead, semantic representations ought to be invariant under such\ntransformations. Specifically, we develop Pretext-Invariant Representation\nLearning (PIRL, pronounced as \"pearl\") that learns invariant representations\nbased on pretext tasks. We use PIRL with a commonly used pretext task that\ninvolves solving jigsaw puzzles. We find that PIRL substantially improves the\nsemantic quality of the learned image representations. Our approach sets a new\nstate-of-the-art in self-supervised learning from images on several popular\nbenchmarks for self-supervised learning. Despite being unsupervised, PIRL\noutperforms supervised pre-training in learning image representations for\nobject detection. Altogether, our results demonstrate the potential of\nself-supervised learning of image representations with good invariance\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:59:48 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Misra", "Ishan", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1912.01992", "submitter": "Wang Dexin", "authors": "Dexin Wang", "title": "Research on dynamic target detection and tracking system of hexapod\n  robot", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic target detection and target tracking are hot issues in the field of\nimage. In order to explore its application value in the field of mobile robot,\na dynamic target detection and tracking system is designed based on hexapod\nrobot. Firstly, the dynamic target detection method is introduced with region\nmerging and adaptive external point filtering based on motion compensation\nmethod. This method achieves the accurate compensation of the moving background\nthrough symmetric matching and adaptive external point filtering, and achieves\ncomplete detection of non-rigid objects by region merging. Secondly, the\napplication of target tracking algorithm based on KCF in hexapod robot platform\nis studied, and the Angle tracking of moving target is realized by adaptive\nadjustment of tracking speed. The last, the architecture of robot monitoring\nsystem is designed, which consists of operator, processor, hexapod robot and\nvision sensor, and the moving object detection and tracking algorithm proposed\nin this paper is applied to the system. The experimental results show that the\nimproved algorithm can effectively detect and track the moving target when\napplied to the system of the mobile hexapod robot.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:02:57 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wang", "Dexin", ""]]}, {"id": "1912.02024", "submitter": "Annalisa Franco", "authors": "Annalisa Franco, Antonio Magnani and Dario Maio", "title": "Template co-updating in multi-modal human activity recognition systems", "comments": null, "journal-ref": null, "doi": "10.1145/3341105.3374085", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal systems are quite common in the context of human activity\nrecognition; widely used RGB-D sensors (Kinect is the most prominent example)\ngive access to parallel data streams, typically RGB images, depth data,\nskeleton information. The richness of multimodal information has been largely\nexploited in many works in the literature, while an analysis of their\neffectiveness for incremental template updating has not been investigated so\nfar. This paper is aimed at defining a general framework for unsupervised\ntemplate updating in multi-modal systems, where the different data sources can\nprovide complementary information, increasing the effectiveness of the updating\nprocedure and reducing at the same time the probability of incorrect template\nmodifications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:39:25 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Franco", "Annalisa", ""], ["Magnani", "Antonio", ""], ["Maio", "Dario", ""]]}, {"id": "1912.02037", "submitter": "Chen Gao", "authors": "Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan", "title": "AdversarialNAS: Adversarial Neural Architecture Search for GANs", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) that aims to automate the procedure of\narchitecture design has achieved promising results in many computer vision\nfields. In this paper, we propose an AdversarialNAS method specially tailored\nfor Generative Adversarial Networks (GANs) to search for a superior generative\nmodel on the task of unconditional image generation. The AdversarialNAS is the\nfirst method that can search the architectures of generator and discriminator\nsimultaneously in a differentiable manner. During searching, the designed\nadversarial search algorithm does not need to comput any extra metric to\nevaluate the performance of the searched architecture, and the search paradigm\nconsiders the relevance between the two network architectures and improves\ntheir mutual balance. Therefore, AdversarialNAS is very efficient and only\ntakes 1 GPU day to search for a superior generative model in the proposed large\nsearch space ($10^{38}$). Experiments demonstrate the effectiveness and\nsuperiority of our method. The discovered generative model sets a new\nstate-of-the-art FID score of $10.87$ and highly competitive Inception Score of\n$8.74$ on CIFAR-10. Its transferability is also proven by setting new\nstate-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10.\nCode is at: \\url{https://github.com/chengaopro/AdversarialNAS}.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:02:03 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 14:01:02 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Gao", "Chen", ""], ["Chen", "Yunpeng", ""], ["Liu", "Si", ""], ["Tan", "Zhenxiong", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1912.02048", "submitter": "Qi Feng", "authors": "Qi Feng, Vitaly Ablavsky, Qinxun Bai, Stan Sclaroff", "title": "Siamese Natural Language Tracker: Tracking by Natural Language\n  Descriptions with Siamese Trackers", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Siamese Natural Language Tracker (SNLT), which brings the\nadvancements in visual tracking to the tracking by natural language (NL)\ndescriptions task. The proposed SNLT is applicable to a wide range of Siamese\ntrackers, providing a new class of baselines for the tracking by NL task and\npromising future improvements from the advancements of Siamese trackers. The\ncarefully designed architecture of the Siamese Natural Language Region Proposal\nNetwork (SNL-RPN), together with the Dynamic Aggregation of vision and language\nmodalities, is introduced to perform the tracking by NL task. Empirical results\nover tracking benchmarks with NL annotations show that the proposed SNLT\nimproves Siamese trackers by 3 to 7 percentage points with a slight tradeoff of\nspeed. The proposed SNLT outperforms all NL trackers to-date and is competitive\namong state-of-the-art real-time trackers on LaSOT benchmarks while running at\n50 frames per second on a single GPU.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:16:32 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 18:03:24 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Feng", "Qi", ""], ["Ablavsky", "Vitaly", ""], ["Bai", "Qinxun", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1912.02079", "submitter": "Chaitanya Kaul", "authors": "Chaitanya Kaul, Nick Pears, Hang Dai, Roderick Murray-Smith, Suresh\n  Manandhar", "title": "FocusNet++: Attentive Aggregated Transformations for Efficient and\n  Accurate Medical Image Segmentation", "comments": "Published at ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new residual block for convolutional neural networks and\ndemonstrate its state-of-the-art performance in medical image segmentation. We\ncombine attention mechanisms with group convolutions to create our group\nattention mechanism, which forms the fundamental building block of our network,\nFocusNet++. We employ a hybrid loss based on balanced cross entropy, Tversky\nloss and the adaptive logarithmic loss to enhance the performance along with\nfast convergence. Our results show that FocusNet++ achieves state-of-the-art\nresults across various benchmark metrics for the ISIC 2018 melanoma\nsegmentation and the cell nuclei segmentation datasets with fewer parameters\nand FLOPs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:10:26 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 23:20:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaul", "Chaitanya", ""], ["Pears", "Nick", ""], ["Dai", "Hang", ""], ["Murray-Smith", "Roderick", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1912.02084", "submitter": "Qiming Yang", "authors": "Qiming Yang, Hongyang Chao, Dan Nguyen, and Steve Jiang", "title": "Mining Domain Knowledge: Improved Framework towards Automatically\n  Standardizing Anatomical Structure Nomenclature in Radiotherapy", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic standardization of nomenclature for anatomical structures in\nradiotherapy (RT) clinical data is a critical prerequisite for data curation\nand data-driven research in the era of big data and artificial intelligence,\nbut it is currently an unmet need. Existing methods either cannot handle\ncross-institutional datasets or suffer from heavy imbalance and poor-quality\ndelineation in clinical RT datasets. To solve these problems, we propose an\nautomated structure nomenclature standardization framework, 3D Non-local\nNetwork with Voting (3DNNV). This framework consists of an improved data\nprocessing strategy, namely, adaptive sampling and adaptive cropping (ASAC)\nwith voting, and an optimized feature extraction module. The framework\nsimulates clinicians' domain knowledge and recognition mechanisms to identify\nsmall-volume organs at risk (OARs) with heavily imbalanced data better than\nother methods. We used partial data from an open-source head-and-neck cancer\ndataset to train the model, then tested the model on three cross-institutional\ndatasets to demonstrate its generalizability. 3DNNV outperformed the baseline\nmodel, achieving higher average true positive rates (TPR) overall categories on\nthe three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More\nimportantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to\n91.17%, in terms of F1 score for a small-volume OAR with only 9 training\nsamples. The results show that 3DNNV can be applied to identify OARs, even\nerror-prone ones. Furthermore, we discussed the limitations and applicability\nof the framework in practical scenarios. The framework we developed can assist\nin standardizing structure nomenclature to facilitate data-driven clinical\nresearch in cancer radiotherapy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:28:23 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 16:24:29 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 13:34:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yang", "Qiming", ""], ["Chao", "Hongyang", ""], ["Nguyen", "Dan", ""], ["Jiang", "Steve", ""]]}, {"id": "1912.02085", "submitter": "Jinghan Yang", "authors": "Jinghan Yang, Ayan Chakrabarti, Yevgeniy Vorobeychik", "title": "Protecting Geolocation Privacy of Photo Collections", "comments": "AAAI 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People increasingly share personal information, including their photos and\nphoto collections, on social media. This information, however, can compromise\nindividual privacy, particularly as social media platforms use it to infer\ndetailed models of user behavior, including tracking their location. We\nconsider the specific issue of location privacy as potentially revealed by\nposting photo collections, which facilitate accurate geolocation with the help\nof deep learning methods even in the absence of geotags. One means to limit\nassociated inadvertent geolocation privacy disclosure is by carefully pruning\nselect photos from photo collections before these are posted publicly. We study\nthis problem formally as a combinatorial optimization problem in the context of\ngeolocation prediction facilitated by deep learning. We first demonstrate the\ncomplexity both by showing that a natural greedy algorithm can be arbitrarily\nbad and by proving that the problem is NP-Hard. We then exhibit an important\ntractable special case, as well as a more general approach based on\nmixed-integer linear programming. Through extensive experiments on real photo\ncollections, we demonstrate that our approaches are indeed highly effective at\npreserving geolocation privacy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:28:52 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Yang", "Jinghan", ""], ["Chakrabarti", "Ayan", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1912.02094", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza", "title": "A Step Towards Exposing Bias in Trained Deep Convolutional Neural\n  Network Models", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World. arXiv admin note: substantial text overlap with\n  arXiv:1908.01224", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Smooth Grad-CAM++, a technique which combines two recent\ntechniques: SMOOTHGRAD and Grad-CAM++. Smooth Grad-CAM++ has the capability of\neither visualizing a layer, subset of feature maps, or subset of neurons within\na feature map at each instance. We experimented with few images, and we\ndiscovered that Smooth Grad-CAM++ produced more visually sharp maps with larger\nnumber of salient pixels highlighted in the given input images when compared\nwith other methods. Smooth Grad-CAM++ will give insight into what our deep CNN\nmodels (including models trained on medical scan or imagery) learn. Hence\ninforming decisions on creating a representative training set.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:06:31 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Omeiza", "Daniel", ""]]}, {"id": "1912.02096", "submitter": "Lorenzo Porzi", "authors": "Lorenzo Porzi, Markus Hofinger, Idoia Ruiz, Joan Serrat, Samuel Rota\n  Bul\\`o, Peter Kontschieder", "title": "Learning Multi-Object Tracking and Segmentation from Automatic\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we contribute a novel pipeline to automatically generate\ntraining data, and to improve over state-of-the-art multi-object tracking and\nsegmentation (MOTS) methods. Our proposed track mining algorithm turns raw\nstreet-level videos into high-fidelity MOTS training data, is scalable and\novercomes the need of expensive and time-consuming manual annotation\napproaches. We leverage state-of-the-art instance segmentation results in\ncombination with optical flow predictions, also trained on automatically\nharvested training data. Our second major contribution is MOTSNet - a deep\nlearning, tracking-by-detection architecture for MOTS - deploying a novel\nmask-pooling layer for improved object association over time. Training MOTSNet\nwith our automatically extracted data leads to significantly improved sMOTSA\nscores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and\nMOTSNet improves by +4.1% over previously best methods on the MOTSChallenge\ndataset. Our most impressive finding is that we can improve over previous\nbest-performing works, even in complete absence of manually annotated MOTS\ntraining data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:38:24 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 13:38:51 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 09:13:31 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Porzi", "Lorenzo", ""], ["Hofinger", "Markus", ""], ["Ruiz", "Idoia", ""], ["Serrat", "Joan", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1912.02109", "submitter": "Bill Yang Cai", "authors": "Bill Cai, Xiaojiang Li, Carlo Ratti", "title": "Quantifying Urban Canopy Cover with Deep Convolutional Neural Networks", "comments": "NeurIPS 2019 Workshop on Climate Change AI at Vancouver, British\n  Columbia, Canada. arXiv admin note: text overlap with arXiv:1808.04754", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban canopy cover is important to mitigate the impact of climate change.\nYet, existing quantification of urban greenery is either manual and not\nscalable, or use traditional computer vision methods that are inaccurate. We\ntrain deep convolutional neural networks (DCNNs) on datasets used for\nself-driving cars to estimate urban greenery instead, and find that our\nsemantic segmentation and direct end-to-end estimation method are more accurate\nand scalable, reducing mean absolute error of estimating the Green View Index\n(GVI) metric from 10.1% to 4.67%. With the revised DCNN methods, the Treepedia\nproject was able to scale and analyze canopy cover in 22 cities\ninternationally, sparking interest and action in public policy and research\nfields.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:14:53 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Cai", "Bill", ""], ["Li", "Xiaojiang", ""], ["Ratti", "Carlo", ""]]}, {"id": "1912.02153", "submitter": "Hanwei Zhang", "authors": "Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg", "title": "Walking on the Edge: Fast, Low-Distortion Adversarial Examples", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIFS.2020.3021899", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples of deep neural networks are receiving ever increasing\nattention because they help in understanding and reducing the sensitivity to\ntheir input. This is natural given the increasing applications of deep neural\nnetworks in our everyday lives. When white-box attacks are almost always\nsuccessful, it is typically only the distortion of the perturbations that\nmatters in their evaluation.\n  In this work, we argue that speed is important as well, especially when\nconsidering that fast attacks are required by adversarial training. Given more\ntime, iterative methods can always find better solutions. We investigate this\nspeed-distortion trade-off in some depth and introduce a new attack called\nboundary projection (BP) that improves upon existing methods by a large margin.\nOur key idea is that the classification boundary is a manifold in the image\nspace: we therefore quickly reach the boundary and then optimize distortion on\nthis manifold.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:04:53 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 12:07:15 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhang", "Hanwei", ""], ["Avrithis", "Yannis", ""], ["Furon", "Teddy", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "1912.02155", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi", "title": "Visual Reaction: Learning to Play Catch with Your Drone", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of visual reaction: the task of\ninteracting with dynamic environments where the changes in the environment are\nnot necessarily caused by the agent itself. Visual reaction entails predicting\nthe future changes in a visual environment and planning accordingly. We study\nthe problem of visual reaction in the context of playing catch with a drone in\nvisually rich synthetic environments. This is a challenging problem since the\nagent is required to learn (1) how objects with different physical properties\nand shapes move, (2) what sequence of actions should be taken according to the\nprediction, (3) how to adjust the actions based on the visual feedback from the\ndynamic environment (e.g., when objects bouncing off a wall), and (4) how to\nreason and act with an unexpected state change in a timely manner. We propose a\nnew dataset for this task, which includes 30K throws of 20 types of objects in\ndifferent directions with different forces. Our results show that our model\nthat integrates a forecaster with a planner outperforms a set of strong\nbaselines that are based on tracking as well as pure model-based and model-free\nRL baselines. The code and dataset are available at\ngithub.com/KuoHaoZeng/Visual_Reaction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:11:26 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 04:45:20 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Mottaghi", "Roozbeh", ""], ["Weihs", "Luca", ""], ["Farhadi", "Ali", ""]]}, {"id": "1912.02184", "submitter": "Daniel Zoran", "authors": "Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott,\n  Pushmeet Kohl", "title": "Towards Robust Image Classification Using Sequential Attention Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to augment a modern neural-network architecture with\nan attention model inspired by human perception. Specifically, we adversarially\ntrain and analyze a neural model incorporating a human inspired, visual\nattention component that is guided by a recurrent top-down sequential process.\nOur experimental evaluation uncovers several notable findings about the\nrobustness and behavior of this new model. First, introducing attention to the\nmodel significantly improves adversarial robustness resulting in\nstate-of-the-art ImageNet accuracies under a wide range of random targeted\nattack strengths. Second, we show that by varying the number of attention steps\n(glances/fixations) for which the model is unrolled, we are able to make its\ndefense capabilities stronger, even in light of stronger attacks --- resulting\nin a \"computational race\" between the attacker and the defender. Finally, we\nshow that some of the adversarial examples generated by attacking our model are\nquite different from conventional adversarial examples --- they contain global,\nsalient and spatially coherent structures coming from the target class that\nwould be recognizable even to a human, and work by distracting the attention of\nthe model away from the main object in the original image.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:58:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zoran", "Daniel", ""], ["Chrzanowski", "Mike", ""], ["Huang", "Po-Sen", ""], ["Gowal", "Sven", ""], ["Mott", "Alex", ""], ["Kohl", "Pushmeet", ""]]}, {"id": "1912.02202", "submitter": "E. Canessa", "authors": "Enrique Canessa and Livio Tenze", "title": "MORPHOLO C++ Library for glasses-free multi-view stereo vision and\n  streaming of live 3D video", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The MORPHOLO C++ extended Library allows to convert a specific stereoscopic\nsnapshot into a Native multi-view image through morphing algorithms taking into\naccount display calibration data for specific slanted lenticular 3D monitors.\nMORPHOLO can also be implemented for glasses-free live applicatons of 3D video\nstreaming, and for diverse innovative scientific, engineering and 3D video game\napplications -see http://www.morpholo.it\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:00:15 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Canessa", "Enrique", ""], ["Tenze", "Livio", ""]]}, {"id": "1912.02241", "submitter": "Jing Bi", "authors": "Jing Bi, Vikas Dhiman, Tianyou Xiao, Chenliang Xu", "title": "Learning from Interventions using Hierarchical Policies for Safe\n  Learning", "comments": "Accepted for publication at the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from Demonstrations (LfD) via Behavior Cloning (BC) works well on\nmultiple complex tasks. However, a limitation of the typical LfD approach is\nthat it requires expert demonstrations for all scenarios, including those in\nwhich the algorithm is already well-trained. The recently proposed Learning\nfrom Interventions (LfI) overcomes this limitation by using an expert overseer.\nThe expert overseer only intervenes when it suspects that an unsafe action is\nabout to be taken. Although LfI significantly improves over LfD, the\nstate-of-the-art LfI fails to account for delay caused by the expert's reaction\ntime and only learns short-term behavior. We address these limitations by 1)\ninterpolating the expert's interventions back in time, and 2) by splitting the\npolicy into two hierarchical levels, one that generates sub-goals for the\nfuture and another that generates actions to reach those desired sub-goals.\nThis sub-goal prediction forces the algorithm to learn long-term behavior while\nalso being robust to the expert's reaction time. Our experiments show that LfI\nusing sub-goals in a hierarchical policy framework trains faster and achieves\nbetter asymptotic performance than typical LfD.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:28:51 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Bi", "Jing", ""], ["Dhiman", "Vikas", ""], ["Xiao", "Tianyou", ""], ["Xu", "Chenliang", ""]]}, {"id": "1912.02249", "submitter": "Senthil Yogamani", "authors": "Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi\n  Kumar, Pavel Krizek, Fabian Burger and Senthil Yogamani", "title": "Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling\n  Detection in Autonomous Driving", "comments": "Camera ready version + supplementary material. Accepted for\n  presentation at Winter Conference on Applications of Computer Vision 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-angle fisheye cameras are commonly used in automated driving for parking\nand low-speed navigation tasks. Four of such cameras form a surround-view\nsystem that provides a complete and detailed view of the vehicle. These cameras\nare directly exposed to harsh environmental settings and can get soiled very\neasily by mud, dust, water, frost. Soiling on the camera lens can severely\ndegrade the visual perception algorithms, and a camera cleaning system\ntriggered by a soiling detection algorithm is increasingly being deployed.\nWhile adverse weather conditions, such as rain, are getting attention recently,\nthere is only limited work on general soiling. The main reason is the\ndifficulty in collecting a diverse dataset as it is a relatively rare event. We\npropose a novel GAN based algorithm for generating unseen patterns of soiled\nimages. Additionally, the proposed method automatically provides the\ncorresponding soiling masks eliminating the manual annotation cost.\nAugmentation of the generated soiled images for training improves the accuracy\nof soiling detection tasks significantly by 18% demonstrating its usefulness.\nThe manually annotated soiling dataset and the generated augmentation dataset\nwill be made public. We demonstrate the generalization of our fisheye trained\nGAN model on the Cityscapes dataset. We provide an empirical evaluation of the\ndegradation of the semantic segmentation algorithm with the soiled data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:01:06 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 22:07:23 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 21:13:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Uricar", "Michal", ""], ["Sistu", "Ganesh", ""], ["Rashed", "Hazem", ""], ["Vobecky", "Antonin", ""], ["Kumar", "Varun Ravi", ""], ["Krizek", "Pavel", ""], ["Burger", "Fabian", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1912.02252", "submitter": "Wei Ke", "authors": "Wei Ke and Tianliang Zhang and Zeyi Huang and Qixiang Ye and\n  Jianzhuang Liu and Dong Huang", "title": "Multiple Anchor Learning for Visual Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and localization are two pillars of visual object detectors.\nHowever, in CNN-based detectors, these two modules are usually optimized under\na fixed set of candidate (or anchor) bounding boxes. This configuration\nsignificantly limits the possibility to jointly optimize classification and\nlocalization. In this paper, we propose a Multiple Instance Learning (MIL)\napproach that selects anchors and jointly optimizes the two modules of a\nCNN-based object detector. Our approach, referred to as Multiple Anchor\nLearning (MAL), constructs anchor bags and selects the most representative\nanchors from each bag. Such an iterative selection process is potentially\nNP-hard to optimize. To address this issue, we solve MAL by repetitively\ndepressing the confidence of selected anchors by perturbing their corresponding\nfeatures. In an adversarial selection-depression manner, MAL not only pursues\noptimal solutions but also fully leverages multiple anchors/features to learn a\ndetection model. Experiments show that MAL improves the baseline RetinaNet with\nsignificant margins on the commonly used MS-COCO object detection benchmark and\nachieves new state-of-the-art detection performance compared with recent\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:25:07 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ke", "Wei", ""], ["Zhang", "Tianliang", ""], ["Huang", "Zeyi", ""], ["Ye", "Qixiang", ""], ["Liu", "Jianzhuang", ""], ["Huang", "Dong", ""]]}, {"id": "1912.02256", "submitter": "Jonathan Stroud", "authors": "Jonathan C. Stroud, Ryan McCaffrey, Rada Mihalcea, Jia Deng, and Olga\n  Russakovsky", "title": "Compositional Temporal Visual Grounding of Natural Language Event\n  Descriptions", "comments": "Project page: jonathancstroud.com/ctg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal grounding entails establishing a correspondence between natural\nlanguage event descriptions and their visual depictions. Compositional modeling\nbecomes central: we first ground atomic descriptions \"girl eating an apple,\"\n\"batter hitting the ball\" to short video segments, and then establish the\ntemporal relationships between the segments. This compositional structure\nenables models to recognize a wider variety of events not seen during training\nthrough recognizing their atomic sub-events. Explicit temporal modeling\naccounts for a wide variety of temporal relationships that can be expressed in\nlanguage: e.g., in the description \"girl stands up from the table after eating\nan apple\" the visual ordering of the events is reversed, with first \"eating an\napple\" followed by \"standing up from the table.\" We leverage these observations\nto develop a unified deep architecture, CTG-Net, to perform temporal grounding\nof natural language event descriptions to videos. We demonstrate that our\nsystem outperforms prior state-of-the-art methods on the DiDeMo, Tempo-TL, and\nTempo-HL temporal grounding datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:36:16 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Stroud", "Jonathan C.", ""], ["McCaffrey", "Ryan", ""], ["Mihalcea", "Rada", ""], ["Deng", "Jia", ""], ["Russakovsky", "Olga", ""]]}, {"id": "1912.02259", "submitter": "Muhammad Aminul Islam", "authors": "Muhammad Aminul Islam and Bryce Murray and Andrew Buck and Derek T.\n  Anderson and Grant Scott and Mihail Popescu and James Keller", "title": "Extending the Morphological Hit-or-Miss Transform to Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most deep learning architectures are built on convolution, alternative\nfoundations like morphology are being explored for purposes like\ninterpretability and its connection to the analysis and processing of geometric\nstructures. The morphological hit-or-miss operation has the advantage that it\ntakes into account both foreground and background information when evaluating\ntarget shape in an image. Herein, we identify limitations in existing\nhit-or-miss neural definitions and we formulate an optimization problem to\nlearn the transform relative to deeper architectures. To this end, we model the\nsemantically important condition that the intersection of the hit and miss\nstructuring elements (SEs) should be empty and we present a way to express\nDon't Care (DNC), which is important for denoting regions of an SE that are not\nrelevant to detecting a target pattern. Our analysis shows that convolution, in\nfact, acts like a hit-miss transform through semantic interpretation of its\nfilter differences. On these premises, we introduce an extension that\noutperforms conventional convolution on benchmark data. Quantitative\nexperiments are provided on synthetic and benchmark data, showing that the\ndirect encoding hit-or-miss transform provides better interpretability on\nlearned shapes consistent with objects whereas our morphologically inspired\ngeneralized convolution yields higher classification accuracy. Last,\nqualitative hit and miss filter visualizations are provided relative to single\nmorphological layer.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:42:34 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 00:11:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Islam", "Muhammad Aminul", ""], ["Murray", "Bryce", ""], ["Buck", "Andrew", ""], ["Anderson", "Derek T.", ""], ["Scott", "Grant", ""], ["Popescu", "Mihail", ""], ["Keller", "James", ""]]}, {"id": "1912.02279", "submitter": "Beidi Chen", "authors": "Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava,\n  Animesh Garg, Anima Anandkumar", "title": "Angular Visual Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent convolutional neural networks (CNNs) have led to impressive\nperformance but often suffer from poor calibration. They tend to be\noverconfident, with the model confidence not always reflecting the underlying\ntrue ambiguity and hardness. In this paper, we propose angular visual hardness\n(AVH), a score given by the normalized angular distance between the sample\nfeature embedding and the target classifier to measure sample hardness. We\nvalidate this score with an in-depth and extensive scientific study, and\nobserve that CNN models with the highest accuracy also have the best AVH\nscores. This agrees with an earlier finding that state-of-art models improve on\nthe classification of harder examples. We observe that the training dynamics of\nAVH is vastly different compared to the training loss. Specifically, AVH\nquickly reaches a plateau for all samples even though the training loss keeps\nimproving. This suggests the need for designing better loss functions that can\ntarget harder examples more effectively. We also find that AVH has a\nstatistically significant correlation with human visual hardness. Finally, we\ndemonstrate the benefit of AVH to a variety of applications such as\nself-training for domain adaptation and domain generalization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:12:42 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:23:12 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 00:23:39 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 20:58:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chen", "Beidi", ""], ["Liu", "Weiyang", ""], ["Yu", "Zhiding", ""], ["Kautz", "Jan", ""], ["Shrivastava", "Anshumali", ""], ["Garg", "Animesh", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1912.02292", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\n  Ilya Sutskever", "title": "Deep Double Descent: Where Bigger Models and More Data Hurt", "comments": "G.K. and Y.B. contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a variety of modern deep learning tasks exhibit a\n\"double-descent\" phenomenon where, as we increase model size, performance first\ngets worse and then gets better. Moreover, we show that double descent occurs\nnot just as a function of model size, but also as a function of the number of\ntraining epochs. We unify the above phenomena by defining a new complexity\nmeasure we call the effective model complexity and conjecture a generalized\ndouble descent with respect to this measure. Furthermore, our notion of model\ncomplexity allows us to identify certain regimes where increasing (even\nquadrupling) the number of train samples actually hurts test performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:47:31 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Kaplun", "Gal", ""], ["Bansal", "Yamini", ""], ["Yang", "Tristan", ""], ["Barak", "Boaz", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1912.02314", "submitter": "Adam Yedidia", "authors": "Miika Aittala, Prafull Sharma, Lukas Murmann, Adam B. Yedidia, Gregory\n  W. Wornell, William T. Freeman, Fredo Durand", "title": "Computational Mirrors: Blind Inverse Light Transport by Deep Matrix\n  Factorization", "comments": "14 pages, 5 figures, Advances in Neural Information Processing\n  Systems 2019", "journal-ref": "Aittala, Miika, et al. \"Computational Mirrors: Blind Inverse Light\n  Transport by Deep Matrix Factorization.\" Advances in Neural Information\n  Processing Systems. 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recover a video of the motion taking place in a hidden scene by observing\nchanges in indirect illumination in a nearby uncalibrated visible region. We\nsolve this problem by factoring the observed video into a matrix product\nbetween the unknown hidden scene video and an unknown light transport matrix.\nThis task is extremely ill-posed, as any non-negative factorization will\nsatisfy the data. Inspired by recent work on the Deep Image Prior, we\nparameterize the factor matrices using randomly initialized convolutional\nneural networks trained in a one-off manner, and show that this results in\ndecompositions that reflect the true motion in the hidden scene.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:06:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Aittala", "Miika", ""], ["Sharma", "Prafull", ""], ["Murmann", "Lukas", ""], ["Yedidia", "Adam B.", ""], ["Wornell", "Gregory W.", ""], ["Freeman", "William T.", ""], ["Durand", "Fredo", ""]]}, {"id": "1912.02315", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee", "title": "12-in-1: Multi-Task Vision and Language Representation Learning", "comments": "Jiasen Lu and Vedanuj Goswami contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of vision-and-language research focuses on a small but diverse set of\nindependent tasks and supporting datasets often studied in isolation; however,\nthe visually-grounded language understanding skills required for success at\nthese tasks overlap significantly. In this work, we investigate these\nrelationships between vision-and-language tasks by developing a large-scale,\nmulti-task training regime. Our approach culminates in a single model on 12\ndatasets from four broad categories of task including visual question\nanswering, caption-based image retrieval, grounding referring expressions, and\nmulti-modal verification. Compared to independently trained single-task models,\nthis represents a reduction from approximately 3 billion parameters to 270\nmillion while simultaneously improving performance by 2.05 points on average\nacross tasks. We use our multi-task framework to perform in-depth analysis of\nthe effect of joint training diverse tasks. Further, we show that finetuning\ntask-specific models from our single multi-task model can lead to further\nimprovements, achieving performance at or above the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:07:35 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 21:39:42 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lu", "Jiasen", ""], ["Goswami", "Vedanuj", ""], ["Rohrbach", "Marcus", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1912.02323", "submitter": "Michael Snower", "authors": "Michael Snower, Asim Kadav, Farley Lai, Hans Peter Graf", "title": "15 Keypoints Is All You Need", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose tracking is an important problem that requires identifying unique human\npose-instances and matching them temporally across different frames of a video.\nHowever, existing pose tracking methods are unable to accurately model temporal\nrelationships and require significant computation, often computing the tracks\noffline. We present an efficient Multi-person Pose Tracking method, KeyTrack,\nthat only relies on keypoint information without using any RGB or optical flow\ninformation to track human keypoints in real-time. Keypoints are tracked using\nour Pose Entailment method, in which, first, a pair of pose estimates is\nsampled from different frames in a video and tokenized. Then, a\nTransformer-based network makes a binary classification as to whether one pose\ntemporally follows another. Furthermore, we improve our top-down pose\nestimation method with a novel, parameter-free, keypoint refinement technique\nthat improves the keypoint estimates used during the Pose Entailment step. We\nachieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18\nbenchmarks while using only a fraction of the computation required by most\nother methods for computing the tracking information.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:38:55 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 17:18:57 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Snower", "Michael", ""], ["Kadav", "Asim", ""], ["Lai", "Farley", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1912.02332", "submitter": "Zelin Ye", "authors": "Zelin Ye, Yan Hao, Liang Xu, Rui Zhu, Cewu Lu", "title": "3D Objectness Estimation via Bottom-up Regret Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D objectness estimation, namely discovering semantic objects from 3D scene,\nis a challenging and significant task in 3D understanding. In this paper, we\npropose a 3D objectness method working in a bottom-up manner. Beginning with\nover-segmented 3D segments, we iteratively group them into object proposals by\nlearning an ingenious grouping predictor to determine whether two 3D segments\ncan be grouped or not. To enhance robustness, a novel regret mechanism is\npresented to withdraw incorrect grouping operations. Hence the irreparable\nconsequences brought by mistaken grouping in prior bottom-up works can be\ngreatly reduced. Our experiments show that our method outperforms\nstate-of-the-art 3D objectness methods with a small number of proposals in two\ndifficult datasets, GMU-kitchen and CTD. Further ablation study also\ndemonstrates the effectiveness of our grouping predictor and regret mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 01:20:56 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ye", "Zelin", ""], ["Hao", "Yan", ""], ["Xu", "Liang", ""], ["Zhu", "Rui", ""], ["Lu", "Cewu", ""]]}, {"id": "1912.02340", "submitter": "Jun Wan", "authors": "Ajian Liu, Zichang Tan, Xuan Li, Jun Wan, Sergio Escalera, Guodong\n  Guo, Stan Z. Li", "title": "Static and Dynamic Fusion for Multi-modal Cross-ethnicity Face\n  Anti-spoofing", "comments": "10 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regardless of the usage of deep learning and handcrafted methods, the dynamic\ninformation from videos and the effect of cross-ethnicity are rarely considered\nin face anti-spoofing. In this work, we propose a static-dynamic fusion\nmechanism for multi-modal face anti-spoofing. Inspired by motion divergences\nbetween real and fake faces, we incorporate the dynamic image calculated by\nrank pooling with static information into a conventional neural network (CNN)\nfor each modality (i.e., RGB, Depth and infrared (IR)). Then, we develop a\npartially shared fusion method to learn complementary information from multiple\nmodalities. Furthermore, in order to study the generalization capability of the\nproposal in terms of cross-ethnicity attacks and unknown spoofs, we introduce\nthe largest public cross-ethnicity Face Anti-spoofing (CASIA-CeFA) dataset,\ncovering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack\ntypes. Experiments demonstrate that the proposed method achieves\nstate-of-the-art results on CASIA-CeFA, CASIA-SURF, OULU-NPU and SiW.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 01:39:56 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 00:55:58 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Ajian", ""], ["Tan", "Zichang", ""], ["Li", "Xuan", ""], ["Wan", "Jun", ""], ["Escalera", "Sergio", ""], ["Guo", "Guodong", ""], ["Li", "Stan Z.", ""]]}, {"id": "1912.02357", "submitter": "Haijuan Hu", "authors": "Haijuan Hu, Jacques Froment, Baoyan Wang, Xiequan Fan", "title": "Spatial-Frequency Domain Nonlocal Total Variation for Image Denoising", "comments": "36 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the pioneering works of Rudin, Osher and Fatemi on total variation\n(TV) and of Buades, Coll and Morel on non-local means (NL-means), the last\ndecade has seen a large number of denoising methods mixing these two\napproaches, starting with the nonlocal total variation (NLTV) model. The\npresent article proposes an analysis of the NLTV model for image denoising as\nwell as a number of improvements, the most important of which being to apply\nthe denoising both in the space domain and in the Fourier domain, in order to\nexploit the complementarity of the representation of image data in both\ndomains. A local version obtained by a regionwise implementation followed by an\naggregation process, called Local Spatial-Frequency NLTV (L- SFNLTV) model, is\nfinally proposed as a new reference algorithm for image denoising among the\nfamily of approaches mixing TV and NL operators. The experiments show the great\nperformance of L-SFNLTV, both in terms of image quality and of computational\nspeed, comparing with other recently proposed NLTV-related methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:02:35 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hu", "Haijuan", ""], ["Froment", "Jacques", ""], ["Wang", "Baoyan", ""], ["Fan", "Xiequan", ""]]}, {"id": "1912.02379", "submitter": "Vishvak Murahari", "authors": "Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das", "title": "Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art\n  Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in visual dialog has focused on training deep neural models on\nVisDial in isolation. Instead, we present an approach to leverage pretraining\non related vision-language datasets before transferring to visual dialog. We\nadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn\nvisually-grounded conversations. Our model is pretrained on the Conceptual\nCaptions and Visual Question Answering datasets, and finetuned on VisDial. Our\nbest single model outperforms prior published work (including model ensembles)\nby more than 1% absolute on NDCG and MRR. Next, we find that additional\nfinetuning using \"dense\" annotations in VisDial leads to even higher NDCG --\nmore than 10% over our base model -- but hurts MRR -- more than 17% below our\nbase model! This highlights a trade-off between the two primary metrics -- NDCG\nand MRR -- which we find is due to dense annotations not correlating well with\nthe original ground-truth answers to questions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:51:11 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 03:12:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Murahari", "Vishvak", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Das", "Abhishek", ""]]}, {"id": "1912.02398", "submitter": "Jie An", "authors": "Jie An, Haoyi Xiong, Jun Huan and Jiebo Luo", "title": "Ultrafast Photorealistic Style Transfer via Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in photorealistic style transfer is that an algorithm\nshould faithfully transfer the style of a reference photo to a content photo\nwhile the generated image should look like one captured by a camera. Although\nseveral photorealistic style transfer algorithms have been proposed, they need\nto rely on post- and/or pre-processing to make the generated images look\nphotorealistic. If we disable the additional processing, these algorithms would\nfail to produce plausible photorealistic stylization in terms of detail\npreservation and photorealism. In this work, we propose an effective solution\nto these issues. Our method consists of a construction step (C-step) to build a\nphotorealistic stylization network and a pruning step (P-step) for\nacceleration. In the C-step, we propose a dense auto-encoder named PhotoNet\nbased on a carefully designed pre-analysis. PhotoNet integrates a feature\naggregation module (BFA) and instance normalized skip links (INSL). To generate\nfaithful stylization, we introduce multiple style transfer modules in the\ndecoder and INSLs. PhotoNet significantly outperforms existing algorithms in\nterms of both efficiency and effectiveness. In the P-step, we adopt a neural\narchitecture search method to accelerate PhotoNet. We propose an automatic\nnetwork pruning framework in the manner of teacher-student learning for\nphotorealistic stylization. The network architecture named PhotoNAS resulted\nfrom the search achieves significant acceleration over PhotoNet while keeping\nthe stylization effects almost intact. We conduct extensive experiments on both\nimage and video transfer. The results show that our method can produce\nfavorable results while achieving 20-30 times acceleration in comparison with\nthe existing state-of-the-art approaches. It is worth noting that the proposed\nalgorithm accomplishes better performance without any pre- or post-processing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:51:54 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 12:56:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["An", "Jie", ""], ["Xiong", "Haoyi", ""], ["Huan", "Jun", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.02401", "submitter": "Megha Nawhal", "authors": "Megha Nawhal, Mengyao Zhai, Andreas Lehrmann, Leonid Sigal, Greg Mori", "title": "Generating Videos of Zero-Shot Compositions of Actions and Objects", "comments": "Accepted at ECCV'20; Project Page:\n  https://www.sfu.ca/~mnawhal/projects/zs_hoi_generation.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity videos involve rich, varied interactions between people and\nobjects. In this paper we develop methods for generating such videos -- making\nprogress toward addressing the important, open problem of video generation in\ncomplex scenes. In particular, we introduce the task of generating human-object\ninteraction videos in a zero-shot compositional setting, i.e., generating\nvideos for action-object compositions that are unseen during training, having\nseen the target action and target object separately. This setting is\nparticularly important for generalization in human activity video generation,\nobviating the need to observe every possible action-object combination in\ntraining and thus avoiding the combinatorial explosion involved in modeling\ncomplex scenes. To generate human-object interaction videos, we propose a novel\nadversarial framework HOI-GAN which includes multiple discriminators focusing\non different aspects of a video. To demonstrate the effectiveness of our\nproposed framework, we perform extensive quantitative and qualitative\nevaluation on two challenging datasets: EPIC-Kitchens and\n20BN-Something-Something v2.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 06:09:13 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 18:21:27 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 06:40:50 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 06:01:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nawhal", "Megha", ""], ["Zhai", "Mengyao", ""], ["Lehrmann", "Andreas", ""], ["Sigal", "Leonid", ""], ["Mori", "Greg", ""]]}, {"id": "1912.02413", "submitter": "Xiu-Shen Wei", "authors": "Boyan Zhou and Quan Cui and Xiu-Shen Wei and Zhao-Min Chen", "title": "BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed\n  Visual Recognition", "comments": "Accepted by CVPR 2020; Our work won the first place in the\n  iNaturalist 2019 large scale species classification competition, and our code\n  is open-source and available at https://github.com/Megvii-Nanjing/BBN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on tackling the challenging but natural visual recognition\ntask of long-tailed data distribution (i.e., a few classes occupy most of the\ndata, while most classes have rarely few samples). In the literature, class\nre-balancing strategies (e.g., re-weighting and re-sampling) are the prominent\nand effective methods proposed to alleviate the extreme imbalance for dealing\nwith long-tailed problems. In this paper, we firstly discover that these\nre-balancing methods achieving satisfactory recognition accuracy owe to that\nthey could significantly promote the classifier learning of deep networks.\nHowever, at the same time, they will unexpectedly damage the representative\nability of the learned deep features to some extent. Therefore, we propose a\nunified Bilateral-Branch Network (BBN) to take care of both representation\nlearning and classifier learning simultaneously, where each branch does perform\nits own duty separately. In particular, our BBN model is further equipped with\na novel cumulative learning strategy, which is designed to first learn the\nuniversal patterns and then pay attention to the tail data gradually. Extensive\nexperiments on four benchmark datasets, including the large-scale iNaturalist\nones, justify that the proposed BBN can significantly outperform\nstate-of-the-art methods. Furthermore, validation experiments can demonstrate\nboth our preliminary discovery and effectiveness of tailored designs in BBN for\nlong-tailed problems. Our method won the first place in the iNaturalist 2019\nlarge scale species classification competition, and our code is open-source and\navailable at https://github.com/Megvii-Nanjing/BBN.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:32:28 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 06:07:03 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 16:48:26 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 09:34:38 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zhou", "Boyan", ""], ["Cui", "Quan", ""], ["Wei", "Xiu-Shen", ""], ["Chen", "Zhao-Min", ""]]}, {"id": "1912.02417", "submitter": "QiKui Zhu", "authors": "Qikui Zhu, Bo Du, Pingkun Yan", "title": "OASIS: One-pass aligned Atlas Set for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is a fundamental task in medical image analysis.\nDespite that deep convolutional neural networks have gained stellar performance\nin this challenging task, they typically rely on large labeled datasets, which\nhave limited their extension to customized applications. By revisiting the\nsuperiority of atlas based segmentation methods, we present a new framework of\nOne-pass aligned Atlas Set for Images Segmentation (OASIS). To address the\nproblem of time-consuming iterative image registration used for atlas warping,\nthe proposed method takes advantage of the power of deep learning to achieve\none-pass image registration. In addition, by applying label constraint, OASIS\nalso makes the registration process to be focused on the regions to be\nsegmented for improving the performance of segmentation. Furthermore, instead\nof using image based similarity for label fusion, which can be distracted by\nthe large background areas, we propose a novel strategy to compute the label\nsimilarity based weights for label fusion. Our experimental results on the\nchallenging task of prostate MR image segmentation demonstrate that OASIS is\nable to significantly increase the segmentation performance compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:39:58 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhu", "Qikui", ""], ["Du", "Bo", ""], ["Yan", "Pingkun", ""]]}, {"id": "1912.02424", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, Stan Z. Li", "title": "Bridging the Gap Between Anchor-based and Anchor-free Detection via\n  Adaptive Training Sample Selection", "comments": "Accepted by CVPR 2020 as Oral; Best Paper Nomination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has been dominated by anchor-based detectors for several\nyears. Recently, anchor-free detectors have become popular due to the proposal\nof FPN and Focal Loss. In this paper, we first point out that the essential\ndifference between anchor-based and anchor-free detection is actually how to\ndefine positive and negative training samples, which leads to the performance\ngap between them. If they adopt the same definition of positive and negative\nsamples during training, there is no obvious difference in the final\nperformance, no matter regressing from a box or a point. This shows that how to\nselect positive and negative training samples is important for current object\ndetectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to\nautomatically select positive and negative samples according to statistical\ncharacteristics of object. It significantly improves the performance of\nanchor-based and anchor-free detectors and bridges the gap between them.\nFinally, we discuss the necessity of tiling multiple anchors per location on\nthe image to detect objects. Extensive experiments conducted on MS COCO support\nour aforementioned analysis and conclusions. With the newly introduced ATSS, we\nimprove state-of-the-art detectors by a large margin to $50.7\\%$ AP without\nintroducing any overhead. The code is available at\nhttps://github.com/sfzhang15/ATSS\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:49:56 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 17:00:35 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 15:22:32 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2020 10:54:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Shifeng", ""], ["Chi", "Cheng", ""], ["Yao", "Yongqiang", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1912.02441", "submitter": "Yusuf Artan", "authors": "Alperen Elihos, Burak Balci, Bensu Alkan, Yusuf Artan", "title": "Deep Learning Based Segmentation Free License Plate Recognition Using\n  Roadway Surveillance Camera Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart automated traffic enforcement solutions have been gaining popularity in\nrecent years. These solutions are ubiquitously used for seat-belt violation\ndetection, red-light violation detection and speed violation detection\npurposes. Highly accurate license plate recognition is an indispensable part of\nthese systems. However, general license plate recognition systems require high\nresolution images for high performance. In this study, we propose a novel\nlicense plate recognition method for general roadway surveillance cameras.\nProposed segmentation free license plate recognition algorithm utilizes deep\nlearning based object detection techniques in the character detection and\nrecognition process. Proposed method has been tested on 2000 images captured on\na roadway.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 09:10:47 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Elihos", "Alperen", ""], ["Balci", "Burak", ""], ["Alkan", "Bensu", ""], ["Artan", "Yusuf", ""]]}, {"id": "1912.02456", "submitter": "Bruno Lecouat", "authors": "Bruno Lecouat, Jean Ponce, Julien Mairal", "title": "Fully Trainable and Interpretable Non-Local Sparse Models for Image\n  Restoration", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self-similarity and sparsity principles have proven to be powerful\npriors for natural image modeling. We propose a novel differentiable relaxation\nof joint sparsity that exploits both principles and leads to a general\nframework for image restoration which is (1) trainable end to end, (2) fully\ninterpretable, and (3) much more compact than competing deep learning\narchitectures. We apply this approach to denoising, jpeg deblocking, and\ndemosaicking, and show that, with as few as 100K parameters, its performance on\nseveral standard benchmarks is on par or better than state-of-the-art methods\nthat may have an order of magnitude or more parameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 09:31:13 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 17:10:29 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 10:40:09 GMT"}, {"version": "v4", "created": "Fri, 26 Jun 2020 09:12:01 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 14:56:27 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lecouat", "Bruno", ""], ["Ponce", "Jean", ""], ["Mairal", "Julien", ""]]}, {"id": "1912.02470", "submitter": "Hao Chen", "authors": "Hao Chen, Mario Valerio Giuffrida, Peter Doerner, and Sotirios A.\n  Tsaftaris", "title": "Blind Inpainting of Large-scale Masks of Thin Structures with\n  Adversarial and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several imaging applications (vessels, retina, plant roots, road networks\nfrom satellites) require the accurate segmentation of thin structures for\nsubsequent analysis. Discontinuities (gaps) in the extracted foreground may\nhinder down-stream image-based analysis of biomarkers, organ structure and\ntopology. In this paper, we propose a general post-processing technique to\nrecover such gaps in large-scale segmentation masks. We cast this problem as a\nblind inpainting task, where the regions of missing lines in the segmentation\nmasks are not known to the algorithm, which we solve with an adversarially\ntrained neural network. One challenge of using large images is the memory\ncapacity of current GPUs. The typical approach of dividing a large image into\nsmaller patches to train the network does not guarantee global coherence of the\nreconstructed image that preserves structure and topology. We use adversarial\ntraining and reinforcement learning (Policy Gradient) to endow the model with\nboth global context and local details. We evaluate our method in several\ndatasets in medical imaging, plant science, and remote sensing. Our experiments\ndemonstrate that our model produces the most realistic and complete inpainted\nresults, outperforming other approaches. In a dedicated study on plant roots we\nfind that our approach is also comparable to human performance. Implementation\navailable at \\url{https://github.com/Hhhhhhhhhhao/Thin-Structure-Inpainting}.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:06:33 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Chen", "Hao", ""], ["Giuffrida", "Mario Valerio", ""], ["Doerner", "Peter", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1912.02487", "submitter": "Amir Mohammadi", "authors": "Raghavendra Ramachandra, Martin Stokkenes, Amir Mohammadi, Sushma\n  Venkatesh, Kiran Raja, Pankaj Wasnik, Eric Poiret, S\\'ebastien Marcel,\n  Christoph Busch", "title": "Smartphone Multi-modal Biometric Authentication: Database and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric-based verification is widely employed on the smartphones for\nvarious applications, including financial transactions. In this work, we\npresent a new multimodal biometric dataset (face, voice, and periocular)\nacquired using a smartphone. The new dataset is comprised of 150 subjects that\nare captured in six different sessions reflecting real-life scenarios of\nsmartphone assisted authentication. One of the unique features of this dataset\nis that it is collected in four different geographic locations representing a\ndiverse population and ethnicity. Additionally, we also present a multimodal\nPresentation Attack (PA) or spoofing dataset using a low-cost Presentation\nAttack Instrument (PAI) such as print and electronic display attacks. The novel\nacquisition protocols and the diversity of the data subjects collected from\ndifferent geographic locations will allow developing a novel algorithm for\neither unimodal or multimodal biometrics. Further, we also report the\nperformance evaluation of the baseline biometric verification and Presentation\nAttack Detection (PAD) on the newly collected dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:36:23 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ramachandra", "Raghavendra", ""], ["Stokkenes", "Martin", ""], ["Mohammadi", "Amir", ""], ["Venkatesh", "Sushma", ""], ["Raja", "Kiran", ""], ["Wasnik", "Pankaj", ""], ["Poiret", "Eric", ""], ["Marcel", "S\u00e9bastien", ""], ["Busch", "Christoph", ""]]}, {"id": "1912.02491", "submitter": "Shan Cao", "authors": "Shan Cao and Yuqian Yao and Gaoyun An", "title": "E2-Capsule Neural Networks for Facial Expression Recognition Using\n  AU-Aware Attention", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule neural network is a new and popular technique in deep learning.\nHowever, the traditional capsule neural network does not extract features\nsufficiently before the dynamic routing between the capsules. In this paper,\nthe one Double Enhanced Capsule Neural Network (E2-Capsnet) that uses AU-aware\nattention for facial expression recognition (FER) is proposed. The E2-Capsnet\ntakes advantage of dynamic routing between the capsules, and has two\nenhancement modules which are beneficial for FER. The first enhancement module\nis the convolutional neural network with AU-aware attention, which can help\nfocus on the active areas of the expression. The second enhancement module is\nthe capsule neural network with multiple convolutional layers, which enhances\nthe ability of the feature representation. Finally, squashing function is used\nto classify the facial expression. We demonstrate the effectiveness of\nE2-Capsnet on the two public benchmark datasets, RAF-DB and EmotioNet. The\nexperimental results show that our E2-Capsnet is superior to the\nstate-of-the-art methods. Our implementation will be publicly available online.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:44:08 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Cao", "Shan", ""], ["Yao", "Yuqian", ""], ["An", "Gaoyun", ""]]}, {"id": "1912.02494", "submitter": "Tomaso Fontanini", "authors": "Tomaso Fontanini, Eleonora Iotti, Luca Donati and Andrea Prati", "title": "MetalGAN: Multi-Domain Label-Less Image Synthesis Using cGANs and\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis is currently one of the most addressed image processing topic\nin computer vision and deep learning fields of study. Researchers have tackled\nthis problem focusing their efforts on its several challenging problems, e.g.\nimage quality and size, domain and pose changing, architecture of the networks,\nand so on. Above all, producing images belonging to different domains by using\na single architecture is a very relevant goal for image generation. In fact, a\nsingle multi-domain network would allow greater flexibility and robustness in\nthe image synthesis task than other approaches. This paper proposes a novel\narchitecture and a training algorithm, which are able to produce multi-domain\noutputs using a single network. A small portion of a dataset is intentionally\nused, and there are no hard-coded labels (or classes). This is achieved by\ncombining a conditional Generative Adversarial Network (cGAN) for image\ngeneration and a Meta-Learning algorithm for domain switch, and we called our\napproach MetalGAN. The approach has proved to be appropriate for solving the\nmulti-domain problem and it is validated on facial attribute transfer, using\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:47:08 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 09:40:52 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Fontanini", "Tomaso", ""], ["Iotti", "Eleonora", ""], ["Donati", "Luca", ""], ["Prati", "Andrea", ""]]}, {"id": "1912.02504", "submitter": "Pavel Bezmaternykh", "authors": "Pavel Bezmaternykh and Dmitry Nikolaev", "title": "A Document Skew Detection Method Using Fast Hough Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of document image analysis systems use a document skew detection\nalgorithm to simplify all its further processing stages. A huge amount of such\nalgorithms based on Hough transform (HT) analysis has already been proposed.\nDespite this, we managed to find only one work where the Fast Hough Transform\n(FHT) usage was suggested to solve the indicated problem. Unfortunately, no\nstudy of that method was provided. In this work, we propose and study a skew\ndetection algorithm for the document images which relies on FHT analysis. To\nmeasure this algorithm quality we use the dataset from the problem oriented\nDISEC'13 contest and its evaluation methodology. Obtained values for AED,\nTOP80, and CE criteria are equal to 0.086, 0.056, 68.80 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 11:07:14 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Bezmaternykh", "Pavel", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "1912.02512", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end iris recognition method designed\nspecifically for post-mortem samples, and thus serving as a perfect application\nfor iris biometrics in forensics. To our knowledge, it is the first method\nspecific for verification of iris samples acquired after demise. We have\nfine-tuned a convolutional neural network-based segmentation model with a large\nset of diversified iris data (including post-mortem and diseased eyes), and\ncombined Gabor kernels with newly designed, iris-specific kernels learnt by\nSiamese networks. The resulting method significantly outperforms the existing\noff-the-shelf iris recognition methods (both academic and commercial) on the\nnewly collected database of post-mortem iris images and for all available time\nhorizons since death. We make all models and the method itself available along\nwith this paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 11:31:56 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1912.02523", "submitter": "Eduardo Soares Mr", "authors": "Plamen Angelov, Eduardo Soares", "title": "Towards Explainable Deep Neural Networks (xDNN)", "comments": "Preprint submitted to the Neural Networks Journal for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an elegant solution that is directly addressing the\nbottlenecks of the traditional deep learning approaches and offers a clearly\nexplainable internal architecture that can outperform the existing methods,\nrequires very little computational resources (no need for GPUs) and short\ntraining times (in the order of seconds). The proposed approach, xDNN is using\nprototypes. Prototypes are actual training data samples (images), which are\nlocal peaks of the empirical data distribution called typicality as well as of\nthe data density. This generative model is identified in a closed form and\nequates to the pdf but is derived automatically and entirely from the training\ndata with no user- or problem-specific thresholds, parameters or intervention.\nThe proposed xDNN offers a new deep learning architecture that combines\nreasoning and learning in a synergy. It is non-iterative and non-parametric,\nwhich explains its efficiency in terms of time and computational resources.\nFrom the user perspective, the proposed approach is clearly understandable to\nhuman users. We tested it on some well-known benchmark data sets such as iRoads\nand Caltech-256. xDNN outperforms the other methods including deep learning in\nterms of accuracy, time to train and offers a clearly explainable classifier.\nIn fact, the result on the very hard Caltech-256 problem (which has 257\nclasses) represents a world record.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:01:15 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Angelov", "Plamen", ""], ["Soares", "Eduardo", ""]]}, {"id": "1912.02605", "submitter": "Shihua Zhang", "authors": "Zhiyang Zhang and Shihua Zhang", "title": "Towards Understanding Residual and Dilated Dense Neural Networks via\n  Convolutional Sparse Coding", "comments": "13 pages, 8 figures", "journal-ref": "National Science Review (2020)", "doi": null, "report-no": "nwaa159", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) and its variants have led to many\nstate-of-art results in various fields. However, a clear theoretical\nunderstanding about them is still lacking. Recently, multi-layer convolutional\nsparse coding (ML-CSC) has been proposed and proved to equal such simply\nstacked networks (plain networks). Here, we think three factors in each layer\nof it including the initialization, the dictionary design and the number of\niterations greatly affect the performance of ML-CSC. Inspired by these\nconsiderations, we propose two novel multi-layer models--residual convolutional\nsparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding\nmodel (MSD-CSC), which have close relationship with the residual neural network\n(ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively.\nMathematically, we derive the shortcut connection in ResNet as a special case\nof a new forward propagation rule on ML-CSC. We find a theoretical\ninterpretation of the dilated convolution and dense connection in MSDNet by\nanalyzing MSD-CSC, which gives a clear mathematical understanding about them.\nWe implement the iterative soft thresholding algorithm (ISTA) and its fast\nversion to solve Res-CSC and MSD-CSC, which can employ the unfolding operation\nfor further improvements. At last, extensive numerical experiments and\ncomparison with competing methods demonstrate their effectiveness using three\ntypical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:46:01 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 03:01:51 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhang", "Zhiyang", ""], ["Zhang", "Shihua", ""]]}, {"id": "1912.02620", "submitter": "Tian Xia", "authors": "Tian Xia, Agisilaos Chartsias, Chengjia Wang, Sotirios A. Tsaftaris", "title": "Learning to synthesise the ageing brain without longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:12:19 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 12:37:51 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 09:49:48 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 09:15:03 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xia", "Tian", ""], ["Chartsias", "Agisilaos", ""], ["Wang", "Chengjia", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1912.02678", "submitter": "Guy Shiran", "authors": "Guy Shiran, Daphna Weinshall", "title": "Multi-Modal Deep Clustering: Unsupervised Partitioning of Images", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering of unlabeled raw images is a daunting task, which has recently\nbeen approached with some success by deep learning methods. Here we propose an\nunsupervised clustering framework, which learns a deep neural network in an\nend-to-end fashion, providing direct cluster assignments of images without\nadditional processing. Multi-Modal Deep Clustering (MMDC), trains a deep\nnetwork to align its image embeddings with target points sampled from a\nGaussian Mixture Model distribution. The cluster assignments are then\ndetermined by mixture component association of image embeddings.\nSimultaneously, the same deep network is trained to solve an additional\nself-supervised task of predicting image rotations. This pushes the network to\nlearn more meaningful image representations that facilitate a better\nclustering. Experimental results show that MMDC achieves or exceeds\nstate-of-the-art performance on six challenging benchmarks. On natural image\ndatasets we improve on previous results with significant margins of up to 20%\nabsolute accuracy points, yielding an accuracy of 82% on CIFAR-10, 45% on\nCIFAR-100 and 69% on STL-10.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 16:03:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 18:14:37 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 10:16:11 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Shiran", "Guy", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1912.02707", "submitter": "Eli (Omid) David", "authors": "Daniel Rika, Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the\n  Reconstruction of Portuguese Tile Panels", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1319-1327, Prague, Czech Republic, July 2019", "doi": "10.1145/3321707.3321821", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel scheme, based on a unique combination of genetic\nalgorithms (GAs) and deep learning (DL), for the automatic reconstruction of\nPortuguese tile panels, a challenging real-world variant of the jigsaw puzzle\nproblem (JPP) with important national heritage implications. Specifically, we\nintroduce an enhanced GA-based puzzle solver, whose integration with a novel\nDL-based compatibility measure (DLCM) yields state-of-the-art performance,\nregarding the above application. Current compatibility measures consider\ntypically (the chromatic information of) edge pixels (between adjacent tiles),\nand help achieve high accuracy for the synthetic JPP variant. However, such\nmeasures exhibit rather poor performance when applied to the Portuguese tile\npanels, which are susceptible to various real-world effects, e.g.,\nmonochromatic panels, non-squared tiles, edge degradation, etc. To overcome\nsuch difficulties, we have developed a novel DLCM to extract high-level\ntexture/color statistics from the entire tile information.\n  Integrating this measure with our enhanced GA-based puzzle solver, we have\ndemonstrated, for the first time, how to deal most effectively with large-scale\nreal-world problems, such as the Portuguese tile problem. Specifically, we have\nachieved 82% accuracy for the reconstruction of Portuguese tile panels with\nunknown piece rotation and puzzle dimension (compared to merely 3.5% average\naccuracy achieved by the best method known for solving this problem variant).\nThe proposed method outperforms even human experts in several cases, correcting\ntheir mistakes in the manual tile assembly.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:21 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rika", "Daniel", ""], ["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02710", "submitter": "Tarang Chugh", "authors": "Tarang Chugh and Anil K. Jain", "title": "Fingerprint Spoof Generalization", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a style-transfer based wrapper, called Universal Material\nGenerator (UMG), to improve the generalization performance of any fingerprint\nspoof detector against spoofs made from materials not seen during training.\nSpecifically, we transfer the style (texture) characteristics between\nfingerprint images of known materials with the goal of synthesizing fingerprint\nimages corresponding to unknown materials, that may occupy the space between\nthe known materials in the deep feature space. Synthetic live fingerprint\nimages are also added to the training dataset to force the CNN to learn\ngenerative-noise invariant features which discriminate between lives and\nspoofs. The proposed approach is shown to improve the generalization\nperformance of a state-of-the-art spoof detector, namely Fingerprint Spoof\nBuster, from TDR of 75.24% to 91.78% @ FDR = 0.2%. These results are based on a\nlarge-scale dataset of 5,743 live and 4,912 spoof images fabricated using 12\ndifferent materials. Additionally, the UMG wrapper is shown to improve the\naverage cross-sensor spoof detection performance from 67.60% to 80.63% when\ntested on the LivDet 2017 dataset. Training the UMG wrapper requires only 100\nlive fingerprint images from the target sensor, alleviating the time and\nresources required to generate large-scale live and spoof datasets for a new\nsensor. We also fabricate physical spoof artifacts using a mixture of known\nspoof materials to explore the role of cross-material style transfer in\nimproving generalization performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 16:43:24 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Chugh", "Tarang", ""], ["Jain", "Anil K.", ""]]}, {"id": "1912.02743", "submitter": "Azim Ahmadzadeh", "authors": "Azim Ahmadzadeh, Sushant S. Mahajan, Dustin J. Kempton, Rafal A.\n  Angryk, and Shihao Ji", "title": "Toward Filament Segmentation Using Deep Neural Networks", "comments": "10 pages, 10 figures, 1 table, accepted in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use a well-known deep neural network framework, called Mask R-CNN, for\nidentification of solar filaments in full-disk H-alpha images from Big Bear\nSolar Observatory (BBSO). The image data, collected from BBSO's archive, are\nintegrated with the spatiotemporal metadata of filaments retrieved from the\nHeliophysics Events Knowledgebase (HEK) system. This integrated data is then\ntreated as the ground-truth in the training process of the model. The available\nspatial metadata are the output of a currently running filament-detection\nmodule developed and maintained by the Feature Finding Team; an international\nconsortium selected by NASA. Despite the known challenges in the identification\nand characterization of filaments by the existing module, which in turn are\ninherited into any other module that intends to learn from such outputs, Mask\nR-CNN shows promising results. Trained and validated on two years worth of BBSO\ndata, this model is then tested on the three following years. Our case-by-case\nand overall analyses show that Mask R-CNN can clearly compete with the existing\nmodule and in some cases even perform better. Several cases of false positives\nand false negatives, that are correctly segmented by this model are also shown.\nThe overall advantages of using the proposed model are two-fold: First, deep\nneural networks' performance generally improves as more annotated data, or\nbetter annotations are provided. Second, such a model can be scaled up to\ndetect other solar events, as well as a single multi-purpose module. The\nresults presented in this study introduce a proof of concept in benefits of\nemploying deep neural networks for detection of solar events, and in\nparticular, filaments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:45:41 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ahmadzadeh", "Azim", ""], ["Mahajan", "Sushant S.", ""], ["Kempton", "Dustin J.", ""], ["Angryk", "Rafal A.", ""], ["Ji", "Shihao", ""]]}, {"id": "1912.02751", "submitter": "Anca-Nicoleta Ciubotaru", "authors": "Anca-Nicoleta Ciubotaru, Arnout Devos, Behzad Bozorgtabar,\n  Jean-Philippe Thiran, Maria Gabrani", "title": "Revisiting Few-Shot Learning for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the existing deep neural nets on automatic facial expression\nrecognition focus on a set of predefined emotion classes, where the amount of\ntraining data has the biggest impact on performance. However, in the standard\nsetting over-parameterised neural networks are not amenable for learning from\nfew samples as they can quickly over-fit. In addition, these approaches do not\nhave such a strong generalisation ability to identify a new category, where the\ndata of each category is too limited and significant variations exist in the\nexpression within the same semantic category. We embrace these challenges and\nformulate the problem as a low-shot learning, where once the base classifier is\ndeployed, it must rapidly adapt to recognise novel classes using a few samples.\nIn this paper, we revisit and compare existing few-shot learning methods for\nthe low-shot facial expression recognition in terms of their generalisation\nability via episode-training. In particular, we extend our analysis on the\ncross-domain generalisation, where training and test tasks are not drawn from\nthe same distribution. We demonstrate the efficacy of low-shot learning methods\nthrough extensive experiments.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:41:56 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 09:20:41 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ciubotaru", "Anca-Nicoleta", ""], ["Devos", "Arnout", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""], ["Gabrani", "Maria", ""]]}, {"id": "1912.02781", "submitter": "Balaji Lakshminarayanan", "authors": "Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer,\n  Balaji Lakshminarayanan", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and\n  Uncertainty", "comments": "Code available at https://github.com/google-research/augmix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks can achieve high accuracy when the training\ndistribution and test distribution are identically distributed, but this\nassumption is frequently violated in practice. When the train and test\ndistributions are mismatched, accuracy can plummet. Currently there are few\ntechniques that improve robustness to unforeseen data shifts encountered during\ndeployment. In this work, we propose a technique to improve the robustness and\nuncertainty estimates of image classifiers. We propose AugMix, a data\nprocessing technique that is simple to implement, adds limited computational\noverhead, and helps models withstand unforeseen corruptions. AugMix\nsignificantly improves robustness and uncertainty measures on challenging image\nclassification benchmarks, closing the gap between previous methods and the\nbest possible performance in some cases by more than half.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:18:10 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 06:16:13 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mu", "Norman", ""], ["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Gilmer", "Justin", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1912.02783", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran,\n  Xiaohua Zhai, Neil Houlsby, Sylvain Gelly, Mario Lucic", "title": "Self-Supervised Learning of Video-Induced Visual Invariances", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for self-supervised learning of transferable\nvisual representations based on Video-Induced Visual Invariances (VIVI). We\nconsider the implicit hierarchy present in the videos and make use of (i)\nframe-level invariances (e.g. stability to color and contrast perturbations),\n(ii) shot/clip-level invariances (e.g. robustness to changes in object\norientation and lighting conditions), and (iii) video-level invariances\n(semantic relationships of scenes across shots/clips), to define a holistic\nself-supervised loss. Training models using different variants of the proposed\nframework on videos from the YouTube-8M (YT8M) data set, we obtain\nstate-of-the-art self-supervised transfer learning results on the 19 diverse\ndownstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only\n1000 labels per task. We then show how to co-train our models jointly with\nlabeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points\nwith 10x fewer labeled images, as well as the previous best supervised model by\n3.7 points using the full ImageNet data set.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:20:31 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 18:29:28 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Tschannen", "Michael", ""], ["Djolonga", "Josip", ""], ["Ritter", "Marvin", ""], ["Mahendran", "Aravindh", ""], ["Zhai", "Xiaohua", ""], ["Houlsby", "Neil", ""], ["Gelly", "Sylvain", ""], ["Lucic", "Mario", ""]]}, {"id": "1912.02792", "submitter": "Hugo Bertiche", "authors": "Hugo Bertiche, Meysam Madadi and Sergio Escalera", "title": "CLOTH3D: Clothed 3D Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents CLOTH3D, the first big scale synthetic dataset of 3D\nclothed human sequences. CLOTH3D contains a large variability on garment type,\ntopology, shape, size, tightness and fabric. Clothes are simulated on top of\nthousands of different pose sequences and body shapes, generating realistic\ncloth dynamics. We provide the dataset with a generative model for cloth\ngeneration. We propose a Conditional Variational Auto-Encoder (CVAE) based on\ngraph convolutions (GCVAE) to learn garment latent spaces. This allows for\nrealistic generation of 3D garments on top of SMPL model for any pose and\nshape.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:34:26 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 14:46:05 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bertiche", "Hugo", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""]]}, {"id": "1912.02801", "submitter": "Justin Liang", "authors": "Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen Xiong, Rui Hu and\n  Raquel Urtasun", "title": "PolyTransform: Deep Polygon Transformer for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose PolyTransform, a novel instance segmentation\nalgorithm that produces precise, geometry-preserving masks by combining the\nstrengths of prevailing segmentation approaches and modern polygon-based\nmethods. In particular, we first exploit a segmentation network to generate\ninstance masks. We then convert the masks into a set of polygons that are then\nfed to a deforming network that transforms the polygons such that they better\nfit the object boundaries. Our experiments on the challenging Cityscapes\ndataset show that our PolyTransform significantly improves the performance of\nthe backbone instance segmentation network and ranks 1st on the Cityscapes\ntest-set leaderboard. We also show impressive gains in the interactive\nannotation setting. We release the code at\nhttps://github.com/uber-research/PolyTransform.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:50:20 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 06:51:02 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 19:34:46 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2021 21:09:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liang", "Justin", ""], ["Homayounfar", "Namdar", ""], ["Ma", "Wei-Chiu", ""], ["Xiong", "Yuwen", ""], ["Hu", "Rui", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1912.02805", "submitter": "Xingyu Liu", "authors": "Xingyu Liu, Rico Jonschkowski, Anelia Angelova, Kurt Konolige", "title": "KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent\n  Objects", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D pose of desktop objects is crucial for applications such as\nrobotic manipulation. Many existing approaches to this problem require a depth\nmap of the object for both training and prediction, which restricts them to\nopaque, lambertian objects that produce good returns in an RGBD sensor. In this\npaper we forgo using a depth sensor in favor of raw stereo input. We address\ntwo problems: first, we establish an easy method for capturing and labeling 3D\nkeypoints on desktop objects with an RGB camera; and second, we develop a deep\nneural network, called $KeyPose$, that learns to accurately predict object\nposes using 3D keypoints, from stereo input, and works even for transparent\nobjects. To evaluate the performance of our method, we create a dataset of 15\nclear objects in five classes, with 48K 3D-keypoint labeled images. We train\nboth instance and category models, and show generalization to new textures,\nposes, and objects. KeyPose surpasses state-of-the-art performance in 3D pose\nestimation on this dataset by factors of 1.5 to 3.5, even in cases where the\ncompeting method is provided with ground-truth depth. Stereo input is essential\nfor this performance as it improves results compared to using monocular input\nby a factor of 2. We will release a public version of the data capture and\nlabeling pipeline, the transparent object database, and the KeyPose models and\nevaluation code. Project website: https://sites.google.com/corp/view/keypose.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:54:07 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 21:46:19 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Xingyu", ""], ["Jonschkowski", "Rico", ""], ["Angelova", "Anelia", ""], ["Konolige", "Kurt", ""]]}, {"id": "1912.02851", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Giuseppe Amato, Fabrizio Falchi", "title": "Cross-Resolution Learning for Face Recognition", "comments": null, "journal-ref": "Image and Vision Computing Volume 99, July 2020, 103927", "doi": "10.1016/j.imavis.2020.103927", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have reached extremely high performances on the\nFace Recognition task. Largely used datasets, such as VGGFace2, focus on\ngender, pose and age variations trying to balance them to achieve better\nresults. However, the fact that images have different resolutions is not\nusually discussed and resize to 256 pixels before cropping is used. While\nspecific datasets for very low resolution faces have been proposed, less\nattention has been payed on the task of cross-resolution matching. Such\nscenarios are of particular interest for forensic and surveillance systems in\nwhich it usually happens that a low-resolution probe has to be matched with\nhigher-resolution galleries. While it is always possible to either increase the\nresolution of the probe image or to reduce the size of the gallery images, to\nthe best of our knowledge an extensive experimentation of cross-resolution\nmatching was missing in the recent deep learning based literature. In the\ncontext of low- and cross-resolution Face Recognition, the contributions of our\nwork are: i) we proposed a training method to fine-tune a state-of-the-art\nmodel in order to make it able to extract resolution-robust deep features; ii)\nwe tested our models on the benchmark datasets IJB-B/C considering images at\nboth full and low resolutions in order to show the effectiveness of the\nproposed training algorithm. To the best of our knowledge, this is the first\nwork testing extensively the performance of a FR model in a cross-resolution\nscenario; iii) we tested our models on the low resolution and low quality\ndatasets QMUL-SurvFace and TinyFace and showed their superior performances,\neven though we did not train our model on low-resolution faces only and our\nmain focus was cross-resolution; iv) we showed that our approach can be more\neffective with respect to preprocessing faces with super resolution techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:40:35 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "1912.02854", "submitter": "Zhenhua Feng", "authors": "Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu and Josef Kittler", "title": "An Accelerated Correlation Filter Tracker", "comments": null, "journal-ref": "Pattern Recognition 102(2019) 107172", "doi": "10.1016/j.patcog.2019.107172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent visual object tracking methods have witnessed a continuous improvement\nin the state-of-the-art with the development of efficient discriminative\ncorrelation filters (DCF) and robust deep neural network features. Despite the\noutstanding performance achieved by the above combination, existing advanced\ntrackers suffer from the burden of high computational complexity of the deep\nfeature extraction and online model learning. We propose an accelerated ADMM\noptimisation method obtained by adding a momentum to the optimisation sequence\niterates, and by relaxing the impact of the error between DCF parameters and\ntheir norm. The proposed optimisation method is applied to an innovative\nformulation of the DCF design, which seeks the most discriminative spatially\nregularised feature channels. A further speed up is achieved by an adaptive\ninitialisation of the filter optimisation process. The significantly increased\nconvergence of the DCF filter is demonstrated by establishing the optimisation\nprocess equivalence with a continuous dynamical system for which the\nconvergence properties can readily be derived. The experimental results\nobtained on several well-known benchmarking datasets demonstrate the efficiency\nand robustness of the proposed ACFT method, with a tracking accuracy comparable\nto the start-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:03:38 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Xu", "Tianyang", ""], ["Feng", "Zhen-Hua", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1912.02861", "submitter": "Owen Mayer", "authors": "Owen Mayer, Matthew C. Stamm", "title": "Exposing Fake Images with Forensic Similarity Graphs", "comments": "16 pages, under review at IEEE Journal of Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new image forgery detection and localization algorithms by\nrecasting these problems as graph-based community detection problems. To do\nthis, we introduce a novel abstract, graph-based representation of an image,\nwhich we call the Forensic Similarity Graph, that captures key forensic\nrelationships among regions in the image. In this representation, small image\npatches are represented by graph vertices with edges assigned according to the\nforensic similarity between patches. Localized tampering introduces unique\nstructure into this graph, which aligns with a concept called ``community\nstructure'' in graph-theory literature. In the Forensic Similarity Graph,\ncommunities correspond to the tampered and unaltered regions in the image. As a\nresult, forgery detection is performed by identifying whether multiple\ncommunities exist, and forgery localization is performed by partitioning these\ncommunities. We present two community detection techniques, adapted from\nliterature, to detect and localize image forgeries. We experimentally show that\nour proposed community detection methods outperform existing state-of-the-art\nforgery detection and localization methods, which do not capture such community\nstructure.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:21:24 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 19:51:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Mayer", "Owen", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "1912.02866", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala", "title": "Classifying Diagrams and Their Parts using Graph Neural Networks: A\n  Comparison of Crowd-Sourced and Expert Annotations", "comments": "9 pages; submitted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article compares two multimodal resources that consist of diagrams which\ndescribe topics in elementary school natural sciences. Both resources contain\nthe same diagrams and represent their structure using graphs, but differ in\nterms of their annotation schema and how the annotations have been created -\ndepending on the resource in question - either by crowd-sourced workers or\ntrained experts. This article reports on two experiments that evaluate how\neffectively crowd-sourced and expert-annotated graphs can represent the\nmultimodal structure of diagrams for representation learning using various\ngraph neural networks. The results show that the identity of diagram elements\ncan be learned from their layout features, while the expert annotations provide\nbetter representations of diagram types.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:34:53 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hiippala", "Tuomo", ""]]}, {"id": "1912.02889", "submitter": "Gruber Tobias", "authors": "Tobias Gruber and Mariia Kokhova and Werner Ritter and Norbert Haala\n  and Klaus Dietmayer", "title": "Learning Super-resolved Depth from Active Gated Imaging", "comments": null, "journal-ref": "Published in: 2018 21st International Conference on Intelligent\n  Transportation Systems (ITSC)", "doi": "10.1109/ITSC.2018.8569590", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environment perception for autonomous driving is doomed by the trade-off\nbetween range-accuracy and resolution: current sensors that deliver very\nprecise depth information are usually restricted to low resolution because of\ntechnology or cost limitations. In this work, we exploit depth information from\nan active gated imaging system based on cost-sensitive diode and CMOS\ntechnology. Learning a mapping between pixel intensities of three gated slices\nand depth produces a super-resolved depth map image with respectable relative\naccuracy of 5% in between 25-80 m. By design, depth information is perfectly\naligned with pixel intensity values.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:35:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Gruber", "Tobias", ""], ["Kokhova", "Mariia", ""], ["Ritter", "Werner", ""], ["Haala", "Norbert", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1912.02907", "submitter": "Ukash Nakarmi", "authors": "Jeffrey Ma, Ukash Nakarmi, Cedric Yue Sik Kin, Christopher Sandino,\n  Joseph Y. Cheng, Ali B. Syed, Peter Wei, John M. Pauly, Shreyas Vasanawala", "title": "Diagnostic Image Quality Assessment and Classification in Medical\n  Imaging: Opportunities and Challenges", "comments": "4 pages, 8 Figures, Conference Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) suffers from several artifacts, the most\ncommon of which are motion artifacts. These artifacts often yield images that\nare of non-diagnostic quality. To detect such artifacts, images are\nprospectively evaluated by experts for their diagnostic quality, which\nnecessitates patient-revisits and rescans whenever non-diagnostic quality scans\nare encountered. This motivates the need to develop an automated framework\ncapable of accessing medical image quality and detecting diagnostic and\nnon-diagnostic images. In this paper, we explore several convolutional neural\nnetwork-based frameworks for medical image quality assessment and investigate\nseveral challenges therein.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:44:54 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Ma", "Jeffrey", ""], ["Nakarmi", "Ukash", ""], ["Kin", "Cedric Yue Sik", ""], ["Sandino", "Christopher", ""], ["Cheng", "Joseph Y.", ""], ["Syed", "Ali B.", ""], ["Wei", "Peter", ""], ["Pauly", "John M.", ""], ["Vasanawala", "Shreyas", ""]]}, {"id": "1912.02908", "submitter": "Thomas Sch\\\"ops", "authors": "Thomas Sch\\\"ops, Viktor Larsson, Marc Pollefeys, Torsten Sattler", "title": "Why Having 10,000 Parameters in Your Camera Model is Better Than Twelve", "comments": "15 pages, 12 figures, accepted to CVPR 2020 as an oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera calibration is an essential first step in setting up 3D Computer\nVision systems. Commonly used parametric camera models are limited to a few\ndegrees of freedom and thus often do not optimally fit to complex real lens\ndistortion. In contrast, generic camera models allow for very accurate\ncalibration due to their flexibility. Despite this, they have seen little use\nin practice. In this paper, we argue that this should change. We propose a\ncalibration pipeline for generic models that is fully automated, easy to use,\nand can act as a drop-in replacement for parametric calibration, with a focus\non accuracy. We compare our results to parametric calibrations. Considering\nstereo depth estimation and camera pose estimation as examples, we show that\nthe calibration error acts as a bias on the results. We thus argue that in\ncontrast to current common practice, generic models should be preferred over\nparametric ones whenever possible. To facilitate this, we released our\ncalibration pipeline at https://github.com/puzzlepaint/camera_calibration,\nmaking both easy-to-use and accurate camera calibration available to everyone.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:48:50 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 10:49:39 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 15:51:07 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Sch\u00f6ps", "Thomas", ""], ["Larsson", "Viktor", ""], ["Pollefeys", "Marc", ""], ["Sattler", "Torsten", ""]]}, {"id": "1912.02911", "submitter": "Davood Karimi", "authors": "Davood Karimi, Haoran Dou, Simon K. Warfield, Ali Gholipour", "title": "Deep learning with noisy labels: exploring techniques and remedies in\n  medical image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Supervised training of deep learning models requires large labeled datasets.\nThere is a growing interest in obtaining such datasets for medical image\nanalysis applications. However, the impact of label noise has not received\nsufficient attention. Recent studies have shown that label noise can\nsignificantly impact the performance of deep learning models in many machine\nlearning and computer vision applications. This is especially concerning for\nmedical applications, where datasets are typically small, labeling requires\ndomain expertise and suffers from high inter- and intra-observer variability,\nand erroneous predictions may influence decisions that directly impact human\nhealth. In this paper, we first review the state-of-the-art in handling label\nnoise in deep learning. Then, we review studies that have dealt with label\nnoise in deep learning for medical image analysis. Our review shows that recent\nprogress on handling label noise in deep learning has gone largely unnoticed by\nthe medical image analysis community. To help achieve a better understanding of\nthe extent of the problem and its potential remedies, we conducted experiments\nwith three medical imaging datasets with different types of label noise, where\nwe investigated several existing strategies and developed new methods to combat\nthe negative effect of label noise. Based on the results of these experiments\nand our review of the literature, we have made recommendations on methods that\ncan be used to alleviate the effects of different types of label noise on deep\nmodels trained for medical image analysis. We hope that this article helps the\nmedical image analysis researchers and developers in choosing and devising new\ntechniques that effectively handle label noise in deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:58:55 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 16:14:37 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 04:18:09 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 22:45:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Karimi", "Davood", ""], ["Dou", "Haoran", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "1912.02913", "submitter": "Arian Mehrfard", "authors": "Arian Mehrfard, Javad Fotouhi, Giacomo Taylor, Tess Forster, Nassir\n  Navab, and Bernhard Fuerst", "title": "A Comparative Analysis of Virtual Reality Head-Mounted Display Systems", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances of Virtual Reality (VR) technology, the deployment of\nsuch will dramatically increase in non-entertainment environments, such as\nprofessional education and training, manufacturing, service, or low\nfrequency/high risk scenarios. Clinical education is an area that especially\nstands to benefit from VR technology due to the complexity, high cost, and\ndifficult logistics. The effectiveness of the deployment of VR systems, is\nsubject to factors that may not be necessarily considered for devices targeting\nthe entertainment market. In this work, we systematically compare a wide range\nof VR Head-Mounted Displays (HMDs) technologies and designs by defining a new\nset of metrics that are 1) relevant to most generic VR solutions and 2) are of\nparamount importance for VR-based education and training. We evaluated ten HMDs\nbased on various criteria, including neck strain, heat development, and color\naccuracy. Other metrics such as text readability, comfort, and contrast\nperception were evaluated in a multi-user study on three selected HMDs, namely\nOculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC\nVive Pro performs best with regards to comfort, display quality and\ncompatibility with glasses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:01:33 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mehrfard", "Arian", ""], ["Fotouhi", "Javad", ""], ["Taylor", "Giacomo", ""], ["Forster", "Tess", ""], ["Navab", "Nassir", ""], ["Fuerst", "Bernhard", ""]]}, {"id": "1912.02914", "submitter": "Yuyan Li", "authors": "Truc Le, Yuyan Li, Ye Duan", "title": "RED-NET: A Recursive Encoder-Decoder Network for Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce RED-NET: A Recursive Encoder-Decoder Network with\nSkip-Connections for edge detection in natural images. The proposed network is\na novel integration of a Recursive Neural Network with an Encoder-Decoder\narchitecture. The recursive network enables us to increase the network depth\nwithout increasing the number of parameters. Adding skip-connections between\nencoder and decoder helps the gradients reach all the layers of a network more\neasily and allows information related to finer details in the early stage of\nthe encoder to be fully utilized in the decoder. Based on our extensive\nexperiments on popular boundary detection datasets including BSDS500\n\\cite{Arbelaez2011}, NYUD \\cite{Silberman2012} and Pascal Context\n\\cite{Mottaghi2014}, RED-NET significantly advances the state-of-the-art on\nedge detection regarding standard evaluation metrics such as Optimal Dataset\nScale (ODS) F-measure, Optimal Image Scale (OIS) F-measure, and Average\nPrecision (AP).\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:05:26 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Le", "Truc", ""], ["Li", "Yuyan", ""], ["Duan", "Ye", ""]]}, {"id": "1912.02918", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Fabio Carrara, Giuseppe Amato, Fabrizio Falchi", "title": "Detection of Face Recognition Adversarial Attacks", "comments": null, "journal-ref": "Computer Vision and Image Understanding Volume 202, January 2021,\n  103103", "doi": "10.1016/j.cviu.2020.103103", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning methods have become state-of-the-art for solving tasks such as\nFace Recognition (FR). Unfortunately, despite their success, it has been\npointed out that these learning models are exposed to adversarial inputs -\nimages to which an imperceptible amount of noise for humans is added to\nmaliciously fool a neural network - thus limiting their adoption in real-world\napplications. While it is true that an enormous effort has been spent in order\nto train robust models against this type of threat, adversarial detection\ntechniques have recently started to draw attention within the scientific\ncommunity. A detection approach has the advantage that it does not require to\nre-train any model, thus it can be added on top of any system. In this context,\nwe present our work on adversarial samples detection in forensics mainly\nfocused on detecting attacks against FR systems in which the learning model is\ntypically used only as a features extractor. Thus, in these cases, train a more\nrobust classifier might not be enough to defence a FR system. In this frame,\nthe contribution of our work is four-fold: i) we tested our recently proposed\nadversarial detection approach against classifier attacks, i.e. adversarial\nsamples crafted to fool a FR neural network acting as a classifier; ii) using a\nk-Nearest Neighbor (kNN) algorithm as a guidance, we generated deep features\nattacks against a FR system based on a DL model acting as features extractor,\nfollowed by a kNN which gives back the query identity based on features\nsimilarity; iii) we used the deep features attacks to fool a FR system on the\n1:1 Face Verification task and we showed their superior effectiveness with\nrespect to classifier attacks in fooling such type of system; iv) we used the\ndetectors trained on classifier attacks to detect deep features attacks, thus\nshowing that such approach is generalizable to different types of offensives.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:24:33 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Carrara", "Fabio", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "1912.02923", "submitter": "Yan Zhang", "authors": "Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J. Black, Siyu Tang", "title": "Generating 3D People in Scenes without People", "comments": "cvpr'20, camera ready version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic system that takes a 3D scene and generates\nplausible 3D human bodies that are posed naturally in that 3D scene. Given a 3D\nscene without people, humans can easily imagine how people could interact with\nthe scene and the objects in it. However, this is a challenging task for a\ncomputer as solving it requires that (1) the generated human bodies to be\nsemantically plausible within the 3D environment (e.g. people sitting on the\nsofa or cooking near the stove), and (2) the generated human-scene interaction\nto be physically feasible such that the human body and scene do not\ninterpenetrate while, at the same time, body-scene contact supports physical\ninteractions. To that end, we make use of the surface-based 3D human model\nSMPL-X. We first train a conditional variational autoencoder to predict\nsemantically plausible 3D human poses conditioned on latent scene\nrepresentations, then we further refine the generated 3D bodies using scene\nconstraints to enforce feasible physical interaction. We show that our approach\nis able to synthesize realistic and expressive 3D human bodies that naturally\ninteract with 3D environment. We perform extensive experiments demonstrating\nthat our generative framework compares favorably with existing methods, both\nqualitatively and quantitatively. We believe that our scene-conditioned 3D\nhuman generation pipeline will be useful for numerous applications; e.g. to\ngenerate training data for human pose estimation, in video games and in VR/AR.\nOur project page for data and code can be seen at:\n\\url{https://vlg.inf.ethz.ch/projects/PSI/}.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:49:27 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 10:41:36 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 18:22:59 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Yan", ""], ["Hassan", "Mohamed", ""], ["Neumann", "Heiko", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "1912.02937", "submitter": "Shaofei Wang", "authors": "Shaofei Wang, Vishnu Lokhande, Maneesh Singh, Konrad Kording, Julian\n  Yarkony", "title": "End-to-end Training of CNN-CRF via Differentiable Dual-Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision (CV) is often based on convolutional neural networks\n(CNNs) that excel at hierarchical feature extraction. The previous generation\nof CV approaches was often based on conditional random fields (CRFs) that excel\nat modeling flexible higher order interactions. As their benefits are\ncomplementary they are often combined. However, these approaches generally use\nmean-field approximations and thus, arguably, did not directly optimize the\nreal problem. Here we revisit dual-decomposition-based approaches to CRF\noptimization, an alternative to the mean-field approximation. These algorithms\ncan efficiently and exactly solve sub-problems and directly optimize a convex\nupper bound of the real problem, providing optimality certificates on the way.\nOur approach uses a novel fixed-point iteration algorithm which enjoys\ndual-monotonicity, dual-differentiability and high parallelism. The whole\nsystem, CRF and CNN can thus be efficiently trained using back-propagation. We\ndemonstrate the effectiveness of our system on semantic image segmentation,\nshowing consistent improvement over baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:49:48 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Wang", "Shaofei", ""], ["Lokhande", "Vishnu", ""], ["Singh", "Maneesh", ""], ["Kording", "Konrad", ""], ["Yarkony", "Julian", ""]]}, {"id": "1912.02942", "submitter": "Junyu Chen", "authors": "Junyu Chen, Ye Li, Yong Du, Eric C. Frey", "title": "Generating Anthropomorphic Phantoms Using Fully Unsupervised Deformable\n  Image Registration with Convolutional Neural Networks", "comments": null, "journal-ref": "Med. Phys., 47: 6366-6380 (2020)", "doi": "10.1002/mp.14545", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Computerized phantoms play an essential role in various\napplications of medical imaging research. Although the existing computerized\nphantoms can model anatomical variations through organ and phantom scaling,\nthis does not provide a way to fully reproduce anatomical variations seen in\nhumans. However, having a population of phantoms that models the variations in\npatient anatomy and, in nuclear medicine, uptake realization is essential for\ncomprehensive validation and training. In this work, we present a novel image\nregistration method for creating highly anatomically detailed anthropomorphic\nphantoms from a single digital phantom. Methods: We propose a\ndeep-learning-based registration algorithm to predict deformation parameters\nfor warping an XCAT phantom to a patient CT scan. This proposed algorithm\noptimizes a novel SSIM-based objective function for a given image pair\nindependently of the training data and thus is truly and fully unsupervised. We\nevaluate the proposed method on a publicly available low-dose CT dataset from\nTCIA. Results: The performance of the proposed model was compared with that of\nseveral state-of-the-art methods, and outperformed them by more than 8%,\nmeasured by the SSIM and less than 30%, by the MSE. Conclusion: A\ndeep-learning-based unsupervised registration method was developed to create\nanthropomorphic phantoms while providing \"gold-standard\" anatomies that can be\nused as the basis for modeling organ properties. Significance: Experimental\nresults demonstrate the effectiveness of the proposed method. The resulting\nanthropomorphic phantom is highly realistic. Combined with realistic\nsimulations of the image formation process, the generated phantoms could serve\nin many applications of medical imaging research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:31:34 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 16:39:13 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 20:41:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Junyu", ""], ["Li", "Ye", ""], ["Du", "Yong", ""], ["Frey", "Eric C.", ""]]}, {"id": "1912.02973", "submitter": "Albert Zhao", "authors": "Albert Zhao and Tong He and Yitao Liang and Haibin Huang and Guy Van\n  den Broeck and Stefano Soatto", "title": "SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy\n  Learning", "comments": "Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a policy learning approach to map visual inputs to driving\ncontrols conditioned on turning command that leverages side tasks on semantics\nand object affordances via a learned representation trained for driving. To\nlearn this representation, we train a squeeze network to drive using\nannotations for the side task as input. This representation encodes the\ndriving-relevant information associated with the side task while ideally\nthrowing out side task-relevant but driving-irrelevant nuisances. We then train\na mimic network to drive using only images as input and use the squeeze\nnetwork's latent representation to supervise the mimic network via a mimicking\nloss. Notably, we do not aim to achieve the side task nor to learn features for\nit; instead, we aim to learn, via the mimicking loss, a representation of the\nside task annotations directly useful for driving. We test our approach using\nthe CARLA simulator. In addition, we introduce a more challenging but realistic\nevaluation protocol that considers a run that reaches the destination\nsuccessful only if it does not violate common traffic rules. A video\nsummarizing this work is available at https://youtu.be/ipKAMzmJpMs , and code\nis available at https://github.com/twsq/sam-driving .\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 04:41:51 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 07:50:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhao", "Albert", ""], ["He", "Tong", ""], ["Liang", "Yitao", ""], ["Huang", "Haibin", ""], ["Broeck", "Guy Van den", ""], ["Soatto", "Stefano", ""]]}, {"id": "1912.02983", "submitter": "Eli (Omid) David", "authors": "Katia Huri, Eli David, Nathan S. Netanyahu", "title": "DeepEthnic: Multi-Label Ethnic Classification from Face Images", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 604-612, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_59", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethnic group classification is a well-researched problem, which has been\npursued mainly during the past two decades via traditional approaches of image\nprocessing and machine learning. In this paper, we propose a method of\nclassifying an image face into an ethnic group by applying transfer learning\nfrom a previously trained classification network for large-scale data\nrecognition. Our proposed method yields state-of-the-art success rates of\n99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups:\nAfrican, Asian, Caucasian, and Indian.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:59:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Huri", "Katia", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02984", "submitter": "Qiangeng Xu", "authors": "Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, Ulrich Neumann", "title": "Grid-GCN for Fast and Scalable Point Cloud Learning", "comments": null, "journal-ref": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the sparsity and irregularity of the point cloud data, methods that\ndirectly consume points have become popular. Among all point-based models,\ngraph convolutional networks (GCN) lead to notable performance by fully\npreserving the data granularity and exploiting point interrelation. However,\npoint-based networks spend a significant amount of time on data structuring\n(e.g., Farthest Point Sampling (FPS) and neighbor points querying), which limit\nthe speed and scalability. In this paper, we present a method, named Grid-GCN,\nfor fast and scalable point cloud learning. Grid-GCN uses a novel data\nstructuring strategy, Coverage-Aware Grid Query (CAGQ). By leveraging the\nefficiency of grid space, CAGQ improves spatial coverage while reducing the\ntheoretical time complexity. Compared with popular sampling methods such as\nFarthest Point Sampling (FPS) and Ball Query, CAGQ achieves up to 50X speed-up.\nWith a Grid Context Aggregation (GCA) module, Grid-GCN achieves\nstate-of-the-art performance on major point cloud classification and\nsegmentation benchmarks with significantly faster runtime than previous\nstudies. Remarkably, Grid-GCN achieves the inference speed of 50fps on ScanNet\nusing 81920 points per scene as input.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:59:40 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 01:38:35 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 22:44:27 GMT"}, {"version": "v4", "created": "Sun, 14 Feb 2021 05:32:33 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 03:37:00 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Qiangeng", ""], ["Sun", "Xudong", ""], ["Wu", "Cho-Ying", ""], ["Wang", "Panqu", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1912.03000", "submitter": "Murari Mandal", "authors": "Shivangi Dwivedi, Murari Mandal, Shekhar Yadav, Santosh Kumar\n  Vipparthi", "title": "3D CNN with Localized Residual Connections for Hyperspectral Image\n  Classification", "comments": "4th International Conference on Computer Vision and Image Processing\n  (CVIP-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel 3D CNN network with localized residual\nconnections for hyperspectral image classification. Our work chalks a\ncomparative study with the existing methods employed for abstracting deeper\nfeatures and propose a model which incorporates residual features from multiple\nstages in the network. The proposed architecture processes individual\nspatiospectral feature rich cubes from hyperspectral images through 3D\nconvolutional layers. The residual connections result in improved performance\ndue to assimilation of both low-level and high-level features. We conduct\nexperiments over Pavia University and Pavia Center dataset for performance\nanalysis. We compare our method with two recent state-of-the-art methods for\nhyperspectral image classification method. The proposed network outperforms the\nexisting approaches by a good margin.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:46:01 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Dwivedi", "Shivangi", ""], ["Mandal", "Murari", ""], ["Yadav", "Shekhar", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "1912.03001", "submitter": "Hongwei Yi", "authors": "Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen,\n  Guoping Wang, Yu-Wing Tai", "title": "Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation", "comments": "Accepted by ECCV2020 as a Poster", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  n this paper, we propose an effective and efficient pyramid multi-view stereo\n(MVS) net with self-adaptive view aggregation for accurate and complete dense\npoint cloud reconstruction. Different from using mean square variance to\ngenerate cost volume in previous deep-learning based MVS methods, our\n\\textbf{VA-MVSNet} incorporates the cost variances in different views with\nsmall extra memory consumption by introducing two novel self-adaptive view\naggregations: pixel-wise view aggregation and voxel-wise view aggregation. To\nfurther boost the robustness and completeness of 3D point cloud reconstruction,\nwe extend VA-MVSNet with pyramid multi-scale images input as\n\\textbf{PVA-MVSNet}, where multi-metric constraints are leveraged to aggregate\nthe reliable depth estimation at the coarser scale to fill in the mismatched\nregions at the finer scale. Experimental results show that our approach\nestablishes a new state-of-the-art on the \\textsl{\\textbf{DTU}} dataset with\nsignificant improvements in the completeness and overall quality, and has\nstrong generalization by achieving a comparable performance as the\nstate-of-the-art methods on the \\textsl{\\textbf{Tanks and Temples}} benchmark.\nOur codebase is at\n\\hyperlink{https://github.com/yhw-yhw/PVAMVSNet}{https://github.com/yhw-yhw/PVAMVSNet}\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 07:06:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:48:01 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yi", "Hongwei", ""], ["Wei", "Zizhuang", ""], ["Ding", "Mingyu", ""], ["Zhang", "Runze", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1912.03005", "submitter": "Chuong Nguyen", "authors": "Hengjia Li and Chuong Nguyen", "title": "Perspective-consistent multifocus multiview 3D reconstruction of small\n  objects", "comments": "Accepted to DICTA 2019, best student scientific paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based 3D reconstruction or 3D photogrammetry of small-scale objects\nincluding insects and biological specimens is challenging due to the use of\nhigh magnification lens with inherent limited depth of field, and the object's\nfine structures and complex surface properties. Due to these challenges,\ntraditional 3D reconstruction techniques cannot be applied without suitable\nimage pre-processings. One such preprocessing technique is multifocus stacking\nthat combines a set of partially focused images captured from the same viewing\nangle to create a single in-focus image. Traditional multifocus image capture\nuses a camera on a macro rail. Furthermore, the scale and shift are not\nproperly considered by multifocus stacking techniques. As a consequence, the\nresulting in-focus images contain artifacts that violate perspective image\nformation. A 3D reconstruction using such images will fail to produce an\naccurate 3D model of the object. This paper shows how this problem can be\nsolved effectively by a new multifocus stacking procedure which includes a new\nFixed-Lens Multifocus Capture and camera calibration for image scale and shift.\nInitial experimental results are presented to confirm our expectation and show\nthat the camera poses of fixed-lens images are at least 3-times less noisy than\nthose of conventional moving lens images.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 07:16:27 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Li", "Hengjia", ""], ["Nguyen", "Chuong", ""]]}, {"id": "1912.03035", "submitter": "Marcus Bloice", "authors": "Marcus D. Bloice, Peter M. Roth, Andreas Holzinger", "title": "Performing Arithmetic Using a Neural Network Trained on Digit\n  Permutation Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a neural network is trained to perform simple arithmetic using\nimages of concatenated handwritten digit pairs. A convolutional neural network\nwas trained with images consisting of two side-by-side handwritten digits,\nwhere the image's label is the summation of the two digits contained in the\ncombined image. Crucially, the network was tested on permutation pairs that\nwere not present during training in an effort to see if the network could learn\nthe task of addition, as opposed to simply mapping images to labels. A dataset\nwas generated for all possible permutation pairs of length 2 for the digits 0-9\nusing MNIST as a basis for the images, with one thousand samples generated for\neach permutation pair. For testing the network, samples generated from\npreviously unseen permutation pairs were fed into the trained network, and its\npredictions measured. Results were encouraging, with the network achieving an\naccuracy of over 90% on some permutation train/test splits. This suggests that\nthe network learned at first digit recognition, and subsequently the further\ntask of addition based on the two recognised digits. As far as the authors are\naware, no previous work has concentrated on learning a mathematical operation\nin this way.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:23:25 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bloice", "Marcus D.", ""], ["Roth", "Peter M.", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1912.03063", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec (LIRIS), Grigory Antipov, Moez Baccouche, Christian\n  Wolf (LIRIS)", "title": "Weak Supervision helps Emergence of Word-Object Alignment and improves\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large adoption of the self-attention (i.e. transformer model) and\nBERT-like training principles has recently resulted in a number of high\nperforming models on a large panoply of vision-and-language problems (such as\nVisual Question Answering (VQA), image retrieval, etc.). In this paper we claim\nthat these State-Of-The-Art (SOTA) approaches perform reasonably well in\nstructuring information inside a single modality but, despite their impressive\nperformances , they tend to struggle to identify fine-grained inter-modality\nrelationships. Indeed, such relations are frequently assumed to be implicitly\nlearned during training from application-specific losses, mostly cross-entropy\nfor classification. While most recent works provide inductive bias for\ninter-modality relationships via cross attention modules, in this work, we\ndemonstrate (1) that the latter assumption does not hold, i.e. modality\nalignment does not necessarily emerge automatically, and (2) that adding weak\nsupervision for alignment between visual objects and words improves the quality\nof the learned models on tasks requiring reasoning. In particular , we\nintegrate an object-word alignment loss into SOTA vision-language reasoning\nmodels and evaluate it on two tasks VQA and Language-driven Comparison of\nImages. We show that the proposed fine-grained inter-modality supervision\nsignificantly improves performance on both tasks. In particular, this new\nlearning signal allows obtaining SOTA-level performances on GQA dataset (VQA\ntask) with pre-trained models without finetuning on the task, and a new SOTA on\nNLVR2 dataset (Language-driven Comparison of Images). Finally, we also\nillustrate the impact of the contribution on the models reasoning by\nvisualizing attention distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 11:04:08 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kervadec", "Corentin", "", "LIRIS"], ["Antipov", "Grigory", "", "LIRIS"], ["Baccouche", "Moez", "", "LIRIS"], ["Wolf", "Christian", "", "LIRIS"]]}, {"id": "1912.03083", "submitter": "Zhen Liu", "authors": "Jing Ge, Guangyu Gao, and Zhen Liu", "title": "Visual-Textual Association with Hardest and Semi-Hard Negative Pairs\n  Mining for Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching persons in large-scale image databases with the query of natural\nlanguage description is a more practical important applications in video\nsurveillance. Intuitively, for person search, the core issue should be\nvisual-textual association, which is still an extremely challenging task, due\nto the contradiction between the high abstraction of textual description and\nthe intuitive expression of visual images. However, for this task, while\npositive image-text pairs are always well provided, most existing methods\ndoesn't tackle this problem effectively by mining more reasonable negative\npairs. In this paper, we proposed a novel visual-textual association approach\nwith visual and textual attention, and cross-modality hardest and semi-hard\nnegative pair mining. In order to evaluate the effectiveness and feasibility of\nthe proposed approach, we conduct extensive experiments on typical person\nsearch datasdet: CUHK-PEDES, in which our approach achieves the top1 score of\n55.32% as a new state-of-the-art. Besides, we also evaluate the semi-hard pair\nmining approach in COCO caption dataset, and validate the effectiveness and\ncomplementarity of the methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 12:21:06 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Ge", "Jing", ""], ["Gao", "Guangyu", ""], ["Liu", "Zhen", ""]]}, {"id": "1912.03085", "submitter": "Hyojin Bahng", "authors": "Hyojin Bahng, Sunghyo Chung, Seungjoo Yoo, Jaegul Choo", "title": "Exploring Unlabeled Faces for Novel Attribute Discovery", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable success in unpaired image-to-image translation, existing\nsystems still require a large amount of labeled images. This is a bottleneck\nfor their real-world applications; in practice, a model trained on labeled\nCelebA dataset does not work well for test images from a different distribution\n-- greatly limiting their application to unlabeled images of a much larger\nquantity. In this paper, we attempt to alleviate this necessity for labeled\ndata in the facial image translation domain. We aim to explore the degree to\nwhich you can discover novel attributes from unlabeled faces and perform\nhigh-quality translation. To this end, we use prior knowledge about the visual\nworld as guidance to discover novel attributes and transfer them via a novel\nnormalization method. Experiments show that our method trained on unlabeled\ndata produces high-quality translations, preserves identity, and be\nperceptually realistic as good as, or better than, state-of-the-art methods\ntrained on labeled data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 12:27:37 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bahng", "Hyojin", ""], ["Chung", "Sunghyo", ""], ["Yoo", "Seungjoo", ""], ["Choo", "Jaegul", ""]]}, {"id": "1912.03095", "submitter": "Daniel Gehrig", "authors": "Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carri\\'o, Davide\n  Scaramuzza", "title": "Video to Events: Recycling Video Datasets for Event Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel sensors that output brightness changes in the form of\na stream of asynchronous \"events\" instead of intensity frames. They offer\nsignificant advantages with respect to conventional cameras: high dynamic range\n(HDR), high temporal resolution, and no motion blur. Recently, novel learning\napproaches operating on event data have achieved impressive results. Yet, these\nmethods require a large amount of event data for training, which is hardly\navailable due the novelty of event sensors in computer vision research. In this\npaper, we present a method that addresses these needs by converting any\nexisting video dataset recorded with conventional cameras to synthetic event\ndata. This unlocks the use of a virtually unlimited number of existing video\ndatasets for training networks designed for real event data. We evaluate our\nmethod on two relevant vision tasks, i.e., object recognition and semantic\nsegmentation, and show that models trained on synthetic events have several\nbenefits: (i) they generalize well to real event data, even in scenarios where\nstandard-camera images are blurry or overexposed, by inheriting the outstanding\nproperties of event cameras; (ii) they can be used for fine-tuning on real data\nto improve over state-of-the-art for both classification and semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:10:59 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 16:25:30 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gehrig", "Daniel", ""], ["Gehrig", "Mathias", ""], ["Hidalgo-Carri\u00f3", "Javier", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1912.03098", "submitter": "Jordi Pont-Tuset", "authors": "Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu\n  Soricut and Vittorio Ferrari", "title": "Connecting Vision and Language with Localized Narratives", "comments": "ECCV 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Localized Narratives, a new form of multimodal image annotations\nconnecting vision and language. We ask annotators to describe an image with\ntheir voice while simultaneously hovering their mouse over the region they are\ndescribing. Since the voice and the mouse pointer are synchronized, we can\nlocalize every single word in the description. This dense visual grounding\ntakes the form of a mouse trace segment per word and is unique to our data. We\nannotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and\nADE20K datasets, and 671k images of Open Images, all of which we make publicly\navailable. We provide an extensive analysis of these annotations showing they\nare diverse, accurate, and efficient to produce. We also demonstrate their\nutility on the application of controlled image captioning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:21:16 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 07:28:00 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 15:19:46 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 17:18:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pont-Tuset", "Jordi", ""], ["Uijlings", "Jasper", ""], ["Changpinyo", "Soravit", ""], ["Soricut", "Radu", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1912.03130", "submitter": "Md Mahfuzur Rahman", "authors": "U. Mahmood, M. M. Rahman, A. Fedorov, Z. Fu, V. D. Calhoun, S. M. Plis", "title": "Learnt dynamics generalizes across tasks, datasets, and populations", "comments": "11 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:1911.06813", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiating multivariate dynamic signals is a difficult learning problem\nas the feature space may be large yet often only a few training examples are\navailable. Traditional approaches to this problem either proceed from\nhandcrafted features or require large datasets to combat the m >> n problem. In\nthis paper, we show that the source of the problem---signal dynamics---can be\nused to our advantage and noticeably improve classification performance on a\nrange of discrimination tasks when training data is scarce. We demonstrate that\nself-supervised pre-training guided by signal dynamics produces embedding that\ngeneralizes across tasks, datasets, data collection sites, and data\ndistributions. We perform an extensive evaluation of this approach on a range\nof tasks including simulated data, keyword detection problem, and a range of\nfunctional neuroimaging data, where we show that a single embedding learnt on\nhealthy subjects generalizes across a number of disorders, age groups, and\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:21:50 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mahmood", "U.", ""], ["Rahman", "M. M.", ""], ["Fedorov", "A.", ""], ["Fu", "Z.", ""], ["Calhoun", "V. D.", ""], ["Plis", "S. M.", ""]]}, {"id": "1912.03133", "submitter": "Aristotelis Papadopoulos", "authors": "Aristotelis-Angelos Papadopoulos, Nazim Shaikh, Mohammad Reza Rajati", "title": "Why Should we Combine Training and Post-Training Methods for\n  Out-of-Distribution Detection?", "comments": "Preprint, 9 pages. arXiv admin note: text overlap with\n  arXiv:1906.03509", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to achieve superior results in classification\ntasks. However, it has been recently shown that they are incapable to detect\nexamples that are generated by a distribution which is different than the one\nthey have been trained on since they are making overconfident prediction for\nOut-Of-Distribution (OOD) examples. OOD detection has attracted a lot of\nattention recently. In this paper, we review some of the most seminal recent\nalgorithms in the OOD detection field, we divide those methods into training\nand post-training and we experimentally show how the combination of the former\nwith the latter can achieve state-of-the-art results in the OOD detection task.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:24:14 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Papadopoulos", "Aristotelis-Angelos", ""], ["Shaikh", "Nazim", ""], ["Rajati", "Mohammad Reza", ""]]}, {"id": "1912.03145", "submitter": "He-Feng Yin", "authors": "He-Feng Yin, Xiao-Jun Wu and Josef Kittler", "title": "Face Recognition via Locality Constrained Low Rank Representation and\n  Dictionary Learning", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been widely studied due to its importance in smart\ncities applications. However, the case when both training and test images are\ncorrupted is not well solved. To address such a problem, this paper proposes a\nlocality constrained low rank representation and dictionary learning (LCLRRDL)\nalgorithm for robust face recognition. In particular, we present three\ncontributions in the proposed formulation. First, a low-rank representation is\nintroduced to handle the possible contamination of the training as well as test\ndata. Second, a locality constraint is incorporated to acknowledge the\nintrinsic manifold structure of training data. With the locality constraint\nterm, our scheme induces similar samples to have similar representations.\nThird, a compact dictionary is learned to handle the problem of corrupted data.\nThe experimental results on two public databases demonstrate the effectiveness\nof the proposed approach. Matlab code of our proposed LCLRRDL can be downloaded\nfrom https://github.com/yinhefeng/LCLRRDL.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:24:52 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1912.03151", "submitter": "Xu Qin", "authors": "Xu Qin and Zhilin Wang", "title": "NASNet: A Neuron Attention Stage-by-Stage Net for Single Image Deraining", "comments": "underreviewed by conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured under complicated rain conditions often suffer from\nnoticeable degradation of visibility. The rain models generally introduce\ndiversity visibility degradation, which includes rain streak, rain drop as well\nas rain mist. Numerous existing single image deraining methods focus on the\nonly one type rain model, which does not have strong generalization ability. In\nthis paper, we propose a novel end-to-end Neuron Attention Stage-by-Stage Net\n(NASNet), which can solve all types of rain model tasks efficiently. For one\nthing, we pay more attention on the Neuron relationship and propose a\nlightweight Neuron Attention (NA) architectural mechanism. It can adaptively\nrecalibrate neuron-wise feature responses by modelling interdependencies and\nmutual influence between neurons. Our NA architecture consists of Depthwise\nConv and Pointwise Conv, which has slight computation cost and higher\nperformance than SE block by our contrasted experiments. For another, we\npropose a stage-by-stage unified pattern network architecture, the\nstage-by-stage strategy guides the later stage by incorporating the useful\ninformation in previous stage. We concatenate and fuse stage-level information\ndynamically by NA module. Extensive experiments demonstrate that our proposed\nNASNet significantly outperforms the state-of-theart methods by a large margin\nin terms of both quantitative and qualitative measures on all six public\nlarge-scale datasets for three rain model tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:28:20 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 03:08:43 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Qin", "Xu", ""], ["Wang", "Zhilin", ""]]}, {"id": "1912.03157", "submitter": "Marcel Sheeny", "authors": "Marcel Sheeny, Andrew Wallace, Sen Wang", "title": "300 GHz Radar Object Recognition based on Deep Neural Networks and\n  Transfer Learning", "comments": "This paper is a preprint of a paper submitted to IET Radar, Sonar and\n  Navigation. If accepted, the copy of record will be available at the IET\n  Digital Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high resolution scene mapping and object recognition, optical\ntechnologies such as cameras and LiDAR are the sensors of choice. However, for\nrobust future vehicle autonomy and driver assistance in adverse weather\nconditions, improvements in automotive radar technology, and the development of\nalgorithms and machine learning for robust mapping and recognition are\nessential. In this paper, we describe a methodology based on deep neural\nnetworks to recognise objects in 300GHz radar images, investigating robustness\nto changes in range, orientation and different receivers in a laboratory\nenvironment. As the training data is limited, we have also investigated the\neffects of transfer learning. As a necessary first step before road trials, we\nhave also considered detection and classification in multiple object scenes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:42:48 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Sheeny", "Marcel", ""], ["Wallace", "Andrew", ""], ["Wang", "Sen", ""]]}, {"id": "1912.03161", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Aurelien Lucchi, Thomas Hofmann", "title": "Controlling Style and Semantics in Weakly-Supervised Image Generation", "comments": "European Conference on Computer Vision (ECCV) 2020, Spotlight. Code\n  at https://github.com/dariopavllo/style-semantics", "journal-ref": null, "doi": "10.1007/978-3-030-58539-6_29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly-supervised approach for conditional image generation of\ncomplex scenes where a user has fine control over objects appearing in the\nscene. We exploit sparse semantic maps to control object shapes and classes, as\nwell as textual descriptions or attributes to control both local and global\nstyle. In order to condition our model on textual descriptions, we introduce a\nsemantic attention module whose computational cost is independent of the image\nresolution. To further augment the controllability of the scene, we propose a\ntwo-step generation scheme that decomposes background and foreground. The label\nmaps used to train our model are produced by a large-vocabulary object\ndetector, which enables access to unlabeled data and provides structured\ninstance information. In such a setting, we report better FID scores compared\nto fully-supervised settings where the model is trained on ground-truth\nsemantic maps. We also showcase the ability of our model to manipulate a scene\non complex datasets such as COCO and Visual Genome.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:47:50 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 16:18:18 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Pavllo", "Dario", ""], ["Lucchi", "Aurelien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1912.03183", "submitter": "Bruno Artacho", "authors": "Bruno Artacho and Andreas Savakis", "title": "Waterfall Atrous Spatial Pooling Architecture for Efficient Semantic\n  Segmentation", "comments": "17 pages, 11 figures", "journal-ref": "Sensors, 19(24), 2019", "doi": "10.3390/s19245361", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new efficient architecture for semantic segmentation, based on a\n\"Waterfall\" Atrous Spatial Pooling architecture, that achieves a considerable\naccuracy increase while decreasing the number of network parameters and memory\nfootprint. The proposed Waterfall architecture leverages the efficiency of\nprogressive filtering in the cascade architecture while maintaining multiscale\nfields-of-view comparable to spatial pyramid configurations. Additionally, our\nmethod does not rely on a postprocessing stage with Conditional Random Fields,\nwhich further reduces complexity and required training time. We demonstrate\nthat the Waterfall approach with a ResNet backbone is a robust and efficient\narchitecture for semantic segmentation obtaining state-of-the-art results with\nsignificant reduction in the number of parameters for the Pascal VOC dataset\nand the Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:29:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Artacho", "Bruno", ""], ["Savakis", "Andreas", ""]]}, {"id": "1912.03192", "submitter": "Sven Gowal", "authors": "Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy\n  Dvijotham, Timothy Mann, Pushmeet Kohli", "title": "Achieving Robustness in the Wild via Adversarial Mixing with\n  Disentangled Representations", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has made the surprising finding that state-of-the-art deep\nlearning models sometimes fail to generalize to small variations of the input.\nAdversarial training has been shown to be an effective approach to overcome\nthis problem. However, its application has been limited to enforcing invariance\nto analytically defined transformations like $\\ell_p$-norm bounded\nperturbations. Such perturbations do not necessarily cover plausible real-world\nvariations that preserve the semantics of the input (such as a change in\nlighting conditions). In this paper, we propose a novel approach to express and\nformalize robustness to these kinds of real-world transformations of the input.\nThe two key ideas underlying our formulation are (1) leveraging disentangled\nrepresentations of the input to define different factors of variations, and (2)\ngenerating new input images by adversarially composing the representations of\ndifferent images. We use a StyleGAN model to demonstrate the efficacy of this\nframework. Specifically, we leverage the disentangled latent representations\ncomputed by a StyleGAN model to generate perturbations of an image that are\nsimilar to real-world variations (like adding make-up, or changing the\nskin-tone of a person) and train models to be invariant to these perturbations.\nExtensive experiments show that our method improves generalization and reduces\nthe effect of spurious correlations (reducing the error rate of a \"smile\"\ndetector by 21% for example).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:56:53 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 09:33:57 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Gowal", "Sven", ""], ["Qin", "Chongli", ""], ["Huang", "Po-Sen", ""], ["Cemgil", "Taylan", ""], ["Dvijotham", "Krishnamurthy", ""], ["Mann", "Timothy", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1912.03201", "submitter": "Rene Larisch", "authors": "Ren\\'e Larisch and Michael Teichmann and Fred H. Hamker", "title": "A Neural Spiking Approach Compared to Deep Feedforward Networks on\n  Stepwise Pixel Erasement", "comments": "Published in ICANN 2018: Artificial Neural Networks and Machine\n  Learning - ICANN 2018\n  https://link.springer.com/chapter/10.1007/978-3-030-01418-6_25 The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-01418-6_25", "journal-ref": null, "doi": "10.1007/978-3-030-01418-6_25", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world scenarios, objects are often partially occluded. This requires\na robustness for object recognition against these perturbations. Convolutional\nnetworks have shown good performances in classification tasks. The learned\nconvolutional filters seem similar to receptive fields of simple cells found in\nthe primary visual cortex. Alternatively, spiking neural networks are more\nbiological plausible. We developed a two layer spiking network, trained on\nnatural scenes with a biologically plausible learning rule. It is compared to\ntwo deep convolutional neural networks using a classification task of stepwise\npixel erasement on MNIST. In comparison to these networks the spiking approach\nachieves good accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:08:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Larisch", "Ren\u00e9", ""], ["Teichmann", "Michael", ""], ["Hamker", "Fred H.", ""]]}, {"id": "1912.03203", "submitter": "Thomas Verelst", "authors": "Thomas Verelst, Tinne Tuytelaars", "title": "Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference", "comments": "CVPR 2020 (poster) https://github.com/thomasverelst/dynconv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern convolutional neural networks apply the same operations on every pixel\nin an image. However, not all image regions are equally important. To address\nthis inefficiency, we propose a method to dynamically apply convolutions\nconditioned on the input image. We introduce a residual block where a small\ngating branch learns which spatial positions should be evaluated. These\ndiscrete gating decisions are trained end-to-end using the Gumbel-Softmax\ntrick, in combination with a sparsity criterion. Our experiments on CIFAR,\nImageNet and MPII show that our method has better focus on the region of\ninterest and better accuracy than existing methods, at a lower computational\ncomplexity. Moreover, we provide an efficient CUDA implementation of our\ndynamic convolutions using a gather-scatter approach, achieving a significant\nimprovement in inference speed with MobileNetV2 residual blocks. On human pose\nestimation, a task that is inherently spatially sparse, the processing speed is\nincreased by 60% with no loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:11:16 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:09:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Verelst", "Thomas", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1912.03207", "submitter": "Boyang Deng", "authors": "Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey\n  Hinton, Mohammad Norouzi, Andrea Tagliasacchi", "title": "NASA: Neural Articulated Shape Approximation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient representation of articulated objects such as human bodies is an\nimportant problem in computer vision and graphics. To efficiently simulate\ndeformation, existing approaches represent 3D objects using polygonal meshes\nand deform them using skinning techniques. This paper introduces neural\narticulated shape approximation (NASA), an alternative framework that enables\nefficient representation of articulated deformable objects using neural\nindicator functions that are conditioned on pose. Occupancy testing using NASA\nis straightforward, circumventing the complexity of meshes and the issue of\nwater-tightness. We demonstrate the effectiveness of NASA for 3D tracking\napplications, and discuss other potential extensions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:18:35 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:23:51 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 14:49:13 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 18:57:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Deng", "Boyang", ""], ["Lewis", "JP", ""], ["Jeruzalski", "Timothy", ""], ["Pons-Moll", "Gerard", ""], ["Hinton", "Geoffrey", ""], ["Norouzi", "Mohammad", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1912.03221", "submitter": "Martin Robert", "authors": "Martin Robert, Patrick Dallaire and Philippe Gigu\\`ere", "title": "Tree bark re-identification using a deep-learning feature descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to visually re-identify objects is a fundamental capability in\nvision systems. Oftentimes, it relies on collections of visual signatures based\non descriptors, such as SIFT or SURF. However, these traditional descriptors\nwere designed for a certain domain of surface appearances and geometries\n(limited relief). Consequently, highly-textured surfaces such as tree bark pose\na challenge to them. In turn, this makes it more difficult to use trees as\nidentifiable landmarks for navigational purposes (robotics) or to track felled\nlumber along a supply chain (logistics). We thus propose to use data-driven\ndescriptors trained on bark images for tree surface re-identification. To this\neffect, we collected a large dataset containing 2,400 bark images with strong\nillumination changes, annotated by surface and with the ability to pixel-align\nthem. We used this dataset to sample from more than 2 million 64x64 pixel\npatches to train our novel local descriptors DeepBark and SqueezeBark. Our\nDeepBark method has shown a clear advantage against the hand-crafted\ndescriptors SIFT and SURF. For instance, we demonstrated that DeepBark can\nreach a mAP of 87.2% when retrieving 11 relevant bark images, i.e.\ncorresponding to the same physical surface, to a bark query against 7,900\nimages. Our work thus suggests that re-identifying tree surfaces in a\nchallenging illuminations context is possible. We also make public our dataset,\nwhich can be used to benchmark surface re-identification techniques.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:43:02 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 15:14:40 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Robert", "Martin", ""], ["Dallaire", "Patrick", ""], ["Gigu\u00e8re", "Philippe", ""]]}, {"id": "1912.03227", "submitter": "Jannik Zuern", "authors": "Jannik Z\\\"urn, Wolfram Burgard, Abhinav Valada", "title": "Self-Supervised Visual Terrain Classification from Unsupervised Acoustic\n  Feature Learning", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots operating in unknown urban environments encounter a wide range\nof complex terrains to which they must adapt their planned trajectory for safe\nand efficient navigation. Most existing approaches utilize supervised learning\nto classify terrains from either an exteroceptive or a proprioceptive sensor\nmodality. However, this requires a tremendous amount of manual labeling effort\nfor each newly encountered terrain as well as for variations of terrains caused\nby changing environmental conditions. In this work, we propose a novel terrain\nclassification framework leveraging an unsupervised proprioceptive classifier\nthat learns from vehicle-terrain interaction sounds to self-supervise an\nexteroceptive classifier for pixel-wise semantic segmentation of images. To\nthis end, we first learn a discriminative embedding space for vehicle-terrain\ninteraction sounds from triplets of audio clips formed using visual features of\nthe corresponding terrain patches and cluster the resulting embeddings. We\nsubsequently use these clusters to label the visual terrain patches by\nprojecting the traversed tracks of the robot into the camera images. Finally,\nwe use the sparsely labeled images to train our semantic segmentation network\nin a weakly supervised manner. We present extensive quantitative and\nqualitative results that demonstrate that our proprioceptive terrain classifier\nexceeds the state-of-the-art among unsupervised methods and our self-supervised\nexteroceptive semantic segmentation model achieves a comparable performance to\nsupervised learning with manually labeled data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:54:10 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Z\u00fcrn", "Jannik", ""], ["Burgard", "Wolfram", ""], ["Valada", "Abhinav", ""]]}, {"id": "1912.03238", "submitter": "Gruber Tobias", "authors": "Mario Bijelic and Tobias Gruber and Werner Ritter", "title": "Benchmarking Image Sensors Under Adverse Weather Conditions for\n  Autonomous Driving", "comments": null, "journal-ref": "Published in: 2018 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2018.8500659", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse weather conditions are very challenging for autonomous driving\nbecause most of the state-of-the-art sensors stop working reliably under these\nconditions. In order to develop robust sensors and algorithms, tests with\ncurrent sensors in defined weather conditions are crucial for determining the\nimpact of bad weather for each sensor. This work describes a testing and\nevaluation methodology that helps to benchmark novel sensor technologies and\ncompare them to state-of-the-art sensors. As an example, gated imaging is\ncompared to standard imaging under foggy conditions. It is shown that gated\nimaging outperforms state-of-the-art standard passive imaging due to\ntime-synchronized active illumination.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:26:25 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bijelic", "Mario", ""], ["Gruber", "Tobias", ""], ["Ritter", "Werner", ""]]}, {"id": "1912.03249", "submitter": "Arno Solin", "authors": "Yuxin Hou, Ari Heljakka, Arno Solin", "title": "Gaussian Process Priors for View-Aware Inference", "comments": "Appearing in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While frame-independent predictions with deep neural networks have become the\nprominent solutions to many computer vision tasks, the potential benefits of\nutilizing correlations between frames have received less attention. Even though\nprobabilistic machine learning provides the ability to encode correlation as\nprior knowledge for inference, there is a tangible gap between the theory and\npractice of applying probabilistic methods to modern vision problems. For this,\nwe derive a principled framework to combine information coupling between camera\nposes (translation and orientation) with deep models. We proposed a novel view\nkernel that generalizes the standard periodic kernel in $\\mathrm{SO}(3)$. We\nshow how this soft-prior knowledge can aid several pose-related vision tasks\nlike novel view synthesis and predict arbitrary points in the latent space of\ngenerative models, pointing towards a range of new applications for inter-frame\nreasoning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:41:37 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 20:02:15 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Hou", "Yuxin", ""], ["Heljakka", "Ari", ""], ["Solin", "Arno", ""]]}, {"id": "1912.03251", "submitter": "Gruber Tobias", "authors": "Mario Bijelic and Tobias Gruber and Werner Ritter", "title": "A Benchmark for Lidar Sensors in Fog: Is Detection Breaking Down?", "comments": null, "journal-ref": "Published in: 2018 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2018.8500543", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving at level five does not only means self-driving in the\nsunshine. Adverse weather is especially critical because fog, rain, and snow\ndegrade the perception of the environment. In this work, current state of the\nart light detection and ranging (lidar) sensors are tested in controlled\nconditions in a fog chamber. We present current problems and disturbance\npatterns for four different state of the art lidar systems. Moreover, we\ninvestigate how tuning internal parameters can improve their performance in bad\nweather situations. This is of great importance because most state of the art\ndetection algorithms are based on undisturbed lidar data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:49:10 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bijelic", "Mario", ""], ["Gruber", "Tobias", ""], ["Ritter", "Werner", ""]]}, {"id": "1912.03263", "submitter": "Will Grathwohl", "authors": "Will Grathwohl, Kuan-Chieh Wang, J\\\"orn-Henrik Jacobsen, David\n  Duvenaud, Mohammad Norouzi, Kevin Swersky", "title": "Your Classifier is Secretly an Energy Based Model and You Should Treat\n  it Like One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to reinterpret a standard discriminative classifier of p(y|x) as\nan energy based model for the joint distribution p(x,y). In this setting, the\nstandard class probabilities can be easily computed as well as unnormalized\nvalues of p(x) and p(x|y). Within this framework, standard discriminative\narchitectures may beused and the model can also be trained on unlabeled data.\nWe demonstrate that energy based training of the joint distribution improves\ncalibration, robustness, andout-of-distribution detection while also enabling\nour models to generate samplesrivaling the quality of recent GAN approaches. We\nimprove upon recently proposed techniques for scaling up the training of energy\nbased models and presentan approach which adds little overhead compared to\nstandard classification training. Our approach is the first to achieve\nperformance rivaling the state-of-the-artin both generative and discriminative\nlearning within one hybrid model.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:00:36 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 19:57:55 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 15:40:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Grathwohl", "Will", ""], ["Wang", "Kuan-Chieh", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""], ["Duvenaud", "David", ""], ["Norouzi", "Mohammad", ""], ["Swersky", "Kevin", ""]]}, {"id": "1912.03264", "submitter": "Guocheng Qian", "authors": "Guocheng Qian and Abdulellah Abualshour and Guohao Li and Ali Thabet\n  and Bernard Ghanem", "title": "PU-GCN: Point Cloud Upsampling using Graph Convolutional Networks", "comments": "Get accepted to CVPR 2021. The source code of this work is available\n  at https://github.com/guochengqian/PU-GCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of learning-based point cloud upsampling pipelines heavily\nrelies on the upsampling modules and feature extractors used therein. For the\npoint upsampling module, we propose a novel model called NodeShuffle, which\nuses a Graph Convolutional Network (GCN) to better encode local point\ninformation from point neighborhoods. NodeShuffle is versatile and can be\nincorporated into any point cloud upsampling pipeline. Extensive experiments\nshow how NodeShuffle consistently improves state-of-the-art upsampling methods.\nFor feature extraction, we also propose a new multi-scale point feature\nextractor, called Inception DenseGCN. By aggregating features at multiple\nscales, this feature extractor enables further performance gain in the final\nupsampled point clouds. We combine Inception DenseGCN with NodeShuffle into a\nnew point upsampling pipeline called PU-GCN. PU-GCN sets new state-of-art\nperformance with much fewer parameters and more efficient inference.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 13:18:19 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 09:09:54 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 15:34:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Qian", "Guocheng", ""], ["Abualshour", "Abdulellah", ""], ["Li", "Guohao", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1912.03280", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco and Renato A. Krohling", "title": "Recent advances in deep learning applied to skin cancer detection", "comments": "Paper accepted in the Retrospectives Workshop @ NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is a major public health problem around the world. Its early\ndetection is very important to increase patient prognostics. However, the lack\nof qualified professionals and medical instruments are significant issues in\nthis field. In this context, over the past few years, deep learning models\napplied to automated skin cancer detection have become a trend. In this paper,\nwe present an overview of the recent advances reported in this field as well as\na discussion about the challenges and opportunities for improvement in the\ncurrent models. In addition, we also present some important aspects regarding\nthe use of these models in smartphones and indicate future directions we\nbelieve the field will take.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:23:30 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Pacheco", "Andre G. C.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1912.03310", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov", "title": "Geometric Capsule Autoencoders for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn object representations from 3D point clouds\nusing bundles of geometrically interpretable hidden units, which we call\ngeometric capsules. Each geometric capsule represents a visual entity, such as\nan object or a part, and consists of two components: a pose and a feature. The\npose encodes where the entity is, while the feature encodes what it is. We use\nthese capsules to construct a Geometric Capsule Autoencoder that learns to\ngroup 3D points into parts (small local surfaces), and these parts into the\nwhole object, in an unsupervised manner. Our novel Multi-View Agreement voting\nmechanism is used to discover an object's canonical pose and its pose-invariant\nfeature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the\nproperties of the learned representations and show the benefits of having\nmultiple votes agree. We perform alignment and retrieval of arbitrarily rotated\nobjects -- tasks that evaluate our model's object identification and canonical\npose recovery capabilities -- and obtained insightful results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:10:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Srivastava", "Nitish", ""], ["Goh", "Hanlin", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1912.03330", "submitter": "Ishan Misra", "authors": "Xueting Yan and Ishan Misra and Abhinav Gupta and Deepti Ghadiyaram\n  and Dhruv Mahajan", "title": "ClusterFit: Improving Generalization of Visual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training convolutional neural networks with weakly-supervised and\nself-supervised strategies is becoming increasingly popular for several\ncomputer vision tasks. However, due to the lack of strong discriminative\nsignals, these learned representations may overfit to the pre-training\nobjective (e.g., hashtag prediction) and not generalize well to downstream\ntasks. In this work, we present a simple strategy - ClusterFit (CF) to improve\nthe robustness of the visual representations learned during pre-training. Given\na dataset, we (a) cluster its features extracted from a pre-trained network\nusing k-means and (b) re-train a new network from scratch on this dataset using\ncluster assignments as pseudo-labels. We empirically show that clustering helps\nreduce the pre-training task-specific information from the extracted features\nthereby minimizing overfitting to the same. Our approach is extensible to\ndifferent pre-training frameworks -- weak- and self-supervised, modalities --\nimages and videos, and pre-training tasks -- object and action classification.\nThrough extensive transfer learning experiments on 11 different target datasets\nof varied vocabularies and granularities, we show that ClusterFit significantly\nimproves the representation quality compared to the state-of-the-art\nlarge-scale (millions / billions) weakly-supervised image and video models and\nself-supervised image models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 19:56:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Yan", "Xueting", ""], ["Misra", "Ishan", ""], ["Gupta", "Abhinav", ""], ["Ghadiyaram", "Deepti", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "1912.03354", "submitter": "Tayssir Doghri", "authors": "Tayssir Doghri, Leszek Szczecinski, Jacob Benesty and Amar Mitiche", "title": "Bilinear Models for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we define and analyze the bilinear models which replace the\nconventional linear operation used in many building blocks of machine learning\n(ML). The main idea is to devise the ML algorithms which are adapted to the\nobjects they treat. In the case of monochromatic images, we show that the\nbilinear operation exploits better the structure of the image than the\nconventional linear operation which ignores the spatial relationship between\nthe pixels. This translates into significantly smaller number of parameters\nrequired to yield the same performance. We show numerical examples of\nclassification in the MNIST data set.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:59:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Doghri", "Tayssir", ""], ["Szczecinski", "Leszek", ""], ["Benesty", "Jacob", ""], ["Mitiche", "Amar", ""]]}, {"id": "1912.03379", "submitter": "Davood Karimi", "authors": "Davood Karimi", "title": "Sparse and redundant signal representations for x-ray computed\n  tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image models are central to all image processing tasks. The great\nadvancements in digital image processing would not have been made possible\nwithout powerful models which, themselves, have evolved over time. In the past\ndecade, patch-based models have emerged as one of the most effective models for\nnatural images. Patch-based methods have outperformed other competing methods\nin many image processing tasks. These developments have come at a time when\ngreater availability of powerful computational resources and growing concerns\nover the health risks of the ionizing radiation encourage research on image\nprocessing algorithms for computed tomography (CT). The goal of this paper is\nto explain the principles of patch-based methods and to review some of their\nrecent applications in CT. We review the central concepts in patch-based image\nprocessing and explain some of the state-of-the-art algorithms, with a focus on\naspects that are more relevant to CT. Then, we review some of the recent\napplication of patch-based methods in CT.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 22:50:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Karimi", "Davood", ""]]}, {"id": "1912.03383", "submitter": "Yan Wang", "authors": "Yan Wang, Xu Wei, Fengze Liu, Jieneng Chen, Yuyin Zhou, Wei Shen,\n  Elliot K. Fishman, Alan L. Yuille", "title": "Deep Distance Transform for Tubular Structure Segmentation in CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tubular structure segmentation in medical images, e.g., segmenting vessels in\nCT scans, serves as a vital step in the use of computers to aid in screening\nearly stages of related diseases. But automatic tubular structure segmentation\nin CT scans is a challenging problem, due to issues such as poor contrast,\nnoise and complicated background. A tubular structure usually has a\ncylinder-like shape which can be well represented by its skeleton and\ncross-sectional radii (scales). Inspired by this, we propose a geometry-aware\ntubular structure segmentation method, Deep Distance Transform (DDT), which\ncombines intuitions from the classical distance transform for skeletonization\nand modern deep segmentation networks. DDT first learns a multi-task network to\npredict a segmentation mask for a tubular structure and a distance map. Each\nvalue in the map represents the distance from each tubular structure voxel to\nthe tubular structure surface. Then the segmentation mask is refined by\nleveraging the shape prior reconstructed from the distance map. We apply our\nDDT on six medical image datasets. The experiments show that (1) DDT can boost\ntubular structure segmentation performance significantly (e.g., over 13%\nimprovement measured by DSC for pancreatic duct segmentation), and (2) DDT\nadditionally provides a geometrical measurement for a tubular structure, which\nis important for clinical diagnosis (e.g., the cross-sectional scale of a\npancreatic duct can be an indicator for pancreatic cancer).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 23:04:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Yan", ""], ["Wei", "Xu", ""], ["Liu", "Fengze", ""], ["Chen", "Jieneng", ""], ["Zhou", "Yuyin", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1912.03406", "submitter": "Malhar Jere", "authors": "Malhar Jere, Sandro Herbig, Christine Lind, Farinaz Koushanfar", "title": "Principal Component Properties of Adversarial Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks for image classification have been found to be\nvulnerable to adversarial samples, which consist of sub-perceptual noise added\nto a benign image that can easily fool trained neural networks, posing a\nsignificant risk to their commercial deployment. In this work, we analyze\nadversarial samples through the lens of their contributions to the principal\ncomponents of each image, which is different than prior works in which authors\nperformed PCA on the entire dataset. We investigate a number of\nstate-of-the-art deep neural networks trained on ImageNet as well as several\nattacks for each of the networks. Our results demonstrate empirically that\nadversarial samples across several attacks have similar properties in their\ncontributions to the principal components of neural network inputs. We propose\na new metric for neural networks to measure their robustness to adversarial\nsamples, termed the (k,p) point. We utilize this metric to achieve 93.36%\naccuracy in detecting adversarial samples independent of architecture and\nattack type for models trained on ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 01:15:40 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jere", "Malhar", ""], ["Herbig", "Sandro", ""], ["Lind", "Christine", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1912.03418", "submitter": "Donghuan Lu", "authors": "Donghuan Lu, Morgan Heisler, Da Ma, Setareh Dabiri, Sieun Lee, Gavin\n  Weiguang Ding, Marinko V. Sarunic and Mirza Faisal Beg", "title": "Cascaded Deep Neural Networks for Retinal Layer Segmentation of Optical\n  Coherence Tomography with Fluid Presence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is a non-invasive imaging technology which\ncan provide micrometer-resolution cross-sectional images of the inner\nstructures of the eye. It is widely used for the diagnosis of ophthalmic\ndiseases with retinal alteration, such as layer deformation and fluid\naccumulation. In this paper, a novel framework was proposed to segment retinal\nlayers with fluid presence. The main contribution of this study is two folds:\n1) we developed a cascaded network framework to incorporate the prior\nstructural knowledge; 2) we proposed a novel deep neural network based on U-Net\nand fully convolutional network, termed LF-UNet. Cross validation experiments\nproved that the proposed LF-UNet has superior performance comparing with the\nstate-of-the-art methods, and incorporating the relative distance map\nstructural prior information could further improve the performance regardless\nthe network.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:45:36 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lu", "Donghuan", ""], ["Heisler", "Morgan", ""], ["Ma", "Da", ""], ["Dabiri", "Setareh", ""], ["Lee", "Sieun", ""], ["Ding", "Gavin Weiguang", ""], ["Sarunic", "Marinko V.", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "1912.03426", "submitter": "Rares Ambrus", "authors": "Jiexiong Tang, Rares Ambrus, Vitor Guizilini, Sudeep Pillai, Hanme\n  Kim, Patric Jensfelt, Adrien Gaidon", "title": "Self-Supervised 3D Keypoint Learning for Ego-motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and matching robust viewpoint-invariant keypoints is critical for\nvisual SLAM and Structure-from-Motion. State-of-the-art learning-based methods\ngenerate training samples via homography adaptation to create 2D synthetic\nviews with known keypoint matches from a single image. This approach, however,\ndoes not generalize to non-planar 3D scenes with illumination variations\ncommonly seen in real-world videos. In this work, we propose self-supervised\nlearning of depth-aware keypoints directly from unlabeled videos. We jointly\nlearn keypoint and depth estimation networks by combining appearance and\ngeometric matching via a differentiable structure-from-motion module based on\nProcrustean residual pose correction. We describe how our self-supervised\nkeypoints can be integrated into state-of-the-art visual odometry frameworks\nfor robust and accurate ego-motion estimation of autonomous vehicles in\nreal-world conditions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 03:44:28 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 19:32:56 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 04:14:23 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tang", "Jiexiong", ""], ["Ambrus", "Rares", ""], ["Guizilini", "Vitor", ""], ["Pillai", "Sudeep", ""], ["Kim", "Hanme", ""], ["Jensfelt", "Patric", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1912.03432", "submitter": "Peyman Bateni", "authors": "Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood and Leonid\n  Sigal", "title": "Improved Few-Shot Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a fundamental task in computer vision that carries the\npromise of alleviating the need for exhaustively labeled data. Most few-shot\nlearning approaches to date have focused on progressively more complex neural\nfeature extractors and classifier adaptation strategies, as well as the\nrefinement of the task definition itself. In this paper, we explore the\nhypothesis that a simple class-covariance-based distance metric, namely the\nMahalanobis distance, adopted into a state of the art few-shot learning\napproach (CNAPS) can, in and of itself, lead to a significant performance\nimprovement. We also discover that it is possible to learn adaptive feature\nextractors that allow useful estimation of the high dimensional feature\ncovariances required by this metric from surprisingly few samples. The result\nof our work is a new \"Simple CNAPS\" architecture which has up to 9.2% fewer\ntrainable parameters than CNAPS and performs up to 6.1% better than state of\nthe art on the standard few-shot image classification benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 04:04:12 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 21:37:18 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 16:59:41 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Bateni", "Peyman", ""], ["Goyal", "Raghav", ""], ["Masrani", "Vaden", ""], ["Wood", "Frank", ""], ["Sigal", "Leonid", ""]]}, {"id": "1912.03442", "submitter": "Behnoosh Parsa", "authors": "Behnoosh Parsa, Athma Narayanan, Behzad Dariush", "title": "Spatio-Temporal Pyramid Graph Convolutions for Human Action Recognition\n  and Postural Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition of human actions and associated interactions with objects and the\nenvironment is an important problem in computer vision due to its potential\napplications in a variety of domains. The most versatile methods can generalize\nto various environments and deal with cluttered backgrounds, occlusions, and\nviewpoint variations. Among them, methods based on graph convolutional networks\nthat extract features from the skeleton have demonstrated promising\nperformance. In this paper, we propose a novel Spatio-Temporal Pyramid Graph\nConvolutional Network (ST-PGN) for online action recognition for ergonomic risk\nassessment that enables the use of features from all levels of the skeleton\nfeature hierarchy. The proposed algorithm outperforms state-of-art action\nrecognition algorithms tested on two public benchmark datasets typically used\nfor postural assessment (TUM and UW-IOM). We also introduce a pipeline to\nenhance postural assessment methods with online action recognition techniques.\nFinally, the proposed algorithm is integrated with a traditional ergonomic risk\nindex (REBA) to demonstrate the potential value for assessment of\nmusculoskeletal disorders in occupational safety.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:16:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Parsa", "Behnoosh", ""], ["Narayanan", "Athma", ""], ["Dariush", "Behzad", ""]]}, {"id": "1912.03445", "submitter": "Junru Wu", "authors": "Junru Wu, Xiang Yu, Ding Liu, Manmohan Chandraker, Zhangyang Wang", "title": "DAVID: Dual-Attentional Video Deblurring", "comments": "Accepted in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind video deblurring restores sharp frames from a blurry sequence without\nany prior. It is a challenging task because the blur due to camera shake,\nobject movement and defocusing is heterogeneous in both temporal and spatial\ndimensions. Traditional methods train on datasets synthesized with a single\nlevel of blur, and thus do not generalize well across levels of blurriness. To\naddress this challenge, we propose a dual attention mechanism to dynamically\naggregate temporal cues for deblurring with an end-to-end trainable network\nstructure. Specifically, an internal attention module adaptively selects the\noptimal temporal scales for restoring the sharp center frame. An external\nattention module adaptively aggregates and refines multiple sharp frame\nestimates, from several internal attention modules designed for different blur\nlevels. To train and evaluate on more diverse blur severity levels, we propose\na Challenging DVD dataset generated from the raw DVD video set by pooling\nframes with different temporal windows. Our framework achieves consistently\nbetter performance on this more challenging dataset while obtaining strongly\ncompetitive results on the original DVD benchmark. Extensive ablative studies\nand qualitative visualizations further demonstrate the advantage of our method\nin handling real video blur.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:31:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wu", "Junru", ""], ["Yu", "Xiang", ""], ["Liu", "Ding", ""], ["Chandraker", "Manmohan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1912.03455", "submitter": "Chih-Fan Chen", "authors": "Ruizhe Wang, Chih-Fan Chen, Hao Peng, Xudong Liu, Oliver Liu and Xin\n  Li", "title": "Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image", "comments": "8 pages + 11 pages (supplemental material), 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to generate high fidelity 3D face avatar with a\nhigh-resolution UV texture map from a single image. To estimate the face\ngeometry, we use a deep neural network to directly predict vertex coordinates\nof the 3D face model from the given image. The 3D face geometry is further\nrefined by a non-rigid deformation process to more accurately capture facial\nlandmarks before texture projection. A key novelty of our approach is to train\nthe shape regression network on facial images synthetically generated using a\nhigh-quality rendering engine. Moreover, our shape estimator fully leverages\nthe discriminative power of deep facial identity features learned from millions\nof facial images. We have conducted extensive experiments to demonstrate the\nsuperiority of our optimized 2D-to-3D rendering approach, especially its\nexcellent generalization property on real-world selfie images. Our proposed\nsystem of rendering 3D avatars from 2D images has a wide range of applications\nfrom virtual/augmented reality (VR/AR) and telepsychiatry to human-computer\ninteraction and social networks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 07:36:10 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Ruizhe", ""], ["Chen", "Chih-Fan", ""], ["Peng", "Hao", ""], ["Liu", "Xudong", ""], ["Liu", "Oliver", ""], ["Li", "Xin", ""]]}, {"id": "1912.03458", "submitter": "Yinpeng Chen", "authors": "Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan,\n  Zicheng Liu", "title": "Dynamic Convolution: Attention over Convolution Kernels", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-weight convolutional neural networks (CNNs) suffer performance\ndegradation as their low computational budgets constrain both the depth (number\nof convolution layers) and the width (number of channels) of CNNs, resulting in\nlimited representation capability. To address this issue, we present Dynamic\nConvolution, a new design that increases model complexity without increasing\nthe network depth or width. Instead of using a single convolution kernel per\nlayer, dynamic convolution aggregates multiple parallel convolution kernels\ndynamically based upon their attentions, which are input dependent. Assembling\nmultiple kernels is not only computationally efficient due to the small kernel\nsize, but also has more representation power since these kernels are aggregated\nin a non-linear way via attention. By simply using dynamic convolution for the\nstate-of-the-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet\nclassification is boosted by 2.9% with only 4% additional FLOPs and 2.9 AP gain\nis achieved on COCO keypoint detection.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 07:51:35 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 21:56:49 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Chen", "Yinpeng", ""], ["Dai", "Xiyang", ""], ["Liu", "Mengchen", ""], ["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Liu", "Zicheng", ""]]}, {"id": "1912.03463", "submitter": "Erdem Varol", "authors": "Erdem Varol, Amin Nejatbakhsh, Conor McGrory", "title": "Temporal Wasserstein non-negative matrix factorization for non-rigid\n  motion segmentation and spatiotemporal deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV cs.LG eess.IV q-bio.QM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motion segmentation for natural images commonly relies on dense optic flow to\nyield point trajectories which can be grouped into clusters through various\nmeans including spectral clustering or minimum cost multicuts. However, in\nbiological imaging scenarios, such as fluorescence microscopy or calcium\nimaging, where the signal to noise ratio is compromised and intensity\nfluctuations occur, optical flow may be difficult to approximate. To this end,\nwe propose an alternative paradigm for motion segmentation based on optimal\ntransport which models the video frames as time-varying mass represented as\nhistograms. Thus, we cast motion segmentation as a temporal non-linear matrix\nfactorization problem with Wasserstein metric loss. The dictionary elements of\nthis factorization yield segmentation of motion into coherent objects while the\nloading coefficients allow for time-varying intensity signal of the moving\nobjects to be captured. We demonstrate the use of the proposed paradigm on a\nsimulated multielectrode drift scenario, as well as calcium indicating\nfluorescence microscopy videos of the nematode Caenorhabditis elegans (C.\nelegans). The latter application has the added utility of extracting neural\nactivity of the animal in freely conducted behavior.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 08:30:23 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Varol", "Erdem", ""], ["Nejatbakhsh", "Amin", ""], ["McGrory", "Conor", ""]]}, {"id": "1912.03467", "submitter": "Mohamed Karim Belaid", "authors": "Mohamed Karim Belaid", "title": "Comparison of Neuronal Attention Models", "comments": null, "journal-ref": "Data Science Seminar, 2019, Uni Passau", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent models for image processing are using the Convolutional neural network\n(CNN) which requires a pixel per pixel analysis of the input image. This method\nworks well. However, it is time-consuming if we have large images. To increase\nthe performance, by improving the training time or the accuracy, we need a\nsize-independent method. As a solution, we can add a Neuronal Attention model\n(NAM). The power of this new approach is that it can efficiently choose several\nsmall regions from the initial image to focus on. The purpose of this paper is\nto explain and also test each of the NAM's parameters.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 09:00:18 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Belaid", "Mohamed Karim", ""]]}, {"id": "1912.03478", "submitter": "Yiyi Zhou", "authors": "Yiyi Zhou, Rongrong Ji, Gen Luo, Xiaoshuai Sun, Jinsong Su, Xinghao\n  Ding, Chia-wen Lin, Qi Tian", "title": "A Real-time Global Inference Network for One-stage Referring Expression\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring Expression Comprehension (REC) is an emerging research spot in\ncomputer vision, which refers to detecting the target region in an image given\nan text description. Most existing REC methods follow a multi-stage pipeline,\nwhich are computationally expensive and greatly limit the application of REC.\nIn this paper, we propose a one-stage model towards real-time REC, termed\nReal-time Global Inference Network (RealGIN). RealGIN addresses the diversity\nand complexity issues in REC with two innovative designs: the Adaptive Feature\nSelection (AFS) and the Global Attentive ReAsoNing unit (GARAN). AFS adaptively\nfuses features at different semantic levels to handle the varying content of\nexpressions. GARAN uses the textual feature as a pivot to collect\nexpression-related visual information from all regions, and thenselectively\ndiffuse such information back to all regions, which provides sufficient context\nfor modeling the complex linguistic conditions in expressions. On five\nbenchmark datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, ReferIt and Flickr30k,\nthe proposed RealGIN outperforms most prior works and achieves very competitive\nperformances against the most advanced method, i.e., MAttNet. Most importantly,\nunder the same hardware, RealGIN can boost the processing speed by about 10\ntimes over the existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 09:45:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhou", "Yiyi", ""], ["Ji", "Rongrong", ""], ["Luo", "Gen", ""], ["Sun", "Xiaoshuai", ""], ["Su", "Jinsong", ""], ["Ding", "Xinghao", ""], ["Lin", "Chia-wen", ""], ["Tian", "Qi", ""]]}, {"id": "1912.03485", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Yongqin Wang, Keshav Balasubramaniam,\n  Murali Annavaram", "title": "Privacy-Preserving Inference in Machine Learning Services Using Trusted\n  Execution Environments", "comments": "13 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Origami, which provides privacy-preserving inference for\nlarge deep neural network (DNN) models through a combination of enclave\nexecution, cryptographic blinding, interspersed with accelerator-based\ncomputation. Origami partitions the ML model into multiple partitions. The\nfirst partition receives the encrypted user input within an SGX enclave. The\nenclave decrypts the input and then applies cryptographic blinding to the input\ndata and the model parameters. Cryptographic blinding is a technique that adds\nnoise to obfuscate data. Origami sends the obfuscated data for computation to\nan untrusted GPU/CPU. The blinding and de-blinding factors are kept private by\nthe SGX enclave, thereby preventing any adversary from denoising the data, when\nthe computation is offloaded to a GPU/CPU. The computed output is returned to\nthe enclave, which decodes the computation on noisy data using the unblinding\nfactors privately stored within SGX. This process may be repeated for each DNN\nlayer, as has been done in prior work Slalom.\n  However, the overhead of blinding and unblinding the data is a limiting\nfactor to scalability. Origami relies on the empirical observation that the\nfeature maps after the first several layers can not be used, even by a powerful\nconditional GAN adversary to reconstruct input. Hence, Origami dynamically\nswitches to executing the rest of the DNN layers directly on an accelerator\nwithout needing any further cryptographic blinding intervention to preserve\nprivacy. We empirically demonstrate that using Origami, a conditional GAN\nadversary, even with an unlimited inference budget, cannot reconstruct the\ninput. We implement and demonstrate the performance gains of Origami using the\nVGG-16 and VGG-19 models. Compared to running the entire VGG-19 model within\nSGX, Origami inference improves the performance of private inference from 11x\nwhile using Slalom to 15.1x.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 10:27:33 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Wang", "Yongqin", ""], ["Balasubramaniam", "Keshav", ""], ["Annavaram", "Murali", ""]]}, {"id": "1912.03538", "submitter": "Sara Beery", "authors": "Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang", "title": "Context R-CNN: Long Term Temporal Context for Per-Camera Object\n  Detection", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In static monitoring cameras, useful contextual information can stretch far\nbeyond the few seconds typical video understanding models might see: subjects\nmay exhibit similar behavior over multiple days, and background objects remain\nstatic. Due to power and storage constraints, sampling frequencies are low,\noften no faster than one frame per second, and sometimes are irregular due to\nthe use of a motion trigger. In order to perform well in this setting, models\nmust be robust to irregular sampling rates. In this paper we propose a method\nthat leverages temporal context from the unlabeled frames of a novel camera to\nimprove performance at that camera. Specifically, we propose an attention-based\napproach that allows our model, Context R-CNN, to index into a long term memory\nbank constructed on a per-camera basis and aggregate contextual features from\nother frames to boost object detection performance on the current frame.\n  We apply Context R-CNN to two settings: (1) species detection using camera\ntraps, and (2) vehicle detection in traffic cameras, showing in both settings\nthat Context R-CNN leads to performance gains over strong baselines. Moreover,\nwe show that increasing the contextual time horizon leads to improved results.\nWhen applied to camera trap data from the Snapshot Serengeti dataset, Context\nR-CNN with context from up to a month of images outperforms a single-frame\nbaseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by\n11.2% mAP.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 17:53:48 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 21:52:33 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 15:09:17 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Beery", "Sara", ""], ["Wu", "Guanhang", ""], ["Rathod", "Vivek", ""], ["Votel", "Ronny", ""], ["Huang", "Jonathan", ""]]}, {"id": "1912.03539", "submitter": "Christopher Ren", "authors": "Hannah J. Murphy, Christopher X. Ren, Matthew T. Calef", "title": "Feature Augmentation Improves Anomalous Change Detection for Human\n  Activity Identification in Synthetic Aperture Radar Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomalous change detection (ACD) methods separate common, uninteresting\nchanges from rare, significant changes in co-registered images collected at\ndifferent points in time. In this paper we evaluate methods to improve the\nperformance of ACD in detecting human activity in SAR imagery using outdoor\nmusic festivals as a target. Our results show that the low dimensionality of\nSAR data leads to poor performance of ACD when compared to simpler methods such\nas image differencing, but augmenting the dimensionality of our input feature\nspace by incorporating local spatial information leads to enhanced performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 17:54:18 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 16:13:30 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Murphy", "Hannah J.", ""], ["Ren", "Christopher X.", ""], ["Calef", "Matthew T.", ""]]}, {"id": "1912.03587", "submitter": "Qun Liu", "authors": "Qun Liu, Subhashis Hazarika, John M. Patchett, James Paul Ahrens, Ayan\n  Biswas", "title": "Deep Learning-Based Feature-Aware Data Modeling for Complex Physics\n  Simulations", "comments": "Accepted as a research poster at the International Conference for\n  High Performance Computing, Networking, Storage, and Analysis (SC19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data modeling and reduction for in situ is important. Feature-driven methods\nfor in situ data analysis and reduction are a priority for future exascale\nmachines as there are currently very few such methods. We investigate a\ndeep-learning based workflow that targets in situ data processing using\nautoencoders. We propose a Residual Autoencoder integrated Residual in Residual\nDense Block (RRDB) to obtain better performance. Our proposed framework\ncompressed our test data into 66 KB from 2.1 MB per 3D volume timestep.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:14:47 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Qun", ""], ["Hazarika", "Subhashis", ""], ["Patchett", "John M.", ""], ["Ahrens", "James Paul", ""], ["Biswas", "Ayan", ""]]}, {"id": "1912.03590", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo", "title": "Learning 2D Temporal Adjacent Networks for Moment Localization with\n  Natural Language", "comments": "This paper is accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving a specific moment from an untrimmed\nvideo by a query sentence. This is a challenging problem because a target\nmoment may take place in relations to other temporal moments in the untrimmed\nvideo. Existing methods cannot tackle this challenge well since they consider\ntemporal moments individually and neglect the temporal dependencies. In this\npaper, we model the temporal relations between video moments by a\ntwo-dimensional map, where one dimension indicates the starting time of a\nmoment and the other indicates the end time. This 2D temporal map can cover\ndiverse video moments with different lengths, while representing their adjacent\nrelations. Based on the 2D map, we propose a Temporal Adjacent Network\n(2D-TAN), a single-shot framework for moment localization. It is capable of\nencoding the adjacent temporal relation, while learning discriminative features\nfor matching video moments with referring expressions. We evaluate the proposed\n2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet\nCaptions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:34:39 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 13:44:56 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2020 15:27:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Songyang", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.03604", "submitter": "Brian Wandell", "authors": "Zhenyi Liu, Trisha Lian, Joyce Farrell, Brian Wandell", "title": "Neural Network Generalization: The impact of camera parameters", "comments": "11 pages, 11 figures, in preparation for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We quantify the generalization of a convolutional neural network (CNN)\ntrained to identify cars. First, we perform a series of experiments to train\nthe network using one image dataset - either synthetic or from a camera - and\nthen test on a different image dataset. We show that generalization between\nimages obtained with different cameras is roughly the same as generalization\nbetween images from a camera and ray-traced multispectral synthetic images.\nSecond, we use ISETAuto, a soft prototyping tool that creates ray-traced\nmultispectral simulations of camera images, to simulate sensor images with a\nrange of pixel sizes, color filters, acquisition and post-acquisition\nprocessing. These experiments reveal how variations in specific camera\nparameters and image processing operations impact CNN generalization. We find\nthat (a) pixel size impacts generalization, (b) demosaicking substantially\nimpacts performance and generalization for shallow (8-bit) bit-depths but not\ndeeper ones (10-bit), and (c) the network performs well using raw (not\ndemosaicked) sensor data for 10-bit pixels.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 03:14:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Zhenyi", ""], ["Lian", "Trisha", ""], ["Farrell", "Joyce", ""], ["Wandell", "Brian", ""]]}, {"id": "1912.03606", "submitter": "John Zech", "authors": "John R. Zech, Jessica Zosa Forde, Michael L. Littman", "title": "Individual predictions matter: Assessing the effect of data ordering in\n  training fine-tuned CNNs for medical imaging", "comments": "J.Z. and J.F. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reproduced the results of CheXNet with fixed hyperparameters and 50\ndifferent random seeds to identify 14 finding in chest radiographs (x-rays).\nBecause CheXNet fine-tunes a pre-trained DenseNet, the random seed affects the\nordering of the batches of training data but not the initialized model weights.\nWe found substantial variability in predictions for the same radiograph across\nmodel runs (mean ln[(maximum probability)/(minimum probability)] 2.45,\ncoefficient of variation 0.543). This individual radiograph-level variability\nwas not fully reflected in the variability of AUC on a large test set.\nAveraging predictions from 10 models reduced variability by nearly 70% (mean\ncoefficient of variation from 0.543 to 0.169, t-test 15.96, p-value < 0.0001).\nWe encourage researchers to be aware of the potential variability of CNNs and\nensemble predictions from multiple models to minimize the effect this\nvariability may have on the care of individual patients when these models are\ndeployed clinically.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 03:18:18 GMT"}], "update_date": "2019-12-28", "authors_parsed": [["Zech", "John R.", ""], ["Forde", "Jessica Zosa", ""], ["Littman", "Michael L.", ""]]}, {"id": "1912.03612", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Houwen Peng, Le Yang, Jianlong Fu, Jiebo Luo", "title": "Learning Sparse 2D Temporal Adjacent Networks for Temporal Action\n  Localization", "comments": "This is our winner solution for the HACS Temporal Action Localization\n  Challenge at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we introduce the Winner method for HACS Temporal Action\nLocalization Challenge 2019. Temporal action localization is challenging since\na target proposal may be related to several other candidate proposals in an\nuntrimmed video. Existing methods cannot tackle this challenge well since\ntemporal proposals are considered individually and their temporal dependencies\nare neglected. To address this issue, we propose sparse 2D temporal adjacent\nnetworks to model the temporal relationship between candidate proposals. This\nmethod is built upon the recent proposed 2D-TAN approach. The sampling strategy\nin 2D-TAN introduces the unbalanced context problem, where short proposals can\nperceive more context than long proposals. Therefore, we further propose a\nSparse 2D Temporal Adjacent Network (S-2D-TAN). It is capable of involving more\ncontext information for long proposals and further learning discriminative\nfeatures from them. By combining our S-2D-TAN with a simple action classifier,\nour method achieves a mAP of 23.49 on the test set, which win the first place\nin the HACS challenge.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 04:16:28 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Songyang", ""], ["Peng", "Houwen", ""], ["Yang", "Le", ""], ["Fu", "Jianlong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.03613", "submitter": "Tae Soo Kim", "authors": "Tae Soo Kim, Jonathan D. Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi\n  Zhang, Weichao Qiu, Alan Yuille, Gregory D. Hager", "title": "DASZL: Dynamic Action Signatures for Zero-shot Learning", "comments": "10 pages, 4 figures, 3 tables, AAAI2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many realistic applications of activity recognition where the set\nof potential activity descriptions is combinatorially large. This makes\nend-to-end supervised training of a recognition system impractical as no\ntraining set is practically able to encompass the entire label set. In this\npaper, we present an approach to fine-grained recognition that models\nactivities as compositions of dynamic action signatures. This compositional\napproach allows us to reframe fine-grained recognition as zero-shot activity\nrecognition, where a detector is composed \"on the fly\" from simple\nfirst-principles state machines supported by deep-learned components. We\nevaluate our method on the Olympic Sports and UCF101 datasets, where our model\nestablishes a new state of the art under multiple experimental paradigms. We\nalso extend this method to form a unique framework for zero-shot joint\nsegmentation and classification of activities in video and demonstrate the\nfirst results in zero-shot decoding of complex action sequences on a\nwidely-used surgical dataset. Lastly, we show that we can use off-the-shelf\nobject detectors to recognize activities in completely de-novo settings with no\nadditional training.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 04:30:59 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 18:19:04 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 03:53:54 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kim", "Tae Soo", ""], ["Jones", "Jonathan D.", ""], ["Peven", "Michael", ""], ["Xiao", "Zihao", ""], ["Bai", "Jin", ""], ["Zhang", "Yi", ""], ["Qiu", "Weichao", ""], ["Yuille", "Alan", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1912.03623", "submitter": "Qingnan Fan", "authors": "Yingda Yin, Qingnan Fan, Dongdong Chen, Yujie Wang, Angelica\n  Aviles-Rivero, Ruoteng Li, Carola-Bibiane Schnlieb, Dani Lischinski, Baoquan\n  Chen", "title": "Deep Reflection Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections are very common phenomena in our daily photography, which\ndistract people's attention from the scene behind the glass. The problem of\nremoving reflection artifacts is important but challenging due to its ill-posed\nnature. Recent learning-based approaches have demonstrated a significant\nimprovement in removing reflections. However, these methods are limited as they\nrequire a large number of synthetic reflection/clean image pairs for\nsupervision, at the risk of overfitting in the synthetic image domain. In this\npaper, we propose a learning-based approach that captures the reflection\nstatistical prior for single image reflection removal. Our algorithm is driven\nby optimizing the target with joint constraints enhanced between multiple input\nimages during the training stage, but is able to eliminate reflections only\nfrom a single input for evaluation. Our framework allows to predict both\nbackground and reflection via a one-branch deep neural network, which is\nimplemented by the controllable latent code that indicates either the\nbackground or reflection output. We demonstrate superior performance over the\nstate-of-the-art methods on a large range of real-world images. We further\nprovide insightful analysis behind the learned latent code, which may inspire\nmore future work.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 06:10:49 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 05:59:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yin", "Yingda", ""], ["Fan", "Qingnan", ""], ["Chen", "Dongdong", ""], ["Wang", "Yujie", ""], ["Aviles-Rivero", "Angelica", ""], ["Li", "Ruoteng", ""], ["Schnlieb", "Carola-Bibiane", ""], ["Lischinski", "Dani", ""], ["Chen", "Baoquan", ""]]}, {"id": "1912.03628", "submitter": "Adithyavairavan Murali", "authors": "Adithyavairavan Murali, Arsalan Mousavian, Clemens Eppner, Chris\n  Paxton, Dieter Fox", "title": "6-DOF Grasping for Target-driven Object Manipulation in Clutter", "comments": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping in cluttered environments is a fundamental but challenging robotic\nskill. It requires both reasoning about unseen object parts and potential\ncollisions with the manipulator. Most existing data-driven approaches avoid\nthis problem by limiting themselves to top-down planar grasps which is\ninsufficient for many real-world scenarios and greatly limits possible grasps.\nWe present a method that plans 6-DOF grasps for any desired object in a\ncluttered scene from partial point cloud observations. Our method achieves a\ngrasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing\n9 cluttered table scenes (which contain 23 unknown objects and 51 picks in\ntotal) on a real robotic platform. By using our learned collision checking\nmodule, we can even reason about effective grasp sequences to retrieve objects\nthat are not immediately accessible. Supplementary video can be found at\nhttps://youtu.be/w0B5S-gCsJk.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:09:53 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 03:23:05 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Murali", "Adithyavairavan", ""], ["Mousavian", "Arsalan", ""], ["Eppner", "Clemens", ""], ["Paxton", "Chris", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.03629", "submitter": "Francis Williams", "authors": "Francis Williams, Daniele Panozzo, Kwang Moo Yi, Andrea Tagliasacchi", "title": "VoronoiNet: General Functional Approximators with Local Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voronoi diagrams are highly compact representations that are used in various\nGraphics applications. In this work, we show how to embed a differentiable\nversion of it -- via a novel deep architecture -- into a generative deep\nnetwork. By doing so, we achieve a highly compact latent embedding that is able\nto provide much more detailed reconstructions, both in 2D and 3D, for various\nshapes. In this tech report, we introduce our representation and present a set\nof preliminary results comparing it with recently proposed implicit occupancy\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:12:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Williams", "Francis", ""], ["Panozzo", "Daniele", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1912.03630", "submitter": "Xudong Liu", "authors": "Xudong Liu, Ruizhe Wang, Chih-Fan Chen, Minglei Yin, Hao Peng, Shukhan\n  Ng, and Xin Li", "title": "Face Beautification: Beyond Makeup Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial appearance plays an important role in our social lives. Subjective\nperception of women's beauty depends on various face-related (e.g., skin,\nshape, hair) and environmental (e.g., makeup, lighting, angle) factors. Similar\nto cosmetic surgery in the physical world, virtual face beautification is an\nemerging field with many open issues to be addressed. Inspired by the latest\nadvances in style-based synthesis and face beauty prediction, we propose a\nnovel framework of face beautification. For a given reference face with a high\nbeauty score, our GAN-based architecture is capable of translating an inquiry\nface into a sequence of beautified face images with referenced beauty style and\ntargeted beauty score values. To achieve this objective, we propose to\nintegrate both style-based beauty representation (extracted from the reference\nface) and beauty score prediction (trained on SCUT-FBP database) into the\nprocess of beautification. Unlike makeup transfer, our approach targets at\nmany-to-many (instead of one-to-one) translation where multiple outputs can be\ndefined by either different references or varying beauty scores. Extensive\nexperimental results are reported to demonstrate the effectiveness and\nflexibility of the proposed face beautification framework.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:16:53 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 18:29:08 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Liu", "Xudong", ""], ["Wang", "Ruizhe", ""], ["Chen", "Chih-Fan", ""], ["Yin", "Minglei", ""], ["Peng", "Hao", ""], ["Ng", "Shukhan", ""], ["Li", "Xin", ""]]}, {"id": "1912.03632", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Chhavi Dhiman, Dinesh Kumar Vishwakarma", "title": "View-invariant Deep Architecture for Human Action Recognition using late\n  fusion", "comments": "10 pages, 7 figures", "journal-ref": "2019", "doi": "10.1109/TIP.2020.2965299", "report-no": "8960517", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action Recognition for unknown views is a challenging task. We propose\na view-invariant deep human action recognition framework, which is a novel\nintegration of two important action cues: motion and shape temporal dynamics\n(STD). The motion stream encapsulates the motion content of action as RGB\nDynamic Images (RGB-DIs) which are processed by the fine-tuned InceptionV3\nmodel. The STD stream learns long-term view-invariant shape dynamics of action\nusing human pose model (HPM) based view-invariant features mined from\nstructural similarity index matrix (SSIM) based key depth human pose frames. To\npredict the score of the test sample, three types of late fusion (maximum,\naverage and product) techniques are applied on individual stream scores. To\nvalidate the performance of the proposed novel framework the experiments are\nperformed using both cross subject and cross-view validation schemes on three\npublically available benchmarks- NUCLA multi-view dataset, UWA3D-II Activity\ndataset and NTU RGB-D Activity dataset. Our algorithm outperforms with existing\nstate-of-the-arts significantly that is reported in terms of accuracy, receiver\noperating characteristic (ROC) curve and area under the curve (AUC).\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:30:48 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Dhiman", "Chhavi", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "1912.03634", "submitter": "Ali Ghofrani", "authors": "Ali Ghofrani, Rahil Mahdian Toroghi", "title": "Capsule-Based Persian/Arabic Robust Handwritten Digit Recognition Using\n  EM Routing", "comments": "5 pages, 10 figures, 4th International Conference on Pattern\n  Recognition and Image Analysis (IPRIA2019), IEEE", "journal-ref": null, "doi": "10.1109/PRIA.2019.8785981", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of handwritten digit recognition has been\naddressed. However, the underlying language is Persian/Arabic, and the system\nwith which this task is a capsule network (CapsNet) has recently emerged as a\nmore advanced architecture than its ancestor, namely CNN (Convolutional Neural\nNetwork). The training of the architecture is performed using the Hoda dataset,\nwhich has been provided for Persian/Arabic handwritten digits. The output of\nthe system clearly outperforms the results achieved by its ancestors, as well\nas other previously presented recognition algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:58:26 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 20:00:22 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ghofrani", "Ali", ""], ["Toroghi", "Rahil Mahdian", ""]]}, {"id": "1912.03641", "submitter": "Sauradip Nag", "authors": "Kitty Varghese, Sauradip Nag", "title": "SaLite : A light-weight model for salient object detection", "comments": "This was submitted to NCVPRIPG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection is a prevalent computer vision task that has\napplications ranging from abnormality detection to abnormality processing.\nContext modelling is an important criterion in the domain of saliency\ndetection. A global context helps in determining the salient object in a given\nimage by contrasting away other objects in the global view of the scene.\nHowever, the local context features detects the boundaries of the salient\nobject with higher accuracy in a given region. To incorporate the best of both\nworlds, our proposed SaLite model uses both global and local contextual\nfeatures. It is an encoder-decoder based architecture in which the encoder uses\na lightweight SqueezeNet and decoder is modelled using convolution layers.\nModern deep based models entitled for saliency detection use a large number of\nparameters, which is difficult to deploy on embedded systems. This paper\nattempts to solve the above problem using SaLite which is a lighter process for\nsalient object detection without compromising on performance. Our approach is\nextensively evaluated on three publicly available datasets namely DUTS,\nMSRA10K, and SOC. Experimental results show that our proposed SaLite has\nsignificant and consistent improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 08:45:08 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Varghese", "Kitty", ""], ["Nag", "Sauradip", ""]]}, {"id": "1912.03647", "submitter": "Dingheng Wang", "authors": "Dingheng Wang and Guangshe Zhao and Guoqi Li and Lei Deng and Yang Wu", "title": "Compressing 3DCNNs Based on Tensor Train Decomposition", "comments": "Accepted by Neural Networks. Please see the final version by the DOI\n  below", "journal-ref": null, "doi": "10.1016/j.neunet.2020.07.028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional convolutional neural networks (3DCNNs) have been applied in\nmany tasks, e.g., video and 3D point cloud recognition. However, due to the\nhigher dimension of convolutional kernels, the space complexity of 3DCNNs is\ngenerally larger than that of traditional two dimensional convolutional neural\nnetworks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining\nenvironments such as embedded devices, neural network compression is a\npromising approach. In this work, we adopt the tensor train (TT) decomposition,\na straightforward and simple in situ training compression method, to shrink the\n3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT\nformat, we investigate how to select appropriate TT ranks for achieving higher\ncompression ratio. We have also discussed the redundancy of 3D convolutional\nkernels for compression, core significance and future directions of this work,\nas well as the theoretical computation complexity versus practical executing\ntime of convolution in TT. In the light of multiple contrast experiments based\non VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT\ndecomposition can compress 3DCNNs by around one hundred times without\nsignificant accuracy loss, which will enable its applications in extensive real\nworld scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 09:51:08 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 03:42:21 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Dingheng", ""], ["Zhao", "Guangshe", ""], ["Li", "Guoqi", ""], ["Deng", "Lei", ""], ["Wu", "Yang", ""]]}, {"id": "1912.03650", "submitter": "Yair Shemer", "authors": "Yair Shemer, Daniel Rotman, Nahum Shimkin", "title": "ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in building video\nsummarization tools, where the goal is to automatically create a short summary\nof an input video that properly represents the original content. We consider\nshot-based video summarization where the summary consists of a subset of the\nvideo shots which can be of various lengths. A straightforward approach to\nmaximize the representativeness of a subset of shots is by minimizing the total\ndistance between shots and their nearest selected shots. We formulate the task\nof video summarization as an optimization problem with a knapsack-like\nconstraint on the total summary duration. Previous studies have proposed greedy\nalgorithms to solve this problem approximately, but no experiments were\npresented to measure the ability of these methods to obtain solutions with low\ntotal distance. Indeed, our experiments on video summarization datasets show\nthat the success of current methods in obtaining results with low total\ndistance still has much room for improvement. In this paper, we develop\nILS-SUMM, a novel video summarization algorithm to solve the subset selection\nproblem under the knapsack constraint. Our algorithm is based on the well-known\nmetaheuristic optimization framework -- Iterated Local Search (ILS), known for\nits ability to avoid weak local minima and obtain a good near-global minimum.\nExtensive experiments show that our method finds solutions with significantly\nbetter total distance than previous methods. Moreover, to indicate the high\nscalability of ILS-SUMM, we introduce a new dataset consisting of videos of\nvarious lengths.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 10:30:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Shemer", "Yair", ""], ["Rotman", "Daniel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1912.03656", "submitter": "Maurits Bleeker", "authors": "Maurits Bleeker and Maarten de Rijke", "title": "Bidirectional Scene Text Recognition with a Single Decoder", "comments": "8 pages. In 24th European Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Text Recognition (STR) is the problem of recognizing the correct word\nor character sequence in a cropped word image. To obtain more robust output\nsequences, the notion of bidirectional STR has been introduced. So far,\nbidirectional STRs have been implemented by using two separate decoders; one\nfor left-to-right decoding and one for right-to-left. Having two separate\ndecoders for almost the same task with the same output space is undesirable\nfrom a computational and optimization point of view. We introduce the\nbidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR\nmethod with a single decoder for bidirectional text decoding. With its single\ndecoder, Bi-STET outperforms methods that apply bidirectional decoding by using\ntwo separate decoders while also being more efficient than those methods,\nFurthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR\nbenchmarks with Bi-STET. Finally, we provide analyses and insights into the\nperformance of Bi-STET.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:20:35 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 14:44:34 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bleeker", "Maurits", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1912.03663", "submitter": "Itai Lang", "authors": "Itai Lang, Asaf Manor, Shai Avidan", "title": "SampleNet: Differentiable Point Cloud Sampling", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing number of tasks that work directly on point clouds. As the\nsize of the point cloud grows, so do the computational demands of these tasks.\nA possible solution is to sample the point cloud first. Classic sampling\napproaches, such as farthest point sampling (FPS), do not consider the\ndownstream task. A recent work showed that learning a task-specific sampling\ncan improve results significantly. However, the proposed technique did not deal\nwith the non-differentiability of the sampling operation and offered a\nworkaround instead. We introduce a novel differentiable relaxation for point\ncloud sampling that approximates sampled points as a mixture of points in the\nprimary input cloud. Our approximation scheme leads to consistently good\nresults on classification and geometry reconstruction applications. We also\nshow that the proposed sampling method can be used as a front to a point cloud\nregistration network. This is a challenging task since sampling must be\nconsistent across two different point clouds for a shared downstream task. In\nall cases, our approach outperforms existing non-learned and learned sampling\nalternatives. Our code is publicly available at\nhttps://github.com/itailang/SampleNet.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 12:06:20 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 19:39:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lang", "Itai", ""], ["Manor", "Asaf", ""], ["Avidan", "Shai", ""]]}, {"id": "1912.03672", "submitter": "Junyu Gao", "authors": "Junyu Gao, Yuan Yuan, Qi Wang", "title": "Feature-aware Adaptation and Density Alignment for Crowd Counting in\n  Video Surveillance", "comments": "accepted by IEEE T-CYB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep neural networks, the performance of crowd\ncounting and pixel-wise density estimation are continually being refreshed.\nDespite this, there are still two challenging problems in this field: 1)\ncurrent supervised learning needs a large amount of training data, but\ncollecting and annotating them is difficult; 2) existing methods can not\ngeneralize well to the unseen domain. A recently released synthetic crowd\ndataset alleviates these two problems. However, the domain gap between the\nreal-world data and synthetic images decreases the models' performance. To\nreduce the gap, in this paper, we propose a domain-adaptation-style crowd\ncounting method, which can effectively adapt the model from synthetic data to\nthe specific real-world scenes. It consists of Multi-level Featureaware\nAdaptation (MFA) and Structured Density map Alignment (SDA). To be specific,\nMFA boosts the model to extract domain-invariant features from multiple layers.\nSDA guarantees the network outputs fine density maps with a reasonable\ndistribution on the real domain. Finally, we evaluate the proposed method on\nfour mainstream surveillance crowd datasets, Shanghai Tech Part B,\nWorldExpo'10, Mall and UCSD. Extensive experiments evidence that our approach\noutperforms the state-of-the-art methods for the same cross-domain counting\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 12:40:51 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 08:13:50 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gao", "Junyu", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1912.03673", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Kira Maag, Robin Chan, Fabian H\\\"uger, Peter\n  Schlicht, Hanno Gottschalk", "title": "Detection of False Positive and False Negative Samples in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods have outperformed other methods in\nimage recognition. This has fostered imagination of potential application of\ndeep learning technology including safety relevant applications like the\ninterpretation of medical images or autonomous driving. The passage from\nassistance of a human decision maker to ever more automated systems however\nincreases the need to properly handle the failure modes of deep learning\nmodules. In this contribution, we review a set of techniques for the\nself-monitoring of machine-learning algorithms based on uncertainty\nquantification. In particular, we apply this to the task of semantic\nsegmentation, where the machine learning algorithm decomposes an image\naccording to semantic categories. We discuss false positive and false negative\nerror modes at instance-level and review techniques for the detection of such\nerrors that have been recently proposed by the authors. We also give an outlook\non future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 13:04:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Rottmann", "Matthias", ""], ["Maag", "Kira", ""], ["Chan", "Robin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1912.03677", "submitter": "Junyu Gao", "authors": "Junyu Gao, Tao Han, Qi Wang, Yuan Yuan", "title": "Domain-adaptive Crowd Counting via Inter-domain Features Segregation and\n  Gaussian-prior Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, crowd counting using supervised learning achieves a remarkable\nimprovement. Nevertheless, most counters rely on a large amount of manually\nlabeled data. With the release of synthetic crowd data, a potential alternative\nis transferring knowledge from them to real data without any manual label.\nHowever, there is no method to effectively suppress domain gaps and output\nelaborate density maps during the transferring. To remedy the above problems,\nthis paper proposed a Domain-Adaptive Crowd Counting (DACC) framework, which\nconsists of Inter-domain Features Segregation (IFS) and Gaussian-prior\nReconstruction (GPR). To be specific, IFS translates synthetic data to\nrealistic images, which contains domain-shared features extraction and\ndomain-independent features decoration. Then a coarse counter is trained on\ntranslated data and applied to the real world. Moreover, according to the\ncoarse predictions, GPR generates pseudo labels to improve the prediction\nquality of the real data. Next, we retrain a final counter using these pseudo\nlabels. Adaptation experiments on six real-world datasets demonstrate that the\nproposed method outperforms the state-of-the-art methods. Furthermore, the code\nand pre-trained models will be released as soon as possible.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 13:38:48 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 07:27:30 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Gao", "Junyu", ""], ["Han", "Tao", ""], ["Wang", "Qi", ""], ["Yuan", "Yuan", ""]]}, {"id": "1912.03681", "submitter": "Udaranga Wickramasinghe", "authors": "Udaranga Wickramasinghe, Edoardo Remelli, Graham Knott, and Pascal Fua", "title": "Voxel2Mesh: 3D Mesh Model Generation from Volumetric Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN-based volumetric methods that label individual voxels now dominate the\nfield of biomedical segmentation. However, 3D surface representations are often\nrequired for proper analysis. They can be obtained by post-processing the\nlabeled volumes which typically introduces artifacts and prevents end-to-end\ntraining. In this paper, we therefore introduce a novel architecture that goes\ndirectly from 3D image volumes to 3D surfaces without post-processing and with\nbetter accuracy than current methods. We evaluate it on Electron Microscopy and\nMRI brain images as well as CT liver scans. We will show that it outperforms\nstate-of-the-art segmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 13:53:22 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 09:07:58 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wickramasinghe", "Udaranga", ""], ["Remelli", "Edoardo", ""], ["Knott", "Graham", ""], ["Fua", "Pascal", ""]]}, {"id": "1912.03685", "submitter": "Haishan Wu", "authors": "Xin Hou, Biao Wang, Wanqi Hu, Lei Yin, Haishan Wu", "title": "SolarNet: A Deep Learning Framework to Map Solar Power Plants In China\n  From Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable energy such as solar power is critical to fight the ever more\nserious climate change. China is the world leading installer of solar panel and\nnumerous solar power plants were built. In this paper, we proposed a deep\nlearning framework named SolarNet which is designed to perform semantic\nsegmentation on large scale satellite imagery data to detect solar farms.\nSolarNet has successfully mapped 439 solar farms in China, covering near 2000\nsquare kilometers, equivalent to the size of whole Shenzhen city or two and a\nhalf of New York city. To the best of our knowledge, it is the first time that\nwe used deep learning to reveal the locations and sizes of solar farms in\nChina, which could provide insights for solar power companies, market analysts\nand the government.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 14:19:47 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:27:33 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hou", "Xin", ""], ["Wang", "Biao", ""], ["Hu", "Wanqi", ""], ["Yin", "Lei", ""], ["Wu", "Haishan", ""]]}, {"id": "1912.03699", "submitter": "Ying Jin", "authors": "Ying Jin, Ximei Wang, Mingsheng Long, Jianmin Wang", "title": "Minimum Class Confusion for Versatile Domain Adaptation", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a variety of Domain Adaptation (DA) scenarios subject to label sets\nand domain configurations, including closed-set and partial-set DA, as well as\nmulti-source and multi-target DA. It is notable that existing DA methods are\ngenerally designed only for a specific scenario, and may underperform for\nscenarios they are not tailored to. To this end, this paper studies Versatile\nDomain Adaptation (VDA), where one method can handle several different DA\nscenarios without any modification. Towards this goal, a more general inductive\nbias other than the domain alignment should be explored. We delve into a\nmissing piece of existing methods: class confusion, the tendency that a\nclassifier confuses the predictions between the correct and ambiguous classes\nfor target examples, which is common in different DA scenarios. We uncover that\nreducing such pairwise class confusion leads to significant transfer gains.\nWith this insight, we propose a general loss function: Minimum Class Confusion\n(MCC). It can be characterized as (1) a non-adversarial DA method without\nexplicitly deploying domain alignment, enjoying faster convergence speed; (2) a\nversatile approach that can handle four existing scenarios: Closed-Set,\nPartial-Set, Multi-Source, and Multi-Target DA, outperforming the\nstate-of-the-art methods in these scenarios, especially on one of the largest\nand hardest datasets to date (7.3% on DomainNet). Its versatility is further\njustified by two scenarios proposed in this paper: Multi-Source Partial DA and\nMulti-Target Partial DA. In addition, it can also be used as a general\nregularizer that is orthogonal and complementary to a variety of existing DA\nmethods, accelerating convergence and pushing these readily competitive methods\nto stronger ones. Code is available at\nhttps://github.com/thuml/Versatile-Domain-Adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:31:14 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 12:41:22 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 15:49:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jin", "Ying", ""], ["Wang", "Ximei", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""]]}, {"id": "1912.03713", "submitter": "Vincent Christlein", "authors": "Vincent Christlein and Anguelos Nicolaou and Mathias Seuret and\n  Dominique Stutzmann and Andreas Maier", "title": "ICDAR 2019 Competition on Image Retrieval for Historical Handwritten\n  Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This competition investigates the performance of large-scale retrieval of\nhistorical document images based on writing style. Based on large image data\nsets provided by cultural heritage institutions and digital libraries,\nproviding a total of 20 000 document images representing about 10 000 writers,\ndivided in three types: writers of (i) manuscript books, (ii) letters, (iii)\ncharters and legal documents. We focus on the task of automatic image retrieval\nto simulate common scenarios of humanities research, such as writer retrieval.\nThe most teams submitted traditional methods not using deep learning\ntechniques. The competition results show that a combination of methods is\noutperforming single methods. Furthermore, letters are much more difficult to\nretrieve than manuscripts.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 16:48:27 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Christlein", "Vincent", ""], ["Nicolaou", "Anguelos", ""], ["Seuret", "Mathias", ""], ["Stutzmann", "Dominique", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.03716", "submitter": "Yunbo Wang", "authors": "Zhiyu Yao, Yunbo Wang, Xingqiang Du, Mingsheng Long, Jianmin Wang", "title": "Adversarial Pyramid Network for Video Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new research problem of video domain generalization\n(video DG) where most state-of-the-art action recognition networks degenerate\ndue to the lack of exposure to the target domains of divergent distributions.\nWhile recent advances in video understanding focus on capturing the temporal\nrelations of the long-term video context, we observe that the global temporal\nfeatures are less generalizable in the video DG settings. The reason is that\nvideos from other unseen domains may have unexpected absence, misalignment, or\nscale transformation of the temporal relations, which is known as the temporal\ndomain shift. Therefore, the video DG is even more challenging than the image\nDG, which is also under-explored, because of the entanglement of the spatial\nand temporal domain shifts.\n  This finding has led us to view the key to video DG as how to effectively\nlearn the local-relation features of different time scales that are more\ngeneralizable, and how to exploit them along with the global-relation features\nto maintain the discriminability. This paper presents the Adversarial Pyramid\nNetwork (APN), which captures the local-relation, global-relation, and\nmultilayer cross-relation features progressively. This pyramid network not only\nimproves the feature transferability from the view of representation learning,\nbut also enhances the diversity and quality of the new data points that can\nbridge different domains when it is integrated with an improved version of the\nimage DG adversarial data augmentation method. We construct four video DG\nbenchmarks: UCF-HMDB, Something-Something, PKU-MMD, and NTU, in which the\nsource and target domains are divided according to different datasets,\ndifferent consequences of actions, or different camera views. The APN\nconsistently outperforms previous action recognition models over all\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:13:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Yao", "Zhiyu", ""], ["Wang", "Yunbo", ""], ["Du", "Xingqiang", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""]]}, {"id": "1912.03730", "submitter": "Fan Yang", "authors": "Fan Yang, Cheng Lu, Yandong Guo, Longin Jan Latecki, Haibin Ling", "title": "Dually Supervised Feature Pyramid for Object Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid architecture has been broadly adopted in object detection and\nsegmentation to deal with multi-scale problem. However, in this paper we show\nthat the capacity of the architecture has not been fully explored due to the\ninadequate utilization of the supervision information. Such insufficient\nutilization is caused by the supervision signal degradation in back\npropagation. Thus inspired, we propose a dually supervised method, named dually\nsupervised FPN (DSFPN), to enhance the supervision signal when training the\nfeature pyramid network (FPN). In particular, DSFPN is constructed by attaching\nextra prediction (i.e., detection or segmentation) heads to the bottom-up\nsubnet of FPN. Hence, the features can be optimized by the additional heads\nbefore being forwarded to subsequent networks. Further, the auxiliary heads can\nserve as a regularization term to facilitate the model training. In addition,\nto strengthen the capability of the detection heads in DSFPN for handling two\ninhomogeneous tasks, i.e., classification and regression, the originally shared\nhidden feature space is separated by decoupling classification and regression\nsubnets. To demonstrate the generalizability, effectiveness, and efficiency of\nthe proposed method, DSFPN is integrated into four representative detectors\n(Faster RCNN, Mask RCNN, Cascade RCNN, and Cascade Mask RCNN) and assessed on\nthe MS COCO dataset. Promising precision improvement, state-of-the-art\nperformance, and negligible additional computational cost are demonstrated\nthrough extensive experiments. Code will be provided.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 18:06:06 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:28:51 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yang", "Fan", ""], ["Lu", "Cheng", ""], ["Guo", "Yandong", ""], ["Latecki", "Longin Jan", ""], ["Ling", "Haibin", ""]]}, {"id": "1912.03737", "submitter": "Tarang Chugh", "authors": "Rohit Gajawada and Additya Popli and Tarang Chugh and Anoop Namboodiri\n  and Anil K. Jain", "title": "Universal Material Translator: Towards Spoof Fingerprint Generalization", "comments": "8 pages, 6 figures, conference", "journal-ref": "IAPR International Conference on Biometrics (ICB), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoof detectors are classifiers that are trained to distinguish spoof\nfingerprints from bonafide ones. However, state of the art spoof detectors do\nnot generalize well on unseen spoof materials. This study proposes a style\ntransfer based augmentation wrapper that can be used on any existing spoof\ndetector and can dynamically improve the robustness of the spoof detection\nsystem on spoof materials for which we have very low data. Our method is an\napproach for synthesizing new spoof images from a few spoof examples that\ntransfers the style or material properties of the spoof examples to the content\nof bonafide fingerprints to generate a larger number of examples to train the\nclassifier on. We demonstrate the effectiveness of our approach on materials in\nthe publicly available LivDet 2015 dataset and show that the proposed approach\nleads to robustness to fingerprint spoofs of the target material.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 18:32:05 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gajawada", "Rohit", ""], ["Popli", "Additya", ""], ["Chugh", "Tarang", ""], ["Namboodiri", "Anoop", ""], ["Jain", "Anil K.", ""]]}, {"id": "1912.03818", "submitter": "Changxu Cheng", "authors": "Changxu Cheng, Qiuhui Huang, Xiang Bai, Bin Feng, Wenyu Liu", "title": "Patch Aggregator for Scene Text Script Identification", "comments": "Accepted as ICDAR2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Script identification in the wild is of great importance in a multi-lingual\nrobust-reading system. The scripts deriving from the same language family share\na large set of characters, which makes script identification a fine-grained\nclassification problem. Most existing methods make efforts to learn a single\nrepresentation that combines the local features by making a weighted average or\nother clustering methods, which may reduce the discriminatory power of some\nimportant parts in each script for the interference of redundant features. In\nthis paper, we present a novel module named Patch Aggregator (PA), which learns\na more discriminative representation for script identification by taking into\naccount the prediction scores of local patches. Specifically, we design a\nCNN-based method consisting of a standard CNN classifier and a PA module.\nExperiments demonstrate that the proposed PA module brings significant\nperformance improvements over the baseline CNN model, achieving the\nstate-of-the-art results on three benchmark datasets for script identification:\nSIW-13, CVSI 2015 and RRC-MLT 2017.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:17:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Cheng", "Changxu", ""], ["Huang", "Qiuhui", ""], ["Bai", "Xiang", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "1912.03825", "submitter": "Ying Wang", "authors": "Ying Wang, Zezhou Sun, Cheng-Zhong Xu, Sanjay Sarma, Jian Yang, Hui\n  Kong", "title": "LiDAR Iris for Loop-Closure Detection", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a global descriptor for a LiDAR point cloud, called LiDAR\nIris, is proposed for fast and accurate loop-closure detection. A binary\nsignature image can be obtained for each point cloud after several LoG-Gabor\nfiltering and thresholding operations on the LiDAR-Iris image representation.\nGiven two point clouds, their similarities can be calculated as the Hamming\ndistance of two corresponding binary signature images extracted from the two\npoint clouds, respectively. Our LiDAR-Iris method can achieve a pose-invariant\nloop-closure detection at a descriptor level with the Fourier transform of the\nLiDAR-Iris representation if assuming a 3D (x,y,yaw) pose space, although our\nmethod can generally be applied to a 6D pose space by re-aligning point clouds\nwith an additional IMU sensor. Experimental results on five road-scene\nsequences demonstrate its excellent performance in loop-closure detection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 03:04:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 02:31:39 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 11:26:42 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Ying", ""], ["Sun", "Zezhou", ""], ["Xu", "Cheng-Zhong", ""], ["Sarma", "Sanjay", ""], ["Yang", "Jian", ""], ["Kong", "Hui", ""]]}, {"id": "1912.03829", "submitter": "Run Wang", "authors": "Run Wang, Felix Juefei-Xu, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma,\n  Yang Liu", "title": "Amora: Black-box Adversarial Morphing Attack", "comments": "Accepted by ACM MM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, digital facial content manipulation has become ubiquitous and\nrealistic with the success of generative adversarial networks (GANs), making\nface recognition (FR) systems suffer from unprecedented security concerns. In\nthis paper, we investigate and introduce a new type of adversarial attack to\nevade FR systems by manipulating facial content, called\n\\textbf{\\underline{a}dversarial \\underline{mor}phing \\underline{a}ttack}\n(a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel\nintensity values by adding human-imperceptible noise, our proposed adversarial\nmorphing attack works at the semantic level that perturbs pixels spatially in a\ncoherent manner. To tackle the black-box attack problem, we devise a simple yet\neffective joint dictionary learning pipeline to obtain a proprietary optical\nflow field for each attack. Our extensive evaluation on two popular FR systems\ndemonstrates the effectiveness of our adversarial morphing attack at various\nlevels of morphing intensity with smiling facial expression manipulations. Both\nopen-set and closed-set experimental results indicate that a novel black-box\nadversarial attack based on local deformation is possible, and is vastly\ndifferent from additive noise attacks. The findings of this work potentially\npave a new research direction towards a more thorough understanding and\ninvestigation of image-based adversarial attacks and defenses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 03:23:36 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 05:26:41 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 06:57:08 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2020 03:58:29 GMT"}, {"version": "v5", "created": "Sat, 15 Aug 2020 13:29:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Run", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Huang", "Yihao", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Liu", "Yang", ""]]}, {"id": "1912.03837", "submitter": "Yuan Xue", "authors": "Yuan Xue, Jiarong Ye, Rodney Long, Sameer Antani, Zhiyun Xue, Xiaolei\n  Huang", "title": "Selective Synthetic Augmentation with Quality Assurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised training of an automated medical image analysis system often\nrequires a large amount of expert annotations that are hard to collect.\nMoreover, the proportions of data available across different classes may be\nhighly imbalanced for rare diseases. To mitigate these issues, we investigate a\nnovel data augmentation pipeline that selectively adds new synthetic images\ngenerated by conditional Adversarial Networks (cGANs), rather than extending\ndirectly the training set with synthetic images. The selection mechanisms that\nwe introduce to the synthetic augmentation pipeline are motivated by the\nobservation that, although cGAN-generated images can be visually appealing,\nthey are not guaranteed to contain essential features for classification\nperformance improvement. By selecting synthetic images based on the confidence\nof their assigned labels and their feature similarity to real labeled images,\nour framework provides quality assurance to synthetic augmentation by ensuring\nthat adding the selected synthetic images to the training set will improve\nperformance. We evaluate our model on a medical histopathology dataset, and two\nnatural image classification benchmarks, CIFAR10 and SVHN. Results on these\ndatasets show significant and consistent improvements in classification\nperformance (with 6.8%, 3.9%, 1.6% higher accuracy, respectively) by leveraging\ncGAN generated images with selective augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 04:08:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Xue", "Yuan", ""], ["Ye", "Jiarong", ""], ["Long", "Rodney", ""], ["Antani", "Sameer", ""], ["Xue", "Zhiyun", ""], ["Huang", "Xiaolei", ""]]}, {"id": "1912.03840", "submitter": "Yuan Xue", "authors": "Yuan Xue, Zihan Zhou, Xiaolei Huang", "title": "Neural Wireframe Renderer: Learning Wireframe to Image Translations", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In architecture and computer-aided design, wireframes (i.e., line-based\nmodels) are widely used as basic 3D models for design evaluation and fast\ndesign iterations. However, unlike a full design file, a wireframe model lacks\ncritical information, such as detailed shape, texture, and materials, needed by\na conventional renderer to produce 2D renderings of the objects or scenes. In\nthis paper, we bridge the information gap by generating photo-realistic\nrendering of indoor scenes from wireframe models in an image translation\nframework. While existing image synthesis methods can generate visually\npleasing images for common objects such as faces and birds, these methods do\nnot explicitly model and preserve essential structural constraints in a\nwireframe model, such as junctions, parallel lines, and planar surfaces. To\nthis end, we propose a novel model based on a structure-appearance joint\nrepresentation learned from both images and wireframes. In our model,\nstructural constraints are explicitly enforced by learning a joint\nrepresentation in a shared encoder network that must support the generation of\nboth images and wireframes. Experiments on a wireframe-scene dataset show that\nour wireframe-to-image translation model significantly outperforms the\nstate-of-the-art methods in both visual quality and structural integrity of\ngenerated images.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 04:17:37 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 04:57:26 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Xue", "Yuan", ""], ["Zhou", "Zihan", ""], ["Huang", "Xiaolei", ""]]}, {"id": "1912.03849", "submitter": "Yuan Xue", "authors": "Yuan Xue, Hui Tang, Zhi Qiao, Guanzhong Gong, Yong Yin, Zhen Qian,\n  Chao Huang, Wei Fan, Xiaolei Huang", "title": "Shape-Aware Organ Segmentation by Predicting Signed Distance Maps", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose to resolve the issue existing in current deep\nlearning based organ segmentation systems that they often produce results that\ndo not capture the overall shape of the target organ and often lack smoothness.\nSince there is a rigorous mapping between the Signed Distance Map (SDM)\ncalculated from object boundary contours and the binary segmentation map, we\nexploit the feasibility of learning the SDM directly from medical scans. By\nconverting the segmentation task into predicting an SDM, we show that our\nproposed method retains superior segmentation performance and has better\nsmoothness and continuity in shape. To leverage the complementary information\nin traditional segmentation training, we introduce an approximated Heaviside\nfunction to train the model by predicting SDMs and segmentation maps\nsimultaneously. We validate our proposed models by conducting extensive\nexperiments on a hippocampus segmentation dataset and the public MICCAI 2015\nHead and Neck Auto Segmentation Challenge dataset with multiple organs. While\nour carefully designed backbone 3D segmentation network improves the Dice\ncoefficient by more than 5% compared to current state-of-the-arts, the proposed\nmodel with SDM learning produces smoother segmentation results with smaller\nHausdorff distance and average surface distance, thus proving the effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 04:52:24 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Xue", "Yuan", ""], ["Tang", "Hui", ""], ["Qiao", "Zhi", ""], ["Gong", "Guanzhong", ""], ["Yin", "Yong", ""], ["Qian", "Zhen", ""], ["Huang", "Chao", ""], ["Fan", "Wei", ""], ["Huang", "Xiaolei", ""]]}, {"id": "1912.03858", "submitter": "Yu Chen", "authors": "Yu Chen and Yisong Chen and Guoping Wang", "title": "Bundle Adjustment Revisited", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction has been developing all these two decades, from moderate to\nmedium size and to large scale. It's well known that bundle adjustment plays an\nimportant role in 3D reconstruction, mainly in Structure from Motion(SfM) and\nSimultaneously Localization and Mapping(SLAM). While bundle adjustment\noptimizes camera parameters and 3D points as a non-negligible final step, it\nsuffers from memory and efficiency requirements in very large scale\nreconstruction. In this paper, we study the development of bundle adjustment\nelaborately in both conventional and distributed approaches. The detailed\nderivation and pseudo code are also given in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 05:47:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Yu", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""]]}, {"id": "1912.03865", "submitter": "Tao Wang", "authors": "Tao Wang, Xuming He, Yuanzheng Cai and Guobao Xiao", "title": "Learning a Layout Transfer Network for Context Aware Object Detection", "comments": "Paper accepted by the IEEE Transactions on Intelligent Transportation\n  Systems", "journal-ref": null, "doi": "10.1109/TITS.2019.2939213", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a context aware object detection method based on a\nretrieve-and-transform scene layout model. Given an input image, our approach\nfirst retrieves a coarse scene layout from a codebook of typical layout\ntemplates. In order to handle large layout variations, we use a variant of the\nspatial transformer network to transform and refine the retrieved layout,\nresulting in a set of interpretable and semantically meaningful feature maps of\nobject locations and scales. The above steps are implemented as a Layout\nTransfer Network which we integrate into Faster RCNN to allow for joint\nreasoning of object detection and scene layout estimation. Extensive\nexperiments on three public datasets verified that our approach provides\nconsistent performance improvements to the state-of-the-art object detection\nbaselines on a variety of challenging tasks in the traffic surveillance and the\nautonomous driving domains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 06:07:44 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Tao", ""], ["He", "Xuming", ""], ["Cai", "Yuanzheng", ""], ["Xiao", "Guobao", ""]]}, {"id": "1912.03874", "submitter": "Robin Heinzler", "authors": "Robin Heinzler, Florian Piewak, Philipp Schindler, Wilhelm Stork", "title": "CNN-based Lidar Point Cloud De-Noising in Adverse Weather", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (2020)", "doi": "10.1109/LRA.2020.2972865", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar sensors are frequently used in environment perception for autonomous\nvehicles and mobile robotics to complement camera, radar, and ultrasonic\nsensors. Adverse weather conditions are significantly impacting the performance\nof lidar-based scene understanding by causing undesired measurement points that\nin turn effect missing detections and false positives. In heavy rain or dense\nfog, water drops could be misinterpreted as objects in front of the vehicle\nwhich brings a mobile robot to a full stop. In this paper, we present the first\nCNN-based approach to understand and filter out such adverse weather effects in\npoint cloud data. Using a large data set obtained in controlled weather\nenvironments, we demonstrate a significant performance improvement of our\nmethod over state-of-the-art involving geometric filtering. Data is available\nat https://github.com/rheinzler/PointCloudDeNoising.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:00:33 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 05:32:07 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Heinzler", "Robin", ""], ["Piewak", "Florian", ""], ["Schindler", "Philipp", ""], ["Stork", "Wilhelm", ""]]}, {"id": "1912.03877", "submitter": "Xu Shibing", "authors": "Shibing Xu, Zishu Gao and Guojun Xie", "title": "Bi-Semantic Reconstructing Generative Network for Zero-shot Learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent methods of zero-shot learning (ZSL) attempt to utilize generative\nmodel to generate the unseen visual samples from semantic descriptions and\nrandom noise. Therefore, the ZSL problem becomes a traditional supervised\nclassification problem. However, most of the existing methods based on the\ngenerative model only focus on the quality of synthesized samples at the\ntraining stage, and ignore the importance of the zero-shot recognition stage.\nIn this paper, we consider both the above two points and propose a novel\napproach. Specially, we select the Generative Adversarial Network (GAN) as our\ngenerative model. In order to improve the quality of synthesized samples,\nconsidering the internal relation of the semantic description in the semantic\nspace as well as the fact that the seen and unseen visual information belong to\ndifferent domains, we propose a bi-semantic reconstructing (BSR) component\nwhich contain two different semantic reconstructing regressors to lead the\ntraining of GAN. Since the semantic descriptions are available during the\ntraining stage, to further improve the ability of classifier, we combine the\nvisual samples and semantic descriptions to train a classifier. At the\nrecognition stage, we naturally utilize the BSR component to transfer the\nvisual features and semantic descriptions, and concatenate them for\nclassification. Experimental results show that our method outperforms the state\nof the art on several ZSL benchmark datasets with significant improvements.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:12:18 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 07:48:36 GMT"}, {"version": "v3", "created": "Sun, 5 Jan 2020 16:47:28 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Xu", "Shibing", ""], ["Gao", "Zishu", ""], ["Xie", "Guojun", ""]]}, {"id": "1912.03879", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala and Malihe Alikhani and Jonas Haverinen and Timo\n  Kalliokoski and Evanfiya Logacheva and Serafina Orekhova and Aino Tuomainen\n  and Matthew Stone and John A. Bateman", "title": "AI2D-RST: A multimodal corpus of 1000 primary school science diagrams", "comments": "24 pages; revised version submitted to Language Resources &\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces AI2D-RST, a multimodal corpus of 1000\nEnglish-language diagrams that represent topics in primary school natural\nsciences, such as food webs, life cycles, moon phases and human physiology. The\ncorpus is based on the Allen Institute for Artificial Intelligence Diagrams\n(AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which\nwas originally developed to support research on automatic diagram understanding\nand visual question answering. Building on the segmentation of diagram layouts\nin AI2D, the AI2D-RST corpus presents a new multi-layer annotation schema that\nprovides a rich description of their multimodal structure. Annotated by trained\nexperts, the layers describe (1) the grouping of diagram elements into\nperceptual units, (2) the connections set up by diagrammatic elements such as\narrows and lines, and (3) the discourse relations between diagram elements,\nwhich are described using Rhetorical Structure Theory (RST). Each annotation\nlayer in AI2D-RST is represented using a graph. The corpus is freely available\nfor research and teaching.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:22:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:03:17 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Hiippala", "Tuomo", ""], ["Alikhani", "Malihe", ""], ["Haverinen", "Jonas", ""], ["Kalliokoski", "Timo", ""], ["Logacheva", "Evanfiya", ""], ["Orekhova", "Serafina", ""], ["Tuomainen", "Aino", ""], ["Stone", "Matthew", ""], ["Bateman", "John A.", ""]]}, {"id": "1912.03880", "submitter": "Takuya Ohashi", "authors": "Takuya Ohashi, Yosuke Ikegami, Kazuki Yamamoto, Wataru Takano,\n  Yoshihiko Nakamura", "title": "Video Motion Capture from the Part Confidence Maps of Multi-Camera\n  Images by Spatiotemporal Filtering Using the Human Skeletal Model", "comments": "International Conference on Intelligent Robots and Systems (IROS),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses video motion capture, namely, 3D reconstruction of human\nmotion from multi-camera images. After the Part Confidence Maps are computed\nfrom each camera image, the proposed spatiotemporal filter is applied to\ndeliver the human motion data with accuracy and smoothness for human motion\nanalysis. The spatiotemporal filter uses the human skeleton and mixes temporal\nsmoothing in two-time inverse kinematics computations. The experimental results\nshow that the mean per joint position error was 26.1mm for regular motions and\n38.8mm for inverted motions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:25:33 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:56:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Ohashi", "Takuya", ""], ["Ikegami", "Yosuke", ""], ["Yamamoto", "Kazuki", ""], ["Takano", "Wataru", ""], ["Nakamura", "Yoshihiko", ""]]}, {"id": "1912.03966", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, Christopher Yeh, Stefano Ermon", "title": "Efficient Object Detection in Large Images using Deep Reinforcement\n  Learning", "comments": "Published in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, an object detector is applied to every part of the scene of\ninterest, and its accuracy and computational cost increases with higher\nresolution images. However, in some application domains such as remote sensing,\npurchasing high spatial resolution images is expensive. To reduce the large\ncomputational and monetary cost associated with using high spatial resolution\nimages, we propose a reinforcement learning agent that adaptively selects the\nspatial resolution of each image that is provided to the detector. In\nparticular, we train the agent in a dual reward setting to choose low spatial\nresolution images to be run through a coarse level detector when the image is\ndominated by large objects, and high spatial resolution images to be run\nthrough a fine level detector when it is dominated by small objects. This\nreduces the dependency on high spatial resolution images for building a robust\ndetector and increases run-time efficiency. We perform experiments on the xView\ndataset, consisting of large images, where we increase run-time efficiency by\n50% and use high resolution images only 30% of the time while maintaining\nsimilar accuracy as a detector that uses only high resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:10:55 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 21:01:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Uzkent", "Burak", ""], ["Yeh", "Christopher", ""], ["Ermon", "Stefano", ""]]}, {"id": "1912.03978", "submitter": "Tan Nguyen", "authors": "Tan M. Nguyen, Animesh Garg, Richard G. Baraniuk, Anima Anandkumar", "title": "InfoCNF: An Efficient Conditional Continuous Normalizing Flow with\n  Adaptive Solvers", "comments": "17 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous Normalizing Flows (CNFs) have emerged as promising deep generative\nmodels for a wide range of tasks thanks to their invertibility and exact\nlikelihood estimation. However, conditioning CNFs on signals of interest for\nconditional image generation and downstream predictive tasks is inefficient due\nto the high-dimensional latent code generated by the model, which needs to be\nof the same size as the input data. In this paper, we propose InfoCNF, an\nefficient conditional CNF that partitions the latent space into a\nclass-specific supervised code and an unsupervised code that shared among all\nclasses for efficient use of labeled information. Since the partitioning\nstrategy (slightly) increases the number of function evaluations (NFEs),\nInfoCNF also employs gating networks to learn the error tolerances of its\nordinary differential equation (ODE) solvers for better speed and performance.\nWe show empirically that InfoCNF improves the test accuracy over the baseline\nwhile yielding comparable likelihood scores and reducing the NFEs on CIFAR10.\nFurthermore, applying the same partitioning strategy in InfoCNF on time-series\ndata helps improve extrapolation performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:37:22 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nguyen", "Tan M.", ""], ["Garg", "Animesh", ""], ["Baraniuk", "Richard G.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1912.03991", "submitter": "Chenying Liu", "authors": "Chenying Liu and Jun Li and Lin He and Antonio J. Plaza and Shutao Li\n  and Bo Li", "title": "Naive Gabor Networks for Hyperspectral Image Classification", "comments": "This paper has been accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many convolutional neural network (CNN) methods have been designed\nfor hyperspectral image (HSI) classification since CNNs are able to produce\ngood representations of data, which greatly benefits from a huge number of\nparameters. However, solving such a high-dimensional optimization problem often\nrequires a large amount of training samples in order to avoid overfitting.\nAdditionally, it is a typical non-convex problem affected by many local minima\nand flat regions. To address these problems, in this paper, we introduce naive\nGabor Networks or Gabor-Nets which, for the first time in the literature,\ndesign and learn CNN kernels strictly in the form of Gabor filters, aiming to\nreduce the number of involved parameters and constrain the solution space, and\nhence improve the performances of CNNs. Specifically, we develop an innovative\nphase-induced Gabor kernel, which is trickily designed to perform the Gabor\nfeature learning via a linear combination of local low-frequency and\nhigh-frequency components of data controlled by the kernel phase. With the\nphase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to\nautomatically adapt to the local harmonic characteristics of the HSI data and\nthus yields more representative harmonic features. Also, this kernel can\nfulfill the traditional complex-valued Gabor filtering in a real-valued manner,\nhence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our\nnewly developed Gabor-Nets on three well-known HSIs, suggesting that our\nproposed Gabor-Nets can significantly improve the performance of CNNs,\nparticularly with a small training set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:16:48 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 06:21:41 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Liu", "Chenying", ""], ["Li", "Jun", ""], ["He", "Lin", ""], ["Plaza", "Antonio J.", ""], ["Li", "Shutao", ""], ["Li", "Bo", ""]]}, {"id": "1912.03992", "submitter": "Lucas Matias", "authors": "Lucas P. N. Matias, Jefferson R. Souza, Denis F. Wolf", "title": "Environment reconstruction on depth images using Generative Adversarial\n  Networks", "comments": "12 pages; 10 figures; open sourced; code and demo available in\n  https://github.com/nuneslu/VeIGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust perception systems are essential for autonomous vehicle safety. To\nnavigate in a complex urban environment, it is necessary precise sensors with\nreliable data. The task of understanding the surroundings is hard by itself;\nfor intelligent vehicles, it is even more critical due to the high speed in\nwhich the vehicle navigates. To successfully navigate in an urban environment,\nthe perception system must quickly receive, process, and execute an action to\nguarantee both passenger and pedestrian safety. Stereo cameras collect\nenvironment information at many levels, e.g., depth, color, texture, shape,\nwhich guarantee ample knowledge about the surroundings. Even so, when compared\nto human, computational methods lack the ability to deal with missing\ninformation, i.e., occlusions. For many perception tasks, this lack of data can\nbe a hindrance due to the environment incomplete information. In this paper, we\naddress this problem and discuss recent methods to deal with occluded areas\ninference. We then introduce a loss function focused on disparity and\nenvironment depth data reconstruction, and a Generative Adversarial Network\n(GAN) architecture able to deal with occluded information inference. Our\nresults present a coherent reconstruction on depth maps, estimating regions\noccluded by different obstacles. Our final contribution is a loss function\nfocused on disparity data and a GAN able to extract depth features and estimate\ndepth data by inpainting disparity images.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:18:45 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Matias", "Lucas P. N.", ""], ["Souza", "Jefferson R.", ""], ["Wolf", "Denis F.", ""]]}, {"id": "1912.04016", "submitter": "Du Chen", "authors": "Du Chen, Zewei He, Yanpeng Cao, Jiangxin Yang, Yanlong Cao, Michael\n  Ying Yang, Siliang Tang and Yueting Zhuang", "title": "Deep Neural Network for Fast and Accurate Single Image Super-Resolution\n  via Channel-Attention-based Fusion of Orientation-aware Features", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have been successfully adopted\nto solve the ill-posed single image super-resolution (SISR) problem. A commonly\nused strategy to boost the performance of CNN-based SISR models is deploying\nvery deep networks, which inevitably incurs many obvious drawbacks (e.g., a\nlarge number of network parameters, heavy computational loads, and difficult\nmodel training). In this paper, we aim to build more accurate and faster SISR\nmodels via developing better-performing feature extraction and fusion\ntechniques. Firstly, we proposed a novel Orientation-Aware feature extraction\nand fusion Module (OAM), which contains a mixture of 1D and 2D convolutional\nkernels (i.e., 5 x 1, 1 x 5, and 3 x 3) for extracting orientation-aware\nfeatures. Secondly, we adopt the channel attention mechanism as an effective\ntechnique to adaptively fuse features extracted in different directions and in\nhierarchically stacked convolutional stages. Based on these two important\nimprovements, we present a compact but powerful CNN-based model for\nhigh-quality SISR via Channel Attention-based fusion of Orientation-Aware\nfeatures (SISR-CA-OA). Extensive experimental results verify the superiority of\nthe proposed SISR-CA-OA model, performing favorably against the\nstate-of-the-art SISR models in terms of both restoration accuracy and\ncomputational efficiency. The source codes will be made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:18:58 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Du", ""], ["He", "Zewei", ""], ["Cao", "Yanpeng", ""], ["Yang", "Jiangxin", ""], ["Cao", "Yanlong", ""], ["Yang", "Michael Ying", ""], ["Tang", "Siliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1912.04022", "submitter": "J\\\"org Schl\\\"otterer", "authors": "Christian Reiser and J\\\"org Schl\\\"otterer and Michael Granitzer", "title": "Parallel Total Variation Distance Estimation with Neural Networks for\n  Merging Over-Clusterings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the initial situation where a dataset has been over-partitioned\ninto $k$ clusters and seek a domain independent way to merge those initial\nclusters. We identify the total variation distance (TVD) as suitable for this\ngoal. By exploiting the relation of the TVD to the Bayes accuracy we show how\nneural networks can be used to estimate TVDs between all pairs of clusters in\nparallel. Crucially, the needed memory space is decreased by reducing the\nrequired number of output neurons from $k^2$ to $k$. On realistically obtained\nover-clusterings of ImageNet subsets it is demonstrated that our TVD estimates\nlead to better merge decisions than those obtained by relying on\nstate-of-the-art unsupervised representations. Further the generality of the\napproach is verified by evaluating it on a a point cloud dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:25:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Reiser", "Christian", ""], ["Schl\u00f6tterer", "J\u00f6rg", ""], ["Granitzer", "Michael", ""]]}, {"id": "1912.04023", "submitter": "Anil Baslamisli", "authors": "Anil S. Baslamisli, Partha Das, Hoang-An Le, Sezer Karaoglu, Theo\n  Gevers", "title": "ShadingNet: Image Intrinsics by Fine-Grained Shading Decomposition", "comments": "Submitted to International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, intrinsic image decomposition algorithms interpret shading as one\nunified component including all photometric effects. As shading transitions are\ngenerally smoother than reflectance (albedo) changes, these methods may fail in\ndistinguishing strong photometric effects from reflectance variations.\nTherefore, in this paper, we propose to decompose the shading component into\ndirect (illumination) and indirect shading (ambient light and shadows)\nsubcomponents. The aim is to distinguish strong photometric effects from\nreflectance variations. An end-to-end deep convolutional neural network\n(ShadingNet) is proposed that operates in a fine-to-coarse manner with a\nspecialized fusion and refinement unit exploiting the fine-grained shading\nmodel. It is designed to learn specific reflectance cues separated from\nspecific photometric effects to analyze the disentanglement capability. A\nlarge-scale dataset of scene-level synthetic images of outdoor natural\nenvironments is provided with fine-grained intrinsic image ground-truths. Large\nscale experiments show that our approach using fine-grained shading\ndecompositions outperforms state-of-the-art algorithms utilizing unified\nshading on NED, MPI Sintel, GTA V, IIW, MIT Intrinsic Images, 3DRMS and SRD\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:30:11 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 15:53:23 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 16:50:00 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Baslamisli", "Anil S.", ""], ["Das", "Partha", ""], ["Le", "Hoang-An", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "1912.04070", "submitter": "G\u00fcl Varol", "authors": "G\\\"ul Varol, Ivan Laptev, Cordelia Schmid, Andrew Zisserman", "title": "Synthetic Humans for Action Recognition from Unseen Viewpoints", "comments": "21 pages", "journal-ref": "International Journal of Computer Vision (2021)", "doi": "10.1007/s11263-021-01467-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although synthetic training data has been shown to be beneficial for tasks\nsuch as human pose estimation, its use for RGB human action recognition is\nrelatively unexplored. Our goal in this work is to answer the question whether\nsynthetic humans can improve the performance of human action recognition, with\na particular focus on generalization to unseen viewpoints. We make use of the\nrecent advances in monocular 3D human body reconstruction from real action\nsequences to automatically render synthetic training videos for the action\nlabels. We make the following contributions: (i) we investigate the extent of\nvariations and augmentations that are beneficial to improving performance at\nnew viewpoints. We consider changes in body shape and clothing for individuals,\nas well as more action relevant augmentations such as non-uniform frame\nsampling, and interpolating between the motion of individuals performing the\nsame action; (ii) We introduce a new data generation methodology, SURREACT,\nthat allows training of spatio-temporal CNNs for action classification; (iii)\nWe substantially improve the state-of-the-art action recognition performance on\nthe NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally,\n(iv) we extend the augmentation approach to in-the-wild videos from a subset of\nthe Kinetics dataset to investigate the case when only one-shot training data\nis available, and demonstrate improvements in this case as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:17:03 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 14:14:16 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 14:08:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Varol", "G\u00fcl", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1912.04071", "submitter": "Fuyang Huang", "authors": "Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, Qiang Xu", "title": "DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation\n  from Multi-View Image", "comments": null, "journal-ref": "WACV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-stage fully 3D network, namely\n\\textbf{DeepFuse}, to estimate human pose in 3D space by fusing body-worn\nInertial Measurement Unit (IMU) data and multi-view images deeply. The first\nstage is designed for pure vision estimation. To preserve data primitiveness of\nmulti-view inputs, the vision stage uses multi-channel volume as data\nrepresentation and 3D soft-argmax as activation layer. The second one is the\nIMU refinement stage which introduces an IMU-bone layer to fuse the IMU and\nvision data earlier at data level. without requiring a given skeleton model a\npriori, we can achieve a mean joint error of $28.9$mm on TotalCapture dataset\nand $13.4$mm on Human3.6M dataset under protocol 1, improving the SOTA result\nby a large margin. Finally, we discuss the effectiveness of a fully 3D network\nfor 3D pose estimation experimentally which may benefit future research.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:17:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Huang", "Fuyang", ""], ["Zeng", "Ailing", ""], ["Liu", "Minhao", ""], ["Lai", "Qiuxia", ""], ["Xu", "Qiang", ""]]}, {"id": "1912.04134", "submitter": "Christoph Dalitz", "authors": "Regina Pohle-Fr\\\"ohlich, Christoph Dalitz, Charlotte Richter, Benjamin\n  St\\\"audle, Kirsten Albracht", "title": "Estimation of Muscle Fascicle Orientation in Ultrasonic Images", "comments": "7 pages, 7 figures, accepted for VISAPP 2020", "journal-ref": "International Conference on Computer Vision Theory and\n  Applications (VISAPP), pp. 79-86 (2020)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare four different algorithms for automatically estimating the muscle\nfascicle angle from ultrasonic images: the vesselness filter, the Radon\ntransform, the projection profile method and the gray level cooccurence matrix\n(GLCM). The algorithm results are compared to ground truth data generated by\nthree different experts on 425 image frames from two videos recorded during\ndifferent types of motion. The best agreement with the ground truth data was\nachieved by a combination of pre-processing with a vesselness filter and\nmeasuring the angle with the projection profile method. The robustness of the\nestimation is increased by applying the algorithms to subregions with high\ngradients and performing a LOESS fit through these estimates.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:46:38 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Pohle-Fr\u00f6hlich", "Regina", ""], ["Dalitz", "Christoph", ""], ["Richter", "Charlotte", ""], ["St\u00e4udle", "Benjamin", ""], ["Albracht", "Kirsten", ""]]}, {"id": "1912.04158", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Niloy J. Mitra, Tobias Ritschel", "title": "Learning a Neural 3D Texture Space from 2D Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model of 2D and 3D natural textures with diversity,\nvisual fidelity and at high computational efficiency. This is enabled by a\nfamily of methods that extend ideas from classic stochastic procedural\ntexturing (Perlin noise) to learned, deep, non-linearities. The key idea is a\nhard-coded, tunable and differentiable step that feeds multiple transformed\nrandom 2D or 3D fields into an MLP that can be sampled over infinite domains.\nOur model encodes all exemplars from a diverse set of textures without a need\nto be re-trained for each exemplar. Applications include texture interpolation,\nand learning 3D textures from 2D exemplars.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:35:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:26:48 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Henzler", "Philipp", ""], ["Mitra", "Niloy J.", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1912.04216", "submitter": "Ilya Kavalerov", "authors": "Ilya Kavalerov, Wojciech Czaja, Rama Chellappa", "title": "cGANs with Multi-Hinge Loss", "comments": "Accepted to Winter Conference on Applications of Computer Vision\n  (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to incorporate class conditional information into\nthe critic of GANs via a multi-class generalization of the commonly used Hinge\nloss that is compatible with both supervised and semi-supervised settings. We\nstudy the compromise between training a state of the art generator and an\naccurate classifier simultaneously, and propose a way to use our algorithm to\nmeasure the degree to which a generator and critic are class conditional. We\nshow the trade-off between a generator-critic pair respecting class\nconditioning inputs and generating the highest quality images. With our\nmulti-hinge loss modification we are able to improve Inception Scores and\nFrechet Inception Distance on the Imagenet dataset. We make our tensorflow code\navailable at https://github.com/ilyakava/gan.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 17:51:50 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 21:01:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kavalerov", "Ilya", ""], ["Czaja", "Wojciech", ""], ["Chellappa", "Rama", ""]]}, {"id": "1912.04217", "submitter": "Tom White", "authors": "Tom White", "title": "Shared Visual Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents abstract art created by neural networks and broadly\nrecognizable across various computer vision systems. The existence of abstract\nforms that trigger specific labels independent of neural architecture or\ntraining set suggests convolutional neural networks build shared visual\nrepresentations for the categories they understand. Computer vision classifiers\nencountering these drawings often respond with strong responses for specific\nlabels - in extreme cases stronger than all examples from the validation set.\nBy surveying human subjects we confirm that these abstract artworks are also\nbroadly recognizable by people, suggesting visual representations triggered by\nthese drawings are shared across human and computer vision systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 18:51:02 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["White", "Tom", ""]]}, {"id": "1912.04218", "submitter": "Hideaki Hayashi D.Eng.", "authors": "Hideaki Hayashi, Taro Shibanoki and Toshio Tsuji", "title": "A Neural Network Based on the Johnson $S_\\mathrm{U}$ Translation System\n  and Related Application to Electromyogram Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromyogram (EMG) classification is a key technique in EMG-based control\nsystems. The existing EMG classification methods do not consider the\ncharacteristics of EMG features that the distribution has skewness and\nkurtosis, causing drawbacks such as the requirement of hyperparameter tuning.\nIn this paper, we propose a neural network based on the Johnson $S_\\mathrm{U}$\ntranslation system that is capable of representing distributions with skewness\nand kurtosis. The Johnson system is a normalizing translation that transforms\nnon-normal data to a normal distribution, thereby enabling the representation\nof a wide range of distributions. In this study, a discriminative model based\non the multivariate Johnson $S_\\mathrm{U}$ translation system is transformed\ninto a linear combination of coefficients and input vectors using\nlog-linearization. This is then incorporated into a neural network structure,\nthereby allowing the calculation of the posterior probability of the input\nvectors for each class and the determination of model parameters as weight\ncoefficients of the network. The uniqueness of convergence of the network\nlearning is theoretically guaranteed. In the experiments, the suitability of\nthe proposed network for distributions including skewness and kurtosis is\nevaluated using artificially generated data. Its applicability for real\nbiological data is also evaluated via an EMG classification experiment. The\nresults show that the proposed network achieves high classification performance\nwithout the need for hyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 10:28:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hayashi", "Hideaki", ""], ["Shibanoki", "Taro", ""], ["Tsuji", "Toshio", ""]]}, {"id": "1912.04219", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Jin Chao, Jestine Paul, Yiqun Li, Ma Tin Lay\n  Nwe, Shudong Xie, Ashish James, Arulmurugan Ambikapathi, Zeng Zeng, and Vijay\n  Ramaseshan Chandrasekhar", "title": "FaultNet: Faulty Rail-Valves Detection using Deep Learning and Computer\n  Vision", "comments": "8 pages, 8 figures, ITSC 2019", "journal-ref": "IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE - ITSC 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular inspection of rail valves and engines is an important task to ensure\nthe safety and efficiency of railway networks around the globe. Over the past\ndecade, computer vision and pattern recognition based techniques have gained\ntraction for such inspection and defect detection tasks. An automated\nend-to-end trained system can potentially provide a low-cost, high throughput,\nand cheap alternative to manual visual inspection of these components. However,\nsuch systems require a huge amount of defective images for networks to\nunderstand complex defects. In this paper, a multi-phase deep learning based\ntechnique is proposed to perform accurate fault detection of rail-valves. Our\napproach uses a two-step method to perform high precision image segmentation of\nrail-valves resulting in pixel-wise accurate segmentation. Thereafter, a\ncomputer vision technique is used to identify faulty valves. We demonstrate\nthat the proposed approach results in improved detection performance when\ncompared to current state-of-theart techniques used in fault detection.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:33:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Chao", "Jin", ""], ["Paul", "Jestine", ""], ["Li", "Yiqun", ""], ["Nwe", "Ma Tin Lay", ""], ["Xie", "Shudong", ""], ["James", "Ashish", ""], ["Ambikapathi", "Arulmurugan", ""], ["Zeng", "Zeng", ""], ["Chandrasekhar", "Vijay Ramaseshan", ""]]}, {"id": "1912.04229", "submitter": "Indra Deep Mastan", "authors": "Indra Deep Mastan and Shanmuganathan Raman", "title": "DCIL: Deep Contextual Internal Learning for Image Restoration and Image\n  Retargeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is a vast interest in developing methods which are\nindependent of the training samples such as deep image prior, zero-shot\nlearning, and internal learning. The methods above are based on the common goal\nof maximizing image features learning from a single image despite inherent\ntechnical diversity. In this work, we bridge the gap between the various\nunsupervised approaches above and propose a general framework for image\nrestoration and image retargeting. We use contextual feature learning and\ninternal learning to improvise the structure similarity between the source and\nthe target images. We perform image resize application in the following setups:\nclassical image resize using super-resolution, a challenging image resize where\nthe low-resolution image contains noise, and content-aware image resize using\nimage retargeting. We also provide comparisons to the relevant state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:12:49 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1912.04250", "submitter": "Qi Dai", "authors": "Qi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool,\n  Konrad Schindler", "title": "Self-supervised Object Motion and Depth Estimation from Video", "comments": "Camera Ready Version for CVPRW, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised learning framework to estimate the individual\nobject motion and monocular depth from video. We model the object motion as a 6\ndegree-of-freedom rigid-body transformation. The instance segmentation mask is\nleveraged to introduce the information of object. Compared with methods which\npredict dense optical flow map to model the motion, our approach significantly\nreduces the number of values to be estimated. Our system eliminates the scale\nambiguity of motion prediction through imposing a novel geometric constraint\nloss term. Experiments on KITTI driving dataset demonstrate our system is\ncapable to capture the object motion without external annotation. Our system\noutperforms previous self-supervised approaches in terms of 3D scene flow\nprediction, and contribute to the disparity prediction in dynamic area.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:40:47 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:20:11 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Dai", "Qi", ""], ["Patil", "Vaishakh", ""], ["Hecker", "Simon", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""], ["Schindler", "Konrad", ""]]}, {"id": "1912.04251", "submitter": "Naoufel Werghi Dr.", "authors": "Taimur Hassan, Salman H. Khan, Samet Akcay, Mohammed Bennamoun,\n  Naoufel Werghi", "title": "Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from Multi-Vendor X-ray Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, luggage scanning has globally become one of the\nprime aviation security concerns. Manual screening of the baggage items is a\ncumbersome, subjective and inefficient process. Hence, many researchers have\ndeveloped Xray imagery-based autonomous systems to address these shortcomings.\nHowever, to the best of our knowledge, there is no framework, up to now, that\ncan recognize heavily occluded and cluttered baggage items from multi-vendor\nX-ray scans. This paper presents a cascaded structure tensor framework which\ncan automatically extract and recognize suspicious items irrespective of their\nposition and orientation in the multi-vendor X-ray scans. The proposed\nframework is unique, as it intelligently extracts each object by iteratively\npicking contour based transitional information from different orientations and\nuses only a single feedforward convolutional neural network for the\nrecognition. The proposed framework has been rigorously tested on publicly\navailable GDXray and SIXray datasets containing a total of 1,067,381 X-ray\nscans where it significantly outperformed the state-of-the-art solutions by\nachieving the mean average precision score of 0.9343 and 0.9595 for extracting\nand recognizing suspicious items from GDXray and SIXray scans, respectively.\nFurthermore, the proposed framework has achieved 15.78% better time\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:40:47 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 05:28:20 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hassan", "Taimur", ""], ["Khan", "Salman H.", ""], ["Akcay", "Samet", ""], ["Bennamoun", "Mohammed", ""], ["Werghi", "Naoufel", ""]]}, {"id": "1912.04259", "submitter": "Moein Hasani", "authors": "Moein Hasani, Hassan Khotanlou", "title": "An Empirical Study on Position of the Batch Normalization Layer in\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICSPIS48872.2019.9066113", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have studied how the training of the convolutional neural\nnetworks (CNNs) can be affected by changing the position of the batch\nnormalization (BN) layer. Three different convolutional neural networks have\nbeen chosen for our experiments. These networks are AlexNet, VGG-16, and\nResNet- 20. We show that the speed up in training provided by the BN algorithm\ncan be improved by using other positions for the BN layer than the one\nsuggested by its original paper. Also, we discuss how the BN layer in a certain\nposition can aid the training of one network but not the other. Three different\npositions for the BN layer have been studied in this research. These positions\nare: the BN layer between the convolution layer and the non-linear activation\nfunction, the BN layer after the non-linear activation function and finally,\nthe BN layer before each of the convolutional layers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:52:30 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 07:19:58 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 05:00:20 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hasani", "Moein", ""], ["Khotanlou", "Hassan", ""]]}, {"id": "1912.04260", "submitter": "Jiaqi Wang", "authors": "Jiaqi Wang, Wenwei Zhang, Yuhang Cao, Kai Chen, Jiangmiao Pang, Tao\n  Gong, Jianping Shi, Chen Change Loy, Dahua Lin", "title": "Side-Aware Boundary Localization for More Precise Object Detection", "comments": "ECCV 2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current object detection frameworks mainly rely on bounding box regression to\nlocalize objects. Despite the remarkable progress in recent years, the\nprecision of bounding box regression remains unsatisfactory, hence limiting\nperformance in object detection. We observe that precise localization requires\ncareful placement of each side of the bounding box. However, the mainstream\napproach, which focuses on predicting centers and sizes, is not the most\neffective way to accomplish this task, especially when there exists\ndisplacements with large variance between the anchors and the targets. In this\npaper, we propose an alternative approach, named as Side-Aware Boundary\nLocalization (SABL), where each side of the bounding box is respectively\nlocalized with a dedicated network branch. To tackle the difficulty of precise\nlocalization in the presence of displacements with large variance, we further\npropose a two-step localization scheme, which first predicts a range of\nmovement through bucket prediction and then pinpoints the precise position\nwithin the predicted bucket. We test the proposed method on both two-stage and\nsingle-stage detection frameworks. Replacing the standard bounding box\nregression branch with the proposed design leads to significant improvements on\nFaster R-CNN, RetinaNet, and Cascade R-CNN, by 3.0%, 1.7%, and 0.9%,\nrespectively. Code is available at https://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:54:05 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 13:03:16 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Wang", "Jiaqi", ""], ["Zhang", "Wenwei", ""], ["Cao", "Yuhang", ""], ["Chen", "Kai", ""], ["Pang", "Jiangmiao", ""], ["Gong", "Tao", ""], ["Shi", "Jianping", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1912.04278", "submitter": "Huidong Xie", "authors": "Huidong Xie, Hongming Shan, Wenxiang Cong, Chi Liu, Xiaohua Zhang,\n  Shaohua Liu, Ruola Ning, Ge Wang", "title": "Deep Efficient End-to-end Reconstruction (DEER) Network for Few-view\n  Breast CT Image Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3033795", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast CT provides image volumes with isotropic resolution in high contrast,\nenabling detection of small calcification (down to a few hundred microns in\nsize) and subtle density differences. Since breast is sensitive to x-ray\nradiation, dose reduction of breast CT is an important topic, and for this\npurpose, few-view scanning is a main approach. In this article, we propose a\nDeep Efficient End-to-end Reconstruction (DEER) network for few-view breast CT\nimage reconstruction. The major merits of our network include high dose\nefficiency, excellent image quality, and low model complexity. By the design,\nthe proposed network can learn the reconstruction process with as few as O(N)\nparameters, where N is the side length of an image to be reconstructed, which\nrepresents orders of magnitude improvements relative to the state-of-the-art\ndeep-learning-based reconstruction methods that map raw data to tomographic\nimages directly. Also, validated on a cone-beam breast CT dataset prepared by\nKoning Corporation on a commercial scanner, our method demonstrates a\ncompetitive performance over the state-of-the-art reconstruction networks in\nterms of image quality. The source code of this paper is available at:\nhttps://github.com/HuidongXie/DEER.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:44:24 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 11:35:10 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 21:00:05 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Xie", "Huidong", ""], ["Shan", "Hongming", ""], ["Cong", "Wenxiang", ""], ["Liu", "Chi", ""], ["Zhang", "Xiaohua", ""], ["Liu", "Shaohua", ""], ["Ning", "Ruola", ""], ["Wang", "Ge", ""]]}, {"id": "1912.04302", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Michael Zollh\\\"ofer, Christian Theobalt,\n  Matthias Nie{\\ss}ner", "title": "DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised\n  Data", "comments": "Video: https://youtu.be/OrHLacCDZVQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying data-driven approaches to non-rigid 3D reconstruction has been\ndifficult, which we believe can be attributed to the lack of a large-scale\ntraining corpus. Unfortunately, this method fails for important cases such as\nhighly non-rigid deformations. We first address this problem of lack of data by\nintroducing a novel semi-supervised strategy to obtain dense inter-frame\ncorrespondences from a sparse set of annotations. This way, we obtain a large\ndataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned\nframe pairs; in addition, we provide a test set along with several metrics for\nevaluation. Based on this corpus, we introduce a data-driven non-rigid feature\nmatching approach, which we integrate into an optimization-based reconstruction\npipeline. Here, we propose a new neural network that operates on RGB-D frames,\nwhile maintaining robustness under large non-rigid deformations and producing\naccurate predictions. Our approach significantly outperforms existing non-rigid\nreconstruction methods that do not use learned data terms, as well as\nlearning-based approaches that only use self-supervision.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:00:04 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 19:00:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.04316", "submitter": "Matteo Tomei", "authors": "Matteo Tomei, Lorenzo Baraldi, Simone Calderara, Simone Bronzin, Rita\n  Cucchiara", "title": "Video action detection by learning graph-based spatio-temporal\n  interactions", "comments": "This is the authors version of an article accepted for publication in\n  Computer Vision and Image Understanding (CVIU), available online February\n  2021", "journal-ref": "Computer Vision and Image Understanding (CVIU), 2021", "doi": "10.1016/j.cviu.2021.103187", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action Detection is a complex task that aims to detect and classify human\nactions in video clips. Typically, it has been addressed by processing\nfine-grained features extracted from a video classification backbone. Recently,\nthanks to the robustness of object and people detectors, a deeper focus has\nbeen added on relationship modelling. Following this line, we propose a\ngraph-based framework to learn high-level interactions between people and\nobjects, in both space and time. In our formulation, spatio-temporal\nrelationships are learned through self-attention on a multi-layer graph\nstructure which can connect entities from consecutive clips, thus considering\nlong-range spatial and temporal dependencies. The proposed module is backbone\nindependent by design and does not require end-to-end training. Extensive\nexperiments are conducted on the AVA dataset, where our model demonstrates\nstate-of-the-art results and consistent improvements over baselines built with\ndifferent backbones. Code is publicly available at\nhttps://github.com/aimagelab/STAGE_action_detection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:01:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 14:46:59 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 10:37:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tomei", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Calderara", "Simone", ""], ["Bronzin", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1912.04333", "submitter": "Daniel Mohr", "authors": "Daniel P. Mohr, Peter Huber, Mierk Schwabe, Christina A. Knapek", "title": "3D Particle Positions from Computer Stereo Vision in PK-4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex plasmas consist of microparticles embedded in a low-temperature\nplasma containing ions, electrons and neutral particles. The microparticles\nform a dynamical system that can be used to study a multitude of effects on the\nlevel of the constituent particles. The microparticles are usually illuminated\nwith a sheet of laser light, and the scattered light can be observed with\ndigital cameras. Some complex plasma microgravity research facilities use two\ncameras with an overlapping field of view.\n  An overlapping field of view can be used to combine the resulting images into\none and trace the particles in the larger field of view. In previous work this\nwas discussed for the images recorded by the PK-4 Laboratory on board the\nInternational Space Station. In that work the width of the laser sheet was,\nhowever, not taken into account. In this paper, we will discuss how to improve\nthe transformation of the features into a joint coordinate system, and possibly\nextract information on the 3D position of particles in the overlap region.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:21:19 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mohr", "Daniel P.", ""], ["Huber", "Peter", ""], ["Schwabe", "Mierk", ""], ["Knapek", "Christina A.", ""]]}, {"id": "1912.04344", "submitter": "Shuran Song", "authors": "Shuran Song, Andy Zeng, Johnny Lee, Thomas Funkhouser", "title": "Grasping in the Wild:Learning 6DoF Closed-Loop Grasping from Low-Cost\n  Demonstrations", "comments": "Project Webpage https://graspinwild.cs.columbia.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent manipulation benefits from the capacity to flexibly control an\nend-effector with high degrees of freedom (DoF) and dynamically react to the\nenvironment. However, due to the challenges of collecting effective training\ndata and learning efficiently, most grasping algorithms today are limited to\ntop-down movements and open-loop execution. In this work, we propose a new\nlow-cost hardware interface for collecting grasping demonstrations by people in\ndiverse environments. Leveraging this data, we show that it is possible to\ntrain a robust end-to-end 6DoF closed-loop grasping model with reinforcement\nlearning that transfers to real robots. A key aspect of our grasping model is\nthat it uses \"action-view\" based rendering to simulate future states with\nrespect to different possible actions. By evaluating these states using a\nlearned value function (Q-function), our method is able to better select\ncorresponding actions that maximize total rewards (i.e., grasping success). Our\nfinal grasping system is able to achieve reliable 6DoF closed-loop grasping of\nnovel objects across various scene configurations, as well as dynamic scenes\nwith moving objects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:47:08 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 20:05:35 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Song", "Shuran", ""], ["Zeng", "Andy", ""], ["Lee", "Johnny", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1912.04363", "submitter": "Pengfei Li", "authors": "Pengfei Li, Weichao Qiu, Michael Peven, Gregory D. Hager, Alan L.\n  Yuille", "title": "Car Pose in Context: Accurate Pose Estimation with Ground Plane\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene context is a powerful constraint on the geometry of objects within the\nscene in cases, such as surveillance, where the camera geometry is unknown and\nimage quality may be poor. In this paper, we describe a method for estimating\nthe pose of cars in a scene jointly with the ground plane that supports them.\nWe formulate this as a joint optimization that accounts for varying car shape\nusing a statistical atlas, and which simultaneously computes geometry and\ninternal camera parameters. We demonstrate that this method produces\nsignificant improvements for car pose estimation, and we show that the\nresulting 3D geometry, when computed over a video sequence, makes it possible\nto improve on state of the art classification of car behavior. We also show\nthat introducing the planar constraint allows us to estimate camera focal\nlength in a reliable manner.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 20:36:17 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Li", "Pengfei", ""], ["Qiu", "Weichao", ""], ["Peven", "Michael", ""], ["Hager", "Gregory D.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1912.04376", "submitter": "Mohammad Rashidi", "authors": "Tyler Dauphinee, Nikunj Patel, Mohammad Rashidi", "title": "Modular Multimodal Architecture for Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Page classification is a crucial component to any document analysis system,\nallowing for complex branching control flows for different components of a\ngiven document. Utilizing both the visual and textual content of a page, the\nproposed method exceeds the current state-of-the-art performance on the\nRVL-CDIP benchmark at 93.03% test accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 21:06:15 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Dauphinee", "Tyler", ""], ["Patel", "Nikunj", ""], ["Rashidi", "Mohammad", ""]]}, {"id": "1912.04384", "submitter": "Alexander Mai", "authors": "Alexander Mai, Joseph Menke, Allen Yang", "title": "Training Deep Neural Networks to Detect Repeatable 2D Features Using\n  Large Amounts of 3D World Capture Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image space feature detection is the act of selecting points or parts of an\nimage that are easy to distinguish from the surrounding image region. By\ncombining a repeatable point detection with a descriptor, parts of an image can\nbe matched with one another, which is useful in applications like estimating\npose from camera input or rectifying images. Recently, precise indoor tracking\nhas started to become important for Augmented and Virtual reality as it is\nnecessary to allow positioning of a headset in 3D space without the need for\nexternal tracking devices. Several modern feature detectors use homographies to\nsimulate different viewpoints, not only to train feature detection and\ndescription, but test them as well. The problem is that, often, views of indoor\nspaces contain high depth disparity. This makes the approximation that a\nhomography applied to an image represents a viewpoint change inaccurate. We\nclaim that in order to train detectors to work well in indoor environments,\nthey must be robust to this type of geometry, and repeatable under true\nviewpoint change instead of homographies. Here we focus on the problem of\ndetecting repeatable feature locations under true viewpoint change. To this\nend, we generate labeled 2D images from a photo-realistic 3D dataset. These\nimages are used for training a neural network based feature detector. We\nfurther present an algorithm for automatically generating labels of repeatable\n2D features, and present a fast, easy to use test algorithm for evaluating a\ndetector in an 3D environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 21:28:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mai", "Alexander", ""], ["Menke", "Joseph", ""], ["Yang", "Allen", ""]]}, {"id": "1912.04418", "submitter": "Jakub Mare\\v{c}ek", "authors": "Albert Akhriev and Jakub Marecek", "title": "Deep Autoencoders with Value-at-Risk Thresholding for Unsupervised\n  Anomaly Detection", "comments": null, "journal-ref": "IEEE International Symposium on Multimedia 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world monitoring and surveillance applications require non-trivial\nanomaly detection to be run in the streaming model. We consider an\nincremental-learning approach, wherein a deep-autoencoding (DAE) model of what\nis normal is trained and used to detect anomalies at the same time. In the\ndetection of anomalies, we utilise a novel thresholding mechanism, based on\nvalue at risk (VaR). We compare the resulting convolutional neural network\n(CNN) against a number of subspace methods, and present results on\nchangedetection net.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 23:14:05 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Akhriev", "Albert", ""], ["Marecek", "Jakub", ""]]}, {"id": "1912.04421", "submitter": "Ayan Chakrabarti", "authors": "Zhihao Xia, Federico Perazzi, Micha\\\"el Gharbi, Kalyan Sunkavalli,\n  Ayan Chakrabarti", "title": "Basis Prediction Networks for Effective Burst Denoising with Large\n  Kernels", "comments": "CVPR 2020. Project website at\n  https://www.cse.wustl.edu/~zhihao.xia/bpn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bursts of images exhibit significant self-similarity across both time and\nspace. This motivates a representation of the kernels as linear combinations of\na small set of basis elements. To this end, we introduce a novel basis\nprediction network that, given an input burst, predicts a set of global basis\nkernels -- shared within the image -- and the corresponding mixing coefficients\n-- which are specific to individual pixels. Compared to state-of-the-art\ntechniques that output a large tensor of per-pixel spatiotemporal kernels, our\nformulation substantially reduces the dimensionality of the network output.\nThis allows us to effectively exploit comparatively larger denoising kernels,\nachieving both significant quality improvements (over 1dB PSNR) and faster\nrun-times over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 23:28:30 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 00:19:45 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Xia", "Zhihao", ""], ["Perazzi", "Federico", ""], ["Gharbi", "Micha\u00ebl", ""], ["Sunkavalli", "Kalyan", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1912.04423", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem, Rodrigo Alves Lima, Bruno Padilha, Joao Eduardo\n  Ferreira, Calton Pu", "title": "Robust, Extensible, and Fast: Teamed Classifiers for Vehicle Tracking\n  and Vehicle Re-ID in Multi-Camera Networks", "comments": null, "journal-ref": "2019 IEEE Conference on Cognitive Machine Intelligence", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As camera networks have become more ubiquitous over the past decade, the\nresearch interest in video management has shifted to analytics on multi-camera\nnetworks. This includes performing tasks such as object detection, attribute\nidentification, and vehicle/person tracking across different cameras without\noverlap. Current frameworks for management are designed for multi-camera\nnetworks in a closed dataset environment where there is limited variability in\ncameras and characteristics of the surveillance environment are well known.\nFurthermore, current frameworks are designed for offline analytics with\nguidance from human operators for forensic applications. This paper presents a\nteamed classifier framework for video analytics in heterogeneous many-camera\nnetworks with adversarial conditions such as multi-scale, multi-resolution\ncameras capturing the environment with varying occlusion, blur, and\norientations. We describe an implementation for vehicle tracking and vehicle\nre-identification (re-id), where we implement a zero-shot learning (ZSL) system\nthat performs automated tracking of all vehicles all the time. Our evaluations\non VeRi-776 and Cars196 show the teamed classifier framework is robust to\nadversarial conditions, extensible to changing video characteristics such as\nnew vehicle types/brands and new cameras, and offers real-time performance\ncompared to current offline video analytics approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 23:34:33 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 13:54:53 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Suprem", "Abhijit", ""], ["Lima", "Rodrigo Alves", ""], ["Padilha", "Bruno", ""], ["Ferreira", "Joao Eduardo", ""], ["Pu", "Calton", ""]]}, {"id": "1912.04430", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar and Brendan Morris", "title": "HalluciNet-ing Spatiotemporal Representations Using a 2D-CNN", "comments": "Codebase: https://github.com/ParitoshParmar/HalluciNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatiotemporal representations learned using 3D convolutional neural networks\n(CNN) are currently used in state-of-the-art approaches for action related\ntasks. However, 3D-CNN are notorious for being memory and compute resource\nintensive as compared with more simple 2D-CNN architectures. We propose to\nhallucinate spatiotemporal representations from a 3D-CNN teacher with a 2D-CNN\nstudent. By requiring the 2D-CNN to predict the future and intuit upcoming\nactivity, it is encouraged to gain a deeper understanding of actions and how\nthey evolve. The hallucination task is treated as an auxiliary task, which can\nbe used with any other action related task in a multitask learning setting.\nThorough experimental evaluation shows that the hallucination task indeed helps\nimprove performance on action recognition, action quality assessment, and\ndynamic scene recognition tasks. From a practical standpoint, being able to\nhallucinate spatiotemporal representations without an actual 3D-CNN can enable\ndeployment in resource-constrained scenarios, such as with limited computing\npower and/or lower bandwidth. Codebase is available here:\nhttps://github.com/ParitoshParmar/HalluciNet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 00:44:25 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 04:33:52 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 07:05:24 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan", ""]]}, {"id": "1912.04441", "submitter": "Lukas Cavigelli", "authors": "Xiaying Wang, Lukas Cavigelli, Manuel Eggimann, Michele Magno, Luca\n  Benini", "title": "HR-SAR-Net: A Deep Neural Network for Urban Scene Segmentation from\n  High-Resolution SAR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) data is becoming increasingly available to a\nwide range of users through commercial service providers with resolutions\nreaching 0.5m/px. Segmenting SAR data still requires skilled personnel,\nlimiting the potential for large-scale use. We show that it is possible to\nautomatically and reliably perform urban scene segmentation from next-gen\nresolution SAR data (0.15m/px) using deep neural networks (DNNs), achieving a\npixel accuracy of 95.19% and a mean IoU of 74.67% with data collected over a\nregion of merely 2.2km${}^2$. The presented DNN is not only effective, but is\nvery small with only 63k parameters and computationally simple enough to\nachieve a throughput of around 500Mpx/s using a single GPU. We further identify\nthat additional SAR receive antennas and data from multiple flights massively\nimprove the segmentation accuracy. We describe a procedure for generating a\nhigh-quality segmentation ground truth from multiple inaccurate building and\nroad annotations, which has been crucial to achieving these segmentation\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:24:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Xiaying", ""], ["Cavigelli", "Lukas", ""], ["Eggimann", "Manuel", ""], ["Magno", "Michele", ""], ["Benini", "Luca", ""]]}, {"id": "1912.04443", "submitter": "Marvin Zhang", "authors": "Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, Sergey Levine", "title": "AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human\n  Videos", "comments": "Robotics: Science and Systems (RSS) 2020 camera ready submission.\n  Project website: https://sites.google.com/view/rss20avid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic reinforcement learning (RL) holds the promise of enabling robots to\nlearn complex behaviors through experience. However, realizing this promise for\nlong-horizon tasks in the real world requires mechanisms to reduce human burden\nin terms of defining the task and scaffolding the learning process. In this\npaper, we study how these challenges can be alleviated with an automated\nrobotic learning framework, in which multi-stage tasks are defined simply by\nproviding videos of a human demonstrator and then learned autonomously by the\nrobot from raw image observations. A central challenge in imitating human\nvideos is the difference in appearance between the human and robot, which\ntypically requires manual correspondence. We instead take an automated approach\nand perform pixel-level image translation via CycleGAN to convert the human\ndemonstration into a video of a robot, which can then be used to construct a\nreward function for a model-based RL algorithm. The robot then learns the task\none stage at a time, automatically learning how to reset each stage to retry it\nmultiple times without human-provided resets. This makes the learning process\nlargely automatic, from intuitive task specification via a video to automated\ntraining with minimal human intervention. We demonstrate that our approach is\ncapable of learning complex tasks, such as operating a coffee machine, directly\nfrom raw image observations, requiring only 20 minutes to provide human\ndemonstrations and about 180 minutes of robot interaction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:36:18 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 04:29:36 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2020 20:16:28 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Smith", "Laura", ""], ["Dhawan", "Nikita", ""], ["Zhang", "Marvin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.04459", "submitter": "Yingqian Wang", "authors": "Yingqian Wang, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan\n  Guo", "title": "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background objects occluded in some views of a light field (LF) camera can be\nseen by other views. Consequently, occluded surfaces are possible to be\nreconstructed from LF images. In this paper, we handle the LF de-occlusion\n(LF-DeOcc) problem using a deep encoder-decoder network (namely, DeOccNet). In\nour method, sub-aperture images (SAIs) are first given to the encoder to\nincorporate both spatial and angular information. The encoded representations\nare then used by the decoder to render an occlusionfree center-view SAI. To the\nbest of our knowledge, DeOccNet is the first deep learning-based LF-DeOcc\nmethod. To handle the insufficiency of training data, we propose an LF\nsynthesis approach to embed selected occlusion masks into existing LF images.\nBesides, several synthetic and realworld LFs are developed for performance\nevaluation. Experimental results show that, after training on the generated\ndata, our DeOccNet can effectively remove foreground occlusions and achieves\nsuperior performance as compared to other state-of-the-art methods. Source\ncodes are available at: https://github.com/YingqianWang/DeOccNet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:35:29 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Yingqian", ""], ["Wu", "Tianhao", ""], ["Yang", "Jungang", ""], ["Wang", "Longguang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "1912.04461", "submitter": "Hyunjun Eun", "authors": "Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, Changick Kim", "title": "Learning to Discriminate Information for Online Action Detection", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a streaming video, online action detection aims to identify actions in\nthe present. For this task, previous methods use recurrent networks to model\nthe temporal sequence of current action frames. However, these methods overlook\nthe fact that an input image sequence includes background and irrelevant\nactions as well as the action of interest. For online action detection, in this\npaper, we propose a novel recurrent unit to explicitly discriminate the\ninformation relevant to an ongoing action from others. Our unit, named\nInformation Discrimination Unit (IDU), decides whether to accumulate input\ninformation based on its relevance to the current action. This enables our\nrecurrent network with IDU to learn a more discriminative representation for\nidentifying ongoing actions. In experiments on two benchmark datasets, TVSeries\nand THUMOS-14, the proposed method outperforms state-of-the-art methods by a\nsignificant margin. Moreover, we demonstrate the effectiveness of our recurrent\nunit by conducting comprehensive ablation studies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:45:37 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 06:04:57 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 03:48:06 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Eun", "Hyunjun", ""], ["Moon", "Jinyoung", ""], ["Park", "Jongyoul", ""], ["Jung", "Chanho", ""], ["Kim", "Changick", ""]]}, {"id": "1912.04462", "submitter": "Shiyuan Huang", "authors": "Shiyuan Huang, Xudong Lin, Svebor Karaman, Shih-Fu Chang", "title": "Flow-Distilled IP Two-Stream Networks for Compressed Video Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stream networks have achieved great success in video recognition. A\ntwo-stream network combines a spatial stream of RGB frames and a temporal\nstream of Optical Flow to make predictions. However, the temporal redundancy of\nRGB frames as well as the high-cost of optical flow computation creates\nchallenges for both the performance and efficiency. Recent works instead use\nmodern compressed video modalities as an alternative to the RGB spatial stream\nand improve the inference speed by orders of magnitudes. Previous works create\none stream for each modality which are combined with an additional temporal\nstream through late fusion. This is redundant since some modalities like motion\nvectors already contain temporal information. Based on this observation, we\npropose a compressed domain two-stream network IP TSN for compressed video\nrecognition, where the two streams are represented by the two types of frames\n(I and P frames) in compressed videos, without needing a separate temporal\nstream. With this goal, we propose to fully exploit the motion information of\nP-stream through generalized distillation from optical flow, which largely\nimproves the efficiency and accuracy. Our P-stream runs 60 times faster than\nusing optical flow while achieving higher accuracy. Our full IP TSN, evaluated\nover public action recognition benchmarks (UCF101, HMDB51 and a subset of\nKinetics), outperforms other compressed domain methods by large margins while\nimproving the total inference speed by 20%.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:52:09 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 16:44:21 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Huang", "Shiyuan", ""], ["Lin", "Xudong", ""], ["Karaman", "Svebor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1912.04465", "submitter": "Yudong Jiang", "authors": "Yudong Jiang, Kaixu Cui, Leilei Chen, Canjin Wang, Changliang Xu", "title": "SoccerDB: A Large-Scale Database for Comprehensive Video Understanding", "comments": "accepted by MM2020 sports workshop", "journal-ref": null, "doi": "10.1145/3422844.3423051", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Soccer videos can serve as a perfect research object for video understanding\nbecause soccer games are played under well-defined rules while complex and\nintriguing enough for researchers to study. In this paper, we propose a new\nsoccer video database named SoccerDB, comprising 171,191 video segments from\n346 high-quality soccer games. The database contains 702,096 bounding boxes,\n37,709 essential event labels with time boundary and 17,115 highlight\nannotations for object detection, action recognition, temporal action\nlocalization, and highlight detection tasks. To our knowledge, it is the\nlargest database for comprehensive sports video understanding on various\naspects. We further survey a collection of strong baselines on SoccerDB, which\nhave demonstrated state-of-the-art performances on independent tasks. Our\nevaluation suggests that we can benefit significantly when jointly considering\nthe inner correlations among those tasks. We believe the release of SoccerDB\nwill tremendously advance researches around comprehensive video understanding.\n{\\itshape Our dataset and code published on\nhttps://github.com/newsdata/SoccerDB.}\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:57:28 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 05:51:47 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 03:16:38 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 13:27:22 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jiang", "Yudong", ""], ["Cui", "Kaixu", ""], ["Chen", "Leilei", ""], ["Wang", "Canjin", ""], ["Xu", "Changliang", ""]]}, {"id": "1912.04478", "submitter": "He-Feng Yin", "authors": "Pei Xie, He-Feng Yin and Xiao-Jun Wu", "title": "Low-rank representations with incoherent dictionary for face recognition", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition remains a hot topic in computer vision, and it is\nchallenging to tackle the problem that both the training and testing images are\ncorrupted. In this paper, we propose a novel semi-supervised method based on\nthe theory of the low-rank matrix recovery for face recognition, which can\nsimultaneously learn discriminative low-rank and sparse representations for\nboth training and testing images. To this end, a correlation penalty term is\nintroduced into the formulation of our proposed method to learn an incoherent\ndictionary. Experimental results on several face image databases demonstrate\nthe effectiveness of our method, i.e., the proposed method is robust to the\nillumination, expression and pose variations, as well as images with noises\nsuch as block occlusion or uniform noises.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:44:25 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Xie", "Pei", ""], ["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1912.04485", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Tat-Jun Chin and Ian Reid", "title": "NeuRoRA: Neural Robust Rotation Averaging", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple rotation averaging is an essential task for structure from motion,\nmapping, and robot navigation. The task is to estimate the absolute\norientations of several cameras given some of their noisy relative orientation\nmeasurements. The conventional methods for this task seek parameters of the\nabsolute orientations that agree best with the observed noisy measurements\naccording to a robust cost function. These robust cost functions are highly\nnonlinear and are designed based on certain assumptions about the noise and\noutlier distributions. In this work, we aim to build a neural network that\nlearns the noise patterns from the data and predict/regress the model\nparameters from the noisy relative orientations. The proposed network is a\ncombination of two networks: (1) a view-graph cleaning network, which detects\noutlier edges in the view-graph and rectifies noisy measurements; and (2) a\nfine-tuning network, which fine-tunes an initialization of absolute\norientations bootstrapped from the cleaned graph, in a single step. The\nproposed combined network is very fast, moreover, being trained on a large\nnumber of synthetic graphs, it is more accurate than the conventional iterative\noptimization methods. Although the idea of replacing robust optimization\nmethods by a graph-based network is demonstrated only for multiple rotation\naveraging, it could easily be extended to other graph-based geometric problems,\nfor example, pose-graph optimization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 04:04:06 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 09:02:34 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 06:13:42 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Purkait", "Pulak", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""]]}, {"id": "1912.04486", "submitter": "Junjie Zhang", "authors": "Junjie Zhang, Lingqiao Liu, Peng Wang, Chunhua Shen", "title": "To Balance or Not to Balance: A Simple-yet-Effective Approach for\n  Learning with Long-Tailed Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world visual data often exhibits a long-tailed distribution, where some\n''head'' classes have a large number of samples, yet only a few samples are\navailable for ''tail'' classes. Such imbalanced distribution causes a great\nchallenge for learning a deep neural network, which can be boiled down into a\ndilemma: on the one hand, we prefer to increase the exposure of tail class\nsamples to avoid the excessive dominance of head classes in the classifier\ntraining. On the other hand, oversampling tail classes makes the network prone\nto over-fitting, since head class samples are often consequently\nunder-represented. To resolve this dilemma, in this paper, we propose a\nsimple-yet-effective auxiliary learning approach. The key idea is to split a\nnetwork into a classifier part and a feature extractor part, and then employ\ndifferent training strategies for each part. Specifically, to promote the\nawareness of tail-classes, a class-balanced sampling scheme is utilised for\ntraining both the classifier and the feature extractor. For the feature\nextractor, we also introduce an auxiliary training task, which is to train a\nclassifier under the regular random sampling scheme. In this way, the feature\nextractor is jointly trained from both sampling strategies and thus can take\nadvantage of all training data and avoid the over-fitting issue. Apart from\nthis basic auxiliary task, we further explore the benefit of using\nself-supervised learning as the auxiliary task. Without using any bells and\nwhistles, our model achieves superior performance over the state-of-the-art\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 04:11:53 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 00:02:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zhang", "Junjie", ""], ["Liu", "Lingqiao", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1912.04487", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, Lorenzo Torresani", "title": "Listen to Look: Action Recognition by Previewing Audio", "comments": "Appears in CVPR 2020; Project page:\n  http://vision.cs.utexas.edu/projects/listen_to_look/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the face of the video data deluge, today's expensive clip-level\nclassifiers are increasingly impractical. We propose a framework for efficient\naction recognition in untrimmed video that uses audio as a preview mechanism to\neliminate both short-term and long-term visual redundancies. First, we devise\nan ImgAud2Vid framework that hallucinates clip-level features by distilling\nfrom lighter modalities---a single frame and its accompanying audio---reducing\nshort-term temporal redundancy for efficient clip-level recognition. Second,\nbuilding on ImgAud2Vid, we further propose ImgAud-Skimming, an attention-based\nlong short-term memory network that iteratively selects useful moments in\nuntrimmed videos, reducing long-term temporal redundancy for efficient\nvideo-level recognition. Extensive experiments on four action recognition\ndatasets demonstrate that our method achieves the state-of-the-art in terms of\nboth recognition accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 04:15:24 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 02:06:18 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 04:53:38 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gao", "Ruohan", ""], ["Oh", "Tae-Hyun", ""], ["Grauman", "Kristen", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1912.04488", "submitter": "Chunhua Shen", "authors": "Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li", "title": "SOLO: Segmenting Objects by Locations", "comments": "Accepted to Proc. Eur. Conf. Computer Vision (ECCV) 2020. Code is\n  available at https://git.io/AdelaiDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new, embarrassingly simple approach to instance segmentation in\nimages. Compared to many other dense prediction tasks, e.g., semantic\nsegmentation, it is the arbitrary number of instances that have made instance\nsegmentation much more challenging. In order to predict a mask for each\ninstance, mainstream approaches either follow the 'detect-thensegment' strategy\nas used by Mask R-CNN, or predict category masks first then use clustering\ntechniques to group pixels into individual instances. We view the task of\ninstance segmentation from a completely new perspective by introducing the\nnotion of \"instance categories\", which assigns categories to each pixel within\nan instance according to the instance's location and size, thus nicely\nconverting instance mask segmentation into a classification-solvable problem.\nNow instance segmentation is decomposed into two classification tasks. We\ndemonstrate a much simpler and flexible instance segmentation framework with\nstrong performance, achieving on par accuracy with Mask R-CNN and outperforming\nrecent singleshot instance segmenters in accuracy. We hope that this very\nsimple and strong framework can serve as a baseline for many instance-level\nrecognition tasks besides instance segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 04:22:41 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 06:01:13 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 08:22:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Xinlong", ""], ["Kong", "Tao", ""], ["Shen", "Chunhua", ""], ["Jiang", "Yuning", ""], ["Li", "Lei", ""]]}, {"id": "1912.04497", "submitter": "Kirthi Shankar Sivamani", "authors": "Kirthi Shankar Sivamani", "title": "Feature Losses for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made tremendous advances in computer vision tasks such as\nimage classification. However, recent studies have shown that deep learning\nmodels are vulnerable to specifically crafted adversarial inputs that are\nquasi-imperceptible to humans. In this work, we propose a novel approach to\ndefending adversarial attacks. We employ an input processing technique based on\ndenoising autoencoders as a defense. It has been shown that the input\nperturbations grow and accumulate as noise in feature maps while propagating\nthrough a convolutional neural network (CNN). We exploit the noisy feature maps\nby using an additional subnetwork to extract image feature maps and train an\nauto-encoder on perceptual losses of these feature maps. This technique\nachieves close to state-of-the-art results on defending MNIST and CIFAR10\ndatasets, but more importantly, shows a new way of employing a defense that\ncannot be trivially trained end-to-end by the attacker. Empirical results\ndemonstrate the effectiveness of this approach on the MNIST and CIFAR10\ndatasets on simple as well as iterative LP attacks. Our method can be applied\nas a preprocessing technique to any off the shelf CNN.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 04:58:45 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Sivamani", "Kirthi Shankar", ""]]}, {"id": "1912.04514", "submitter": "Wenchi Ma", "authors": "Wenchi Ma, Yuanwei Wu, Feng Cen, Guanghui Wang", "title": "MDFN: Multi-Scale Deep Feature Learning Network for Object Detection", "comments": null, "journal-ref": "Pattern Recognition 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an innovative object detector by leveraging deep features\nlearned in high-level layers. Compared with features produced in earlier\nlayers, the deep features are better at expressing semantic and contextual\ninformation. The proposed deep feature learning scheme shifts the focus from\nconcrete features with details to abstract ones with semantic information. It\nconsiders not only individual objects and local contexts but also their\nrelationships by building a multi-scale deep feature learning network (MDFN).\nMDFN efficiently detects the objects by introducing information square and\ncubic inception modules into the high-level layers, which employs\nparameter-sharing to enhance the computational efficiency. MDFN provides a\nmulti-scale object detector by integrating multi-box, multi-scale and\nmulti-level technologies. Although MDFN employs a simple framework with a\nrelatively small base network (VGG-16), it achieves better or competitive\ndetection results than those with a macro hierarchical structure that is either\nvery deep or very wide for stronger ability of feature extraction. The proposed\ntechnique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets,\nwhich achieves the best results on KITTI and leading performance on PASCAL VOC\nand COCO. This study reveals that deep features provide prominent semantic\ninformation and a variety of contextual contents, which contribute to its\nsuperior performance in detecting small or occluded objects. In addition, the\nMDFN model is computationally efficient, making a good trade-off between the\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 05:57:04 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Ma", "Wenchi", ""], ["Wu", "Yuanwei", ""], ["Cen", "Feng", ""], ["Wang", "Guanghui", ""]]}, {"id": "1912.04518", "submitter": "Shuaicheng Liu Prof.", "authors": "Shuaicheng Liu, Zehao Zhang, Kai Song, Bing Zeng", "title": "Arithmetic addition of two integers by deep image classification\n  networks: experiments to quantify their autonomous reasoning ability", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented performance achieved by deep convolutional neural networks\nfor image classification is linked primarily to their ability of capturing rich\nstructural features at various layers within networks. Here we design a series\nof experiments, inspired by children's learning of the arithmetic addition of\ntwo integers, to showcase that such deep networks can go beyond the structural\nfeatures to learn deeper knowledge. In our experiments, a set of images is\nconstructed, each image containing an arithmetic addition $n+m$ in its central\narea, and several classification networks are then trained over a subset of\nimages, using the sum as the label. Tests on the excluded images show that, as\nthe image set gets larger, the networks have well learnt the law of arithmetic\nadditions so as to build up their autonomous reasoning ability strongly. For\ninstance, networks trained over a small percentage of images can classify a big\nmajority of the remaining images correctly, and many arithmetic additions\ninvolving some integers that have never been seen during the training can also\nbe solved correctly by the trained networks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:02:59 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Liu", "Shuaicheng", ""], ["Zhang", "Zehao", ""], ["Song", "Kai", ""], ["Zeng", "Bing", ""]]}, {"id": "1912.04523", "submitter": "Victoria Lin", "authors": "Victoria Lin, Jeffrey M. Girard, Louis-Philippe Morency", "title": "Context-Dependent Models for Predicting and Characterizing Facial\n  Expressiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, extensive research has emerged in affective computing on\ntopics like automatic emotion recognition and determining the signals that\ncharacterize individual emotions. Much less studied, however, is\nexpressiveness, or the extent to which someone shows any feeling or emotion.\nExpressiveness is related to personality and mental health and plays a crucial\nrole in social interaction. As such, the ability to automatically detect or\npredict expressiveness can facilitate significant advancements in areas ranging\nfrom psychiatric care to artificial social intelligence. Motivated by these\npotential applications, we present an extension of the BP4D+ dataset with human\nratings of expressiveness and develop methods for (1) automatically predicting\nexpressiveness from visual data and (2) defining relationships between\ninterpretable visual signals and expressiveness. In addition, we study the\nemotional context in which expressiveness occurs and hypothesize that different\nsets of signals are indicative of expressiveness in different contexts (e.g.,\nin response to surprise or in response to pain). Analysis of our statistical\nmodels confirms our hypothesis. Consequently, by looking at expressiveness\nseparately in distinct emotional contexts, our predictive models show\nsignificant improvements over baselines and achieve comparable results to human\nperformance in terms of correlation with the ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:10:25 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lin", "Victoria", ""], ["Girard", "Jeffrey M.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1912.04527", "submitter": "Francesca Baldini", "authors": "Francesca Baldini, Animashree Anandkumar, and Richard M. Murray", "title": "Learning Pose Estimation for UAV Autonomous Navigation andLanding Using\n  Visual-Inertial Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new learning approach for autonomous navigation\nand landing of an Unmanned-Aerial-Vehicle (UAV). We develop a multimodal fusion\nof deep neural architectures for visual-inertial odometry. We train the model\nin an end-to-end fashion to estimate the current vehicle pose from streams of\nvisual and inertial measurements. We first evaluate the accuracy of our\nestimation by comparing the prediction of the model to traditional algorithms\non the publicly available EuRoC MAV dataset. The results illustrate a $25 \\%$\nimprovement in estimation accuracy over the baseline. Finally, we integrate the\narchitecture in the closed-loop flight control system of Airsim - a plugin\nsimulator for Unreal Engine - and we provide simulation results for autonomous\nnavigation and landing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:37:30 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 11:18:55 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Baldini", "Francesca", ""], ["Anandkumar", "Animashree", ""], ["Murray", "Richard M.", ""]]}, {"id": "1912.04536", "submitter": "Jia Guo", "authors": "Jia Guo, Yuxuan Mu, Dong Xue, Huiqi Li, Junxian Chen, Huanxin Yan,\n  Hailin Xu, Wei Wang", "title": "Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant\n  Landmark Detection for Calcaneal Angle Measurement, Fracture Identification\n  and Fracture Region Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcaneus is the largest tarsal bone to withstand the daily stresses of\nweight-bearing. The calcaneal fracture is the most common type in the tarsal\nbone fractures. After a fracture is suspected, plain radiographs should be\ntaken first. Bohler's Angle (BA) and Critical Angle of Gissane (CAG), measured\nby four anatomic landmarks in lateral foot radiograph, can guide fracture\ndiagnosis and facilitate operative recovery of the fractured calcaneus. This\nstudy aims to develop an analysis system that can automatically locate four\nanatomic landmarks, measure BA and CAG for fracture assessment, identify\nfractured calcaneus, and segment fractured regions. For landmark detection, we\nproposed a coarse-to-fine Rotation-Invariant Regression-Voting (RIRV) landmark\ndetection method based on regressive Multi-Layer Perceptron (MLP) and Scale\nInvariant Feature Transform (SIFT) patch descriptor, which solves the problem\nof fickle rotation of calcaneus. By implementing a novel normalization\napproach, the RIRV method is explicitly rotation-invariance comparing with\ntraditional regressive methods. For fracture identification and segmentation, a\nconvolution neural network (CNN) based on U-Net with auxiliary classification\nhead (U-Net-CH) is designed. The input ROIs of the CNN are normalized by\ndetected landmarks to uniform view, orientation, and scale. The advantage of\nthis approach is the multi-task learning that combines classification and\nsegmentation. Our system can accurately measure BA and CAG with a mean angle\nerror of 3.8 and 6.2 respectively. For fracture identification and fracture\nregion segmentation, our system presents good performance with an F1-score of\n96.55%, recall of 94.99%, and segmentation IoU-score of 0.586.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:53:29 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:38:44 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 17:32:59 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2020 08:06:57 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 10:24:04 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Guo", "Jia", ""], ["Mu", "Yuxuan", ""], ["Xue", "Dong", ""], ["Li", "Huiqi", ""], ["Chen", "Junxian", ""], ["Yan", "Huanxin", ""], ["Xu", "Hailin", ""], ["Wang", "Wei", ""]]}, {"id": "1912.04538", "submitter": "Zhikai Chen", "authors": "Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, Qi Tian", "title": "Appending Adversarial Frames for Universal Video Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many efforts in attacking image classification models with\nadversarial perturbations, but the same topic on video classification has not\nyet been thoroughly studied. This paper presents a novel idea of video-based\nattack, which appends a few dummy frames (e.g., containing the texts of `thanks\nfor watching') to a video clip and then adds adversarial perturbations only on\nthese new frames. Our approach enjoys three major benefits, namely, a high\nsuccess rate, a low perceptibility, and a strong ability in transferring across\ndifferent networks. These benefits mostly come from the common dummy frame\nwhich pushes all samples towards the boundary of classification. On the other\nhand, such attacks are easily to be concealed since most people would not\nnotice the abnormality behind the perturbed video clips. We perform experiments\non two popular datasets with six state-of-the-art video classification models,\nand demonstrate the effectiveness of our approach in the scenario of universal\nvideo attacks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:54:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Zhikai", ""], ["Xie", "Lingxi", ""], ["Pang", "Shanmin", ""], ["He", "Yong", ""], ["Tian", "Qi", ""]]}, {"id": "1912.04554", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Christopher Zach and Ian Reid", "title": "SG-VAE: Scene Grammar Variational Autoencoder to generate new indoor\n  scenes", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been used in recent years to learn coherent\nlatent representations in order to synthesize high-quality images. In this\nwork, we propose a neural network to learn a generative model for sampling\nconsistent indoor scene layouts. Our method learns the co-occurrences, and\nappearance parameters such as shape and pose, for different objects categories\nthrough a grammar-based auto-encoder, resulting in a compact and accurate\nrepresentation for scene layouts. In contrast to existing grammar-based methods\nwith a user-specified grammar, we construct the grammar automatically by\nextracting a set of production rules on reasoning about object co-occurrences\nin training data. The extracted grammar is able to represent a scene by an\naugmented parse tree. The proposed auto-encoder encodes these parse trees to a\nlatent code, and decodes the latent code to a parse tree, thereby ensuring the\ngenerated scene is always valid. We experimentally demonstrate that the\nproposed auto-encoder learns not only to generate valid scenes (i.e. the\narrangements and appearances of objects), but it also learns coherent latent\nrepresentations where nearby latent samples decode to similar scene outputs.\nThe obtained generative model is applicable to several computer vision tasks\nsuch as 3D pose and layout estimation from RGB-D data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 07:53:36 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 03:31:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Purkait", "Pulak", ""], ["Zach", "Christopher", ""], ["Reid", "Ian", ""]]}, {"id": "1912.04561", "submitter": "Jinjin Zhang", "authors": "Jinjin Zhang, Wei Wang, Di Huang, Qingjie Liu and Yunhong Wang", "title": "A Feasible Framework for Arbitrary-Shaped Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have achieved surprising progress in Scene Text\nRecognition (STR), one of classic problems in computer vision. In this paper,\nwe propose a feasible framework for multi-lingual arbitrary-shaped STR,\nincluding instance segmentation based text detection and language model based\nattention mechanism for text recognition. Our STR algorithm not only recognizes\nLatin and Non-Latin characters, but also supports arbitrary-shaped text\nrecognition. Our method wins the championship on Scene Text Spotting Task\n(Latin Only, Latin and Chinese) of ICDAR2019 Robust Reading Challenge on\nArbitraryShaped Text Competition. Code is available at\nhttps://github.com/zhang0jhon/AttentionOCR.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:13:34 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 05:38:23 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Zhang", "Jinjin", ""], ["Wang", "Wei", ""], ["Huang", "Di", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "1912.04563", "submitter": "Jyoti Islam", "authors": "Jyoti Islam, Yanqing Zhang", "title": "Understanding 3D CNN Behavior for Alzheimer's Disease Diagnosis from\n  Brain PET Scan", "comments": "Science Meets Engineering of Deep Learning (SEDL) Workshop at NeurIPS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent days, Convolutional Neural Networks (CNN) have demonstrated\nimpressive performance in medical image analysis. However, there is a lack of\nclear understanding of why and how the Convolutional Neural Network performs so\nwell for image analysis task. How CNN analyzes an image and discriminates among\nsamples of different classes are usually considered as non-transparent. As a\nresult, it becomes difficult to apply CNN based approaches in clinical\nprocedures and automated disease diagnosis systems. In this paper, we consider\nthis issue and work on visualizing and understanding the decision of\nConvolutional Neural Network for Alzheimer's Disease (AD) Diagnosis. We develop\na 3D deep convolutional neural network for AD diagnosis using brain PET scans\nand propose using five visualizations techniques - Sensitivity Analysis\n(Backpropagation), Guided Backpropagation, Occlusion, Brain Area Occlusion, and\nLayer-wise Relevance Propagation (LRP) to understand the decision of the CNN by\nhighlighting the relevant areas in the PET data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:17:22 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 03:35:41 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Islam", "Jyoti", ""], ["Zhang", "Yanqing", ""]]}, {"id": "1912.04564", "submitter": "Sankalan Pal Chowdhury", "authors": "Arnab Kumar Mondal, Sankalan Pal Chowdhury, Aravind Jayendran, Parag\n  Singla, Himanshu Asnani, Prathosh AP", "title": "MaskAAE: Latent space optimization for Adversarial Auto-Encoders", "comments": "To be presented at UAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neural generative models is dominated by the highly successful\nGenerative Adversarial Networks (GANs) despite their challenges, such as\ntraining instability and mode collapse. Auto-Encoders (AE) with regularized\nlatent space provide an alternative framework for generative models, albeit\ntheir performance levels have not reached that of GANs. In this work, we\nhypothesise that the dimensionality of the AE model's latent space has a\ncritical effect on the quality of generated data. Under the assumption that\nnature generates data by sampling from a \"true\" generative latent space\nfollowed by a deterministic function, we show that the optimal performance is\nobtained when the dimensionality of the latent space of the AE-model matches\nwith that of the \"true\" generative latent space. Further, we propose an\nalgorithm called the Mask Adversarial Auto-Encoder (MaskAAE), in which the\ndimensionality of the latent space of an adversarial auto encoder is brought\ncloser to that of the \"true\" generative latent space, via a procedure to mask\nthe spurious latent dimensions. We demonstrate through experiments on synthetic\nand several real-world datasets that the proposed formulation yields betterment\nin the generation quality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:18:13 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 11:26:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Chowdhury", "Sankalan Pal", ""], ["Jayendran", "Aravind", ""], ["Singla", "Parag", ""], ["Asnani", "Himanshu", ""], ["AP", "Prathosh", ""]]}, {"id": "1912.04573", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Lorenzo Torresani", "title": "Classifying, Segmenting, and Tracking Object Instances in Video with\n  Mask Propagation", "comments": "CVPR 2020 Best Paper Nominee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for simultaneously classifying, segmenting and tracking\nobject instances in a video sequence. Our method, named MaskProp, adapts the\npopular Mask R-CNN to video by adding a mask propagation branch that propagates\nframe-level object instance masks from each video frame to all the other frames\nin a video clip. This allows our system to predict clip-level instance tracks\nwith respect to the object instances segmented in the middle frame of the clip.\nClip-level instance tracks generated densely for each frame in the sequence are\nfinally aggregated to produce video-level object instance segmentation and\nclassification. Our experiments demonstrate that our clip-level instance\nsegmentation makes our approach robust to motion blur and object occlusions in\nvideo. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,\noutperforming the ICCV 2019 video instance segmentation challenge winner\ndespite being much simpler and using orders of magnitude less labeled data\n(1.3M vs 1B images and 860K vs 14M bounding boxes).\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:40:10 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:38:37 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 21:30:26 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 19:21:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1912.04591", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas and Vittorio Ferrari", "title": "Neural Voxel Renderer: Learning an Accurate and Controllable Rendering\n  Tool", "comments": "Additional results: http://www.krematas.com/nvr/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural rendering framework that maps a voxelized scene into a\nhigh quality image. Highly-textured objects and scene element interactions are\nrealistically rendered by our method, despite having a rough representation as\nan input. Moreover, our approach allows controllable rendering: geometric and\nappearance modifications in the input are accurately propagated to the output.\nThe user can move, rotate and scale an object, change its appearance and\ntexture or modify the position of the light and all these edits are represented\nin the final rendering. We demonstrate the effectiveness of our approach by\nrendering scenes with varying appearance, from single color per object to\ncomplex, high-frequency textures. We show that our rerendering network can\ngenerate very detailed images that represent precisely the appearance of the\ninput scene. Our experiments illustrate that our approach achieves more\naccurate image synthesis results compared to alternatives and can also handle\nlow voxel grid resolutions. Finally, we show how our neural rendering framework\ncan capture and faithfully render objects from real images and from a diverse\nset of classes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:30:03 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 14:34:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1912.04608", "submitter": "Basura Fernando", "authors": "Yan Bin Ng and Basura Fernando", "title": "Forecasting future action sequences with attention: a new approach to\n  weakly supervised action forecasting", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3021497", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future human action forecasting from partial observations of activities is an\nimportant problem in many practical applications such as assistive robotics,\nvideo surveillance and security. We present a method to forecast actions for\nthe unseen future of the video using a neural machine translation technique\nthat uses encoder-decoder architecture. The input to this model is the observed\nRGB video, and the objective is to forecast the correct future symbolic action\nsequence. Unlike prior methods that make action predictions for some unseen\npercentage of video one for each frame, we predict the complete action sequence\nthat is required to accomplish the activity. We coin this task action sequence\nforecasting. To cater for two types of uncertainty in the future predictions,\nwe propose a novel loss function. We show a combination of optimal transport\nand future uncertainty losses help to boost results. We evaluate our model in\nthree challenging video datasets (Charades, MPII cooking and Breakfast).\n  We extend our action sequence forecasting model to perform weakly supervised\naction forecasting. Specifically, we propose a model to predict actions of\nfuture unseen frames without using frame level annotations during training. Our\nfully supervised model outperforms the state-of-the-art action forecasting\nmodel by 4.6%. Our weakly supervised model is only 0.6% behind the most recent\nstate-of-the-art supervised model and obtains comparable results to other\npublished fully supervised methods, and sometimes even outperforms them.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:08:21 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 01:26:53 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ng", "Yan Bin", ""], ["Fernando", "Basura", ""]]}, {"id": "1912.04618", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Mert Kayhan, Okan K\\\"op\\\"ukl\\\"u, Mhd Hasan Sarhan, Mehmet Yigitsoy,\n  Abouzar Eslami, Gerhard Rigoll", "title": "Deep Attention Based Semi-Supervised 2D-Pose Estimation for Surgical\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many practical problems and applications, it is not feasible to create a\nvast and accurately labeled dataset, which restricts the application of deep\nlearning in many areas. Semi-supervised learning algorithms intend to improve\nperformance by also leveraging unlabeled data. This is very valuable for\n2D-pose estimation task where data labeling requires substantial time and is\nsubject to noise. This work aims to investigate if semi-supervised learning\ntechniques can achieve acceptable performance level that makes using these\nalgorithms during training justifiable. To this end, a lightweight network\narchitecture is introduced and mean teacher, virtual adversarial training and\npseudo-labeling algorithms are evaluated on 2D-pose estimation for surgical\ninstruments. For the applicability of pseudo-labelling algorithm, we propose a\nnovel confidence measure, total variation. Experimental results show that\nutilization of semi-supervised learning improves the performance on unseen\ngeometries drastically while maintaining high accuracy for seen geometries. For\nRMIT benchmark, our lightweight architecture outperforms state-of-the-art with\nsupervised learning. For Endovis benchmark, pseudo-labelling algorithm improves\nthe supervised baseline achieving the new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:27:22 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 13:42:25 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kayhan", "Mert", ""], ["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Sarhan", "Mhd Hasan", ""], ["Yigitsoy", "Mehmet", ""], ["Eslami", "Abouzar", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1912.04619", "submitter": "Mohammad Ibrahim Sarker", "authors": "Mohammad Ibrahim Sarker, Hyongsuk Kim, Denis Tarasov, Dinar\n  Akhmetzanov", "title": "Inception Architecture and Residual Connections in Classification of\n  Breast Cancer Histology Images", "comments": "Achieved 23rd place out if 50 accepted positions (ICIAR Grand\n  Challenge on Brest cancer histology images)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of applying Inception v4 deep convolutional\nneural network to ICIAR-2018 Breast Cancer Classification Grand Challenge, part\na. The Challenge task is to classify breast cancer biopsy results, presented in\nform of hematoxylin and eosin stained images. Breast cancer classification is\nof primary interest to the medical practitioners and thus binary classification\nof breast cancer images have been under investigation by many researchers, but\nmulti-class categorization of histology breast images have been challenging due\nto the subtle differences among the categories. In this work extensive data\naugmentation is conducted to reduce overfitting and effectiveness of committee\nof several Inception v4 networks is studied. We report 89% accuracy on 4 class\nclassification task and 93.7% on carcinoma/non-carcinoma two class\nclassification task using our test set of 80 images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:27:29 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Sarker", "Mohammad Ibrahim", ""], ["Kim", "Hyongsuk", ""], ["Tarasov", "Denis", ""], ["Akhmetzanov", "Dinar", ""]]}, {"id": "1912.04627", "submitter": "Jacek Komorowski", "authors": "Grzegorz Kurzejamski, Jacek Komorowski, Lukasz Dabala, Konrad\n  Czarnota, Simon Lynen, Tomasz Trzcinski", "title": "SuperNCN: Neighbourhood consensus network for robust outdoor scenes\n  matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework for computing dense keypoint\ncorrespondences between images under strong scene appearance changes.\nTraditional methods, based on nearest neighbour search in the feature\ndescriptor space, perform poorly when environmental conditions vary, e.g. when\nimages are taken at different times of the day or seasons. Our method improves\nfinding keypoint correspondences in such difficult conditions. First, we use\nNeighbourhood Consensus Networks to build spatially consistent matching grid\nbetween two images at a coarse scale. Then, we apply Superpoint-like corner\ndetector to achieve pixel-level accuracy. Both parts use features learned with\ndomain adaptation to increase robustness against strong scene appearance\nvariations. The framework has been tested on a RobotCar Seasons dataset,\nproving large improvement on pose estimation task under challenging\nenvironmental conditions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:36:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Kurzejamski", "Grzegorz", ""], ["Komorowski", "Jacek", ""], ["Dabala", "Lukasz", ""], ["Czarnota", "Konrad", ""], ["Lynen", "Simon", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1912.04643", "submitter": "Pablo Laiz Trece\\~no PhD Student", "authors": "Pablo Laiz, Jordi Vitri\\`a, Hagen Wenzek, Carolina Malagelada,\n  Fernando Azpiroz, Santi Segu\\'i", "title": "WCE Polyp Detection with Triplet based Embeddings", "comments": "19 pages, 13 figures, 9 tables, Accepted in Computerized Medical\n  Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless capsule endoscopy is a medical procedure used to visualize the\nentire gastrointestinal tract and to diagnose intestinal conditions, such as\npolyps or bleeding. Current analyses are performed by manually inspecting\nnearly each one of the frames of the video, a tedious and error-prone task.\nAutomatic image analysis methods can be used to reduce the time needed for\nphysicians to evaluate a capsule endoscopy video, however these methods are\nstill in a research phase. In this paper we focus on computer-aided polyp\ndetection in capsule endoscopy images. This is a challenging problem because of\nthe diversity of polyp appearance, the imbalanced dataset structure and the\nscarcity of data. We have developed a new polyp computer-aided decision system\nthat combines a deep convolutional neural network and metric learning. The key\npoint of the method is the use of the triplet loss function with the aim of\nimproving feature extraction from the images when having small dataset. The\ntriplet loss function allows to train robust detectors by forcing images from\nthe same category to be represented by similar embedding vectors while ensuring\nthat images from different categories are represented by dissimilar vectors.\nEmpirical results show a meaningful increase of AUC values compared to baseline\nmethods. A good performance is not the only requirement when considering the\nadoption of this technology to clinical practice. Trust and explainability of\ndecisions are as important as performance. With this purpose, we also provide a\nmethod to generate visual explanations of the outcome of our polyp detector.\nThese explanations can be used to build a physician's trust in the system and\nalso to convey information about the inner working of the method to the\ndesigner for debugging purposes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 11:08:45 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:44:08 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 11:51:38 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Laiz", "Pablo", ""], ["Vitri\u00e0", "Jordi", ""], ["Wenzek", "Hagen", ""], ["Malagelada", "Carolina", ""], ["Azpiroz", "Fernando", ""], ["Segu\u00ed", "Santi", ""]]}, {"id": "1912.04645", "submitter": "Shuaicheng Liu Prof.", "authors": "Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, Bing Zeng", "title": "Neural Point Cloud Rendering via Multi-Plane Projection", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep point cloud rendering pipeline through multi-plane\nprojections. The input to the network is the raw point cloud of a scene and the\noutput are image or image sequences from a novel view or along a novel camera\ntrajectory. Unlike previous approaches that directly project features from 3D\npoints onto 2D image domain, we propose to project these features into a\nlayered volume of camera frustum. In this way, the visibility of 3D points can\nbe automatically learnt by the network, such that ghosting effects due to false\nvisibility check as well as occlusions caused by noise interferences are both\navoided successfully. Next, the 3D feature volume is fed into a 3D CNN to\nproduce multiple layers of images w.r.t. the space division in the depth\ndirections. The layered images are then blended based on learned weights to\nproduce the final rendering results. Experiments show that our network produces\nmore stable renderings compared to previous methods, especially near the object\nboundaries. Moreover, our pipeline is robust to noisy and relatively sparse\npoint cloud for a variety of challenging scenes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 11:11:50 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 04:28:05 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Dai", "Peng", ""], ["Zhang", "Yinda", ""], ["Li", "Zhuwen", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""]]}, {"id": "1912.04663", "submitter": "Shohei Nobuhara", "authors": "Kohei Yamashita, Shohei Nobuhara and Ko Nishino", "title": "3D-GMNet: Single-View 3D Shape Recovery as A Gaussian Mixture", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce 3D-GMNet, a deep neural network for 3D object\nshape reconstruction from a single image. As the name suggests, 3D-GMNet\nrecovers 3D shape as a Gaussian mixture. In contrast to voxels, point clouds,\nor meshes, a Gaussian mixture representation provides an analytical expression\nwith a small memory footprint while accurately representing the target 3D\nshape. At the same time, it offers a number of additional advantages including\ninstant pose estimation and controllable level-of-detail reconstruction, while\nalso enabling interpretation as a point cloud, volume, and a mesh model. We\ntrain 3D-GMNet end-to-end with single input images and corresponding 3D models\nby introducing two novel loss functions, a 3D Gaussian mixture loss and a 2D\nmulti-view loss, which collectively enable accurate shape reconstruction as\nkernel density estimation. We thoroughly evaluate the effectiveness of 3D-GMNet\nwith synthetic and real images of objects. The results show accurate\nreconstruction with a compact representation that also realizes novel\napplications of single-image 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:23:24 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 14:18:31 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yamashita", "Kohei", ""], ["Nobuhara", "Shohei", ""], ["Nishino", "Ko", ""]]}, {"id": "1912.04670", "submitter": "Yi Zhou", "authors": "Yi Zhou, Boyang Wang, Xiaodong He, Shanshan Cui and Ling Shao", "title": "DR-GAN: Conditional Generative Adversarial Network for Fine-Grained\n  Lesion Synthesis on Diabetic Retinopathy Images", "comments": "Extension work of our MICCAI paper", "journal-ref": "IEEE Journal of Biomedical and Health Informatics 2020", "doi": "10.1109/JBHI.2020.3045475", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a complication of diabetes that severely affects\neyes. It can be graded into five levels of severity according to international\nprotocol. However, optimizing a grading model to have strong generalizability\nrequires a large amount of balanced training data, which is difficult to\ncollect particularly for the high severity levels. Typical data augmentation\nmethods, including random flipping and rotation, cannot generate data with high\ndiversity. In this paper, we propose a diabetic retinopathy generative\nadversarial network (DR-GAN) to synthesize high-resolution fundus images which\ncan be manipulated with arbitrary grading and lesion information. Thus,\nlarge-scale generated data can be used for more meaningful augmentation to\ntrain a DR grading and lesion segmentation model. The proposed retina generator\nis conditioned on the structural and lesion masks, as well as adaptive grading\nvectors sampled from the latent grading space, which can be adopted to control\nthe synthesized grading severity. Moreover, a multi-scale spatial and channel\nattention module is devised to improve the generation ability to synthesize\ndetails. Multi-scale discriminators are designed to operate from large to small\nreceptive fields, and joint adversarial losses are adopted to optimize the\nwhole network in an end-to-end manner. With extensive experiments evaluated on\nthe EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we\nvalidate the effectiveness of our method, which can both synthesize highly\nrealistic (1280 x 1280) controllable fundus images and contribute to the DR\ngrading task.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:53:09 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 11:59:26 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 14:20:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhou", "Yi", ""], ["Wang", "Boyang", ""], ["He", "Xiaodong", ""], ["Cui", "Shanshan", ""], ["Shao", "Ling", ""]]}, {"id": "1912.04711", "submitter": "Quim Comas Mart\\'inez", "authors": "Joaquim Comas, Decky Aspandi and Xavier Binefa", "title": "End-to-end facial and physiological model for Affective Computing and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Affective Computing and its applications have become a\nfast-growing research topic. Furthermore, the rise of Deep Learning has\nintroduced significant improvements in the emotion recognition system compared\nto classical methods. In this work, we propose a multi-modal emotion\nrecognition model based on deep learning techniques using the combination of\nperipheral physiological signals and facial expressions. Moreover, we present\nan improvement to proposed models by introducing latent features extracted from\nour internal Bio Auto-Encoder (BAE). Both models are trained and evaluated on\nAMIGOS datasets reporting valence, arousal, and emotion state classification.\nFinally, to demonstrate a possible medical application in affective computing\nusing deep learning techniques, we applied the proposed method to the\nassessment of anxiety therapy. To this purpose, a reduced multi-modal database\nhas been collected by recording facial expressions and peripheral signals such\nas Electrocardiogram (ECG) and Galvanic Skin Response (GSR) of each patient.\nValence and arousal estimation was extracted using the proposed model from the\nbeginning until the end of the therapy, with successful evaluation to the\ndifferent emotional changes in the temporal domain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:37:15 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 11:25:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Comas", "Joaquim", ""], ["Aspandi", "Decky", ""], ["Binefa", "Xavier", ""]]}, {"id": "1912.04749", "submitter": "Shoufa Chen", "authors": "Shoufa Chen, Yunpeng Chen, Shuicheng Yan, Jiashi Feng", "title": "Efficient Differentiable Neural Architecture Search with Meta Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The searching procedure of neural architecture search (NAS) is notoriously\ntime consuming and cost prohibitive.To make the search space continuous, most\nexisting gradient-based NAS methods relax the categorical choice of a\nparticular operation to a softmax over all possible operations and calculate\nthe weighted sum of multiple features, resulting in a large memory requirement\nand a huge computation burden. In this work, we propose an efficient and novel\nsearch strategy with meta kernels. We directly encode the supernet from the\nperspective on convolution kernels and \"shrink\" multiple convolution kernel\ncandidates into a single one before these candidates operate on the input\nfeature. In this way, only a single feature is generated between two\nintermediate nodes. The memory for storing intermediate features and the\nresource budget for conducting convolution operations are both reduced\nremarkably. Despite high efficiency, our search strategy can search in a more\nfine-grained way than existing works and increases the capacity for\nrepresenting possible networks. We demonstrate the effectiveness of our search\nstrategy by conducting extensive experiments. Specifically, our method achieves\n77.0% top-1 accuracy on ImageNet benchmark dataset with merely 357M FLOPs,\noutperforming both EfficientNet and MobileNetV3 under the same FLOPs\nconstraints. Compared to models discovered by the start-of-the-art NAS method,\nour method achieves the same (sometimes even better) performance, while faster\nby three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:08:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Chen", "Shoufa", ""], ["Chen", "Yunpeng", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1912.04775", "submitter": "Yonglin Tian", "authors": "Yonglin Tian, Lichao Huang, Xuesong Li, Kunfeng Wang, Zilei Wang,\n  Fei-Yue Wang", "title": "Context-Aware Dynamic Feature Extraction for 3D Object Detection in\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying density of point clouds increases the difficulty of 3D detection. In\nthis paper, we present a context-aware dynamic network (CADNet) to capture the\nvariance of density by considering both point context and semantic context.\nPoint-level contexts are generated from original point clouds to enlarge the\neffective receptive filed. They are extracted around the voxelized pillars\nbased on our extended voxelization method and processed with the context\nencoder in parallel with the pillar features. With a large perception range, we\nare able to capture the variance of features for potential objects and generate\nattentive spatial guidance to help adjust the strengths for different regions.\nIn the region proposal network, considering the limited representation ability\nof traditional convolution where same kernels are shared among different\nsamples and positions, we propose a decomposable dynamic convolutional layer to\nadapt to the variance of input features by learning from local semantic\ncontext. It adaptively generates the position-dependent coefficients for\nmultiple fixed kernels and combines them to convolve with local feature\nwindows. Based on our dynamic convolution, we design a dual-path convolution\nblock to further improve the representation ability. We conduct experiments\nwith our Network on KITTI dataset and achieve good performance on 3D detection\ntask for both precision and speed. Our one-stage detector outperforms SECOND\nand PointPillars by a large margin and achieves the speed of 30 FPS.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:46:28 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 11:00:45 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 15:42:39 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Tian", "Yonglin", ""], ["Huang", "Lichao", ""], ["Li", "Xuesong", ""], ["Wang", "Kunfeng", ""], ["Wang", "Zilei", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1912.04783", "submitter": "Xavier Boix", "authors": "Stephen Casper, Xavier Boix, Vanessa D'Amario, Ling Guo, Martin\n  Schrimpf, Kasper Vinken, Gabriel Kreiman", "title": "Frivolous Units: Wider Networks Are Not Really That Wide", "comments": null, "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A remarkable characteristic of overparameterized deep neural networks (DNNs)\nis that their accuracy does not degrade when the network's width is increased.\nRecent evidence suggests that developing compressible representations is key\nfor adjusting the complexity of large networks to the learning task at hand.\nHowever, these compressible representations are poorly understood. A promising\nstrand of research inspired from biology is understanding representations at\nthe unit level as it offers a more granular and intuitive interpretation of the\nneural mechanisms. In order to better understand what facilitates increases in\nwidth without decreases in accuracy, we ask: Are there mechanisms at the unit\nlevel by which networks control their effective complexity as their width is\nincreased? If so, how do these depend on the architecture, dataset, and\ntraining parameters? We identify two distinct types of \"frivolous\" units that\nproliferate when the network's width is increased: prunable units which can be\ndropped out of the network without significant change to the output and\nredundant units whose activities can be expressed as a linear combination of\nothers. These units imply complexity constraints as the function the network\nrepresents could be expressed by a network without them. We also identify how\nthe development of these units can be influenced by architecture and a number\nof training factors. Together, these results help to explain why the accuracy\nof DNNs does not degrade when width is increased and highlight the importance\nof frivolous units toward understanding implicit regularization in DNNs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:53:45 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 19:41:16 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 16:20:23 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 02:56:07 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 23:42:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Casper", "Stephen", ""], ["Boix", "Xavier", ""], ["D'Amario", "Vanessa", ""], ["Guo", "Ling", ""], ["Schrimpf", "Martin", ""], ["Vinken", "Kasper", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1912.04799", "submitter": "Mingyu Ding", "authors": "Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu,\n  Ping Luo", "title": "Learning Depth-Guided Convolutions for Monocular 3D Object Detection", "comments": "12 pages, 8 figures, modify email and add code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection from a single image without LiDAR is a challenging task\ndue to the lack of accurate depth information. Conventional 2D convolutions are\nunsuitable for this task because they fail to capture local object and its\nscale information, which are vital for 3D object detection. To better represent\n3D structure, prior arts typically transform depth maps estimated from 2D\nimages into a pseudo-LiDAR representation, and then apply existing 3D\npoint-cloud based object detectors. However, their results depend heavily on\nthe accuracy of the estimated depth maps, resulting in suboptimal performance.\nIn this work, instead of using pseudo-LiDAR representation, we improve the\nfundamental 2D fully convolutions by proposing a new local convolutional\nnetwork (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D$^4$LCN),\nwhere the filters and their receptive fields can be automatically learned from\nimage-based depth maps, making different pixels of different images have\ndifferent filters. D$^4$LCN overcomes the limitation of conventional 2D\nconvolutions and narrows the gap between image representation and 3D point\ncloud representation. Extensive experiments show that D$^4$LCN outperforms\nexisting works by large margins. For example, the relative improvement of\nD$^4$LCN against the state-of-the-art on KITTI is 9.1\\% in the moderate\nsetting. The code is available at https://github.com/dingmyu/D4LCN.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:31:22 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 08:46:29 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ding", "Mingyu", ""], ["Huo", "Yuqi", ""], ["Yi", "Hongwei", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Lu", "Zhiwu", ""], ["Luo", "Ping", ""]]}, {"id": "1912.04801", "submitter": "Debaditya Roy", "authors": "Debaditya Roy, Tetsuhiro Ishizaka, Krishna Mohan C., and Atsushi\n  Fukuda", "title": "Detection of Collision-Prone Vehicle Behavior at Intersections using\n  Siamese Interaction LSTM", "comments": "10 pages, 4 figures, submitted to IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a large proportion of road accidents occur at intersections, monitoring\ntraffic safety of intersections is important. Existing approaches are designed\nto investigate accidents in lane-based traffic. However, such approaches are\nnot suitable in a lane-less mixed-traffic environment where vehicles often ply\nvery close to each other. Hence, we propose an approach called Siamese\nInteraction Long Short-Term Memory network (SILSTM) to detect collision prone\nvehicle behavior. The SILSTM network learns the interaction trajectory of a\nvehicle that describes the interactions of a vehicle with its neighbors at an\nintersection. Among the hundreds of interactions for every vehicle, there maybe\nonly some interactions which may be unsafe and hence, a temporal attention\nlayer is used in the SILSTM network. Furthermore, the comparison of interaction\ntrajectories requires labeling the trajectories as either unsafe or safe, but\nsuch a distinction is highly subjective, especially in lane-less traffic.\nHence, in this work, we compute the characteristics of interaction trajectories\ninvolved in accidents using the collision energy model. The interaction\ntrajectories that match accident characteristics are labeled as unsafe while\nthe rest are considered safe. Finally, there is no existing dataset that allows\nus to monitor a particular intersection for a long duration. Therefore, we\nintroduce the SkyEye dataset that contains 1 hour of continuous aerial footage\nfrom each of the 4 chosen intersections in the city of Ahmedabad in India. A\ndetailed evaluation of SILSTM on the SkyEye dataset shows that unsafe\n(collision-prone) interaction trajectories can be effectively detected at\ndifferent intersections.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:35:10 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Roy", "Debaditya", ""], ["Ishizaka", "Tetsuhiro", ""], ["C.", "Krishna Mohan", ""], ["Fukuda", "Atsushi", ""]]}, {"id": "1912.04838", "submitter": "Pei Sun", "authors": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard,\n  Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin\n  Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev,\n  Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang\n  Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov", "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research community has increasing interest in autonomous driving\nresearch, despite the resource intensity of obtaining representative real world\ndata. Existing self-driving datasets are limited in the scale and variation of\nthe environments they capture, even though generalization within and between\noperating regions is crucial to the overall viability of the technology. In an\neffort to help align the research community's contributions with real-world\nself-driving problems, we introduce a new large scale, high quality, diverse\ndataset. Our new dataset consists of 1150 scenes that each span 20 seconds,\nconsisting of well synchronized and calibrated high quality LiDAR and camera\ndata captured across a range of urban and suburban geographies. It is 15x more\ndiverse than the largest camera+LiDAR dataset available based on our proposed\ndiversity metric. We exhaustively annotated this data with 2D (camera image)\nand 3D (LiDAR) bounding boxes, with consistent identifiers across frames.\nFinally, we provide strong baselines for 2D as well as 3D detection and\ntracking tasks. We further study the effects of dataset size and generalization\nacross geographies on 3D detection methods. Find data, code and more up-to-date\ninformation at http://www.waymo.com/open.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:28:55 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 17:51:21 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 19:21:41 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 00:42:38 GMT"}, {"version": "v5", "created": "Wed, 18 Dec 2019 01:41:49 GMT"}, {"version": "v6", "created": "Mon, 30 Mar 2020 03:22:30 GMT"}, {"version": "v7", "created": "Tue, 12 May 2020 23:28:05 GMT"}], "update_date": "2020-05-24", "authors_parsed": [["Sun", "Pei", ""], ["Kretzschmar", "Henrik", ""], ["Dotiwalla", "Xerxes", ""], ["Chouard", "Aurelien", ""], ["Patnaik", "Vijaysai", ""], ["Tsui", "Paul", ""], ["Guo", "James", ""], ["Zhou", "Yin", ""], ["Chai", "Yuning", ""], ["Caine", "Benjamin", ""], ["Vasudevan", "Vijay", ""], ["Han", "Wei", ""], ["Ngiam", "Jiquan", ""], ["Zhao", "Hang", ""], ["Timofeev", "Aleksei", ""], ["Ettinger", "Scott", ""], ["Krivokon", "Maxim", ""], ["Gao", "Amy", ""], ["Joshi", "Aditya", ""], ["Zhao", "Sheng", ""], ["Cheng", "Shuyang", ""], ["Zhang", "Yu", ""], ["Shlens", "Jonathon", ""], ["Chen", "Zhifeng", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1912.04896", "submitter": "Damith Senanayake PhD", "authors": "Damith Senanayake, Wei Wang, Shalin H. Naik, Saman Halgamuge", "title": "Self Organizing Nebulous Growths for Robust and Incremental Data\n  Visualization", "comments": "in IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3023941.", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-parametric dimensionality reduction techniques, such as t-SNE and UMAP,\nare proficient in providing visualizations for datasets of fixed sizes.\nHowever, they cannot incrementally map and insert new data points into an\nalready provided data visualization. We present Self-Organizing Nebulous\nGrowths (SONG), a parametric nonlinear dimensionality reduction technique that\nsupports incremental data visualization, i.e., incremental addition of new data\nwhile preserving the structure of the existing visualization. In addition, SONG\nis capable of handling new data increments, no matter whether they are similar\nor heterogeneous to the already observed data distribution. We test SONG on a\nvariety of real and simulated datasets. The results show that SONG is superior\nto Parametric t-SNE, t-SNE and UMAP in incremental data visualization.\nSpecifically, for heterogeneous increments, SONG improves over Parametric t-SNE\nby 14.98 % on the Fashion MNIST dataset and 49.73% on the MNIST dataset\nregarding the cluster quality measured by the Adjusted Mutual Information\nscores. On similar or homogeneous increments, the improvements are 8.36% and\n42.26% respectively. Furthermore, even when the above datasets are presented\nall at once, SONG performs better or comparable to UMAP, and superior to t-SNE.\nWe also demonstrate that the algorithmic foundations of SONG render it more\ntolerant to noise compared to UMAP and t-SNE, thus providing greater utility\nfor data with high variance, high mixing of clusters, or noise.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 22:11:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 21:59:58 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 01:18:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Senanayake", "Damith", ""], ["Wang", "Wei", ""], ["Naik", "Shalin H.", ""], ["Halgamuge", "Saman", ""]]}, {"id": "1912.04898", "submitter": "Sasikanth Raghava", "authors": "Sasikanth Raghava Goteti", "title": "Modelling curvature of a bent paper leaf", "comments": "5 pages , 5 figures", "journal-ref": null, "doi": "10.7287/PEERJ.PREPRINTS.161", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we briefly describe various tools and approaches that\nalgebraic geometry has to offer to straighten bent objects. Throughout this\narticle we will consider a specific example of a bent or curved piece of paper\nwhich in our case acts very much like an elastica curve. We conclude this\narticle with a suggestion to algebraic geometry as a viable and fast\nperformance alternative of neural networks in vision and machine learning. The\npurpose of this article is not to build a full blown framework but to show\npossibility of using algebraic geometry as an alternative to neural networks\nfor recognizing or extracting features on manifolds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:10:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Goteti", "Sasikanth Raghava", ""]]}, {"id": "1912.04943", "submitter": "Georgi Tinchev", "authors": "Georgi Tinchev, Adrian Penate-Sanchez and Maurice Fallon", "title": "SKD: Keypoint Detection for Point Clouds using Saliency Estimation", "comments": "Accepted for publication at 2021 IEEE Robotics and Automation Letters\n  (RA-L) + IEEE International Conference on Robotics and Automation (ICRA)\n  presentation option. Video preview available here:\n  https://youtu.be/Wx6FEWCgWDk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SKD, a novel keypoint detector that uses saliency to determine the\nbest candidates from a point cloud for tasks such as registration and\nreconstruction. The approach can be applied to any differentiable deep learning\ndescriptor by using the gradients of that descriptor with respect to the 3D\nposition of the input points as a measure of their saliency. The saliency is\ncombined with the original descriptor and context information in a neural\nnetwork, which is trained to learn robust keypoint candidates. The key\nintuition behind this approach is that keypoints are not extracted solely as a\nresult of the geometry surrounding a point, but also take into account the\ndescriptor's response. The approach was evaluated on two large LIDAR datasets -\nthe Oxford RobotCar dataset and the KITTI dataset, where we obtain up to 50%\nimprovement over the state-of-the-art in both matchability and repeatability.\nWhen performing sparse matching with the keypoints computed by our method we\nachieve a higher inlier ratio and faster convergence.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:30:00 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 07:59:23 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 19:24:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tinchev", "Georgi", ""], ["Penate-Sanchez", "Adrian", ""], ["Fallon", "Maurice", ""]]}, {"id": "1912.04950", "submitter": "Ryan Szeto", "authors": "Ryan Szeto, Mostafa El-Khamy, Jungwon Lee, Jason J. Corso", "title": "HyperCon: Image-To-Video Model Transfer for Video-To-Video Translation\n  Tasks", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-to-video translation is more difficult than image-to-image translation\ndue to the temporal consistency problem that, if unaddressed, leads to\ndistracting flickering effects. Although video models designed from scratch\nproduce temporally consistent results, training them to match the vast visual\nknowledge captured by image models requires an intractable number of videos. To\ncombine the benefits of image and video models, we propose an image-to-video\nmodel transfer method called Hyperconsistency (HyperCon) that transforms any\nwell-trained image model into a temporally consistent video model without\nfine-tuning. HyperCon works by translating a temporally interpolated video\nframe-wise and then aggregating over temporally localized windows on the\ninterpolated video. It handles both masked and unmasked inputs, enabling\nsupport for even more video-to-video translation tasks than prior\nimage-to-video model transfer techniques. We demonstrate HyperCon on video\nstyle transfer and inpainting, where it performs favorably compared to prior\nstate-of-the-art methods without training on a single stylized or incomplete\nvideo. Our project website is available at\nhttps://ryanszeto.com/projects/hypercon .\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:47:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 16:18:34 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Szeto", "Ryan", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""], ["Corso", "Jason J.", ""]]}, {"id": "1912.04958", "submitter": "Samuli Laine", "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko\n  Lehtinen, Timo Aila", "title": "Analyzing and Improving the Image Quality of StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:44:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:21:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Karras", "Tero", ""], ["Laine", "Samuli", ""], ["Aittala", "Miika", ""], ["Hellsten", "Janne", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1912.04973", "submitter": "Debasmit Das", "authors": "Debasmit Das and C. S. George Lee", "title": "A Two-Stage Approach to Few-Shot Learning for Image Recognition", "comments": "To Appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2959254", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-layer neural network structure for few-shot image\nrecognition of novel categories. The proposed multi-layer neural network\narchitecture encodes transferable knowledge extracted from a large annotated\ndataset of base categories. This architecture is then applied to novel\ncategories containing only a few samples. The transfer of knowledge is carried\nout at the feature-extraction and the classification levels distributed across\nthe two training stages. In the first-training stage, we introduce the relative\nfeature to capture the structure of the data as well as obtain a\nlow-dimensional discriminative space. Secondly, we account for the variable\nvariance of different categories by using a network to predict the variance of\neach class. Classification is then performed by computing the Mahalanobis\ndistance to the mean-class representation in contrast to previous approaches\nthat used the Euclidean distance. In the second-training stage, a\ncategory-agnostic mapping is learned from the mean-sample representation to its\ncorresponding class-prototype representation. This is because the mean-sample\nrepresentation may not accurately represent the novel category prototype.\nFinally, we evaluate the proposed network structure on four standard few-shot\nimage recognition datasets, where our proposed few-shot learning system\nproduces competitive performance compared to previous work. We also extensively\nstudied and analyzed the contribution of each component of our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:45:35 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Das", "Debasmit", ""], ["Lee", "C. S. George", ""]]}, {"id": "1912.04976", "submitter": "Peiyun Hu", "authors": "Peiyun Hu, David Held, Deva Ramanan", "title": "Learning to Optimally Segment Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of class-agnostic instance segmentation of LiDAR\npoint clouds. We propose an approach that combines graph-theoretic search with\ndata-driven learning: it searches over a set of candidate segmentations and\nreturns one where individual segments score well according to a data-driven\npoint-based model of \"objectness\". We prove that if we score a segmentation by\nthe worst objectness among its individual segments, there is an efficient\nalgorithm that finds the optimal worst-case segmentation among an exponentially\nlarge number of candidate segmentations. We also present an efficient algorithm\nfor the average-case. For evaluation, we repurpose KITTI 3D detection as a\nsegmentation benchmark and empirically demonstrate that our algorithms\nsignificantly outperform past bottom-up segmentation approaches and top-down\nobject-based algorithms on segmenting point clouds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:53:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Hu", "Peiyun", ""], ["Held", "David", ""], ["Ramanan", "Deva", ""]]}, {"id": "1912.04979", "submitter": "Takuya Yoshioka", "authors": "Takuya Yoshioka, Igor Abramovski, Cem Aksoylar, Zhuo Chen, Moshe\n  David, Dimitrios Dimitriadis, Yifan Gong, Ilya Gurvich, Xuedong Huang, Yan\n  Huang, Aviv Hurvitz, Li Jiang, Sharon Koubi, Eyal Krupka, Ido Leichter,\n  Changliang Liu, Partha Parthasarathy, Alon Vinnikov, Lingfeng Wu, Xiong Xiao,\n  Wayne Xiong, Huaming Wang, Zhenghao Wang, Jun Zhang, Yong Zhao, Tianyan Zhou", "title": "Advances in Online Audio-Visual Meeting Transcription", "comments": "To appear in Proc. IEEE ASRU Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a system that generates speaker-annotated transcripts of\nmeetings by using a microphone array and a 360-degree camera. The hallmark of\nthe system is its ability to handle overlapped speech, which has been an\nunsolved problem in realistic settings for over a decade. We show that this\nproblem can be addressed by using a continuous speech separation approach. In\naddition, we describe an online audio-visual speaker diarization method that\nleverages face tracking and identification, sound source localization, speaker\nidentification, and, if available, prior speaker information for robustness to\nvarious real world challenges. All components are integrated in a meeting\ntranscription framework called SRD, which stands for \"separate, recognize, and\ndiarize\". Experimental results using recordings of natural meetings involving\nup to 11 attendees are reported. The continuous speech separation improves a\nword error rate (WER) by 16.1% compared with a highly tuned beamformer. When a\ncomplete list of meeting attendees is available, the discrepancy between WER\nand speaker-attributed WER is only 1.0%, indicating accurate word-to-speaker\nassociation. This increases marginally to 1.6% when 50% of the attendees are\nunknown to the system.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:59:24 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yoshioka", "Takuya", ""], ["Abramovski", "Igor", ""], ["Aksoylar", "Cem", ""], ["Chen", "Zhuo", ""], ["David", "Moshe", ""], ["Dimitriadis", "Dimitrios", ""], ["Gong", "Yifan", ""], ["Gurvich", "Ilya", ""], ["Huang", "Xuedong", ""], ["Huang", "Yan", ""], ["Hurvitz", "Aviv", ""], ["Jiang", "Li", ""], ["Koubi", "Sharon", ""], ["Krupka", "Eyal", ""], ["Leichter", "Ido", ""], ["Liu", "Changliang", ""], ["Parthasarathy", "Partha", ""], ["Vinnikov", "Alon", ""], ["Wu", "Lingfeng", ""], ["Xiao", "Xiong", ""], ["Xiong", "Wayne", ""], ["Wang", "Huaming", ""], ["Wang", "Zhenghao", ""], ["Zhang", "Jun", ""], ["Zhao", "Yong", ""], ["Zhou", "Tianyan", ""]]}, {"id": "1912.04981", "submitter": "Tobias Uelwer", "authors": "Tobias Uelwer, Alexander Oberstra{\\ss}, Stefan Harmeling", "title": "Phase Retrieval Using Conditional Generative Adversarial Networks", "comments": "Accepted at the 25th International Conference on Pattern Recognition\n  2020 (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the application of conditional generative\nadversarial networks to solve various phase retrieval problems. We show that\nincluding knowledge of the measurement process at training time leads to an\noptimization at test time that is more robust to initialization than existing\napproaches involving generative models. In addition, conditioning the generator\nnetwork on the measurements enables us to achieve much more detailed results.\nWe empirically demonstrate that these advantages provide meaningful solutions\nto the Fourier and the compressive phase retrieval problem and that our method\noutperforms well-established projection-based methods as well as existing\nmethods that are based on neural networks. Like other deep learning methods,\nour approach is very robust to noise and can therefore be very useful for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 21:03:59 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 07:37:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Uelwer", "Tobias", ""], ["Oberstra\u00df", "Alexander", ""], ["Harmeling", "Stefan", ""]]}, {"id": "1912.04986", "submitter": "Peiyun Hu", "authors": "Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan", "title": "What You See is What You Get: Exploiting Visibility for 3D Object\n  Detection", "comments": "CVPR'20. More at https://www.cs.cmu.edu/~peiyunh/wysiwyg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D sensing have created unique challenges for computer\nvision. One fundamental challenge is finding a good representation for 3D\nsensor data. Most popular representations (such as PointNet) are proposed in\nthe context of processing truly 3D data (e.g. points sampled from mesh models),\nignoring the fact that 3D sensored data such as a LiDAR sweep is in fact 2.5D.\nWe argue that representing 2.5D data as collections of (x, y, z) points\nfundamentally destroys hidden information about freespace. In this paper, we\ndemonstrate such knowledge can be efficiently recovered through 3D raycasting\nand readily incorporated into batch-based gradient learning. We describe a\nsimple approach to augmenting voxel-based networks with visibility: we add a\nvoxelized visibility map as an additional input stream. In addition, we show\nthat visibility can be combined with two crucial modifications common to\nstate-of-the-art 3D detectors: synthetic data augmentation of virtual objects\nand temporal aggregation of LiDAR sweeps over multiple time frames. On the\nNuScenes 3D detection benchmark, we show that, by adding an additional stream\nfor visibility input, we can significantly improve the overall detection\naccuracy of a state-of-the-art 3D detector.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 21:15:37 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 00:24:14 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 22:42:25 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hu", "Peiyun", ""], ["Ziglar", "Jason", ""], ["Held", "David", ""], ["Ramanan", "Deva", ""]]}, {"id": "1912.05000", "submitter": "Nadir Bengana", "authors": "Nadir Bengana and Janne Heikkil\\\"a", "title": "Improving land cover segmentation across satellites using domain\n  adaptation", "comments": "12 pages, Transaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use and land cover mapping are essential to various fields of study,\nincluding forestry, agriculture, and urban management. Using earth observation\nsatellites both facilitate and accelerate the task. Lately, deep learning\nmethods have proven to be excellent at automating the mapping via semantic\nimage segmentation. However, because deep neural networks require large amounts\nof labeled data, it is not easy to exploit the full potential of satellite\nimagery. Additionally, the land cover tends to differ in appearance from one\nregion to another; therefore, having labeled data from one location does not\nnecessarily help in mapping others. Furthermore, satellite images come in\nvarious multispectral bands (the bands could range from RGB to over twelve\nbands). In this paper, we aim at using domain adaptation to solve the\naforementioned problems. We applied a well-performing domain adaptation\napproach on datasets we have built using RGB images from Sentinel-2,\nWorldView-2, and Pleiades-1 satellites with Corine Land Cover as ground-truth\nlabels. We have also used the DeepGlobe land cover dataset. Experiments show a\nsignificant improvement over results obtained without the use of domain\nadaptation. In some cases, an improvement of over 20% MIoU. At times it even\nmanages to correct errors in the ground-truth labels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:41:14 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 14:34:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Bengana", "Nadir", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1912.05002", "submitter": "Boying Li", "authors": "Boying Li, Danping Zou, Daniele Sartori, Ling Pei, Wenxian Yu", "title": "TextSLAM: Visual SLAM with Planar Text Features", "comments": "Accepted by ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to integrate text objects in man-made scenes tightly into the\nvisual SLAM pipeline. The key idea of our novel text-based visual SLAM is to\ntreat each detected text as a planar feature which is rich of textures and\nsemantic meanings. The text feature is compactly represented by three\nparameters and integrated into visual SLAM by adopting the\nillumination-invariant photometric error. We also describe important details\ninvolved in implementing a full pipeline of text-based visual SLAM. To our best\nknowledge, this is the first visual SLAM method tightly coupled with the text\nfeatures. We tested our method in both indoor and outdoor environments. The\nresults show that with text features, the visual SLAM system becomes more\nrobust and produces much more accurate 3D text maps that could be useful for\nnavigation and scene understanding in robotic or augmented reality\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:10:25 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 15:16:21 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Li", "Boying", ""], ["Zou", "Danping", ""], ["Sartori", "Daniele", ""], ["Pei", "Ling", ""], ["Yu", "Wenxian", ""]]}, {"id": "1912.05003", "submitter": "Bo Chen", "authors": "Bo Chen, Decai Li, Yuqing He, Chunsheng Hua", "title": "SCR-Graph: Spatial-Causal Relationships based Graph Reasoning Network\n  for Human Action Prediction", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technologies to predict human actions are extremely important for\napplications such as human robot cooperation and autonomous driving. However, a\nmajority of the existing algorithms focus on exploiting visual features of the\nvideos and do not consider the mining of relationships, which include spatial\nrelationships between human and scene elements as well as causal relationships\nin temporal action sequences. In fact, human beings are good at using spatial\nand causal relational reasoning mechanism to predict the actions of others.\nInspired by this idea, we proposed a Spatial and Causal Relationship based\nGraph Reasoning Network (SCR-Graph), which can be used to predict human actions\nby modeling the action-scene relationship, and causal relationship between\nactions, in spatial and temporal dimensions respectively. Here, in spatial\ndimension, a hierarchical graph attention module is designed by iteratively\naggregating the features of different kinds of scene elements in different\nlevel. In temporal dimension, we designed a knowledge graph based causal\nreasoning module and map the past actions to temporal causal features through\nDiffusion RNN. Finally, we integrated the causality features into the\nheterogeneous graph in the form of shadow node, and introduced a self-attention\nmodule to determine the time when the knowledge graph information should be\nactivated. Extensive experimental results on the VIRAT datasets demonstrate the\nfavorable performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:54:40 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Chen", "Bo", ""], ["Li", "Decai", ""], ["He", "Yuqing", ""], ["Hua", "Chunsheng", ""]]}, {"id": "1912.05004", "submitter": "Xingchao Peng", "authors": "Yichen Li, Xingchao Peng", "title": "Learning Domain Adaptive Features with Unlabeled Domain Bridges", "comments": "Both authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional cross-domain image-to-image translation or unsupervised domain\nadaptation methods assume that the source domain and target domain are closely\nrelated. This neglects a practical scenario where the domain discrepancy\nbetween the source and target is excessively large. In this paper, we propose a\nnovel approach to learn domain adaptive features between the largely-gapped\nsource and target domains with unlabeled domain bridges. Firstly, we introduce\nthe framework of Cycle-consistency Flow Generative Adversarial Networks (CFGAN)\nthat utilizes domain bridges to perform image-to-image translation between two\ndistantly distributed domains. Secondly, we propose the Prototypical\nAdversarial Domain Adaptation (PADA) model which utilizes unlabeled bridge\ndomains to align feature distribution between source and target with a large\ndiscrepancy. Extensive quantitative and qualitative experiments are conducted\nto demonstrate the effectiveness of our proposed models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:07:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Li", "Yichen", ""], ["Peng", "Xingchao", ""]]}, {"id": "1912.05006", "submitter": "Zhenyu Weng", "authors": "Zhenyu Weng, Yuesheng Zhu", "title": "Efficient Querying from Weighted Binary Codes", "comments": "13 pages, accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary codes are widely used to represent the data due to their small storage\nand efficient computation. However, there exists an ambiguity problem that lots\nof binary codes share the same Hamming distance to a query. To alleviate the\nambiguity problem, weighted binary codes assign different weights to each bit\nof binary codes and compare the binary codes by the weighted Hamming distance.\nTill now, performing the querying from the weighted binary codes efficiently is\nstill an open issue. In this paper, we propose a new method to rank the\nweighted binary codes and return the nearest weighted binary codes of the query\nefficiently. In our method, based on the multi-index hash tables, two\nalgorithms, the table bucket finding algorithm and the table merging algorithm,\nare proposed to select the nearest weighted binary codes of the query in a\nnon-exhaustive and accurate way. The proposed algorithms are justified by\nproving their theoretic properties. The experiments on three large-scale\ndatasets validate both the search efficiency and the search accuracy of our\nmethod. Especially for the number of weighted binary codes up to one billion,\nour method shows a great improvement of more than 1000 times faster than the\nlinear scan.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 06:48:29 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 02:41:13 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Weng", "Zhenyu", ""], ["Zhu", "Yuesheng", ""]]}, {"id": "1912.05007", "submitter": "Alexander Ziller", "authors": "Alexander Ziller, Julius Hansjakob, Vitalii Rusinov, Daniel Z\\\"ugner,\n  Peter Vogel, Stephan G\\\"unnemann", "title": "Oktoberfest Food Dataset", "comments": "Dataset publication of Oktoberfest Food Dataset. 4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We release a realistic, diverse, and challenging dataset for object detection\non images. The data was recorded at a beer tent in Germany and consists of 15\ndifferent categories of food and drink items. We created more than 2,500 object\nannotations by hand for 1,110 images captured by a video camera above the\ncheckout. We further make available the remaining 600GB of (unlabeled) data\ncontaining days of footage. Additionally, we provide our trained models as a\nbenchmark. Possible applications include automated checkout systems which could\nsignificantly speed up the process.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:28:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ziller", "Alexander", ""], ["Hansjakob", "Julius", ""], ["Rusinov", "Vitalii", ""], ["Z\u00fcgner", "Daniel", ""], ["Vogel", "Peter", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1912.05008", "submitter": "Desmond Ong", "authors": "Desmond C. Ong, Zhengxuan Wu, Tan Zhi-Xuan, Marianne Reddan, Isabella\n  Kahhale, Alison Mattek, Jamil Zaki", "title": "Modeling emotion in complex stories: the Stanford Emotional Narratives\n  Dataset", "comments": "16 pages, 7 figures; accepted for publication at IEEE Transactions on\n  Affective Computing", "journal-ref": null, "doi": "10.1109/TAFFC.2019.2955949", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotions unfold over time, and more affective computing research has to\nprioritize capturing this crucial component of real-world affect. Modeling\ndynamic emotional stimuli requires solving the twin challenges of time-series\nmodeling and of collecting high-quality time-series datasets. We begin by\nassessing the state-of-the-art in time-series emotion recognition, and we\nreview contemporary time-series approaches in affective computing, including\ndiscriminative and generative models. We then introduce the first version of\nthe Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal\nvideos of self-paced, unscripted emotional narratives, annotated for emotional\nvalence over time. The complex narratives and naturalistic expressions in this\ndataset provide a challenging test for contemporary time-series emotion\nrecognition models. We demonstrate several baseline and state-of-the-art\nmodeling approaches on the SEND, including a Long Short-Term Memory model and a\nmultimodal Variational Recurrent Neural Network, which perform comparably to\nthe human-benchmark. We end by discussing the implications for future research\nin time-series affective computing.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:55:08 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ong", "Desmond C.", ""], ["Wu", "Zhengxuan", ""], ["Zhi-Xuan", "Tan", ""], ["Reddan", "Marianne", ""], ["Kahhale", "Isabella", ""], ["Mattek", "Alison", ""], ["Zaki", "Jamil", ""]]}, {"id": "1912.05011", "submitter": "Felix Biessmann", "authors": "Felix Biessmann and Dionysius Irza Refiano", "title": "A psychophysics approach for quantitative comparison of interpretable\n  computer vision models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of transparent Machine Learning (ML) has contributed many novel\nmethods aiming at better interpretability for computer vision and ML models in\ngeneral. But how useful the explanations provided by transparent ML methods are\nfor humans remains difficult to assess. Most studies evaluate interpretability\nin qualitative comparisons, they use experimental paradigms that do not allow\nfor direct comparisons amongst methods or they report only offline experiments\nwith no humans in the loop. While there are clear advantages of evaluations\nwith no humans in the loop, such as scalability, reproducibility and less\nalgorithmic bias than with humans in the loop, these metrics are limited in\ntheir usefulness if we do not understand how they relate to other metrics that\ntake human cognition into account. Here we investigate the quality of\ninterpretable computer vision algorithms using techniques from psychophysics.\nIn crowdsourced annotation tasks we study the impact of different\ninterpretability approaches on annotation accuracy and task time. In order to\nrelate these findings to quality measures for interpretability without humans\nin the loop we compare quality metrics with and without humans in the loop. Our\nresults demonstrate that psychophysical experiments allow for robust quality\nassessment of transparency in machine learning. Interestingly the quality\nmetrics computed without humans in the loop did not provide a consistent\nranking of interpretability methods nor were they representative for how useful\nan explanation was for humans. These findings highlight the potential of\nmethods from classical psychophysics for modern machine learning applications.\nWe hope that our results provide convincing arguments for evaluating\ninterpretability in its natural habitat, human-ML interaction, if the goal is\nto obtain an authentic assessment of interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 16:51:20 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Biessmann", "Felix", ""], ["Refiano", "Dionysius Irza", ""]]}, {"id": "1912.05012", "submitter": "Yaoyu Hu", "authors": "Yaoyu Hu, Weikun Zhen and Sebastian Scherer", "title": "Deep-Learning Assisted High-Resolution Binocular Stereo Depth\n  Reconstruction", "comments": "Submitted to International Conference on Robotics and Automation\n  (ICRA2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents dense stereo reconstruction using high-resolution images\nfor infrastructure inspections. The state-of-the-art stereo reconstruction\nmethods, both learning and non-learning ones, consume too much computational\nresource on high-resolution data. Recent learning-based methods achieve top\nranks on most benchmarks. However, they suffer from the generalization issue\ndue to lack of task-specific training data. We propose to use a less resource\ndemanding non-learning method, guided by a learning-based model, to handle\nhigh-resolution images and achieve accurate stereo reconstruction. The\ndeep-learning model produces an initial disparity prediction with uncertainty\nfor each pixel of the down-sampled stereo image pair. The uncertainty serves as\na self-measurement of its generalization ability and the per-pixel searching\nrange around the initially predicted disparity. The downstream process performs\na modified version of the Semi-Global Block Matching method with the up-sampled\nper-pixel searching range. The proposed deep-learning assisted method is\nevaluated on the Middlebury dataset and high-resolution stereo images collected\nby our customized binocular stereo camera. The combination of learning and\nnon-learning methods achieves better performance on 12 out of 15 cases of the\nMiddlebury dataset. In our infrastructure inspection experiments, the average\n3D reconstruction error is less than 0.004m.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 00:55:28 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 20:11:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hu", "Yaoyu", ""], ["Zhen", "Weikun", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1912.05013", "submitter": "Huai Yu", "authors": "Huai Yu, Weikun Zhen, Wen Yang and Sebastian Scherer", "title": "Line-based Camera Pose Estimation in Point Cloud of Structured\n  Environments", "comments": "8 pages, Work has been submitted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate registration of 2D imagery with point clouds is a key technology for\nimage-LiDAR point cloud fusion, camera to laser scanner calibration and camera\nlocalization. Despite continuous improvements, automatic registration of 2D and\n3D data without using additional textured information still faces great\nchallenges. In this paper, we propose a new 2D-3D registration method to\nestimate 2D-3D line feature correspondences and the camera pose in untextured\npoint clouds of structured environments. Specifically, we first use geometric\nconstraints between vanishing points and 3D parallel lines to compute all\nfeasible camera rotations. Then, we utilize a hypothesis testing strategy to\nestimate the 2D-3D line correspondences and the translation vector. By checking\nthe consistency with computed correspondences, the best rotation matrix can be\nfound. Finally, the camera pose is further refined using non-linear\noptimization with all the 2D-3D line correspondences. The experimental results\ndemonstrate the effectiveness of the proposed method on the synthetic and real\ndataset (outdoors and indoors) with repeated structures and rapid depth\nchanges.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 00:58:46 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 02:29:29 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yu", "Huai", ""], ["Zhen", "Weikun", ""], ["Yang", "Wen", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1912.05014", "submitter": "Sayan Nag", "authors": "Mayukh Bhattacharyya, Sayan Nag", "title": "Hybrid Style Siamese Network: Incorporating style loss in complementary\n  apparels retrieval", "comments": "Paper Accepted in the Third Workshop on Computer Vision for Fashion,\n  Art and Design, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Retrieval grows to be an integral part of fashion e-commerce ecosystem\nas it keeps expanding in multitudes. Other than the retrieval of visually\nsimilar items, the retrieval of visually compatible or complementary items is\nalso an important aspect of it. Normal Siamese Networks tend to work well on\ncomplementary items retrieval. But it fails to identify low level style\nfeatures which make items compatible in human eyes. These low level style\nfeatures are captured to a large extent in techniques used in neural style\ntransfer. This paper proposes a mechanism of utilising those methods in this\nretrieval task and capturing the low level style features through a hybrid\nsiamese network coupled with a hybrid loss. The experimental results indicate\nthat the proposed method outperforms traditional siamese networks in retrieval\ntasks for complementary items.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 05:56:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 23:48:47 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bhattacharyya", "Mayukh", ""], ["Nag", "Sayan", ""]]}, {"id": "1912.05015", "submitter": "Wilson Yan", "authors": "Wilson Yan, Jonathan Ho, Pieter Abbeel", "title": "Natural Image Manipulation for Autoregressive Models Using Fisher Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive models are one of the most powerful models that exist\ntoday which achieve state-of-the-art bits per dim. However, they lie at a\nstrict disadvantage when it comes to controlled sample generation compared to\nlatent variable models. Latent variable models such as VAEs and normalizing\nflows allow meaningful semantic manipulations in latent space, which\nautoregressive models do not have. In this paper, we propose using Fisher\nscores as a method to extract embeddings from an autoregressive model to use\nfor interpolation and show that our method provides more meaningful sample\nmanipulation compared to alternate embeddings such as network activations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 05:31:30 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 03:21:32 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yan", "Wilson", ""], ["Ho", "Jonathan", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1912.05016", "submitter": "Saman Fahandezh-Saadi", "authors": "Saman Fahandezh-Saadi, Di Wang, Masayoshi Tomizuka", "title": "Robust Feature-Based Point Registration Using Directional Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust probabilistic point registration method for\nestimating the rigid transformation (i.e. rotation matrix and translation\nvector) between two pointcloud dataset. The method improves the robustness of\npoint registration and consequently the robot localization in the presence of\noutliers in the pointclouds which always occurs due to occlusion, dynamic\nobjects, and sensor errors. The framework models the point registration task\nbased on directional statistics on a unit sphere. In particular, a Kent\ndistribution mixture model is adopted and the process of point registration has\nbeen carried out in the two phases of Expectation-Maximization algorithm. The\nproposed method has been evaluated on the pointcloud dataset from LiDAR sensors\nin an indoor environment.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:37:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Fahandezh-Saadi", "Saman", ""], ["Wang", "Di", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1912.05017", "submitter": "Sannidhan M S", "authors": "Sannidhan MS, Sunil Kumar Aithal and Abhir Bhandary", "title": "A Comprehensive Review On Various State Of Art Techniques For Eye Blink\n  Detection", "comments": "International Journal of Current Engineering and Scientific Research\n  (IJCESR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision is considered to be one of the most important areas in\nresearch and has focused on developing many applications that has proved to be\nuseful for both research and societal benefits. Today we have been witnessing\nmany of the road mishaps happening just because of the lack of concentration\nwhile driving.As a part of avoiding this kind of disaster happening in day to\nday life there are many technologies focusing on keeping track of the vehicle\ndrivers concentration.One such technology uses the method of eye blink\ndetection to find out the concentration level of the driver.With the advent of\nmany high end camera devices with cost effectiveness factor today it has become\nmore efficient and cheaper to use eye blink detection for keeping track of the\nconcentration level of the driver.Hence this paper presents an exhaustive\nreview on the implementations of various eye blink detection algorithms.The\ndetection system has also extended its application in various other fields like\ndrowsiness detection and fatigue detection and expression detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 03:23:42 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["MS", "Sannidhan", ""], ["Aithal", "Sunil Kumar", ""], ["Bhandary", "Abhir", ""]]}, {"id": "1912.05018", "submitter": "Enes Altinisik", "authors": "Enes Altinisik, Husrev Taha Sencar", "title": "Source Camera Verification from Strongly Stabilized Videos", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2020.3016830", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stabilization performed during imaging and/or post-processing poses one\nof the most significant challenges to photo-response non-uniformity based\nsource camera attribution from videos. When performed digitally, stabilization\ninvolves cropping, warping, and inpainting of video frames to eliminate\nunwanted camera motion. Hence, successful attribution requires the inversion of\nthese transformations in a blind manner. To address this challenge, we\nintroduce a source camera verification method for videos that takes into\naccount the spatially variant nature of stabilization transformations and\nassumes a larger degree of freedom in their search. Our method identifies\ntransformations at a sub-frame level, incorporates a number of constraints to\nvalidate their correctness, and offers computational flexibility in the search\nfor the correct transformation. The method also adopts a holistic approach in\ncountering disruptive effects of other video generation steps, such as video\ncoding and downsizing, for more reliable attribution. Tests performed on one\npublic and two custom datasets show that the proposed method is able to verify\nthe source of 23-30% of all videos that underwent stronger stabilization,\ndepending on computation load, without a significant impact on false\nattribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 14:30:52 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 09:39:13 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 10:58:58 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 08:43:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Altinisik", "Enes", ""], ["Sencar", "Husrev Taha", ""]]}, {"id": "1912.05019", "submitter": "Emmanuel Iarussi", "authors": "Pablo Navarro, Jos\\'e Ignacio Orlando, Claudio Delrieux, and Emmanuel\n  Iarussi", "title": "SketchZooms: Deep multi-view descriptors for matching line drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding point-wise correspondences between images is a long-standing problem\nin image analysis. This becomes particularly challenging for sketch images, due\nto the varying nature of human drawing style, projection distortions and\nviewport changes. In this paper we present the first attempt to obtain a\nlearned descriptor for dense registration in line drawings. Based on recent\ndeep learning techniques for corresponding photographs, we designed descriptors\nto locally match image pairs where the object of interest belongs to the same\nsemantic category, yet still differ drastically in shape, form, and projection\nangle. To this end, we have specifically crafted a data set of synthetic\nsketches using non-photorealistic rendering over a large collection of\npart-based registered 3D models. After training, a neural network generates\ndescriptors for every pixel in an input image, which are shown to generalize\ncorrectly in unseen sketches hand-drawn by humans. We evaluate our method\nagainst a baseline of correspondences data collected from expert designers, in\naddition to comparisons with other descriptors that have been proven effective\nin sketches. Code, data and further resources will be publicly released by the\ntime of publication.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:31:33 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 19:50:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Navarro", "Pablo", ""], ["Orlando", "Jos\u00e9 Ignacio", ""], ["Delrieux", "Claudio", ""], ["Iarussi", "Emmanuel", ""]]}, {"id": "1912.05020", "submitter": "Sebastian Risi", "authors": "Nicola Zaltron, Luisa Zurlo, Sebastian Risi", "title": "CG-GAN: An Interactive Evolutionary GAN-based Approach for Facial\n  Composite Generation)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial composites are graphical representations of an eyewitness's memory of\na face. Many digital systems are available for the creation of such composites\nbut are either unable to reproduce features unless previously designed or do\nnot allow holistic changes to the image. In this paper, we improve the\nefficiency of composite creation by removing the reliance on expert knowledge\nand letting the system learn to represent faces from examples. The novel\napproach, Composite Generating GAN (CG-GAN), applies generative and\nevolutionary computation to allow casual users to easily create facial\ncomposites. Specifically, CG-GAN utilizes the generator network of a pg-GAN to\ncreate high-resolution human faces. Users are provided with several functions\nto interactively breed and edit faces. CG-GAN offers a novel way of generating\nand handling static and animated photo-realistic facial composites, with the\npossibility of combining multiple representations of the same perpetrator,\ngenerated by different eyewitnesses.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 21:08:43 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zaltron", "Nicola", ""], ["Zurlo", "Luisa", ""], ["Risi", "Sebastian", ""]]}, {"id": "1912.05021", "submitter": "Xiao Yang", "authors": "Xiao Yang, Fangyun Wei, Hongyang Zhang, Jun Zhu", "title": "Design and Interpretation of Universal Adversarial Patches in Face\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider universal adversarial patches for faces -- small visual elements\nwhose addition to a face image reliably destroys the performance of face\ndetectors. Unlike previous work that mostly focused on the algorithmic design\nof adversarial examples in terms of improving the success rate as an attacker,\nin this work we show an interpretation of such patches that can prevent the\nstate-of-the-art face detectors from detecting the real faces. We investigate a\nphenomenon: patches designed to suppress real face detection appear face-like.\nThis phenomenon holds generally across different initialization, locations,\nscales of patches, backbones, and state-of-the-art face detection frameworks.\nWe propose new optimization-based approaches to automatic design of universal\nadversarial patches for varying goals of the attack, including scenarios in\nwhich true positives are suppressed without introducing false positives. Our\nproposed algorithms perform well on real-world datasets, deceiving\nstate-of-the-art face detectors in terms of multiple precision/recall metrics\nand transferability.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 12:43:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:00:41 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 09:37:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yang", "Xiao", ""], ["Wei", "Fangyun", ""], ["Zhang", "Hongyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1912.05023", "submitter": "Dong Han", "authors": "Dong Han, Zuhao Zou, Lujia Wang and Cheng-Zhong Xu", "title": "A Robust Stereo Camera Localization Method with Prior LiDAR Map\n  Constrains", "comments": "6 pages, 5 figures, The 2019 IEEE International Conference on\n  Robotics and Biomimetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex environments, low-cost and robust localization is a challenging\nproblem. For example, in a GPSdenied environment, LiDAR can provide accurate\nposition information, but the cost is high. In general, visual SLAM based\nlocalization methods become unreliable when the sunlight changes greatly.\nTherefore, inexpensive and reliable methods are required. In this paper, we\npropose a stereo visual localization method based on the prior LiDAR map.\nDifferent from the conventional visual localization system, we design a novel\nvisual optimization model by matching planar information between the LiDAR map\nand visual image. Bundle adjustment is built by using coplanarity constraints.\nTo solve the optimization problem, we use a graph-based optimization algorithm\nand a local window optimization method. Finally, we estimate a full six degrees\nof freedom (DOF) pose without scale drift. To validate the efficiency, the\nproposed method has been tested on the KITTI dataset. The results show that our\nmethod is more robust and accurate than the state-of-art ORB-SLAM2.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:17:44 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Han", "Dong", ""], ["Zou", "Zuhao", ""], ["Wang", "Lujia", ""], ["Xu", "Cheng-Zhong", ""]]}, {"id": "1912.05024", "submitter": "Yulin Yan", "authors": "Yulin Yan and Youngryel Ryu", "title": "Google street view and deep learning: a new ground truthing approach for\n  crop mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground referencing is essential for supervised crop mapping. However,\nconventional ground truthing involves extensive field surveys and post\nprocessing, which is costly in terms of time and labor. In this study, we\napplied a convolutional neural network (CNN) model to explore the efficacy of\nautomatic ground truthing via Google street view (GSV) images in two distinct\nfarming regions: central Illinois and southern California. We demonstrated the\nfeasibility and reliability of the new ground referencing technique further by\nperforming pixel-based crop mapping with vegetation indices as the model input.\nThe results were evaluated using the United States Department of Agriculture\n(USDA) crop data layer (CDL) products. From 8,514 GSV images, the CNN model\nscreened out 2,645 target crop images. These images were well classified into\ncrop types, including alfalfa, almond, corn, cotton, grape, soybean, and\npistachio. The overall GSV image classification accuracy reached 93% in\nCalifornia and 97% in Illinois. We then shifted the image geographic\ncoordinates using fixed empirical coefficients to produce 8,173 crop reference\npoints including 1,764 in Illinois and 6,409 in California. Evaluation of these\nnew reference points with CDL products showed satisfactory coherence, with 94\nto 97% agreement. CNN-based mapping also captured the general pattern of crop\ntype distributions. The overall differences between CDL products and our\nmapping results were 4% in California and 5% in Illinois. Thus, using these\ndeep learning and GSV image techniques, we have provided an efficient and\ncost-effective alternative method for ground referencing and crop mapping.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:41:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yan", "Yulin", ""], ["Ryu", "Youngryel", ""]]}, {"id": "1912.05026", "submitter": "Stefan Oehmcke", "authors": "Stefan Oehmcke, Christoffer Thrys{\\o}e, Andreas Borgstad, Marcos\n  Antonio Vaz Salles, Martin Brandt, Fabian Gieseke", "title": "Detecting Hardly Visible Roads in Low-Resolution Satellite Time Series\n  Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amounts of satellite data have been gathered over time, holding the\npotential to unveil a spatiotemporal chronicle of the surface of Earth. These\ndata allow scientists to investigate various important issues, such as land use\nchanges, on a global scale. However, not all land-use phenomena are equally\nvisible on satellite imagery. In particular, the creation of an inventory of\nthe planet's road infrastructure remains a challenge, despite being crucial to\nanalyze urbanization patterns and their impact. Towards this end, this work\nadvances data-driven approaches for the automatic identification of roads based\non open satellite data. Given the typical resolutions of these historical\nsatellite data, we observe that there is inherent variation in the visibility\nof different road types. Based on this observation, we propose two deep\nlearning frameworks that extend state-of-the-art deep learning methods by\nformalizing road detection as an ordinal classification task. In contrast to\nrelated schemes, one of the two models also resorts to satellite time series\ndata that are potentially affected by missing data and cloud occlusion. Taking\nthese time series data into account eliminates the need to manually curate\ndatasets of high-quality image tiles, substantially simplifying the application\nof such models on a global scale. We evaluate our approaches on a dataset that\nis based on Sentinel~2 satellite imagery and OpenStreetMap vector data. Our\nresults indicate that the proposed models can successfully identify large and\nmedium-sized roads. We also discuss opportunities and challenges related to the\ndetection of roads and other infrastructure on a global scale.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:40:43 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Oehmcke", "Stefan", ""], ["Thrys\u00f8e", "Christoffer", ""], ["Borgstad", "Andreas", ""], ["Salles", "Marcos Antonio Vaz", ""], ["Brandt", "Martin", ""], ["Gieseke", "Fabian", ""]]}, {"id": "1912.05027", "submitter": "Xianzhi Du", "authors": "Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan,\n  Yin Cui, Quoc V. Le, Xiaodan Song", "title": "SpineNet: Learning Scale-Permuted Backbone for Recognition and\n  Localization", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks typically encode an input image into a series\nof intermediate features with decreasing resolutions. While this structure is\nsuited to classification tasks, it does not perform well for tasks requiring\nsimultaneous recognition and localization (e.g., object detection). The\nencoder-decoder architectures are proposed to resolve this by applying a\ndecoder network onto a backbone model designed for classification tasks. In\nthis paper, we argue encoder-decoder architecture is ineffective in generating\nstrong multi-scale features because of the scale-decreased backbone. We propose\nSpineNet, a backbone with scale-permuted intermediate features and cross-scale\nconnections that is learned on an object detection task by Neural Architecture\nSearch. Using similar building blocks, SpineNet models outperform ResNet-FPN\nmodels by ~3% AP at various scales while using 10-20% fewer FLOPs. In\nparticular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and\nachieves 52.1% AP with a RetinaNet detector on COCO for a single model without\ntest-time augmentation, significantly outperforms prior art of detectors.\nSpineNet can transfer to classification tasks, achieving 5% top-1 accuracy\nimprovement on a challenging iNaturalist fine-grained dataset. Code is at:\nhttps://github.com/tensorflow/tpu/tree/master/models/official/detection.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:13:42 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 18:24:21 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 16:37:15 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Du", "Xianzhi", ""], ["Lin", "Tsung-Yi", ""], ["Jin", "Pengchong", ""], ["Ghiasi", "Golnaz", ""], ["Tan", "Mingxing", ""], ["Cui", "Yin", ""], ["Le", "Quoc V.", ""], ["Song", "Xiaodan", ""]]}, {"id": "1912.05028", "submitter": "Gunjan Aggarwal", "authors": "Gunjan Aggarwal, Abhishek Sinha", "title": "cFineGAN: Unsupervised multi-conditional fine-grained image generation", "comments": "Accepted at NeurIPS Workshop on Machine Learning for Creativity and\n  Design 3.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised multi-conditional image generation pipeline:\ncFineGAN, that can generate an image conditioned on two input images such that\nthe generated image preserves the texture of one and the shape of the other\ninput. To achieve this goal, we extend upon the recently proposed work of\nFineGAN \\citep{singh2018finegan} and make use of standard as well as\nshape-biased pre-trained ImageNet models. We demonstrate both qualitatively as\nwell as quantitatively the benefit of using the shape-biased network. We\npresent our image generation result across three benchmark datasets-\nCUB-200-2011, Stanford Dogs and UT Zappos50k.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 04:16:08 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Aggarwal", "Gunjan", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1912.05029", "submitter": "Luca Erculiani Mr", "authors": "Luca Erculiani and Fausto Giunchiglia and Andrea Passerini", "title": "Continual egocentric object recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework capable of tackilng the problem of continual object\nrecognition in a setting which resembles that under whichhumans see and learn.\nThis setting has a set of unique characteristics:it assumes an egocentric\npoint-of-view bound to the needs of a singleperson, which implies a relatively\nlow diversity of data and a coldstart with no data; it requires to operate in\nan open world, where newobjects can be encounteredat any time; supervision is\nscarce and hasto be solicited to the user, and completelyunsupervised\nrecognitionof new objects should be possible. Note that this setting differs\nfromthe one addressed in the open world recognition literature, where\nsupervised feedback is always requested to be able to incorporate newobjects.\nWe propose a first solution to this problem in the form ofa memory-based\nincremental framework that is capable of storinginformation of each and any\nobject it encounters, while using the supervision of the user to learn to\ndiscriminate between known and unknown objects. Our approach is based on four\nmain features: the useof time and space persistence (i.e., the appearance of\nobjects changesrelatively slowly), the use of similarity as the main driving\nprinciplefor object recognition and novelty detection, the progressive\nintroduction of new objects in a developmental fashion and the\nselectiveelicitation of user feedback in an online active learning fashion.\nExperimental results show the feasibility of open world, generic\nobjectrecognition, the ability to recognize, memorize and re-identify\nnewobjects even in complete absence of user supervision, and the utilityof\npersistence and incrementality in boosting performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 12:10:59 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 15:22:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Erculiani", "Luca", ""], ["Giunchiglia", "Fausto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1912.05035", "submitter": "Luisa Polania", "authors": "Maria Ximena Bastidas Rodriguez, Adrien Gruson, Luisa F. Polania, Shin\n  Fujieda, Flavio Prieto Ortiz, Kohei Takayama, Toshiya Hachisuka", "title": "Deep Adaptive Wavelet Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though convolutional neural networks have become the method of choice in\nmany fields of computer vision, they still lack interpretability and are\nusually designed manually in a cumbersome trial-and-error process. This paper\naims at overcoming those limitations by proposing a deep neural network, which\nis designed in a systematic fashion and is interpretable, by integrating\nmultiresolution analysis at the core of the deep neural network design. By\nusing the lifting scheme, it is possible to generate a wavelet representation\nand design a network capable of learning wavelet coefficients in an end-to-end\nform. Compared to state-of-the-art architectures, the proposed model requires\nless hyper-parameter tuning and achieves competitive accuracy in image\nclassification tasks\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:43:16 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Rodriguez", "Maria Ximena Bastidas", ""], ["Gruson", "Adrien", ""], ["Polania", "Luisa F.", ""], ["Fujieda", "Shin", ""], ["Ortiz", "Flavio Prieto", ""], ["Takayama", "Kohei", ""], ["Hachisuka", "Toshiya", ""]]}, {"id": "1912.05067", "submitter": "Sanja \\v{S}\\'cepanovi\\'c", "authors": "Sanja \\v{S}\\'cepanovi\\'c, Oleg Antropov, Pekka Laurila, Yrj\\\"o Rauste,\n  Vladimir Ignatenko, Jaan Praks", "title": "Wide-Area Land Cover Mapping with Sentinel-1 Imagery using Deep Learning\n  Semantic Segmentation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land cover mapping is essential to monitoring the environment and\nunderstanding the effects of human activities on it. The automatic approaches\nto land cover mapping (i.e., image segmentation) mostly used traditional\nmachine learning that requires heuristic feature design. On natural images,\ndeep learning has outperformed traditional machine learning approaches for\nimage segmentation. On remote sensing images, recent studies demonstrate\nsuccessful applications of specific deep learning models to small-scale land\ncover mapping tasks (e.g., to classify wetland complexes). However, it is not\nreadily clear which of the existing models are the best candidates for which\nremote sensing task. In this study, we answer that question for mapping the\nfundamental land cover classes using satellite radar data. We took Sentinel-1\nC-band SAR images available at no cost to users as representative data. CORINE\nland cover map was used as a reference, and the models were trained to\ndistinguish between the 5 major CORINE classes. We selected seven among the\nstate-of-the-art semantic segmentation models so that they cover a diverse set\nof approaches: U-Net, DeepLabV3+, PSPNet, BiSeNet, SegNet, FC-DenseNet, and\nFRRN-B. The models were pre-trained on the ImageNet dataset and further\nfine-tuned in this study. All the models demonstrated solid performance with\noverall accuracy between 87.9% and 93.1%, and with good to a very good\nagreement (kappa statistic between 0.75 and 0.86). The two best models were\nFC-DenseNet and SegNet, with the latter having a much smaller inference time.\nOverall, our results indicate that the semantic segmentation models are\nsuitable for efficient wide-area mapping using satellite SAR imagery and also\nprovide baseline accuracy against which the newly proposed models should be\nevaluated.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 00:38:37 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 17:33:54 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 21:32:17 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 14:05:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["\u0160\u0107epanovi\u0107", "Sanja", ""], ["Antropov", "Oleg", ""], ["Laurila", "Pekka", ""], ["Rauste", "Yrj\u00f6", ""], ["Ignatenko", "Vladimir", ""], ["Praks", "Jaan", ""]]}, {"id": "1912.05070", "submitter": "Shaoru Wang", "authors": "Shaoru Wang, Yongchao Gong, Junliang Xing, Lichao Huang, Chang Huang,\n  Weiming Hu", "title": "RDSNet: A New Deep Architecture for Reciprocal Object Detection and\n  Instance Segmentation", "comments": "Accepted by AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and instance segmentation are two fundamental computer\nvision tasks. They are closely correlated but their relations have not yet been\nfully explored in most previous work. This paper presents RDSNet, a novel deep\narchitecture for reciprocal object detection and instance segmentation. To\nreciprocate these two tasks, we design a two-stream structure to learn features\non both the object level (i.e., bounding boxes) and the pixel level (i.e.,\ninstance masks) jointly. Within this structure, information from the two\nstreams is fused alternately, namely information on the object level introduces\nthe awareness of instance and translation variance to the pixel level, and\ninformation on the pixel level refines the localization accuracy of objects on\nthe object level in return. Specifically, a correlation module and a cropping\nmodule are proposed to yield instance masks, as well as a mask based boundary\nrefinement module for more accurate bounding boxes. Extensive experimental\nanalyses and comparisons on the COCO dataset demonstrate the effectiveness and\nefficiency of RDSNet. The source code is available at\nhttps://github.com/wangsr126/RDSNet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 01:10:33 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Shaoru", ""], ["Gong", "Yongchao", ""], ["Xing", "Junliang", ""], ["Huang", "Lichao", ""], ["Huang", "Chang", ""], ["Hu", "Weiming", ""]]}, {"id": "1912.05074", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming\n  Liang", "title": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in\n  Image Segmentation", "comments": "Journal of IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art models for medical image segmentation are variants of\nU-Net and fully convolutional networks (FCN). Despite their success, these\nmodels have two limitations: (1) their optimal depth is apriori unknown,\nrequiring extensive architecture search or inefficient ensemble of models of\nvarying depths; and (2) their skip connections impose an unnecessarily\nrestrictive fusion scheme, forcing aggregation only at the same-scale feature\nmaps of the encoder and decoder sub-networks. To overcome these two\nlimitations, we propose UNet++, a new neural architecture for semantic and\ninstance segmentation, by (1) alleviating the unknown network depth with an\nefficient ensemble of U-Nets of varying depths, which partially share an\nencoder and co-learn simultaneously using deep supervision; (2) redesigning\nskip connections to aggregate features of varying semantic scales at the\ndecoder sub-networks, leading to a highly flexible feature fusion scheme; and\n(3) devising a pruning scheme to accelerate the inference speed of UNet++. We\nhave evaluated UNet++ using six different medical image segmentation datasets,\ncovering multiple imaging modalities such as computed tomography (CT), magnetic\nresonance imaging (MRI), and electron microscopy (EM), and demonstrating that\n(1) UNet++ consistently outperforms the baseline models for the task of\nsemantic segmentation across different datasets and backbone architectures; (2)\nUNet++ enhances segmentation quality of varying-size objects -- an improvement\nover the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design)\noutperforms the original Mask R-CNN for the task of instance segmentation; and\n(4) pruned UNet++ models achieve significant speedup while showing only modest\nperformance degradation. Our implementation and pre-trained models are\navailable at https://github.com/MrGiovanni/UNetPlusPlus.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 01:26:22 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 23:15:28 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Zhou", "Zongwei", ""], ["Siddiquee", "Md Mahfuzur Rahman", ""], ["Tajbakhsh", "Nima", ""], ["Liang", "Jianming", ""]]}, {"id": "1912.05086", "submitter": "Hengduo Li", "authors": "Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, Larry\n  S. Davis", "title": "Learning from Noisy Anchors for One-stage Object Detection", "comments": "CVPR 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detectors rely on regressing and classifying an\nextensive list of possible anchors, which are divided into positive and\nnegative samples based on their intersection-over-union (IoU) with\ncorresponding groundtruth objects. Such a harsh split conditioned on IoU\nresults in binary labels that are potentially noisy and challenging for\ntraining. In this paper, we propose to mitigate noise incurred by imperfect\nlabel assignment such that the contributions of anchors are dynamically\ndetermined by a carefully constructed cleanliness score associated with each\nanchor. Exploring outputs from both regression and classification branches, the\ncleanliness scores, estimated without incurring any additional computational\noverhead, are used not only as soft labels to supervise the training of the\nclassification branch but also sample re-weighting factors for improved\nlocalization and classification accuracy. We conduct extensive experiments on\nCOCO, and demonstrate, among other things, the proposed approach steadily\nimproves RetinaNet by ~2% with various backbones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:17:32 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 18:35:34 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Li", "Hengduo", ""], ["Wu", "Zuxuan", ""], ["Zhu", "Chen", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Davis", "Larry S.", ""]]}, {"id": "1912.05090", "submitter": "Huihong Zhang", "authors": "Huihong Zhang, Jianlong Yang, Kang Zhou, Zhenjie Chai, Jun Cheng,\n  Shenghua Gao, Jiang Liu", "title": "BioNet: Infusing Biomarker Prior into Global-to-Local Network for\n  Choroid Segmentation in Optical Coherence Tomography Images", "comments": "This paper has been cast for ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choroid is the vascular layer of the eye, which is directly related to the\nincidence and severity of many ocular diseases. Optical Coherence Tomography\n(OCT) is capable of imaging both the cross-sectional view of retina and\nchoroid, but the segmentation of the choroid region is challenging because of\nthe fuzzy choroid-sclera interface (CSI). In this paper, we propose a biomarker\ninfused global-to-local network (BioNet) for choroid segmentation, which\nsegments the choroid with higher credibility and robustness. Firstly, our\nmethod trains a biomarker prediction network to learn the features of the\nbiomarker. Then a global multi-layers segmentation module is applied to segment\nthe OCT image into 12 layers. Finally, the global multi-layered result and the\noriginal OCT image are fed into a local choroid segmentation module to segment\nthe choroid region with the biomarker infused as regularizer. We conducted\ncomparison experiments with the state-of-the-art methods on a dataset (named\nAROD). The experimental results demonstrate the superiority of our method with\n$90.77\\%$ Dice-index and 6.23 pixels Average-unsigned-surface-detection-error,\netc.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:57:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zhang", "Huihong", ""], ["Yang", "Jianlong", ""], ["Zhou", "Kang", ""], ["Chai", "Zhenjie", ""], ["Cheng", "Jun", ""], ["Gao", "Shenghua", ""], ["Liu", "Jiang", ""]]}, {"id": "1912.05094", "submitter": "Arman Afrasiyabi", "authors": "Arman Afrasiyabi, Jean-Fran\\c{c}ois Lalonde, Christian Gagn\\'e", "title": "Associative Alignment for Few-shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification aims at training a model from only a few\nexamples for each of the \"novel\" classes. This paper proposes the idea of\nassociative alignment for leveraging part of the base data by aligning the\nnovel training instances to the closely related ones in the base training set.\nThis expands the size of the effective novel training set by adding extra\n\"related base\" instances to the few novel ones, thereby allowing a constructive\nfine-tuning. We propose two associative alignment strategies: 1) a\nmetric-learning loss for minimizing the distance between related base samples\nand the centroid of novel instances in the feature space, and 2) a conditional\nadversarial alignment loss based on the Wasserstein distance. Experiments on\nfour standard datasets and three backbones demonstrate that combining our\ncentroid-based alignment loss results in absolute accuracy improvements of\n4.4%, 1.2%, and 6.2% in 5-shot learning over the state of the art for object\nrecognition, fine-grained classification, and cross-domain adaptation,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:14:48 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 09:25:00 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 16:09:17 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Afrasiyabi", "Arman", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1912.05096", "submitter": "Zhenzhou Wang", "authors": "ZhenZhou Wang", "title": "Bottleneck detection by slope difference distribution: a robust approach\n  for separating overlapped cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To separate the overlapped cells, a bottleneck detection approach is proposed\nin this paper. The cell image is segmented by slope difference distribution\n(SDD) threshold selection. For each segmented binary clump, its one-dimensional\nboundary is computed as the distance distribution between its centroid and each\npoint on the two-dimensional boundary. The bottleneck points of the\none-dimensional boundary is detected by SDD and then transformed back into\ntwo-dimensional bottleneck points. Two largest concave parts of the binary\nclump are used to select the valid bottleneck points. Two bottleneck points\nfrom different concave parts with the minimum Euclidean distance is connected\nto separate the binary clump with minimum-cut. The binary clumps are separated\niteratively until the number of computed concave parts is smaller than two. We\nuse four types of open-accessible cell datasets to verify the effectiveness of\nthe proposed approach and experimental results showed that the proposed\napproach is significantly more robust than state of the art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:16:01 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "ZhenZhou", ""]]}, {"id": "1912.05101", "submitter": "Yang Tang", "authors": "Chaoqiang Zhao, Yang Tang, Qiyu Sun and Athanasios V. Vasilakos", "title": "Deep Direct Visual Odometry", "comments": "10 pages,8 figures", "journal-ref": null, "doi": "10.1109/TITS.2021.3071886", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional monocular direct visual odometry (DVO) is one of the most famous\nmethods to estimate the ego-motion of robots and map environments from images\nsimultaneously. However, DVO heavily relies on high-quality images and accurate\ninitial pose estimation during tracking. With the outstanding performance of\ndeep learning, previous works have shown that deep neural networks can\neffectively learn 6-DoF (Degree of Freedom) poses between frames from monocular\nimage sequences in the unsupervised manner. However, these unsupervised deep\nlearning-based frameworks cannot accurately generate the full trajectory of a\nlong monocular video because of the scale-inconsistency between each pose. To\naddress this problem, we use several geometric constraints to improve the\nscale-consistency of the pose network, including improving the previous loss\nfunction and proposing a novel scale-to-trajectory constraint for unsupervised\ntraining. We call the pose network trained by the proposed novel constraint as\nTrajNet. In addition, a new DVO architecture, called deep direct sparse\nodometry (DDSO), is proposed to overcome the drawbacks of the previous direct\nsparse odometry (DSO) framework by embedding TrajNet. Extensive experiments on\nthe KITTI dataset show that the proposed constraints can effectively improve\nthe scale-consistency of TrajNet when compared with previous unsupervised\nmonocular methods, and integration with TrajNet makes the initialization and\ntracking of DSO more robust and accurate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:22:03 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 00:53:16 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 06:53:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhao", "Chaoqiang", ""], ["Tang", "Yang", ""], ["Sun", "Qiyu", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "1912.05107", "submitter": "Kanav Vats", "authors": "Kanav Vats, William McNally, Chris Dulhanty, Zhong Qiu Lin, David A.\n  Clausi, John Zelek", "title": "PuckNet: Estimating hockey puck location from broadcast video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Puck location in ice hockey is essential for hockey analysts for determining\nthe location of play and analyzing game events. However, because of the\ndifficulty involved in obtaining accurate annotations due to the extremely low\nvisibility and commonly occurring occlusions of the puck, the problem is very\nchallenging. The problem becomes even more challenging in broadcast videos with\nchanging camera angles. We introduce a novel methodology for determining puck\nlocation from approximate puck location annotations in broadcast video. Our\nmethod uniquely leverages the existing puck location information that is\npublicly available in existing hockey event data and uses the corresponding\none-second broadcast video clips as input to the network. The rationale behind\nusing video as input instead of static images is that with video, the temporal\ninformation can be utilized to handle puck occlusions. The network outputs a\nheatmap representing the probability of the puck location using a 3D CNN based\narchitecture. The network is able to regress the puck location from broadcast\nhockey video clips with varying camera angles. Experimental results demonstrate\nthe capability of the method, achieving 47.07% AUC on the test dataset. The\nnetwork is also able to estimate the puck location in defensive/offensive zones\nwith an accuracy of greater than 80%.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:52:37 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 20:01:44 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 03:42:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Vats", "Kanav", ""], ["McNally", "William", ""], ["Dulhanty", "Chris", ""], ["Lin", "Zhong Qiu", ""], ["Clausi", "David A.", ""], ["Zelek", "John", ""]]}, {"id": "1912.05131", "submitter": "Shuaicheng Liu Prof.", "authors": "Nianjin Ye, Chuan Wang, Shuaicheng Liu, Lanpeng Jia, Jue Wang,\n  Yongqing Cui", "title": "DeepMeshFlow: Content Adaptive Mesh Deformation for Robust Image\n  Registration", "comments": "9 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1909.05983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image alignment by mesh warps, such as meshflow, is a fundamental task which\nhas been widely applied in various vision applications(e.g., multi-frame\nHDR/denoising, video stabilization). Traditional mesh warp methods detect and\nmatch image features, where the quality of alignment highly depends on the\nquality of image features. However, the image features are not robust in\noccurrence of low-texture and low-light scenes. Deep homography methods, on the\nother hand, are free from such problem by learning deep features for robust\nperformance. However, a homography is limited to plane motions. In this work,\nwe present a deep meshflow motion model, which takes two images as input and\noutput a sparse motion field with motions located at mesh vertexes. The deep\nmeshflow enjoys the merics of meshflow that can describe nonlinear motions\nwhile also shares advantage of deep homography that is robust against\nchallenging textureless scenarios. In particular, a new unsupervised network\nstructure is presented with content-adaptive capability. On one hand, the image\ncontent that cannot be aligned under mesh representation are rejected by our\nlearned mask, similar to the RANSAC procedure. On the other hand, we learn\nmultiple mesh resolutions, combining to a non-uniform mesh division. Moreover,\na comprehensive dataset is presented, covering various scenes for training and\ntesting. The comparison between both traditional mesh warp methods and deep\nbased methods show the effectiveness of our deep meshflow motion model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:03:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ye", "Nianjin", ""], ["Wang", "Chuan", ""], ["Liu", "Shuaicheng", ""], ["Jia", "Lanpeng", ""], ["Wang", "Jue", ""], ["Cui", "Yongqing", ""]]}, {"id": "1912.05135", "submitter": "Nelson Nauata Junior", "authors": "Nelson Nauata and Yasutaka Furukawa", "title": "Vectorizing World Buildings: Planar Graph Reconstruction by Primitive\n  Detection and Relationship Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles a 2D architecture vectorization problem, whose task is to\ninfer an outdoor building architecture as a 2D planar graph from a single RGB\nimage. We provide a new benchmark with ground-truth annotations for 2,001\ncomplex buildings across the cities of Atlanta, Paris, and Las Vegas. We also\npropose a novel algorithm utilizing 1) convolutional neural networks (CNNs)\nthat detects geometric primitives and infers their relationships and 2) an\ninteger programming (IP) that assembles the information into a 2D planar graph.\nWhile being a trivial task for human vision, the inference of a graph structure\nwith an arbitrary topology is still an open problem for computer vision.\nQualitative and quantitative evaluations demonstrate that our algorithm makes\nsignificant improvements over the current state-of-the-art, towards an\nintelligent system at the level of human perception. We will share code and\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:23:26 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 04:51:36 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 22:38:43 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nauata", "Nelson", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1912.05156", "submitter": "Lambert Schomaker", "authors": "Lambert Schomaker", "title": "Lifelong learning for text retrieval and recognition in historical\n  handwritten document collections", "comments": "To appear as chapter in book: Handwritten Historical Document\n  Analysis, Recognition, and Retrieval -- State of the Art and Future Trends,\n  in the book series: Series in Machine Perception and Artificial Intelligence\n  World Scientific, ISSN (print): 1793-0839 Original version deposited at\n  Zenodo: https://zenodo.org/record/2346885#.XfCfsq5ytpg on December 17, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides an overview of the problems that need to be dealt with\nwhen constructing a lifelong-learning retrieval, recognition and indexing\nengine for large historical document collections in multiple scripts and\nlanguages, the Monk system. This application is highly variable over time,\nsince the continuous labeling by end users changes the concept of what a\n'ground truth' constitutes. Although current advances in deep learning provide\na huge potential in this application domain, the scale of the problem, i.e.,\nmore than 520 hugely diverse books, documents and manuscripts precludes the\ncurrent meticulous and painstaking human effort which is required in designing\nand developing successful deep-learning systems. The ball-park principle is\nintroduced, which describes the evolution from the sparsely-labeled stage that\ncan only be addressed by traditional methods or nearest-neighbor methods on\nembedded vectors of pre-trained neural networks, up to the other end of the\nspectrum where massive labeling allows reliable training of deep-learning\nmethods. Contents: Introduction, Expectation management, Deep learning, The\nball-park principle, Technical realization, Work flow, Quality and quantity of\nmaterial, Industrialization and scalability, Human effort, Algorithms, Object\nof recognition, Processing pipeline, Performance,Compositionality, Conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:56:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schomaker", "Lambert", ""]]}, {"id": "1912.05159", "submitter": "Huibing Wang", "authors": "Guangqi Jiang, Huibing Wang, Jinjia Peng, Dongyan Chen, Xianping Fu", "title": "Graph-based Multi-view Binary Learning for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing techniques, also known as binary code learning, have recently gained\nincreasing attention in large-scale data analysis and storage. Generally, most\nexisting hash clustering methods are single-view ones, which lack complete\nstructure or complementary information from multiple views. For cluster tasks,\nabundant prior researches mainly focus on learning discrete hash code while few\nworks take original data structure into consideration. To address these\nproblems, we propose a novel binary code algorithm for clustering, which adopts\ngraph embedding to preserve the original data structure, called (Graph-based\nMulti-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding\nthe information of multiple views into a compact binary code, which explores\ncomplementary information from multiple views. In particular, in order to\nmaintain the graph-based structure of the original data, we adopt a Laplacian\nmatrix to preserve the local linear relationship of the data and map it to the\nHamming space. Considering different views have distinctive contributions to\nthe final clustering results, GMBL adopts a strategy of automatically assign\nweights for each view to better guide the clustering. Finally, An alternating\niterative optimization method is adopted to optimize discrete binary codes\ndirectly instead of relaxing the binary constraint in two steps. Experiments on\nfive public datasets demonstrate the superiority of our proposed method\ncompared with previous approaches in terms of clustering performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:04:56 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Jiang", "Guangqi", ""], ["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Chen", "Dongyan", ""], ["Fu", "Xianping", ""]]}, {"id": "1912.05163", "submitter": "Zhe Liu", "authors": "Zhe Liu, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, Xiang Bai", "title": "TANet: Robust 3D Object Detection from Point Clouds with Triple\n  Attention", "comments": "AAAI 2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on exploring the robustness of the 3D object\ndetection in point clouds, which has been rarely discussed in existing\napproaches. We observe two crucial phenomena: 1) the detection accuracy of the\nhard objects, e.g., Pedestrians, is unsatisfactory, 2) when adding additional\nnoise points, the performance of existing approaches decreases rapidly. To\nalleviate these problems, a novel TANet is introduced in this paper, which\nmainly contains a Triple Attention (TA) module, and a Coarse-to-Fine Regression\n(CFR) module. By considering the channel-wise, point-wise and voxel-wise\nattention jointly, the TA module enhances the crucial information of the target\nwhile suppresses the unstable cloud points. Besides, the novel stacked TA\nfurther exploits the multi-level feature attention. In addition, the CFR module\nboosts the accuracy of localization without excessive computation cost.\nExperimental results on the validation set of KITTI dataset demonstrate that,\nin the challenging noisy cases, i.e., adding additional random noisy points\naround each object,the presented approach goes far beyond state-of-the-art\napproaches. Furthermore, for the 3D object detection task of the KITTI\nbenchmark, our approach ranks the first place on Pedestrian class, by using the\npoint clouds as the only input. The running speed is around 29 frames per\nsecond.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:13:36 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Liu", "Zhe", ""], ["Zhao", "Xin", ""], ["Huang", "Tengteng", ""], ["Hu", "Ruolan", ""], ["Zhou", "Yu", ""], ["Bai", "Xiang", ""]]}, {"id": "1912.05170", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy", "title": "Image Classification with Deep Learning in the Presence of Noisy Labels:\n  A Survey", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2021.106771", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image classification systems recently made a giant leap with the advancement\nof deep neural networks. However, these systems require an excessive amount of\nlabeled data to be adequately trained. Gathering a correctly annotated dataset\nis not always feasible due to several factors, such as the expensiveness of the\nlabeling process or difficulty of correctly classifying data, even for the\nexperts. Because of these practical challenges, label noise is a common problem\nin real-world datasets, and numerous methods to train deep neural networks with\nlabel noise are proposed in the literature. Although deep neural networks are\nknown to be relatively robust to label noise, their tendency to overfit data\nmakes them vulnerable to memorizing even random noise. Therefore, it is crucial\nto consider the existence of label noise and develop counter algorithms to fade\naway its adverse effects to train deep neural networks efficiently. Even though\nan extensive survey of machine learning techniques under label noise exists,\nthe literature lacks a comprehensive survey of methodologies centered\nexplicitly around deep learning in the presence of noisy labels. This paper\naims to present these algorithms while categorizing them into one of the two\nsubgroups: noise model based and noise model free methods. Algorithms in the\nfirst group aim to estimate the noise structure and use this information to\navoid the adverse effects of noisy labels. Differently, methods in the second\ngroup try to come up with inherently noise robust algorithms by using\napproaches like robust losses, regularizers or other learning paradigms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:26:57 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 09:56:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 08:50:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""]]}, {"id": "1912.05189", "submitter": "Andr\\'e Nortje", "authors": "Andr\\'e Nortje, Willie Brink, Herman A. Engelbrecht, Herman Kamper", "title": "BINet: a binary inpainting network for deep patch-based image\n  compression", "comments": "Signal Processing: Image Communication", "journal-ref": "Signal Processing: Image Communication 92C (2021) 116119", "doi": "10.1016/j.image.2020.116119", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent deep learning models outperform standard lossy image compression\ncodecs. However, applying these models on a patch-by-patch basis requires that\neach image patch be encoded and decoded independently. The influence from\nadjacent patches is therefore lost, leading to block artefacts at low bitrates.\nWe propose the Binary Inpainting Network (BINet), an autoencoder framework\nwhich incorporates binary inpainting to reinstate interdependencies between\nadjacent patches, for improved patch-based compression of still images. When\ndecoding a patch, BINet additionally uses the binarised encodings from\nsurrounding patches to guide its reconstruction. In contrast to sequential\ninpainting methods where patches are decoded based on previons reconstructions,\nBINet operates directly on the binary codes of surrounding patches without\naccess to the original or reconstructed image data. Encoding and decoding can\ntherefore be performed in parallel. We demonstrate that BINet improves the\ncompression quality of a competitive deep image codec across a range of\ncompression levels.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:11:01 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:33:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nortje", "Andr\u00e9", ""], ["Brink", "Willie", ""], ["Engelbrecht", "Herman A.", ""], ["Kamper", "Herman", ""]]}, {"id": "1912.05190", "submitter": "Li Zhu", "authors": "Li Zhu, Zihao Xie, Liman Liu, Bo Tao, Wenbing Tao", "title": "IoU-uniform R-CNN: Breaking Through the Limitations of RPN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region Proposal Network (RPN) is the cornerstone of two-stage object\ndetectors, it generates a sparse set of object proposals and alleviates the\nextrem foregroundbackground class imbalance problem during training. However,\nwe find that the potential of the detector has not been fully exploited due to\nthe IoU distribution imbalance and inadequate quantity of the training samples\ngenerated by RPN. With the increasing intersection over union (IoU), the\nexponentially smaller numbers of positive samples would lead to the\ndistribution skewed towards lower IoUs, which hinders the optimization of\ndetector at high IoU levels. In this paper, to break through the limitations of\nRPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly\ngenerates training samples with uniform IoU distribution for the regression\nbranch as well as the IoU prediction branch. Besides, we improve the\nperformance of IoU prediction branch by eliminating the feature offsets of RoIs\nat inference, which helps the NMS procedure by preserving accurately localized\nbounding box. Extensive experiments on the PASCAL VOC and MS COCO dataset show\nthe effectiveness of our method, as well as its compatibility and adaptivity to\nmany object detection architectures. The code is made publicly available at\nhttps://github.com/zl1994/IoU-Uniform-R-CNN,\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:13:02 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zhu", "Li", ""], ["Xie", "Zihao", ""], ["Liu", "Liman", ""], ["Tao", "Bo", ""], ["Tao", "Wenbing", ""]]}, {"id": "1912.05193", "submitter": "Herman Kamper", "authors": "Andr\\'e Nortje, Herman A. Engelbrecht, Herman Kamper", "title": "Deep motion estimation for parallel inter-frame prediction in video\n  compression", "comments": "25 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard video codecs rely on optical flow to guide inter-frame prediction:\npixels from reference frames are moved via motion vectors to predict target\nvideo frames. We propose to learn binary motion codes that are encoded based on\nan input video sequence. These codes are not limited to 2D translations, but\ncan capture complex motion (warping, rotation and occlusion). Our motion codes\nare learned as part of a single neural network which also learns to compress\nand decode them. This approach supports parallel video frame decoding instead\nof the sequential motion estimation and compensation of flow-based methods. We\nalso introduce 3D dynamic bit assignment to adapt to object displacements\ncaused by motion, yielding additional bit savings. By replacing the optical\nflow-based block-motion algorithms found in an existing video codec with our\nlearned inter-frame prediction model, our approach outperforms the standard\nH.264 and H.265 video codecs across at low bitrates.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:16:17 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Nortje", "Andr\u00e9", ""], ["Engelbrecht", "Herman A.", ""], ["Kamper", "Herman", ""]]}, {"id": "1912.05220", "submitter": "Dogukan Aksu", "authors": "Sertap Kam\\c{c}{\\i}, Dogukan Aksu, Muhammed Ali Aydin", "title": "Lane Detection For Prototype Autonomous Vehicle", "comments": "The paper was presented in the 6th International Conference on\n  Signal, Image Processing and Multimedia SPM 2019 in Zurich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned vehicle technologies are an area of great interest in theory and\npractice today. These technologies have advanced considerably after the first\napplications have been implemented and cause a rapid change in human life.\nAutonomous vehicles are also a big part of these technologies. The most\nimportant action of a driver has to do is to follow the lanes on the way to the\ndestination. By using image processing and artificial intelligence techniques,\nan autonomous vehicle can move successfully without a driver help. They can go\nfrom the initial point to the specified target by applying pre-defined rules.\nThere are also rules for proper tracking of the lanes. Many accidents are\ncaused due to insufficient follow-up of the lanes and non-compliance with these\nrules. The majority of these accidents also result in injury and death.\n  In this paper, we present an autonomous vehicle prototype that follows lanes\nvia image processing techniques, which are a major part of autonomous vehicle\ntechnology. Autonomous movement capability is provided by using some image\nprocessing algorithms such as canny edge detection, Sobel filter, etc. We\nimplemented and tested these algorithms on the vehicle. The vehicle detected\nand followed the determined lanes. By that way, it went to the destination\nsuccessfully.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 10:27:45 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Kam\u00e7\u0131", "Sertap", ""], ["Aksu", "Dogukan", ""], ["Aydin", "Muhammed Ali", ""]]}, {"id": "1912.05222", "submitter": "Johannes Wolf Kuenzel", "authors": "Johannes K\\\"unzel, Thomas Werner, Ronja M\\\"oller, Peter Eisert, Jan\n  Waschnewski, Ralf Hilpert", "title": "Automatic Analysis of Sewer Pipes Based on Unrolled Monocular Fisheye\n  Images", "comments": "Published in: 2018 IEEE Winter Conference on Applications of Computer\n  Vision (WACV)", "journal-ref": "J. Kunzel, T. Werner, P. Eisert and J. Waschnewski, \"Automatic\n  Analysis of Sewer Pipes Based on Unrolled Monocular Fisheye Images,\" 2018\n  IEEE Winter Conference on Applications of Computer Vision (WACV), Lake Tahoe,\n  NV, 2018, pp. 2019-2027", "doi": "10.1109/WACV.2018.00223", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of detecting and classifying damages in sewer pipes offers an\nimportant application area for computer vision algorithms. This paper describes\na system, which is capable of accomplishing this task solely based on low\nquality and severely compressed fisheye images from a pipe inspection robot.\nRelying on robust image features, we estimate camera poses, model the image\nlighting, and exploit this information to generate high quality cylindrical\nunwraps of the pipes' surfaces.Based on the generated images, we apply semantic\nlabeling based on deep convolutional neural networks to detect and classify\ndefects as well as structural elements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 10:32:35 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["K\u00fcnzel", "Johannes", ""], ["Werner", "Thomas", ""], ["M\u00f6ller", "Ronja", ""], ["Eisert", "Peter", ""], ["Waschnewski", "Jan", ""], ["Hilpert", "Ralf", ""]]}, {"id": "1912.05227", "submitter": "Kishan Sharma", "authors": "Kishan Sharma, Moritz Gold, Christian Zurbruegg, Laura Leal-Taix\\'e,\n  Jan Dirk Wegner", "title": "HistoNet: Predicting size histograms of object instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to predict histograms of object sizes in crowded scenes directly\nwithout any explicit object instance segmentation. What makes this task\nchallenging is the high density of objects (of the same category), which makes\ninstance identification hard. Instead of explicitly segmenting object\ninstances, we show that directly learning histograms of object sizes improves\naccuracy while using drastically less parameters. This is very useful for\napplication scenarios where explicit, pixel-accurate instance segmentation is\nnot needed, but there lies interest in the overall distribution of instance\nsizes. Our core applications are in biology, where we estimate the size\ndistribution of soldier fly larvae, and medicine, where we estimate the size\ndistribution of cancer cells as an intermediate step to calculate the tumor\ncellularity score. Given an image with hundreds of small object instances, we\noutput the total count and the size histogram. We also provide a new data set\nfor this task, the FlyLarvae data set, which consists of 11,000 larvae\ninstances labeled pixel-wise. Our method results in an overall improvement in\nthe count and size distribution prediction as compared to state-of-the-art\ninstance segmentation method Mask R-CNN.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:01:35 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 10:44:13 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 08:32:37 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Sharma", "Kishan", ""], ["Gold", "Moritz", ""], ["Zurbruegg", "Christian", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "1912.05236", "submitter": "Fangting Lin", "authors": "Fangting Lin, Chao Yang, Huizhou Li, Bin Jiang", "title": "Boundary-Aware Salient Object Detection via Recurrent Two-Stream Guided\n  Refinement Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based salient object detection methods which utilize\nboth saliency and boundary features have achieved remarkable performance.\nHowever, most of them ignore the complementarity between saliency features and\nboundary features, thus get worse predictions in scenes with low contrast\nbetween foreground and background. To address this issue, we propose a novel\nRecurrent Two-Stream Guided Refinement Network (RTGRNet) that consists of\niterating Two-Stream Guided Refinement Modules (TGRMs). TGRM consists of a\nGuide Block and two feature streams: saliency and boundary, the Guide Block\nutilizes the refined features after previous TGRM to further improve the\nperformance of two feature streams in current TGRM. Meanwhile, the low-level\nintegrated features are also utilized as a reference to get better details.\nFinally, we progressively refine these features by recurrently stacking more\nTGRMs. Extensive experiments on six public datasets show that our proposed\nRTGRNet achieves the state-of-the-art performance in salient object detection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:19:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Lin", "Fangting", ""], ["Yang", "Chao", ""], ["Li", "Huizhou", ""], ["Jiang", "Bin", ""]]}, {"id": "1912.05237", "submitter": "Yiyi Liao", "authors": "Yiyi Liao, Katja Schwarz, Lars Mescheder, Andreas Geiger", "title": "Towards Unsupervised Learning of Generative Models for 3D Controllable\n  Image Synthesis", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Generative Adversarial Networks have achieved impressive\nresults in photorealistic image synthesis. This progress nurtures hopes that\none day the classical rendering pipeline can be replaced by efficient models\nthat are learned directly from images. However, current image synthesis models\noperate in the 2D domain where disentangling 3D properties such as camera\nviewpoint or object pose is challenging. Furthermore, they lack an\ninterpretable and controllable representation. Our key hypothesis is that the\nimage generation process should be modeled in 3D space as the physical world\nsurrounding us is intrinsically three-dimensional. We define the new task of 3D\ncontrollable image synthesis and propose an approach for solving it by\nreasoning both in 3D space and in the 2D image domain. We demonstrate that our\nmodel is able to disentangle latent 3D factors of simple multi-object scenes in\nan unsupervised fashion from raw images. Compared to pure 2D baselines, it\nallows for synthesizing scenes that are consistent wrt. changes in viewpoint or\nobject pose. We further evaluate various 3D representations in terms of their\nusefulness for this challenging task.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:20:36 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 12:24:56 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Liao", "Yiyi", ""], ["Schwarz", "Katja", ""], ["Mescheder", "Lars", ""], ["Geiger", "Andreas", ""]]}, {"id": "1912.05240", "submitter": "Sulaiman Vesal", "authors": "Dominik Eckert, Sulaiman Vesal, Ludwig Ritschl, Steffen Kappler and\n  Andreas Maier", "title": "Deep Learning-based Denoising of Mammographic Images using\n  Physics-driven Data Augmentation", "comments": "Accepted at BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammography is using low-energy X-rays to screen the human breast and is\nutilized by radiologists to detect breast cancer. Typically radiologists\nrequire a mammogram with impeccable image quality for an accurate diagnosis. In\nthis study, we propose a deep learning method based on Convolutional Neural\nNetworks (CNNs) for mammogram denoising to improve the image quality. We first\nenhance the noise level and employ Anscombe Transformation (AT) to transform\nPoisson noise to white Gaussian noise. With this data augmentation, a deep\nresidual network is trained to learn the noise map of the noisy images. We\nshow, that the proposed method can remove not only simulated but also real\nnoise. Furthermore, we also compare our results with state-of-the-art denoising\nmethods, such as BM3D and DNCNN. In an early investigation, we achieved\nqualitatively better mammogram denoising results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:32:33 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Eckert", "Dominik", ""], ["Vesal", "Sulaiman", ""], ["Ritschl", "Ludwig", ""], ["Kappler", "Steffen", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.05260", "submitter": "Han Liu", "authors": "Hong Luo, Han Liu, Kejun Li, Bo Zhang", "title": "Automatic quality assessment for 2D fetal sonographic standard plane\n  based on multi-task learning", "comments": "14 pages, 10 figures, this paper is submitted to \"Medical Image\n  Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality control of fetal sonographic (FS) images is essential for the\ncorrect biometric measurements and fetal anomaly diagnosis. However, quality\ncontrol requires professional sonographers to perform and is often\nlabor-intensive. To solve this problem, we propose an automatic image quality\nassessment scheme based on multi-task learning to assist in FS image quality\ncontrol. An essential criterion for FS image quality control is that all the\nessential anatomical structures in the section should appear full and\nremarkable with a clear boundary. Therefore, our scheme aims to identify those\nessential anatomical structures to judge whether an FS image is the standard\nimage, which is achieved by three convolutional neural networks. The Feature\nExtraction Network aims to extract deep level features of FS images. Based on\nthe extracted features, the Class Prediction Network determines whether the\nstructure meets the standard and Region Proposal Network identifies its\nposition. The scheme has been applied to three types of fetal sections, which\nare the head, abdominal, and heart. The experimental results show that our\nmethod can make a quality assessment of an FS image within less a second. Also,\nour method achieves competitive performance in both the detection and\nclassification compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:29:00 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Luo", "Hong", ""], ["Liu", "Han", ""], ["Li", "Kejun", ""], ["Zhang", "Bo", ""]]}, {"id": "1912.05270", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad\n  Shahbaz Khan, Joost van de Weijer", "title": "MineGAN: effective knowledge transfer from GANs to target domains with\n  few images", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the attractive characteristics of deep neural networks is their\nability to transfer knowledge obtained in one domain to other related domains.\nAs a result, high-quality networks can be trained in domains with relatively\nlittle training data. This property has been extensively studied for\ndiscriminative networks but has received significantly less attention for\ngenerative models. Given the often enormous effort required to train GANs, both\ncomputationally as well as in the dataset collection, the re-use of pretrained\nGANs is a desirable objective. We propose a novel knowledge transfer method for\ngenerative models based on mining the knowledge that is most beneficial to a\nspecific target domain, either from a single or multiple pretrained GANs. This\nis done using a miner network that identifies which part of the generative\ndistribution of each pretrained GAN outputs samples closest to the target\ndomain. Mining effectively steers GAN sampling towards suitable regions of the\nlatent space, which facilitates the posterior finetuning and avoids pathologies\nof other methods such as mode collapse and lack of flexibility. We perform\nexperiments on several complex datasets using various GAN architectures\n(BigGAN, Progressive GAN) and show that the proposed method, called MineGAN,\neffectively transfers knowledge to domains with few target images,\noutperforming existing methods. In addition, MineGAN can successfully transfer\nknowledge from multiple pretrained GANs. Our code is available at:\nhttps://github.com/yaxingwang/MineGAN.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:43:01 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:41:13 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 09:14:10 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wang", "Yaxing", ""], ["Gonzalez-Garcia", "Abel", ""], ["Berga", "David", ""], ["Herranz", "Luis", ""], ["Khan", "Fahad Shahbaz", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1912.05293", "submitter": "Jingwen He", "authors": "Jingwen He, Chao Dong, Yu Qiao", "title": "Interactive Multi-Dimension Modulation with Dynamic Controllable\n  Residual Learning for Image Restoration", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive image restoration aims to generate restored images by adjusting a\ncontrolling coefficient which determines the restoration level. Previous works\nare restricted in modulating image with a single coefficient. However, real\nimages always contain multiple types of degradation, which cannot be well\ndetermined by one coefficient. To make a step forward, this paper presents a\nnew problem setup, called multi-dimension (MD) modulation, which aims at\nmodulating output effects across multiple degradation types and levels.\nCompared with the previous single-dimension (SD) modulation, the MD is setup to\nhandle multiple degradations adaptively and relief unbalanced learning problem\nin different degradations. We also propose a deep architecture - CResMD with\nnewly introduced controllable residual connections for multi-dimension\nmodulation. Specifically, we add a controlling variable on the conventional\nresidual connection to allow a weighted summation of input and residual. The\nvalues of these weights are generated by another condition network. We further\npropose a new data sampling strategy based on beta distribution to balance\ndifferent degradation types and levels. With corrupted image and degradation\ninformation as inputs, the network can output the corresponding restored image.\nBy tweaking the condition vector, users can control the output effects in MD\nspace at test time. Extensive experiments demonstrate that the proposed CResMD\nachieve excellent performance on both SD and MD modulation tasks. Code is\navailable at https://github.com/hejingwenhejingwen/CResMD.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 13:38:01 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 08:16:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["He", "Jingwen", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""]]}, {"id": "1912.05295", "submitter": "Priyank Pathak", "authors": "Priyank Pathak, Amir Erfan Eshratifar, Michael Gormish", "title": "Video Person Re-ID: Fantastic Techniques and Where to Find Them", "comments": "2 Page (Student Abstract) accepted in AAAI-20", "journal-ref": null, "doi": null, "report-no": "AAAI-20 SA-572", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to identify the same person from multiple camera views without\nthe explicit use of facial recognition is receiving commercial and academic\ninterest. The current status-quo solutions are based on attention neural\nmodels. In this paper, we propose Attention and CL loss, which is a hybrid of\ncenter and Online Soft Mining (OSM) loss added to the attention loss on top of\na temporal attention-based neural network. The proposed loss function applied\nwith bag-of-tricks for training surpasses the state of the art on the common\nperson Re-ID datasets, MARS and PRID 2011. Our source code is publicly\navailable on github.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:52:41 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Pathak", "Priyank", ""], ["Eshratifar", "Amir Erfan", ""], ["Gormish", "Michael", ""]]}, {"id": "1912.05307", "submitter": "Kanchana Ranasinghe Mr", "authors": "Sadeep Jayasumana, Kanchana Ranasinghe, Mayuka Jayawardhana, Sahan\n  Liyanaarachchi, Harsha Ranasinghe", "title": "Bipartite Conditional Random Fields for Panoptic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the panoptic segmentation problem with a conditional random field\n(CRF) model. Panoptic segmentation involves assigning a semantic label and an\ninstance label to each pixel of a given image. At each pixel, the semantic\nlabel and the instance label should be compatible. Furthermore, a good panoptic\nsegmentation should have a number of other desirable properties such as the\nspatial and color consistency of the labeling (similar looking neighboring\npixels should have the same semantic label and the instance label). To tackle\nthis problem, we propose a CRF model, named Bipartite CRF or BCRF, with two\ntypes of random variables for semantic and instance labels. In this\nformulation, various energies are defined within and across the two types of\nrandom variables to encourage a consistent panoptic segmentation. We propose a\nmean-field-based efficient inference algorithm for solving the CRF and\nempirically show its convergence properties. This algorithm is fully\ndifferentiable, and therefore, BCRF inference can be included as a trainable\nmodule in a deep network. In the experimental evaluation, we quantitatively and\nqualitatively show that the BCRF yields superior panoptic segmentation results\nin practice.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 13:50:36 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 02:18:27 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Ranasinghe", "Kanchana", ""], ["Jayawardhana", "Mayuka", ""], ["Liyanaarachchi", "Sahan", ""], ["Ranasinghe", "Harsha", ""]]}, {"id": "1912.05317", "submitter": "Jovita Lukasik", "authors": "David Friede, Jovita Lukasik, Heiner Stuckenschmidt, Margret Keuper", "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture\n  Performance Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision research, the process of automating architecture\nengineering, Neural Architecture Search (NAS), has gained substantial interest.\nIn the past, NAS was hardly accessible to researchers without access to\nlarge-scale compute systems, due to very long compute times for the recurrent\nsearch and evaluation of new candidate architectures. The NAS-Bench-101 dataset\nfacilitates a paradigm change towards classical methods such as supervised\nlearning to evaluate neural architectures. In this paper, we propose a graph\nencoder built upon Graph Neural Networks (GNN). We demonstrate the\neffectiveness of the proposed encoder on NAS performance prediction for seen\narchitecture types as well an unseen ones (i.e., zero shot prediction). We also\nprovide a new variational-sequential graph autoencoder (VS-GAE) based on the\nproposed graph encoder. The VS-GAE is specialized on encoding and decoding\ngraphs of varying length utilizing GNNs. Experiments on different sampling\nmethods show that the embedding space learned by our VS-GAE increases the\nstability on the accuracy prediction task.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:02:07 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 09:50:48 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Friede", "David", ""], ["Lukasik", "Jovita", ""], ["Stuckenschmidt", "Heiner", ""], ["Keuper", "Margret", ""]]}, {"id": "1912.05333", "submitter": "Moein Hasani", "authors": "Moein Hasani, Amin Nasim Saravi, Hassan Khotanlou", "title": "An Efficient Approach for Using Expectation Maximization Algorithm in\n  Capsule Networks", "comments": null, "journal-ref": null, "doi": "10.1109/MVIP49855.2020.9116870", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks (CapsNets) are brand-new architectures that have shown\nground-breaking results in certain areas of Computer Vision (CV). In 2017,\nHinton and his team introduced CapsNets with routing-by-agreement in \"Sabour et\nal\" and in a more recent paper \"Matrix Capsules with EM Routing\" they proposed\na more complete architecture with Expectation-Maximization (EM) algorithm.\nUnlike the traditional convolutional neural networks (CNNs), this architecture\nis able to preserve the pose of the objects in the picture. Due to this\ncharacteristic, it has been able to beat the previous state-of-theart results\non the smallNORB dataset, which includes samples with various view points.\nAlso, this architecture is more robust to white box adversarial attacks.\nHowever, CapsNets have two major drawbacks. They can't perform as well as CNNs\non complex datasets and, they need a huge amount of time for training. We try\nto mitigate these shortcomings by finding optimum settings of EM routing\niterations for training CapsNets. Unlike the past studies, we use un-equal\nnumbers of EM routing iterations for different stages of the CapsNet. For our\nresearch, we use three datasets: Yale face dataset, Belgium Traffic Sign\ndataset, and Fashion-MNIST dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:13:15 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 11:35:25 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 13:10:16 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Hasani", "Moein", ""], ["Saravi", "Amin Nasim", ""], ["Khotanlou", "Hassan", ""]]}, {"id": "1912.05345", "submitter": "Girmaw Abebe Tadesse", "authors": "Girmaw Abebe Tadesse, Tingting Zhu, Nhan Le Nguyen Thanh, Nguyen Thanh\n  Hung, Ha Thi Hai Duong, Truong Huu Khanh, Pham Van Quang, Duc Duong Tran,\n  LamMinh Yen, H Rogier Van Doorn, Nguyen Van Hao, John Prince, Hamza Javed,\n  DaniKiyasseh, Le Van Tan, Louise Thwaites, and David A. Clifton", "title": "Severity Detection Tool for Patients with Infectious Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand, foot and mouth disease (HFMD) and tetanus are serious infectious\ndiseases in low and middle income countries. Tetanus in particular has a high\nmortality rate and its treatment is resource-demanding. Furthermore, HFMD often\naffects a large number of infants and young children. As a result, its\ntreatment consumes enormous healthcare resources, especially when outbreaks\noccur. Autonomic nervous system dysfunction (ANSD) is the main cause of death\nfor both HFMD and tetanus patients. However, early detection of ANSD is a\ndifficult and challenging problem. In this paper, we aim to provide a\nproof-of-principle to detect the ANSD level automatically by applying machine\nlearning techniques to physiological patient data, such as electrocardiogram\n(ECG) and photoplethysmogram (PPG) waveforms, which can be collected using\nlow-cost wearable sensors. Efficient features are extracted that encode\nvariations in the waveforms in the time and frequency domains. A support vector\nmachine is employed to classify the ANSD levels. The proposed approach is\nvalidated on multiple datasets of HFMD and tetanus patients in Vietnam. Results\nshow that encouraging performance is achieved in classifying ANSD levels.\nMoreover, the proposed features are simple, more generalisable and outperformed\nthe standard heart rate variability (HRV) analysis. The proposed approach would\nfacilitate both the diagnosis and treatment of infectious diseases in low and\nmiddle income countries, and thereby improve overall patient care.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:51:37 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Tadesse", "Girmaw Abebe", ""], ["Zhu", "Tingting", ""], ["Thanh", "Nhan Le Nguyen", ""], ["Hung", "Nguyen Thanh", ""], ["Duong", "Ha Thi Hai", ""], ["Khanh", "Truong Huu", ""], ["Van Quang", "Pham", ""], ["Tran", "Duc Duong", ""], ["Yen", "LamMinh", ""], ["Van Doorn", "H Rogier", ""], ["Van Hao", "Nguyen", ""], ["Prince", "John", ""], ["Javed", "Hamza", ""], ["DaniKiyasseh", "", ""], ["Van Tan", "Le", ""], ["Thwaites", "Louise", ""], ["Clifton", "David A.", ""]]}, {"id": "1912.05357", "submitter": "Anders Eklund", "authors": "Anders Eklund", "title": "Feeding the zombies: Synthesizing brain volumes using a 3D progressive\n  growing GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning requires large datasets for training (convolutional) networks\nwith millions of parameters. In neuroimaging, there are few open datasets with\nmore than 100 subjects, which makes it difficult to, for example, train a\nclassifier to discriminate controls from diseased persons. Generative\nadversarial networks (GANs) can be used to synthesize data, but virtually all\nresearch is focused on 2D images. In medical imaging, and especially in\nneuroimaging, most datasets are 3D or 4D. Here we therefore present preliminary\nresults showing that a 3D progressive growing GAN can be used to synthesize MR\nbrain volumes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:35:37 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 15:59:48 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Eklund", "Anders", ""]]}, {"id": "1912.05361", "submitter": "Sudhanshu Mittal", "authors": "Sudhanshu Mittal, Maxim Tatarchenko, \\\"Ozg\\\"un \\c{C}i\\c{c}ek, Thomas\n  Brox", "title": "Parting with Illusions about Deep Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to reduce the high labeling cost involved in training\nmachine learning models on large datasets by efficiently labeling only the most\ninformative samples. Recently, deep active learning has shown success on\nvarious tasks. However, the conventional evaluation scheme used for deep active\nlearning is below par. Current methods disregard some apparent parallel work in\nthe closely related fields. Active learning methods are quite sensitive w.r.t.\nchanges in the training procedure like data augmentation. They improve by a\nlarge-margin when integrated with semi-supervised learning, but barely perform\nbetter than the random baseline. We re-implement various latest active learning\napproaches for image classification and evaluate them under more realistic\nsettings. We further validate our findings for semantic segmentation. Based on\nour observations, we realistically assess the current state of the field and\npropose a more suitable evaluation protocol.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:43:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mittal", "Sudhanshu", ""], ["Tatarchenko", "Maxim", ""], ["\u00c7i\u00e7ek", "\u00d6zg\u00fcn", ""], ["Brox", "Thomas", ""]]}, {"id": "1912.05384", "submitter": "Chaoxu Guo", "authors": "Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, Chunhong Pan", "title": "AugFPN: Improving Multi-scale Feature Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art detectors typically exploit feature pyramid to\ndetect objects at different scales. Among them, FPN is one of the\nrepresentative works that build a feature pyramid by multi-scale features\nsummation. However, the design defects behind prevent the multi-scale features\nfrom being fully exploited. In this paper, we begin by first analyzing the\ndesign defects of feature pyramid in FPN, and then introduce a new feature\npyramid architecture named AugFPN to address these problems. Specifically,\nAugFPN consists of three components: Consistent Supervision, Residual Feature\nAugmentation, and Soft RoI Selection. AugFPN narrows the semantic gaps between\nfeatures of different scales before feature fusion through Consistent\nSupervision. In feature fusion, ratio-invariant context information is\nextracted by Residual Feature Augmentation to reduce the information loss of\nfeature map at the highest pyramid level. Finally, Soft RoI Selection is\nemployed to learn a better RoI feature adaptively after feature fusion. By\nreplacing FPN with AugFPN in Faster R-CNN, our models achieve 2.3 and 1.6\npoints higher Average Precision (AP) when using ResNet50 and MobileNet-v2 as\nbackbone respectively. Furthermore, AugFPN improves RetinaNet by 1.6 points AP\nand FCOS by 0.9 points AP when using ResNet50 as backbone. Codes will be made\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:19:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Guo", "Chaoxu", ""], ["Fan", "Bin", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1912.05393", "submitter": "Martijn Wezel Van", "authors": "M.J.A. van Wezel, L.J. Hamburger, Y. Napolean", "title": "Fine-grained Classification of Rowing teams", "comments": "7 pages, NCCV 2019, 6 figures, deep learning, attention learning,\n  CNN, rowing boat, team detector, club detector, data set, dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained classification tasks such as identifying different breeds of dog\nare quite challenging as visual differences between categories is quite small\nand can be easily overwhelmed by external factors such as object pose,\nlighting, etc. This work focuses on the specific case of classifying rowing\nteams from various associations. Currently, the photos are taken at rowing\ncompetitions and are manually classified by a small set of members, in what is\na painstaking process. To alleviate this, Deep learning models can be utilised\nas a faster method to classify the images. Recent studies show that localising\nthe manually defined parts, and modelling based on these parts, improves on\nvanilla convolution models, so this work also investigates the detection of\nclothing attributes. The networks were trained and tested on a partially\nlabelled data set mainly consisting of rowers from multiple associations. This\npaper resulted in the classification of up to ten rowing associations by using\ndeep learning networks the smaller VGG network achieved 90.1\\% accuracy whereas\nResNet was limited to 87.20\\%. Adding attention to the ResNet resulted into a\ndrop of performance as only 78.10\\% was achieved.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:36:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["van Wezel", "M. J. A.", ""], ["Hamburger", "L. J.", ""], ["Napolean", "Y.", ""]]}, {"id": "1912.05396", "submitter": "Aiham Taleb", "authors": "Aiham Taleb, Christoph Lippert, Tassilo Klein, and Moin Nabi", "title": "Multimodal Self-Supervised Learning for Medical Image Analysis", "comments": "NeurIPS 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning approaches leverage unlabeled samples to acquire\ngeneric knowledge about different concepts, hence allowing for\nannotation-efficient downstream task learning. In this paper, we propose a\nnovel self-supervised method that leverages multiple imaging modalities. We\nintroduce the multimodal puzzle task, which facilitates rich representation\nlearning from multiple image modalities. The learned representations allow for\nsubsequent fine-tuning on different downstream tasks. To achieve that, we learn\na modality-agnostic feature embedding by confusing image modalities at the\ndata-level. Together with the Sinkhorn operator, with which we formulate the\npuzzle solving optimization as permutation matrix inference instead of\nclassification, they allow for efficient solving of multimodal puzzles with\nvarying levels of complexity. In addition, we also propose to utilize\ncross-modal generation techniques for multimodal data augmentation used for\ntraining self-supervised tasks. In other words, we exploit synthetic images for\nself-supervised pretraining, instead of downstream tasks directly, in order to\ncircumvent quality issues associated with synthetic images, while improving\ndata-efficiency and representations quality. Our experimental results, which\nassess the gains in downstream performance and data-efficiency, show that\nsolving our multimodal puzzles yields better semantic representations, compared\nto treating each modality independently. Our results also highlight the\nbenefits of exploiting synthetic images for self-supervised pretraining. We\nshowcase our approach on four downstream tasks: Brain tumor segmentation and\nsurvival days prediction using four MRI modalities, Prostate segmentation using\ntwo MRI modalities, and Liver segmentation using unregistered CT and MRI\nmodalities. We outperform many previous solutions, and achieve results\ncompetitive to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:44:00 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:39:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Taleb", "Aiham", ""], ["Lippert", "Christoph", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1912.05404", "submitter": "Rhona Asgari", "authors": "Rhona Asgari, Sebastian Waldstein, Ferdinand Schlanitz, Magdalena\n  Baratsits, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\\'c", "title": "U-Net with spatial pyramid pooling for drusen segmentation in optical\n  coherence tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of drusen is the main hallmark of early/intermediate age-related\nmacular degeneration (AMD). Therefore, automated drusen segmentation is an\nimportant step in image-guided management of AMD. There are two common\napproaches to drusen segmentation. In the first, the drusen are segmented\ndirectly as a binary classification task. In the second approach, the\nsurrounding retinal layers (outer boundary retinal pigment epithelium (OBRPE)\nand Bruch's membrane (BM)) are segmented and the remaining space between these\ntwo layers is extracted as drusen. In this work, we extend the standard U-Net\narchitecture with spatial pyramid pooling components to introduce global\nfeature context. We apply the model to the task of segmenting drusen together\nwith BM and OBRPE. The proposed network was trained and evaluated on a\nlongitudinal OCT dataset of 425 scans from 38 patients with early/intermediate\nAMD. This preliminary study showed that the proposed network consistently\noutperformed the standard U-net model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:00:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Asgari", "Rhona", ""], ["Waldstein", "Sebastian", ""], ["Schlanitz", "Ferdinand", ""], ["Baratsits", "Magdalena", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Bogunovi\u0107", "Hrvoje", ""]]}, {"id": "1912.05405", "submitter": "Igor Slinko", "authors": "Igor Slinko, Anna Vorontsova, Dmitry Zhukov, Olga Barinova, Anton\n  Konushin", "title": "Training Deep SLAM on Single Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based visual odometry and SLAM methods demonstrate a steady\nimprovement over past years. However, collecting ground truth poses to train\nthese methods is difficult and expensive. This could be resolved by training in\nan unsupervised mode, but there is still a large gap between performance of\nunsupervised and supervised methods. In this work, we focus on generating\nsynthetic data for deep learning-based visual odometry and SLAM methods that\ntake optical flow as an input. We produce training data in a form of optical\nflow that corresponds to arbitrary camera movement between a real frame and a\nvirtual frame. For synthesizing data we use depth maps either produced by a\ndepth sensor or estimated from stereo pair. We train visual odometry model on\nsynthetic data and do not use ground truth poses hence this model can be\nconsidered unsupervised. Also it can be classified as monocular as we do not\nuse depth maps on inference. We also propose a simple way to convert any visual\nodometry model into a SLAM method based on frame matching and graph\noptimization. We demonstrate that both the synthetically-trained visual\nodometry model and the proposed SLAM method build upon this model yields\nstate-of-the-art results among unsupervised methods on KITTI dataset and shows\npromising results on a challenging EuRoC dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:02:20 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Slinko", "Igor", ""], ["Vorontsova", "Anna", ""], ["Zhukov", "Dmitry", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "1912.05416", "submitter": "Xiaolong Ma", "authors": "Geng Yuan, Xiaolong Ma, Sheng Lin, Zhengang Li, Caiwen Ding", "title": "A SOT-MRAM-based Processing-In-Memory Engine for Highly Compressed DNN\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.DC cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing wall and data movement challenges of deep neural networks\n(DNNs) have exposed the limitations of conventional CMOS-based DNN\naccelerators. Furthermore, the deep structure and large model size will make\nDNNs prohibitive to embedded systems and IoT devices, where low power\nconsumption are required. To address these challenges, spin orbit torque\nmagnetic random-access memory (SOT-MRAM) and SOT-MRAM based\nProcessing-In-Memory (PIM) engines have been used to reduce the power\nconsumption of DNNs since SOT-MRAM has the characteristic of near-zero standby\npower, high density, none-volatile. However, the drawbacks of SOT-MRAM based\nPIM engines such as high writing latency and requiring low bit-width data\ndecrease its popularity as a favorable energy efficient DNN accelerator. To\nmitigate these drawbacks, we propose an ultra energy efficient framework by\nusing model compression techniques including weight pruning and quantization\nfrom the software level considering the architecture of SOT-MRAM PIM. And we\nincorporate the alternating direction method of multipliers (ADMM) into the\ntraining phase to further guarantee the solution feasibility and satisfy\nSOT-MRAM hardware constraints. Thus, the footprint and power consumption of\nSOT-MRAM PIM can be reduced, while increasing the overall system throughput at\nthe meantime, making our proposed ADMM-based SOT-MRAM PIM more energy\nefficiency and suitable for embedded systems or IoT devices. Our experimental\nresults show the accuracy and compression rate of our proposed framework is\nconsistently outperforming the reference works, while the efficiency (area \\&\npower) and throughput of SOT-MRAM PIM engine is significantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:03:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Ding", "Caiwen", ""]]}, {"id": "1912.05435", "submitter": "Jing Tian", "authors": "Zihan Zeng, Jing Tian", "title": "Deep Learning Methods for Signature Verification", "comments": "Submit to ICMR. arXiv admin note: text overlap with arXiv:1907.11845\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signature is widely used in human daily lives, and serves as a supplementary\ncharacteristic for verifying human identity. However, there is rare work of\nverifying signature. In this paper, we propose a few deep learning\narchitectures to tackle this task, ranging from CNN, RNN to CNN-RNN compact\nmodel. We also improve Path Signature Features by encoding temporal information\nin order to enlarge the discrepancy between genuine and forgery signatures. Our\nnumerical experiments demonstrate the effectiveness of our constructed models\nand features representations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 00:37:47 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zeng", "Zihan", ""], ["Tian", "Jing", ""]]}, {"id": "1912.05440", "submitter": "Andrew Simpson", "authors": "Shuyang Du, Haoli Guo, Andrew Simpson", "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition", "comments": "9 pages 13 figures. Paper originally from CS231n (Stanford) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles have expanded dramatically over the last few years.\nUdacity has release a dataset containing, among other data, a set of images\nwith the steering angle captured during driving. The Udacity challenge aimed to\npredict steering angle based on only the provided images. We explore two\ndifferent models to perform high quality prediction of steering angles based on\nimages using different deep learning techniques including Transfer Learning, 3D\nCNN, LSTM and ResNet. If the Udacity challenge was still ongoing, both of our\nmodels would have placed in the top ten of all entries.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:44:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Du", "Shuyang", ""], ["Guo", "Haoli", ""], ["Simpson", "Andrew", ""]]}, {"id": "1912.05445", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas", "title": "FootAndBall: Integrated player and ball detector", "comments": "arXiv admin note: text overlap with arXiv:1902.07304", "journal-ref": null, "doi": "10.5220/0008916000470056", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a deep neural network-based detector dedicated for ball\nand players detection in high resolution, long shot, video recordings of soccer\nmatches. The detector, dubbed FootAndBall, has an efficient fully convolutional\narchitecture and can operate on input video stream with an arbitrary\nresolution. It produces ball confidence map encoding the position of the\ndetected ball, player confidence map and player bounding boxes tensor encoding\nplayers' positions and bounding boxes. The network uses Feature Pyramid Network\ndesing pattern, where lower level features with higher spatial resolution are\ncombined with higher level features with bigger receptive field. This improves\ndiscriminability of small objects (the ball) as larger visual context around\nthe object of interest is taken into account for the classification. Due to its\nspecialized design, the network has two orders of magnitude less parameters\nthan a generic deep neural network-based object detector, such as SSD or YOLO.\nThis allows real-time processing of high resolution input video stream. Our\ncode and pre-trained model can be found on the project website:\nhttps://github.com/jac99/FootAndBall .\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:21:49 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 22:04:30 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Komorowski", "Jacek", ""], ["Kurzejamski", "Grzegorz", ""], ["Sarwas", "Grzegorz", ""]]}, {"id": "1912.05469", "submitter": "Svitlana Alkhimova", "authors": "Svitlana Alkhimova and Svitlana Sliusar", "title": "Analysis of effectiveness of thresholding in perfusion ROI detection on\n  T2-weighted MR images with abnormal brain anatomy", "comments": null, "journal-ref": "KPI Science News. - 2019. - V. 126, N. 4. - P.35-43", "doi": "10.20535/kpi-sn.2019.4.180237", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain perfusion ROI detection being a preliminary step, designed to\nexclude non-brain tissues from analyzed DSC perfusion MR images. Its accuracy\nis considered as the key factor for delivering correct results of perfusion\ndata analysis. Despite the large variety of algorithms developed on brain\ntissues segmentation, there is no one that works reliably and robustly on\n2T-waited MR images of a human head with abnormal brain anatomy. Therefore,\nthresholding method is still the state-of-the-art technique that is widely used\nas a way of managing pixels involved in brain perfusion ROI. This paper\npresents the analysis of effectiveness of thresholding techniques in brain\nperfusion ROI detection on 2T-waited MR images of a human head with abnormal\nbrain anatomy. Four threshold-based algorithms implementation are considered:\naccording to Otsu method as global thresholding, according to Niblack method as\nlocal thresholding, thresholding in approximate anatomical brain location, and\nbrute force thresholding. The analysis is done using comparison of qualitative\nmaps produced from thresholded images and from the reference ones. Pearson\ncorrelation analysis showed strong positive (r was ranged from 0.7123 to\n0.8518, p<0.01) and weak positive (r<0.35, p<0.01) relationship in case of\nconducted experiments with CBF, CBV, MTT and Tmax maps, respectively. Linear\nregression analysis showed at level of 95% confidence interval that maps\nproduced from thresholded images were subject to scale and offset errors in all\nconducted experiments. The experimental results showed that widely used\nthresholding methods are an ineffective way of managing pixels involved in\nbrain perfusion ROI. Thresholding as brain segmentation tool can lead to poor\nplacement of perfusion ROI and, as a result, produced maps will be subject to\nartifacts and can cause falsely high or falsely low perfusion parameters\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:38:21 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Alkhimova", "Svitlana", ""], ["Sliusar", "Svitlana", ""]]}, {"id": "1912.05470", "submitter": "Jian Chen", "authors": "Skylar W. Wurster and Arkadiusz Sitek and Jian Chen and Karla Evans\n  and Gaeun Kim and Jeremy M. Wolfe", "title": "Human Gist Processing Augments Deep Learning Breast Cancer Risk\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Radiologists can classify a mammogram as normal or abnormal at better than\nchance levels after less than a second's exposure to the images. In this work,\nwe combine these radiologists' gist inputs into pre-trained machine learning\nmodels to validate that integrating gist with a CNN model can achieve an AUC\n(area under the curve) statistically significantly higher than either the gist\nperception of radiologists or the model without gist input.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 04:27:06 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wurster", "Skylar W.", ""], ["Sitek", "Arkadiusz", ""], ["Chen", "Jian", ""], ["Evans", "Karla", ""], ["Kim", "Gaeun", ""], ["Wolfe", "Jeremy M.", ""]]}, {"id": "1912.05471", "submitter": "Svitlana Alkhimova", "authors": "Svitlana Alkhimova", "title": "Impact of perfusion ROI detection to the quality of CBV perfusion map", "comments": null, "journal-ref": "Technology Audit and Production Reserves. - 2019. - V. 5, N. 2\n  (49). - P.4-7", "doi": "10.15587/2312-8372.2019.182789", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The object of research in this study is quality of CBV perfusion map,\nconsidering detection of perfusion ROI as a key component in processing of\ndynamic susceptibility contrast magnetic resonance images of a human head. CBV\nmap is generally accepted to be the best among others to evaluate location and\nsize of stroke lesions and angiogenesis of brain tumors. Its poor accuracy can\ncause failed results for both quantitative measurements and visual assessment\nof cerebral blood volume. The impact of perfusion ROI detection on the quality\nof maps was analyzed through comparison of maps produced from threshold and\nreference images of the same datasets from 12 patients with cerebrovascular\ndisease. Brain perfusion ROI was placed to exclude low intensity (air and\nnon-brain tissues regions) and high intensity (cerebrospinal fluid regions)\npixels. Maps were produced using area under the curve and deconvolution\nmethods. For both methods compared maps were primarily correlational according\nto Pearson correlation analysis: r=0.8752 and r=0.8706 for area under the curve\nand deconvolution, respectively, p<2.2*10^-16. In spite of this, for both\nmethods scatter plots had data points associated with missed blood regions and\nregression lines indicated presence of scale and offset errors for maps\nproduced from threshold images. Obtained results indicate that thresholding is\nan ineffective way to detect brain perfusion ROI, which usage can cause\ndegradation of CBV map quality. Perfusion ROI detection should be standardized\nand accepted into validation protocols of new systems for perfusion data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:46:43 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Alkhimova", "Svitlana", ""]]}, {"id": "1912.05480", "submitter": "Kerstin Hammernik", "authors": "Jo Schlemper, Chen Qin, Jinming Duan, Ronald M. Summers, Kerstin\n  Hammernik", "title": "$\\Sigma$-net: Ensembled Iterative Deep Neural Networks for Accelerated\n  Parallel MR Image Reconstruction", "comments": "fastMRI challenge submission (team: holykspace)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an ensembled $\\Sigma$-net for fast parallel MR imaging, including\nparallel coil networks, which perform implicit coil weighting, and sensitivity\nnetworks, involving explicit sensitivity maps. The networks in $\\Sigma$-net are\ntrained in a supervised way, including content and GAN losses, and with various\nways of data consistency, i.e., proximal mappings, gradient descent and\nvariable splitting. A semi-supervised finetuning scheme allows us to adapt to\nthe k-space data at test time, which, however, decreases the quantitative\nmetrics, although generating the visually most textured and sharp images. For\nthis challenge, we focused on robust and high SSIM scores, which we achieved by\nensembling all models to a $\\Sigma$-net.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:23:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schlemper", "Jo", ""], ["Qin", "Chen", ""], ["Duan", "Jinming", ""], ["Summers", "Ronald M.", ""], ["Hammernik", "Kerstin", ""]]}, {"id": "1912.05515", "submitter": "Dawei Du", "authors": "Wenzhang Zhou, Longyin Wen, Libo Zhang, Dawei Du, Tiejian Luo, Yanjun\n  Wu", "title": "SiamMan: Siamese Motion-aware Network for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel siamese motion-aware network (SiamMan) for\nvisual tracking, which consists of the siamese feature extraction subnetwork,\nfollowed by the classification, regression, and localization branches in\nparallel. The classification branch is used to distinguish the foreground from\nbackground, and the regression branch is adopt to regress the bounding box of\ntarget. To reduce the impact of manually designed anchor boxes to adapt to\ndifferent target motion patterns, we design the localization branch, which aims\nto coarsely localize the target to help the regression branch to generate\naccurate results. Meanwhile, we introduce the global context module into the\nlocalization branch to capture long-range dependency for more robustness in\nlarge displacement of target. In addition, we design a multi-scale learnable\nattention module to guide these three branches to exploit discriminative\nfeatures for better performance. The whole network is trained offline in an\nend-to-end fashion with large-scale image pairs using the standard SGD\nalgorithm with back-propagation. Extensive experiments on five challenging\nbenchmarks, i.e., VOT2016, VOT2018, OTB100, UAV123 and LTB35, demonstrate that\nSiamMan achieves leading accuracy with high efficiency. Code can be found at\nhttps://isrc.iscas.ac.cn/gitlab/research/siamman.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:26:44 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 18:08:03 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhou", "Wenzhang", ""], ["Wen", "Longyin", ""], ["Zhang", "Libo", ""], ["Du", "Dawei", ""], ["Luo", "Tiejian", ""], ["Wu", "Yanjun", ""]]}, {"id": "1912.05523", "submitter": "Yaohui Wang", "authors": "Yaohui Wang, Piotr Bilinski, Francois Bremond, Antitza Dantcheva", "title": "G3AN: Disentangling Appearance and Motion for Video Generation", "comments": "CVPR 2020, project link https://wyhsirius.github.io/G3AN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating realistic human videos entails the challenge of being able to\nsimultaneously generate both appearance, as well as motion. To tackle this\nchallenge, we introduce G$^{3}$AN, a novel spatio-temporal generative model,\nwhich seeks to capture the distribution of high dimensional video data and to\nmodel appearance and motion in disentangled manner. The latter is achieved by\ndecomposing appearance and motion in a three-stream Generator, where the main\nstream aims to model spatio-temporal consistency, whereas the two auxiliary\nstreams augment the main stream with multi-scale appearance and motion\nfeatures, respectively. An extensive quantitative and qualitative analysis\nshows that our model systematically and significantly outperforms\nstate-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as\nwell as the Weizmann and UCF101 datasets on human action. Additional analysis\non the learned latent representations confirms the successful decomposition of\nappearance and motion. Source code and pre-trained models are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:46:53 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 17:56:54 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 10:58:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Yaohui", ""], ["Bilinski", "Piotr", ""], ["Bremond", "Francois", ""], ["Dantcheva", "Antitza", ""]]}, {"id": "1912.05524", "submitter": "Prune Truong", "authors": "Prune Truong, Martin Danelljan, and Radu Timofte", "title": "GLU-Net: Global-Local Universal Network for Dense Flow and\n  Correspondences", "comments": "Website: https://prunetruong.com/research/glu-net. Code:\n  https://github.com/PruneTruong/GLU-Net. Video:\n  https://www.youtube.com/watch?v=xB2gNx8f8Xc. CVPR 2020 (ORAL)", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing dense correspondences between a pair of images is an important\nand general problem, covering geometric matching, optical flow and semantic\ncorrespondences. While these applications share fundamental challenges, such as\nlarge displacements, pixel-accuracy, and appearance changes, they are currently\naddressed with specialized network architectures, designed for only one\nparticular task. This severely limits the generalization capabilities of such\nnetworks to new scenarios, where e.g. robustness to larger displacements or\nhigher accuracy is required.\n  In this work, we propose a universal network architecture that is directly\napplicable to all the aforementioned dense correspondence problems. We achieve\nboth high accuracy and robustness to large displacements by investigating the\ncombined use of global and local correlation layers. We further propose an\nadaptive resolution strategy, allowing our network to operate on virtually any\ninput image resolution. The proposed GLU-Net achieves state-of-the-art\nperformance for geometric and semantic matching as well as optical flow, when\nusing the same network and weights. Code and trained models are available at\nhttps://github.com/PruneTruong/GLU-Net.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:47:11 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 11:58:41 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 13:31:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Truong", "Prune", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""]]}, {"id": "1912.05526", "submitter": "Fei Yang", "authors": "Fei Yang, Luis Herranz, Joost van de Weijer, Jos\\'e A. Iglesias\n  Guiti\\'an, Antonio L\\'opez and Mikhail Mozerov", "title": "Variable Rate Deep Image Compression with Modulated Autoencoder", "comments": "Published as a journal paper in IEEE Signal Processing Letters", "journal-ref": "IEEE SPL,VOL.27(2020),331-335", "doi": "10.1109/LSP.2020.2970539", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable rate is a requirement for flexible and adaptable image and video\ncompression. However, deep image compression methods are optimized for a single\nfixed rate-distortion tradeoff. While this can be addressed by training\nmultiple models for different tradeoffs, the memory requirements increase\nproportionally to the number of models. Scaling the bottleneck representation\nof a shared autoencoder can provide variable rate compression with a single\nshared autoencoder. However, the R-D performance using this simple mechanism\ndegrades in low bitrates, and also shrinks the effective range of bit rates.\nAddressing these limitations, we formulate the problem of variable\nrate-distortion optimization for deep image compression, and propose modulated\nautoencoders (MAEs), where the representations of a shared autoencoder are\nadapted to the specific rate-distortion tradeoff via a modulation network.\nJointly training this modulated autoencoder and modulation network provides an\neffective way to navigate the R-D operational curve. Our experiments show that\nthe proposed method can achieve almost the same R-D performance of independent\nmodels with significantly fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:51:32 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 20:11:08 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Yang", "Fei", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""], ["Guiti\u00e1n", "Jos\u00e9 A. Iglesias", ""], ["L\u00f3pez", "Antonio", ""], ["Mozerov", "Mikhail", ""]]}, {"id": "1912.05534", "submitter": "Jinwoo Choi", "authors": "Jinwoo Choi, Chen Gao, Joseph C. E. Messou, Jia-Bin Huang", "title": "Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action\n  Recognition", "comments": "NeurIPS 2019. Project webpage: http://chengao.vision/SDN/ Code:\n  https://github.com/vt-vl-lab/SDN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities often occur in specific scene contexts, e.g., playing\nbasketball on a basketball court. Training a model using existing video\ndatasets thus inevitably captures and leverages such bias (instead of using the\nactual discriminative cues). The learned representation may not generalize well\nto new action classes or different tasks. In this paper, we propose to mitigate\nscene bias for video representation learning. Specifically, we augment the\nstandard cross-entropy loss for action classification with 1) an adversarial\nloss for scene types and 2) a human mask confusion loss for videos where the\nhuman actors are masked out. These two losses encourage learning\nrepresentations that are unable to predict the scene types and the correct\nactions when there is no evidence. We validate the effectiveness of our method\nby transferring our pre-trained model to three different tasks, including\naction classification, temporal localization, and spatio-temporal action\ndetection. Our results show consistent improvement over the baseline model\nwithout debiasing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:59:15 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Choi", "Jinwoo", ""], ["Gao", "Chen", ""], ["Messou", "Joseph C. E.", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1912.05566", "submitter": "Justus Thies", "authors": "Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt,\n  Matthias Nie{\\ss}ner", "title": "Neural Voice Puppetry: Audio-driven Facial Reenactment", "comments": "Video: https://youtu.be/s74_yQiJMXA Project/Demo/Code:\n  https://justusthies.github.io/posts/neural-voice-puppetry/", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Voice Puppetry, a novel approach for audio-driven facial\nvideo synthesis. Given an audio sequence of a source person or digital\nassistant, we generate a photo-realistic output video of a target person that\nis in sync with the audio of the source input. This audio-driven facial\nreenactment is driven by a deep neural network that employs a latent 3D face\nmodel space. Through the underlying 3D representation, the model inherently\nlearns temporal stability while we leverage neural rendering to generate\nphoto-realistic output frames. Our approach generalizes across different\npeople, allowing us to synthesize videos of a target actor with the voice of\nany unknown source actor or even synthetic voices that can be generated\nutilizing standard text-to-speech approaches. Neural Voice Puppetry has a\nvariety of use-cases, including audio-driven video avatars, video dubbing, and\ntext-driven video synthesis of a talking head. We demonstrate the capabilities\nof our method in a series of audio- and text-based puppetry examples, including\ncomparisons to state-of-the-art techniques and a user study.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:00:18 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:49:45 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Thies", "Justus", ""], ["Elgharib", "Mohamed", ""], ["Tewari", "Ayush", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.05575", "submitter": "Orod Razeghi", "authors": "Orod Razeghi and Guoping Qiu", "title": "Object Recognition with Human in the Loop Intelligent Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers embedded within human in the loop visual object recognition\nframeworks commonly utilise two sources of information: one derived directly\nfrom the imagery data of an object, and the other obtained interactively from\nuser interactions. These computer vision frameworks exploit human high-level\ncognitive power to tackle particularly difficult visual object recognition\ntasks. In this paper, we present innovative techniques to combine the two\nsources of information intelligently for the purpose of improving recognition\naccuracy. We firstly employ standard algorithms to build two classifiers for\nthe two sources independently, and subsequently fuse the outputs from these\nclassifiers to make a conclusive decision. The two fusion techniques proposed\nare: i) a modified naive Bayes algorithm that adaptively selects an individual\nclassifier's output or combines both to produce a definite answer, and ii) a\nneural network based algorithm which feeds the outputs of the two classifiers\nto a 4-layer feedforward network to generate a final output. We present\nextensive experimental results on 4 challenging visual recognition tasks to\nillustrate that the new intelligent techniques consistently outperform\ntraditional approaches to fusing the two sources of information.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:05:39 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Razeghi", "Orod", ""], ["Qiu", "Guoping", ""]]}, {"id": "1912.05591", "submitter": "Gagan Kanojia", "authors": "Gagan Kanojia and Shanmuganathan Raman", "title": "Simultaneous Detection and Removal of Dynamic Objects in Multi-view\n  Images", "comments": "Accepted at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of images of a scene consisting of moving objects captured\nusing a hand-held camera. In this work, we propose an algorithm which takes\nthis set of multi-view images as input, detects the dynamic objects present in\nthe scene, and replaces them with the static regions which are being occluded\nby them. The proposed algorithm scans the reference image in the row-major\norder at the pixel level and classifies each pixel as static or dynamic. During\nthe scan, when a pixel is classified as dynamic, the proposed algorithm\nreplaces that pixel value with the corresponding pixel value of the static\nregion which is being occluded by that dynamic region. We show that we achieve\nartifact-free removal of dynamic objects in multi-view images of several\nreal-world scenes. To the best of our knowledge, we propose the first method\nwhich simultaneously detects and removes the dynamic objects present in\nmulti-view images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:44:35 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Kanojia", "Gagan", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1912.05604", "submitter": "Clemens Eppner", "authors": "Clemens Eppner and Arsalan Mousavian and Dieter Fox", "title": "A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a\n  Dense, Physics-based Grasp Data Set", "comments": "For associated web page, see\n  https://sites.google.com/view/abillionwaystograsp . 19th International\n  Symposium of Robotics Research (ISRR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot grasping is often formulated as a learning problem. With the increasing\nspeed and quality of physics simulations, generating large-scale grasping data\nsets that feed learning algorithms is becoming more and more popular. An often\noverlooked question is how to generate the grasps that make up these data sets.\nIn this paper, we review, classify, and compare different grasp sampling\nstrategies. Our evaluation is based on a fine-grained discretization of SE(3)\nand uses physics-based simulation to evaluate the quality and robustness of the\ncorresponding parallel-jaw grasps. Specifically, we consider more than 1\nbillion grasps for each of the 21 objects from the YCB data set. This dense\ndata set lets us evaluate existing sampling schemes w.r.t. their bias and\nefficiency. Our experiments show that some popular sampling schemes contain\nsignificant bias and do not cover all possible ways an object can be grasped.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 20:22:52 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Eppner", "Clemens", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.05631", "submitter": "Orod Razeghi", "authors": "Orod Razeghi and Guoping Qiu", "title": "Discriminative Dimension Reduction based on Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"curse of dimensionality\" is a well-known problem in pattern recognition.\nA widely used approach to tackling the problem is a group of subspace methods,\nwhere the original features are projected onto a new space. The lower\ndimensional subspace is then used to approximate the original features for\nclassification. However, most subspace methods were not originally developed\nfor classification. We believe that direct adoption of these subspace methods\nfor pattern classification should not be considered best practice. In this\npaper, we present a new information theory based algorithm for selecting\nsubspaces, which can always result in superior performance over conventional\nmethods. This paper makes the following main contributions: i) it improves a\ncommon practice widely used by practitioners in the field of pattern\nrecognition, ii) it develops an information theory based technique for\nsystematically selecting the subspaces that are discriminative and therefore\nare suitable for pattern recognition/classification purposes, iii) it presents\nextensive experimental results on a variety of computer vision and pattern\nrecognition tasks to illustrate that the subspaces selected based on maximum\nmutual information criterion will always enhance performance regardless of the\nclassification techniques used.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:12:16 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Razeghi", "Orod", ""], ["Qiu", "Guoping", ""]]}, {"id": "1912.05636", "submitter": "Sudheer Achary", "authors": "Sudheer Achary, K L Bhanu Moorthy, Syed Ashar Javed, Nikita Shravan,\n  Vineet Gandhi, Anoop Namboodiri", "title": "CineFilter: Unsupervised Filtering for Real Time Autonomous Camera\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous camera systems are often subjected to an optimization/filtering\noperation to smoothen and stabilize the rough trajectory estimates. Most common\nfiltering techniques do reduce the irregularities in data; however, they fail\nto mimic the behavior of a human cameraman. Global filtering methods modeling\nhuman camera operators have been successful; however, they are limited to\noffline settings. In this paper, we propose two online filtering methods called\nCinefilters, which produce smooth camera trajectories that are motivated by\ncinematographic principles. The first filter (CineConvex) uses a sliding\nwindow-based convex optimization formulation, and the second (CineCNN) is a CNN\nbased encoder-decoder model. We evaluate the proposed filters in two different\nsettings, namely a basketball dataset and a stage performance dataset. Our\nmodels outperform previous methods and baselines on both quantitative and\nqualitative metrics. The CineConvex and CineCNN filters operate at about 250fps\nand 1000fps, respectively, with a minor latency (half a second), making them\napt for a variety of real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:23:59 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 19:25:24 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 11:53:39 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 10:24:37 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Achary", "Sudheer", ""], ["Moorthy", "K L Bhanu", ""], ["Javed", "Syed Ashar", ""], ["Shravan", "Nikita", ""], ["Gandhi", "Vineet", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "1912.05654", "submitter": "Nikolaos Passalis", "authors": "Nikolaos Passalis and Stavros Doropoulos", "title": "deepsing: Generating Sentiment-aware Visual Stories using Cross-modal\n  Music Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a deep learning method for performing\nattributed-based music-to-image translation. The proposed method is applied for\nsynthesizing visual stories according to the sentiment expressed by songs. The\ngenerated images aim to induce the same feelings to the viewers, as the\noriginal song does, reinforcing the primary aim of music, i.e., communicating\nfeelings. The process of music-to-image translation poses unique challenges,\nmainly due to the unstable mapping between the different modalities involved in\nthis process. In this paper, we employ a trainable cross-modal translation\nmethod to overcome this limitation, leading to the first, to the best of our\nknowledge, deep learning method for generating sentiment-aware visual stories.\nVarious aspects of the proposed method are extensively evaluated and discussed\nusing different songs.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:46:41 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Passalis", "Nikolaos", ""], ["Doropoulos", "Stavros", ""]]}, {"id": "1912.05656", "submitter": "Muhammed Kocabas", "authors": "Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black", "title": "VIBE: Video Inference for Human Body Pose and Shape Estimation", "comments": "CVPR-2020 camera ready. Code is available at\n  https://github.com/mkocabas/VIBE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion is fundamental to understanding behavior. Despite progress on\nsingle-image 3D pose and shape estimation, existing video-based\nstate-of-the-art methods fail to produce accurate and natural motion sequences\ndue to a lack of ground-truth 3D motion data for training. To address this\nproblem, we propose Video Inference for Body Pose and Shape Estimation (VIBE),\nwhich makes use of an existing large-scale motion capture dataset (AMASS)\ntogether with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty\nis an adversarial learning framework that leverages AMASS to discriminate\nbetween real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that\nadversarial training, at the sequence level, produces kinematically plausible\nmotion sequences without in-the-wild ground-truth 3D labels. We perform\nextensive experimentation to analyze the importance of motion and demonstrate\nthe effectiveness of VIBE on challenging 3D pose estimation datasets, achieving\nstate-of-the-art performance. Code and pretrained models are available at\nhttps://github.com/mkocabas/VIBE.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:47:26 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 10:40:15 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 19:35:34 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Kocabas", "Muhammed", ""], ["Athanasiou", "Nikos", ""], ["Black", "Michael J.", ""]]}, {"id": "1912.05661", "submitter": "Juan C. P\\'erez", "authors": "Juan C. P\\'erez, Motasem Alfarra, Guillaume Jeanneret, Adel Bibi, Ali\n  Thabet, Bernard Ghanem, Pablo Arbel\\'aez", "title": "Gabor Layers Enhance Network Robustness", "comments": "32 pages, 23 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the benefits of merging classical vision concepts with deep\nlearning models. In particular, we explore the effect on robustness against\nadversarial attacks of replacing the first layers of various deep architectures\nwith Gabor layers, i.e. convolutional layers with filters that are based on\nlearnable Gabor parameters. We observe that architectures enhanced with Gabor\nlayers gain a consistent boost in robustness over regular models and preserve\nhigh generalizing test performance, even though these layers come at a\nnegligible increase in the number of parameters. We then exploit the closed\nform expression of Gabor filters to derive an expression for a Lipschitz\nconstant of such filters, and harness this theoretical result to develop a\nregularizer we use during training to further enhance network robustness. We\nconduct extensive experiments with various architectures (LeNet, AlexNet, VGG16\nand WideResNet) on several datasets (MNIST, SVHN, CIFAR10 and CIFAR100) and\ndemonstrate large empirical robustness gains. Furthermore, we experimentally\nshow how our regularizer provides consistent robustness improvements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:59:59 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 21:52:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["P\u00e9rez", "Juan C.", ""], ["Alfarra", "Motasem", ""], ["Jeanneret", "Guillaume", ""], ["Bibi", "Adel", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""], ["Arbel\u00e1ez", "Pablo", ""]]}, {"id": "1912.05684", "submitter": "Samet Akcay", "authors": "Bruna G. Maciel-Pearson, Letizia Marchegiani, Samet Akcay, Amir\n  Atapour-Abarghouei, James Garforth, Toby P. Breckon", "title": "Online Deep Reinforcement Learning for Autonomous UAV Navigation and\n  Exploration of Outdoor Environments", "comments": "Journal Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing expansion in the use of UAVs, the ability to\nautonomously navigate in varying environments and weather conditions remains a\nhighly desirable but as-of-yet unsolved challenge. In this work, we use Deep\nReinforcement Learning to continuously improve the learning and understanding\nof a UAV agent while exploring a partially observable environment, which\nsimulates the challenges faced in a real-life scenario. Our innovative approach\nuses a double state-input strategy that combines the acquired knowledge from\nthe raw image and a map containing positional information. This positional data\naids the network understanding of where the UAV has been and how far it is from\nthe target position, while the feature map from the current scene highlights\ncluttered areas that are to be avoided. Our approach is extensively tested\nusing variants of Deep Q-Network adapted to cope with double state input data.\nFurther, we demonstrate that by altering the reward and the Q-value function,\nthe agent is capable of consistently outperforming the adapted Deep Q-Network,\nDouble Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate\nthat our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of\nnavigating through multiple unseen environments and under severe weather\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:02:24 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Maciel-Pearson", "Bruna G.", ""], ["Marchegiani", "Letizia", ""], ["Akcay", "Samet", ""], ["Atapour-Abarghouei", "Amir", ""], ["Garforth", "James", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1912.05687", "submitter": "Omid Bazgir", "authors": "Omid Bazgir, Ruibo Zhang, Saugato Rahman Dhruba, Raziur Rahman,\n  Souparno Ghosh, Ranadip Pal", "title": "REFINED (REpresentation of Features as Images with NEighborhood\n  Dependencies): A novel feature representation for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-18197-y", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning with Convolutional Neural Networks has shown great promise in\nvarious areas of image-based classification and enhancement but is often\nunsuitable for predictive modeling involving non-image based features or\nfeatures without spatial correlations. We present a novel approach for\nrepresentation of high dimensional feature vector in a compact image form,\ntermed REFINED (REpresentation of Features as Images with NEighborhood\nDependencies), that is conducible for convolutional neural network based deep\nlearning. We consider the correlations between features to generate a compact\nrepresentation of the features in the form of a two-dimensional image using\nminimization of pairwise distances similar to multi-dimensional scaling. We\nhypothesize that this approach enables embedded feature selection and\nintegrated with Convolutional Neural Network based Deep Learning can produce\nmore accurate predictions as compared to Artificial Neural Networks, Random\nForests and Support Vector Regression. We illustrate the superior predictive\nperformance of the proposed representation, as compared to existing approaches,\nusing synthetic datasets, cell line efficacy prediction based on drug chemical\ndescriptors for NCI60 dataset and drug sensitivity prediction based on\ntranscriptomic data and chemical descriptors using GDSC dataset. Results\nillustrated on both synthetic and biological datasets shows the higher\nprediction accuracy of the proposed framework as compared to existing\nmethodologies while maintaining desirable properties in terms of bias and\nfeature extraction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:18:05 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:26:15 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Bazgir", "Omid", ""], ["Zhang", "Ruibo", ""], ["Dhruba", "Saugato Rahman", ""], ["Rahman", "Raziur", ""], ["Ghosh", "Souparno", ""], ["Pal", "Ranadip", ""]]}, {"id": "1912.05688", "submitter": "Mohammad Akbari", "authors": "Mohammad Akbari, Jie Liang, Jingning Han, Chengjie Tu", "title": "Learned Variable-Rate Image Compression with Residual Divisive\n  Normalization", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been shown that deep learning-based image compression has\nshown the potential to outperform traditional codecs. However, most existing\nmethods train multiple networks for multiple bit rates, which increases the\nimplementation complexity. In this paper, we propose a variable-rate image\ncompression framework, which employs more Generalized Divisive Normalization\n(GDN) layers than previous GDN-based methods. Novel GDN-based residual\nsub-networks are also developed in the encoder and decoder networks. Our scheme\nalso uses a stochastic rounding-based scalable quantization. To further improve\nthe performance, we encode the residual between the input and the reconstructed\nimage from the decoder network as an enhancement layer. To enable a single\nmodel to operate with different bit rates and to learn multi-rate image\nfeatures, a new objective function is introduced. Experimental results show\nthat the proposed framework trained with variable-rate objective function\noutperforms all standard codecs such as H.265/HEVC-based BPG and\nstate-of-the-art learning-based variable-rate methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:19:58 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Han", "Jingning", ""], ["Tu", "Chengjie", ""]]}, {"id": "1912.05699", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay and Yew-Soon Ong", "title": "What it Thinks is Important is Important: Robustness Transfers through\n  Input Gradients", "comments": "Accepted as Oral in CVPR 2020, Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial perturbations are imperceptible changes to input pixels that can\nchange the prediction of deep learning models. Learned weights of models robust\nto such perturbations are previously found to be transferable across different\ntasks but this applies only if the model architecture for the source and target\ntasks is the same. Input gradients characterize how small changes at each input\npixel affect the model output. Using only natural images, we show here that\ntraining a student model's input gradients to match those of a robust teacher\nmodel can gain robustness close to a strong baseline that is robustly trained\nfrom scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and\nTiny-ImageNet, we show that our proposed method, input gradient adversarial\nmatching, can transfer robustness across different tasks and even across\ndifferent model architectures. This demonstrates that directly targeting the\nsemantics of input gradients is a feasible way towards adversarial robustness.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:51:37 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 07:50:06 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 13:45:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1912.05727", "submitter": "Toru Tamaki", "authors": "Toru Tamaki, Daisuke Ogawa, Bisser Raytchev, Kazufumi Kaneda", "title": "Semantic segmentation of trajectories with improved agent models for\n  pedestrian behavior analysis", "comments": null, "journal-ref": "Advanced Robotics, Volume 33, 2019 - Issue 3-4: Special Issue on\n  Systems Science of Bio-navigation, Pages 153-168", "doi": "10.1080/01691864.2018.1554508", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for semantic segmentation of pedestrian\ntrajectories based on pedestrian behavior models, or agents. The agents model\nthe dynamics of pedestrian movements in two-dimensional space using a linear\ndynamics model and common start and goal locations of trajectories. First,\nagent models are estimated from the trajectories obtained from image sequences.\nOur method is built on top of the Mixture model of Dynamic pedestrian Agents\n(MDA); however, the MDA's trajectory modeling and estimation are improved.\nThen, the trajectories are divided into semantically meaningful segments. The\nsubsegments of a trajectory are modeled by applying a hidden Markov model using\nthe estimated agent models. Experimental results with a real trajectory dataset\nshow the effectiveness of the proposed method as compared to the well-known\nclassical Ramer-Douglas-Peucker algorithm and also to the original MDA model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:04:43 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Tamaki", "Toru", ""], ["Ogawa", "Daisuke", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "1912.05729", "submitter": "Toru Tamaki", "authors": "Daisuke Ogawa, Toru Tamaki, Tsubasa Hirakawa, Bisser Raytchev,\n  Kazufumi Kaneda, Ken Yoda", "title": "Improved Activity Forecasting for Generating Trajectories", "comments": "The 2019 International Workshop on Frontiers of Computer Vision\n  (IW-FCV2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient inverse reinforcement learning for generating trajectories is\nproposed based of 2D and 3D activity forecasting. We modify reward function\nwith $L_p$ norm and propose convolution into value iteration steps, which is\ncalled convolutional value iteration. Experimental results with seabird\ntrajectories (43 for training and 10 for test), our method is best in terms of\nMHD error and performs fastest. Generated trajectories for interpolating\nmissing parts of trajectories look much similar to real seabird trajectories\nthan those by the previous works.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:05:24 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Ogawa", "Daisuke", ""], ["Tamaki", "Toru", ""], ["Hirakawa", "Tsubasa", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Yoda", "Ken", ""]]}, {"id": "1912.05730", "submitter": "Toru Tamaki", "authors": "Rushi J. Babariya, Toru Tamaki", "title": "Meaning guided video captioning", "comments": "The 5th Asian Conference on Pattern Recognition (ACPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current video captioning approaches often suffer from problems of missing\nobjects in the video to be described, while generating captions semantically\nsimilar with ground truth sentences. In this paper, we propose a new approach\nto video captioning that can describe objects detected by object detection, and\ngenerate captions having similar meaning with correct captions. Our model\nrelies on S2VT, a sequence-to-sequence model for video captioning. Given a\nsequence of video frames, the encoding RNN takes a frame as well as detected\nobjects in the frame in order to incorporate the information of the objects in\nthe scene. The following decoding RNN outputs are then fed into an attention\nlayer and then to a decoder for generating captions. The caption is compared\nwith the ground truth by learning metric so that vector representations of\ngenerated captions are semantically similar to those of ground truth.\nExperimental results with the MSDV dataset demonstrate that the performance of\nthe proposed approach is much better than the model without the proposed\nmeaning-guided framework, showing the effectiveness of the proposed model. Code\nare publicly available at\nhttps://github.com/captanlevi/Meaning-guided-video-captioning-.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:05:45 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Babariya", "Rushi J.", ""], ["Tamaki", "Toru", ""]]}, {"id": "1912.05758", "submitter": "Vivek Roy", "authors": "Yan Xu, Vivek Roy, Kris Kitani", "title": "Estimating 3D Camera Pose from 2D Pedestrian Trajectories", "comments": "Accepted in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of re-calibrating the 3D pose of a static surveillance\ncamera, whose pose may change due to external forces, such as birds, wind,\nfalling objects or earthquakes. Conventionally, camera pose estimation can be\nsolved with a PnP (Perspective-n-Point) method using 2D-to-3D feature\ncorrespondences, when 3D points are known. However, 3D point annotations are\nnot always available or practical to obtain in real-world applications. We\npropose an alternative strategy for extracting 3D information to solve for\ncamera pose by using pedestrian trajectories. We observe that 2D pedestrian\ntrajectories indirectly contain useful 3D information that can be used for\ninferring camera pose. To leverage this information, we propose a data-driven\napproach by training a neural network (NN) regressor to model a direct mapping\nfrom 2D pedestrian trajectories projected on the image plane to 3D camera pose.\nWe demonstrate that our regressor trained only on synthetic data can be\ndirectly applied to real data, thus eliminating the need to label any real\ndata. We evaluate our method across six different scenes from the Town Centre\nStreet and DUKEMTMC datasets. Our method achieves an improvement of $\\sim50\\%$\non both position and orientation prediction accuracy when compared to other\nSOTA methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 03:41:03 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 03:24:30 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Xu", "Yan", ""], ["Roy", "Vivek", ""], ["Kitani", "Kris", ""]]}, {"id": "1912.05759", "submitter": "Hanchi Liu", "authors": "Bin Liu, Yuxiao Ren, Hanchi Liu, Hui Xu, Zhengfang Wang, Anthony G.\n  Cohn, and Peng Jiang", "title": "GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion\n  for Tunnel Lining", "comments": "15pages,11figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A DNN architecture called GPRInvNet is proposed to tackle the challenge of\nmapping Ground Penetrating Radar (GPR) B-Scan data to complex permittivity maps\nof subsurface structure. GPRInvNet consists of a trace-to-trace encoder and a\ndecoder. It is specially designed to take account of the characteristics of GPR\ninversion when faced with complex GPR B-Scan data as well as addressing the\nspatial alignment issue between time-series B-Scan data and spatial\npermittivity maps. It fuses features from several adjacent traces on the B-Scan\ndata to enhance each trace, and then further condense the features of each\ntrace separately. The sensitive zone on the permittivity map spatially aligned\nto the enhanced trace is reconstructed accurately. GPRInvNet has been utilized\nto reconstruct the permittivity map of tunnel linings. A diverse range of\ndielectric models of tunnel lining containing complex defects has been\nreconstructed using GPRInvNet, and results demonstrate that GPRInvNet is\ncapable of effectively reconstructing complex tunnel lining defects with clear\nboundaries. Comparative results with existing baseline methods also demonstrate\nthe superiority of the GPRInvNet. To generalize GPRInvNet to real GPR data, we\nintegrated background noise patches recorded form a practical model testing\ninto synthetic GPR data to train GPRInvNet. The model testing has been\nconducted for validation, and experimental results show that GPRInvNet achieves\nsatisfactory results on real data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 03:43:09 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 03:35:45 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Liu", "Bin", ""], ["Ren", "Yuxiao", ""], ["Liu", "Hanchi", ""], ["Xu", "Hui", ""], ["Wang", "Zhengfang", ""], ["Cohn", "Anthony G.", ""], ["Jiang", "Peng", ""]]}, {"id": "1912.05763", "submitter": "Liangzhi Li", "authors": "Liangzhi Li, Manisha Verma, Yuta Nakashima, Hajime Nagahara, Ryo\n  Kawasaki", "title": "IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in\n  Vessel Networks", "comments": "Accepted in 2020 Winter Conference on Applications of Computer Vision\n  (WACV 20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is of great interest for diagnosis of retinal\nvascular diseases. To further improve the performance of vessel segmentation,\nwe propose IterNet, a new model based on UNet, with the ability to find\nobscured details of the vessel from the segmented vessel image itself, rather\nthan the raw input image. IterNet consists of multiple iterations of a\nmini-UNet, which can be 4$\\times$ deeper than the common UNet. IterNet also\nadopts the weight-sharing and skip-connection features to facilitate training;\ntherefore, even with such a large architecture, IterNet can still learn from\nmerely 10$\\sim$20 labeled images, without pre-training or any prior knowledge.\nIterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream\ndatasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are\nthe best scores in the literature. The source code is available.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 04:03:57 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Li", "Liangzhi", ""], ["Verma", "Manisha", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Kawasaki", "Ryo", ""]]}, {"id": "1912.05765", "submitter": "Sarkar Snigdha Sarathi Das", "authors": "Sarkar Snigdha Sarathi Das, Syed Md. Mukit Rashid, and Mohammed Eunus\n  Ali", "title": "CCCNet: An Attention Based Deep Learning Framework for Categorized Crowd\n  Counting", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting problem that counts the number of people in an image has been\nextensively studied in recent years. In this paper, we introduce a new variant\nof crowd counting problem, namely \"Categorized Crowd Counting\", that counts the\nnumber of people sitting and standing in a given image. Categorized crowd\ncounting has many real-world applications such as crowd monitoring, customer\nservice, and resource management. The major challenges in categorized crowd\ncounting come from high occlusion, perspective distortion and the seemingly\nidentical upper body posture of sitting and standing persons. Existing density\nmap based approaches perform well to approximate a large crowd, but lose\nimportant local information necessary for categorization. On the other hand,\ntraditional detection-based approaches perform poorly in occluded environments,\nespecially when the crowd size gets bigger. Hence, to solve the categorized\ncrowd counting problem, we develop a novel attention-based deep learning\nframework that addresses the above limitations. In particular, our approach\nworks in three phases: i) We first generate basic detection based sitting and\nstanding density maps to capture the local information; ii) Then, we generate a\ncrowd counting based density map as global counting feature; iii) Finally, we\nhave a cross-branch segregating refinement phase that splits the crowd density\nmap into final sitting and standing density maps using attention mechanism.\nExtensive experiments show the efficacy of our approach in solving the\ncategorized crowd counting problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 04:13:54 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Das", "Sarkar Snigdha Sarathi", ""], ["Rashid", "Syed Md. Mukit", ""], ["Ali", "Mohammed Eunus", ""]]}, {"id": "1912.05766", "submitter": "Vinit Chunilal Sarode", "authors": "Vinit Sarode, Xueqian Li, Hunter Goforth, Yasuhiro Aoki, Animesh\n  Dhagat, Rangaprasad Arun Srivatsan, Simon Lucey, Howie Choset", "title": "One Framework to Register Them All: PointNet Encoding for Point Cloud\n  Alignment", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1908.07906", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PointNet has recently emerged as a popular representation for unstructured\npoint cloud data, allowing application of deep learning to tasks such as object\ndetection, segmentation and shape completion. However, recent works in\nliterature have shown the sensitivity of the PointNet representation to pose\nmisalignment. This paper presents a novel framework that uses PointNet encoding\nto align point clouds and perform registration for applications such as 3D\nreconstruction, tracking and pose estimation. We develop a framework that\ncompares PointNet features of template and source point clouds to find the\ntransformation that aligns them accurately. In doing so, we avoid\ncomputationally expensive correspondence finding steps, that are central to\npopular registration methods such as ICP and its variants. Depending on the\nprior information about the shape of the object formed by the point clouds, our\nframework can produce approaches that are shape specific or general to unseen\nshapes. Our framework produces approaches that are robust to noise and initial\nmisalignment in data and work robustly with sparse as well as partial point\nclouds. We perform extensive simulation and real-world experiments to validate\nthe efficacy of our approach and compare the performance with state-of-art\napproaches. Code is available at\nhttps://github.com/vinits5/pointnet-registrationframework.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 04:16:47 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Sarode", "Vinit", ""], ["Li", "Xueqian", ""], ["Goforth", "Hunter", ""], ["Aoki", "Yasuhiro", ""], ["Dhagat", "Animesh", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Lucey", "Simon", ""], ["Choset", "Howie", ""]]}, {"id": "1912.05790", "submitter": "Tong Shen", "authors": "Jia Li, Tong Shen, Wei Zhang, Hui Ren, Dan Zeng, Tao Mei", "title": "Zooming into Face Forensics: A Pixel-level Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stunning progress in face manipulation methods has made it possible to\nsynthesize realistic fake face images, which poses potential threats to our\nsociety. It is urgent to have face forensics techniques to distinguish those\ntampered images. A large scale dataset \"FaceForensics++\" has provided enormous\ntraining data generated from prominent face manipulation methods to facilitate\nanti-fake research. However, previous works focus more on casting it as a\nclassification problem by only considering a global prediction. Through\ninvestigation to the problem, we find that training a classification network\noften fails to capture high quality features, which might lead to sub-optimal\nsolutions. In this paper, we zoom in on the problem by conducting a pixel-level\nanalysis, i.e. formulating it as a pixel-level segmentation task. By evaluating\nmultiple architectures on both segmentation and classification tasks, We show\nthe superiority of viewing the problem from a segmentation perspective.\nDifferent ablation studies are also performed to investigate what makes an\neffective and efficient anti-fake model. Strong baselines are also established,\nwhich, we hope, could shed some light on the field of face forensics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 06:17:37 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Li", "Jia", ""], ["Shen", "Tong", ""], ["Zhang", "Wei", ""], ["Ren", "Hui", ""], ["Zeng", "Dan", ""], ["Mei", "Tao", ""]]}, {"id": "1912.05831", "submitter": "Shayan Hassantabar", "authors": "Shayan Hassantabar, Xiaoliang Dai, Niraj K. Jha", "title": "STEERAGE: Synthesis of Neural Networks Using Architecture Search and\n  Grow-and-Prune Methods", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks (NNs) have been successfully deployed in many applications.\nHowever, architectural design of these models is still a challenging problem.\nMoreover, neural networks are known to have a lot of redundancy. This increases\nthe computational cost of inference and poses an obstacle to deployment on\nInternet-of-Thing sensors and edge devices. To address these challenges, we\npropose the STEERAGE synthesis methodology. It consists of two complementary\napproaches: efficient architecture search, and grow-and-prune NN synthesis. The\nfirst step, covered in a global search module, uses an accuracy predictor to\nefficiently navigate the architectural search space. The predictor is built\nusing boosted decision tree regression, iterative sampling, and efficient\nevolutionary search. The second step involves local search. By using various\ngrow-and-prune methodologies for synthesizing convolutional and feed-forward\nNNs, it reduces the network redundancy, while boosting its performance. We have\nevaluated STEERAGE performance on various datasets, including MNIST and\nCIFAR-10. On MNIST dataset, our CNN architecture achieves an error rate of\n0.66%, with 8.6x fewer parameters compared to the LeNet-5 baseline. For the\nCIFAR-10 dataset, we used the ResNet architectures as the baseline. Our\nSTEERAGE-synthesized ResNet-18 has a 2.52% accuracy improvement over the\noriginal ResNet-18, 1.74% over ResNet-101, and 0.16% over ResNet-1001, while\nhaving comparable number of parameters and FLOPs to the original ResNet-18.\nThis shows that instead of just increasing the number of layers to increase\naccuracy, an alternative is to use a better NN architecture with fewer layers.\nIn addition, STEERAGE achieves an error rate of just 3.86% with a variant of\nResNet architecture with 40 layers. To the best of our knowledge, this is the\nhighest accuracy obtained by ResNet-based architectures on the CIFAR-10\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:42:13 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Hassantabar", "Shayan", ""], ["Dai", "Xiaoliang", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1912.05845", "submitter": "Anthony Ortiz", "authors": "Anthony Ortiz, Caleb Robinson, Dan Morris, Olac Fuentes, Christopher\n  Kiekintveld, Md Mahmudulla Hassan and Nebojsa Jojic", "title": "Local Context Normalization: Revisiting Local Normalization", "comments": "Accepted as a CVPR 2020 oral paper. arXiv admin note: text overlap\n  with arXiv:1803.08494 by other authors", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers have been shown to improve convergence in deep neural\nnetworks, and even add useful inductive biases. In many vision applications the\nlocal spatial context of the features is important, but most common\nnormalization schemes including Group Normalization (GN), Instance\nNormalization (IN), and Layer Normalization (LN) normalize over the entire\nspatial dimension of a feature. This can wash out important signals and degrade\nperformance. For example, in applications that use satellite imagery, input\nimages can be arbitrarily large; consequently, it is nonsensical to normalize\nover the entire area. Positional Normalization (PN), on the other hand, only\nnormalizes over a single spatial position at a time. A natural compromise is to\nnormalize features by local context, while also taking into account group level\ninformation. In this paper, we propose Local Context Normalization (LCN): a\nnormalization layer where every feature is normalized based on a window around\nit and the filters in its group. We propose an algorithmic solution to make LCN\nefficient for arbitrary window sizes, even if every point in the image has a\nunique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN\ncounterparts for object detection, semantic segmentation, and instance\nsegmentation applications in several benchmark datasets, while keeping\nperformance independent of the batch size and facilitating transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 09:28:24 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 06:22:50 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 09:27:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ortiz", "Anthony", ""], ["Robinson", "Caleb", ""], ["Morris", "Dan", ""], ["Fuentes", "Olac", ""], ["Kiekintveld", "Christopher", ""], ["Hassan", "Md Mahmudulla", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1912.05846", "submitter": "Jonathan Heras", "authors": "\\'Angela Casado-Garc\\'ia and C\\'esar Dom\\'inguez and J\\'onathan Heras\n  and Eloy Mata and Vico Pascual", "title": "The Benefits of Close-Domain Fine-Tuning for Table Detection in Document\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correct localisation of tables in a document is instrumental for\ndetermining their structure and extracting their contents; therefore, table\ndetection is a key step in table understanding. Nowadays, the most successful\nmethods for table detection in document images employ deep learning algorithms;\nand, particularly, a technique known as fine-tuning. In this context, such a\ntechnique exports the knowledge acquired to detect objects in natural images to\ndetect tables in document images. However, there is only a vague relation\nbetween natural and document images, and fine-tuning works better when there is\na close relation between the source and target task. In this paper, we show\nthat it is more beneficial to employ fine-tuning from a closer domain. To this\naim, we train different object detection algorithms (namely, Mask R-CNN,\nRetinaNet, SSD and YOLO) using the TableBank dataset (a dataset of images of\nacademic documents designed for table detection and recognition), and fine-tune\nthem for several heterogeneous table detection datasets. Using this approach,\nwe considerably improve the accuracy of the detection models fine-tuned from\nnatural images (in mean a 17%, and, in the best case, up to a 60%).\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 09:30:02 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Casado-Garc\u00eda", "\u00c1ngela", ""], ["Dom\u00ednguez", "C\u00e9sar", ""], ["Heras", "J\u00f3nathan", ""], ["Mata", "Eloy", ""], ["Pascual", "Vico", ""]]}, {"id": "1912.05864", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Totally Deep Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) have been successful in solving many computer\nvision tasks including image and video category recognition especially for\nsmall and mid-scale training problems. The principle of these non-parametric\nmodels is to learn hyperplanes that separate data belonging to different\nclasses while maximizing their margins. However, SVMs constrain the learned\nhyperplanes to lie in the span of support vectors, fixed/taken from training\ndata, and this reduces their representational power and may lead to limited\ngeneralization performances. In this paper, we relax this constraint and allow\nthe support vectors to be learned (instead of being fixed/taken from training\ndata) in order to better fit a given classification task. Our approach,\nreferred to as deep total variation support vector machines, is parametric and\nrelies on a novel deep architecture that learns not only the SVM and the kernel\nparameters but also the support vectors, resulting into highly effective\nclassifiers. We also show (under a particular setting of the activation\nfunctions in this deep architecture) that a large class of kernels and their\ncombinations can be learned. Experiments conducted on the challenging task of\nskeleton-based action recognition show the outperformance of our deep total\nvariation SVMs w.r.t different baselines as well as the related work.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 10:18:17 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "1912.05888", "submitter": "Aaron Wewior", "authors": "Aaron Wewior, Joachim Weickert", "title": "Variational Coupling Revisited: Simpler Models, Theoretical Connections,\n  and Novel Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational models with coupling terms are becoming increasingly popular in\nimage analysis. They involve auxiliary variables, such that their energy\nminimisation splits into multiple fractional steps that can be solved easier\nand more efficiently. In our paper we show that coupling models offer a number\nof interesting properties that go far beyond their obvious numerical benefits.\nWe demonstrate that discontinuity-preserving denoising can be achieved even\nwith quadratic data and smoothness terms, provided that the coupling term\ninvolves the $L^1$ norm. We show that such an $L^1$ coupling term provides\nadditional information as a powerful edge detector that has remained unexplored\nso far. While coupling models in the literature approximate higher order\nregularisation, we argue that already first order coupling models can be\nuseful. As a specific example, we present a first order coupling model that\noutperforms classical TV regularisation. It also establishes a theoretical\nconnection between TV regularisation and the Mumford-Shah segmentation\napproach. Unlike other Mumford-Shah algorithms, it is a strictly convex\napproximation, for which we can guarantee convergence of a split Bregman\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 11:44:53 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Wewior", "Aaron", ""], ["Weickert", "Joachim", ""]]}, {"id": "1912.05905", "submitter": "Radu Alexandru Rosu", "authors": "Radu Alexandru Rosu, Peer Sch\\\"utt, Jan Quenzel, Sven Behnke", "title": "LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. However, applying the same\nmethods on 3D data still poses challenges due to the heavy memory requirements\nand the lack of structured data. Here, we propose LatticeNet, a novel approach\nfor 3D semantic segmentation, which takes as input raw point clouds. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on various datasets where our method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 13:01:36 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 15:29:58 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 16:33:59 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rosu", "Radu Alexandru", ""], ["Sch\u00fctt", "Peer", ""], ["Quenzel", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "1912.05909", "submitter": "Daniel Barath", "authors": "Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri Matas", "title": "MAGSAC++, a fast, reliable and accurate robust estimator", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.02295", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for robust estimation, MAGSAC++, is proposed. It introduces a\nnew model quality (scoring) function that does not require the inlier-outlier\ndecision, and a novel marginalization procedure formulated as an iteratively\nre-weighted least-squares approach. We also propose a new sampler, Progressive\nNAPSAC, for RANSAC-like robust estimators. Exploiting the fact that nearby\npoints often originate from the same model in real-world data, it finds local\nstructures earlier than global samplers. The progressive transition from local\nto global sampling does not suffer from the weaknesses of purely localized\nsamplers. On six publicly available real-world datasets for homography and\nfundamental matrix fitting, MAGSAC++ produces results superior to\nstate-of-the-art robust methods. It is faster, more geometrically accurate and\nfails less often.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:39:33 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Barath", "Daniel", ""], ["Noskova", "Jana", ""], ["Ivashechkin", "Maksym", ""], ["Matas", "Jiri", ""]]}, {"id": "1912.05949", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa", "title": "Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD\n  Filter with CNN-Based Re-Identification", "comments": "arXiv admin note: text overlap with arXiv:1908.03945", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel online multi-object visual tracking algorithm via a\ntracking-by-detection paradigm using a Gaussian mixture Probability Hypothesis\nDensity (GM-PHD) filter and deep Convolutional Neural Network (CNN) appearance\nrepresentations learning. The GM-PHD filter has a linear complexity with the\nnumber of objects and observations while estimating the states and cardinality\nof unknown and time-varying number of objects in the scene. Though it handles\nobject birth, death and clutter in a unified framework, it is susceptible to\nmiss-detections and does not include the identity of objects. We use\nvisual-spatio-temporal information obtained from object bounding boxes and\ndeeply learned appearance representations to perform estimates-to-tracks data\nassociation for labeling of each target as well as formulate an augmented\nlikelihood and then integrate into the update step of the GM-PHD filter. We\nlearn the deep CNN appearance representations by training an identification\nnetwork (IdNet) on large-scale person re-identification data sets. We also\nemploy additional unassigned tracks prediction after the data association step\nto overcome the susceptibility of the GM-PHD filter towards miss-detections\ncaused by occlusion. Our tracker which runs in real-time is applied to track\nmultiple objects in video sequences acquired under varying environmental\nconditions and objects density. Lastly, we make extensive evaluations on\nMultiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and\nfind out that our online tracker significantly outperforms several\nstate-of-the-art trackers in terms of tracking accuracy and identification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:18:42 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 23:10:44 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 21:06:07 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2020 16:19:20 GMT"}, {"version": "v5", "created": "Thu, 31 Dec 2020 13:34:25 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Baisa", "Nathanael L.", ""]]}, {"id": "1912.05950", "submitter": "Caroline Petitjean", "authors": "Z. Lambert, C. Petitjean, B. Dubray, S. Ruan", "title": "SegTHOR: Segmentation of Thoracic Organs at Risk in CT images", "comments": "Submitted to a journal in december 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of open science, public datasets, along with common experimental\nprotocol, help in the process of designing and validating data science\nalgorithms; they also contribute to ease reproductibility and fair comparison\nbetween methods. Many datasets for image segmentation are available, each\npresenting its own challenges; however just a very few exist for radiotherapy\nplanning. This paper is the presentation of a new dataset dedicated to the\nsegmentation of organs at risk (OARs) in the thorax, i.e. the organs\nsurrounding the tumour that must be preserved from irradiations during\nradiotherapy. This dataset is called SegTHOR (Segmentation of THoracic Organs\nat Risk). In this dataset, the OARs are the heart, the trachea, the aorta and\nthe esophagus, which have varying spatial and appearance characteristics. The\ndataset includes 60 3D CT scans, divided into a training set of 40 and a test\nset of 20 patients, where the OARs have been contoured manually by an\nexperienced radiotherapist. Along with the dataset, we present some baseline\nresults, obtained using both the original, state-of-the-art architecture U-Net\nand a simplified version. We investigate different configurations of this\nbaseline architecture that will serve as comparison for future studies on the\nSegTHOR dataset. Preliminary results show that room for improvement is left,\nespecially for smallest organs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 13:46:15 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Lambert", "Z.", ""], ["Petitjean", "C.", ""], ["Dubray", "B.", ""], ["Ruan", "S.", ""]]}, {"id": "1912.05971", "submitter": "Yucheng Zhu", "authors": "Yucheng Zhu, Xiongkuo Min, DanDan Zhu, Ke Gu, Jiantao Zhou, Guangtao\n  Zhai, Xiaokang Yang, and Wenjun Zhang", "title": "Toward Better Understanding of Saliency Prediction in Augmented 360\n  Degree Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) overlays digital content onto the reality. In AR\nsystem, correct and precise estimations of user's visual fixations and head\nmovements can enhance the quality of experience by allocating more computation\nresources on the areas of interest. However, there is inadequate research about\nunderstanding the visual exploration of users when using an AR system or\nmodeling AR visual attention. To bridge the gap between the saliency prediction\non real-world scene and on scene augmented by virtual information, we construct\nthe ARVR saliency dataset with 12 diverse videos viewed by 20 people. The\nvirtual reality (VR) technique is employed to simulate the real-world.\nAnnotations of object recognition and tracking as augmented contents are\nblended into the omnidirectional videos. The saliency annotations of head and\neye movements for both original and augmented videos are collected and together\nconstitute the ARVR dataset. We also design a model which is capable of solving\nthe saliency prediction problem in AR. Local block images are extracted to\nsimulate the viewport and offset the projection distortion. Conspicuous visual\ncues in local viewports are extracted to constitute the spatial features. The\noptical flow information is estimated as the important temporal feature. We\nalso consider the interplay between virtual information and reality. The\ncomposition of the augmentation information is distinguished, and the joint\neffects of adversarial augmentation and complementary augmentation are\nestimated. We generate a graph by taking each block image as one node. Both the\nvisual saliency mechanism and the characteristics of viewing behaviors are\nconsidered in the computation of edge weights on the graph which are\ninterpreted as Markov chains. The fraction of the visual attention that is\ndiverted to each block image is estimated through equilibrium distribution on\nof this chain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:16:05 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 13:54:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Yucheng", ""], ["Min", "Xiongkuo", ""], ["Zhu", "DanDan", ""], ["Gu", "Ke", ""], ["Zhou", "Jiantao", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1912.05992", "submitter": "Shengkai Wu", "authors": "Shengkai Wu, Xiaoping Li, Xinggang Wang", "title": "IoU-aware Single-stage Object Detector for Accurate Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the simpleness and high efficiency, single-stage object detectors have\nbeen widely applied in many computer vision applications . However, the low\ncorrelation between the classification score and localization accuracy of the\npredicted detections has severely hurt the localization accuracy of models. In\nthis paper, IoU-aware single-stage object detector is proposed to solve this\nproblem. Specifically, IoU-aware single-stage object detector predicts the IoU\nfor each detected box. Then the classification score and predicted IoU are\nmultiplied to compute the final detection confidence, which is more correlated\nwith the localization accuracy. The detection confidence is then used as the\ninput of the subsequent NMS and COCO AP computation, which will substantially\nimprove the localization accuracy of models. Sufficient experiments on COCO and\nPASCAL VOC datasets demonstrate the effectiveness of IoU-aware single-stage\nobject detector on improving model's localization accuracy. Without whistles\nand bells, the proposed method can substantially improve AP by $1.7\\%\\sim1.9\\%$\nand AP75 by $2.2\\%\\sim2.5\\%$ on COCO \\textit{test-dev}. On PASCAL VOC, the\nproposed method can substantially improve AP by $2.9\\%\\sim4.4\\%$ and AP80, AP90\nby $4.6\\%\\sim10.2\\%$. Code is available here:\n{https://github.com/ShengkaiWu/IoU-aware-single-stage-object-detector}.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:38:34 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 07:10:53 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 08:55:55 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 14:26:40 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Wu", "Shengkai", ""], ["Li", "Xiaoping", ""], ["Wang", "Xinggang", ""]]}, {"id": "1912.06010", "submitter": "Homayoun Valafar", "authors": "Liang Zhao, Brendan Odigwe, Susan Lessner, Daniel G. Clair, Firas\n  Mussa, Homayoun Valafar", "title": "Automated Analysis of Femoral Artery Calcification Using Machine\n  Learning Techniques", "comments": "6 pages, submitted for consideration to CSCI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report an object tracking algorithm that combines geometrical constraints,\nthresholding, and motion detection for tracking of the descending aorta and the\nnetwork of major arteries that branch from the aorta including the iliac and\nfemoral arteries. Using our automated identification and analysis, arterial\nsystem was identified with more than 85% success when compared to human\nannotation. Furthermore, the reported automated system is capable of producing\na stenosis profile, and a calcification score similar to the Agatston score.\nThe use of stenosis and calcification profiles will lead to the development of\nbetter-informed diagnostic and prognostic tools.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:02:24 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Zhao", "Liang", ""], ["Odigwe", "Brendan", ""], ["Lessner", "Susan", ""], ["Clair", "Daniel G.", ""], ["Mussa", "Firas", ""], ["Valafar", "Homayoun", ""]]}, {"id": "1912.06013", "submitter": "Gencer Sumbul", "authors": "Kexin Zhang, Gencer Sumbul, Beg\\\"um Demir", "title": "An Approach to Super-Resolution of Sentinel-2 Images Based on Generative\n  Adversarial Networks", "comments": "Accepted at IEEE Mediterranean and Middle-East Geoscience and Remote\n  Sensing Symposium (M2GARSS) 2020", "journal-ref": null, "doi": "10.1109/M2GARSS47143.2020.9105165", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generative adversarial network based super-resolution\n(SR) approach (which is called as S2GAN) to enhance the spatial resolution of\nSentinel-2 spectral bands. The proposed approach consists of two main steps.\nThe first step aims to increase the spatial resolution of the bands with 20m\nand 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To\nthis end, we introduce a generator network that performs SR on the lower\nresolution bands with the guidance of the bands associated to 10m spatial\nresolution by utilizing the convolutional layers with residual connections and\na long skip-connection between inputs and outputs. The second step aims to\ndistinguish SR bands from their ground truth bands. This is achieved by the\nproposed discriminator network, which alternately characterizes the high level\nfeatures of the two sets of bands and applying binary classification on the\nextracted features. Then, we formulate the adversarial learning of the\ngenerator and discriminator networks as a min-max game. In this learning\nprocedure, the generator aims to produce realistic SR bands as much as possible\nso that the discriminator incorrectly classifies SR bands. Experimental results\nobtained on different Sentinel-2 images show the effectiveness of the proposed\napproach compared to both conventional and deep learning based SR approaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:04:25 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 17:01:17 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Kexin", ""], ["Sumbul", "Gencer", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "1912.06015", "submitter": "Andre Manoel", "authors": "Gaspar Rochette, Andre Manoel, Eric W. Tramel", "title": "Efficient Per-Example Gradient Computations in Convolutional Neural\n  Networks", "comments": null, "journal-ref": "Theory and Practice of Differential Privacy (TPDP) workshop at CCS\n  2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks leverage GPUs to perform massively-parallel\ncomputations over batches of many training examples efficiently. However, for\ncertain tasks, one may be interested in performing per-example computations,\nfor instance using per-example gradients to evaluate a quantity of interest\nunique to each example. One notable application comes from the field of\ndifferential privacy, where per-example gradients must be norm-bounded in order\nto limit the impact of each example on the aggregated batch gradient. In this\nwork, we discuss how per-example gradients can be efficiently computed in\nconvolutional neural networks (CNNs). We compare existing strategies by\nperforming a few steps of differentially-private training on CNNs of varying\nsizes. We also introduce a new strategy for per-example gradient calculation,\nwhich is shown to be advantageous depending on the model architecture and how\nthe model is trained. This is a first step in making differentially-private\ntraining of CNNs practical.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:10:14 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Rochette", "Gaspar", ""], ["Manoel", "Andre", ""], ["Tramel", "Eric W.", ""]]}, {"id": "1912.06044", "submitter": "Mor Avi-Aharon", "authors": "Mor Avi-Aharon, Assaf Arbelle, and Tammy Riklin Raviv", "title": "Hue-Net: Intensity-based Image-to-Image Translation with Differentiable\n  Histogram Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Hue-Net - a novel Deep Learning framework for Intensity-based\nImage-to-Image Translation. The key idea is a new technique termed network\naugmentation which allows a differentiable construction of intensity histograms\nfrom images. We further introduce differentiable representations of (1D) cyclic\nand joint (2D) histograms and use them for defining loss functions based on\ncyclic Earth Mover's Distance (EMD) and Mutual Information (MI). While the\nHue-Net can be applied to several image-to-image translation tasks, we choose\nto demonstrate its strength on color transfer problems, where the aim is to\npaint a source image with the colors of a different target image. Note that the\ndesired output image does not exist and therefore cannot be used for supervised\npixel-to-pixel learning. This is accomplished by using the HSV color-space and\ndefining an intensity-based loss that is built on the EMD between the cyclic\nhue histograms of the output and the target images. To enforce color-free\nsimilarity between the source and the output images, we define a semantic-based\nloss by a differentiable approximation of the MI of these images. The\nincorporation of histogram loss functions in addition to an adversarial loss\nenables the construction of semantically meaningful and realistic images.\nPromising results are presented for different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:48:55 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Avi-Aharon", "Mor", ""], ["Arbelle", "Assaf", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1912.06075", "submitter": "Felix Denzinger", "authors": "Felix Denzinger, Michael Wels, Nishant Ravikumar, Katharina\n  Breininger, Anika Reidelsh\\\"ofer, Joachim Eckert, Michael S\\\"uhling, Axel\n  Schmermund, and Andreas Maier", "title": "Coronary Artery Plaque Characterization from CCTA Scans using Deep\n  Learning and Radiomics", "comments": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention. Springer, Cham, 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32251-9_65", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing coronary artery plaque segments in coronary CT angiography scans is\nan important task to improve patient management and clinical outcomes, as it\ncan help to decide whether invasive investigation and treatment are necessary.\nIn this work, we present three machine learning approaches capable of\nperforming this task. The first approach is based on radiomics, where a plaque\nsegmentation is used to calculate various shape-, intensity- and texture-based\nfeatures under different image transformations. A second approach is based on\ndeep learning and relies on centerline extraction as sole prerequisite. In the\nthird approach, we fuse the deep learning approach with radiomic features. On\nour data the methods reached similar scores as simulated fractional flow\nreserve (FFR) measurements, which - in contrast to our methods - requires an\nexact segmentation of the whole coronary tree and often time-consuming manual\ninteraction. In literature, the performance of simulated FFR reaches an AUC\nbetween 0.79-0.93 predicting an abnormal invasive FFR that demands\nrevascularization. The radiomics approach achieves an AUC of 0.86, the deep\nlearning approach 0.84 and the combined method 0.88 for predicting the\nrevascularization decision directly. While all three proposed methods can be\ndetermined within seconds, the FFR simulation typically takes several minutes.\nProvided representative training data in sufficient quantities, we believe that\nthe presented methods can be used to create systems for fully automatic\nnon-invasive risk assessment for a variety of adverse cardiac events.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:51:09 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 11:34:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Denzinger", "Felix", ""], ["Wels", "Michael", ""], ["Ravikumar", "Nishant", ""], ["Breininger", "Katharina", ""], ["Reidelsh\u00f6fer", "Anika", ""], ["Eckert", "Joachim", ""], ["S\u00fchling", "Michael", ""], ["Schmermund", "Axel", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.06079", "submitter": "Julian Tanke", "authors": "Julian Tanke, Andreas Weber, Juergen Gall", "title": "Human Motion Anticipation with Symbolic Label", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating human motion depends on two factors: the past motion and the\nperson's intention. While the first factor has been extensively utilized to\nforecast short sequences of human motion, the second one remains elusive. In\nthis work we approximate a person's intention via a symbolic representation,\nfor example fine-grained action labels such as walking or sitting down.\nForecasting a symbolic representation is much easier than forecasting the full\nbody pose with its complex inter-dependencies. However, knowing the future\nactions makes forecasting human motion easier. We exploit this connection by\nfirst anticipating symbolic labels and then generate human motion, conditioned\non the human motion input sequence as well as on the forecast labels. This\nallows the model to anticipate motion changes many steps ahead and adapt the\nposes accordingly. We achieve state-of-the-art results on short-term as well as\non long-term human motion forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:56:32 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 09:38:48 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tanke", "Julian", ""], ["Weber", "Andreas", ""], ["Gall", "Juergen", ""]]}, {"id": "1912.06102", "submitter": "Vijay Rengarajan", "authors": "Vijay Rengarajan, Shuo Zhao, Ruiwen Zhen, John Glotzbach, Hamid\n  Sheikh, Aswin C. Sankaranarayanan", "title": "Photosequencing of Motion Blur using Short and Long Exposures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photosequencing aims to transform a motion blurred image to a sequence of\nsharp images. This problem is challenging due to the inherent ambiguities in\ntemporal ordering as well as the recovery of lost spatial textures due to blur.\nAdopting a computational photography approach, we propose to capture two short\nexposure images, along with the original blurred long exposure image to aid in\nthe aforementioned challenges. Post-capture, we recover the sharp photosequence\nusing a novel blur decomposition strategy that recursively splits the long\nexposure image into smaller exposure intervals. We validate the approach by\ncapturing a variety of scenes with interesting motions using machine vision\ncameras programmed to capture short and long exposure sequences. Our\nexperimental results show that the proposed method resolves both fast and fine\nmotions better than prior works.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:23:14 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Rengarajan", "Vijay", ""], ["Zhao", "Shuo", ""], ["Zhen", "Ruiwen", ""], ["Glotzbach", "John", ""], ["Sheikh", "Hamid", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1912.06112", "submitter": "Hao Tang", "authors": "Hao Tang, Hong Liu, Nicu Sebe", "title": "Unified Generative Adversarial Networks for Controllable Image-to-Image\n  Translation", "comments": "Accepted to TIP, an extended version of a paper published in ACM MM\n  2018. arXiv admin note: substantial text overlap with arXiv:1808.04859", "journal-ref": null, "doi": "10.1109/TIP.2020.3021789", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified Generative Adversarial Network (GAN) for controllable\nimage-to-image translation, i.e., transferring an image from a source to a\ntarget domain guided by controllable structures. In addition to conditioning on\na reference image, we show how the model can generate images conditioned on\ncontrollable structures, e.g., class labels, object keypoints, human skeletons,\nand scene semantic maps. The proposed model consists of a single generator and\na discriminator taking a conditional image and the target controllable\nstructure as input. In this way, the conditional image can provide appearance\ninformation and the controllable structure can provide the structure\ninformation for generating the target result. Moreover, our model learns the\nimage-to-image mapping through three novel losses, i.e., color loss,\ncontrollable structure guided cycle-consistency loss, and controllable\nstructure guided self-content preserving loss. Also, we present the Fr\\'echet\nResNet Distance (FRD) to evaluate the quality of the generated images.\nExperiments on two challenging image translation tasks, i.e., hand\ngesture-to-gesture translation and cross-view image translation, show that our\nmodel generates convincing results, and significantly outperforms other\nstate-of-the-art methods on both tasks. Meanwhile, the proposed framework is a\nunified solution, thus it can be applied to solving other controllable\nstructure guided image translation tasks such as landmark guided facial\nexpression translation and keypoint guided person image generation. To the best\nof our knowledge, we are the first to make one GAN framework work on all such\ncontrollable structure guided image translation tasks. Code is available at\nhttps://github.com/Ha0Tang/GestureGAN.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:21:30 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 11:00:42 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Hong", ""], ["Sebe", "Nicu", ""]]}, {"id": "1912.06126", "submitter": "Kyle Genova", "authors": "Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas\n  Funkhouser", "title": "Local Deep Implicit Functions for 3D Shape", "comments": "Camera ready version for CVPR 2020 Oral. Prior to review, this paper\n  was referred to as DSIF, \"Deep Structured Implicit Functions.\" 11 pages, 9\n  figures. Project video at https://youtu.be/3RAITzNWVJs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to learn a 3D shape representation that enables\naccurate surface reconstruction, compact storage, efficient computation,\nconsistency for similar shapes, generalization across diverse shape categories,\nand inference from depth camera observations. Towards this end, we introduce\nLocal Deep Implicit Functions (LDIF), a 3D shape representation that decomposes\nspace into a structured set of learned implicit functions. We provide networks\nthat infer the space decomposition and local deep implicit functions from a 3D\nmesh or posed depth image. During experiments, we find that it provides 10.3\npoints higher surface reconstruction accuracy (F-Score) than the\nstate-of-the-art (OccNet), while requiring fewer than 1 percent of the network\nparameters. Experiments on posed depth image completion and generalization to\nunseen classes show 15.8 and 17.8 point improvements over the state-of-the-art,\nwhile producing a structured 3D representation for each input with consistency\nacross diverse shape collections.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:50:46 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 03:26:47 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Genova", "Kyle", ""], ["Cole", "Forrester", ""], ["Sud", "Avneesh", ""], ["Sarna", "Aaron", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1912.06135", "submitter": "Yuyang Liu", "authors": "Yuyang Liu and Yang Cong and Gan Sun", "title": "L3DOC: Lifelong 3D Object Classification", "comments": "10 pages,17 figures, CVPR 2020 underreview", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object classification has been widely-applied into both academic and\nindustrial scenarios. However, most state-of-the-art algorithms are facing with\na fixed 3D object classification task set, which cannot well tackle the new\ncoming data with incremental tasks as human ourselves. Meanwhile, the\nperformance of most state-of-the-art lifelong learning models can be\ndeteriorated easily on previously learned classification tasks, due to the\nexisting of unordered, large-scale, and irregular 3D geometry data. To address\nthis challenge, in this paper, we propose a Lifelong 3D Object Classification\n(i.e., L3DOC) framewor, which can consecutively learn new 3D object\nclassification tasks via imitating 'human learning'. Specifically, the core\nidea of our proposed L3DOC model is to factorize PointNet in a perspective of\nlifelong learning, while capturing and storing the shared point-knowledge in a\nperspective of layer-wise tensor factorization architecture. To further\ntransfer the task-specific knowledge from previous tasks to the new coming\nclassification task, a memory attention mechanism is proposed to connect the\ncurrent task with relevant previously tasks, which can effectively prevent\ncatastrophic forgetting via soft-transferring previous knowledge. To our best\nknowledge, this is the first work about using lifelong learning to handle 3D\nobject classification task without model fine-tuning or retraining.\nFurthermore, our L3DOC model can also be extended to other backbone network\n(e.g., PointNet++). To the end, comparisons on several point cloud datasets\nvalidate that our L3DOC model can reduce averaged 1.68~3.36 times parameters\nfor the overall model, without sacrificing classification accuracy of each\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 06:41:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 14:51:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Yuyang", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""]]}, {"id": "1912.06185", "submitter": "Himanshu Rai", "authors": "Yichao Lu, Cheng Chang, Himanshu Rai, Guangwei Yu, Maksims Volkovs", "title": "Learning Effective Visual Relationship Detector on 1 GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our winning solution to the Open Images 2019 Visual Relationship\nchallenge. This is the largest challenge of its kind to date with nearly 9\nmillion training images. Challenge task consists of detecting objects and\nidentifying relationships between them in complex scenes. Our solution has\nthree stages, first object detection model is fine-tuned for the challenge\nclasses using a novel weight transfer approach. Then, spatio-semantic and\nvisual relationship models are trained on candidate object pairs. Finally,\nfeatures and model predictions are combined to generate the final relationship\nprediction. Throughout the challenge we focused on minimizing the hardware\nrequirements of our architecture. Specifically, our weight transfer approach\nenables much faster optimization, allowing the entire architecture to be\ntrained on a single GPU in under two days. In addition to efficient\noptimization, our approach also achieves superior accuracy winning first place\nout of over 200 teams, and outperforming the second place team by over $5\\%$ on\nthe held-out private leaderboard.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:59:41 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Lu", "Yichao", ""], ["Chang", "Cheng", ""], ["Rai", "Himanshu", ""], ["Yu", "Guangwei", ""], ["Volkovs", "Maksims", ""]]}, {"id": "1912.06199", "submitter": "Artur Andr\\'e Almeida De Macedo Oliveira", "authors": "Artur A. M. Oliveira, Nina S. T. Hirata, Roberto Hirata Jr", "title": "Greenery Segmentation In Urban Images By Deep Learning", "comments": "Supplemental material can be found at\n  http://greenery_data.arturao.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vegetation is a relevant feature in the urban scenery and its awareness can\nbe measured in an image by the Green View Index (GVI). Previous approaches to\nestimate the GVI were based upon heuristics image processing approaches and\nrecently by deep learning networks (DLN). By leveraging some recent DLN\narchitectures tuned to the image segmentation problem and exploiting a\nweighting strategy in the loss function (LF) we improved previously reported\nresults in similar datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:35:15 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Oliveira", "Artur A. M.", ""], ["Hirata", "Nina S. T.", ""], ["Hirata", "Roberto", "Jr"]]}, {"id": "1912.06203", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr", "title": "ManiGAN: Text-Guided Image Manipulation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our paper is to semantically edit parts of an image matching a\ngiven text that describes desired attributes (e.g., texture, colour, and\nbackground), while preserving other contents that are irrelevant to the text.\nTo achieve this, we propose a novel generative adversarial network (ManiGAN),\nwhich contains two key components: text-image affine combination module (ACM)\nand detail correction module (DCM). The ACM selects image regions relevant to\nthe given text and then correlates the regions with corresponding semantic\nwords for effective manipulation. Meanwhile, it encodes original image features\nto help reconstruct text-irrelevant contents. The DCM rectifies mismatched\nattributes and completes missing contents of the synthetic image. Finally, we\nsuggest a new metric for evaluating image manipulation results, in terms of\nboth the generation of new attributes and the reconstruction of text-irrelevant\ncontents. Extensive experiments on the CUB and COCO datasets demonstrate the\nsuperior performance of the proposed method. Code is available at\nhttps://github.com/mrlibw/ManiGAN.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:48:52 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:42:35 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Lukasiewicz", "Thomas", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1912.06218", "submitter": "Chong Zhou", "authors": "Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee", "title": "YOLACT++: Better Real-time Instance Segmentation", "comments": "Journal extension of our previous conference paper arXiv:1904.02689", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3014297", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, fully-convolutional model for real-time (>30 fps)\ninstance segmentation that achieves competitive results on MS COCO evaluated on\na single Titan Xp, which is significantly faster than any previous\nstate-of-the-art approach. Moreover, we obtain this result after training on\nonly one GPU. We accomplish this by breaking instance segmentation into two\nparallel subtasks: (1) generating a set of prototype masks and (2) predicting\nper-instance mask coefficients. Then we produce instance masks by linearly\ncombining the prototypes with the mask coefficients. We find that because this\nprocess doesn't depend on repooling, this approach produces very high-quality\nmasks and exhibits temporal stability for free. Furthermore, we analyze the\nemergent behavior of our prototypes and show they learn to localize instances\non their own in a translation variant manner, despite being\nfully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster\nreplacement for standard NMS that only has a marginal performance penalty.\nFinally, by incorporating deformable convolutions into the backbone network,\noptimizing the prediction head with better anchor scales and aspect ratios, and\nadding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1\nmAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art\napproaches while still running at real-time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:58:03 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 03:42:52 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Bolya", "Daniel", ""], ["Zhou", "Chong", ""], ["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1912.06227", "submitter": "Bo Wu", "authors": "Jui-Hsin Lai, Bo Wu, Xin Wang, Dan Zeng, Tao Mei, Jingen Liu", "title": "Theme-Matters: Fashion Compatibility Learning via Theme Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion compatibility learning is important to many fashion markets such as\noutfit composition and online fashion recommendation. Unlike previous work, we\nargue that fashion compatibility is not only a visual appearance compatible\nproblem but also a theme-matters problem. An outfit, which consists of a set of\nfashion items (e.g., shirt, suit, shoes, etc.), is considered to be compatible\nfor a \"dating\" event, yet maybe not for a \"business\" occasion. In this paper,\nwe aim at solving the fashion compatibility problem given specific themes. To\nthis end, we built the first real-world theme-aware fashion dataset comprising\n14K around outfits labeled with 32 themes. In this dataset, there are more than\n40K fashion items labeled with 152 fine-grained categories. We also propose an\nattention model learning fashion compatibility given a specific theme. It\nstarts with a category-specific subspace learning, which projects compatible\noutfit items in certain categories to be close in the subspace. Thanks to\nstrong connections between fashion themes and categories, we then build a\ntheme-attention model over the category-specific embedding space. This model\nassociates themes with the pairwise compatibility with attention, and thus\ncompute the outfit-wise compatibility. To the best of our knowledge, this is\nthe first attempt to estimate outfit compatibility conditional on a theme. We\nconduct extensive qualitative and quantitative experiments on our new dataset.\nOur method outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:32:28 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 23:05:51 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 03:30:08 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lai", "Jui-Hsin", ""], ["Wu", "Bo", ""], ["Wang", "Xin", ""], ["Zeng", "Dan", ""], ["Mei", "Tao", ""], ["Liu", "Jingen", ""]]}, {"id": "1912.06253", "submitter": "Chao Yang Mr.", "authors": "Chao Yang and Ser-Nam Lim", "title": "Unconstrained Facial Expression Transfer using Style-based Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression transfer and reenactment has been an important research\nproblem given its applications in face editing, image manipulation, and\nfabricated videos generation. We present a novel method for image-based facial\nexpression transfer, leveraging the recent style-based GAN shown to be very\neffective for creating realistic looking images. Given two face images, our\nmethod can create plausible results that combine the appearance of one image\nand the expression of the other. To achieve this, we first propose an\noptimization procedure based on StyleGAN to infer hierarchical style vector\nfrom an image that disentangle different attributes of the face. We further\nintroduce a linear combination scheme that fuses the style vectors of the two\ngiven images and generate a new face that combines the expression and\nappearance of the inputs. Our method can create high-quality synthesis with\naccurate facial reenactment. Unlike many existing methods, we do not rely on\ngeometry annotations, and can be applied to unconstrained facial images of any\nidentities without the need for retraining, making it feasible to generate\nlarge-scale expression-transferred results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:01:15 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Yang", "Chao", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1912.06258", "submitter": "Yuanxin Zhong", "authors": "Yiqun Dong, Yuanxin Zhong, Wenbo Yu, Minghan Zhu, Pingping Lu, Yeyang\n  Fang, Jiajun Hong, Huei Peng", "title": "Mcity Data Collection for Automated Vehicles Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to introduce the data collection effort at\nMcity targeting automated vehicle development. We captured a comprehensive set\nof data from a set of perception sensors (Lidars, Radars, Cameras) as well as\nvehicle steering/brake/throttle inputs and an RTK unit. Two in-cabin cameras\nrecord the human driver's behaviors for possible future use. The naturalistic\ndriving on selected open roads is recorded at different time of day and weather\nconditions. We also perform designed choreography data collection inside the\nMcity test facility focusing on vehicle to vehicle, and vehicle to vulnerable\nroad user interactions which is quite unique among existing open-source\ndatasets. The vehicle platform, data content, tags/labels, and selected\nanalysis results are shown in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:13:59 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Dong", "Yiqun", ""], ["Zhong", "Yuanxin", ""], ["Yu", "Wenbo", ""], ["Zhu", "Minghan", ""], ["Lu", "Pingping", ""], ["Fang", "Yeyang", ""], ["Hong", "Jiajun", ""], ["Peng", "Huei", ""]]}, {"id": "1912.06265", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Xiaofeng Liu, Qingming Tang and C.-C. Jay Kuo", "title": "Towards Disentangled Representations for Human Retargeting by Multi-view\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning disentangled representations for data across\nmultiple domains and its applications in human retargeting. Our goal is to map\nan input image to an identity-invariant latent representation that captures\nintrinsic factors such as expressions and poses. To this end, we present a\nnovel multi-view learning approach that leverages various data sources such as\nimages, keypoints, and poses. Our model consists of multiple id-conditioned\nVAEs for different views of the data. During training, we encourage the latent\nembeddings to be consistent across these views. Our observation is that\nauxiliary data like keypoints and poses contain critical, id-agnostic semantic\ninformation, and it is easier to train a disentangling CVAE on these simpler\nviews to separate such semantics from other id-specific attributes. We show\nthat training multi-view CVAEs and encourage latent-consistency guides the\nimage encoding to preserve the semantics of expressions and poses, leading to\nimproved disentangled representations and better human retargeting results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:35:25 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Yang", "Chao", ""], ["Liu", "Xiaofeng", ""], ["Tang", "Qingming", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1912.06268", "submitter": "Gengshan Yang", "authors": "Gengshan Yang, Peiyun Hu and Deva Ramanan", "title": "Inferring Distributions Over Depth from a Single Image", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building a geometric scene understanding system for autonomous vehicles,\nit is crucial to know when the system might fail. Most contemporary approaches\ncast the problem as depth regression, whose output is a depth value for each\npixel. Such approaches cannot diagnose when failures might occur. One\nattractive alternative is a deep Bayesian network, which captures uncertainty\nin both model parameters and ambiguous sensor measurements. However, estimating\nuncertainties is often slow and the distributions are often limited to be\nuni-modal. In this paper, we recast the continuous problem of depth regression\nas discrete binary classification, whose output is an un-normalized\ndistribution over possible depths for each pixel. Such output allows one to\nreliably and efficiently capture multi-modal depth distributions in ambiguous\ncases, such as depth discontinuities and reflective surfaces. Results on\nstandard benchmarks show that our method produces accurate depth predictions\nand significantly better uncertainty estimations than prior art while running\nnear real-time. Finally, by making use of uncertainties of the predicted\ndistribution, we significantly reduce streak-like artifacts and improves\naccuracy as well as memory efficiency in 3D map reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:56:57 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Yang", "Gengshan", ""], ["Hu", "Peiyun", ""], ["Ramanan", "Deva", ""]]}, {"id": "1912.06274", "submitter": "Pau Panareda Busto", "authors": "Pau Panareda Busto and Juergen Gall", "title": "Joint Viewpoint and Keypoint Estimation with Real and Synthetic Data", "comments": "11 pages, 4 figures", "journal-ref": "German Conference on Pattern Recognition. Lecture Notes in\n  Computer Science, Volume 11824, Year 2019, Pages 107-121", "doi": "10.1007/978-3-030-33676-9_8", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The estimation of viewpoints and keypoints effectively enhance object\ndetection methods by extracting valuable traits of the object instances. While\nthe output of both processes differ, i.e., angles vs. list of characteristic\npoints, they indeed share the same focus on how the object is placed in the\nscene, inducing that there is a certain level of correlation between them.\nTherefore, we propose a convolutional neural network that jointly computes the\nviewpoint and keypoints for different object categories. By training both tasks\ntogether, each task improves the accuracy of the other. Since the labelling of\nobject keypoints is very time consuming for human annotators, we also introduce\na new synthetic dataset with automatically generated viewpoint and keypoints\nannotations. Our proposed network can also be trained on datasets that contain\nviewpoint and keypoints annotations or only one of them. The experiments show\nthat the proposed approach successfully exploits this implicit correlation\nbetween the tasks and outperforms previous techniques that are trained\nindependently.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 00:46:19 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Busto", "Pau Panareda", ""], ["Gall", "Juergen", ""]]}, {"id": "1912.06290", "submitter": "Sean Hendryx", "authors": "Sean M. Hendryx, Andrew B. Leach, Paul D. Hein, Clayton T. Morrison", "title": "Meta-Learning Initializations for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend first-order model agnostic meta-learning algorithms (including\nFOMAML and Reptile) to image segmentation, present a novel neural network\narchitecture built for fast learning which we call EfficientLab, and leverage a\nformal definition of the test error of meta-learning algorithms to decrease\nerror on out of distribution tasks. We show state of the art results on the\nFSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian\noptimization to infer the optimal test-time adaptation routine hyperparameters.\nWe also construct a small benchmark dataset, FP-k, for the empirical study of\nhow meta-learning systems perform in both few- and many-shot settings. On the\nFP-k dataset, we show that meta-learned initializations provide value for\ncanonical few-shot image segmentation but their performance is quickly matched\nby conventional transfer learning with performance being equal beyond 10\nlabeled examples. Our code, meta-learned model, and the FP-k dataset are\navailable at https://github.com/ml4ai/mliis .\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:58:36 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 06:44:55 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 20:52:49 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 23:33:07 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Hendryx", "Sean M.", ""], ["Leach", "Andrew B.", ""], ["Hein", "Paul D.", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1912.06295", "submitter": "Ye Yuan", "authors": "Ye Yuan, Jian Guan, Pengming Feng, Yanxia Wu", "title": "A Practical Solution for SAR Despeckling With Adversarial Learning\n  Generated Speckled-to-Speckled Images", "comments": "5 pages, 4 figures", "journal-ref": "IEEE Geoscience and Remote Sensing Letters,(2020)1-5", "doi": "10.1109/LGRS.2020.3034470", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we aim to address a synthetic aperture radar (SAR)\ndespeckling problem with the necessity of neither clean (speckle-free) SAR\nimages nor independent speckled image pairs from the same scene, and a\npractical solution for SAR despeckling (PSD) is proposed. First, an adversarial\nlearning framework is designed to generate speckled-to-speckled (S2S) image\npairs from the same scene in the situation where only single speckled SAR\nimages are available. Then, the S2S SAR image pairs are employed to train a\nmodified despeckling Nested-UNet model using the Noise2Noise (N2N) strategy.\nMoreover, an iterative version of the PSD method (PSDi) is also presented.\nExperiments are conducted on both synthetic speckled and real SAR data to\ndemonstrate the superiority of the proposed methods compared with several\nstate-of-the-art methods. The results show that our methods can reach a good\ntradeoff between feature preservation and speckle suppression.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 02:16:29 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 20:48:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yuan", "Ye", ""], ["Guan", "Jian", ""], ["Feng", "Pengming", ""], ["Wu", "Yanxia", ""]]}, {"id": "1912.06314", "submitter": "Jialing Lyu", "authors": "Jialing Lyu, Weichao Qiu, Xinyue Wei, Yi Zhang, Alan Yuille, Zheng-Jun\n  Zha", "title": "Identity Preserve Transform: Understand What Activity Classification\n  Models Have Learnt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity classification has observed great success recently. The performance\non small dataset is almost saturated and people are moving towards larger\ndatasets. What leads to the performance gain on the model and what the model\nhas learnt? In this paper we propose identity preserve transform (IPT) to study\nthis problem. IPT manipulates the nuisance factors (background, viewpoint,\netc.) of the data while keeping those factors related to the task (human\nmotion) unchanged. To our surprise, we found popular models are using highly\ncorrelated information (background, object) to achieve high classification\naccuracy, rather than using the essential information (human motion). This can\nexplain why an activity classification model usually fails to generalize to\ndatasets it is not trained on. We implement IPT in two forms, i.e. image-space\ntransform and 3D transform, using synthetic images. The tool will be made\nopen-source to help study model and dataset design.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:55:07 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Lyu", "Jialing", ""], ["Qiu", "Weichao", ""], ["Wei", "Xinyue", ""], ["Zhang", "Yi", ""], ["Yuille", "Alan", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "1912.06316", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jinsong Su, Jiebo Luo", "title": "Grounding-Tracking-Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Tracking by Language that localizes the target box\nsequence in a video based on a language query. We propose a framework called\nGTI that decomposes the problem into three sub-tasks: Grounding, Tracking, and\nIntegration. The three sub-task modules operate simultaneously and predict the\nbox sequence frame-by-frame. \"Grounding\" predicts the referred region directly\nfrom the language query. \"Tracking\" localizes the target based on the history\nof the grounded regions in previous frames. \"Integration\" generates final\npredictions by synergistically combining grounding and tracking. With the\n\"integration\" task as the key, we explore how to indicate the quality of the\ngrounded regions in each frame and achieve the desired mutually beneficial\ncombination. To this end, we propose an \"RT-integration\" method that defines\nand predicts two scores to guide the integration: 1) R-score represents the\nRegion correctness whether the grounding prediction accurately covers the\ntarget, and 2) T-score represents the Template quality whether the region\nprovides informative visual cues to improve tracking in future frames. We\npresent our real-time GTI implementation with the proposed RT-integration, and\nbenchmark the framework on LaSOT and Lingual OTB99 with highly promising\nresults. Moreover, we produce a disambiguated version of LaSOT queries to\nfacilitate future tracking by language studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 04:07:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 17:09:44 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Kumar", "Tushar", ""], ["Chen", "Tianlang", ""], ["Su", "Jinsong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.06319", "submitter": "Marcella Astrid", "authors": "Jeong-Seon Lim, Marcella Astrid, Hyun-Jin Yoon, Seung-Ik Lee", "title": "Small Object Detection using Context and Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many limitations applying object detection algorithm on various\nenvironments. Especially detecting small objects is still challenging because\nthey have low resolution and limited information. We propose an object\ndetection method using context for improving accuracy of detecting small\nobjects. The proposed method uses additional features from different layers as\ncontext by concatenating multi-scale features. We also propose object detection\nwith attention mechanism which can focus on the object in image, and it can\ninclude contextual information from target layer. Experimental results shows\nthat proposed method also has higher accuracy than conventional SSD on\ndetecting small objects. Also, for 300$\\times$300 input, we achieved 78.1% Mean\nAverage Precision (mAP) on the PASCAL VOC2007 test set.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 04:19:48 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 04:39:29 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lim", "Jeong-Seon", ""], ["Astrid", "Marcella", ""], ["Yoon", "Hyun-Jin", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "1912.06321", "submitter": "Joanne Truong", "authors": "Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik\n  Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra", "title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World\n  Performance?", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (RA-L) 2020", "doi": "10.1109/LRA.2020.3013848", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does progress in simulation translate to progress on robots? If one method\noutperforms another in simulation, how likely is that trend to hold in reality\non a robot? We examine this question for embodied PointGoal navigation,\ndeveloping engineering tools and a research paradigm for evaluating a simulator\nby its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),\na library for seamless execution of identical code on simulated agents and\nrobots, transferring simulation-trained agents to a LoCoBot platform with a\none-line code change. Second, we investigate the sim2real predictivity of\nHabitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create\na virtualized replica, and run parallel tests of 9 different models in reality\nand simulation. We present a new metric called Sim-vs-Real Correlation\nCoefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as\nused for the CVPR19 challenge is low (0.18 for the success metric), suggesting\nthat performance differences in this simulator-based challenge do not persist\nafter physical deployment. This gap is largely due to AI agents learning to\nexploit simulator imperfections, abusing collision dynamics to 'slide' along\nwalls, leading to shortcuts through otherwise non-navigable space. Naturally,\nsuch exploits do not work in the real world. Our experiments show that it is\npossible to tune simulation parameters to improve sim2real predictivity (e.g.\nimproving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that\nin-simulation comparisons will translate to deployed systems in reality.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 04:29:38 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 03:26:55 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kadian", "Abhishek", ""], ["Truong", "Joanne", ""], ["Gokaslan", "Aaron", ""], ["Clegg", "Alexander", ""], ["Wijmans", "Erik", ""], ["Lee", "Stefan", ""], ["Savva", "Manolis", ""], ["Chernova", "Sonia", ""], ["Batra", "Dhruv", ""]]}, {"id": "1912.06324", "submitter": "Jiahang Wang", "authors": "Jiahang Wang, Wei Zhang, Weizhong Liu, Tao Mei", "title": "Down to the Last Detail: Virtual Try-on with Detail Carving", "comments": "Insufficient results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual try-on under arbitrary poses has attracted lots of research attention\ndue to its huge potential applications. However, existing methods can hardly\npreserve the details in clothing texture and facial identity (face, hair) while\nfitting novel clothes and poses onto a person. In this paper, we propose a\nnovel multi-stage framework to synthesize person images, where rich details in\nsalient regions can be well preserved. Specifically, a multi-stage framework is\nproposed to decompose the generation into spatial alignment followed by a\ncoarse-to-fine generation. To better preserve the details in salient areas such\nas clothing and facial areas, we propose a Tree-Block (tree dilated fusion\nblock) to harness multi-scale features in the generator networks. With\nend-to-end training of multiple stages, the whole framework can be jointly\noptimized for results with significantly better visual fidelity and richer\ndetails. Extensive experiments on standard datasets demonstrate that our\nproposed framework achieves the state-of-the-art performance, especially in\npreserving the visual details in clothing texture and facial identity. Our\nimplementation will be publicly available soon.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 04:55:19 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 13:48:08 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 14:44:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Jiahang", ""], ["Zhang", "Wei", ""], ["Liu", "Weizhong", ""], ["Mei", "Tao", ""]]}, {"id": "1912.06329", "submitter": "Kevin Liang", "authors": "Kevin J Liang, John B. Sigman, Gregory P. Spell, Dan Strellis, William\n  Chang, Felix Liu, Tejas Mehta, Lawrence Carin", "title": "Toward Automatic Threat Recognition for Airport X-ray Baggage Screening\n  with Deep Convolutional Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the safety of the traveling public, the Transportation Security\nAdministration (TSA) operates security checkpoints at airports in the United\nStates, seeking to keep dangerous items off airplanes. At these checkpoints,\nthe TSA employs a fleet of X-ray scanners, such as the Rapiscan 620DV, so\nTransportation Security Officers (TSOs) can inspect the contents of carry-on\npossessions. However, identifying and locating all potential threats can be a\nchallenging task. As a result, the TSA has taken a recent interest in deep\nlearning-based automated detection algorithms that can assist TSOs. In a\ncollaboration funded by the TSA, we collected a sizable new dataset of X-ray\nscans with a diverse set of threats in a wide array of contexts, trained\nseveral deep convolutional object detection models, and integrated such models\ninto the Rapiscan 620DV, resulting in functional prototypes capable of\noperating in real time. We show performance of our models on held-out\nevaluation sets, analyze several design parameters, and demonstrate the\npotential of such systems for automated detection of threats that can be found\nin airports.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 05:26:25 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liang", "Kevin J", ""], ["Sigman", "John B.", ""], ["Spell", "Gregory P.", ""], ["Strellis", "Dan", ""], ["Chang", "William", ""], ["Liu", "Felix", ""], ["Mehta", "Tejas", ""], ["Carin", "Lawrence", ""]]}, {"id": "1912.06347", "submitter": "Yusheng Wu", "authors": "Zhifeng Yu, Yusheng Wu, Tianyou Wang", "title": "A Method for Arbitrary Instance Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to synthesize style and content of different images to form a\nvisually coherent image holds great promise in various applications such as\nstylistic painting, design prototyping, image editing, and augmented reality.\nHowever, the majority of works in image style transfer have focused on\ntransferring the style of an image to the entirety of another image, and only a\nvery small number of works have experimented on methods to transfer style to an\ninstance of another image. Researchers have proposed methods to circumvent the\ndifficulty of transferring style to an instance in an arbitrary shape. In this\npaper, we propose a topologically inspired algorithm called Forward Stretching\nto tackle this problem by transforming an instance into a tensor\nrepresentation, which allows us to transfer style to this instance itself\ndirectly. Forward Stretching maps pixels to specific positions and interpolate\nvalues between pixels to transform an instance to a tensor. This algorithm\nallows us to introduce a method to transfer arbitrary style to an instance in\nan arbitrary shape. We showcase the results of our method in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:42:35 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Yu", "Zhifeng", ""], ["Wu", "Yusheng", ""], ["Wang", "Tianyou", ""]]}, {"id": "1912.06348", "submitter": "Haojie Liu", "authors": "Haojie Liu, Han shen, Lichao Huang, Ming Lu, Tong Chen, Zhan Ma", "title": "Learned Video Compression via Joint Spatial-Temporal Correlation\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional video compression technologies have been developed over decades\nin pursuit of higher coding efficiency. Efficient temporal information\nrepresentation plays a key role in video coding. Thus, in this paper, we\npropose to exploit the temporal correlation using both first-order optical flow\nand second-order flow prediction. We suggest an one-stage learning approach to\nencapsulate flow as quantized features from consecutive frames which is then\nentropy coded with adaptive contexts conditioned on joint spatial-temporal\npriors to exploit second-order correlations. Joint priors are embedded in\nautoregressive spatial neighbors, co-located hyper elements and temporal\nneighbors using ConvLSTM recurrently. We evaluate our approach for the\nlow-delay scenario with High-Efficiency Video Coding (H.265/HEVC), H.264/AVC\nand another learned video compression method, following the common test\nsettings. Our work offers the state-of-the-art performance, with consistent\ngains across all popular test sequences.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:45:44 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liu", "Haojie", ""], ["shen", "Han", ""], ["Huang", "Lichao", ""], ["Lu", "Ming", ""], ["Chen", "Tong", ""], ["Ma", "Zhan", ""]]}, {"id": "1912.06353", "submitter": "Jaime Garcia Guevara", "authors": "Jaime Garcia Guevara (MIMESIS), Igor Peterlik (IHU Strasbourg),\n  Marie-Odile Berger (MAGRIT), St\\'ephane Cotin (MIMESIS)", "title": "Elastic registration based on compliance analysis and biomechanical\n  graph matching", "comments": "Annals of Biomedical Engineering, Springer Verlag, 2019", "journal-ref": null, "doi": "10.1007/s10439-019-02364-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic elastic registration method suited for vascularized organs is\nproposed. The vasculature in both the preoperative and intra-operative images\nis represented as a graph. A typical application of this method is the fusion\nof pre-operative information onto the organ during surgery, to compensate for\nthe limited details provided by the intra-operative imaging modality (e.g.\nCBCT) and to cope with changes in the shape of the organ. Due to image\nmodalities differences and organ deformation, each graph has a different\ntopology and shape. The Adaptive Compliance Graph Matching (ACGM) method\npresented does not require any manual initialization, handles intra-operative\nnonrigid deformations of up to 65 mm and computes a complete displacement field\nover the organ from only the matched vasculature. ACGM is better than the\nprevious Biomechanical Graph Matching method 3 (BGM) because it uses an\nefficient biomechanical vascularized liver model to compute the organ's\ntransformation and the vessels bifurcations compliance. This allows to\nefficiently find the best graph matches with a novel compliance-based adaptive\nsearch. These contributions are evaluated on ten realistic synthetic and two\nreal porcine automatically segmented datasets. ACGM obtains better target\nregistration error (TRE) than BGM, with an average TRE in the real datasets of\n4.2 mm compared to 6.5 mm, respectively. It also is up to one order of\nmagnitude faster, less dependent on the parameters used and more robust to\nnoise.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 08:05:58 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Guevara", "Jaime Garcia", "", "MIMESIS"], ["Peterlik", "Igor", "", "IHU Strasbourg"], ["Berger", "Marie-Odile", "", "MAGRIT"], ["Cotin", "St\u00e9phane", "", "MIMESIS"]]}, {"id": "1912.06354", "submitter": "Julian Tanke", "authors": "Julian Tanke, Oh-Hun Kwon, Patrick Stotko, Radu Alexandru Rosu,\n  Michael Weinmann, Hassan Errami, Sven Behnke, Maren Bennewitz, Reinhard\n  Klein, Andreas Weber, Angela Yao, Juergen Gall", "title": "Bonn Activity Maps: Dataset Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key prerequisite for accessing the huge potential of current machine\nlearning techniques is the availability of large databases that capture the\ncomplex relations of interest. Previous datasets are focused on either 3D scene\nrepresentations with semantic information, tracking of multiple persons and\nrecognition of their actions, or activity recognition of a single person in\ncaptured 3D environments. We present Bonn Activity Maps, a large-scale dataset\nfor human tracking, activity recognition and anticipation of multiple persons.\nOur dataset comprises four different scenes that have been recorded by\ntime-synchronized cameras each only capturing the scene partially, the\nreconstructed 3D models with semantic annotations, motion trajectories for\nindividual people including 3D human poses as well as human activity\nannotations. We utilize the annotations to generate activity likelihoods on the\n3D models called activity maps.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 08:09:57 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tanke", "Julian", ""], ["Kwon", "Oh-Hun", ""], ["Stotko", "Patrick", ""], ["Rosu", "Radu Alexandru", ""], ["Weinmann", "Michael", ""], ["Errami", "Hassan", ""], ["Behnke", "Sven", ""], ["Bennewitz", "Maren", ""], ["Klein", "Reinhard", ""], ["Weber", "Andreas", ""], ["Yao", "Angela", ""], ["Gall", "Juergen", ""]]}, {"id": "1912.06365", "submitter": "Zhengcong Fei", "authors": "Zheng-cong Fei", "title": "Fast Image Caption Generation with Position Alignment", "comments": "AAAI2020 oral presentation on workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural network models for image captioning usually employ an\nencoder-decoder architecture, where the decoder adopts a recursive sequence\ndecoding way. However, such autoregressive decoding may result in sequential\nerror accumulation and slow generation which limit the applications in\npractice. Non-autoregressive (NA) decoding has been proposed to cover these\nissues but suffers from language quality problem due to the indirect modeling\nof the target distribution. Towards that end, we propose an improved NA\nprediction framework to accelerate image captioning. Our decoding part consists\nof a position alignment to order the words that describe the content detected\nin the given image, and a fine non-autoregressive decoder to generate elegant\ndescriptions. Furthermore, we introduce an inference strategy that regards\nposition information as a latent variable to guide the further sentence\ngeneration. The Experimental results on public datasets show that our proposed\nmodel achieves better performance compared to general NA captioning models,\nwhile achieves comparable performance as autoregressive image captioning models\nwith a significant speedup.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 09:06:46 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Fei", "Zheng-cong", ""]]}, {"id": "1912.06378", "submitter": "Zuozhuo Dai", "authors": "Xiaodong Gu, Zhiwen Fan, Zuozhuo Dai, Siyu Zhu, Feitong Tan, Ping Tan", "title": "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo\n  Matching", "comments": "Accepted by CVPR2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep multi-view stereo (MVS) and stereo matching approaches generally\nconstruct 3D cost volumes to regularize and regress the output depth or\ndisparity. These methods are limited when high-resolution outputs are needed\nsince the memory and time costs grow cubically as the volume resolution\nincreases. In this paper, we propose a both memory and time efficient cost\nvolume formulation that is complementary to existing multi-view stereo and\nstereo matching approaches based on 3D cost volumes. First, the proposed cost\nvolume is built upon a standard feature pyramid encoding geometry and context\nat gradually finer scales. Then, we can narrow the depth (or disparity) range\nof each stage by the depth (or disparity) map from the previous stage. With\ngradually higher cost volume resolution and adaptive adjustment of depth (or\ndisparity) intervals, the output is recovered in a coarser to fine manner.\n  We apply the cascade cost volume to the representative MVS-Net, and obtain a\n23.1% improvement on DTU benchmark (1st place), with 50.6% and 74.2% reduction\nin GPU memory and run-time. It is also the state-of-the-art learning-based\nmethod on Tanks and Temples benchmark. The statistics of accuracy, run-time and\nGPU memory on other representative stereo CNNs also validate the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 09:57:07 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 12:42:18 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 02:18:55 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Gu", "Xiaodong", ""], ["Fan", "Zhiwen", ""], ["Dai", "Zuozhuo", ""], ["Zhu", "Siyu", ""], ["Tan", "Feitong", ""], ["Tan", "Ping", ""]]}, {"id": "1912.06395", "submitter": "Wang Yifan", "authors": "Wang Yifan, Noam Aigerman, Vladimir G. Kim, Siddhartha Chaudhuri, Olga\n  Sorkine-Hornung", "title": "Neural Cages for Detail-Preserving 3D Deformations", "comments": "accepted for oral presentation at CVPR 2020, code available at\n  https://github.com/yifita/deep_cage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learnable representation for detail-preserving shape\ndeformation. The goal of our method is to warp a source shape to match the\ngeneral structure of a target shape, while preserving the surface details of\nthe source. Our method extends a traditional cage-based deformation technique,\nwhere the source shape is enclosed by a coarse control mesh termed \\emph{cage},\nand translations prescribed on the cage vertices are interpolated to any point\non the source mesh via special weight functions. The use of this sparse cage\nscaffolding enables preserving surface details regardless of the shape's\nintricacy and topology. Our key contribution is a novel neural network\narchitecture for predicting deformations by controlling the cage. We\nincorporate a differentiable cage-based deformation module in our architecture,\nand train our network end-to-end. Our method can be trained with common\ncollections of 3D models in an unsupervised fashion, without any cage-specific\nannotations. We demonstrate the utility of our method for synthesizing shape\nvariations and deformation transfer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:25:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:33:27 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Yifan", "Wang", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1912.06404", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg, Arjan Kuijper", "title": "Real-time texturing for 6D object instance detection from RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For objected detection, the availability of color cues strongly influences\ndetection rates and is even a prerequisite for many methods. However, when\ntraining on synthetic CAD data, this information is not available. We therefore\npresent a method for generating a texture-map from image sequences in\nreal-time. The method relies on 6 degree-of-freedom poses and a 3D-model being\navailable. In contrast to previous works this allows interleaving detection and\ntexturing for upgrading the detector on-the-fly. Our evaluation shows that the\nacquired texture-map significantly improves detection rates using the LINEMOD\ndetector on RGB images only. Additionally, we use the texture-map to\ndifferentiate instances of the same object by surface color.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:54:25 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Rojtberg", "Pavel", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1912.06418", "submitter": "Xin Sun", "authors": "Hongwei Xv, Xin Sun, Junyu Dong, Shu Zhang, Qiong Li", "title": "Multi-level Similarity Learning for Low-Shot Recognition", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-shot learning indicates the ability to recognize unseen objects based on\nvery limited labeled training samples, which simulates human visual\nintelligence. According to this concept, we propose a multi-level similarity\nmodel (MLSM) to capture the deep encoded distance metric between the support\nand query samples. Our approach is achieved based on the fact that the image\nsimilarity learning can be decomposed into image-level, global-level, and\nobject-level. Once the similarity function is established, MLSM will be able to\nclassify images for unseen classes by computing the similarity scores between a\nlimited number of labeled samples and the target images. Furthermore, we\nconduct 5-way experiments with both 1-shot and 5-shot setting on Caltech-UCSD\ndatasets. It is demonstrated that the proposed model can achieve promising\nresults compared with the existing methods in practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:31:21 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Xv", "Hongwei", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Zhang", "Shu", ""], ["Li", "Qiong", ""]]}, {"id": "1912.06430", "submitter": "Antoine Miech", "authors": "Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef\n  Sivic, Andrew Zisserman", "title": "End-to-End Learning of Visual Representations from Uncurated\n  Instructional Videos", "comments": "CVPR'2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:59:58 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 15:03:21 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 14:50:16 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 12:20:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Miech", "Antoine", ""], ["Alayrac", "Jean-Baptiste", ""], ["Smaira", "Lucas", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1912.06433", "submitter": "Alan Dolhasz", "authors": "Alan Dolhasz, Carlo Harvey, Ian Williams", "title": "Learning to Observe: Approximating Human Perceptual Thresholds for\n  Detection of Suprathreshold Image Transformations", "comments": "8 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision are often calibrated and evaluated relative to\nhuman perception. In this paper, we propose to directly approximate the\nperceptual function performed by human observers completing a visual detection\ntask. Specifically, we present a novel methodology for learning to detect image\ntransformations visible to human observers through approximating perceptual\nthresholds. To do this, we carry out a subjective two-alternative forced-choice\nstudy to estimate perceptual thresholds of human observers detecting local\nexposure shifts in images. We then leverage transformation equivariant\nrepresentation learning to overcome issues of limited perceptual data. This\nrepresentation is then used to train a dense convolutional classifier capable\nof detecting local suprathreshold exposure shifts - a distortion common to\nimage composites. In this context, our model can approximate perceptual\nthresholds with an average error of 0.1148 exposure stops between empirical and\npredicted thresholds. It can also be trained to detect a range of different\nlocal transformations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:08:00 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 15:49:21 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 12:33:21 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Dolhasz", "Alan", ""], ["Harvey", "Carlo", ""], ["Williams", "Ian", ""]]}, {"id": "1912.06442", "submitter": "Delia Velasco-Montero", "authors": "Delia Velasco-Montero (1), Jorge Fern\\'andez-Berni (1), Ricardo\n  Carmona-Gal\\'an (1), \\'Angel Rodr\\'iguez-V\\'azquez (1) ((1) Instituto de\n  Microelectr\\'onica de Sevilla (Universidad de Sevilla-CSIC))", "title": "PreVIous: A Methodology for Prediction of Visual Inference Performance\n  on IoT Devices", "comments": "18 pages. 7 figures", "journal-ref": null, "doi": "10.1109/JIOT.2020.2981684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents PreVIous, a methodology to predict the performance of\nconvolutional neural networks (CNNs) in terms of throughput and energy\nconsumption on vision-enabled devices for the Internet of Things. CNNs\ntypically constitute a massive computational load for such devices, which are\ncharacterized by scarce hardware resources to be shared among multiple\nconcurrent tasks. Therefore, it is critical to select the optimal CNN\narchitecture for a particular hardware platform according to prescribed\napplication requirements. However, the zoo of CNN models is already vast and\nrapidly growing. To facilitate a suitable selection, we introduce a prediction\nframework that allows to evaluate the performance of CNNs prior to their actual\nimplementation. The proposed methodology is based on PreVIousNet, a neural\nnetwork specifically designed to build accurate per-layer performance\npredictive models. PreVIousNet incorporates the most usual parameters found in\nstate-of-the-art network architectures. The resulting predictive models for\ninference time and energy have been tested against comprehensive\ncharacterizations of seven well-known CNN models running on two different\nsoftware frameworks and two different embedded platforms. To the best of our\nknowledge, this is the most extensive study in the literature concerning CNN\nperformance prediction on low-power low-cost devices. The average deviation\nbetween predictions and real measurements is remarkably low, ranging from 3% to\n10%. This means state-of-the-art modeling accuracy. As an additional asset, the\nfine-grained a priori analysis provided by PreVIous could also be exploited by\nneural architecture search engines.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:41:40 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 08:08:46 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Velasco-Montero", "Delia", ""], ["Fern\u00e1ndez-Berni", "Jorge", ""], ["Carmona-Gal\u00e1n", "Ricardo", ""], ["Rodr\u00edguez-V\u00e1zquez", "\u00c1ngel", ""]]}, {"id": "1912.06445", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann", "title": "The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction", "comments": "CVPR 2020. Code, models and dataset are available at:\n  https://next.cs.cmu.edu/multiverse/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of predicting the distribution over multiple\npossible future paths of people as they move through various visual scenes. We\nmake two main contributions. The first contribution is a new dataset, created\nin a realistic 3D simulator, which is based on real world trajectory data, and\nthen extrapolated by human annotators to achieve different latent goals. This\nprovides the first benchmark for quantitative evaluation of the models to\npredict multi-future trajectories. The second contribution is a new model to\ngenerate multiple plausible future trajectories, which contains novel designs\nof using multi-scale location encodings and convolutional RNNs over graphs. We\nrefer to our model as Multiverse. We show that our model achieves the best\nresults on our dataset, as well as on the real-world VIRAT/ActEV dataset (which\njust contains one possible future).\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:52:50 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 02:24:48 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 14:44:12 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Murphy", "Kevin", ""], ["Yu", "Ting", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1912.06446", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Zemin Tang, Zheng Zhang, Yang Wang, Jie Qin, Meng Wang", "title": "Fully-Convolutional Intensive Feature Flow Neural Network for Text\n  Recognition", "comments": "Accepted by the 24th European Conference on Artificial Intelligence\n  (ECAI 2020). arXiv admin note: text overlap with arXiv:1912.07016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep Convolutional Neural Networks (CNNs) have obtained a great success\nfor pattern recognition, such as recognizing the texts in images. But existing\nCNNs based frameworks still have several drawbacks: 1) the traditaional pooling\noperation may lose important feature information and is unlearnable; 2) the\ntradi-tional convolution operation optimizes slowly and the hierar-chical\nfeatures from different layers are not fully utilized. In this work, we address\nthese problems by developing a novel deep network model called\nFully-Convolutional Intensive Feature Flow Neural Network (IntensiveNet).\nSpecifically, we design a further dense block called intensive block to extract\nthe feature information, where the original inputs and two dense blocks are\nconnected tightly. To encode data appropriately, we present the concepts of\ndense fusion block and further dense fusion opera-tions for our new intensive\nblock. By adding short connections to different layers, the feature flow and\ncoupling between layers are enhanced. We also replace the traditional\nconvolution by depthwise separable convolution to make the operation efficient.\nTo prevent important feature information being lost to a certain extent, we use\na convolution operation with stride 2 to replace the original pooling operation\nin the customary transition layers. The recognition results on large-scale\nChinese string and MNIST datasets show that our IntensiveNet can deliver\nenhanced recog-nition results, compared with other related deep models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:54:19 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 12:14:32 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Zhang", "Zhao", ""], ["Tang", "Zemin", ""], ["Zhang", "Zheng", ""], ["Wang", "Yang", ""], ["Qin", "Jie", ""], ["Wang", "Meng", ""]]}, {"id": "1912.06448", "submitter": "Hisham Cholakkal", "authors": "Hisham Cholakkal, Guolei Sun, Salman Khan, Fahad Shahbaz Khan, Ling\n  Shao and Luc Van Gool", "title": "Towards Partial Supervision for Generic Object Counting in Natural\n  Scenes", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 2020. First two authors contributed equally.\n  Corresponding author: Guolei Sun. This work is a journal extension of our\n  CVPR 2019 paper arXiv:1903.02494", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic object counting in natural scenes is a challenging computer vision\nproblem. Existing approaches either rely on instance-level supervision or\nabsolute count information to train a generic object counter. We introduce a\npartially supervised setting that significantly reduces the supervision level\nrequired for generic object counting. We propose two novel frameworks, named\nlower-count (LC) and reduced lower-count (RLC), to enable object counting under\nthis setting. Our frameworks are built on a novel dual-branch architecture that\nhas an image classification and a density branch. Our LC framework reduces the\nannotation cost due to multiple instances in an image by using only lower-count\nsupervision for all object categories. Our RLC framework further reduces the\nannotation cost arising from large numbers of object categories in a dataset by\nonly using lower-count supervision for a subset of categories and class-labels\nfor the remaining ones. The RLC framework extends our dual-branch LC framework\nwith a novel weight modulation layer and a category-independent density map\nprediction. Experiments are performed on COCO, Visual Genome and PASCAL 2007\ndatasets. Our frameworks perform on par with state-of-the-art approaches using\nhigher levels of supervision. Additionally, we demonstrate the applicability of\nour LC supervised density map for image-level supervised instance segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:57:04 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:07:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Cholakkal", "Hisham", ""], ["Sun", "Guolei", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["Van Gool", "Luc", ""]]}, {"id": "1912.06449", "submitter": "Doreen Jirak", "authors": "Doreen Jirak, David Biertimpel, Matthias Kerzel, Stefan Wermter", "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Whenever we are addressing a specific object or refer to a certain spatial\nlocation, we are using referential or deictic gestures usually accompanied by\nsome verbal description. Especially pointing gestures are necessary to dissolve\nambiguities in a scene and they are of crucial importance when verbal\ncommunication may fail due to environmental conditions or when two persons\nsimply do not speak the same language. With the currently increasing advances\nof humanoid robots and their future integration in domestic domains, the\ndevelopment of gesture interfaces complementing human-robot interaction\nscenarios is of substantial interest. The implementation of an intuitive\ngesture scenario is still challenging because both the pointing intention and\nthe corresponding object have to be correctly recognized in real-time. The\ndemand increases when considering pointing gestures in a cluttered environment,\nas is the case in households. Also, humans perform pointing in many different\nways and those variations have to be captured. Research in this field often\nproposes a set of geometrical computations which do not scale well with the\nnumber of gestures and objects, use specific markers or a predefined set of\npointing directions. In this paper, we propose an unsupervised learning\napproach to model the distribution of pointing gestures using a\ngrowing-when-required (GWR) network. We introduce an interaction scenario with\na humanoid robot and define so-called ambiguity classes. Our implementation for\nthe hand and object detection is independent of any markers or skeleton models,\nthus it can be easily reproduced. Our evaluation comparing a baseline computer\nvision approach with our GWR model shows that the pointing-object association\nis well learned even in cases of ambiguities resulting from close object\nproximity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:57:33 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Jirak", "Doreen", ""], ["Biertimpel", "David", ""], ["Kerzel", "Matthias", ""], ["Wermter", "Stefan", ""]]}, {"id": "1912.06450", "submitter": "Zhao Zhang", "authors": "Xianzhen Li, Zhao Zhang, Yang Wang, Guangcan Liu, Shuicheng Yan, Meng\n  Wang", "title": "Multilayer Collaborative Low-Rank Coding Network for Robust Deep\n  Subspace Discovery", "comments": "Accepted by the 24th European Conference on Artificial Intelligence\n  (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For subspace recovery, most existing low-rank representation (LRR) models\nperforms in the original space in single-layer mode. As such, the deep\nhierarchical information cannot be learned, which may result in inaccurate\nrecoveries for complex real data. In this paper, we explore the deep\nmulti-subspace recovery problem by designing a multilayer architecture for\nlatent LRR. Technically, we propose a new Multilayer Collabora-tive Low-Rank\nRepresentation Network model termed DeepLRR to discover deep features and deep\nsubspaces. In each layer (>2), DeepLRR bilinearly reconstructs the data matrix\nby the collabo-rative representation with low-rank coefficients and projection\nmatrices in the previous layer. The bilinear low-rank reconstruc-tion of\nprevious layer is directly fed into the next layer as the input and low-rank\ndictionary for representation learning, and is further decomposed into a deep\nprincipal feature part, a deep salient feature part and a deep sparse error. As\nsuch, the coher-ence issue can be also resolved due to the low-rank dictionary,\nand the robustness against noise can also be enhanced in the feature subspace.\nTo recover the sparse errors in layers accurately, a dynamic growing strategy\nis used, as the noise level will be-come smaller for the increase of layers.\nBesides, a neighborhood reconstruction error is also included to encode the\nlocality of deep salient features by deep coefficients adaptively in each\nlayer. Extensive results on public databases show that our DeepLRR outperforms\nother related models for subspace discovery and clustering.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:57:57 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 09:49:02 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 12:12:44 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Li", "Xianzhen", ""], ["Zhang", "Zhao", ""], ["Wang", "Yang", ""], ["Liu", "Guangcan", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.06464", "submitter": "Daniel Barath", "authors": "Levente Hajder, Daniel Barath", "title": "Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new closed-form solver is proposed minimizing the algebraic error\noptimally, in the least-squares sense, to estimate the relative planar motion\nof two calibrated cameras. The main objective is to solve the over-determined\ncase, i.e., when a larger-than-minimal sample of point correspondences is given\n- thus, estimating the motion from at least three correspondences. The\nalgorithm requires the camera movement to be constrained to a plane, e.g.\nmounted to a vehicle, and the image plane to be orthogonal to the ground. The\nsolver obtains the motion parameters as the roots of a 6-th degree polynomial.\nIt is validated both in synthetic experiments and on publicly available\nreal-world datasets that using the proposed solver leads to results superior to\nthe state-of-the-art in terms of geometric accuracy with no noticeable\ndeterioration in the processing time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 13:26:27 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Hajder", "Levente", ""], ["Barath", "Daniel", ""]]}, {"id": "1912.06465", "submitter": "Daniel Barath", "authors": "Levente Hajder and Daniel Barath", "title": "Relative planar motion for vehicle-mounted cameras from a single affine\n  correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two solvers are proposed for estimating the extrinsic camera parameters from\na single affine correspondence assuming general planar motion. In this case,\nthe camera movement is constrained to a plane and the image plane is orthogonal\nto the ground. The algorithms do not assume other constraints, e.g.\\ the\nnon-holonomic one, to hold. A new minimal solver is proposed for the\nsemi-calibrated case, i.e. the camera parameters are known except a common\nfocal length. Another method is proposed for the fully calibrated case. Due to\nrequiring a single correspondence, robust estimation, e.g. histogram voting,\nleads to a fast and accurate procedure. The proposed methods are tested in our\nsynthetic environment and on publicly available real datasets consisting of\nvideos through tens of kilometres. They are superior to the state-of-the-art\nboth in terms of accuracy and processing time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 13:30:10 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Hajder", "Levente", ""], ["Barath", "Daniel", ""]]}, {"id": "1912.06466", "submitter": "Savva Ignatyev", "authors": "Vage Egiazarian, Savva Ignatyev, Alexey Artemov, Oleg Voynov, Andrey\n  Kravchenko, Youyi Zheng, Luiz Velho, Evgeny Burnaev", "title": "Latent-Space Laplacian Pyramids for Adversarial Representation Learning\n  with 3D Point Clouds", "comments": null, "journal-ref": null, "doi": "10.5220/0009102604210428", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing high-quality generative models for 3D shapes is a fundamental\ntask in computer vision with diverse applications in geometry processing,\nengineering, and design. Despite the recent progress in deep generative\nmodelling, synthesis of finely detailed 3D surfaces, such as high-resolution\npoint clouds, from scratch has not been achieved with existing approaches. In\nthis work, we propose to employ the latent-space Laplacian pyramid\nrepresentation within a hierarchical generative model for 3D point clouds. We\ncombine the recently proposed latent-space GAN and Laplacian GAN architectures\nto form a multi-scale model capable of generating 3D point clouds at increasing\nlevels of detail. Our evaluation demonstrates that our model outperforms the\nexisting generative models for 3D point clouds.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 13:32:28 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Egiazarian", "Vage", ""], ["Ignatyev", "Savva", ""], ["Artemov", "Alexey", ""], ["Voynov", "Oleg", ""], ["Kravchenko", "Andrey", ""], ["Zheng", "Youyi", ""], ["Velho", "Luiz", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1912.06501", "submitter": "Lu Sang", "authors": "Lu Sang and Bjoern Haefner and Daniel Cremers", "title": "Inferring Super-Resolution Depth from a Moving Light-Source Enhanced\n  RGB-D Sensor: A Variational Approach", "comments": "WACV2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach towards depth map super-resolution using multi-view\nuncalibrated photometric stereo is presented. Practically, an LED light source\nis attached to a commodity RGB-D sensor and is used to capture objects from\nmultiple viewpoints with unknown motion. This non-static camera-to-object setup\nis described with a nonconvex variational approach such that no calibration on\nlighting or camera motion is required due to the formulation of an end-to-end\njoint optimization problem. Solving the proposed variational model results in\nhigh resolution depth, reflectance and camera pose estimates, as we show on\nchallenging synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 14:07:30 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Sang", "Lu", ""], ["Haefner", "Bjoern", ""], ["Cremers", "Daniel", ""]]}, {"id": "1912.06556", "submitter": "Xingxing Zhang", "authors": "Zhenfeng Zhu, Yingying Meng, Deqiang Kong, Xingxing Zhang, Yandong\n  Guo, and Yao Zhao", "title": "To See in the Dark: N2DGAN for Background Modeling in Nighttime Scene", "comments": "11 pages, 11 figures. The paper has been accepted by IEEE\n  Transactions on Circuits and Systems for Video Technology(TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the deteriorated conditions of \\mbox{illumination} lack and uneven\nlighting, nighttime images have lower contrast and higher noise than their\ndaytime counterparts of the same scene, which limits seriously the performances\nof conventional background modeling methods. For such a challenging problem of\nbackground modeling under nighttime scene, an innovative and reasonable\nsolution is proposed in this paper, which paves a new way completely different\nfrom the existing ones. To make background modeling under nighttime scene\nperforms as well as in daytime condition, we put forward a promising\ngeneration-based background modeling framework for foreground surveillance.\nWith a pre-specified daytime reference image as background frame, the\n{\\bfseries GAN} based generation model, called {\\bfseries N2DGAN}, is trained\nto transfer each frame of {\\bfseries n}ighttime video {\\bfseries to} a virtual\n{\\bfseries d}aytime image with the same scene to the reference image except for\nthe foreground region. Specifically, to balance the preservation of background\nscene and the foreground object(s) in generating the virtual daytime image, we\npresent a two-pathway generation model, in which the global and local\nsub-networks are well combined with spatial and temporal consistency\nconstraints. For the sequence of generated virtual daytime images, a\nmulti-scale Bayes model is further proposed to characterize pertinently the\ntemporal variation of background. We evaluate on collected datasets with\nmanually labeled ground truth, which provides a valuable resource for related\nresearch community. The impressive results illustrated in both the main paper\nand supplementary show efficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 04:41:38 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 06:53:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhu", "Zhenfeng", ""], ["Meng", "Yingying", ""], ["Kong", "Deqiang", ""], ["Zhang", "Xingxing", ""], ["Guo", "Yandong", ""], ["Zhao", "Yao", ""]]}, {"id": "1912.06602", "submitter": "Rahul Shome", "authors": "Malihe Alikhani, Baber Khalid, Rahul Shome, Chaitanya Mitash, Kostas\n  Bekris, Matthew Stone", "title": "That and There: Judging the Intent of Pointing Actions with Robotic Arms", "comments": "Accepted to AAAI 2020, New York City", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative robotics requires effective communication between a robot and a\nhuman partner. This work proposes a set of interpretive principles for how a\nrobotic arm can use pointing actions to communicate task information to people\nby extending existing models from the related literature. These principles are\nevaluated through studies where English-speaking human subjects view animations\nof simulated robots instructing pick-and-place tasks. The evaluation\ndistinguishes two classes of pointing actions that arise in pick-and-place\ntasks: referential pointing (identifying objects) and locating pointing\n(identifying locations). The study indicates that human subjects show greater\nflexibility in interpreting the intent of referential pointing compared to\nlocating pointing, which needs to be more deliberate. The results also\ndemonstrate the effects of variation in the environment and task context on the\ninterpretation of pointing. Our corpus, experiments and design principles\nadvance models of context, common sense reasoning and communication in embodied\ncommunication.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 16:54:38 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Alikhani", "Malihe", ""], ["Khalid", "Baber", ""], ["Shome", "Rahul", ""], ["Mitash", "Chaitanya", ""], ["Bekris", "Kostas", ""], ["Stone", "Matthew", ""]]}, {"id": "1912.06606", "submitter": "Qifeng Chen", "authors": "Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen", "title": "Music-oriented Dance Video Synthesis with Pose Perceptual Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach with pose perceptual loss for automatic\nmusic video generation. Our method can produce a realistic dance video that\nconforms to the beats and rhymes of almost any given music. To achieve this, we\nfirstly generate a human skeleton sequence from music and then apply the\nlearned pose-to-appearance mapping to generate the final video. In the stage of\ngenerating skeleton sequences, we utilize two discriminators to capture\ndifferent aspects of the sequence and propose a novel pose perceptual loss to\nproduce natural dances. Besides, we also provide a new cross-modal evaluation\nto evaluate the dance quality, which is able to estimate the similarity between\ntwo modalities of music and dance. Finally, a user study is conducted to\ndemonstrate that dance video synthesized by the presented approach produces\nsurprisingly realistic results. The results are shown in the supplementary\nvideo at https://youtu.be/0rMuFMZa_K4\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:01:21 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ren", "Xuanchi", ""], ["Li", "Haoran", ""], ["Huang", "Zijian", ""], ["Chen", "Qifeng", ""]]}, {"id": "1912.06613", "submitter": "Nicolas Scheiner", "authors": "Nicolas Scheiner, Florian Kraus, Fangyin Wei, Buu Phan, Fahim Mannan,\n  Nils Appenrodt, Werner Ritter, J\\\"urgen Dickmann, Klaus Dietmayer, Bernhard\n  Sick, Felix Heide", "title": "Seeing Around Street Corners: Non-Line-of-Sight Detection and Tracking\n  In-the-Wild Using Doppler Radar", "comments": "First three authors contributed equally; Accepted at CVPR 2020", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2020, pp. 2068-2077", "doi": "10.1109/CVPR42600.2020.00214", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sensor systems record information about directly visible\nobjects, whereas occluded scene components are considered lost in the\nmeasurement process. Non-line-of-sight (NLOS) methods try to recover such\nhidden objects from their indirect reflections - faint signal components,\ntraditionally treated as measurement noise. Existing NLOS approaches struggle\nto record these low-signal components outside the lab, and do not scale to\nlarge-scale outdoor scenes and high-speed motion, typical in automotive\nscenarios. In particular, optical NLOS capture is fundamentally limited by the\nquartic intensity falloff of diffuse indirect reflections. In this work, we\ndepart from visible-wavelength approaches and demonstrate detection,\nclassification, and tracking of hidden objects in large-scale dynamic\nenvironments using Doppler radars that can be manufactured at low-cost in\nseries production. To untangle noisy indirect and direct reflections, we learn\nfrom temporal sequences of Doppler velocity and position measurements, which we\nfuse in a joint NLOS detection and tracking network over time. We validate the\napproach on in-the-wild automotive scenes, including sequences of parked cars\nor house facades as relay surfaces, and demonstrate low-cost, real-time NLOS in\ndynamic automotive environments.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:23:29 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 09:25:11 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Scheiner", "Nicolas", ""], ["Kraus", "Florian", ""], ["Wei", "Fangyin", ""], ["Phan", "Buu", ""], ["Mannan", "Fahim", ""], ["Appenrodt", "Nils", ""], ["Ritter", "Werner", ""], ["Dickmann", "J\u00fcrgen", ""], ["Dietmayer", "Klaus", ""], ["Sick", "Bernhard", ""], ["Heide", "Felix", ""]]}, {"id": "1912.06617", "submitter": "Hazel Doughty", "authors": "Hazel Doughty, Ivan Laptev, Walterio Mayol-Cuevas and Dima Damen", "title": "Action Modifiers: Learning from Adverbs in Instructional Videos", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to learn a representation for adverbs from instructional\nvideos using weak supervision from the accompanying narrations. Key to our\nmethod is the fact that the visual representation of the adverb is highly\ndependant on the action to which it applies, although the same adverb will\nmodify multiple actions in a similar way. For instance, while 'spread quickly'\nand 'mix quickly' will look dissimilar, we can learn a common representation\nthat allows us to recognize both, among other actions. We formulate this as an\nembedding problem, and use scaled dot-product attention to learn from\nweakly-supervised video narrations. We jointly learn adverbs as invertible\ntransformations operating on the embedding space, so as to add or remove the\neffect of the adverb. As there is no prior work on weakly supervised learning\nfrom adverbs, we gather paired action-adverb annotations from a subset of the\nHowTo100M dataset for 6 adverbs: quickly/slowly, finely/coarsely, and\npartially/completely. Our method outperforms all baselines for video-to-adverb\nretrieval with a performance of 0.719 mAP. We also demonstrate our model's\nability to attend to the relevant video parts in order to determine the adverb\nfor a given action.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:27:25 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 15:00:30 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 16:40:21 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Doughty", "Hazel", ""], ["Laptev", "Ivan", ""], ["Mayol-Cuevas", "Walterio", ""], ["Damen", "Dima", ""]]}, {"id": "1912.06640", "submitter": "Steven Schwarcz", "authors": "Steven Schwarcz, Peng Xu, David D'Ambrosio, Juhana Kangaspunta, Anelia\n  Angelova, Huong Phan, Navdeep Jaitly", "title": "SPIN: A High Speed, High Resolution Vision Dataset for Tracking and\n  Action Recognition in Ping Pong", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new high resolution, high frame rate stereo video dataset,\nwhich we call SPIN, for tracking and action recognition in the game of ping\npong. The corpus consists of ping pong play with three main annotation streams\nthat can be used to learn tracking and action recognition models -- tracking of\nthe ping pong ball and poses of humans in the videos and the spin of the ball\nbeing hit by humans. The training corpus consists of 53 hours of data with\nlabels derived from previous models in a semi-supervised method. The testing\ncorpus contains 1 hour of data with the same information, except that crowd\ncompute was used to obtain human annotations of the ball position, from which\nball spin has been derived. Along with the dataset we introduce several\nbaseline models that were trained on this data. The models were specifically\nchosen to be able to perform inference at the same rate as the images are\ngenerated -- specifically 150 fps. We explore the advantages of multi-task\ntraining on this data, and also show interesting properties of ping pong ball\ntrajectories that are derived from our observational data, rather than from\nprior physics models. To our knowledge this is the first large scale dataset of\nping pong; we offer it to the community as a rich dataset that can be used for\na large variety of machine learning and vision tasks such as tracking, pose\nestimation, semi-supervised and unsupervised learning and generative modeling.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 18:30:29 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Schwarcz", "Steven", ""], ["Xu", "Peng", ""], ["D'Ambrosio", "David", ""], ["Kangaspunta", "Juhana", ""], ["Angelova", "Anelia", ""], ["Phan", "Huong", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1912.06649", "submitter": "Wanxin Sun", "authors": "Yao Xiang, Wanxin Sun, Changli Pan, Meng Yan, Zhihua Yin, Yixiong\n  Liang", "title": "A Novel Automation-Assisted Cervical Cancer Reading Method Based on\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most previous automation-assisted reading methods can improve\nefficiency, their performance often relies on the success of accurate cell\nsegmentation and hand-craft feature extraction. This paper presents an\nefficient and totally segmentation-free method for automated cervical cell\nscreening that utilizes modern object detector to directly detect cervical\ncells or clumps, without the design of specific hand-crafted feature.\nSpecifically, we use the state-of-the-art CNN-based object detection methods,\nYOLOv3, as our baseline model. In order to improve the classification\nperformance of hard examples which are four highly similar categories, we\ncascade an additional task-specific classifier. We also investigate the\npresence of unreliable annotations and cope with them by smoothing the\ndistribution of noisy labels. We comprehensively evaluate our methods on test\nset which is consisted of 1,014 annotated cervical cell images with size of\n4000*3000 and complex cellular situation corresponding to 10 categories. Our\nmodel achieves 97.5% sensitivity (Sens) and 67.8% specificity (Spec) on\ncervical cell image-level screening. Moreover, we obtain a mean Average\nPrecision (mAP) of 63.4% on cervical cell-level diagnosis, and improve the\nAverage Precision (AP) of hard examples which are valuable but difficult to\ndistinguish. Our automation-assisted cervical cell reading method not only\nachieves cervical cell image-level classification but also provides more\ndetailed location and category information of abnormal cells. The results\nindicate feasible performance of our method, together with the efficiency and\nrobustness, providing a new idea for future development of computer-assisted\nreading system in clinical cervical screening.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 07:26:42 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Xiang", "Yao", ""], ["Sun", "Wanxin", ""], ["Pan", "Changli", ""], ["Yan", "Meng", ""], ["Yin", "Zhihua", ""], ["Liang", "Yixiong", ""]]}, {"id": "1912.06679", "submitter": "Mathieu Pag\\'e Fortin", "authors": "Mathieu Pag\\'e Fortin and Brahim Chaib-draa", "title": "Towards Contextual Learning in Few-shot Object Classification", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot Learning (FSL) aims to classify new concepts from a small number of\nexamples. While there have been an increasing amount of work on few-shot object\nclassification in the last few years, most current approaches are limited to\nimages with only one centered object. On the opposite, humans are able to\nleverage prior knowledge to quickly learn new concepts, such as semantic\nrelations with contextual elements. Inspired by the concept of contextual\nlearning in educational sciences, we propose to make a step towards adopting\nthis principle in FSL by studying the contribution that context can have in\nobject classification in a low-data regime. To this end, we first propose an\napproach to perform FSL on images of complex scenes. We develop two\nplug-and-play modules that can be incorporated into existing FSL methods to\nenable them to leverage contextual learning. More specifically, these modules\nare trained to weight the most important context elements while learning a\nparticular concept, and then use this knowledge to ground visual class\nrepresentations in context semantics. Extensive experiments on Visual Genome\nand Open Images show the superiority of contextual learning over learning\nindividual objects in isolation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:56:39 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 04:29:36 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 18:11:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fortin", "Mathieu Pag\u00e9", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "1912.06683", "submitter": "Taha Emara", "authors": "Taha Emara, Hossam E. Abd El Munim, Hazem M. Abbas", "title": "LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation", "comments": "Accepted, DICTA 2019", "journal-ref": null, "doi": "10.1109/DICTA47822.2019.8945975", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:05:52 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Emara", "Taha", ""], ["Munim", "Hossam E. Abd El", ""], ["Abbas", "Hazem M.", ""]]}, {"id": "1912.06686", "submitter": "Claas Flint", "authors": "Claas Flint, Micah Cearns, Nils Opel, Ronny Redlich, David M. A.\n  Mehler, Daniel Emden, Nils R. Winter, Ramona Leenings, Simon B. Eickhoff,\n  Tilo Kircher, Axel Krug, Igor Nenadic, Volker Arolt, Scott Clark, Bernhard T.\n  Baune, Xiaoyi Jiang, Udo Dannlowski, Tim Hahn", "title": "Systematic Misestimation of Machine Learning Performance in Neuroimaging\n  Studies of Depression", "comments": null, "journal-ref": "Neuropsychopharmacology 46 (2021) 1510-1517", "doi": "10.1038/s41386-021-01020-7", "report-no": null, "categories": "q-bio.NC cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We currently observe a disconcerting phenomenon in machine learning studies\nin psychiatry: While we would expect larger samples to yield better results due\nto the availability of more data, larger machine learning studies consistently\nshow much weaker performance than the numerous small-scale studies. Here, we\nsystematically investigated this effect focusing on one of the most heavily\nstudied questions in the field, namely the classification of patients suffering\nfrom major depressive disorder (MDD) and healthy control (HC) based on\nneuroimaging data. Drawing upon structural magnetic resonance imaging (MRI)\ndata from a balanced sample of $N = 1,868$ MDD patients and HC from our recent\ninternational Predictive Analytics Competition (PAC), we first trained and\ntested a classification model on the full dataset which yielded an accuracy of\n$61\\,\\%$. Next, we mimicked the process by which researchers would draw samples\nof various sizes ($N = 4$ to $N = 150$) from the population and showed a strong\nrisk of misestimation. Specifically, for small sample sizes ($N = 20$), we\nobserve accuracies of up to $95\\,\\%$. For medium sample sizes ($N = 100$)\naccuracies up to $75\\,\\%$ were found. Importantly, further investigation showed\nthat sufficiently large test sets effectively protect against performance\nmisestimation whereas larger datasets per se do not. While these results\nquestion the validity of a substantial part of the current literature, we\noutline the relatively low-cost remedy of larger test sets, which is readily\navailable in most cases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:12:52 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 15:10:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Flint", "Claas", ""], ["Cearns", "Micah", ""], ["Opel", "Nils", ""], ["Redlich", "Ronny", ""], ["Mehler", "David M. A.", ""], ["Emden", "Daniel", ""], ["Winter", "Nils R.", ""], ["Leenings", "Ramona", ""], ["Eickhoff", "Simon B.", ""], ["Kircher", "Tilo", ""], ["Krug", "Axel", ""], ["Nenadic", "Igor", ""], ["Arolt", "Volker", ""], ["Clark", "Scott", ""], ["Baune", "Bernhard T.", ""], ["Jiang", "Xiaoyi", ""], ["Dannlowski", "Udo", ""], ["Hahn", "Tim", ""]]}, {"id": "1912.06688", "submitter": "Joerg Zimmermann", "authors": "Kristina Enes, Hassan Errami, Moritz Wolter, Tim Krake, Bernhard\n  Eberhardt, Andreas Weber, J\\\"org Zimmermann", "title": "Unsupervised and Generic Short-Term Anticipation of Human Body Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various neural network based methods are capable of anticipating human body\nmotions from data for a short period of time. What these methods lack are the\ninterpretability and explainability of the network and its results. We propose\nto use Dynamic Mode Decomposition with delays to represent and anticipate human\nbody motions. Exploring the influence of the number of delays on the\nreconstruction and prediction of various motion classes, we show that the\nanticipation errors in our results are comparable or even better for very short\nanticipation times ($<0.4$ sec) to a recurrent neural network based method. We\nperceive our method as a first step towards the interpretability of the results\nby representing human body motions as linear combinations of ``factors''. In\naddition, compared to the neural network based methods large training times are\nnot needed. Actually, our methods do not even regress to any other motions than\nthe one to be anticipated and hence is of a generic nature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:13:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Enes", "Kristina", ""], ["Errami", "Hassan", ""], ["Wolter", "Moritz", ""], ["Krake", "Tim", ""], ["Eberhardt", "Bernhard", ""], ["Weber", "Andreas", ""], ["Zimmermann", "J\u00f6rg", ""]]}, {"id": "1912.06697", "submitter": "Wei-Lin Hsiao", "authors": "Wei-Lin Hsiao, Kristen Grauman", "title": "ViBE: Dressing for Diverse Body Shapes", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body shape plays an important role in determining what garments will best\nsuit a given person, yet today's clothing recommendation methods take a \"one\nshape fits all\" approach. These body-agnostic vision methods and datasets are a\nbarrier to inclusion, ill-equipped to provide good suggestions for diverse body\nshapes. We introduce ViBE, a VIsual Body-aware Embedding that captures\nclothing's affinity with different body shapes. Given an image of a person, the\nproposed embedding identifies garments that will flatter her specific body\nshape. We show how to learn the embedding from an online catalog displaying\nfashion models of various shapes and sizes wearing the products, and we devise\na method to explain the algorithm's suggestions for well-fitting garments. We\napply our approach to a dataset of diverse subjects, and demonstrate its strong\nadvantages over the status quo body-agnostic recommendation, both according to\nautomated metrics and human opinion.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:39:22 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 23:25:45 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hsiao", "Wei-Lin", ""], ["Grauman", "Kristen", ""]]}, {"id": "1912.06704", "submitter": "Gengshan Yang", "authors": "Gengshan Yang, Joshua Manela, Michael Happold and Deva Ramanan", "title": "Hierarchical Deep Stereo Matching on High-resolution Images", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of real-time stereo matching on high-res imagery. Many\nstate-of-the-art (SOTA) methods struggle to process high-res imagery because of\nmemory constraints or speed limitations. To address this issue, we propose an\nend-to-end framework that searches for correspondences incrementally over a\ncoarse-to-fine hierarchy. Because high-res stereo datasets are relatively rare,\nwe introduce a dataset with high-res stereo pairs for both training and\nevaluation. Our approach achieved SOTA performance on Middlebury-v3 and\nKITTI-15 while running significantly faster than its competitors. The\nhierarchical design also naturally allows for anytime on-demand reports of\ndisparity by capping intermediate coarse results, allowing us to accurately\npredict disparity for near-range structures with low latency (30ms). We\ndemonstrate that the performance-vs-speed trade-off afforded by on-demand\nhierarchies may address sensing needs for time-critical applications such as\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:57:15 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yang", "Gengshan", ""], ["Manela", "Joshua", ""], ["Happold", "Michael", ""], ["Ramanan", "Deva", ""]]}, {"id": "1912.06727", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, David B. Lindell, Gordon Wetzstein", "title": "Keyhole Imaging: Non-Line-of-Sight Imaging and Tracking of Moving\n  Objects Along a Single Optical Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-line-of-sight (NLOS) imaging and tracking is an emerging technology that\nallows the shape or position of objects around corners or behind diffusers to\nbe recovered from transient, time-of-flight measurements. However, existing\nNLOS approaches require the imaging system to scan a large area on a visible\nsurface, where the indirect light paths of hidden objects are sampled. In many\napplications, such as robotic vision or autonomous driving, optical access to a\nlarge scanning area may not be available, which severely limits the\npracticality of existing NLOS techniques. Here, we propose a new approach,\ndubbed keyhole imaging, that captures a sequence of transient measurements\nalong a single optical path, for example, through a keyhole. Assuming that the\nhidden object of interest moves during the acquisition time, we effectively\ncapture a series of time-resolved projections of the object's shape from\nunknown viewpoints. We derive inverse methods based on expectation-maximization\nto recover the object's shape and location using these measurements. Then, with\nthe help of long exposure times and retroreflective tape, we demonstrate\nsuccessful experimental results with a prototype keyhole imaging system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:05:18 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 21:29:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 18:22:14 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Lindell", "David B.", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1912.06729", "submitter": "Alejandro Murillo-Gonz\\'alez", "authors": "Alejandro Murillo-Gonz\\'alez, Jos\\'e David Ortega Pab\\'on, Juan\n  Guillermo Paniagua, Olga Luc\\'ia Quintero Montoya", "title": "Laguerre-Gauss Preprocessing: Line Profiles as Image Features for Aerial\n  Images Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image preprocessing methodology based on Fourier analysis together with\nthe Laguerre-Gauss Spatial Filter is proposed. This is an alternative to obtain\nfeatures from aerial images that reduces the feature space significantly,\npreserving enough information for classification tasks. Experiments on a\nchallenging data set of aerial images show that it is possible to learn a\nrobust classifier from this transformed and smaller feature space using simple\nmodels, with similar performance to the complete feature space and more complex\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:21:26 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Murillo-Gonz\u00e1lez", "Alejandro", ""], ["Pab\u00f3n", "Jos\u00e9 David Ortega", ""], ["Paniagua", "Juan Guillermo", ""], ["Montoya", "Olga Luc\u00eda Quintero", ""]]}, {"id": "1912.06765", "submitter": "Dhritimaan Das", "authors": "Dhritimaan Das, Ayush Agarwal, Pratik Chattopadhyay, Lipo Wang", "title": "RGait-NET: An Effective Network for Recovering Missing Information from\n  Occluded Gait Cycles", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gait of a person refers to his/her walking pattern, and according to medical\nstudies gait of every individual is unique. Over the past decade, several\ncomputer vision-based gait recognition approaches have been proposed in which\nwalking information corresponding to a complete gait cycle has been used to\nconstruct gait features for person identification. These methods compute gait\nfeatures with the inherent assumption that a complete gait cycle is always\navailable. However, in most public places occlusion is an inevitable\noccurrence, and due to this, only a fraction of a gait cycle gets captured by\nthe monitoring camera. Unavailability of complete gait cycle information\ndrastically affects the accuracy of the extracted features, and till date, only\na few occlusion handling strategies to gait recognition have been proposed. But\nnone of these performs reliably and robustly in the presence of a single cycle\nwith incomplete information, and because of this practical application of gait\nrecognition is quite limited. In this work, we develop deep learning-based\nalgorithm to accurately identify the affected frames as well as predict the\nmissing frames to reconstruct a complete gait cycle. While occlusion detection\nhas been carried out by employing a VGG-16 model, the model for frame\nreconstruction is based on Long-Short Term Memory network that has been trained\nto optimize a multi-objective function based on dice coefficient and\ncross-entropy loss. The effectiveness of the proposed occlusion reconstruction\nalgorithm is evaluated by computing the accuracy of the popular Gait Energy\nFeature on the reconstructed sequence. Experimental evaluation on public data\nsets and comparative analysis with other occlusion handling methods verify the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 01:53:10 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 16:54:18 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 13:51:17 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Das", "Dhritimaan", ""], ["Agarwal", "Ayush", ""], ["Chattopadhyay", "Pratik", ""], ["Wang", "Lipo", ""]]}, {"id": "1912.06785", "submitter": "Igor Gilitschenski", "authors": "Igor Gilitschenski, Guy Rosman, Arjun Gupta, Sertac Karaman, Daniela\n  Rus", "title": "Deep Context Maps: Agent Trajectory Prediction using Location-specific\n  Latent Maps", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2020.3004800", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for agent motion prediction in\ncluttered environments. One of the main challenges in predicting agent motion\nis accounting for location and context-specific information. Our main\ncontribution is the concept of learning context maps to improve the prediction\ntask. Context maps are a set of location-specific latent maps that are trained\nalongside the predictor. Thus, the proposed maps are capable of capturing\nlocation context beyond visual context cues (e.g. usual average speeds and\ntypical trajectories) or predefined map primitives (such as lanes and stop\nlines). We pose context map learning as a multi-task training problem and\ndescribe our map model and its incorporation into a state-of-the-art trajectory\npredictor. In extensive experiments, it is shown that use of learned maps can\nsignificantly improve predictor accuracy. Furthermore, the performance can be\nadditionally boosted by providing partial knowledge of map semantics.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 05:16:59 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 22:18:03 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Gilitschenski", "Igor", ""], ["Rosman", "Guy", ""], ["Gupta", "Arjun", ""], ["Karaman", "Sertac", ""], ["Rus", "Daniela", ""]]}, {"id": "1912.06794", "submitter": "Matt Zhang", "authors": "Dawit Belayneh, Federico Carminati, Amir Farbin, Benjamin Hooberman,\n  Gulrukh Khattak, Miaoyuan Liu, Junze Liu, Dominick Olivito, Vit\\'oria Barin\n  Pacela, Maurizio Pierini, Alexander Schwing, Maria Spiropulu, Sofia\n  Vallecorsa, Jean-Roch Vlimant, Wei Wei, Matt Zhang", "title": "Calorimetry with Deep Learning: Particle Simulation and Reconstruction\n  for Collider Physics", "comments": "26 pages, 38 figures. Corrected typos and added additional references\n  in v2. Extended Acknowledgements section in v3", "journal-ref": null, "doi": "10.1140/epjc/s10052-020-8251-9", "report-no": null, "categories": "physics.ins-det cs.CV cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using detailed simulations of calorimeter showers as training data, we\ninvestigate the use of deep learning algorithms for the simulation and\nreconstruction of particles produced in high-energy physics collisions. We\ntrain neural networks on shower data at the calorimeter-cell level, and show\nsignificant improvements for simulation and reconstruction when using these\nnetworks compared to methods which rely on currently-used state-of-the-art\nalgorithms. We define two models: an end-to-end reconstruction network which\nperforms simultaneous particle identification and energy regression of\nparticles when given calorimeter shower data, and a generative network which\ncan provide reasonable modeling of calorimeter showers for different particle\ntypes at specified angles and energies. We investigate the optimization of our\nmodels with hyperparameter scans. Furthermore, we demonstrate the applicability\nof the reconstruction model to shower inputs from other detector geometries,\nspecifically ATLAS-like and CMS-like geometries. These networks can serve as\nfast and computationally light methods for particle shower simulation and\nreconstruction for current and future experiments at particle colliders.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 06:19:04 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 21:13:51 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 22:41:43 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Belayneh", "Dawit", ""], ["Carminati", "Federico", ""], ["Farbin", "Amir", ""], ["Hooberman", "Benjamin", ""], ["Khattak", "Gulrukh", ""], ["Liu", "Miaoyuan", ""], ["Liu", "Junze", ""], ["Olivito", "Dominick", ""], ["Pacela", "Vit\u00f3ria Barin", ""], ["Pierini", "Maurizio", ""], ["Schwing", "Alexander", ""], ["Spiropulu", "Maria", ""], ["Vallecorsa", "Sofia", ""], ["Vlimant", "Jean-Roch", ""], ["Wei", "Wei", ""], ["Zhang", "Matt", ""]]}, {"id": "1912.06798", "submitter": "Xun Wang", "authors": "Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott", "title": "Cross-Batch Memory for Embedding Learning", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining informative negative instances are of central importance to deep\nmetric learning (DML), however this task is intrinsically limited by mini-batch\ntraining, where only a mini-batch of instances is accessible at each iteration.\nIn this paper, we identify a \"slow drift\" phenomena by observing that the\nembedding features drift exceptionally slow even as the model parameters are\nupdating throughout the training process. This suggests that the features of\ninstances computed at preceding iterations can be used to considerably\napproximate their features extracted by the current model. We propose a\ncross-batch memory (XBM) mechanism that memorizes the embeddings of past\niterations, allowing the model to collect sufficient hard negative pairs across\nmultiple mini-batches - even over the whole dataset. Our XBM can be directly\nintegrated into a general pair-based DML framework, where the XBM augmented DML\ncan boost performance considerably. In particular, without bells and whistles,\na simple contrastive loss with our XBM can have large R@1 improvements of\n12%-22.5% on three large-scale image retrieval datasets, surpassing the most\nsophisticated state-of-the-art methods, by a large margin. Our XBM is\nconceptually simple, easy to implement - using several lines of codes, and is\nmemory efficient - with a negligible 0.2 GB extra GPU memory. Code is available\nat: https://github.com/MalongTech/research-xbm.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 07:38:53 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 17:09:17 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 02:54:52 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wang", "Xun", ""], ["Zhang", "Haozhi", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "1912.06838", "submitter": "Burak Uzkent", "authors": "Vishnu Sarukkai, Anirudh Jain, Burak Uzkent, Stefano Ermon", "title": "Cloud Removal in Satellite Images Using Spatiotemporal Generative\n  Networks", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite images hold great promise for continuous environmental monitoring\nand earth observation. Occlusions cast by clouds, however, can severely limit\ncoverage, making ground information extraction more difficult. Existing\npipelines typically perform cloud removal with simple temporal composites and\nhand-crafted filters. In contrast, we cast the problem of cloud removal as a\nconditional image synthesis challenge, and we propose a trainable\nspatiotemporal generator network (STGAN) to remove clouds. We train our model\non a new large-scale spatiotemporal dataset that we construct, containing 97640\nimage pairs covering all continents. We demonstrate experimentally that the\nproposed STGAN model outperforms standard models and can generate realistic\ncloud-free images with high PSNR and SSIM values across a variety of\natmospheric conditions, leading to improved performance in downstream tasks\nsuch as land cover classification.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 13:03:31 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sarukkai", "Vishnu", ""], ["Jain", "Anirudh", ""], ["Uzkent", "Burak", ""], ["Ermon", "Stefano", ""]]}, {"id": "1912.06840", "submitter": "Heng Wang", "authors": "Heng Wang, Donghao Zhang, Yang Song, Heng Huang, Mei Chen, Weidong Cai", "title": "Region and Object based Panoptic Image Synthesis through Conditional\n  GANs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is significant to many computer vision and machine\nlearning tasks such as image synthesis and video synthesis. It has primary\napplications in the graphics editing and animation industries. With the\ndevelopment of generative adversarial networks, a lot of attention has been\ndrawn to image-to-image translation tasks. In this paper, we propose and\ninvestigate a novel task named as panoptic-level image-to-image translation and\na naive baseline of solving this task. Panoptic-level image translation extends\nthe current image translation task to two separate objectives of semantic style\ntranslation (adjust the style of objects to that of different domains) and\ninstance transfiguration (swap between different types of objects). The\nproposed task generates an image from a complete and detailed panoptic\nperspective which can enrich the context of real-world vision synthesis. Our\ncontribution consists of the proposal of a significant task worth investigating\nand a naive baseline of solving it. The proposed baseline consists of the\nmultiple instances sequential translation and semantic-level translation with\ndomain-invariant content code.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 13:26:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wang", "Heng", ""], ["Zhang", "Donghao", ""], ["Song", "Yang", ""], ["Huang", "Heng", ""], ["Chen", "Mei", ""], ["Cai", "Weidong", ""]]}, {"id": "1912.06842", "submitter": "Guolei Sun", "authors": "Guolei Sun, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan, Ling\n  Shao", "title": "Fine-grained Recognition: Accounting for Subtle Differences between\n  Similar Classes", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 13:33:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sun", "Guolei", ""], ["Cholakkal", "Hisham", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1912.06874", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray,\n  Aniket Bera, Dinesh Manocha", "title": "The Liar's Walk: Detecting Deception with Gait and Gesture", "comments": "10 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven deep neural algorithm for detecting deceptive\nwalking behavior using nonverbal cues like gaits and gestures. We conducted an\nelaborate user study, where we recorded many participants performing tasks\ninvolving deceptive walking. We extract the participants' walking gaits as\nseries of 3D poses. We annotate various gestures performed by participants\nduring their tasks. Based on the gait and gesture data, we train an LSTM-based\ndeep neural network to obtain deep features. Finally, we use a combination of\npsychology-based gait, gesture, and deep features to detect deceptive walking\nwith an accuracy of 88.41%. This is an improvement of 10.6% over handcrafted\ngait and gesture features and an improvement of 4.7% and 9.2% over classifiers\nbased on the state-of-the-art emotion and action classification algorithms,\nrespectively. Additionally, we present a novel dataset, DeceptiveWalk, that\ncontains gaits and gestures with their associated deception labels. To the best\nof our knowledge, ours is the first algorithm to detect deceptive behavior\nusing non-verbal cues of gait and gesture.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:18:56 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 03:22:47 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 14:24:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bhattacharya", "Uttaran", ""], ["Kapsaskis", "Kyra", ""], ["Gray", "Kurt", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1912.06888", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Michael S. Brown", "title": "Sensor-Independent Illumination Estimation for DNN Models", "comments": null, "journal-ref": "BMVC 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern deep neural networks (DNNs) achieve state-of-the-art results for\nilluminant estimation, it is currently necessary to train a separate DNN for\neach type of camera sensor. This means when a camera manufacturer uses a new\nsensor, it is necessary to retrain an existing DNN model with training images\ncaptured by the new sensor. This paper addresses this problem by introducing a\nnovel sensor-independent illuminant estimation framework. Our method learns a\nsensor-independent working space that can be used to canonicalize the RGB\nvalues of any arbitrary camera sensor. Our learned space retains the linear\nproperty of the original sensor raw-RGB space and allows unseen camera sensors\nto be used on a single DNN model trained on this working space. We demonstrate\nthe effectiveness of this approach on several different camera sensors and show\nit provides performance on par with state-of-the-art methods that were trained\nper sensor.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 17:36:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Brown", "Michael S.", ""]]}, {"id": "1912.06895", "submitter": "Cristian Canton Ferrer", "authors": "Hao Guo, Brian Dolhansky, Eric Hsin, Phong Dinh, Cristian Canton\n  Ferrer, Song Wang", "title": "Deep Poisoning: Towards Robust Image Data Sharing against Visual\n  Disclosure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to respectively limited training data, different entities addressing the\nsame vision task based on certain sensitive images may not train a robust deep\nnetwork. This paper introduces a new vision task where various entities share\ntask-specific image data to enlarge each other's training data volume without\nvisually disclosing sensitive contents (e.g. illegal images). Then, we present\na new structure-based training regime to enable different entities learn\ntask-specific and reconstruction-proof image representations for image data\nsharing. Specifically, each entity learns a private Deep Poisoning Module (DPM)\nand insert it to a pre-trained deep network, which is designed to perform the\nspecific vision task. The DPM deliberately poisons convolutional image features\nto prevent image reconstructions, while ensuring that the altered image data is\nfunctionally equivalent to the non-poisoned data for the specific vision task.\nGiven this equivalence, the poisoned features shared from one entity could be\nused by another entity for further model refinement. Experimental results on\nimage classification prove the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 18:02:53 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 02:56:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Guo", "Hao", ""], ["Dolhansky", "Brian", ""], ["Hsin", "Eric", ""], ["Dinh", "Phong", ""], ["Ferrer", "Cristian Canton", ""], ["Wang", "Song", ""]]}, {"id": "1912.06931", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Hong Liu, Nicu Sebe", "title": "Asymmetric Generative Adversarial Networks for Image-to-Image\n  Translation", "comments": "An extended version of a paper published in ACCV2018. arXiv admin\n  note: substantial text overlap with arXiv:1901.04604", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models for unpaired image-to-image translation with\nGenerative Adversarial Networks (GANs) can learn the mapping from the source\ndomain to the target domain using a cycle-consistency loss. The intuition\nbehind these models is that if we translate from one domain to the other and\nback again we should arrive at where we started. However, existing methods\nalways adopt a symmetric network architecture to learn both forward and\nbackward cycles. Because of the task complexity and cycle input difference\nbetween the source and target image domains, the inequality in bidirectional\nforward-backward cycle translations is significant and the amount of\ninformation between two domains is different. In this paper, we analyze the\nlimitation of the existing symmetric GAN models in asymmetric translation\ntasks, and propose an AsymmetricGAN model with both translation and\nreconstruction generators of unequal sizes and different parameter-sharing\nstrategy to adapt to the asymmetric need in both unsupervised and supervised\nimage-to-image translation tasks. Moreover, the training stage of existing\nmethods has the common problem of model collapse that degrades the quality of\nthe generated images, thus we explore different optimization losses for better\ntraining of AsymmetricGAN, and thus make image-to-image translation with higher\nconsistency and better stability. Extensive experiments on both supervised and\nunsupervised generative tasks with several publicly available datasets\ndemonstrate that the proposed AsymmetricGAN achieves superior model capacity\nand better generation performance compared with existing GAN models. To the\nbest of our knowledge, we are the first to investigate the asymmetric GAN\nframework on both unsupervised and supervised image-to-image translation tasks.\nThe source code, data and trained models are available at\nhttps://github.com/Ha0Tang/AsymmetricGAN.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:24:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Liu", "Hong", ""], ["Sebe", "Nicu", ""]]}, {"id": "1912.06960", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Michael S Brown", "title": "What Else Can Fool Deep Learning? Addressing Color Constancy Errors on\n  Deep Neural Network Performance", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is active research targeting local image manipulations that can fool\ndeep neural networks (DNNs) into producing incorrect results. This paper\nexamines a type of global image manipulation that can produce similar adverse\neffects. Specifically, we explore how strong color casts caused by incorrectly\napplied computational color constancy - referred to as white balance (WB) in\nphotography - negatively impact the performance of DNNs targeting image\nsegmentation and classification. In addition, we discuss how existing image\naugmentation methods used to improve the robustness of DNNs are not well suited\nfor modeling WB errors. To address this problem, a novel augmentation method is\nproposed that can emulate accurate color constancy degradation. We also explore\npre-processing training and testing images with a recent WB correction\nalgorithm to reduce the effects of incorrectly white-balanced images. We\nexamine both augmentation and pre-processing strategies on different datasets\nand demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 02:07:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Brown", "Michael S", ""]]}, {"id": "1912.06971", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng and Hanqing Lu", "title": "Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3028207", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph convolutional networks (GCNs), which generalize CNNs to more generic\nnon-Euclidean structures, have achieved remarkable performance for\nskeleton-based action recognition. However, there still exist several issues in\nthe previous GCN-based models. First, the topology of the graph is set\nheuristically and fixed over all the model layers and input data. This may not\nbe suitable for the hierarchy of the GCN model and the diversity of the data in\naction recognition tasks. Second, the second-order information of the skeleton\ndata, i.e., the length and orientation of the bones, is rarely investigated,\nwhich is naturally more informative and discriminative for the human action\nrecognition. In this work, we propose a novel multi-stream attention-enhanced\nadaptive graph convolutional neural network (MS-AAGCN) for skeleton-based\naction recognition. The graph topology in our model can be either uniformly or\nindividually learned based on the input data in an end-to-end manner. This\ndata-driven approach increases the flexibility of the model for graph\nconstruction and brings more generality to adapt to various data samples.\nBesides, the proposed adaptive graph convolutional layer is further enhanced by\na spatial-temporal-channel attention module, which helps the model pay more\nattention to important joints, frames and features. Moreover, the information\nof both the joints and bones, together with their motion information, are\nsimultaneously modeled in a multi-stream framework, which shows notable\nimprovement for the recognition accuracy. Extensive experiments on the two\nlarge-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the\nperformance of our model exceeds the state-of-the-art with a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 04:10:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "1912.06980", "submitter": "Baoyang Chen", "authors": "Weimian Li, Baoyang Chen, Wenmin Wang", "title": "Brain-Inspired Inference on Missing Video Sequence", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end architecture that could generate\na variety of plausible video sequences correlating two given discontinuous\nframes. Our work is inspired by the human ability of inference. Specifically,\ngiven two static images, human are capable of inferring what might happen in\nbetween as well as present diverse versions of their inference. We firstly\ntrain our model to learn the transformation to understand the movement trends\nwithin given frames. For the sake of imitating the inference of human, we\nintroduce a latent variable sampled from Gaussian distribution. By means of\nintegrating different latent variables with learned transformation features,\nthe model could learn more various possible motion modes. Then applying these\nmotion modes on the original frame, we could acquire various corresponding\nintermediate video sequence. Moreover, the framework is trained in adversarial\nfashion with unsupervised learning. Evaluating on the moving Mnist dataset and\nthe 2D Shapes dataset, we show that our model is capable of imitating the human\ninference to some extent.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:38:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Weimian", ""], ["Chen", "Baoyang", ""], ["Wang", "Wenmin", ""]]}, {"id": "1912.06992", "submitter": "Jingwei Ji", "authors": "Jingwei Ji, Ranjay Krishna, Li Fei-Fei, Juan Carlos Niebles", "title": "Action Genome: Actions as Composition of Spatio-temporal Scene Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has typically treated actions and activities as monolithic\nevents that occur in videos. However, there is evidence from Cognitive Science\nand Neuroscience that people actively encode activities into consistent\nhierarchical part structures. However in Computer Vision, few explorations on\nrepresentations encoding event partonomies have been made. Inspired by evidence\nthat the prototypical unit of an event is an action-object interaction, we\nintroduce Action Genome, a representation that decomposes actions into\nspatio-temporal scene graphs. Action Genome captures changes between objects\nand their pairwise relationships while an action occurs. It contains 10K videos\nwith 0.4M objects and 1.7M visual relationships annotated. With Action Genome,\nwe extend an existing action recognition model by incorporating scene graphs as\nspatio-temporal feature banks to achieve better performance on the Charades\ndataset. Next, by decomposing and learning the temporal changes in visual\nrelationships that result in an action, we demonstrate the utility of a\nhierarchical event decomposition by enabling few-shot action recognition,\nachieving 42.7% mAP using as few as 10 examples. Finally, we benchmark existing\nscene graph models on the new task of spatio-temporal scene graph prediction.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 06:56:33 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ji", "Jingwei", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1912.06994", "submitter": "ByungIn Yoo", "authors": "ByungIn Yoo, Tristan Sylvain, Yoshua Bengio, Junmo Kim", "title": "Joint Learning of Generative Translator and Classifier for Visually\n  Similar Classes", "comments": "14 pages, 17 figures, 13 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3042302", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Generative Translation Classification Network\n(GTCN) for improving visual classification accuracy in settings where classes\nare visually similar and data is scarce. For this purpose, we propose joint\nlearning from a scratch to train a classifier and a generative stochastic\ntranslation network end-to-end. The translation network is used to perform\non-line data augmentation across classes, whereas previous works have mostly\ninvolved domain adaptation. To help the model further benefit from this\ndata-augmentation, we introduce an adaptive fade-in loss and a quadruplet loss.\nWe perform experiments on multiple datasets to demonstrate the proposed\nmethod's performance in varied settings. Of particular interest, training on\n40% of the dataset is enough for our model to surpass the performance of\nbaselines trained on the full dataset. When our architecture is trained on the\nfull dataset, we achieve comparable performance with state-of-the-art methods\ndespite using a light-weight architecture.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 07:08:44 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 15:16:38 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yoo", "ByungIn", ""], ["Sylvain", "Tristan", ""], ["Bengio", "Yoshua", ""], ["Kim", "Junmo", ""]]}, {"id": "1912.07009", "submitter": "Albert Pumarola", "authors": "Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, Vittorio\n  Ferrari", "title": "C-Flow: Conditional Generative Flow Models for Images and 3D Point\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models have highly desirable properties like exact\nlog-likelihood evaluation and exact latent-variable inference, however they are\nstill in their infancy and have not received as much attention as alternative\ngenerative models. In this paper, we introduce C-Flow, a novel conditioning\nscheme that brings normalizing flows to an entirely new scenario with great\npossibilities for multi-modal data modeling. C-Flow is based on a parallel\nsequence of invertible mappings in which a source flow guides the target flow\nat every step, enabling fine-grained control over the generation process. We\nalso devise a new strategy to model unordered 3D point clouds that, in\ncombination with the conditioning scheme, makes it possible to address 3D\nreconstruction from a single image and its inverse problem of rendering an\nimage given a point cloud. We demonstrate our conditioning method to be very\nadaptable, being also applicable to image manipulation, style transfer and\nmulti-modal image-to-image mapping in a diversity of domains, including RGB\nimages, segmentation maps, and edge masks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 09:10:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 11:17:23 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Pumarola", "Albert", ""], ["Popov", "Stefan", ""], ["Moreno-Noguer", "Francesc", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1912.07010", "submitter": "Zhe Chen", "authors": "Zhe Chen, Wanli Ouyang, Tongliang Liu, Dacheng Tao", "title": "A Shape Transformation-based Dataset Augmentation Framework for\n  Pedestrian Detection", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-020-01412-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based computer vision is usually data-hungry. Many researchers\nattempt to augment datasets with synthesized data to improve model robustness.\nHowever, the augmentation of popular pedestrian datasets, such as Caltech and\nCitypersons, can be extremely challenging because real pedestrians are commonly\nin low quality. Due to the factors like occlusions, blurs, and low-resolution,\nit is significantly difficult for existing augmentation approaches, which\ngenerally synthesize data using 3D engines or generative adversarial networks\n(GANs), to generate realistic-looking pedestrians. Alternatively, to access\nmuch more natural-looking pedestrians, we propose to augment pedestrian\ndetection datasets by transforming real pedestrians from the same dataset into\ndifferent shapes. Accordingly, we propose the Shape Transformation-based\nDataset Augmentation (STDA) framework. The proposed framework is composed of\ntwo subsequent modules, i.e. the shape-guided deformation and the environment\nadaptation. In the first module, we introduce a shape-guided warping field to\nhelp deform the shape of a real pedestrian into a different shape. Then, in the\nsecond stage, we propose an environment-aware blending map to better adapt the\ndeformed pedestrians into surrounding environments, obtaining more\nrealistic-looking pedestrians and more beneficial augmentation results for\npedestrian detection. Extensive empirical studies on different pedestrian\ndetection benchmarks show that the proposed STDA framework consistently\nproduces much better augmentation results than other pedestrian synthesis\napproaches using low-quality pedestrians. By augmenting the original datasets,\nour proposed framework also improves the baseline pedestrian detector by up to\n38% on the evaluated benchmarks, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 09:12:04 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Zhe", ""], ["Ouyang", "Wanli", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1912.07011", "submitter": "Jesper Christensen", "authors": "Jesper Haahr Christensen, Sascha Hornauer, Stella Yu", "title": "BatVision: Learning to See 3D Spatial Layout with Two Ears", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many species have evolved advanced non-visual perception while artificial\nsystems fall behind. Radar and ultrasound complement camera-based vision but\nthey are often too costly and complex to set up for very limited information\ngain. In nature, sound is used effectively by bats, dolphins, whales, and\nhumans for navigation and communication. However, it is unclear how to best\nharness sound for machine perception. Inspired by bats' echolocation mechanism,\nwe design a low-cost BatVision system that is capable of seeing the 3D spatial\nlayout of space ahead by just listening with two ears. Our system emits short\nchirps from a speaker and records returning echoes through microphones in an\nartificial human pinnae pair. During training, we additionally use a stereo\ncamera to capture color images for calculating scene depths. We train a model\nto predict depth maps and even grayscale images from the sound alone. During\ntesting, our trained BatVision provides surprisingly good predictions of 2D\nvisual scenes from two 1D audio signals. Such a sound to vision system would\nbenefit robot navigation and machine vision, especially in low-light or\nno-light conditions. Our code and data are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 09:33:04 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 12:14:36 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 07:57:28 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Christensen", "Jesper Haahr", ""], ["Hornauer", "Sascha", ""], ["Yu", "Stella", ""]]}, {"id": "1912.07015", "submitter": "Zhao Zhang", "authors": "Yanyan Wei, Zhao Zhang, Yang Wang, Mingliang Xu, Yi Yang, Shuicheng\n  Yan, Meng Wang", "title": "DerainCycleGAN: Rain Attentive CycleGAN for Single Image Deraining and\n  Rainmaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image deraining (SID) is an important and challenging topic in\nemerging vision applications, and most of emerged deraining methods are\nsupervised relying on the ground truth (i.e., paired images) in recent years.\nHowever, in practice it is rather common to have no un-paired images in real\nderaining task, in such cases how to remove the rain streaks in an unsupervised\nway will be a very challenging task due to lack of constraints between images\nand hence suffering from low-quality recovery results. In this paper, we\nexplore the unsupervised SID task using unpaired data and propose a novel net\ncalled Attention-guided Deraining by Constrained CycleGAN (or shortly,\nDerainCycleGAN), which can fully utilize the constrained transfer learning\nabilitiy and circulatory structure of CycleGAN. Specifically, we design an\nunsu-pervised attention guided rain streak extractor (U-ARSE) that utilizes a\nmemory to extract the rain streak masks with two constrained cycle-consistency\nbranches jointly by paying attention to both the rainy and rain-free image\ndomains. As a by-product, we also contribute a new paired rain image dataset\ncalled Rain200A, which is constructed by our network automatically. Compared\nwith existing synthesis datasets, the rainy streaks in Rain200A contains more\nobvious and diverse shapes and directions. As a result, existing supervised\nmethods trained on Rain200A can perform much better for processing real rainy\nimages. Extensive experiments on synthesis and real datasets show that our net\nis superior to existing unsupervised deraining networks, and is also very\ncompetitive to other related supervised networks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 10:26:54 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 13:55:14 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 11:29:33 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 08:46:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wei", "Yanyan", ""], ["Zhang", "Zhao", ""], ["Wang", "Yang", ""], ["Xu", "Mingliang", ""], ["Yang", "Yi", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.07016", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Zemin Tang, Yang Wang, Haijun Zhang, Shuicheng Yan, Meng\n  Wang", "title": "Compressed DenseNet for Lightweight Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Recurrent Neural Network (CRNN) is a popular network for\nrecognizing texts in images. Advances like the variant of CRNN, such as Dense\nConvolutional Network with Connectionist Temporal Classification, has reduced\nthe running time of the network, but exposing the inner computation cost and\nweight size of the convolutional networks as a bottleneck. Specifically, the\nDenseNet based models utilize the dense blocks as the core module, but the\ninner features are combined in the form of concatenation in dense blocks. As\nsuch, the number of channels of combined features delivered as the input of the\nlayers close to the output and the relevant computational cost grows rapidly\nwith the dense blocks getting deeper. This will severely bring heavy\ncomputational cost and big weight size, which restrict the depth of dense\nblocks. In this paper, we propose a compressed convolution block called\nLightweight Dense Block (LDB). To reduce the computing cost and weight size, we\nre-define and re-design the way of combining internal features of the dense\nblocks. LDB is a convolutional block similarly as dense block, but it can\nreduce the computation cost and weight size to (1/L, 2/L), compared with\noriginal ones, where L is the number of layers in blocks. Moreover, LDB can be\nused to replace the original dense block in any DenseNet based models. Based on\nthe LDBs, we propose a Compressed DenseNet (CDenseNet) for the lightweight\ncharacter recognition. Extensive experiments demonstrate that CDenseNet can\neffectively reduce the weight size while delivering the promising recognition\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 10:26:59 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 08:42:59 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 11:28:17 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhang", "Zhao", ""], ["Tang", "Zemin", ""], ["Wang", "Yang", ""], ["Zhang", "Haijun", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.07025", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Abhishek Prusty, Sowmya Aitha, Abhishek Trivedi, Ravi Kiran\n  Sarvadevabhatla", "title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of\n  Historical Indic Manuscripts", "comments": "Oral presentation at International Conference on Document Analysis\n  and Recognition (ICDAR) - 2019. For dataset, pre-trained networks and\n  additional details, visit project page at http://ihdia.iiit.ac.in/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical palm-leaf manuscript and early paper documents from Indian\nsubcontinent form an important part of the world's literary and cultural\nheritage. Despite their importance, large-scale annotated Indic manuscript\nimage datasets do not exist. To address this deficiency, we introduce\nIndiscapes, the first ever dataset with multi-regional layout annotations for\nhistorical Indic manuscripts. To address the challenge of large diversity in\nscripts and presence of dense, irregular layout elements (e.g. text lines,\npictures, multiple documents per image), we adapt a Fully Convolutional Deep\nNeural Network architecture for fully automatic, instance-level spatial layout\nparsing of manuscript images. We demonstrate the effectiveness of proposed\narchitecture on images from the Indiscapes dataset. For annotation flexibility\nand keeping the non-technical nature of domain experts in mind, we also\ncontribute a custom, web-based GUI annotation tool and a dashboard-style\nanalytics portal. Overall, our contributions set the stage for enabling\ndownstream applications such as OCR and word-spotting in historical Indic\nmanuscripts at scale.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 11:42:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Prusty", "Abhishek", ""], ["Aitha", "Sowmya", ""], ["Trivedi", "Abhishek", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "1912.07105", "submitter": "Jianqing Jia", "authors": "Jianqing Jia, Semir Elezovikj, Heng Fan, Shuojin Yang, Jing Liu, Wei\n  Guo, Chiu C. Tan, Haibin Ling", "title": "Semantic-Aware Label Placement for Augmented Reality in Street View", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an augmented reality (AR) application, placing labels in a manner that is\nclear and readable without occluding the critical information from the\nreal-world can be a challenging problem. This paper introduces a label\nplacement technique for AR used in street view scenarios. We propose a\nsemantic-aware task-specific label placement method by identifying potentially\nimportant image regions through a novel feature map, which we refer to as\nguidance map. Given an input image, its saliency information, semantic\ninformation and the task-specific importance prior are integrated into the\nguidance map for our labeling task. To learn the task prior, we created a label\nplacement dataset with the users' labeling preferences, as well as use it for\nevaluation. Our solution encodes the constraints for placing labels in an\noptimization problem to obtain the final label layout, and the labels will be\nplaced in appropriate positions to reduce the chances of overlaying important\nreal-world objects in street view AR scenarios. The experimental validation\nshows clearly the benefits of our method over previous solutions in the AR\nstreet view navigation and similar applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:29:37 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Jia", "Jianqing", ""], ["Elezovikj", "Semir", ""], ["Fan", "Heng", ""], ["Yang", "Shuojin", ""], ["Liu", "Jing", ""], ["Guo", "Wei", ""], ["Tan", "Chiu C.", ""], ["Ling", "Haibin", ""]]}, {"id": "1912.07106", "submitter": "Runde Yang", "authors": "Runde Yang", "title": "Towards Building a Real Time Mobile Device Bird Counting System Through\n  Synthetic Data Training and Model Compression", "comments": "The paper is in a wrong format for ICML. I really need to withdraw\n  the paper to modify the content and submit it to other computer vision\n  conferences. Some sections need to be completely rewritten and I recognize\n  certain parts that are not consistent with the major theme of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of birds in an open sky setting has been an challenging\nproblem due to the large number of bird flocks and the birds can overlap.\nAnother difficulty is the lack of accurate training samples since the cost of\nlabeling images of bird flocks can be extremely high and each sample picture\ncan contain thousands of birds in a high resolution image. Inspired by recent\nwork on training with synthetic data to perform crowd counting, we design a\nmechanism to generate synthetic bird dataset with precise bird count and the\ncorresponding density maps. We then train a Unet model on the synthetic dataset\nto perform density map estimation that produces the count for each input. Our\nmethod is able to achieve MSE of approximately 12.4 on real dataset. In order\nto build a scalable system for fast bird counting under storage and\ncomputational constraints, we use model compression techniques and efficient\nmodel structures to increase the inference speed and save storage cost. We are\nable to reduce storage cost from 55MB to less than 5MB for the model with\nminimum loss of accuracy. This paper describes the pipelines of building an\nefficient bird counting system.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:30:24 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 02:37:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Yang", "Runde", ""]]}, {"id": "1912.07109", "submitter": "Yue Jiang", "authors": "Yue Jiang, Dantong Ji, Zhizhong Han, Matthias Zwicker", "title": "SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SDFDiff, a novel approach for image-based shape optimization using\ndifferentiable rendering of 3D shapes represented by signed distance functions\n(SDF). Compared to other representations, SDFs have the advantage that they can\nrepresent shapes with arbitrary topology, and that they guarantee watertight\nsurfaces. We apply our approach to the problem of multi-view 3D reconstruction,\nwhere we achieve high reconstruction quality and can capture complex topology\nof 3D objects. In addition, we employ a multi-resolution strategy to obtain a\nrobust optimization algorithm. We further demonstrate that our SDF-based\ndifferentiable renderer can be integrated with deep learning models, which\nopens up options for learning approaches on 3D objects without 3D supervision.\nIn particular, we apply our method to single-view 3D reconstruction and achieve\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 21:06:46 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jiang", "Yue", ""], ["Ji", "Dantong", ""], ["Han", "Zhizhong", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1912.07116", "submitter": "Yujun Shen", "authors": "Jinjin Gu, Yujun Shen, Bolei Zhou", "title": "Image Processing Using Multi-Code GAN Prior", "comments": "CVPR2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs) in image\nsynthesis, applying trained GAN models to real image processing remains\nchallenging. Previous methods typically invert a target image back to the\nlatent space either by back-propagation or by learning an additional encoder.\nHowever, the reconstructions from both of the methods are far from ideal. In\nthis work, we propose a novel approach, called mGANprior, to incorporate the\nwell-trained GANs as effective prior to a variety of image processing tasks. In\nparticular, we employ multiple latent codes to generate multiple feature maps\nat some intermediate layer of the generator, then compose them with adaptive\nchannel importance to recover the input image. Such an over-parameterization of\nthe latent space significantly improves the image reconstruction quality,\noutperforming existing competitors. The resulting high-fidelity image\nreconstruction enables the trained GAN models as prior to many real-world\napplications, such as image colorization, super-resolution, image inpainting,\nand semantic manipulation. We further analyze the properties of the layer-wise\nrepresentation learned by GAN models and shed light on what knowledge each\nlayer is capable of representing.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 21:35:43 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 10:20:50 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gu", "Jinjin", ""], ["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "1912.07124", "submitter": "Suman Saha", "authors": "Suman Saha, Wenhao Xu, Menelaos Kanakis, Stamatios Georgoulis, Yuhua\n  Chen, Danda Pani Paudel, Luc Van Gool", "title": "Domain Agnostic Feature Learning for Image and Video Based Face\n  Anti-spoofing", "comments": "CVPR 2020 Biometrics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the increasingly growing number of mobile and computing devices has\nled to a demand for safer user authentication systems. Face anti-spoofing is a\nmeasure towards this direction for bio-metric user authentication, and in\nparticular face recognition, that tries to prevent spoof attacks. The\nstate-of-the-art anti-spoofing techniques leverage the ability of deep neural\nnetworks to learn discriminative features, based on cues from the training set\nimages or video samples, in an effort to detect spoof attacks. However, due to\nthe particular nature of the problem, i.e. large variability due to factors\nlike different backgrounds, lighting conditions, camera resolutions, spoof\nmaterials, etc., these techniques typically fail to generalize to new samples.\nIn this paper, we explicitly tackle this problem and propose a\nclass-conditional domain discriminator module, that, coupled with a gradient\nreversal layer, tries to generate live and spoof features that are\ndiscriminative, but at the same time robust against the aforementioned\nvariability factors. Extensive experimental analysis shows the effectiveness of\nthe proposed method over existing image- and video-based anti-spoofing\ntechniques, both in terms of numerical improvement as well as when visualizing\nthe learned features.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:46:53 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:09:31 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Saha", "Suman", ""], ["Xu", "Wenhao", ""], ["Kanakis", "Menelaos", ""], ["Georgoulis", "Stamatios", ""], ["Chen", "Yuhua", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "1912.07133", "submitter": "Hugh Kennedy Dr.", "authors": "Hugh L. Kennedy", "title": "Digital filters with vanishing moments for shape analysis", "comments": "Fixed some cut-and-paste typos in Table V", "journal-ref": "SPIE Journal of Electronic Imaging, vol. 27, no. 5, 051219, May\n  2018", "doi": "10.1117/1.JEI.27.5.051219", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shape- and scale-selective digital-filters, with steerable finite/infinite\nimpulse responses (FIR/IIRs) and non-recursive/recursive realizations, that are\nseparable in both spatial dimensions and adequately isotropic, are derived. The\nfilters are conveniently designed in the frequency domain via derivative\nconstraints at dc, which guarantees orthogonality and monomial selectivity in\nthe pixel domain (i.e. vanishing moments), unlike more commonly used FIR\nfilters derived from Gaussian functions. A two-stage low-pass/high-pass\narchitecture, for blur/derivative operations, is recommended. Expressions for\nthe coefficients of a low-order IIR blur filter with repeated poles are\nprovided, as a function of scale; discrete Butterworth (IIR), and colored\nSavitzky-Golay (FIR), blurs are also examined. Parallel software\nimplementations on central processing units (CPUs) and graphics processing\nunits (GPUs), for scale-selective blob-detection in aerial surveillance\nimagery, are analyzed. It is shown that recursive IIR filters are significantly\nfaster than non-recursive FIR filters when detecting large objects at coarse\nscales, i.e. using filters with long impulse responses; however, the margin of\noutperformance decreases as the degree of parallelization increases.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 23:24:58 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 01:51:38 GMT"}, {"version": "v3", "created": "Thu, 26 Dec 2019 02:51:31 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 00:07:09 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kennedy", "Hugh L.", ""]]}, {"id": "1912.07142", "submitter": "Weixin Jiang", "authors": "Weixin Jiang, Eric Schwenker, Maria Chan, Oliver Cossairt", "title": "Semantic Segmentation for Compound figures", "comments": "This paper has been withdrawn by the authors. This paper has been\n  superseded by arXiv:2101.09903", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific literature contains large volumes of unstructured data,with over\n30\\% of figures constructed as a combination of multiple images, these compound\nfigures cannot be analyzed directly with existing information retrieval tools.\nIn this paper, we propose a semantic segmentation approach for compound figure\nseparation, decomposing the compound figures into \"master images\". Each master\nimage is one part of a compound figure governed by a subfigure label (typically\n\"(a), (b), (c), etc\"). In this way, the separated subfigures can be easily\nassociated with the description information in the caption. In particular, we\npropose an anchor-based master image detection algorithm, which leverages the\ncorrelation between master images and subfigure labels and locates the master\nimages in a two-step manner. First, a subfigure label detector is built to\nextract the global layout information of the compound figure. Second, the\nlayout information is combined with local features to locate the master images.\nWe validate the effectiveness of proposed method on our labeled testing dataset\nboth quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 00:42:06 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:34:42 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 17:37:44 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Jiang", "Weixin", ""], ["Schwenker", "Eric", ""], ["Chan", "Maria", ""], ["Cossairt", "Oliver", ""]]}, {"id": "1912.07148", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Predicting the Future: A Jointly Learnt Model for Action Anticipation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by human neurological structures for action anticipation, we present\nan action anticipation model that enables the prediction of plausible future\nactions by forecasting both the visual and temporal future. In contrast to\ncurrent state-of-the-art methods which first learn a model to predict future\nvideo features and then perform action anticipation using these features, the\nproposed framework jointly learns to perform the two tasks, future visual and\ntemporal representation synthesis, and early action anticipation. The joint\nlearning framework ensures that the predicted future embeddings are informative\nto the action anticipation task. Furthermore, through extensive experimental\nevaluations we demonstrate the utility of using both visual and temporal\nsemantics of the scene, and illustrate how this representation synthesis could\nbe achieved through a recurrent Generative Adversarial Network (GAN) framework.\nOur model outperforms the current state-of-the-art methods on multiple\ndatasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 01:03:45 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1912.07150", "submitter": "Wenhan Yang", "authors": "Wenhan Yang, Robby T. Tan, Shiqi Wang, Yuming Fang, Jiaying Liu", "title": "Single Image Deraining: From Model-Based to Data-Driven and Beyond", "comments": "https://flyywh.github.io/Single_rain_removal_survey/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of single-image deraining is to restore the rain-free background\nscenes of an image degraded by rain streaks and rain accumulation. The early\nsingle-image deraining methods employ a cost function, where various priors are\ndeveloped to represent the properties of rain and background layers. Since\n2017, single-image deraining methods step into a deep-learning era, and exploit\nvarious types of networks, i.e. convolutional neural networks, recurrent neural\nnetworks, generative adversarial networks, etc., demonstrating impressive\nperformance. Given the current rapid development, in this paper, we provide a\ncomprehensive survey of deraining methods over the last decade. We summarize\nthe rain appearance models, and discuss two categories of deraining approaches:\nmodel-based and data-driven approaches. For the former, we organize the\nliterature based on their basic models and priors. For the latter, we discuss\ndeveloped ideas related to architectures, constraints, loss functions, and\ntraining datasets. We present milestones of single-image deraining methods,\nreview a broad selection of previous works in different categories, and provide\ninsights on the historical development route from the model-based to\ndata-driven methods. We also summarize performance comparisons quantitatively\nand qualitatively. Beyond discussing the technicality of deraining methods, we\nalso discuss the future directions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 01:17:05 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 18:48:55 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Wenhan", ""], ["Tan", "Robby T.", ""], ["Wang", "Shiqi", ""], ["Fang", "Yuming", ""], ["Liu", "Jiaying", ""]]}, {"id": "1912.07160", "submitter": "Sizhe Chen", "authors": "Sizhe Chen, Xiaolin Huang, Zhengbao He, Chengjin Sun", "title": "DAmageNet: A Universal Adversarial Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well known that deep neural networks (DNNs) are vulnerable to\nadversarial attack. Adversarial samples are similar to the clean ones, but are\nable to cheat the attacked DNN to produce incorrect predictions in high\nconfidence. But most of the existing adversarial attacks have high success rate\nonly when the information of the attacked DNN is well-known or could be\nestimated by massive queries. A promising way is to generate adversarial\nsamples with high transferability. By this way, we generate 96020 transferable\nadversarial samples from original ones in ImageNet. The average difference,\nmeasured by root means squared deviation, is only around 3.8 on average.\nHowever, the adversarial samples are misclassified by various models with an\nerror rate up to 90\\%. Since the images are generated independently with the\nattacked DNNs, this is essentially zero-query adversarial attack. We call the\ndataset \\emph{DAmageNet}, which is the first universal adversarial dataset that\nbeats many models trained in ImageNet. By finding the drawbacks, DAmageNet\ncould serve as a benchmark to study and improve robustness of DNNs. DAmageNet\ncould be downloaded in http://www.pami.sjtu.edu.cn/Show/56/122.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:11:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chen", "Sizhe", ""], ["Huang", "Xiaolin", ""], ["He", "Zhengbao", ""], ["Sun", "Chengjin", ""]]}, {"id": "1912.07161", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Shafin Rahman, Dylan Campbell, Lars Petersson", "title": "Transductive Zero-Shot Learning for 3D Point Cloud Classification", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning, the task of learning to recognize new classes not seen\nduring training, has received considerable attention in the case of 2D image\nclassification. However despite the increasing ubiquity of 3D sensors, the\ncorresponding 3D point cloud classification problem has not been meaningfully\nexplored and introduces new challenges. This paper extends, for the first time,\ntransductive Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL)\napproaches to the domain of 3D point cloud classification. To this end, a novel\ntriplet loss is developed that takes advantage of unlabeled test data. While\ndesigned for the task of 3D point cloud classification, the method is also\nshown to be applicable to the more common use-case of 2D image classification.\nAn extensive set of experiments is carried out, establishing state-of-the-art\nfor ZSL and GZSL in the 3D point cloud domain, as well as demonstrating the\napplicability of the approach to the image domain.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:24:10 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 01:18:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cheraghian", "Ali", ""], ["Rahman", "Shafin", ""], ["Campbell", "Dylan", ""], ["Petersson", "Lars", ""]]}, {"id": "1912.07167", "submitter": "Riqiang Gao", "authors": "Yiyuan Yang, Riqiang Gao, Yucheng Tang, Sanja L. Antic, Steve Deppen,\n  Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman", "title": "Internal-transfer Weighting of Multi-task Learning for Lung Cancer\n  Detection", "comments": "Accepted by Medical Imaging, SPIE2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-task networks have shown to both offer additional estimation\ncapabilities, and, perhaps more importantly, increased performance over\nsingle-task networks on a \"main/primary\" task. However, balancing the\noptimization criteria of multi-task networks across different tasks is an area\nof active exploration. Here, we extend a previously proposed 3D attention-based\nnetwork with four additional multi-task subnetworks for the detection of lung\ncancer and four auxiliary tasks (diagnosis of asthma, chronic bronchitis,\nchronic obstructive pulmonary disease, and emphysema). We introduce and\nevaluate a learning policy, Periodic Focusing Learning Policy (PFLP), that\nalternates the dominance of tasks throughout the training. To improve\nperformance on the primary task, we propose an Internal-Transfer Weighting\n(ITW) strategy to suppress the loss functions on auxiliary tasks for the final\nstages of training. To evaluate this approach, we examined 3386 patients\n(single scan per patient) from the National Lung Screening Trial (NLST) and\nde-identified data from the Vanderbilt Lung Screening Program, with a\n2517/277/592 (scans) split for training, validation, and testing. Baseline\nnetworks include a single-task strategy and a multi-task strategy without\nadaptive weights (PFLP/ITW), while primary experiments are multi-task trials\nwith either PFLP or ITW or both. On the test set for lung cancer prediction,\nthe baseline single-task network achieved prediction AUC of 0.8080 and the\nmulti-task baseline failed to converge (AUC 0.6720). However, applying PFLP\nhelped multi-task network clarify and achieved test set lung cancer prediction\nAUC of 0.8402. Furthermore, our ITW technique boosted the PFLP enabled\nmulti-task network and achieved an AUC of 0.8462 (McNemar test, p < 0.01).\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:46:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yang", "Yiyuan", ""], ["Gao", "Riqiang", ""], ["Tang", "Yucheng", ""], ["Antic", "Sanja L.", ""], ["Deppen", "Steve", ""], ["Huo", "Yuankai", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1912.07183", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Rui Zeng, Sabesan Sivapalan, Sridha\n  Sridharan, Clinton Fookes", "title": "MTRNet++: One-stage Mask-based Scene Text Eraser", "comments": "This paper is under CVIU review (after major revision)", "journal-ref": null, "doi": "10.1016/j.cviu.2020.103066", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A precise, controllable, interpretable and easily trainable text removal\napproach is necessary for both user-specific and large-scale text removal\napplications. To achieve this, we propose a one-stage mask-based text\ninpainting network, MTRNet++. It has a novel architecture that includes\nmask-refine, coarse-inpainting and fine-inpainting branches, and attention\nblocks. With this architecture, MTRNet++ can remove text either with or without\nan external mask. It achieves state-of-the-art results on both the Oxford and\nSCUT datasets without using external ground-truth masks. The results of\nablation studies demonstrate that the proposed multi-branch architecture with\nattention blocks is effective and essential. It also demonstrates\ncontrollability and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 04:11:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 06:44:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Zeng", "Rui", ""], ["Sivapalan", "Sabesan", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1912.07190", "submitter": "Ryosuke Furuta", "authors": "Ryosuke Furuta, Naoto Inoue, Toshihiko Yamasaki", "title": "PixelRL: Fully Convolutional Network with Reinforcement Learning for\n  Image Processing", "comments": "To appear in IEEE Transactions on Multimedia (TMM), Special Issue on\n  Multimedia Computing with Interpretable Machine Learning. Extended version of\n  our paper in AAAI 2019 (arXiv:1811.04323)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles a new problem setting: reinforcement learning with\npixel-wise rewards (pixelRL) for image processing. After the introduction of\nthe deep Q-network, deep RL has been achieving great success. However, the\napplications of deep reinforcement learning (RL) for image processing are still\nlimited. Therefore, we extend deep RL to pixelRL for various image processing\napplications. In pixelRL, each pixel has an agent, and the agent changes the\npixel value by taking an action. We also propose an effective learning method\nfor pixelRL that significantly improves the performance by considering not only\nthe future states of the own pixel but also those of the neighbor pixels. The\nproposed method can be applied to some image processing tasks that require\npixel-wise manipulations, where deep RL has never been applied. Besides, it is\npossible to visualize what kind of operation is employed for each pixel at each\niteration, which would help us understand why and how such an operation is\nchosen. We also believe that our technology can enhance the explainability and\ninterpretability of the deep neural networks. In addition, because the\noperations executed at each pixels are visualized, we can change or modify the\noperations if necessary. We apply the proposed method to a variety of image\nprocessing tasks: image denoising, image restoration, local color enhancement,\nand saliency-driven image editing. Our experimental results demonstrate that\nthe proposed method achieves comparable or better performance, compared with\nthe state-of-the-art methods based on supervised learning. The source code is\navailable on https://github.com/rfuruta/pixelRL.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 04:42:37 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Furuta", "Ryosuke", ""], ["Inoue", "Naoto", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "1912.07192", "submitter": "Soumyabrata Dev", "authors": "Lucie L\\'ev\\^eque, Soumyabrata Dev, Murhaf Hossari, Yee Hui Lee and\n  Stefan Winkler", "title": "Subjective Quality Assessment of Ground-based Camera Images", "comments": "Published in Proc. Progress In Electromagnetics Research Symposium\n  (PIERS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment is critical to control and maintain the perceived\nquality of visual content. Both subjective and objective evaluations can be\nutilised, however, subjective image quality assessment is currently considered\nthe most reliable approach. Databases containing distorted images and mean\nopinion scores are needed in the field of atmospheric research with a view to\nimprove the current state-of-the-art methodologies. In this paper, we focus on\nusing ground-based sky camera images to understand the atmospheric events. We\npresent a new image quality assessment dataset containing original and\ndistorted nighttime images of sky/cloud from SWINSEG database. Subjective\nquality assessment was carried out in controlled conditions, as recommended by\nthe ITU. Statistical analyses of the subjective scores showed the impact of\nnoise type and distortion level on the perceived quality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 04:49:32 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["L\u00e9v\u00eaque", "Lucie", ""], ["Dev", "Soumyabrata", ""], ["Hossari", "Murhaf", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1912.07195", "submitter": "Vishesh Mistry", "authors": "Vishesh Mistry, Joshua J. Engelsma, Anil K. Jain", "title": "Fingerprint Synthesis: Search with 100 Million Prints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of large-scale fingerprint search algorithms has been limited due\nto lack of publicly available datasets. To address this problem, we utilize a\nGenerative Adversarial Network (GAN) to synthesize a fingerprint dataset\nconsisting of 100 million fingerprint images. In contrast to existing\nfingerprint synthesis algorithms, we incorporate an identity loss which guides\nthe generator to synthesize fingerprints corresponding to more distinct\nidentities. The characteristics of our synthesized fingerprints are shown to be\nmore similar to real fingerprints than existing methods via eight different\nmetrics (minutiae count - block and template, minutiae direction - block and\ntemplate, minutiae convex hull area, minutiae spatial distribution, block\nminutiae quality distribution, and NFIQ 2.0 scores). Additionally, the\nsynthetic fingerprints based on our approach are shown to be more distinct than\nsynthetic fingerprints based on published methods through search results and\nimposter distribution statistics. Finally, we report for the first time in open\nliterature, search accuracy against a gallery of 100 million fingerprint images\n(NIST SD4 Rank-1 accuracy of 89.7%).\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:09:52 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 05:06:43 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 18:10:12 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Mistry", "Vishesh", ""], ["Engelsma", "Joshua J.", ""], ["Jain", "Anil K.", ""]]}, {"id": "1912.07197", "submitter": "Seyed Amir Hossein Hosseini", "authors": "Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, Mingyi\n  Hong, and Mehmet Ak\\c{c}akaya", "title": "Dense Recurrent Neural Networks for Accelerated MRI: History-Cognizant\n  Unrolling of Optimization Algorithms", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2020", "doi": "10.1109/JSTSP.2020.3003170", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems for accelerated MRI typically incorporate domain-specific\nknowledge about the forward encoding operator in a regularized reconstruction\nframework. Recently physics-driven deep learning (DL) methods have been\nproposed to use neural networks for data-driven regularization. These methods\nunroll iterative optimization algorithms to solve the inverse problem objective\nfunction, by alternating between domain-specific data consistency and\ndata-driven regularization via neural networks. The whole unrolled network is\nthen trained end-to-end to learn the parameters of the network. Due to\nsimplicity of data consistency updates with gradient descent steps, proximal\ngradient descent (PGD) is a common approach to unroll physics-driven DL\nreconstruction methods. However, PGD methods have slow convergence rates,\nnecessitating a higher number of unrolled iterations, leading to memory issues\nin training and slower reconstruction times in testing. Inspired by efficient\nvariants of PGD methods that use a history of the previous iterates, we propose\na history-cognizant unrolling of the optimization algorithm with dense\nconnections across iterations for improved performance. In our approach, the\ngradient descent steps are calculated at a trainable combination of the outputs\nof all the previous regularization units. We also apply this idea to unrolling\nvariable splitting methods with quadratic relaxation. Our results in\nreconstruction of the fastMRI knee dataset show that the proposed\nhistory-cognizant approach reduces residual aliasing artifacts compared to its\nconventional unrolled counterpart without requiring extra computational power\nor increasing reconstruction time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:20:19 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 05:00:51 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hosseini", "Seyed Amir Hossein", ""], ["Yaman", "Burhaneddin", ""], ["Moeller", "Steen", ""], ["Hong", "Mingyi", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "1912.07200", "submitter": "Yunhui Guo", "authors": "Yunhui Guo, Noel C. Codella, Leonid Karlinsky, James V. Codella, John\n  R. Smith, Kate Saenko, Tajana Rosing, Rogerio Feris", "title": "A Broader Study of Cross-Domain Few-Shot Learning", "comments": "ECCV 2020. Website: https://www.learning-with-limited-labels.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent progress on few-shot learning largely relies on annotated data for\nmeta-learning: base classes sampled from the same domain as the novel classes.\nHowever, in many applications, collecting data for meta-learning is infeasible\nor impossible. This leads to the cross-domain few-shot learning problem, where\nthere is a large shift between base and novel class domains. While\ninvestigations of the cross-domain few-shot scenario exist, these works are\nlimited to natural images that still contain a high degree of visual\nsimilarity. No work yet exists that examines few-shot learning across different\nimaging methods seen in real world scenarios, such as aerial and medical\nimaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot\nLearning (BSCD-FSL) benchmark, consisting of image data from a diverse\nassortment of image acquisition methods. This includes natural images, such as\ncrop disease images, but additionally those that present with an increasing\ndissimilarity to natural images, such as satellite images, dermatology images,\nand radiology images. Extensive experiments on the proposed benchmark are\nperformed to evaluate state-of-art meta-learning approaches, transfer learning\napproaches, and newer methods for cross-domain few-shot learning. The results\ndemonstrate that state-of-art meta-learning methods are surprisingly\noutperformed by earlier meta-learning approaches, and all meta-learning methods\nunderperform in relation to simple fine-tuning by 12.8% average accuracy.\nPerformance gains previously observed with methods specialized for cross-domain\nfew-shot learning vanish in this more challenging benchmark. Finally, accuracy\nof all methods tend to correlate with dataset similarity to natural images,\nverifying the value of the benchmark to better represent the diversity of data\nseen in practice and guiding future research.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:29:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:30:22 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 21:33:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Guo", "Yunhui", ""], ["Codella", "Noel C.", ""], ["Karlinsky", "Leonid", ""], ["Codella", "James V.", ""], ["Smith", "John R.", ""], ["Saenko", "Kate", ""], ["Rosing", "Tajana", ""], ["Feris", "Rogerio", ""]]}, {"id": "1912.07209", "submitter": "Mohammad Majdi", "authors": "Mohammad S Majdi (1), Mahesh B Keerthivasan (2 and 5), Brian K Rutt\n  (3), Natalie M Zahr (4), Jeffrey J Rodriguez (1), Manojkumar Saranathan (1\n  and 2) ((1) Department of Electrical and Computer Engineering, University of\n  Arizona, (2) Department of Medical Imaging, University of Arizona, (3)\n  Department of Radiology, Stanford University, (4) Department of Psychiatry\n  and Behavioral Sciences, Stanford University, (5) Siemens Healthcare USA)", "title": "Automated Thalamic Nuclei Segmentation Using Multi-Planar Cascaded\n  Convolutional Neural Networks", "comments": "Submitted to Magnetic Resonance Imaging. 34 pages, 6 figures , 2\n  tables, 1 supporting figures, 2 supporting tables. Magnetic Resonance Imaging\n  (2020)", "journal-ref": null, "doi": "10.1016/j.mri.2020.08.005", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cascaded multi-planar scheme with a modified residual U-Net architecture\nwas used to segment thalamic nuclei on conventional and white-matter-nulled\n(WMn) magnetization prepared rapid gradient echo (MPRAGE) data. A single\nnetwork was optimized to work with images from healthy controls and patients\nwith multiple sclerosis (MS) and essential tremor (ET), acquired at both 3T and\n7T field strengths. Dice similarity coefficient and volume similarity index\n(VSI) were used to evaluate performance. Clinical utility was demonstrated by\napplying this method to study the effect of MS on thalamic nuclei atrophy.\nSegmentation of each thalamus into twelve nuclei was achieved in under a\nminute. For 7T WMn-MPRAGE, the proposed method outperforms current\nstate-of-the-art on patients with ET with statistically significant\nimprovements in Dice for five nuclei (increase in the range of 0.05-0.18) and\nVSI for four nuclei (increase in the range of 0.05-0.19), while performing\ncomparably for healthy and MS subjects. Dice and VSI achieved using 7T\nWMn-MPRAGE data are comparable to those using 3T WMn-MPRAGE data. For\nconventional MPRAGE, the proposed method shows a statistically significant Dice\nimprovement in the range of 0.14-0.63 over FreeSurfer for all nuclei and\ndisease types. Effect of noise on network performance shows robustness to\nimages with SNR as low as half the baseline SNR. Atrophy of four thalamic\nnuclei and whole thalamus was observed for MS patients compared to healthy\ncontrol subjects, after controlling for the effect of parallel imaging,\nintracranial volume, gender, and age (p<0.004). The proposed segmentation\nmethod is fast, accurate, performs well across disease types and field\nstrengths, and shows great potential for improving our understanding of\nthalamic nuclei involvement in neurological diseases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:57:33 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 21:06:00 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Majdi", "Mohammad S", "", "2 and 5"], ["Keerthivasan", "Mahesh B", "", "2 and 5"], ["Rutt", "Brian K", "", "1\n  and 2"], ["Zahr", "Natalie M", "", "1\n  and 2"], ["Rodriguez", "Jeffrey J", "", "1\n  and 2"], ["Saranathan", "Manojkumar", "", "1\n  and 2"]]}, {"id": "1912.07213", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Jihyong Oh, Munchurl Kim", "title": "FISR: Deep Joint Frame Interpolation and Super-Resolution with A\n  Multi-scale Temporal Loss", "comments": "The first two authors contributed equally to this work. Accepted at\n  AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) has been widely used to convert low-resolution legacy\nvideos to high-resolution (HR) ones, to suit the increasing resolution of\ndisplays (e.g. UHD TVs). However, it becomes easier for humans to notice motion\nartifacts (e.g. motion judder) in HR videos being rendered on larger-sized\ndisplay devices. Thus, broadcasting standards support higher frame rates for\nUHD (Ultra High Definition) videos (4K@60 fps, 8K@120 fps), meaning that\napplying SR only is insufficient to produce genuine high quality videos. Hence,\nto up-convert legacy videos for realistic applications, not only SR but also\nvideo frame interpolation (VFI) is necessitated. In this paper, we first\npropose a joint VFI-SR framework for up-scaling the spatio-temporal resolution\nof videos from 2K 30 fps to 4K 60 fps. For this, we propose a novel training\nscheme with a multi-scale temporal loss that imposes temporal regularization on\nthe input video sequence, which can be applied to any general video-related\ntask. The proposed structure is analyzed in depth with extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 06:11:44 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kim", "Soo Ye", ""], ["Oh", "Jihyong", ""], ["Kim", "Munchurl", ""]]}, {"id": "1912.07224", "submitter": "Chen Yang", "authors": "Xiaoqing Guo, Chen Yang, Pak Lun Lam, Peter Y.M. Woo, and Yixuan Yuan", "title": "Domain Knowledge Based Brain Tumor Segmentation and Overall Survival\n  Prediction", "comments": "11 pages, 5 figures, BrainLes 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically segmenting sub-regions of gliomas (necrosis, edema and\nenhancing tumor) and accurately predicting overall survival (OS) time from\nmultimodal MRI sequences have important clinical significance in diagnosis,\nprognosis and treatment of gliomas. However, due to the high degree variations\nof heterogeneous appearance and individual physical state, the segmentation of\nsub-regions and OS prediction are very challenging. To deal with these\nchallenges, we utilize a 3D dilated multi-fiber network (DMFNet) with weighted\ndice loss for brain tumor segmentation, which incorporates prior volume\nstatistic knowledge and obtains a balance between small and large objects in\nMRI scans. For OS prediction, we propose a DenseNet based 3D neural network\nwith position encoding convolutional layer (PECL) to extract meaningful\nfeatures from T1 contrast MRI, T2 MRI and previously segmented subregions. Both\nlabeled data and unlabeled data are utilized to prevent over-fitting for\nsemi-supervised learning. Those learned deep features along with handcrafted\nfeatures (such as ages, volume of tumor) and position encoding segmentation\nfeatures are fed to a Gradient Boosting Decision Tree (GBDT) to predict a\nspecific OS day\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 07:21:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Guo", "Xiaoqing", ""], ["Yang", "Chen", ""], ["Lam", "Pak Lun", ""], ["Woo", "Peter Y. M.", ""], ["Yuan", "Yixuan", ""]]}, {"id": "1912.07243", "submitter": "Hao Liu", "authors": "Congcong Zhu, Hao Liu, Zhenhua Yu, Xuehong Sun", "title": "Towards Omni-Supervised Face Alignment for Large Scale Unlabeled Videos", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spatial-temporal relational reasoning networks\n(STRRN) approach to investigate the problem of omni-supervised face alignment\nin videos. Unlike existing fully supervised methods which rely on numerous\nannotations by hand, our learner exploits large scale unlabeled videos plus\navailable labeled data to generate auxiliary plausible training annotations.\nMotivated by the fact that neighbouring facial landmarks are usually correlated\nand coherent across consecutive frames, our approach automatically reasons\nabout discriminative spatial-temporal relationships among landmarks for stable\nface tracking. Specifically, we carefully develop an interpretable and\nefficient network module, which disentangles facial geometry relationship for\nevery static frame and simultaneously enforces the bi-directional\ncycle-consistency across adjacent frames, thus allowing the modeling of\nintrinsic spatial-temporal relations from raw face sequences. Extensive\nexperimental results demonstrate that our approach surpasses the performance of\nmost fully supervised state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:34:10 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhu", "Congcong", ""], ["Liu", "Hao", ""], ["Yu", "Zhenhua", ""], ["Sun", "Xuehong", ""]]}, {"id": "1912.07249", "submitter": "Philippe Weinzaepfel", "authors": "Philippe Weinzaepfel, Gr\\'egory Rogez", "title": "Mimetics: Towards Understanding Human Actions Out of Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for video action recognition have reached outstanding\nperformances on existing benchmarks. However, they tend to leverage context\nsuch as scenes or objects instead of focusing on understanding the human action\nitself. For instance, a tennis field leads to the prediction playing tennis\nirrespectively of the actions performed in the video. In contrast, humans have\na more complete understanding of actions and can recognize them without\ncontext. The best example of out-of-context actions are mimes, that people can\ntypically recognize despite missing relevant objects and scenes. In this paper,\nwe propose to benchmark action recognition methods in such absence of context\nand introduce a novel dataset, Mimetics, consisting of mimed actions for a\nsubset of 50 classes from the Kinetics benchmark. Our experiments show that (a)\nstate-of-the-art 3D convolutional neural networks obtain disappointing results\non such videos, highlighting the lack of true understanding of the human\nactions and (b) models leveraging body language via human pose are less prone\nto context biases. In particular, we show that applying a shallow neural\nnetwork with a single temporal convolution over body pose features transferred\nto the action recognition problem performs surprisingly well compared to 3D\naction recognition methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:49:39 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 15:39:56 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 14:20:11 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Weinzaepfel", "Philippe", ""], ["Rogez", "Gr\u00e9gory", ""]]}, {"id": "1912.07259", "submitter": "Xu Liu", "authors": "Xu Liu, Licheng Jiao, Fang Liu", "title": "PolSF: PolSAR image dataset on San Francisco", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric SAR data has the characteristics of all-weather, all-time and so\non, which is widely used in many fields. However, the data of annotation is\nrelatively small, which is not conducive to our research. In this paper, we\nhave collected five open polarimetric SAR images, which are images of the San\nFrancisco area. These five images come from different satellites at different\ntimes, which has great scientific research value. We annotate the collected\nimages at the pixel level for image classification and segmentation. For the\nconvenience of researchers, the annotated data is open source\nhttps://github.com/liuxuvip/PolSF.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 09:33:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Xu", ""], ["Jiao", "Licheng", ""], ["Liu", "Fang", ""]]}, {"id": "1912.07266", "submitter": "Syed Tahseen Raza Rizvi", "authors": "Syed Tahseen Raza Rizvi, Andreas Dengel, Sheraz Ahmed", "title": "A Hybrid Approach and Unified Framework for Bibliographic Reference\n  Extraction", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3042455", "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publications are an integral part in a scientific community. Bibliographic\nreference extraction from scientific publication is a challenging task due to\ndiversity in referencing styles and document layout. Existing methods perform\nsufficiently on one dataset however, applying these solutions to a different\ndataset proves to be challenging. Therefore, a generic solution was anticipated\nwhich could overcome the limitations of the previous approaches. The\ncontribution of this paper is three-fold. First, it presents a novel approach\ncalled DeepBiRD which is inspired by human visual perception and exploits\nlayout features to identify individual references in a scientific publication.\nSecond, we release a large dataset for image-based reference detection with\n2401 scans containing 38863 references, all manually annotated for individual\nreference. Third, we present a unified and highly configurable end-to-end\nautomatic bibliographic reference extraction framework called BRExSys which\nemploys DeepBiRD along with state-of-the-art text-based models to detect and\nvisualize references from a bibliographic document. Our proposed approach\npre-processes the images in which a hybrid representation is obtained by\nprocessing the given image using different computer vision techniques. Then, it\nperforms layout driven reference detection using Mask R-CNN on a given\nscientific publication. DeepBiRD was evaluated on two different datasets to\ndemonstrate the generalization of this approach. The proposed system achieved\nan AP50 of 98.56% on our dataset. DeepBiRD significantly outperformed the\ncurrent state-of-the-art approach on their dataset. Therefore, suggesting that\nDeepBiRD is significantly superior in performance, generalized, and independent\nof any domain or referencing style.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 09:47:50 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:28:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rizvi", "Syed Tahseen Raza", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1912.07308", "submitter": "Sijia Wen", "authors": "Sijia Wen, Yinqiang Zheng and Feng Lu", "title": "A Sparse Representation Based Joint Demosaicing Method for Single-Chip\n  Polarized Color Sensor", "comments": "10 pages, 7 figures", "journal-ref": "IEEE Transactions on Image Processing 2021", "doi": "10.1109/TIP.2021.3069190", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of the single-chip polarized color sensor now allows for\nsimultaneously capturing chromatic and polarimetric information of the scene on\na monochromatic image plane. However, unlike the usual camera with an embedded\ndemosaicing method, the latest polarized color camera is not delivered with an\nin-built demosaicing tool. For demosaicing, the users have to down-sample the\ncaptured images or to use traditional interpolation techniques. Neither of them\ncan perform well since the polarization and color are interdependent.\nTherefore, joint chromatic and polarimetric demosaicing is the key to obtaining\nhigh-quality polarized color images. In this paper, we propose a joint\nchromatic and polarimetric demosaicing model to address this challenging\nproblem. Instead of mechanically demosaicing for the multi-channel polarized\ncolor image, we further present a sparse representation-based optimization\nstrategy that utilizes chromatic information and polarimetric information to\njointly optimize the model. To avoid the interaction between color and\npolarization during demosaicing, we separately construct the corresponding\ndictionaries. We also build an optical data acquisition system to collect a\ndataset, which contains various sources of polarization, such as illumination,\nreflectance and birefringence. Results of both qualitative and quantitative\nexperiments have shown that our method is capable of faithfully recovering full\nRGB information of four polarization angles for each pixel from a single mosaic\ninput image. Moreover, the proposed method can perform well not only on the\nsynthetic data but the real captured data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:01:24 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 02:33:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wen", "Sijia", ""], ["Zheng", "Yinqiang", ""], ["Lu", "Feng", ""]]}, {"id": "1912.07329", "submitter": "Karan Jakhar", "authors": "Karan Jakhar, Avneet Kaur, Dr. Meenu Gupta", "title": "Pneumothorax Segmentation: Deep Learning Image Segmentation to predict\n  Pneumothorax", "comments": "Will be updated later on. Somethings need to be updated and corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has shown promising results in medical image processing.\nPneumothorax is a deadly condition and if not diagnosed and treated at time\nthen it causes death. It can be diagnosed with chest X-ray images. We need an\nexpert and experienced radiologist to predict whether a person is suffering\nfrom pneumothorax or not by looking at the chest X-ray images. Everyone does\nnot have access to such a facility. Moreover, in some cases, we need quick\ndiagnoses. So we propose an image segmentation model to predict and give the\noutput a mask that will assist the doctor in taking this crucial decision. Deep\nLearning has proved their worth in many areas and outperformed man\nstate-of-the-art models. We want to use the power of these deep learning model\nto solve this problem. We have used U-net [13] architecture with ResNet [17] as\na backbone and achieved promising results. U-net [13] performs very well in\nmedical image processing and semantic segmentation. Our problem falls in the\nsemantic segmentation category.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:00:32 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 11:01:06 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 14:15:23 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 11:42:47 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jakhar", "Karan", ""], ["Kaur", "Avneet", ""], ["Gupta", "Dr. Meenu", ""]]}, {"id": "1912.07333", "submitter": "Max Schwarz", "authors": "Catherine Capellen, Max Schwarz, Sven Behnke", "title": "ConvPoseCNN: Dense Convolutional 6D Object Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D object pose estimation is a prerequisite for many applications. In recent\nyears, monocular pose estimation has attracted much research interest because\nit does not need depth measurements. In this work, we introduce ConvPoseCNN, a\nfully convolutional architecture that avoids cutting out individual objects.\nInstead we propose pixel-wise, dense prediction of both translation and\norientation components of the object pose, where the dense orientation is\nrepresented in Quaternion form. We present different approaches for aggregation\nof the dense orientation predictions, including averaging and clustering\nschemes. We evaluate ConvPoseCNN on the challenging YCB-Video Dataset, where we\nshow that the approach has far fewer parameters and trains faster than\ncomparable methods without sacrificing accuracy. Furthermore, our results\nindicate that the dense orientation prediction implicitly learns to attend to\ntrustworthy, occlusion-free, and feature-rich object regions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:12:23 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Capellen", "Catherine", ""], ["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "1912.07361", "submitter": "Zhyar Rostam Mr.", "authors": "Zhyar Rzgar K. Rostam, Sozan Abdullah Mahmood", "title": "Classification of Brainwave Signals Based on Hybrid Deep Learning and an\n  Evolutionary Algorithm", "comments": "10 pages, 6 tables, and 5 figures", "journal-ref": "JZS (2019) 21_2 35_44", "doi": "10.17656/jzs.10755", "report-no": "4_JZS_Comp_19", "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brainwave signals are read through Electroencephalogram (EEG) devices. These\nsignals are generated from an active brain based on brain activities and\nthoughts. The classification of brainwave signals is a challenging task due to\nits non-stationary nature. To address the issue, this paper proposes a\nConvolutional Neural Network (CNN) model to classify brainwave signals. In\norder to evaluate the performance of the proposed model a dataset is developed\nby recording brainwave signals for two conditions, which are visible and\ninvisible. In the visible mode, the human subjects focus on the color and shape\npresented. Meanwhile, in the invisible mode, the subjects think about specific\ncolors or shapes with closed eyes. A comparison has been provided between the\noriginal CNN and the proposed CNN architecture on the same dataset. The results\nshow that the proposed CNN model achieves higher classification accuracy as\ncompared to the standard CNN. The best accuracy rate achieved when the proposed\nCNN is applied on the visible color mode is 92%. In the future, improvements on\nthe proposed CNN will be able to classify raw EEG signals in an efficient way.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:03:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rostam", "Zhyar Rzgar K.", ""], ["Mahmood", "Sozan Abdullah", ""]]}, {"id": "1912.07372", "submitter": "Michael Niemeyer", "authors": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger", "title": "Differentiable Volumetric Rendering: Learning Implicit 3D\n  Representations without 3D Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based 3D reconstruction methods have shown impressive results.\nHowever, most methods require 3D supervision which is often hard to obtain for\nreal-world datasets. Recently, several works have proposed differentiable\nrendering techniques to train reconstruction models from RGB images.\nUnfortunately, these approaches are currently restricted to voxel- and\nmesh-based representations, suffering from discretization or low resolution. In\nthis work, we propose a differentiable rendering formulation for implicit shape\nand texture representations. Implicit representations have recently gained\npopularity as they represent shape and texture continuously. Our key insight is\nthat depth gradients can be derived analytically using the concept of implicit\ndifferentiation. This allows us to learn implicit shape and texture\nrepresentations directly from RGB images. We experimentally show that our\nsingle-view reconstructions rival those learned with full 3D supervision.\nMoreover, we find that our method can be used for multi-view 3D reconstruction,\ndirectly resulting in watertight meshes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:59:33 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 14:10:27 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Niemeyer", "Michael", ""], ["Mescheder", "Lars", ""], ["Oechsle", "Michael", ""], ["Geiger", "Andreas", ""]]}, {"id": "1912.07394", "submitter": "Giulio Gambardella", "authors": "Giulio Gambardella, Johannes Kappauf, Michaela Blott, Christoph\n  Doehring, Martin Kumm, Peter Zipf and Kees Vissers", "title": "Efficient Error-Tolerant Quantized Neural Network Accelerators", "comments": "6 pages, 5 figures", "journal-ref": "2019 IEEE International Symposium on Defect and Fault Tolerance in\n  VLSI and Nanotechnology Systems (DFT)", "doi": "10.1109/DFT.2019.8875314", "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks are currently one of the most widely deployed machine\nlearning algorithms. In particular, Convolutional Neural Networks (CNNs), are\ngaining popularity and are evaluated for deployment in safety critical\napplications such as self driving vehicles. Modern CNNs feature enormous memory\nbandwidth and high computational needs, challenging existing hardware platforms\nto meet throughput, latency and power requirements. Functional safety and error\ntolerance need to be considered as additional requirement in safety critical\nsystems. In general, fault tolerant operation can be achieved by adding\nredundancy to the system, which is further exacerbating the computational\ndemands. Furthermore, the question arises whether pruning and quantization\nmethods for performance scaling turn out to be counterproductive with regards\nto fail safety requirements. In this work we present a methodology to evaluate\nthe impact of permanent faults affecting Quantized Neural Networks (QNNs) and\nhow to effectively decrease their effects in hardware accelerators. We use\nFPGA-based hardware accelerated error injection, in order to enable the fast\nevaluation. A detailed analysis is presented showing that QNNs containing\nconvolutional layers are by far not as robust to faults as commonly believed\nand can lead to accuracy drops of up to 10%. To circumvent that, we propose two\ndifferent methods to increase their robustness: 1) selective channel\nreplication which adds significantly less redundancy than used by the common\ntriple modular redundancy and 2) a fault-aware scheduling of processing\nelements for folded implementations\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:25:13 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Gambardella", "Giulio", ""], ["Kappauf", "Johannes", ""], ["Blott", "Michaela", ""], ["Doehring", "Christoph", ""], ["Kumm", "Martin", ""], ["Zipf", "Peter", ""], ["Vissers", "Kees", ""]]}, {"id": "1912.07398", "submitter": "Jacqueline Cavazos", "authors": "Jacqueline G. Cavazos, P. Jonathon Phillips, Carlos D. Castillo, Alice\n  J. O'Toole", "title": "Accuracy comparison across face recognition algorithms: Where are we on\n  measuring race bias?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous generations of face recognition algorithms differ in accuracy for\nimages of different races (race bias). Here, we present the possible underlying\nfactors (data-driven and scenario modeling) and methodological considerations\nfor assessing race bias in algorithms. We discuss data driven factors (e.g.,\nimage quality, image population statistics, and algorithm architecture), and\nscenario modeling factors that consider the role of the \"user\" of the algorithm\n(e.g., threshold decisions and demographic constraints). To illustrate how\nthese issues apply, we present data from four face recognition algorithms (a\nprevious-generation algorithm and three deep convolutional neural networks,\nDCNNs) for East Asian and Caucasian faces. First, dataset difficulty affected\nboth overall recognition accuracy and race bias, such that race bias increased\nwith item difficulty. Second, for all four algorithms, the degree of bias\nvaried depending on the identification decision threshold. To achieve equal\nfalse accept rates (FARs), East Asian faces required higher identification\nthresholds than Caucasian faces, for all algorithms. Third, demographic\nconstraints on the formulation of the distributions used in the test, impacted\nestimates of algorithm accuracy. We conclude that race bias needs to be\nmeasured for individual applications and we provide a checklist for measuring\nthis bias in face recognition algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:27:10 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 15:50:26 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Cavazos", "Jacqueline G.", ""], ["Phillips", "P. Jonathon", ""], ["Castillo", "Carlos D.", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "1912.07414", "submitter": "Roei Herzig", "authors": "Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, Amir\n  Globerson", "title": "Learning Canonical Representations for Scene Graph to Image Generation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic images of complex visual scenes becomes challenging when\none wishes to control the structure of the generated images. Previous\napproaches showed that scenes with few entities can be controlled using scene\ngraphs, but this approach struggles as the complexity of the graph (the number\nof objects and edges) increases. In this work, we show that one limitation of\ncurrent methods is their inability to capture semantic equivalence in graphs.\nWe present a novel model that addresses these issues by learning canonical\ngraph representations from the data, resulting in improved image generation for\ncomplex visual scenes. Our model demonstrates improved empirical performance on\nlarge scene graphs, robustness to noise in the input scene graph, and\ngeneralization on semantically equivalent graphs. Finally, we show improved\nperformance of the model on three different benchmarks: Visual Genome, COCO,\nand CLEVR.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:39:45 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:46:32 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 10:54:01 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 11:36:06 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2020 12:29:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Herzig", "Roei", ""], ["Bar", "Amir", ""], ["Xu", "Huijuan", ""], ["Chechik", "Gal", ""], ["Darrell", "Trevor", ""], ["Globerson", "Amir", ""]]}, {"id": "1912.07420", "submitter": "Robin Chan", "authors": "Robin Chan, Matthias Rottmann, Fabian H\\\"uger, Peter Schlicht, Hanno\n  Gottschalk", "title": "MetaFusion: Controlled False-Negative Reduction of Minority Classes in\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic segmentation datasets, classes of high importance are oftentimes\nunderrepresented, e.g., humans in street scenes. Neural networks are usually\ntrained to reduce the overall number of errors, attaching identical loss to\nerrors of all kinds. However, this is not necessarily aligned with human\nintuition. For instance, an overlooked pedestrian seems more severe than an\nincorrectly detected one. One possible remedy is to deploy different decision\nrules by introducing class priors which assigns larger weight to\nunderrepresented classes. While reducing the false-negatives of the\nunderrepresented class, at the same time this leads to a considerable increase\nof false-positive indications. In this work, we combine decision rules with\nmethods for false-positive detection. We therefore fuse false-negative\ndetection with uncertainty based false-positive meta classification. We present\nproof-of-concept results for CIFAR-10, and prove the efficiency of our method\nfor the semantic segmentation of street scenes on the Cityscapes dataset based\non predicted instances of the 'human' class. In the latter we employ an\nadvanced false-positive detection method using uncertainty measures aggregated\nover instances. We thereby achieve improved trade-offs between false-negative\nand false-positive samples of the underrepresented classes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:44:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chan", "Robin", ""], ["Rottmann", "Matthias", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1912.07447", "submitter": "Nian Xue", "authors": "Zhen Li, Hanyang Shao, Nian Xue, Liang Niu and LiangLiang Cao", "title": "Progressive Learning Algorithm for Efficient Person Re-Identification", "comments": "ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of Person Re-Identification (ReID)for\nlarge-scale applications. Recent research efforts have been devoted to building\ncomplicated part models, which introduce considerably high computational cost\nand memory consumption, inhibiting its practicability in large-scale\napplications. This paper aims to develop a novel learning strategy to find\nefficient feature embeddings while maintaining the balance of accuracy and\nmodel complexity. More specifically, we find by enhancing the classical triplet\nloss together with cross-entropy loss, our method can explore the hard examples\nand build a discriminant feature embedding yet compact enough for large-scale\napplications. Our method is carried out progressively using Bayesian\noptimization, and we call it the Progressive Learning Algorithm (PLA).\nExtensive experiments on three large-scale datasets show that our PLA is\ncomparable or better than the-state-of-the-arts. Especially, on the challenging\nMarket-1501 dataset, we achieve Rank-1=94.7\\%/mAP=89.4\\% while saving at least\n30\\% parameters than strong part models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:32:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 22:08:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Li", "Zhen", ""], ["Shao", "Hanyang", ""], ["Xue", "Nian", ""], ["Niu", "Liang", ""], ["Cao", "LiangLiang", ""]]}, {"id": "1912.07478", "submitter": "Dawei Zhu", "authors": "Dawei Zhu, Aditya Mogadala, Dietrich Klakow", "title": "Image Manipulation with Natural Language using Two-sidedAttentive\n  Conditional Generative Adversarial Network", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Altering the content of an image with photo editing tools is a tedious task\nfor an inexperienced user. Especially, when modifying the visual attributes of\na specific object in an image without affecting other constituents such as\nbackground etc. To simplify the process of image manipulation and to provide\nmore control to users, it is better to utilize a simpler interface like natural\nlanguage. Therefore, in this paper, we address the challenge of manipulating\nimages using natural language description. We propose the Two-sidEd Attentive\nconditional Generative Adversarial Network (TEA-cGAN) to generate semantically\nmanipulated images while preserving other contents such as background intact.\nTEA-cGAN uses fine-grained attention both in the generator and discriminator of\nGenerative Adversarial Network (GAN) based framework at different scales.\nExperimental results show that TEA-cGAN which generates 128x128 and 256x256\nresolution images outperforms existing methods on CUB and Oxford-102 datasets\nboth quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:21:13 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhu", "Dawei", ""], ["Mogadala", "Aditya", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1912.07515", "submitter": "Guillem Bras\\'o", "authors": "Guillem Bras\\'o and Laura Leal-Taix\\'e", "title": "Learning a Neural Solver for Multiple Object Tracking", "comments": "Accepted to CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within\nthe tracking-by-detection paradigm. However, they also introduce a major\nchallenge for learning methods, as defining a model that can operate on such\n\\textit{structured domain} is not trivial. As a consequence, most\nlearning-based work has been devoted to learning better features for MOT, and\nthen using these with well-established optimization frameworks. In this work,\nwe exploit the classical network flow formulation of MOT to define a fully\ndifferentiable framework based on Message Passing Networks (MPNs). By operating\ndirectly on the graph domain, our method can reason globally over an entire set\nof detections and predict final solutions. Hence, we show that learning in MOT\ndoes not need to be restricted to feature extraction, but it can also be\napplied to the data association step. We show a significant improvement in both\nMOTA and IDF1 on three publicly available benchmarks. Our code is available at\nhttps://bit.ly/motsolv .\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:27:55 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 10:08:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bras\u00f3", "Guillem", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "1912.07517", "submitter": "Hao Du", "authors": "Hao Du, Jiashi Feng, Mengling Feng", "title": "Zoom in to where it matters: a hierarchical graph based model for\n  mammogram analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice, human radiologists actually review medical images with\nhigh resolution monitors and zoom into region of interests (ROIs) for a\nclose-up examination. Inspired by this observation, we propose a hierarchical\ngraph neural network to detect abnormal lesions from medical images by\nautomatically zooming into ROIs. We focus on mammogram analysis for breast\ncancer diagnosis for this study. Our proposed network consist of two graph\nattention networks performing two tasks: (1) node classification to predict\nwhether to zoom into next level; (2) graph classification to classify whether a\nmammogram is normal/benign or malignant. The model is trained and evaluated on\nINbreast dataset and we obtain comparable AUC with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:28:17 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Du", "Hao", ""], ["Feng", "Jiashi", ""], ["Feng", "Mengling", ""]]}, {"id": "1912.07538", "submitter": "Rakshith Shetty", "authors": "Vedika Agarwal and Rakshith Shetty and Mario Fritz", "title": "Towards Causal VQA: Revealing and Reducing Spurious Correlations by\n  Invariant and Covariant Semantic Editing", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant success in Visual Question Answering (VQA), VQA models\nhave been shown to be notoriously brittle to linguistic variations in the\nquestions. Due to deficiencies in models and datasets, today's models often\nrely on correlations rather than predictions that are causal w.r.t. data. In\nthis paper, we propose a novel way to analyze and measure the robustness of the\nstate of the art models w.r.t semantic visual variations as well as propose\nways to make models more robust against spurious correlations. Our method\nperforms automated semantic image manipulations and tests for consistency in\nmodel predictions to quantify the model robustness as well as generate\nsynthetic data to counter these problems. We perform our analysis on three\ndiverse, state of the art VQA models and diverse question types with a\nparticular focus on challenging counting questions. In addition, we show that\nmodels can be made significantly more robust against inconsistent predictions\nusing our edited data. Finally, we show that results also translate to\nreal-world error cases of state of the art models, which results in improved\noverall performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:45:01 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 21:22:34 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 08:58:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Agarwal", "Vedika", ""], ["Shetty", "Rakshith", ""], ["Fritz", "Mario", ""]]}, {"id": "1912.07648", "submitter": "Angelica I. Aviles-Rivero", "authors": "Jiulong Liu, Angelica I. Aviles-Rivero, Hui Ji and Carola-Bibiane\n  Sch\\\"onlieb", "title": "Rethinking Medical Image Reconstruction via Shape Prior, Going Deeper\n  and Faster: Deep Joint Indirect Registration and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect image registration is a promising technique to improve image\nreconstruction quality by providing a shape prior for the reconstruction task.\nIn this paper, we propose a novel hybrid method that seeks to reconstruct high\nquality images from few measurements whilst requiring low computational cost.\nWith this purpose, our framework intertwines indirect registration and\nreconstruction tasks is a single functional. It is based on two major\nnovelties. Firstly, we introduce a model based on deep nets to solve the\nindirect registration problem, in which the inversion and registration mappings\nare recurrently connected through a fixed-point interaction based sparse\noptimisation. Secondly, we introduce specific inversion blocks, that use the\nexplicit physical forward operator, to map the acquired measurements to the\nimage reconstruction. We also introduce registration blocks based deep nets to\npredict the registration parameters and warp transformation accurately and\nefficiently. We demonstrate, through extensive numerical and visual\nexperiments, that our framework outperforms significantly classic\nreconstruction schemes and other bi-task method; this in terms of both image\nquality and computational time. Finally, we show generalisation capabilities of\nour approach by demonstrating their performance on fast Magnetic Resonance\nImaging (MRI), sparse view computed tomography (CT) and low dose CT with\nmeasurements much below the Nyquist limit.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:28:52 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Liu", "Jiulong", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Ji", "Hui", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1912.07651", "submitter": "Arash Vahdat", "authors": "Arash Vahdat, Arun Mallya, Ming-Yu Liu, Jan Kautz", "title": "UNAS: Differentiable Architecture Search Meets Reinforcement Learning", "comments": "Accepted to CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) aims to discover network architectures with\ndesired properties such as high accuracy or low latency. Recently,\ndifferentiable NAS (DNAS) has demonstrated promising results while maintaining\na search cost orders of magnitude lower than reinforcement learning (RL) based\nNAS. However, DNAS models can only optimize differentiable loss functions in\nsearch, and they require an accurate differentiable approximation of\nnon-differentiable criteria. In this work, we present UNAS, a unified framework\nfor NAS, that encapsulates recent DNAS and RL-based approaches under one\nframework. Our framework brings the best of both worlds, and it enables us to\nsearch for architectures with both differentiable and non-differentiable\ncriteria in one unified framework while maintaining a low search cost. Further,\nwe introduce a new objective function for search based on the generalization\ngap that prevents the selection of architectures prone to overfitting. We\npresent extensive experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets\nand we perform search in two fundamentally different search spaces. We show\nthat UNAS obtains the state-of-the-art average accuracy on all three datasets\nwhen compared to the architectures searched in the DARTS space. Moreover, we\nshow that UNAS can find an efficient and accurate architecture in the\nProxylessNAS search space, that outperforms existing MobileNetV2 based\narchitectures. The source code is available at https://github.com/NVlabs/unas .\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:31:39 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 21:48:42 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Vahdat", "Arash", ""], ["Mallya", "Arun", ""], ["Liu", "Ming-Yu", ""], ["Kautz", "Jan", ""]]}, {"id": "1912.07669", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta\n  Ellermann, K\\^amil U\\u{g}urbil, Mehmet Ak\\c{c}akaya", "title": "Self-Supervised Learning of Physics-Guided Reconstruction Neural\n  Networks without Fully-Sampled Reference Data", "comments": "This work is an extension of our previous work arXiv:1910.09116", "journal-ref": "Magnetic Resonance in Medicine, 2020", "doi": "10.1002/mrm.28378", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a strategy for training a physics-guided MRI\nreconstruction neural network without a database of fully-sampled datasets.\nTheory and Methods: Self-supervised learning via data under-sampling (SSDU) for\nphysics-guided deep learning (DL) reconstruction partitions available\nmeasurements into two disjoint sets, one of which is used in the data\nconsistency units in the unrolled network and the other is used to define the\nloss for training. The proposed training without fully-sampled data is compared\nto fully-supervised training with ground-truth data, as well as conventional\ncompressed sensing and parallel imaging methods using the publicly available\nfastMRI knee database. The same physics-guided neural network is used for both\nproposed SSDU and supervised training. The SSDU training is also applied to\nprospectively 2-fold accelerated high-resolution brain datasets at different\nacceleration rates, and compared to parallel imaging. Results: Results on five\ndifferent knee sequences at acceleration rate of 4 shows that proposed\nself-supervised approach performs closely with supervised learning, while\nsignificantly outperforming conventional compressed sensing and parallel\nimaging, as characterized by quantitative metrics and a clinical reader study.\nThe results on prospectively sub-sampled brain datasets, where supervised\nlearning cannot be employed due to lack of ground-truth reference, show that\nthe proposed self-supervised approach successfully perform reconstruction at\nhigh acceleration rates (4, 6 and 8). Image readings indicate improved visual\nreconstruction quality with the proposed approach compared to parallel imaging\nat acquisition acceleration. Conclusion: The proposed SSDU approach allows\ntraining of physics-guided DL-MRI reconstruction without fully-sampled data,\nwhile achieving comparable results with supervised DL-MRI trained on\nfully-sampled data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:04:02 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 19:13:15 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Moeller", "Steen", ""], ["Ellermann", "Jutta", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "1912.07726", "submitter": "Kaiyu Yang", "authors": "Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, Olga Russakovsky", "title": "Towards Fairer Datasets: Filtering and Balancing the Distribution of the\n  People Subtree in the ImageNet Hierarchy", "comments": "Accepted to FAT* 2020", "journal-ref": null, "doi": "10.1145/3351095.3375709", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision technology is being used by many but remains representative\nof only a few. People have reported misbehavior of computer vision models,\nincluding offensive prediction results and lower performance for\nunderrepresented groups. Current computer vision models are typically developed\nusing datasets consisting of manually annotated images or videos; the data and\nlabel distributions in these datasets are critical to the models' behavior. In\nthis paper, we examine ImageNet, a large-scale ontology of images that has\nspurred the development of many modern computer vision methods. We consider\nthree key factors within the \"person\" subtree of ImageNet that may lead to\nproblematic behavior in downstream computer vision technology: (1) the stagnant\nconcept vocabulary of WordNet, (2) the attempt at exhaustive illustration of\nall categories with images, and (3) the inequality of representation in the\nimages within concepts. We seek to illuminate the root causes of these concerns\nand take the first steps to mitigate them constructively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:03:05 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Yang", "Kaiyu", ""], ["Qinami", "Klint", ""], ["Fei-Fei", "Li", ""], ["Deng", "Jia", ""], ["Russakovsky", "Olga", ""]]}, {"id": "1912.07735", "submitter": "Kirk Scheper", "authors": "Kirk Y.W. Scheper, Guido C.H.E. de Croon", "title": "Evolution of Robust High Speed Optical-Flow-Based Landing for Autonomous\n  MAVs", "comments": "This is an accepted manuscript preprint of published work available\n  at https://doi.org/10.1016/j.robot.2019.103380. Please cite with Kirk Y.W\n  Scheper and Guido C.H.E. de Croon, Evolution of robust high speed\n  optical-flow-based landing for autonomous MAVs, Robotics and Autonomous\n  Systems 124, 2020", "journal-ref": "Robotics and Autonomous Systems, Volume 124, 2020, 103380, ISSN\n  0921-8890", "doi": "10.1016/j.robot.2019.103380", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic optimization of robotic behavior has been the long-standing goal of\nEvolutionary Robotics. Allowing the problem at hand to be solved by automation\noften leads to novel approaches and new insights. A common problem encountered\nwith this approach is that when this optimization occurs in a simulated\nenvironment, the optimized policies are subject to the reality gap when\nimplemented in the real world. This often results in sub-optimal behavior, if\nit works at all. This paper investigates the automatic optimization of\nneurocontrollers to perform quick but safe landing maneuvers for a quadrotor\nmicro air vehicle using the divergence of the optical flow field of a downward\nlooking camera. The optimized policies showed that a piece-wise linear control\nscheme is more effective than the simple linear scheme commonly used, something\nnot yet considered by human designers. Additionally, we show the utility in\nusing abstraction on the input and output of the controller as a tool to\nimprove the robustness of the optimized policies to the reality gap by testing\nour policies optimized in simulation on real world vehicles. We tested the\nneurocontrollers using two different methods to generate and process the visual\ninput, one using a conventional CMOS camera and one a dynamic vision sensor,\nboth of which perform significantly differently than the simulated sensor. The\nuse of the abstracted input resulted in near seamless transfer to the real\nworld with the controllers showing high robustness to a clear reality gap.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:22:21 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Scheper", "Kirk Y. W.", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "1912.07742", "submitter": "Huy Phan", "authors": "Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan", "title": "CAG: A Real-time Low-cost Enhanced-robustness High-transferability\n  Content-aware Adversarial Attack Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial attack despite\ntheir tremendous success in many AI fields. Adversarial attack is a method that\ncauses the intended misclassfication by adding imperceptible perturbations to\nlegitimate inputs. Researchers have developed numerous types of adversarial\nattack methods. However, from the perspective of practical deployment, these\nmethods suffer from several drawbacks such as long attack generating time, high\nmemory cost, insufficient robustness and low transferability. We propose a\nContent-aware Adversarial Attack Generator (CAG) to achieve real-time,\nlow-cost, enhanced-robustness and high-transferability adversarial attack.\nFirst, as a type of generative model-based attack, CAG shows significant\nspeedup (at least 500 times) in generating adversarial examples compared to the\nstate-of-the-art attacks such as PGD and C\\&W. CAG only needs a single\ngenerative model to perform targeted attack to any targeted class. Because CAG\nencodes the label information into a trainable embedding layer, it differs from\nprior generative model-based adversarial attacks that use $n$ different copies\nof generative models for $n$ different targeted classes. As a result, CAG\nsignificantly reduces the required memory cost for generating adversarial\nexamples. CAG can generate adversarial perturbations that focus on the critical\nareas of input by integrating the class activation maps information in the\ntraining process, and hence improve the robustness of CAG attack against the\nstate-of-art adversarial defenses. In addition, CAG exhibits high\ntransferability across different DNN classifier models in black-box attack\nscenario by introducing random dropout in the process of generating\nperturbations. Extensive experiments on different datasets and DNN models have\nverified the real-time, low-cost, enhanced-robustness, and high-transferability\nbenefits of CAG.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:48:38 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Phan", "Huy", ""], ["Xie", "Yi", ""], ["Liao", "Siyu", ""], ["Chen", "Jie", ""], ["Yuan", "Bo", ""]]}, {"id": "1912.07743", "submitter": "Lin Fu", "authors": "Lin Fu and Bruno De Man", "title": "A hierarchical approach to deep learning and its application to\n  tomographic reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has shown unprecedented performance for many image\nanalysis and image enhancement tasks. Yet, solving large-scale inverse problems\nlike tomographic reconstruction remains challenging for DL. These problems\ninvolve non-local and space-variant integral transforms between the input and\noutput domains, for which no efficient neural network models have been found. A\nprior attempt to solve such problems with supervised learning relied on a\nbrute-force fully connected network and applied it to reconstruction for a\n$128^4$ system matrix size. This cannot practically scale to realistic data\nsizes such as $512^4$ and $512^6$ for three-dimensional data sets. Here we\npresent a novel framework to solve such problems with deep learning by casting\nthe original problem as a continuum of intermediate representations between the\ninput and output data. The original problem is broken down into a sequence of\nsimpler transformations that can be well mapped onto an efficient hierarchical\nnetwork architecture, with exponentially fewer parameters than a generic\nnetwork would need. We applied the approach to computed tomography (CT) image\nreconstruction for a $512^4$ system matrix size. To our knowledge, this enabled\nthe first data-driven DL solver for full-size CT reconstruction without relying\non the structure of direct (analytical) or iterative (numerical) inversion\ntechniques. The proposed approach is applicable to other imaging problems such\nas emission and magnetic resonance reconstruction. More broadly, hierarchical\nDL opens the door to a new class of solvers for general inverse problems, which\ncould potentially lead to improved signal-to-noise ratio, spatial resolution\nand computational efficiency in various areas.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:53:14 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Fu", "Lin", ""], ["De Man", "Bruno", ""]]}, {"id": "1912.07744", "submitter": "Siyuan Huang", "authors": "Siyuan Huang, Yixin Chen, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun\n  Zhu", "title": "PerspectiveNet: 3D Object Detection from a Single RGB Image via\n  Perspective Points", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting 3D objects from a single RGB image is intrinsically ambiguous, thus\nrequiring appropriate prior knowledge and intermediate representations as\nconstraints to reduce the uncertainties and improve the consistencies between\nthe 2D image plane and the 3D world coordinate. To address this challenge, we\npropose to adopt perspective points as a new intermediate representation for 3D\nobject detection, defined as the 2D projections of local Manhattan 3D keypoints\nto locate an object; these perspective points satisfy geometric constraints\nimposed by the perspective projection. We further devise PerspectiveNet, an\nend-to-end trainable model that simultaneously detects the 2D bounding box, 2D\nperspective points, and 3D object bounding box for each object from a single\nRGB image. PerspectiveNet yields three unique advantages: (i) 3D object\nbounding boxes are estimated based on perspective points, bridging the gap\nbetween 2D and 3D bounding boxes without the need of category-specific 3D shape\npriors. (ii) It predicts the perspective points by a template-based method, and\na perspective loss is formulated to maintain the perspective constraints. (iii)\nIt maintains the consistency between the 2D perspective points and 3D bounding\nboxes via a differentiable projective function. Experiments on SUN RGB-D\ndataset show that the proposed method significantly outperforms existing\nRGB-based approaches for 3D object detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:58:53 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Huang", "Siyuan", ""], ["Chen", "Yixin", ""], ["Yuan", "Tao", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1912.07745", "submitter": "Janis Dalins", "authors": "Janis Dalins, Campbell Wilson, Douglas Boudry", "title": "PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing\n  Algorithms", "comments": "Submitted to Journal of Digital Investigation 08 SEP 2019. Under\n  review as at 13 December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and reliable automated detection of modified image and multimedia\nfiles has long been a challenge for law enforcement, compounded by the harm\ncaused by repeated exposure to psychologically harmful materials. In August\n2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and\nvideo similarity measurement, respectively. In this report, we review the\nalgorithms' performance on detecting commonly encountered transformations on\nreal-world case data, sourced from contemporary investigations. We also provide\na reference implementation to demonstrate the potential application and\nintegration of such algorithms within existing law enforcement systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:00:09 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Dalins", "Janis", ""], ["Wilson", "Campbell", ""], ["Boudry", "Douglas", ""]]}, {"id": "1912.07748", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo\n  Bremer", "title": "MimicGAN: Robust Projection onto Image Manifolds with Corruption\n  Mimicking", "comments": "International Journal on Computer Vision's (IJCV) Special Issue on\n  GANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, Generative Adversarial Networks (GANs) have\ndramatically advanced our ability to represent and parameterize\nhigh-dimensional, non-linear image manifolds. As a result, they have been\nwidely adopted across a variety of applications, ranging from challenging\ninverse problems like image completion, to problems such as anomaly detection\nand adversarial defense. A recurring theme in many of these applications is the\nnotion of projecting an image observation onto the manifold that is inferred by\nthe generator. In this context, Projected Gradient Descent (PGD) has been the\nmost popular approach, which essentially optimizes for a latent vector that\nminimizes the discrepancy between a generated image and the given observation.\nHowever, PGD is a brittle optimization technique that fails to identify the\nright projection (or latent vector) when the observation is corrupted, or\nperturbed even by a small amount. Such corruptions are common in the real\nworld, for example images in the wild come with unknown crops, rotations,\nmissing pixels, or other kinds of non-linear distributional shifts which break\ncurrent encoding methods, rendering downstream applications unusable. To\naddress this, we propose corruption mimicking -- a new robust projection\ntechnique, that utilizes a surrogate network to approximate the unknown\ncorruption directly at test time, without the need for additional supervision\nor data augmentation. The proposed method is significantly more robust than PGD\nand other competing methods under a wide variety of corruptions, thereby\nenabling a more effective use of GANs in real-world applications. More\nimportantly, we show that our approach produces state-of-the-art performance in\nseveral GAN-based applications -- anomaly detection, domain adaptation, and\nadversarial defense, that benefit from an accurate projection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:14:56 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 19:18:41 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 17:21:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Kailkhura", "Bhavya", ""], ["Bremer", "Timo", ""]]}, {"id": "1912.07764", "submitter": "Samar Alsaleh", "authors": "Samar M. Alsaleh, Angelica I. Aviles-Rivero, Noemie Debroux, James K.\n  Hahn", "title": "Dim the Lights! -- Low-Rank Prior Temporal Data for Specular-Free Video\n  Recovery", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appearance of an object is significantly affected by the illumination\nconditions in the environment. This is more evident with strong reflective\nobjects as they suffer from more dominant specular reflections, causing\ninformation loss and discontinuity in the image domain. In this paper, we\npresent a novel framework for specular-free video recovery with special\nemphasis on dealing with complex motions coming from objects or camera. Our\nsolution is a twostep approach that allows for both detection and restoration\nof the damaged regions on video data. We first propose a spatially adaptive\ndetection term that searches for the damage areas. We then introduce a\nvariational solution for specular-free video recovery that allows exploiting\nspatio-temporal correlations by representing prior data in a low-rank form. We\ndemonstrate that our solution prevents major drawbacks of existing approaches\nwhile improving the performance in both detection accuracy and inpainting\nquality. Finally, we show that our approach can be applied to other problems\nsuch as object removal.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 00:19:46 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Alsaleh", "Samar M.", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Debroux", "Noemie", ""], ["Hahn", "James K.", ""]]}, {"id": "1912.07773", "submitter": "Sonia Baee", "authors": "Sonia Baee, Erfan Pakdamanian, Inki Kim, Lu Feng, Vicente Ordonez,\n  Laura Barnes", "title": "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy\n  Deep Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by human visual attention, we introduce a Maximum Entropy Deep\nInverse Reinforcement Learning (MEDIRL) framework for modeling the visual\nattention allocation of drivers in imminent rear-end collisions. MEDIRL is\ncomposed of visual, driving, and attention modules. Given a front-view driving\nvideo and corresponding eye fixations from humans, the visual and driving\nmodules extract generic and driving-specific visual features, respectively.\nFinally, the attention module learns the intrinsic task-sensitive reward\nfunctions induced by eye fixation policies recorded from attentive drivers.\nMEDIRL uses the learned policies to predict visual attention allocation of\ndrivers. We also introduce EyeCar, a new driver visual attention dataset during\naccident-prone situations. We conduct comprehensive experiments and show that\nMEDIRL outperforms previous state-of-the-art methods on driving task-related\nvisual attention allocation on the following large-scale driving attention\nbenchmark datasets: DR(eye)VE, BDD-A, and DADA-2000. The code and dataset are\nprovided for reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:05:26 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 00:34:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 00:37:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Baee", "Sonia", ""], ["Pakdamanian", "Erfan", ""], ["Kim", "Inki", ""], ["Feng", "Lu", ""], ["Ordonez", "Vicente", ""], ["Barnes", "Laura", ""]]}, {"id": "1912.07776", "submitter": "Zeyu Deng", "authors": "Zeyu Deng, Lihui Wang, Zixiang Kuai, Qijian Chen, Xinyu Cheng, Feng\n  Yang, Jie Yang, Yuemin Zhu", "title": "CNN-Based Invertible Wavelet Scattering for the Investigation of\n  Diffusion Properties of the In Vivo Human Heart in Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vivo diffusion tensor imaging (DTI) is a promising technique to\ninvestigate noninvasively the fiber structures of the in vivo human heart.\nHowever, signal loss due to motions remains a persistent problem in in vivo\ncardiac DTI. We propose a novel motion-compensation method for investigating in\nvivo myocardium structures in DTI with free-breathing acquisitions. The method\nis based on an invertible Wavelet Scattering achieved by means of Convolutional\nNeural Network (WSCNN). It consists of first extracting translation-invariant\nwavelet scattering features from DW images acquired at different trigger delays\nand then mapping the fused scattering features into motion-compensated spatial\nDW images by performing an inverse wavelet scattering transform achieved using\nCNN. The results on both simulated and acquired in vivo cardiac DW images\nshowed that the proposed WSCNN method effectively compensates for\nmotion-induced signal loss and produces in vivo cardiac DW images with better\nquality and more coherent fiber structures with respect to existing methods,\nwhich makes it an interesting method for measuring correctly the diffusion\nproperties of the in vivo human heart in DTI under free breathing.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:26:06 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Deng", "Zeyu", ""], ["Wang", "Lihui", ""], ["Kuai", "Zixiang", ""], ["Chen", "Qijian", ""], ["Cheng", "Xinyu", ""], ["Yang", "Feng", ""], ["Yang", "Jie", ""], ["Zhu", "Yuemin", ""]]}, {"id": "1912.07778", "submitter": "He-Feng Yin", "authors": "Wen Zhao, Xiao-Jun Wu, He-Feng Yin and Zi-Qi Li", "title": "Collaborative representation-based robust face recognition by\n  discriminative low-rank representation", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust face recognition in which both the training\nand test samples might be corrupted because of disguise and occlusion.\nPerformance of conventional subspace learning methods and recently proposed\nsparse representation based classification (SRC) might be degraded when\ncorrupted training samples are provided. In addition, sparsity based approaches\nare time-consuming due to the sparsity constraint. To alleviate the\naforementioned problems to some extent, in this paper, we propose a\ndiscriminative low-rank representation method for collaborative\nrepresentation-based (DLRR-CR) robust face recognition. DLRR-CR not only\nobtains a clean dictionary, it further forces the sub-dictionaries for distinct\nclasses to be as independent as possible by introducing a structural\nincoherence regularization term. Simultaneously, a low-rank projection matrix\ncan be learned to remove the possible corruptions in the testing samples.\nCollaborative representation based classification (CRC) method is exploited in\nour proposed method which has closed-form solution. Experimental results\nobtained on public face databases verify the effectiveness and robustness of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:40:04 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhao", "Wen", ""], ["Wu", "Xiao-Jun", ""], ["Yin", "He-Feng", ""], ["Li", "Zi-Qi", ""]]}, {"id": "1912.07783", "submitter": "Mahmudul Hasan", "authors": "Nowshin Tasnim, Mahmudul Hasan, Ishrak Islam", "title": "Comparisonal study of Deep Learning approaches on Retinal OCT Image", "comments": "Poster in International Conference on Innovation in Engineering and\n  Technology (ICIET) 23-24 December, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical science, the use of computer science in disease detection and\ndiagnosis is gaining popularity. Previously, the detection of disease used to\ntake a significant amount of time and was less reliable. Machine learning (ML)\ntechniques employed in recent biomedical researches are making revolutionary\nchanges by gaining higher accuracy with more concise timing. At present, it is\neven possible to automatically detect diseases from the scanned images with the\nhelp of ML. In this research, we have taken such an attempt to detect retinal\ndiseases from optical coherence tomography (OCT) X-ray images. Here, we propose\na deep learning (DL) based approach in detecting retinal diseases from OCT\nimages which can identify three conditions of the retina. Four different models\nused in this approach are compared with each other. On the test set, the\ndetection accuracy is 98.00\\% for a vanilla convolutional neural network (CNN)\nmodel, 99.07\\% for Xception model, 97.00\\% for ResNet50 model, and 99.17\\% for\nMobileNetV2 model. The MobileNetV2 model acquires the highest accuracy, and the\nclosest to the highest is the Xception model. The proposed approach has a\npotential impact on creating a tool for automatically detecting retinal\ndiseases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:09:11 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Tasnim", "Nowshin", ""], ["Hasan", "Mahmudul", ""], ["Islam", "Ishrak", ""]]}, {"id": "1912.07791", "submitter": "Xuan Zhang", "authors": "Xuan Zhang, Shaofei Qin, Yi Xu, Hongteng Xu", "title": "Quaternion Product Units for Deep Learning on 3D Rotation Groups", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel quaternion product unit (QPU) to represent data on 3D\nrotation groups. The QPU leverages quaternion algebra and the law of 3D\nrotation group, representing 3D rotation data as quaternions and merging them\nvia a weighted chain of Hamilton products. We prove that the representations\nderived by the proposed QPU can be disentangled into \"rotation-invariant\"\nfeatures and \"rotation-equivariant\" features, respectively, which supports the\nrationality and the efficiency of the QPU in theory. We design quaternion\nneural networks based on our QPUs and make our models compatible with existing\ndeep learning models. Experiments on both synthetic and real-world data show\nthat the proposed QPU is beneficial for the learning tasks requiring rotation\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 02:46:54 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 12:47:39 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Zhang", "Xuan", ""], ["Qin", "Shaofei", ""], ["Xu", "Yi", ""], ["Xu", "Hongteng", ""]]}, {"id": "1912.07806", "submitter": "Zi-Rui Wang", "authors": "Zi-Rui Wang, Jun Du", "title": "Joint Architecture and Knowledge Distillation in CNN for Chinese Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of distillation helps transform cumbersome neural network into\ncompact network so that the model can be deployed on alternative hardware\ndevices. The main advantages of distillation based approaches include simple\ntraining process, supported by most off-the-shelf deep learning softwares and\nno special requirement of hardwares. In this paper, we propose a guideline to\ndistill the architecture and knowledge of pre-trained standard CNNs\nsimultaneously. We first make a quantitative analysis of the baseline network,\nincluding computational cost and storage overhead in different components. And\nthen, according to the analysis results, optional strategies can be adopted to\nthe compression of fully-connected layers. For vanilla convolution layers, the\nproposed parsimonious convolution (ParConv) block only consisting of depthwise\nseparable convolution and pointwise convolution is used as a direct replacement\nwithout other adjustments such as the widths and depths in the network.\nFinally, the knowledge distillation with multiple losses is adopted to improve\nperformance of the compact CNN. The proposed algorithm is first verified on\noffline handwritten Chinese text recognition (HCTR) where the CNNs are\ncharacterized by tens of thousands of output nodes and trained by hundreds of\nmillions of training samples. Compared with the CNN in the state-of-the-art\nsystem, our proposed joint architecture and knowledge distillation can reduce\nthe computational cost by >10x and model size by >8x with negligible accuracy\nloss. And then, by conducting experiments on one of the most popular data sets:\nMNIST, we demonstrate the proposed approach can also be successfully applied on\nmainstream backbone networks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 03:48:37 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 12:53:45 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 14:14:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Zi-Rui", ""], ["Du", "Jun", ""]]}, {"id": "1912.07809", "submitter": "Yu Aijing", "authors": "Aijing Yu, Haoxue Wu, Huaibo Huang, Zhen Lei, Ran He", "title": "LAMP-HQ: A Large-Scale Multi-Pose High-Quality Database and Benchmark\n  for NIR-VIS Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-infrared-visible (NIR-VIS) heterogeneous face recognition matches NIR to\ncorresponding VIS face images. However, due to the sensing gap, NIR images\noften lose some identity information so that the recognition issue is more\ndifficult than conventional VIS face recognition. Recently, NIR-VIS\nheterogeneous face recognition has attracted considerable attention in the\ncomputer vision community because of its convenience and adaptability in\npractical applications. Various deep learning-based methods have been proposed\nand substantially increased the recognition performance, but the lack of\nNIR-VIS training samples leads to the difficulty of the model training process.\nIn this paper, we propose a new Large-Scale Multi-Pose High-Quality NIR-VIS\ndatabase LAMP-HQ containing 56,788 NIR and 16,828 VIS images of 573 subjects\nwith large diversities in pose, illumination, attribute, scene and accessory.\nWe furnish a benchmark along with the protocol for NIR-VIS face recognition via\ngeneration on LAMP-HQ, including Pixel2Pixel, CycleGAN, and ADFL. Furthermore,\nwe propose a novel exemplar-based variational spectral attention network to\nproduce high-fidelity VIS images from NIR data. A spectral conditional\nattention module is introduced to reduce the domain gap between NIR and VIS\ndata and then improve the performance of NIR-VIS heterogeneous face recognition\non various databases including the LAMP-HQ.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 04:01:18 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 05:05:36 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yu", "Aijing", ""], ["Wu", "Haoxue", ""], ["Huang", "Huaibo", ""], ["Lei", "Zhen", ""], ["He", "Ran", ""]]}, {"id": "1912.07812", "submitter": "Guangyi Zhang", "authors": "Guangyi Zhang and Ali Etemad", "title": "Capsule Attention for Multimodal EEG-EOG Representation Learning with\n  Application to Driver Vigilance Estimation", "comments": "Accepted by IEEE Transactions on Neural Systems and Rehabilitation\n  Engineering", "journal-ref": null, "doi": "10.1109/TNSRE.2021.3089594", "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver vigilance estimation is an important task for transportation safety.\nWearable and portable brain-computer interface devices provide a powerful means\nfor real-time monitoring of the vigilance level of drivers to help with\navoiding distracted or impaired driving. In this paper, we propose a novel\nmultimodal architecture for in-vehicle vigilance estimation from\nElectroencephalogram and Electrooculogram. To enable the system to focus on the\nmost salient parts of the learned multimodal representations, we propose an\narchitecture composed of a capsule attention mechanism following a deep Long\nShort-Term Memory (LSTM) network. Our model learns hierarchical dependencies in\nthe data through the LSTM and capsule feature representation layers. To better\nexplore the discriminative ability of the learned representations, we study the\neffect of the proposed capsule attention mechanism including the number of\ndynamic routing iterations as well as other parameters. Experiments show the\nrobustness of our method by outperforming other solutions and baseline\ntechniques, setting a new state-of-the-art. We then provide an analysis on\ndifferent frequency bands and brain regions to evaluate their suitability for\ndriver vigilance estimation. Lastly, an analysis on the role of capsule\nattention, multimodality, and robustness to noise is performed, highlighting\nthe advantages of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 04:20:08 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 19:59:07 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 16:31:51 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 16:14:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Guangyi", ""], ["Etemad", "Ali", ""]]}, {"id": "1912.07819", "submitter": "Jiantao Wu", "authors": "JT Wu and L.Wang", "title": "Angular Learning: Toward Discriminative Embedded Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The margin-based softmax loss functions greatly enhance intra-class\ncompactness and perform well on the tasks of face recognition and object\nclassification. Outperformance, however, depends on the careful hyperparameter\nselection. Moreover, the hard angle restriction also increases the risk of\noverfitting. In this paper, angular loss suggested by maximizing the angular\ngradient to promote intra-class compactness avoids overfitting. Besides, our\nmethod has only one adjustable constant for intra-class compactness control. We\ndefine three metrics to measure inter-class separability and intra-class\ncompactness. In experiments, we test our method, as well as other methods, on\nmany well-known datasets. Experimental results reveal that our method has the\nsuperiority of accuracy improvement, discriminative information, and\ntime-consumption.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:08:20 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Wu", "JT", ""], ["Wang", "L.", ""]]}, {"id": "1912.07833", "submitter": "Satoshi Kosugi", "authors": "Satoshi Kosugi, Toshihiko Yamasaki", "title": "Unpaired Image Enhancement Featuring Reinforcement-Learning-Controlled\n  Image Editing Software", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles unpaired image enhancement, a task of learning a mapping\nfunction which transforms input images into enhanced images in the absence of\ninput-output image pairs. Our method is based on generative adversarial\nnetworks (GANs), but instead of simply generating images with a neural network,\nwe enhance images utilizing image editing software such as Adobe Photoshop for\nthe following three benefits: enhanced images have no artifacts, the same\nenhancement can be applied to larger images, and the enhancement is\ninterpretable. To incorporate image editing software into a GAN, we propose a\nreinforcement learning framework where the generator works as the agent that\nselects the software's parameters and is rewarded when it fools the\ndiscriminator. Our framework can use high-quality non-differentiable filters\npresent in image editing software, which enables image enhancement with high\nperformance. We apply the proposed method to two unpaired image enhancement\ntasks: photo enhancement and face beautification. Our experimental results\ndemonstrate that the proposed method achieves better performance, compared to\nthe performances of the state-of-the-art methods based on unpaired learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 06:03:05 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Kosugi", "Satoshi", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "1912.07849", "submitter": "Yingqian Wang", "authors": "Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan\n  Guo", "title": "Spatial-Angular Interaction for Light Field Image Super-Resolution", "comments": "In this version, we have revised the paper and compared our\n  LF-InterNet to the most recent LF-ATO method (CVPR2020). Codes and\n  pre-trained models are available at\n  https://github.com/YingqianWang/LF-InterNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field (LF) cameras record both intensity and directions of light rays,\nand capture scenes from a number of viewpoints. Both information within each\nperspective (i.e., spatial information) and among different perspectives (i.e.,\nangular information) is beneficial to image super-resolution (SR). In this\npaper, we propose a spatial-angular interactive network (namely, LF-InterNet)\nfor LF image SR. Specifically, spatial and angular features are first\nseparately extracted from input LFs, and then repetitively interacted to\nprogressively incorporate spatial and angular information. Finally, the\ninteracted features are fused to superresolve each sub-aperture image.\nExperimental results demonstrate the superiority of LF-InterNet over the\nstate-of-the-art methods, i.e., our method can achieve high PSNR and SSIM\nscores with low computational cost, and recover faithful details in the\nreconstructed images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 07:17:52 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 00:22:54 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 09:33:56 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Wang", "Yingqian", ""], ["Wang", "Longguang", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Yu", "Jingyi", ""], ["Guo", "Yulan", ""]]}, {"id": "1912.07854", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Dimitris Agrafiotis", "title": "Enhanced Spatially Interleaved Techniques for Multi-View Distributed\n  Video Coding", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-view distributed video coding framework for\nindependent camera encoding and centralized decoding. Spatio-temporal-view\nconcealment methods are developed that exploit the interleaved nature of the\nemployed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side\ninformation (SI). We study a number of view concealment methods and develop a\njoint approach that exploits all available correlation for forming the side\ninformation. We apply a diversity technique for fusing multiple such\npredictions thereby achieving more reliable results. We additionally introduce\nsystems enhancements for further improving the rate distortion performance\nthrough selective feedback, inter-view bitplane projection and frame\nsubtraction. Results show a significant improvement in performance relative to\nH.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB\nincrease in PSNR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 07:35:26 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Agrafiotis", "Dimitris", ""]]}, {"id": "1912.07863", "submitter": "Wuyang Chen", "authors": "Ye Yuan, Wuyang Chen, Yang Yang, Zhangyang Wang", "title": "In Defense of the Triplet Loss Again: Learning Robust Person\n  Re-Identification with Fast Approximated Triplet Loss and Label Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparative losses (typically, triplet loss) are appealing choices for\nlearning person re-identification (ReID) features. However, the triplet loss is\ncomputationally much more expensive than the (practically more popular)\nclassification loss, limiting their wider usage in massive datasets. Moreover,\nthe abundance of label noise and outliers in ReID datasets may also put the\nmargin-based loss in jeopardy. This work addresses the above two shortcomings\nof triplet loss, extending its effectiveness to large-scale ReID datasets with\npotentially noisy labels. We propose a fast-approximated triplet (FAT) loss,\nwhich provably converts the point-wise triplet loss into its upper bound form,\nconsisting of a point-to-set loss term plus cluster compactness regularization.\nIt preserves the effectiveness of triplet loss, while leading to linear\ncomplexity to the training set size. A label distillation strategy is further\ndesigned to learn refined soft-labels in place of the potentially noisy labels,\nfrom only an identified subset of confident examples, through teacher-student\nnetworks. We conduct extensive experiments on three most popular ReID\nbenchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT\nloss with distilled labels lead to ReID features with remarkable accuracy,\nefficiency, robustness, and direct transferability to unseen datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:16:45 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:23:44 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Yuan", "Ye", ""], ["Chen", "Wuyang", ""], ["Yang", "Yang", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1912.07868", "submitter": "Kevin Bui", "authors": "Kevin Bui and Fredrick Park and Shuai Zhang and Yingyong Qi and Jack\n  Xin", "title": "$\\ell_0$ Regularized Structured Sparsity Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepening and widening convolutional neural networks (CNNs) significantly\nincreases the number of trainable weight parameters by adding more\nconvolutional layers and feature maps per layer, respectively. By imposing\ninter- and intra-group sparsity onto the weights of the layers during the\ntraining process, a compressed network can be obtained with accuracy comparable\nto a dense one. In this paper, we propose a new variant of sparse group lasso\nthat blends the $\\ell_0$ norm onto the individual weight parameters and the\n$\\ell_{2,1}$ norm onto the output channels of a layer. To address the\nnon-differentiability of the $\\ell_0$ norm, we apply variable splitting\nresulting in an algorithm that consists of executing stochastic gradient\ndescent followed by hard thresholding for each iteration. Numerical experiments\nare demonstrated on LeNet-5 and wide-residual-networks for MNIST and CIFAR\n10/100, respectively. They showcase the effectiveness of our proposed method in\nattaining superior test accuracy with network sparsification on par with the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:34:22 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Bui", "Kevin", ""], ["Park", "Fredrick", ""], ["Zhang", "Shuai", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "1912.07871", "submitter": "Wenbo Hu", "authors": "Kai Xu, Xiao-Jun Wu, Wen-Bo Hu", "title": "Constructing the F-Graph with a Symmetric Constraint for Subspace\n  Clustering", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on further studying the low-rank subspace clustering (LRSC) and\nL2-graph subspace clustering algorithms, we propose a F-graph subspace\nclustering algorithm with a symmetric constraint (FSSC), which constructs a new\nobjective function with a symmetric constraint basing on F-norm, whose the most\nsignificant advantage is to obtain a closed-form solution of the coefficient\nmatrix. Then, take the absolute value of each element of the coefficient\nmatrix, and retain the k largest coefficients per column, set the other\nelements to 0, to get a new coefficient matrix. Finally, FSSC performs spectral\nclustering over the new coefficient matrix. The experimental results on face\nclustering and motion segmentation show FSSC algorithm can not only obviously\nreduce the running time, but also achieve higher accuracy compared with the\nstate-of-the-art representation-based subspace clustering algorithms, which\nverifies that the FSSC algorithm is efficacious and feasible.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:40:01 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Xu", "Kai", ""], ["Wu", "Xiao-Jun", ""], ["Hu", "Wen-Bo", ""]]}, {"id": "1912.07872", "submitter": "Guo Zhiyao", "authors": "Renchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, Shilei Wen", "title": "Cross-Modality Attention with Semantic Graph Embedding for Multi-Label\n  Classification", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image and video classification are fundamental yet challenging\ntasks in computer vision. The main challenges lie in capturing spatial or\ntemporal dependencies between labels and discovering the locations of\ndiscriminative features for each class. In order to overcome these challenges,\nwe propose to use cross-modality attention with semantic graph embedding for\nmulti label classification. Based on the constructed label graph, we propose an\nadjacency-based similarity graph embedding method to learn semantic label\nembeddings, which explicitly exploit label relationships. Then our novel\ncross-modality attention maps are generated with the guidance of learned label\nembeddings. Experiments on two multi-label image classification datasets\n(MS-COCO and NUS-WIDE) show our method outperforms other existing\nstate-of-the-arts. In addition, we validate our method on a large multi-label\nvideo classification dataset (YouTube-8M Segments) and the evaluation results\ndemonstrate the generalization capability of our method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:41:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 08:41:07 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["You", "Renchun", ""], ["Guo", "Zhiyao", ""], ["Cui", "Lei", ""], ["Long", "Xiang", ""], ["Bao", "Yingze", ""], ["Wen", "Shilei", ""]]}, {"id": "1912.07906", "submitter": "Ying Chen", "authors": "Shibo Zhou, Ying Chen, Xiaohua Li, Arindam Sanyal", "title": "Deep SCNN-based Real-time Object Detection for Self-driving Vehicles\n  Using LiDAR Temporal Data", "comments": null, "journal-ref": "IEEE Access, 2020", "doi": "10.1109/ACCESS.2020.2990416", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time accurate detection of three-dimensional (3D) objects is a\nfundamental necessity for self-driving vehicles. Most existing computer vision\napproaches are based on convolutional neural networks (CNNs). Although the\nCNN-based approaches can achieve high detection accuracy, their high energy\nconsumption is a severe drawback. To resolve this problem, novel energy\nefficient approaches should be explored. Spiking neural network (SNN) is a\npromising candidate because it has orders-of-magnitude lower energy consumption\nthan CNN. Unfortunately, the studying of SNN has been limited in small networks\nonly. The application of SNN for large 3D object detection networks has remain\nlargely open. In this paper, we integrate spiking convolutional neural network\n(SCNN) with temporal coding into the YOLOv2 architecture for real-time object\ndetection. To take the advantage of spiking signals, we develop a novel data\npreprocessing layer that translates 3D point-cloud data into spike time data.\nWe propose an analog circuit to implement the non-leaky integrate and fire\nneuron used in our SCNN, from which the energy consumption of each spike is\nestimated. Moreover, we present a method to calculate the network sparsity and\nthe energy consumption of the overall network. Extensive experiments have been\nconducted based on the KITTI dataset, which show that the proposed network can\nreach competitive detection accuracy as existing approaches, yet with much\nlower average energy consumption. If implemented in dedicated hardware, our\nnetwork could have a mean sparsity of 56.24% and extremely low total energy\nconsumption of 0.247mJ only. Implemented in NVIDIA GTX 1080i GPU, we can\nachieve 35.7 fps frame rate, high enough for real-time object detection.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 09:56:15 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 06:31:31 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 05:09:37 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 23:24:12 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zhou", "Shibo", ""], ["Chen", "Ying", ""], ["Li", "Xiaohua", ""], ["Sanyal", "Arindam", ""]]}, {"id": "1912.07943", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Ahsan Ullah, Talha Iqbal, Shahid Khattak", "title": "Pioneer dataset and automatic recognition of Urdu handwritten characters\n  using a deep autoencoder and convolutional neural network", "comments": "SN Applied Sciences, December 2019", "journal-ref": null, "doi": "10.1007/s42452-019-1914-1", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic recognition of Urdu handwritten digits and characters, is a\nchallenging task. It has applications in postal address reading, bank's cheque\nprocessing, and digitization and preservation of handwritten manuscripts from\nold ages. While there exists a significant work for automatic recognition of\nhandwritten English characters and other major languages of the world, the work\ndone for Urdu lan-guage is extremely insufficient. This paper has two goals.\nFirstly, we introduce a pioneer dataset for handwritten digits and characters\nof Urdu, containing samples from more than 900 individuals. Secondly, we report\nresults for automatic recog-nition of handwritten digits and characters as\nachieved by using deep auto-encoder network and convolutional neural network.\nMore specifically, we use a two-layer and a three-layer deep autoencoder\nnetwork and convolutional neural network and evaluate the two frameworks in\nterms of recognition accuracy. The proposed framework of deep autoencoder can\nsuccessfully recognize digits and characters with an accuracy of 97% for digits\nonly, 81% for characters only and 82% for both digits and characters\nsimultaneously. In comparison, the framework of convolutional neural network\nhas accuracy of 96.7% for digits only, 86.5% for characters only and 82.7% for\nboth digits and characters simultaneously. These frameworks can serve as\nbaselines for future research on Urdu handwritten text.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:49:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Ali", "Hazrat", ""], ["Ullah", "Ahsan", ""], ["Iqbal", "Talha", ""], ["Khattak", "Shahid", ""]]}, {"id": "1912.07959", "submitter": "Hui Li", "authors": "Ya-Qiong Zhang, Xiao-Jun Wu, Hui Li", "title": "Multi-focus Image Fusion Based on Similarity Characteristics", "comments": "7 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-focus image fusion algorithm performed in spatial domain based\non similarity characteristics is proposed incorporating with region\nsegmentation. In this paper, a new similarity measure is developed based on the\nstructural similarity (SSIM) index, which is more suitable for multi-focus\nimage segmentation. Firstly, the SSNSIM map is calculated between two input\nimages. Then we segment the SSNSIM map using watershed method, and merge the\nsmall homogeneous regions with fuzzy c-means clustering algorithm (FCM). For\nthree source images, a joint region segmentation method based on segmentation\nof two images is used to obtain the final segmentation result. Finally, the\ncorresponding segmented regions of the source images are fused according to\ntheir average gradient. The performance of the image fusion method is evaluated\nby several criteria including spatial frequency, average gradient, entropy,\nedge retention etc. The evaluation results indicate that the proposed method is\neffective and has good visual perception.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:16:04 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhang", "Ya-Qiong", ""], ["Wu", "Xiao-Jun", ""], ["Li", "Hui", ""]]}, {"id": "1912.07964", "submitter": "Lin Dongdong", "authors": "Israel Goytom, Qin Wang, Tianxiang Yu, Kunjie Dai, Kris Sankaran,\n  Xinfei Zhou, Dongdong Lin", "title": "Nanoscale Microscopy Images Colorization Using Neural Networks", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy images are powerful tools and widely used in the majority of\nresearch areas, such as biology, chemistry, physics and materials fields by\nvarious microscopies (scanning electron microscope (SEM), atomic force\nmicroscope (AFM) and the optical microscope, et al.). However, most of the\nmicroscopy images are colorless due to the unique imaging mechanism. Though\ninvestigating on some popular solutions proposed recently about colorizing\nimages, we notice the process of those methods are usually tedious,\ncomplicated, and time-consuming. In this paper, inspired by the achievement of\nmachine learning algorithms on different science fields, we introduce two\nartificial neural networks for gray microscopy image colorization: An\nend-to-end convolutional neural network (CNN) with a pre-trained model for\nfeature extraction and a pixel-to-pixel neural style transfer convolutional\nneural network (NST-CNN), which can colorize gray microscopy images with\nsemantic information learned from a user-provided colorful image at inference\ntime. The results demonstrate that our algorithm not only can colorize the\nmicroscopy images under complex circumstances precisely but also make the color\nnaturally according to the training of a massive number of nature images with\nproper hue and saturation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:21:42 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 15:03:34 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Goytom", "Israel", ""], ["Wang", "Qin", ""], ["Yu", "Tianxiang", ""], ["Dai", "Kunjie", ""], ["Sankaran", "Kris", ""], ["Zhou", "Xinfei", ""], ["Lin", "Dongdong", ""]]}, {"id": "1912.07966", "submitter": "Franz G\\\"otz-Hahn", "authors": "Franz G\\\"otz-Hahn, Vlad Hosu, Hanhe Lin, Dietmar Saupe", "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of\n  Videos in-the-Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality assessment (VQA) methods focus on particular degradation types,\nusually artificially induced on a small set of reference videos. Hence, most\ntraditional VQA methods under-perform in-the-wild. Deep learning approaches\nhave had limited success due to the small size and diversity of existing VQA\ndatasets, either artificial or authentically distorted. We introduce a new\nin-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k.\nIt consists of a coarsely annotated set of 153,841 videos having five quality\nratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally,\nwe propose new efficient VQA approaches (MLSP-VQA) relying on multi-level\nspatially pooled deep-features (MLSP). They are exceptionally well suited for\ntraining at scale, compared to deep transfer learning approaches. Our best\nmethod, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient\n(SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark\ndataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC)\nand hand-crafted feature-based method (0.78 SRCC). We further investigate how\nalternative approaches perform under different levels of label noise, and\ndataset size, showing that MLSP-VQA-FF is the overall best method for videos\nin-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k\nsets the new state-of-the-art for cross-test performance on KoNViD-1k,\nLIVE-VQC, and LIVE-Qualcomm with a 0.83, 0.75, and 0.64 SRCC, respectively. For\nboth KoNViD-1k and LIVE-VQC this inter-dataset testing outperforms\nintra-dataset experiments, showing excellent generalization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:26:32 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 14:25:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["G\u00f6tz-Hahn", "Franz", ""], ["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1912.07971", "submitter": "Ziming Wang", "authors": "Zi-Ming Wang, Meng-Han Li, Gui-Song Xia", "title": "Conditional Generative ConvNets for Exemplar-based Texture Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of exemplar-based texture synthesis is to generate texture images\nthat are visually similar to a given exemplar. Recently, promising results have\nbeen reported by methods relying on convolutional neural networks (ConvNets)\npretrained on large-scale image datasets. However, these methods have\ndifficulties in synthesizing image textures with non-local structures and\nextending to dynamic or sound textures. In this paper, we present a conditional\ngenerative ConvNet (cgCNN) model which combines deep statistics and the\nprobabilistic framework of generative ConvNet (gCNN) model. Given a texture\nexemplar, the cgCNN model defines a conditional distribution using deep\nstatistics of a ConvNet, and synthesize new textures by sampling from the\nconditional distribution. In contrast to previous deep texture models, the\nproposed cgCNN dose not rely on pre-trained ConvNets but learns the weights of\nConvNets for each input exemplar instead. As a result, the cgCNN model can\nsynthesize high quality dynamic, sound and image textures in a unified manner.\nWe also explore the theoretical connections between our model and other texture\nmodels. Further investigations show that the cgCNN model can be easily\ngeneralized to texture expansion and inpainting. Extensive experiments\ndemonstrate that our model can achieve better or at least comparable results\nthan the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:40:47 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Wang", "Zi-Ming", ""], ["Li", "Meng-Han", ""], ["Xia", "Gui-Song", ""]]}, {"id": "1912.07991", "submitter": "Yatin Dandi", "authors": "Yatin Dandi, Aniket Das, Soumye Singhal, Vinay P. Namboodiri, Piyush\n  Rai", "title": "Jointly Trained Image and Video Generation using Residual Vectors", "comments": "Accepted in 2020 Winter Conference on Applications of Computer Vision\n  (WACV '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a modeling technique for jointly training image and\nvideo generation models by simultaneously learning to map latent variables with\na fixed prior onto real images and interpolate over images to generate videos.\nThe proposed approach models the variations in representations using residual\nvectors encoding the change at each time step over a summary vector for the\nentire video. We utilize the technique to jointly train an image generation\nmodel with a fixed prior along with a video generation model lacking\nconstraints such as disentanglement. The joint training enables the image\ngenerator to exploit temporal information while the video generation model\nlearns to flexibly share information across frames. Moreover, experimental\nresults verify our approach's compatibility with pre-training on videos or\nimages and training on datasets containing a mixture of both. A comprehensive\nset of quantitative and qualitative evaluations reveal the improvements in\nsample quality and diversity over both video generation and image generation\nbaselines. We further demonstrate the technique's capabilities of exploiting\nsimilarity in features across frames by applying it to a model based on\ndecomposing the video into motion and content. The proposed model allows minor\nvariations in content across frames while maintaining the temporal dependence\nthrough latent vectors encoding the pose or motion features.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:14:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Dandi", "Yatin", ""], ["Das", "Aniket", ""], ["Singhal", "Soumye", ""], ["Namboodiri", "Vinay P.", ""], ["Rai", "Piyush", ""]]}, {"id": "1912.08002", "submitter": "Xin Yang", "authors": "Tangxin Xie, Xin Yang, Yu Jia, Chen Zhu, Xiaochuan Li", "title": "Adaptive Densely Connected Super-Resolution Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a better performance in single image super-resolution(SISR), we present\nan image super-resolution algorithm based on adaptive dense connection (ADCSR).\nThe algorithm is divided into two parts: BODY and SKIP. BODY improves the\nutilization of convolution features through adaptive dense connections. Also,\nwe develop an adaptive sub-pixel reconstruction layer (AFSL) to reconstruct the\nfeatures of the BODY output. We pre-trained SKIP to make BODY focus on\nhigh-frequency feature learning. The comparison of PSNR, SSIM, and visual\neffects verify the superiority of our method to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:38:51 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 09:09:13 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Xie", "Tangxin", ""], ["Yang", "Xin", ""], ["Jia", "Yu", ""], ["Zhu", "Chen", ""], ["Li", "Xiaochuan", ""]]}, {"id": "1912.08035", "submitter": "Andrea Simonelli", "authors": "Andrea Simonelli, Samuel Rota Bul\\`o, Lorenzo Porzi, Elisa Ricci,\n  Peter Kontschieder", "title": "Towards Generalization Across Depth for Monocular 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While expensive LiDAR and stereo camera rigs have enabled the development of\nsuccessful 3D object detection methods, monocular RGB-only approaches lag much\nbehind. This work advances the state of the art by introducing MoVi-3D, a\nnovel, single-stage deep architecture for monocular 3D object detection.\nMoVi-3D builds upon a novel approach which leverages geometrical information to\ngenerate, both at training and test time, virtual views where the object\nappearance is normalized with respect to distance. These virtually generated\nviews facilitate the detection task as they significantly reduce the visual\nappearance variability associated to objects placed at different distances from\nthe camera. As a consequence, the deep model is relieved from learning\ndepth-specific representations and its complexity can be significantly reduced.\nIn particular, in this work we show that, thanks to our virtual views\ngeneration process, a lightweight, single-stage architecture suffices to set\nnew state-of-the-art results on the popular KITTI3D benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 14:34:27 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 09:16:02 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 12:34:40 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Simonelli", "Andrea", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Porzi", "Lorenzo", ""], ["Ricci", "Elisa", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1912.08059", "submitter": "Tian Liu Miss", "authors": "Tian Liu, Lichun Wang, Shaofan Wang", "title": "Feature Fusion Use Unsupervised Prior Knowledge to Let Small Object\n  Represent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing low level and high level features is a widely used strategy to provide\ndetails that might be missing during convolution and pooling. Different from\nprevious works, we propose a new fusion mechanism called FillIn which takes\nadvantage of prior knowledge described with superpixel segmentation. According\nto the prior knowledge, the FillIn chooses small region on low level feature\nmap to fill into high level feature map. By using the proposed fusion\nmechanism, the low level features have equal channels for some tiny region as\nhigh level features, which makes the low level features have relatively\nindependent power to decide final semantic label. We demonstrate the\neffectiveness of our model on PASCAL VOC 2012, it achieves competitive test\nresult based on DeepLabv3+ backbone and visualizations of predictions prove our\nfusion can let small objects represent and low level features have potential\nfor segmenting small objects.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:00:56 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Liu", "Tian", ""], ["Wang", "Lichun", ""], ["Wang", "Shaofan", ""]]}, {"id": "1912.08061", "submitter": "Gourav Modanwal", "authors": "Gourav Modanwal, Adithya Vellal, Maciej A. Mazurowski", "title": "Normalization of breast MRIs using Cycle-Consistent Generative\n  Adversarial Networks", "comments": "Final accepted draft in Computer Methods and Programs in Biomedicine", "journal-ref": "Computer Methods and Programs in Biomedicine, 2021", "doi": "10.1016/j.cmpb.2021.106225", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used\nto complement ultrasound examinations and x-ray mammography during the early\ndetection and diagnosis of breast cancer. However, images generated by various\nMRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise\ndistribution, preventing algorithms trained on MRIs from one scanner to\ngeneralize to data from other scanners successfully. We propose a method for\nimage normalization to solve this problem. MRI normalization is challenging\nbecause it requires both normalizing intensity values and mapping between the\nnoise distributions of different scanners. We utilize a cycle-consistent\ngenerative adversarial network to learn a bidirectional mapping between MRIs\nproduced by GE Healthcare and Siemens scanners. This allows us learning the\nmapping between two different scanner types without matched data, which is not\ncommonly available. To ensure the preservation of breast shape and structures\nwithin the breast, we propose two technical innovations. First, we incorporate\na mutual information loss with the CycleGAN architecture to ensure that the\nstructure of the breast is maintained. Second, we propose a modified\ndiscriminator architecture which utilizes a smaller field-of-view to ensure the\npreservation of finer details in the breast tissue. Quantitative and\nqualitative evaluations show that the second proposed method was able to\nconsistently preserve a high level of detail in the breast structure while also\nperforming the proper intensity normalization and noise mapping. Our results\ndemonstrate that the proposed model can successfully learn a bidirectional\nmapping between MRIs produced by different vendors, potentially enabling\nimproved accuracy of downstream computational algorithms for diagnosis and\ndetection of breast cancer. All the data used in this study are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:04:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 15:58:15 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Modanwal", "Gourav", ""], ["Vellal", "Adithya", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1912.08077", "submitter": "Diogo Luvizon", "authors": "Diogo C Luvizon, Hedi Tabia, David Picard", "title": "Multi-task Deep Learning for Real-Time 3D Human Pose Estimation and\n  Action Recognition", "comments": "Accepted to TPAMI. arXiv admin note: text overlap with\n  arXiv:1802.09232", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2976014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation and action recognition are related tasks since both\nproblems are strongly dependent on the human body representation and analysis.\nNonetheless, most recent methods in the literature handle the two problems\nseparately. In this work, we propose a multi-task framework for jointly\nestimating 2D or 3D human poses from monocular color images and classifying\nhuman actions from video sequences. We show that a single architecture can be\nused to solve both problems in an efficient way and still achieves\nstate-of-the-art or comparable results at each task while running at more than\n100 frames per second. The proposed method benefits from high parameters\nsharing between the two tasks by unifying still images and video clips\nprocessing in a single pipeline, allowing the model to be trained with data\nfrom different categories simultaneously and in a seamlessly way. Additionally,\nwe provide important insights for end-to-end training the proposed multi-task\nmodel by decoupling key prediction parts, which consistently leads to better\naccuracy on both tasks. The reported results on four datasets (MPII, Human3.6M,\nPenn Action and NTU RGB+D) demonstrate the effectiveness of our method on the\ntargeted tasks. Our source code and trained weights are publicly available at\nhttps://github.com/dluvizon/deephar.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 04:14:54 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 02:10:31 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Luvizon", "Diogo C", ""], ["Tabia", "Hedi", ""], ["Picard", "David", ""]]}, {"id": "1912.08113", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Brian K.\n  Spears", "title": "Improved Surrogates in Inertial Confinement Fusion with Manifold and\n  Cycle Consistencies", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become very popular in surrogate modeling because of\ntheir ability to characterize arbitrary, high dimensional functions in a data\ndriven fashion. This paper advocates for the training of surrogates that are\nconsistent with the physical manifold -- i.e., predictions are always\nphysically meaningful, and are cyclically consistent -- i.e., when the\npredictions of the surrogate, when passed through an independently trained\ninverse model give back the original input parameters. We find that these two\nconsistencies lead to surrogates that are superior in terms of predictive\nperformance, more resilient to sampling artifacts, and tend to be more data\nefficient. Using Inertial Confinement Fusion (ICF) as a test bed problem, we\nmodel a 1D semi-analytic numerical simulator and demonstrate the effectiveness\nof our approach. Code and data are available at\nhttps://github.com/rushilanirudh/macc/\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:14:43 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Bremer", "Peer-Timo", ""], ["Spears", "Brian K.", ""]]}, {"id": "1912.08136", "submitter": "Yan Luo", "authors": "Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, and Qi Zhao", "title": "Direction Concentration Learning: Enhancing Congruency in Machine\n  Learning", "comments": "This is a preprint and the formal version has been published in TPAMI", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2019.2963387", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the well-known challenges in computer vision tasks is the visual\ndiversity of images, which could result in an agreement or disagreement between\nthe learned knowledge and the visual content exhibited by the current\nobservation. In this work, we first define such an agreement in a concepts\nlearning process as congruency. Formally, given a particular task and\nsufficiently large dataset, the congruency issue occurs in the learning process\nwhereby the task-specific semantics in the training data are highly varying. We\npropose a Direction Concentration Learning (DCL) method to improve congruency\nin the learning process, where enhancing congruency influences the convergence\npath to be less circuitous. The experimental results show that the proposed DCL\nmethod generalizes to state-of-the-art models and optimizers, as well as\nimproves the performances of saliency prediction task, continual learning task,\nand classification task. Moreover, it helps mitigate the catastrophic\nforgetting problem in the continual learning task. The code is publicly\navailable at https://github.com/luoyan407/congruency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:58:04 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 01:28:05 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Luo", "Yan", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan S.", ""], ["Zhao", "Qi", ""]]}, {"id": "1912.08140", "submitter": "Yashaswi Verma", "authors": "Yashaswi Verma", "title": "An Embarrassingly Simple Baseline for eXtreme Multi-label Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of eXtreme Multi-label Learning (XML) is to design and learn a model\nthat can automatically annotate a given data point with the most relevant\nsubset of labels from an extremely large label set. Recently, many techniques\nhave been proposed for XML that achieve reasonable performance on benchmark\ndatasets. Motivated by the complexities of these methods and their subsequent\ntraining requirements, in this paper we propose a simple baseline technique for\nthis task. Precisely, we present a global feature embedding technique for XML\nthat can easily scale to very large datasets containing millions of data points\nin very high-dimensional feature space, irrespective of number of samples and\nlabels. Next we show how an ensemble of such global embeddings can be used to\nachieve further boost in prediction accuracies with only linear increase in\ntraining and prediction time. During testing, we assign the labels using a\nweighted k-nearest neighbour classifier in the embedding space. Experiments\nreveal that though conceptually simple, this technique achieves quite\ncompetitive results, and has training time of less than one minute using a\nsingle CPU core with 15.6 GB RAM even for large-scale datasets such as\nAmazon-3M.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:11:17 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Verma", "Yashaswi", ""]]}, {"id": "1912.08142", "submitter": "Daniel C. Castro", "authors": "Daniel C. Castro, Ian Walker, Ben Glocker", "title": "Causality matters in medical imaging", "comments": "20 pages, 5 figures, 4 tables", "journal-ref": "Nature Communications 11 (2020) 3673", "doi": "10.1038/s41467-020-17478-w", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses how the language of causality can shed new light on\nthe major challenges in machine learning for medical imaging: 1) data scarcity,\nwhich is the limited availability of high-quality annotations, and 2) data\nmismatch, whereby a trained algorithm may fail to generalize in clinical\npractice. Looking at these challenges through the lens of causality allows\ndecisions about data collection, annotation procedures, and learning strategies\nto be made (and scrutinized) more transparently. We discuss how causal\nrelationships between images and annotations can not only have profound effects\non the performance of predictive models, but may even dictate which learning\nstrategies should be considered in the first place. For example, we conclude\nthat semi-supervision may be unsuitable for image segmentation---one of the\npossibly surprising insights from our causal analysis, which is illustrated\nwith representative real-world examples of computer-aided diagnosis (skin\nlesion classification in dermatology) and radiotherapy (automated contouring of\ntumours). We highlight that being aware of and accounting for the causal\nrelationships in medical imaging data is important for the safe development of\nmachine learning and essential for regulation and responsible reporting. To\nfacilitate this we provide step-by-step recommendations for future studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:15:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Castro", "Daniel C.", ""], ["Walker", "Ian", ""], ["Glocker", "Ben", ""]]}, {"id": "1912.08166", "submitter": "Matthew Walmer", "authors": "A. Braunegg, Amartya Chakraborty, Michael Krumdick, Nicole Lape, Sara\n  Leary, Keith Manville, Elizabeth Merkhofer, Laura Strickhart, Matthew Walmer", "title": "APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection", "comments": "23 pages, 14 figures, 3 tables. Updated version as accepted to ECCV\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical adversarial attacks threaten to fool object detection systems, but\nreproducible research on the real-world effectiveness of physical patches and\nhow to defend against them requires a publicly available benchmark dataset. We\npresent APRICOT, a collection of over 1,000 annotated photographs of printed\nadversarial patches in public locations. The patches target several object\ncategories for three COCO-trained detection models, and the photos represent\nnatural variation in position, distance, lighting conditions, and viewing\nangle. Our analysis suggests that maintaining adversarial robustness in\nuncontrolled settings is highly challenging, but it is still possible to\nproduce targeted detections under white-box and sometimes black-box settings.\nWe establish baselines for defending against adversarial patches through\nseveral methods, including a detector supervised with synthetic data and\nunsupervised methods such as kernel density estimation, Bayesian uncertainty,\nand reconstruction error. Our results suggest that adversarial patches can be\neffectively flagged, both in a high-knowledge, attack-specific scenario, and in\nan unsupervised setting where patches are detected as anomalies in natural\nimages. This dataset and the described experiments provide a benchmark for\nfuture research on the effectiveness of and defenses against physical\nadversarial objects in the wild.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:08:01 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 21:37:23 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Braunegg", "A.", ""], ["Chakraborty", "Amartya", ""], ["Krumdick", "Michael", ""], ["Lape", "Nicole", ""], ["Leary", "Sara", ""], ["Manville", "Keith", ""], ["Merkhofer", "Elizabeth", ""], ["Strickhart", "Laura", ""], ["Walmer", "Matthew", ""]]}, {"id": "1912.08167", "submitter": "Matteo Rucco", "authors": "Matteo Rucco and Lorenzo Falsetti and Giovanna Viticchi", "title": "Towards personalized diagnosis of Glioblastoma in Fluid-attenuated\n  inversion recovery (FLAIR) by topological interpretable machine learning", "comments": "22 pages; 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioblastoma multiforme (GBM) is a fast-growing and highly invasive brain\ntumour, it tends to occur in adults between the ages of 45 and 70 and it\naccounts for 52 percent of all primary brain tumours. Usually, GBMs are\ndetected by magnetic resonance images (MRI). Among MRI, Fluid-attenuated\ninversion recovery (FLAIR) sequence produces high quality digital tumour\nrepresentation. Fast detection and segmentation techniques are needed for\novercoming subjective medical doctors (MDs) judgment. In the present\ninvestigation, we intend to demonstrate by means of numerical experiments that\ntopological features combined with textural features can be enrolled for GBM\nanalysis and morphological characterization on FLAIR. To this extent, we have\nperformed three numerical experiments. In the first experiment, Topological\nData Analysis (TDA) of a simplified 2D tumour growth mathematical model had\nallowed to understand the bio-chemical conditions that facilitate tumour\ngrowth: the higher the concentration of chemical nutrients the more virulent\nthe process. In the second experiment topological data analysis was used for\nevaluating GBM temporal progression on FLAIR recorded within 90 days following\ntreatment (e.g., chemo-radiation therapy - CRT) completion and at progression.\nThe experiment had confirmed that persistent entropy is a viable statistics for\nmonitoring GBM evolution during the follow-up period. In the third experiment\nwe had developed a novel methodology based on topological and textural features\nand automatic interpretable machine learning for automatic GBM classification\non FLAIR. The algorithm reached a classification accuracy up to the 97%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:09:50 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 17:27:38 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 09:45:16 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2020 10:00:44 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2020 10:11:09 GMT"}, {"version": "v6", "created": "Sat, 18 Apr 2020 18:51:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rucco", "Matteo", ""], ["Falsetti", "Lorenzo", ""], ["Viticchi", "Giovanna", ""]]}, {"id": "1912.08178", "submitter": "Aneesh Rangnekar", "authors": "Aneesh Rangnekar, Nilay Mokashi, Emmett Ientilucci, Christopher Kanan,\n  Matthew J. Hoffman", "title": "AeroRIT: A New Scene for Hyperspectral Image Analysis", "comments": "To appear in IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2020.2987199", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate applying convolutional neural network (CNN) architecture to\nfacilitate aerial hyperspectral scene understanding and present a new\nhyperspectral dataset-AeroRIT-that is large enough for CNN training. To date\nthe majority of hyperspectral airborne have been confined to various\nsub-categories of vegetation and roads and this scene introduces two new\ncategories: buildings and cars. To the best of our knowledge, this is the first\ncomprehensive large-scale hyperspectral scene with nearly seven million pixel\nannotations for identifying cars, roads, and buildings. We compare the\nperformance of three popular architectures - SegNet, U-Net, and Res-U-Net, for\nscene understanding and object identification via the task of dense semantic\nsegmentation to establish a benchmark for the scene. To further strengthen the\nnetwork, we add squeeze and excitation blocks for better channel interactions\nand use self-supervised learning for better encoder initialization. Aerial\nhyperspectral image analysis has been restricted to small datasets with limited\ntrain/test splits capabilities and we believe that AeroRIT will help advance\nthe research in the field with a more complex object distribution to perform\nwell on. The full dataset, with flight lines in radiance and reflectance\ndomain, is available for download at https://github.com/aneesh3108/AeroRIT.\nThis dataset is the first step towards developing robust algorithms for\nhyperspectral airborne sensing that can robustly perform advanced tasks like\nvehicle tracking and occlusion handling.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:31:56 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 20:23:01 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 23:18:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rangnekar", "Aneesh", ""], ["Mokashi", "Nilay", ""], ["Ientilucci", "Emmett", ""], ["Kanan", "Christopher", ""], ["Hoffman", "Matthew J.", ""]]}, {"id": "1912.08193", "submitter": "Alexander Kirillov", "authors": "Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick", "title": "PointRend: Image Segmentation as Rendering", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for efficient high-quality image segmentation of\nobjects and scenes. By analogizing classical computer graphics methods for\nefficient rendering with over- and undersampling challenges faced in pixel\nlabeling tasks, we develop a unique perspective of image segmentation as a\nrendering problem. From this vantage, we present the PointRend (Point-based\nRendering) neural network module: a module that performs point-based\nsegmentation predictions at adaptively selected locations based on an iterative\nsubdivision algorithm. PointRend can be flexibly applied to both instance and\nsemantic segmentation tasks by building on top of existing state-of-the-art\nmodels. While many concrete implementations of the general idea are possible,\nwe show that a simple design already achieves excellent results. Qualitatively,\nPointRend outputs crisp object boundaries in regions that are over-smoothed by\nprevious methods. Quantitatively, PointRend yields significant gains on COCO\nand Cityscapes, for both instance and semantic segmentation. PointRend's\nefficiency enables output resolutions that are otherwise impractical in terms\nof memory or computation compared to existing approaches. Code has been made\navailable at\nhttps://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:57:02 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 23:18:43 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kirillov", "Alexander", ""], ["Wu", "Yuxin", ""], ["He", "Kaiming", ""], ["Girshick", "Ross", ""]]}, {"id": "1912.08195", "submitter": "Luca Weihs", "authors": "Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M Pratt, Winson\n  Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali\n  Farhadi", "title": "Learning Generalizable Visual Representations via Interactive Gameplay", "comments": "Replaced with version accepted to ICLR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of research suggests that embodied gameplay, prevalent not\njust in human cultures but across a variety of animal species including turtles\nand ravens, is critical in developing the neural flexibility for creative\nproblem solving, decision making, and socialization. Comparatively little is\nknown regarding the impact of embodied gameplay upon artificial agents. While\nrecent work has produced agents proficient in abstract games, these\nenvironments are far removed from the real world and thus these agents can\nprovide little insight into the advantages of embodied play. Hiding games, such\nas hide-and-seek, played universally, provide a rich ground for studying the\nimpact of embodied gameplay on representation learning in the context of\nperspective taking, secret keeping, and false belief understanding. Here we are\nthe first to show that embodied adversarial reinforcement learning agents\nplaying Cache, a variant of hide-and-seek, in a high fidelity, interactive,\nenvironment, learn generalizable representations of their observations encoding\ninformation such as object permanence, free space, and containment. Moving\ncloser to biologically motivated learning strategies, our agents'\nrepresentations, enhanced by intentionality and memory, are developed through\ninteraction and play. These results serve as a model for studying how facets of\nvision develop through interaction, provide an experimental framework for\nassessing what is learned by artificial agents, and demonstrates the value of\nmoving from large, static, datasets towards experiential, interactive,\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:57:50 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 17:45:41 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 17:51:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Weihs", "Luca", ""], ["Kembhavi", "Aniruddha", ""], ["Ehsani", "Kiana", ""], ["Pratt", "Sarah M", ""], ["Han", "Winson", ""], ["Herrasti", "Alvaro", ""], ["Kolve", "Eric", ""], ["Schwenk", "Dustin", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1912.08197", "submitter": "Sungwon Han", "authors": "Sungwon Han, Donghyun Ahn, Hyunji Cha, Jeasurk Yang, Sungwon Park,\n  Meeyoung Cha", "title": "Lightweight and Robust Representation of Economic Scales from Satellite\n  Imagery", "comments": "Accepted for oral presentation at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite imagery has long been an attractive data source that provides a\nwealth of information on human-inhabited areas. While super resolution\nsatellite images are rapidly becoming available, little study has focused on\nhow to extract meaningful information about human habitation patterns and\neconomic scales from such data. We present READ, a new approach for obtaining\nessential spatial representation for any given district from high-resolution\nsatellite imagery based on deep neural networks. Our method combines transfer\nlearning and embedded statistics to efficiently learn critical spatial\ncharacteristics of arbitrary size areas and represent them into a fixed-length\nvector with minimal information loss. Even with a small set of labels, READ can\ndistinguish subtle differences between rural and urban areas and infer the\ndegree of urbanization. An extensive evaluation demonstrates the model\noutperforms the state-of-the-art in predicting economic scales, such as\npopulation density for South Korea (R^2=0.9617), and shows a high potential use\nfor developing countries where district-level economic scales are not known.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:53:01 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Han", "Sungwon", ""], ["Ahn", "Donghyun", ""], ["Cha", "Hyunji", ""], ["Yang", "Jeasurk", ""], ["Park", "Sungwon", ""], ["Cha", "Meeyoung", ""]]}, {"id": "1912.08226", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara", "title": "Meshed-Memory Transformer for Image Captioning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based architectures represent the state of the art in sequence\nmodeling tasks like machine translation and language understanding. Their\napplicability to multi-modal contexts like image captioning, however, is still\nlargely under-explored. With the aim of filling this gap, we present M$^2$ - a\nMeshed Transformer with Memory for Image Captioning. The architecture improves\nboth the image encoding and the language generation steps: it learns a\nmulti-level representation of the relationships between image regions\nintegrating learned a priori knowledge, and uses a mesh-like connectivity at\ndecoding stage to exploit low- and high-level features. Experimentally, we\ninvestigate the performance of the M$^2$ Transformer and different\nfully-attentive models in comparison with recurrent ones. When tested on COCO,\nour proposal achieves a new state of the art in single-model and ensemble\nconfigurations on the \"Karpathy\" test split and on the online test server. We\nalso assess its performances when describing objects unseen in the training\nset. Trained models and code for reproducing the experiments are publicly\navailable at: https://github.com/aimagelab/meshed-memory-transformer.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:03:23 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 19:29:14 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Cornia", "Marcella", ""], ["Stefanini", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1912.08240", "submitter": "Tarang Chugh", "authors": "Tarang Chugh and Anil K. Jain", "title": "Fingerprint Spoof Detection: Temporal Analysis of Image Sequence", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize the dynamics involved in the imaging of a fingerprint on a\ntouch-based fingerprint reader, such as perspiration, changes in skin color\n(blanching), and skin distortion, to differentiate real fingers from spoof\n(fake) fingers. Specifically, we utilize a deep learning-based architecture\n(CNN-LSTM) trained end-to-end using sequences of minutiae-centered local\npatches extracted from ten color frames captured on a COTS fingerprint reader.\nA time-distributed CNN (MobileNet-v1) extracts spatial features from each local\npatch, while a bi-directional LSTM layer learns the temporal relationship\nbetween the patches in the sequence. Experimental results on a database of\n26,650 live frames from 685 subjects (1,333 unique fingers), and 32,910 spoof\nframes of 7 spoof materials (with 14 variants) shows the superiority of the\nproposed approach in both known-material and cross-material (generalization)\nscenarios. For instance, the proposed approach improves the state-of-the-art\ncross-material performance from TDR of 81.65% to 86.20% @ FDR = 0.2%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:23:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chugh", "Tarang", ""], ["Jain", "Anil K.", ""]]}, {"id": "1912.08263", "submitter": "Tobias Feigl", "authors": "Felix Ott, Tobias Feigl, Christoffer L\\\"offler, and Christopher\n  Mutschler", "title": "ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization", "comments": "Conf. on Computer Vision and Pattern Recognition (CVPR): Joint\n  Workshop on Long-Term Visual Localization, Visual Odometry and Geometric and\n  Learning-based SLAM 2020", "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00029", "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Odometry (VO) accumulates a positional drift in long-term robot\nnavigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in\nvarious aspects, VO still suffers from moving obstacles, discontinuous\nobservation of features, and poor textures or visual information. While recent\napproaches estimate a 6DoF pose either directly from (a series of) images or by\nmerging depth maps with optical flow (OF), research that combines absolute pose\nregression with OF is limited. We propose ViPR, a novel modular architecture\nfor long-term 6DoF VO that leverages temporal information and synergies between\nabsolute pose estimates (from PoseNet-like modules) and relative pose estimates\n(from FlowNet-based modules) by combining both through recurrent layers.\nExperiments on known datasets and on our own Industry dataset show that our\nmodular design outperforms state of the art in long-term navigation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 20:29:15 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 09:25:10 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 15:20:45 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ott", "Felix", ""], ["Feigl", "Tobias", ""], ["L\u00f6ffler", "Christoffer", ""], ["Mutschler", "Christopher", ""]]}, {"id": "1912.08265", "submitter": "Jiteng Mu", "authors": "Jiteng Mu, Weichao Qiu, Gregory Hager, Alan Yuille", "title": "Learning from Synthetic Animals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great success in human parsing, progress for parsing other deformable\narticulated objects, like animals, is still limited by the lack of labeled\ndata. In this paper, we use synthetic images and ground truth generated from\nCAD animal models to address this challenge. To bridge the domain gap between\nreal and synthetic images, we propose a novel consistency-constrained\nsemi-supervised learning method (CC-SSL). Our method leverages both spatial and\ntemporal consistencies, to bootstrap weak models trained on synthetic data with\nunlabeled real images. We demonstrate the effectiveness of our method on highly\ndeformable animals, such as horses and tigers. Without using any real image\nlabel, our method allows for accurate keypoint prediction on real images.\nMoreover, we quantitatively show that models using synthetic data achieve\nbetter generalization performance than models trained on real images across\ndifferent domains in the Visual Domain Adaptation Challenge dataset. Our\nsynthetic dataset contains 10+ animals with diverse poses and rich ground\ntruth, which enables us to use the multi-task learning strategy to further\nboost models' performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 20:45:45 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 18:59:56 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mu", "Jiteng", ""], ["Qiu", "Weichao", ""], ["Hager", "Gregory", ""], ["Yuille", "Alan", ""]]}, {"id": "1912.08275", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Mehrtash Harandi, Chandra Sekhar Chellu", "title": "A Probabilistic approach for Learning Embeddings without Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For challenging machine learning problems such as zero-shot learning and\nfine-grained categorization, embedding learning is the machinery of choice\nbecause of its ability to learn generic notions of similarity, as opposed to\nclass-specific concepts in standard classification models. Embedding learning\naims at learning discriminative representations of data such that similar\nexamples are pulled closer, while pushing away dissimilar ones. Despite their\nexemplary performances, supervised embedding learning approaches require huge\nnumber of annotations for training. This restricts their applicability for\nlarge datasets in new applications where obtaining labels require extensive\nmanual efforts and domain knowledge. In this paper, we propose to learn an\nembedding in a completely unsupervised manner without using any class labels.\nUsing a graph-based clustering approach to obtain pseudo-labels, we form\ntriplet-based constraints following a metric learning paradigm. Our novel\nembedding learning approach uses a probabilistic notion, that intuitively\nminimizes the chances of each triplet violating a geometric constraint. Due to\nnature of the search space, we learn the parameters of our approach using\nRiemannian geometry. Our proposed approach performs competitive to\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:14:08 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Harandi", "Mehrtash", ""], ["Chellu", "Chandra Sekhar", ""]]}, {"id": "1912.08283", "submitter": "Dmitry Utyamishev", "authors": "Dmitry Utyamishev, Inna Partin-Vaisband", "title": "Progressive VAE Training on Highly Sparse and Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for training a Variational\nAutoencoder (VAE) on a highly imbalanced data set. The proposed training of a\nhigh-resolution VAE model begins with the training of a low-resolution core\nmodel, which can be successfully trained on imbalanced data set. In subsequent\ntraining steps, new convolutional, upsampling, deconvolutional, and\ndownsampling layers are iteratively attached to the model. In each iteration,\nthe additional layers are trained based on the intermediate pretrained model -\na result of previous training iterations. Thus, the resolution of the model is\nprogressively increased up to the required resolution level. In this paper, the\nprogressive VAE training is exploited for learning a latent representation with\nimbalanced, highly sparse data sets and, consequently, generating routes in a\nconstrained 2D space. Routing problems (e.g., vehicle routing problem,\ntravelling salesman problem, and arc routing) are of special significance in\nmany modern applications (e.g., route planning, network maintenance, developing\nhigh-performance nanoelectronic systems, and others) and typically associated\nwith sparse imbalanced data. In this paper, the critical problem of routing\nbillions of components in nanoelectronic devices is considered. The proposed\napproach exhibits a significant training speedup as compared with\nstate-of-the-art existing VAE training methods, while generating expected image\noutputs from unseen input data. Furthermore, the final progressive VAE models\nexhibit much more precise output representation, than the Generative\nAdversarial Network (GAN) models trained with comparable training time. The\nproposed method is expected to be applicable to a wide range of applications,\nincluding but not limited image impainting, sentence interpolation, and\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:38:51 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Utyamishev", "Dmitry", ""], ["Partin-Vaisband", "Inna", ""]]}, {"id": "1912.08324", "submitter": "Kai Arulkumaran", "authors": "Tianhong Dai, Kai Arulkumaran, Tamara Gerbert, Samyakh Tukra, Feryal\n  Behbahani, Anil Anthony Bharath", "title": "Analysing Deep Reinforcement Learning Agents Trained with Domain\n  Randomisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has the potential to train robots to perform\ncomplex tasks in the real world without requiring accurate models of the robot\nor its environment. A practical approach is to train agents in simulation, and\nthen transfer them to the real world. One popular method for achieving\ntransferability is to use domain randomisation, which involves randomly\nperturbing various aspects of a simulated environment in order to make trained\nagents robust to the reality gap. However, less work has gone into\nunderstanding such agents - which are deployed in the real world - beyond task\nperformance. In this work we examine such agents, through qualitative and\nquantitative comparisons between agents trained with and without visual domain\nrandomisation. We train agents for Fetch and Jaco robots on a visuomotor\ncontrol task and evaluate how well they generalise using different testing\nconditions. Finally, we investigate the internals of the trained agents by\nusing a suite of interpretability techniques. Our results show that the primary\noutcome of domain randomisation is more robust, entangled representations,\naccompanied with larger weights with greater spatial structure; moreover, the\ntypes of changes are heavily influenced by the task setup and presence of\nadditional proprioceptive inputs. Additionally, we demonstrate that our domain\nrandomised agents require higher sample complexity, can overfit and more\nheavily rely on recurrent processing. Furthermore, even with an improved\nsaliency method introduced in this work, we show that qualitative studies may\nnot always correspond with quantitative measures, necessitating the combination\nof inspection tools in order to provide sufficient insights into the behaviour\nof trained agents.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 00:18:17 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 20:53:30 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dai", "Tianhong", ""], ["Arulkumaran", "Kai", ""], ["Gerbert", "Tamara", ""], ["Tukra", "Samyakh", ""], ["Behbahani", "Feryal", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1912.08329", "submitter": "Jiayu Yang", "authors": "Jiayu Yang, Wei Mao, Jose M. Alvarez, Miaomiao Liu", "title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "comments": "CVPR 2020 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cost volume-based neural network for depth inference from\nmulti-view images. We demonstrate that building a cost volume pyramid in a\ncoarse-to-fine manner instead of constructing a cost volume at a fixed\nresolution leads to a compact, lightweight network and allows us inferring high\nresolution depth maps to achieve better reconstruction results. To this end, we\nfirst build a cost volume based on uniform sampling of fronto-parallel planes\nacross the entire depth range at the coarsest resolution of an image. Then,\ngiven current depth estimate, we construct new cost volumes iteratively on the\npixelwise depth residual to perform depth map refinement. While sharing similar\ninsight with Point-MVSNet as predicting and refining depth iteratively, we show\nthat working on cost volume pyramid can lead to a more compact, yet efficient\nnetwork structure compared with the Point-MVSNet on 3D points. We further\nprovide detailed analyses of the relation between (residual) depth sampling and\nimage resolution, which serves as a principle for building compact cost volume\npyramid. Experimental results on benchmark datasets show that our model can\nperform 6x faster and has similar performance as state-of-the-art methods. Code\nis available at https://github.com/JiayuYANG/CVP-MVSNet\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 00:48:00 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 09:54:44 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 03:32:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yang", "Jiayu", ""], ["Mao", "Wei", ""], ["Alvarez", "Jose M.", ""], ["Liu", "Miaomiao", ""]]}, {"id": "1912.08350", "submitter": "Makena Low", "authors": "Makena Low, Priyanka Raina", "title": "Automating Vitiligo Skin Lesion Segmentation Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several skin conditions such as vitiligo, accurate segmentation of\nlesions from skin images is the primary measure of disease progression and\nseverity. Existing methods for vitiligo lesion segmentation require manual\nintervention. Unfortunately, manual segmentation is time and labor-intensive,\nas well as irreproducible between physicians. We introduce a convolutional\nneural network (CNN) that quickly and robustly performs vitiligo skin lesion\nsegmentation. Our CNN has a U-Net architecture with a modified contracting\npath. We use the CNN to generate an initial segmentation of the lesion, then\nrefine it by running the watershed algorithm on high-confidence pixels. We\ntrain the network on 247 images with a variety of lesion sizes, complexity, and\nanatomical sites. The network with our modifications noticeably outperforms the\nstate-of-the-art U-Net, with a Jaccard Index (JI) score of 73.6% (compared to\n36.7%). Moreover, our method requires only a few seconds for segmentation, in\ncontrast with the previously proposed semi-autonomous watershed approach, which\nrequires 2-29 minutes per image.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:15:44 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Low", "Makena", ""], ["Raina", "Priyanka", ""]]}, {"id": "1912.08364", "submitter": "Jeya Maria Jose Valanarasu", "authors": "Jeya Maria Jose V., Rajeev Yasarla, Puyang Wang, Ilker Hacihaliloglu,\n  Vishal M. Patel", "title": "Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of anatomical landmarks from ultrasound (US) plays an\nimportant role in the management of preterm neonates with a very low birth\nweight due to the increased risk of developing intraventricular hemorrhage\n(IVH) or other complications. One major problem in developing an automatic\nsegmentation method for this task is the limited availability of annotated\ndata. To tackle this issue, we propose a novel image synthesis method using\nmulti-scale self attention generator to synthesize US images from various\nsegmentation masks. We show that our method can synthesize high-quality US\nimages for every manipulated segmentation label with qualitative and\nquantitative improvements over the recent state-of-the-art synthesis methods.\nFurthermore, for the segmentation task, we propose a novel method, called\nConfidence-guided Brain Anatomy Segmentation (CBAS) network, where segmentation\nand corresponding confidence maps are estimated at different scales. In\naddition, we introduce a technique which guides CBAS to learn the weights based\non the confidence measure about the estimate. Extensive experiments demonstrate\nthat the proposed method for both synthesis and segmentation tasks achieve\nsignificant improvements over the recent state-of-the-art methods. In\nparticular, we show that the new synthesis framework can be used to generate\nrealistic US images which can be used to improve the performance of a\nsegmentation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:29:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["V.", "Jeya Maria Jose", ""], ["Yasarla", "Rajeev", ""], ["Wang", "Puyang", ""], ["Hacihaliloglu", "Ilker", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1912.08367", "submitter": "Xiwen Li", "authors": "Zhenhua Chen, Xiwen Li, Chuhua Wang, David Crandall", "title": "P-CapsNets: a General Form of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Pure CapsNets (P-CapsNets) which is a generation of normal CNNs\nstructurally. Specifically, we make three modifications to current CapsNets.\nFirst, we remove routing procedures from CapsNets based on the observation that\nthe coupling coefficients can be learned implicitly. Second, we replace the\nconvolutional layers in CapsNets to improve efficiency. Third, we package the\ncapsules into rank-3 tensors to further improve efficiency. The experiment\nshows that P-CapsNets achieve better performance than CapsNets with varied\nrouting procedures by using significantly fewer parameters on MNIST\\&CIFAR10.\nThe high efficiency of P-CapsNets is even comparable to some deep compressing\nmodels. For example, we achieve more than 99\\% percent accuracy on MNIST by\nusing only 3888 parameters. We visualize the capsules as well as the\ncorresponding correlation matrix to show a possible way of initializing\nCapsNets in the future. We also explore the adversarial robustness of\nP-CapsNets compared to CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:45:17 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Zhenhua", ""], ["Li", "Xiwen", ""], ["Wang", "Chuhua", ""], ["Crandall", "David", ""]]}, {"id": "1912.08375", "submitter": "YeongHyeon Park", "authors": "YeongHyeon Park, Il Dong Yun, Si-Hyuck Kang", "title": "The CNN-based Coronary Occlusion Site Localization with Effective\n  Preprocessing Method", "comments": null, "journal-ref": null, "doi": "10.1002/tee.23225", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronary Artery Occlusion (CAO) acutely comes to human, and it highly\nthreats the human's life. When CAO detected, Percutaneous Coronary Intervention\n(PCI) should be conducted timely. Before PCI, localizing the CAO is needed\nfirstly, because the heart is covered with various arteries. We handle the\nthree kinds of CAO in this paper and our purpose is not only localization of\nCAO but also improving the localizing performance via preprocessing method. We\nimprove localization performance from a minimum of 0.150 to a maximum of 0.372\nvia our noise reduction and pulse extraction based method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 04:44:39 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 04:30:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Park", "YeongHyeon", ""], ["Yun", "Il Dong", ""], ["Kang", "Si-Hyuck", ""]]}, {"id": "1912.08387", "submitter": "Chengjiang Long", "authors": "Bhavan Vasu, Chengjiang Long", "title": "Iterative and Adaptive Sampling with Spatial Attention for Black-Box\n  Model Explanations", "comments": "The paper was accepted to the IEEE Winter Conference on Applications\n  of Computer Vision (WACV'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved great success in many real-world\napplications, yet it remains unclear and difficult to explain their\ndecision-making process to an end-user. In this paper, we address the\nexplainable AI problem for deep neural networks with our proposed framework,\nnamed IASSA, which generates an importance map indicating how salient each\npixel is for the model's prediction with an iterative and adaptive sampling\nmodule. We employ an affinity matrix calculated on multi-level deep learning\nfeatures to explore long-range pixel-to-pixel correlation, which can shift the\nsaliency values guided by our long-range and parameter-free spatial attention.\nExtensive experiments on the MS-COCO dataset show that our proposed approach\nmatches or exceeds the performance of state-of-the-art black-box explanation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:37:30 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Vasu", "Bhavan", ""], ["Long", "Chengjiang", ""]]}, {"id": "1912.08393", "submitter": "Changqun Xia", "authors": "Jia Li, Jinming Su, Changqun Xia, Mingcan Ma and Yonghong Tian", "title": "Salient Object Detection with Purificatory Mechanism and Structural\n  Similarity Loss", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the aid of attention mechanisms to weight the image features adaptively,\nrecent advanced deep learning-based models encourage the predicted results to\napproximate the ground-truth masks with as large predictable areas as possible,\nthus achieving the state-of-the-art performance. However, these methods do not\npay enough attention to small areas prone to misprediction. In this way, it is\nstill tough to accurately locate salient objects due to the existence of\nregions with indistinguishable foreground and background and regions with\ncomplex or fine structures. To address these problems, we propose a novel\nconvolutional neural network with purificatory mechanism and structural\nsimilarity loss. Specifically, in order to better locate preliminary salient\nobjects, we first introduce the promotion attention, which is based on spatial\nand channel attention mechanisms to promote attention to salient regions.\nSubsequently, for the purpose of restoring the indistinguishable regions that\ncan be regarded as error-prone regions of one model, we propose the\nrectification attention, which is learned from the areas of wrong prediction\nand guide the network to focus on error-prone regions thus rectifying errors.\nThrough these two attentions, we use the Purificatory Mechanism to impose\nstrict weights with different regions of the whole salient objects and purify\nresults from hard-to-distinguish regions, thus accurately predicting the\nlocations and details of salient objects. In addition to paying different\nattention to these hard-to-distinguish regions, we also consider the structural\nconstraints on complex regions and propose the Structural Similarity Loss. In\nexperiments, the proposed approach outperforms 19 state-of-the-art methods on\nsix datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:49:37 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 10:19:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Jia", ""], ["Su", "Jinming", ""], ["Xia", "Changqun", ""], ["Ma", "Mingcan", ""], ["Tian", "Yonghong", ""]]}, {"id": "1912.08394", "submitter": "Andreas W. Kempa-Liehr", "authors": "Andreas W. Kempa-Liehr and Jonty Oram and Andrew Wong and Mark Finch\n  and Thor Besier", "title": "Feature engineering workflow for activity recognition from synchronized\n  inertial measurement units", "comments": "Multi-Sensor for Action and Gesture Recognition (MAGR), ACPR 2019\n  Workshop, Auckland, New Zealand", "journal-ref": null, "doi": "10.1007/978-981-15-3651-9_20", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous availability of wearable sensors is responsible for driving\nthe Internet-of-Things but is also making an impact on sport sciences and\nprecision medicine. While human activity recognition from smartphone data or\nother types of inertial measurement units (IMU) has evolved to one of the most\nprominent daily life examples of machine learning, the underlying process of\ntime-series feature engineering still seems to be time-consuming. This lengthy\nprocess inhibits the development of IMU-based machine learning applications in\nsport science and precision medicine. This contribution discusses a feature\nengineering workflow, which automates the extraction of time-series feature on\nbased on the FRESH algorithm (FeatuRe Extraction based on Scalable Hypothesis\ntests) to identify statistically significant features from synchronized IMU\nsensors (IMeasureU Ltd, NZ). The feature engineering workflow has five main\nsteps: time-series engineering, automated time-series feature extraction,\noptimized feature extraction, fitting of a specialized classifier, and\ndeployment of optimized machine learning pipeline. The workflow is discussed\nfor the case of a user-specific running-walking classification, and the\ngeneralization to a multi-user multi-activity classification is demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:51:42 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kempa-Liehr", "Andreas W.", ""], ["Oram", "Jonty", ""], ["Wong", "Andrew", ""], ["Finch", "Mark", ""], ["Besier", "Thor", ""]]}, {"id": "1912.08395", "submitter": "Da Chen", "authors": "Da Chen, Yongliang Yang, Zunlei Feng, Xiang Wu, Mingli Song, Wenbin\n  Li, Yuan He, Hui Xue, Feng Mao", "title": "Semantic Regularization: Improve Few-shot Image Classification by\n  Reducing Meta Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification requires the classifier to robustly cope with\nunseen classes even if there are only a few samples for each class. Recent\nadvances benefit from the meta-learning process where episodic tasks are formed\nto train a model that can adapt to class change. However, these task sare\nindependent to each other and existing works mainly rely on limited samples of\nindividual support set in a single meta task. This strategy leads to severe\nmeta shift issues across multiple tasks, meaning the learned prototypes or\nclass descriptors are not stable as each task only involves their own support\nset. To avoid this problem, we propose a concise Semantic RegularizationNetwork\nto learn a common semantic space under the framework of meta-learning. In this\nspace, all class descriptors can be regularized by the learned semantic basis,\nwhich can effectively solve the meta shift problem. The key is to train a class\nencoder and decoder structure that can encode the sample embedding features\ninto the semantic domain with trained semantic basis, and generate a more\nstable and general class descriptor from the decoder. We evaluate our work by\nextensive comparisons with previous methods on three benchmark datasets\n(MiniImageNet, TieredImageNet, and CUB). The results show that the semantic\nregularization module improves performance by 4%-7% over the baseline method,\nand achieves competitive results over the current state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:54:49 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 15:37:41 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Da", ""], ["Yang", "Yongliang", ""], ["Feng", "Zunlei", ""], ["Wu", "Xiang", ""], ["Song", "Mingli", ""], ["Li", "Wenbin", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""], ["Mao", "Feng", ""]]}, {"id": "1912.08435", "submitter": "Sangwoo Cho", "authors": "Sangwoo Cho, Muhammad Hasan Maqbool, Fei Liu, Hassan Foroosh", "title": "Self-Attention Network for Skeleton-based Human Action Recognition", "comments": "WACV 2020 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition has recently attracted a lot of attention.\nResearchers are coming up with new approaches for extracting spatio-temporal\nrelations and making considerable progress on large-scale skeleton-based\ndatasets. Most of the architectures being proposed are based upon recurrent\nneural networks (RNNs), convolutional neural networks (CNNs) and graph-based\nCNNs. When it comes to skeleton-based action recognition, the importance of\nlong term contextual information is central which is not captured by the\ncurrent architectures. In order to come up with a better representation and\ncapturing of long term spatio-temporal relationships, we propose three variants\nof Self-Attention Network (SAN), namely, SAN-V1, SAN-V2 and SAN-V3. Our SAN\nvariants has the impressive capability of extracting high-level semantics by\ncapturing long-range correlations. We have also integrated the Temporal Segment\nNetwork (TSN) with our SAN variants which resulted in improved overall\nperformance. Different configurations of Self-Attention Network (SAN) variants\nand Temporal Segment Network (TSN) are explored with extensive experiments. Our\nchosen configuration outperforms state-of-the-art Top-1 and Top-5 by 4.4% and\n7.9% respectively on Kinetics and shows consistently better performance than\nstate-of-the-art methods on NTU RGB+D.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:07:07 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Cho", "Sangwoo", ""], ["Maqbool", "Muhammad Hasan", ""], ["Liu", "Fei", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1912.08444", "submitter": "Yichuan Charlie Tang", "authors": "Lionel Blond\\'e, Yichuan Charlie Tang, Jian Zhang, Russ Webb", "title": "Relational Mimic for Visual Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new method for imitation learning from video\ndemonstrations. Our method, Relational Mimic (RM), improves on previous visual\nimitation learning methods by combining generative adversarial networks and\nrelational learning. RM is flexible and can be used in conjunction with other\nrecent advances in generative adversarial imitation learning to better address\nthe need for more robust and sample-efficient approaches. In addition, we\nintroduce a new neural network architecture that improves upon the previous\nstate-of-the-art in reinforcement learning and illustrate how increasing the\nrelational reasoning capabilities of the agent enables the latter to achieve\nincreasingly higher performance in a challenging locomotion task with pixel\ninputs. Finally, we study the effects and contributions of relational learning\nin policy evaluation, policy improvement and reward learning through ablation\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:19:39 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Blond\u00e9", "Lionel", ""], ["Tang", "Yichuan Charlie", ""], ["Zhang", "Jian", ""], ["Webb", "Russ", ""]]}, {"id": "1912.08487", "submitter": "Georg Krispel", "authors": "Georg Krispel, Michael Opitz, Georg Waltner, Horst Possegger, Horst\n  Bischof", "title": "FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data", "comments": "Accepted for publication in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple yet effective fusion method of LiDAR and RGB data to\nsegment LiDAR point clouds. Utilizing the dense native range representation of\na LiDAR sensor and the setup calibration, we establish point correspondences\nbetween the two input modalities. Subsequently, we are able to warp and fuse\nthe features from one domain into the other. Therefore, we can jointly exploit\ninformation from both data sources within one single network. To show the merit\nof our method, we extend SqueezeSeg, a point cloud segmentation network, with\nan RGB feature branch and fuse it into the original structure. Our extension\ncalled FuseSeg leads to an improvement of up to 18% IoU on the KITTI benchmark.\nIn addition to the improved accuracy, we also achieve real-time performance at\n50 fps, five times as fast as the KITTI LiDAR data recording speed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:51:13 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 14:52:46 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Krispel", "Georg", ""], ["Opitz", "Michael", ""], ["Waltner", "Georg", ""], ["Possegger", "Horst", ""], ["Bischof", "Horst", ""]]}, {"id": "1912.08519", "submitter": "Sathyaprakash Narayanan", "authors": "Yeshwanth Ravi Theja Bethi, Sathyaprakash Narayanan, Venkat Rangan,\n  Chetan Singh Thakur", "title": "Real-Time Object Detection and Localization in Compressive Sensed Video\n  on Embedded Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day around the world, interminable terabytes of data are being captured\nfor surveillance purposes. A typical 1-2MP CCTV camera generates around 7-12GB\nof data per day. Frame-by-frame processing of such enormous amount of data\nrequires hefty computational resources. In recent years, compressive sensing\napproaches have shown impressive results in signal processing by reducing the\nsampling bandwidth. Different sampling mechanisms were developed to incorporate\ncompressive sensing in image and video acquisition. Pixel-wise coded exposure\nis one among the promising sensing paradigms for capturing videos in the\ncompressed domain, which was also realized into an all-CMOS sensor\n\\cite{Xiong2017}. Though cameras that perform compressive sensing save a lot of\nbandwidth at the time of sampling and minimize the memory required to store\nvideos, we cannot do much in terms of processing until the videos are\nreconstructed to the original frames. But, the reconstruction of\ncompressive-sensed (CS) videos still takes a lot of time and is also\ncomputationally expensive. In this work, we show that object detection and\nlocalization can be possible directly on the CS frames (easily upto 20x\ncompression). To our knowledge, this is the first time that the problem of\nobject detection and localization on CS frames has been attempted. Hence, we\nalso created a dataset for training in the CS domain. We were able to achieve a\ngood accuracy of 46.27\\% mAP(Mean Average Precision) with the proposed model\nwith an inference time of 23ms directly on the compressed frames(approx. 20\noriginal domain frames), this facilitated for real-time inference which was\nverified on NVIDIA TX2 embedded board. Our framework will significantly reduce\nthe communication bandwidth, and thus reduction in power as the video\ncompression will be done at the image sensor processing core.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:07:12 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 14:50:00 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 04:46:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bethi", "Yeshwanth Ravi Theja", ""], ["Narayanan", "Sathyaprakash", ""], ["Rangan", "Venkat", ""], ["Thakur", "Chetan Singh", ""]]}, {"id": "1912.08521", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Sadegh Aliakbarian, Fatemeh Sadat Saleh, Lars Petersson, Stephen\n  Gould, Mathieu Salzmann", "title": "Contextually Plausible and Diverse 3D Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of diverse 3D human motion prediction, that is,\nforecasting multiple plausible future 3D poses given a sequence of observed 3D\nposes. In this context, a popular approach consists of using a Conditional\nVariational Autoencoder (CVAE). However, existing approaches that do so either\nfail to capture the diversity in human motion, or generate diverse but\nsemantically implausible continuations of the observed motion. In this paper,\nwe address both of these problems by developing a new variational framework\nthat accounts for both diversity and context of the generated future motion. To\nthis end, and in contrast to existing approaches, we condition the sampling of\nthe latent variable that acts as source of diversity on the representation of\nthe past observation, thus encouraging it to carry relevant information. Our\nexperiments demonstrate that our approach yields motions not only of higher\nquality while retaining diversity, but also that preserve the contextual\ninformation contained in the observed 3D pose sequence.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:13:44 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 13:16:24 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 01:29:31 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2020 08:59:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Aliakbarian", "Sadegh", ""], ["Saleh", "Fatemeh Sadat", ""], ["Petersson", "Lars", ""], ["Gould", "Stephen", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1912.08531", "submitter": "Lianghua Huang Dr.", "authors": "Lianghua Huang, Xin Zhao and Kaiqi Huang", "title": "GlobalTrack: A Simple and Strong Baseline for Long-term Tracking", "comments": "Accepted in AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key capability of a long-term tracker is to search for targets in very\nlarge areas (typically the entire image) to handle possible target absences or\ntracking failures. However, currently there is a lack of such a strong baseline\nfor global instance search. In this work, we aim to bridge this gap.\nSpecifically, we propose GlobalTrack, a pure global instance search based\ntracker that makes no assumption on the temporal consistency of the target's\npositions and scales. GlobalTrack is developed based on two-stage object\ndetectors, and it is able to perform full-image and multi-scale search of\narbitrary instances with only a single query as the guide. We further propose a\ncross-query loss to improve the robustness of our approach against distractors.\nWith no online learning, no punishment on position or scale changes, no scale\nsmoothing and no trajectory refinement, our pure global instance search based\ntracker achieves comparable, sometimes much better performance on four\nlarge-scale tracking benchmarks (i.e., 52.1% AUC on LaSOT, 63.8% success rate\non TLP, 60.3% MaxGM on OxUvA and 75.4% normalized precision on TrackingNet),\ncompared to state-of-the-art approaches that typically require complex\npost-processing. More importantly, our tracker runs without cumulative errors,\ni.e., any type of temporary tracking failures will not affect its performance\non future frames, making it ideal for long-term tracking. We hope this work\nwill be a strong baseline for long-term tracking and will stimulate future\nworks in this area. Code is available at\nhttps://github.com/huanglianghua/GlobalTrack.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:31:19 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Huang", "Lianghua", ""], ["Zhao", "Xin", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1912.08541", "submitter": "Uehwan Kim", "authors": "In-Ug Yoon, Ue-Hwan Kim and Jong-Hwan", "title": "s-DRN: Stabilized Developmental Resonance Network", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online incremental clustering of sequentially incoming data without prior\nknowledge suffers from changing cluster numbers and tends to fall into local\nextrema according to given data order. To overcome these limitations, we\npropose a stabilized developmental resonance network (s-DRN). First, we analyze\nthe instability of the conventional choice function during the node activation\nprocess and design a scalable activation function to make clustering\nperformance stable over all input data scales. Next, we devise three criteria\nfor the node grouping algorithm: distance, intersection over union (IoU) and\nsize criteria. The proposed node grouping algorithm effectively excludes\nunnecessary clusters from incrementally created clusters, diminishes the\nperformance dependency on vigilance parameters and makes the clustering process\nrobust. To verify the performance of the proposed s-DRN model, comparative\nstudies are conducted on six real-world datasets whose statistical\ncharacteristics are distinctive. The comparative studies demonstrate the\nproposed s-DRN outperforms baselines in terms of stability and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:47:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:06:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yoon", "In-Ug", ""], ["Kim", "Ue-Hwan", ""], ["Jong-Hwan", "", ""]]}, {"id": "1912.08562", "submitter": "Jiadong Liang", "authors": "Jiadong Liang and Wenjie Pei and Feng Lu", "title": "CPGAN: Full-Spectrum Content-Parsing Generative Adversarial Networks for\n  Text-to-Image Synthesis", "comments": "18 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical methods for text-to-image synthesis seek to design effective\ngenerative architecture to model the text-to-image mapping directly. It is\nfairly arduous due to the cross-modality translation. In this paper we\ncircumvent this problem by focusing on parsing the content of both the input\ntext and the synthesized image thoroughly to model the text-to-image\nconsistency in the semantic level. Particularly, we design a memory structure\nto parse the textual content by exploring semantic correspondence between each\nword in the vocabulary to its various visual contexts across relevant images\nduring text encoding. Meanwhile, the synthesized image is parsed to learn its\nsemantics in an object-aware manner. Moreover, we customize a conditional\ndiscriminator to model the fine-grained correlations between words and image\nsub-regions to push for the text-image semantic alignment. Extensive\nexperiments on COCO dataset manifest that our model advances the\nstate-of-the-art performance significantly (from 35.69 to 52.73 in Inception\nScore).\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:31:42 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 10:20:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Jiadong", ""], ["Pei", "Wenjie", ""], ["Lu", "Feng", ""]]}, {"id": "1912.08568", "submitter": "Sena Kiciroglu", "authors": "Sena Kiciroglu, Helge Rhodin, Sudipta N. Sinha, Mathieu Salzmann,\n  Pascal Fua", "title": "ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion\n  Capture", "comments": "For associated video, see https://youtu.be/i58Bu-hbZHs Published in\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of monocular 3D human pose estimation depends on the viewpoint\nfrom which the image is captured. While freely moving cameras, such as on\ndrones, provide control over this viewpoint, automatically positioning them at\nthe location which will yield the highest accuracy remains an open problem.\nThis is the problem that we address in this paper. Specifically, given a short\nvideo sequence, we introduce an algorithm that predicts which viewpoints should\nbe chosen to capture future frames so as to maximize 3D human pose estimation\naccuracy. The key idea underlying our approach is a method to estimate the\nuncertainty of the 3D body pose estimates. We integrate several sources of\nuncertainty, originating from deep learning based regressors and temporal\nsmoothness. Our motion planner yields improved 3D body pose estimates and\noutperforms or matches existing ones that are based on person following and\norbiting.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:44:31 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 07:45:16 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Kiciroglu", "Sena", ""], ["Rhodin", "Helge", ""], ["Sinha", "Sudipta N.", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1912.08577", "submitter": "Fang Aiqing", "authors": "Aiqing Fang and Xinbo Zhao and Jiaqi Yang and Yanning Zhang", "title": "A Cross-Modal Image Fusion Method Guided by Human Visual Characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characteristics of feature selection, nonlinear combination and\nmulti-task auxiliary learning mechanism of the human visual perception system\nplay an important role in real-world scenarios, but the research of image\nfusion theory based on the characteristics of human visual perception is less.\nInspired by the characteristics of human visual perception, we propose a robust\nmulti-task auxiliary learning optimization image fusion theory. Firstly, we\ncombine channel attention model with nonlinear convolutional neural network to\nselect features and fuse nonlinear features. Then, we analyze the impact of the\nexisting image fusion loss on the image fusion quality, and establish the\nmulti-loss function model of unsupervised learning network. Secondly, aiming at\nthe multi-task auxiliary learning mechanism of human visual perception system,\nwe study the influence of multi-task auxiliary learning mechanism on image\nfusion task on the basis of single task multi-loss network model. By simulating\nthe three characteristics of human visual perception system, the fused image is\nmore consistent with the mechanism of human brain image fusion. Finally, in\norder to verify the superiority of our algorithm, we carried out experiments on\nthe combined vision system image data set, and extended our algorithm to the\ninfrared and visible image and the multi-focus image public data set for\nexperimental verification. The experimental results demonstrate the superiority\nof our fusion theory over state-of-arts in generality and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:07:20 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 10:59:55 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 00:51:11 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2020 09:43:38 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Yang", "Jiaqi", ""], ["Zhang", "Yanning", ""]]}, {"id": "1912.08628", "submitter": "Hongruixuan Chen", "authors": "Chen Wu, Hongruixuan Chen, Bo Do, Liangpei Zhang", "title": "Unsupervised Change Detection in Multi-temporal VHR Images Based on Deep\n  Kernel PCA Convolutional Mapping Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Earth observation technology, very-high-resolution\n(VHR) image has become an important data source of change detection. Nowadays,\ndeep learning methods have achieved conspicuous performance in the change\ndetection of VHR images. Nonetheless, most of the existing change detection\nmodels based on deep learning require annotated training samples. In this\npaper, a novel unsupervised model called kernel principal component analysis\n(KPCA) convolution is proposed for extracting representative features from\nmulti-temporal VHR images. Based on the KPCA convolution, an unsupervised deep\nsiamese KPCA convolutional mapping network (KPCA-MNet) is designed for binary\nand multi-class change detection. In the KPCA-MNet, the high-level\nspatial-spectral feature maps are extracted by a deep siamese network\nconsisting of weight-shared PCA convolution layers. Then, the change\ninformation in the feature difference map is mapped into a 2-D polar domain.\nFinally, the change detection results are generated by threshold segmentation\nand clustering algorithms. All procedures of KPCA-MNet does not require labeled\ndata. The theoretical analysis and experimental results demonstrate the\nvalidity, robustness, and potential of the proposed method in two binary change\ndetection data sets and one multi-class change detection data set.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:20:11 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Wu", "Chen", ""], ["Chen", "Hongruixuan", ""], ["Do", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1912.08639", "submitter": "Pingchuan Ma", "authors": "Pingchuan Ma, Stavros Petridis, Maja Pantic", "title": "Detecting Adversarial Attacks On Audiovisual Speech Recognition", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks pose a threat to deep learning models. However, research\non adversarial detection methods, especially in the multi-modal domain, is very\nlimited. In this work, we propose an efficient and straightforward detection\nmethod based on the temporal correlation between audio and video streams. The\nmain idea is that the correlation between audio and video in adversarial\nexamples will be lower than benign examples due to added adversarial noise. We\nuse the synchronisation confidence score as a proxy for audiovisual correlation\nand based on it we can detect adversarial attacks. To the best of our\nknowledge, this is the first work on detection of adversarial attacks on\naudiovisual speech recognition models. We apply recent adversarial attacks on\ntwo audiovisual speech recognition models trained on the GRID and LRW datasets.\nThe experimental results demonstrate that the proposed approach is an effective\nway for detecting such attacks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:43:43 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 17:44:49 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1912.08642", "submitter": "HongYu Liu", "authors": "Hongyu Liu and Bin Jiang and Wei Huang and Chao Yang", "title": "One-Stage Inpainting with Bilateral Attention and Pyramid Filling Block", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based image inpainting methods which utilize contextual\ninformation and two-stage architecture have exhibited remarkable performance.\nHowever, the two-stage architecture is time-consuming, the contextual\ninformation lack high-level semantics and ignores both the semantic relevance\nand distance information of hole's feature patches, these limitations result in\nblurry textures and distorted structures of final result. Motivated by these\nobservations, we propose a new deep generative model-based approach, which\ntrains a shared network twice with different targets and utilizes a single\nnetwork during the testing phase, so that we can effectively save inference\ntime. Specifically, the targets of two training steps are structure\nreconstruction and texture generation respectively. During the second training,\nwe first propose a Pyramid Filling Block (PF-block) to utilize the high-level\nfeatures that the hole regions has been filled to guide the filling process of\nlow-level features progressively, the missing content can be filled from deep\nto shallow in a pyramid fashion. Then, inspired by the classical bilateral\nfilter [30], we propose the Bilateral Attention layer (BA-layer) to optimize\nfilled feature map, which synthesizes feature patches at each position by\ncomputing weighted sums of the surrounding feature patches, these weights are\nderived by considering both distance and value relationships between feature\npatches, thus making the visually plausible inpainting results. Finally,\nexperiments on multiple publicly available datasets show the superior\nperformance of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:46:06 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Liu", "Hongyu", ""], ["Jiang", "Bin", ""], ["Huang", "Wei", ""], ["Yang", "Chao", ""]]}, {"id": "1912.08644", "submitter": "Leonardo Andr\\'es Espinosa Leal", "authors": "Leonardo Espinosa Leal, Kaj-Mikael Bj\\\"ork, Amaury Lendasse, Anton\n  Akusok", "title": "A Web Page Classifier Library Based on Random Image Content Analysis\n  Using Deep Learning", "comments": "4 pages, 3 figures. Proceedings of the 11th PErvasive Technologies\n  Related to Assistive Environments Conference. ACM, 2018", "journal-ref": null, "doi": "10.1145/3197768.3201525", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a methodology and the corresponding Python library\n1 for the classification of webpages. Our method retrieves a fixed number of\nimages from a given webpage, and based on them classifies the webpage into a\nset of established classes with a given probability. The library trains a\nrandom forest model build upon the features extracted from images by a\npre-trained deep network. The implementation is tested by recognizing weapon\nclass webpages in a curated list of 3859 websites. The results show that the\nbest method of classifying a webpage into the studies classes is to assign the\nclass according to the maximum probability of any image belonging to this\n(weapon) class being above the threshold, across all the retrieved images.\nFurther research explores the possibilities for the developed methodology to\nalso apply in image classification for healthcare applications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:50:18 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Leal", "Leonardo Espinosa", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Lendasse", "Amaury", ""], ["Akusok", "Anton", ""]]}, {"id": "1912.08661", "submitter": "Tianrui Liu", "authors": "Tianrui Liu, Wenhan Luo, Lin Ma, Jun-Jie Huang, Tania Stathaki, and\n  Tianhong Dai", "title": "Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer\n  Feature Extraction and Deformable Occlusion Handling", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection methods have been significantly improved with the\ndevelopment of deep convolutional neural networks. Nevertheless, detecting\nsmall-scaled pedestrians and occluded pedestrians remains a challenging\nproblem. In this paper, we propose a pedestrian detection method with a\ncouple-network to simultaneously address these two issues. One of the\nsub-networks, the gated multi-layer feature extraction sub-network, aims to\nadaptively generate discriminative features for pedestrian candidates in order\nto robustly detect pedestrians with large variations on scales. The second\nsub-network targets in handling the occlusion problem of pedestrian detection\nby using deformable regional RoI-pooling. We investigate two different gate\nunits for the gated sub-network, namely, the channel-wise gate unit and the\nspatio-wise gate unit, which can enhance the representation ability of the\nregional convolutional features among the channel dimensions or across the\nspatial domain, repetitively. Ablation studies have validated the effectiveness\nof both the proposed gated multi-layer feature extraction sub-network and the\ndeformable occlusion handling sub-network. With the coupled framework, our\nproposed pedestrian detector achieves state-of-the-art results on the Caltech\nand the CityPersons pedestrian detection benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:28:03 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Liu", "Tianrui", ""], ["Luo", "Wenhan", ""], ["Ma", "Lin", ""], ["Huang", "Jun-Jie", ""], ["Stathaki", "Tania", ""], ["Dai", "Tianhong", ""]]}, {"id": "1912.08679", "submitter": "Xavier Rafael-Palou", "authors": "Ilaria Bonavita, Xavier Rafael-Palou, Mario Ceresa, Gemma Piella,\n  Vicent Ribas, Miguel A. Gonz\\'alez Ballester", "title": "Integration of Convolutional Neural Networks for Pulmonary Nodule\n  Malignancy Assessment in a Lung Cancer Classification Pipeline", "comments": "26 pages, 5 figures", "journal-ref": "Computer Methods and Programs in Biomedicine,\n  volume=185,number=105172, pages=1-9, year=2019, publisher=Elsevier", "doi": "10.1016/j.cmpb.2019.105172", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early identification of malignant pulmonary nodules is critical for\nbetter lung cancer prognosis and less invasive chemo or radio therapies. Nodule\nmalignancy assessment done by radiologists is extremely useful for planning a\npreventive intervention but is, unfortunately, a complex, time-consuming and\nerror-prone task. This explains the lack of large datasets containing\nradiologists malignancy characterization of nodules. In this article, we\npropose to assess nodule malignancy through 3D convolutional neural networks\nand to integrate it in an automated end-to-end existing pipeline of lung cancer\ndetection. For training and testing purposes we used independent subsets of the\nLIDC dataset. Adding the probabilities of nodules malignity in a baseline lung\ncancer pipeline improved its F1-weighted score by 14.7%, whereas integrating\nthe malignancy model itself using transfer learning outperformed the baseline\nprediction by 11.8% of F1-weighted score. Despite the limited size of the lung\ncancer datasets, integrating predictive models of nodule malignancy improves\nprediction of lung cancer.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:56:29 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Bonavita", "Ilaria", ""], ["Rafael-Palou", "Xavier", ""], ["Ceresa", "Mario", ""], ["Piella", "Gemma", ""], ["Ribas", "Vicent", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""]]}, {"id": "1912.08741", "submitter": "Diego Ortego", "authors": "Diego Ortego, Eric Arazo, Paul Albert, Noel E. O'Connor and Kevin\n  McGuinness", "title": "Towards Robust Learning with Different Label Noise Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy labels are an unavoidable consequence of labeling processes and\ndetecting them is an important step towards preventing performance degradations\nin Convolutional Neural Networks. Discarding noisy labels avoids a harmful\nmemorization, while the associated image content can still be exploited in a\nsemi-supervised learning (SSL) setup. Clean samples are usually identified\nusing the small loss trick, i.e. they exhibit a low loss. However, we show that\ndifferent noise distributions make the application of this trick less\nstraightforward and propose to continuously relabel all images to reveal a\ndiscriminative loss against multiple distributions. SSL is then applied twice,\nonce to improve the clean-noisy detection and again for training the final\nmodel. We design an experimental setup based on ImageNet32/64 for better\nunderstanding the consequences of representation learning with differing label\nnoise distributions and find that non-uniform out-of-distribution noise better\nresembles real-world noise and that in most cases intermediate features are not\naffected by label noise corruption. Experiments in CIFAR-10/100, ImageNet32/64\nand WebVision (real-world noise) demonstrate that the proposed label noise\nDistribution Robust Pseudo-Labeling (DRPL) approach gives substantial\nimprovements over recent state-of-the-art. Code is available at\nhttps://git.io/JJ0PV.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:12:29 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:23:32 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 11:32:45 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ortego", "Diego", ""], ["Arazo", "Eric", ""], ["Albert", "Paul", ""], ["O'Connor", "Noel E.", ""], ["McGuinness", "Kevin", ""]]}, {"id": "1912.08765", "submitter": "Muhammed Talo", "authors": "Muhammed Talo", "title": "An Automated Deep Learning Approach for Bacterial Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automated recognition and classification of bacteria species from microscopic\nimages have significant importance in clinical microbiology. Bacteria\nclassification is usually carried out manually by biologists using different\nshapes and morphologic characteristics of bacteria species. The manual taxonomy\nof bacteria types from microscopy images is time-consuming and a challenging\ntask for even experienced biologists. In this study, an automated deep learning\nbased classification approach has been proposed to classify bacterial images\ninto different categories. The ResNet-50 pre-trained CNN architecture has been\nused to classify digital bacteria images into 33 categories. The transfer\nlearning technique was employed to accelerate the training process of the\nnetwork and improve the classification performance of the network. The proposed\nmethod achieved an average classification accuracy of 99.2%. The experimental\nresults demonstrate that the proposed technique surpasses state-of-the-art\nmethods in the literature and can be used for any type of bacteria\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:38:31 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Talo", "Muhammed", ""]]}, {"id": "1912.08766", "submitter": "Varun Nair", "authors": "Varun Nair, Javier Fuentes Alonso, Tony Beltramelli", "title": "RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms", "comments": "Code available at https://github.com/uizard-technologies/realmix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) algorithms have shown great potential in\ntraining regimes when access to labeled data is scarce but access to unlabeled\ndata is plentiful. However, our experiments illustrate several shortcomings\nthat prior SSL algorithms suffer from. In particular, poor performance when\nunlabeled and labeled data distributions differ. To address these observations,\nwe develop RealMix, which achieves state-of-the-art results on standard\nbenchmark datasets across different labeled and unlabeled set sizes while\novercoming the aforementioned challenges. Notably, RealMix achieves an error\nrate of 9.79% on CIFAR10 with 250 labels and is the only SSL method tested able\nto surpass baseline performance when there is significant mismatch in the\nlabeled and unlabeled data distributions. RealMix demonstrates how SSL can be\nused in real world situations with limited access to both data and compute and\nguides further research in SSL with practical applicability in mind.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:03:28 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nair", "Varun", ""], ["Alonso", "Javier Fuentes", ""], ["Beltramelli", "Tony", ""]]}, {"id": "1912.08775", "submitter": "Darvin Yi", "authors": "Darvin Yi, Endre Gr{\\o}vik, Michael Iv, Elizabeth Tong, Kyrre Eeg\n  Emblem, Line Brennhaug Nilsen, Cathrine Saxhaug, Anna Latysheva, Kari Dolven\n  Jacobsen, {\\AA}slaug Helland, Greg Zaharchuk, Daniel Rubin", "title": "MRI Pulse Sequence Integration for Deep-Learning Based Brain Metastasis\n  Segmentation", "comments": "In the IEEE transactions format for submission to IEEE-TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance (MR) imaging is an essential diagnostic tool in clinical\nmedicine. Recently, a variety of deep learning methods have been applied to\nsegmentation tasks in medical images, with promising results for computer-aided\ndiagnosis. For MR images, effectively integrating different pulse sequences is\nimportant to optimize performance. However, the best way to integrate different\npulse sequences remains unclear. In this study, we evaluate multiple\narchitectural features and characterize their effects in the task of metastasis\nsegmentation. Specifically, we consider (1) different pulse sequence\nintegration schemas, (2) different modes of weight sharing for parallel network\nbranches, and (3) a new approach for enabling robustness to missing pulse\nsequences. We find that levels of integration and modes of weight sharing that\nfavor low variance work best in our regime of small data (n = 100). By adding\nan input-level dropout layer, we could preserve the overall performance of\nthese networks while allowing for inference on inputs with missing pulse\nsequence. We illustrate not only the generalizability of the network but also\nthe utility of this robustness when applying the trained model to data from a\ndifferent center, which does not use the same pulse sequences. Finally, we\napply network visualization methods to better understand which input features\nare most important for network performance. Together, these results provide a\nframework for building networks with enhanced robustness to missing data while\nmaintaining comparable performance in medical imaging applications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:12:20 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Yi", "Darvin", ""], ["Gr\u00f8vik", "Endre", ""], ["Iv", "Michael", ""], ["Tong", "Elizabeth", ""], ["Emblem", "Kyrre Eeg", ""], ["Nilsen", "Line Brennhaug", ""], ["Saxhaug", "Cathrine", ""], ["Latysheva", "Anna", ""], ["Jacobsen", "Kari Dolven", ""], ["Helland", "\u00c5slaug", ""], ["Zaharchuk", "Greg", ""], ["Rubin", "Daniel", ""]]}, {"id": "1912.08795", "submitter": "Hongxu Yin", "authors": "Hongxu Yin, Pavlo Molchanov, Zhizhong Li, Jose M. Alvarez, Arun\n  Mallya, Derek Hoiem, Niraj K. Jha and Jan Kautz", "title": "Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepInversion, a new method for synthesizing images from the\nimage distribution used to train a deep neural network. We 'invert' a trained\nnetwork (teacher) to synthesize class-conditional input images starting from\nrandom noise, without using any additional information about the training\ndataset. Keeping the teacher fixed, our method optimizes the input while\nregularizing the distribution of intermediate feature maps using information\nstored in the batch normalization layers of the teacher. Further, we improve\nthe diversity of synthesized images using Adaptive DeepInversion, which\nmaximizes the Jensen-Shannon divergence between the teacher and student network\nlogits. The resulting synthesized images from networks trained on the CIFAR-10\nand ImageNet datasets demonstrate high fidelity and degree of realism, and help\nenable a new breed of data-free applications - ones that do not require any\nreal images or labeled data. We demonstrate the applicability of our proposed\nmethod to three tasks of immense practical importance -- (i) data-free network\npruning, (ii) data-free knowledge transfer, and (iii) data-free continual\nlearning. Code is available at https://github.com/NVlabs/DeepInversion\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:50:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:30:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yin", "Hongxu", ""], ["Molchanov", "Pavlo", ""], ["Li", "Zhizhong", ""], ["Alvarez", "Jose M.", ""], ["Mallya", "Arun", ""], ["Hoiem", "Derek", ""], ["Jha", "Niraj K.", ""], ["Kautz", "Jan", ""]]}, {"id": "1912.08804", "submitter": "Olivia Wiles", "authors": "Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson", "title": "SynSin: End-to-end View Synthesis from a Single Image", "comments": "Project page: www.robots.ox.ac.uk/~ow/synsin.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image view synthesis allows for the generation of new views of a scene\ngiven a single input image. This is challenging, as it requires comprehensively\nunderstanding the 3D scene from a single image. As a result, current methods\ntypically use multiple images, train on ground-truth depth, or are limited to\nsynthetic data. We propose a novel end-to-end model for this task; it is\ntrained on real images without any ground-truth 3D information. To this end, we\nintroduce a novel differentiable point cloud renderer that is used to transform\na latent 3D point cloud of features into the target view. The projected\nfeatures are decoded by our refinement network to inpaint missing regions and\ngenerate a realistic output image. The 3D component inside of our generative\nmodel allows for interpretable manipulation of the latent feature space at test\ntime, e.g. we can animate trajectories from a single image. Unlike prior work,\nwe can generate high resolution images and generalise to other input\nresolutions. We outperform baselines and prior work on the Matterport, Replica,\nand RealEstate10K datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:59:04 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 16:14:02 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wiles", "Olivia", ""], ["Gkioxari", "Georgia", ""], ["Szeliski", "Richard", ""], ["Johnson", "Justin", ""]]}, {"id": "1912.08812", "submitter": "Teofilo de Campos", "authors": "Frederico Guth and Teofilo Emidio de-Campos", "title": "Research Frontiers in Transfer Learning -- a systematic and bibliometric\n  review", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn from very few samples, demonstrating an outstanding\ngeneralization ability that learning algorithms are still far from reaching.\nCurrently, the most successful models demand enormous amounts of well-labeled\ndata, which are expensive and difficult to obtain, becoming one of the biggest\nobstacles to the use of machine learning in practice. This scenario shows the\nmassive potential for Transfer Learning, which aims to harness previously\nacquired knowledge to the learning of new tasks more effectively and\nefficiently. In this systematic review, we apply a quantitative method to\nselect the main contributions to the field and make use of bibliographic\ncoupling metrics to identify research frontiers. We further analyze the\nlinguistic variation between the classics of the field and the frontier and map\npromising research directions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:08:19 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Guth", "Frederico", ""], ["de-Campos", "Teofilo Emidio", ""]]}, {"id": "1912.08813", "submitter": "Jos\\'e A. Ch\\'avez Alvarez", "authors": "Jos\\'e Ch\\'avez, Rensso Mora and Edward Cayllahua-Cahuina", "title": "Ambient Lighting Generation for Flash Images with Guided Conditional\n  Adversarial Networks", "comments": "VISAPP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the challenges that low light conditions produce in images,\nphotographers tend to use the light provided by the camera flash to get better\nillumination. Nevertheless, harsh shadows and non-uniform illumination can\narise from using a camera flash, especially in low light conditions. Previous\nstudies have focused on normalizing the lighting on flash images; however, to\nthe best of our knowledge, no prior studies have examined the sideways shadows\nremoval, reconstruction of overexposed areas, and the generation of synthetic\nambient shadows or natural tone of scene objects. To provide more natural\nillumination on flash images and ensure high-frequency details, we propose a\ngenerative adversarial network in a guided conditional mode. We show that this\napproach not only generates natural illumination but also attenuates harsh\nshadows, simultaneously generating synthetic ambient shadows. Our approach\nachieves promising results on a custom FAID dataset, outperforming our baseline\nstudies. We also analyze the components of our proposal and how they affect the\noverall performance and discuss the opportunities for future work.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:58:29 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 02:44:18 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 23:25:23 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 12:33:16 GMT"}, {"version": "v5", "created": "Thu, 20 Feb 2020 22:12:30 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ch\u00e1vez", "Jos\u00e9", ""], ["Mora", "Rensso", ""], ["Cayllahua-Cahuina", "Edward", ""]]}, {"id": "1912.08830", "submitter": "Dave Zhenyu Chen", "authors": "Dave Zhenyu Chen, Angel X. Chang, Matthias Nie{\\ss}ner", "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language", "comments": "Project page: https://daveredrum.github.io/ScanRefer/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of 3D object localization in RGB-D scans using natural\nlanguage descriptions. As input, we assume a point cloud of a scanned 3D scene\nalong with a free-form description of a specified target object. To address\nthis task, we propose ScanRefer, learning a fused descriptor from 3D object\nproposals and encoded sentence embeddings. This fused descriptor correlates\nlanguage expressions with geometric features, enabling regression of the 3D\nbounding box of a target object. We also introduce the ScanRefer dataset,\ncontaining 51,583 descriptions of 11,046 objects from 800 ScanNet scenes.\nScanRefer is the first large-scale effort to perform object localization via\nnatural language expression directly in 3D.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:00:49 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 21:41:53 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:33:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Dave Zhenyu", ""], ["Chang", "Angel X.", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.08837", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Jocelyn Chanussot and Naoto Yokoya and Jian Kang and\n  Xiao Xiang Zhu", "title": "Learning Shared Cross-modality Representation Using Multispectral-LiDAR\n  and Hyperspectral Data", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Letters, 2020", "doi": "10.1109/LGRS.2019.2944599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ever-growing diversity of the data source, multi-modality feature\nlearning has attracted more and more attention. However, most of these methods\nare designed by jointly learning feature representation from multi-modalities\nthat exist in both training and test sets, yet they are less investigated in\nabsence of certain modality in the test phase. To this end, in this letter, we\npropose to learn a shared feature space across multi-modalities in the training\nprocess. By this way, the out-of-sample from any of multi-modalities can be\ndirectly projected onto the learned space for a more effective cross-modality\nrepresentation. More significantly, the shared space is regarded as a latent\nsubspace in our proposed method, which connects the original multi-modal\nsamples with label information to further improve the feature discrimination.\nExperiments are conducted on the multispectral-Lidar and hyperspectral dataset\nprovided by the 2018 IEEE GRSS Data Fusion Contest to demonstrate the\neffectiveness and superiority of the proposed method in comparison with several\npopular baselines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:06:50 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 09:19:25 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Hong", "Danfeng", ""], ["Chanussot", "Jocelyn", ""], ["Yokoya", "Naoto", ""], ["Kang", "Jian", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1912.08847", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Xin Wu and Pedram Ghamisi and Jocelyn Chanussot and\n  Naoto Yokoya and Xiao Xiang Zhu", "title": "Invariant Attribute Profiles: A Spatial-Frequency Joint Feature\n  Extractor for Hyperspectral Image Classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2019.2957251", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up to the present, an enormous number of advanced techniques have been\ndeveloped to enhance and extract the spatially semantic information in\nhyperspectral image processing and analysis. However, locally semantic change,\nsuch as scene composition, relative position between objects, spectral\nvariability caused by illumination, atmospheric effects, and material mixture,\nhas been less frequently investigated in modeling spatial information. As a\nconsequence, identifying the same materials from spatially different scenes or\npositions can be difficult. In this paper, we propose a solution to address\nthis issue by locally extracting invariant features from hyperspectral imagery\n(HSI) in both spatial and frequency domains, using a method called invariant\nattribute profiles (IAPs). IAPs extract the spatial invariant features by\nexploiting isotropic filter banks or convolutional kernels on HSI and spatial\naggregation techniques (e.g., superpixel segmentation) in the Cartesian\ncoordinate system. Furthermore, they model invariant behaviors (e.g., shift,\nrotation) by the means of a continuous histogram of oriented gradients\nconstructed in a Fourier polar coordinate. This yields a combinatorial\nrepresentation of spatial-frequency invariant features with application to HSI\nclassification. Extensive experiments conducted on three promising\nhyperspectral datasets (Houston2013 and Houston2018) demonstrate the\nsuperiority and effectiveness of the proposed IAP method in comparison with\nseveral state-of-the-art profile-related techniques. The codes will be\navailable from the website:\nhttps://sites.google.com/view/danfeng-hong/data-code.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:27:07 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hong", "Danfeng", ""], ["Wu", "Xin", ""], ["Ghamisi", "Pedram", ""], ["Chanussot", "Jocelyn", ""], ["Yokoya", "Naoto", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1912.08852", "submitter": "Ziyun Wang", "authors": "Ziyun Wang, Volkan Isler, Daniel D. Lee", "title": "Surface HOF: Surface Reconstruction from a Single Image Using Higher\n  Order Function Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of generating a high-resolution surface reconstruction\nfrom a single image. Our approach is to learn a Higher Order Function (HOF)\nwhich takes an image of an object as input and generates a mapping function.\nThe mapping function takes samples from a canonical domain (e.g. the unit\nsphere) and maps each sample to a local tangent plane on the 3D reconstruction\nof the object. Each tangent plane is represented as an origin point and a\nnormal vector at that point. By efficiently learning a continuous mapping\nfunction, the surface can be generated at arbitrary resolution in contrast to\nother methods which generate fixed resolution outputs. We present the Surface\nHOF in which both the higher order function and the mapping function are\nrepresented as neural networks, and train the networks to generate\nreconstructions of PointNet objects. Experiments show that Surface HOF is more\naccurate and uses more efficient representations than other state of the art\nmethods for surface reconstruction. Surface HOF is also easier to train: it\nrequires minimal input pre-processing and output post-processing and generates\nsurface representations that are more parameter efficient. Its accuracy and\nconvenience make Surface HOF an appealing method for single image\nreconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:30:24 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Wang", "Ziyun", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1912.08855", "submitter": "Yue Yao", "authors": "Yue Yao, Liang Zheng, Xiaodong Yang, Milind Naphade and Tom Gedeon", "title": "Simulating Content Consistent Vehicle Datasets with Attribute Descent", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a graphic engine to simulate a large amount of training data\nwith free annotations. Between synthetic and real data, there is a two-level\ndomain gap, i.e., content level and appearance level. While the latter has been\nwidely studied, we focus on reducing the content gap in attributes like\nillumination and viewpoint. To reduce the problem complexity, we choose a\nsmaller and more controllable application, vehicle re-identification (re-ID).\nWe introduce a large-scale synthetic dataset VehicleX. Created in Unity, it\ncontains 1,362 vehicles of various 3D models with fully editable attributes. We\npropose an attribute descent approach to let VehicleX approximate the\nattributes in real-world datasets. Specifically, we manipulate each attribute\nin VehicleX, aiming to minimize the discrepancy between VehicleX and real data\nin terms of the Fr\\'echet Inception Distance (FID). This attribute descent\nalgorithm allows content domain adaptation (DA) orthogonal to existing\nappearance DA methods. We mix the optimized VehicleX data with real-world\nvehicle re-ID datasets, and observe consistent improvement. With the augmented\ndatasets, we report competitive accuracy. We make the dataset, engine and our\ncodes available at https://github.com/yorkeyao/VehicleX.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:40:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:00:03 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Yao", "Yue", ""], ["Zheng", "Liang", ""], ["Yang", "Xiaodong", ""], ["Naphade", "Milind", ""], ["Gedeon", "Tom", ""]]}, {"id": "1912.08860", "submitter": "Emmanuel Kahembwe", "authors": "Emmanuel Kahembwe, Subramanian Ramamoorthy", "title": "Lower Dimensional Kernels for Video Discriminators", "comments": null, "journal-ref": "Neural.Networks 132 (2020) 506-520", "doi": "10.1016/j.neunet.2020.09.016", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an analysis of the discriminators used in Generative\nAdversarial Networks (GANs) for Video. We show that unconstrained video\ndiscriminator architectures induce a loss surface with high curvature which\nmake optimisation difficult. We also show that this curvature becomes more\nextreme as the maximal kernel dimension of video discriminators increases. With\nthese observations in hand, we propose a family of efficient Lower-Dimensional\nVideo Discriminators for GANs (LDVD GANs). The proposed family of\ndiscriminators improve the performance of video GAN models they are applied to\nand demonstrate good performance on complex and diverse datasets such as\nUCF-101. In particular, we show that they can double the performance of\nTemporal-GANs and provide for state-of-the-art performance on a single GPU.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:54:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kahembwe", "Emmanuel", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1912.08866", "submitter": "James Harrison", "authors": "James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone", "title": "Continuous Meta-Learning without Tasks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:10:40 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 00:14:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Harrison", "James", ""], ["Sharma", "Apoorva", ""], ["Finn", "Chelsea", ""], ["Pavone", "Marco", ""]]}, {"id": "1912.08870", "submitter": "Ali Ghofrani", "authors": "Ali Ghofrani, Rahil Mahdian Toroghi, Seyed Mojtaba Tabatabaie", "title": "Attention-Based Face AntiSpoofing of RGB Images, using a Minimal\n  End-2-End Neural Network", "comments": "6 pages , 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing aims at identifying the real face, as well as the fake\none, and gains a high attention in security-sensitive applications, liveness\ndetection, fingerprinting, and so on. In this paper, we address the\nanti-spoofing problem by proposing two end-to-end systems of convolutional\nneural networks. One model is developed based on the EfficientNet B0 network\nwhich has been modified in the final dense layers. The second one, is a very\nlight model of the MobileNet V2, which has been contracted, modified and\nretrained efficiently on the data being created based on the Rose-Youtu\ndataset, for this purpose. The experiments show that, both of the proposed\narchitectures achieve remarkable results on detecting the real and fake images\nof the face input data. The experiments clearly show that the heavy-weight\nmodel could be efficiently employed in server-side implementations, whereas the\nlow-weight model could be easily implemented on the hand-held devices and both\nperform perfectly well using merely RGB input images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:15:27 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ghofrani", "Ali", ""], ["Toroghi", "Rahil Mahdian", ""], ["Tabatabaie", "Seyed Mojtaba", ""]]}, {"id": "1912.08883", "submitter": "Zhongnan Qu", "authors": "Zhongnan Qu, Zimu Zhou, Yun Cheng, Lothar Thiele", "title": "Adaptive Loss-aware Quantization for Multi-bit Networks", "comments": "To appear in CVPR 2020; Code available at\n  https://github.com/zqu1992/ALQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the compression of deep neural networks by quantizing their\nweights and activations into multiple binary bases, known as multi-bit networks\n(MBNs), which accelerate the inference and reduce the storage for the\ndeployment on low-resource mobile and embedded platforms. We propose Adaptive\nLoss-aware Quantization (ALQ), a new MBN quantization pipeline that is able to\nachieve an average bitwidth below one-bit without notable loss in inference\naccuracy. Unlike previous MBN quantization solutions that train a quantizer by\nminimizing the error to reconstruct full precision weights, ALQ directly\nminimizes the quantization-induced error on the loss function involving neither\ngradient approximation nor full precision maintenance. ALQ also exploits\nstrategies including adaptive bitwidth, smooth bitwidth reduction, and\niterative trained quantization to allow a smaller network size without loss in\naccuracy. Experiment results on popular image datasets show that ALQ\noutperforms state-of-the-art compressed networks in terms of both storage and\naccuracy. Code is available at https://github.com/zqu1992/ALQ\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:48:29 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:11:11 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 22:31:11 GMT"}, {"version": "v4", "created": "Sat, 4 Jul 2020 20:24:41 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Qu", "Zhongnan", ""], ["Zhou", "Zimu", ""], ["Cheng", "Yun", ""], ["Thiele", "Lothar", ""]]}, {"id": "1912.08905", "submitter": "Prithvijit Chakrabarty", "authors": "Prithvijit Chakrabarty, Subhransu Maji", "title": "The Spectral Bias of the Deep Image Prior", "comments": "Bayesian Deep Learning Workshop, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"deep image prior\" proposed by Ulyanov et al. is an intriguing property\nof neural nets: a convolutional encoder-decoder network can be used as a prior\nfor natural images. The network architecture implicitly introduces a bias; If\nwe train the model to map white noise to a corrupted image, this bias guides\nthe model to fit the true image before fitting the corrupted regions.\n  This paper explores why the deep image prior helps in denoising natural\nimages. We present a novel method to analyze trajectories generated by the deep\nimage prior optimization and demonstrate:\n  (i) convolution layers of the an encoder-decoder decouple the frequency\ncomponents of the image, learning each at different rates\n  (ii) the model fits lower frequencies first, making early stopping behave as\na low pass filter.\n  The experiments study an extension of Cheng et al which showed that at\ninitialization, the deep image prior is equivalent to a stationary Gaussian\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 21:51:58 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Chakrabarty", "Prithvijit", ""], ["Maji", "Subhransu", ""]]}, {"id": "1912.08914", "submitter": "Istv\\'an Ketyk\\'o", "authors": "Istv\\'an Ketyk\\'o and Ferenc Kov\\'acs", "title": "On the Metrics and Adaptation Methods for Domain Divergences of\n  sEMG-based Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new metric to measure domain divergence and a new domain\nadaptation method for time-series classification. The metric belongs to the\nclass of probability distributions-based metrics, is transductive, and does not\nassume the presence of source data samples. The 2-stage method utilizes an\nimproved autoregressive, RNN-based architecture with deep/non-linear\ntransformation. We assess our metric and the performance of our model in the\ncontext of sEMG/EMG-based gesture recognition under inter-session and\ninter-subject domain shifts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:12:53 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Ketyk\u00f3", "Istv\u00e1n", ""], ["Kov\u00e1cs", "Ferenc", ""]]}, {"id": "1912.08920", "submitter": "Sakshi Udeshi", "authors": "Sakshi Udeshi, Xingbin Jiang, Sudipta Chattopadhyay", "title": "Callisto: Entropy based test generation and data quality assessment for\n  Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning (ML) has seen massive progress in the last decade and as a\nresult, there is a pressing need for validating ML-based systems. To this end,\nwe propose, design and evaluate CALLISTO - a novel test generation and data\nquality assessment framework. To the best of our knowledge, CALLISTO is the\nfirst blackbox framework to leverage the uncertainty in the prediction and\nsystematically generate new test cases for ML classifiers. Our evaluation of\nCALLISTO on four real world data sets reveals thousands of errors. We also show\nthat leveraging the uncertainty in prediction can increase the number of\nerroneous test cases up to a factor of 20, as compared to when no such\nknowledge is used for testing.\n  CALLISTO has the capability to detect low quality data in the datasets that\nmay contain mislabelled data. We conduct and present an extensive user study to\nvalidate the results of CALLISTO on identifying low quality data from four\nstate-of-the-art real world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:20:18 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Udeshi", "Sakshi", ""], ["Jiang", "Xingbin", ""], ["Chattopadhyay", "Sudipta", ""]]}, {"id": "1912.08936", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Naren Doraiswamy, Boris N. Oreshkin, Hengshuai Yao,\n  Martin Jagersand", "title": "One-Shot Weakly Supervised Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional few-shot object segmentation methods learn object segmentation\nfrom a few labelled support images with strongly labelled segmentation masks.\nRecent work has shown to perform on par with weaker levels of supervision in\nterms of scribbles and bounding boxes. However, there has been limited\nattention given to the problem of few-shot object segmentation with image-level\nsupervision. We propose a novel multi-modal interaction module for few-shot\nobject segmentation that utilizes a co-attention mechanism using both visual\nand word embeddings. It enables our model to achieve 5.1% improvement over\npreviously proposed image-level few-shot object segmentation. Our method\ncompares relatively close to the state of the art methods that use strong\nsupervision, while ours use the least possible supervision. We further propose\na novel setup for few-shot weakly supervised video object segmentation(VOS)\nthat relies on image-level labels for the first frame. The proposed setup uses\nweak annotation unlike semi-supervised VOS setting that utilizes strongly\nlabelled segmentation masks. The setup evaluates the effectiveness of\ngeneralizing to novel classes in the VOS setting. The setup splits the VOS data\ninto multiple folds with different categories per fold. It provides a potential\nsetup to evaluate how few-shot object segmentation methods can benefit from\nadditional object poses, or object interactions that is not available in static\nframes as in PASCAL-5i benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:54:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Siam", "Mennatullah", ""], ["Doraiswamy", "Naren", ""], ["Oreshkin", "Boris N.", ""], ["Yao", "Hengshuai", ""], ["Jagersand", "Martin", ""]]}, {"id": "1912.08937", "submitter": "Faisal Mahmood", "authors": "Richard J. Chen, Ming Y. Lu, Jingwen Wang, Drew F. K. Williamson,\n  Scott J. Rodig, Neal I. Lindeman, Faisal Mahmood", "title": "Pathomic Fusion: An Integrated Framework for Fusing Histopathology and\n  Genomic Features for Cancer Diagnosis and Prognosis", "comments": "Code and trained models are made available at:\n  https://github.com/mahmoodlab/PathomicFusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.GN q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer diagnosis, prognosis, and therapeutic response predictions are based\non morphological information from histology slides and molecular profiles from\ngenomic data. However, most deep learning-based objective outcome prediction\nand grading paradigms are based on histology or genomics alone and do not make\nuse of the complementary information in an intuitive manner. In this work, we\npropose Pathomic Fusion, an interpretable strategy for end-to-end multimodal\nfusion of histology image and genomic (mutations, CNV, RNA-Seq) features for\nsurvival outcome prediction. Our approach models pairwise feature interactions\nacross modalities by taking the Kronecker product of unimodal feature\nrepresentations and controls the expressiveness of each representation via a\ngating-based attention mechanism. Following supervised learning, we are able to\ninterpret and saliently localize features across each modality, and understand\nhow feature importance shifts when conditioning on multimodal input. We\nvalidate our approach using glioma and clear cell renal cell carcinoma datasets\nfrom the Cancer Genome Atlas (TCGA), which contains paired whole-slide image,\ngenotype, and transcriptome data with ground truth survival and histologic\ngrade labels. In a 15-fold cross-validation, our results demonstrate that the\nproposed multimodal fusion paradigm improves prognostic determinations from\nground truth grading and molecular subtyping, as well as unimodal deep networks\ntrained on histology and genomic data alone. The proposed method establishes\ninsight and theory on how to train deep networks on multimodal biomedical data\nin an intuitive manner, which will be useful for other problems in medicine\nthat seek to combine heterogeneous data streams for understanding diseases and\npredicting response and resistance to treatment.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:01:26 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 03:50:25 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 16:04:39 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Chen", "Richard J.", ""], ["Lu", "Ming Y.", ""], ["Wang", "Jingwen", ""], ["Williamson", "Drew F. K.", ""], ["Rodig", "Scott J.", ""], ["Lindeman", "Neal I.", ""], ["Mahmood", "Faisal", ""]]}, {"id": "1912.08954", "submitter": "Guanbin Li", "authors": "Jihan Yang, Ruijia Xu, Ruiyu Li, Xiaojuan Qi, Xiaoyong Shen, Guanbin\n  Li, Liang Lin", "title": "An Adversarial Perturbation Oriented Domain Adaptation Approach for\n  Semantic Segmentation", "comments": "To Appear in AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on Unsupervised Domain Adaptation (UDA) for the task of semantic\nsegmentation. Recently, adversarial alignment has been widely adopted to match\nthe marginal distribution of feature representations across two domains\nglobally. However, this strategy fails in adapting the representations of the\ntail classes or small objects for semantic segmentation since the alignment\nobjective is dominated by head categories or large objects. In contrast to\nadversarial alignment, we propose to explicitly train a domain-invariant\nclassifier by generating and defensing against pointwise feature space\nadversarial perturbations. Specifically, we firstly perturb the intermediate\nfeature maps with several attack objectives (i.e., discriminator and\nclassifier) on each individual position for both domains, and then the\nclassifier is trained to be invariant to the perturbations. By perturbing each\nposition individually, our model treats each location evenly regardless of the\ncategory or object size and thus circumvents the aforementioned issue.\nMoreover, the domain gap in feature space is reduced by extrapolating source\nand target perturbed features towards each other with attack on the domain\ndiscriminator. Our approach achieves the state-of-the-art performance on two\nchallenging domain adaptation tasks for semantic segmentation: GTA5 ->\nCityscapes and SYNTHIA -> Cityscapes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:59:24 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Yang", "Jihan", ""], ["Xu", "Ruijia", ""], ["Li", "Ruiyu", ""], ["Qi", "Xiaojuan", ""], ["Shen", "Xiaoyong", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1912.08967", "submitter": "Yen-Liang Lin", "authors": "Yen-Liang Lin, Son Tran, Larry S. Davis", "title": "Fashion Outfit Complementary Item Retrieval", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary fashion item recommendation is critical for fashion outfit\ncompletion. Existing methods mainly focus on outfit compatibility prediction\nbut not in a retrieval setting. We propose a new framework for outfit\ncomplementary item retrieval. Specifically, a category-based subspace attention\nnetwork is presented, which is a scalable approach for learning the subspace\nattentions. In addition, we introduce an outfit ranking loss that better models\nthe item relationships of an entire outfit. We evaluate our method on the\noutfit compatibility, FITB and new retrieval tasks. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in both\ncompatibility prediction and complementary item retrieval\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 00:53:41 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 22:24:34 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Lin", "Yen-Liang", ""], ["Tran", "Son", ""], ["Davis", "Larry S.", ""]]}, {"id": "1912.08969", "submitter": "Anthony Hu", "authors": "Anthony Hu, Alex Kendall, Roberto Cipolla", "title": "Learning a Spatio-Temporal Embedding for Video Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel embedding approach for video instance segmentation. Our\nmethod learns a spatio-temporal embedding integrating cues from appearance,\nmotion, and geometry; a 3D causal convolutional network models motion, and a\nmonocular self-supervised depth loss models geometry. In this embedding space,\nvideo-pixels of the same instance are clustered together while being separated\nfrom other instances, to naturally track instances over time without any\ncomplex post-processing. Our network runs in real-time as our architecture is\nentirely causal - we do not incorporate information from future frames,\ncontrary to previous methods. We show that our model can accurately track and\nsegment instances, even with occlusions and missed detections, advancing the\nstate-of-the-art on the KITTI Multi-Object and Tracking Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 00:59:50 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hu", "Anthony", ""], ["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1912.08990", "submitter": "Jo\\\"el Seytre", "authors": "Jo\\\"el Seytre, Jon Wu, Alessandro Achille", "title": "TextTubes for Detecting Curved Text in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detector for curved text in natural images. We model scene text\ninstances as tubes around their medial axes and introduce a\nparametrization-invariant loss function. We train a two-stage curved text\ndetector, and evaluate it on the curved text benchmarks CTW-1500 and\nTotal-Text. Our approach achieves state-of-the-art results or improves upon\nthem, notably for CTW-1500 by over 8 percentage points in F-score.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 02:13:08 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Seytre", "Jo\u00ebl", ""], ["Wu", "Jon", ""], ["Achille", "Alessandro", ""]]}, {"id": "1912.09021", "submitter": "Chiat Pin Tay", "authors": "Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap", "title": "AANet: Attribute Attention Network for Person Re-Identifications", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Attribute Attention Network (AANet), a new architecture\nthat integrates person attributes and attribute attention maps into a\nclassification framework to solve the person re-identification (re-ID) problem.\nMany person re-ID models typically employ semantic cues such as body parts or\nhuman pose to improve the re-ID performance. Attribute information, however, is\noften not utilized. The proposed AANet leverages on a baseline model that uses\nbody parts and integrates the key attribute information in an unified learning\nframework. The AANet consists of a global person ID task, a part detection task\nand a crucial attribute detection task. By estimating the class responses of\nindividual attributes and combining them to form the attribute attention map\n(AAM), a very strong discriminatory representation is constructed. The proposed\nAANet outperforms the best state-of-the-art method arXiv:1711.09349v3 [cs.CV]\nusing ResNet-50 by 3.36% in mAP and 3.12% in Rank-1 accuracy on DukeMTMC-reID\ndataset. On Market1501 dataset, AANet achieves 92.38% mAP and 95.10% Rank-1\naccuracy with re-ranking, outperforming arXiv:1804.00216v1 [cs.CV], another\nstate of the art method using ResNet-152, by 1.42% in mAP and 0.47% in Rank-1\naccuracy. In addition, AANet can perform person attribute prediction (e.g.,\ngender, hair length, clothing length etc.), and localize the attributes in the\nquery image.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:19:45 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Tay", "Chiat-Pin", ""], ["Roy", "Sharmili", ""], ["Yap", "Kim-Hui", ""]]}, {"id": "1912.09026", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage and Randy Paffenroth", "title": "Bounded Manifold Completion", "comments": "12 pages, 7 figures, submitted to Pattern Recognition Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dimensionality reduction or, equivalently, the approximation of\nhigh-dimensional data using a low-dimensional nonlinear manifold is an active\narea of research. In this paper, we will present a thematically different\napproach to detect the existence of a low-dimensional manifold of a given\ndimension that lies within a set of bounds derived from a given point cloud. A\nmatrix representing the appropriately defined distances on a low-dimensional\nmanifold is low-rank, and our method is based on current techniques for\nrecovering a partially observed matrix from a small set of fully observed\nentries that can be implemented as a low-rank Matrix Completion (MC) problem.\nMC methods are currently used to solve challenging real-world problems, such as\nimage inpainting and recommender systems, and we leverage extent efficient\noptimization techniques that use a nuclear norm convex relaxation as a\nsurrogate for non-convex and discontinuous rank minimization. Our proposed\nmethod provides several advantages over current nonlinear dimensionality\nreduction techniques, with the two most important being theoretical guarantees\non the detection of low-dimensional embeddings and robustness to non-uniformity\nin the sampling of the manifold. We validate the performance of this approach\nusing both a theoretical analysis as well as synthetic and real-world benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:42:21 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""]]}, {"id": "1912.09028", "submitter": "Yuchen Fan", "authors": "Yuchen Fan, Jiahui Yu, Ding Liu, Thomas S. Huang", "title": "Scale-wise Convolution for Image Restoration", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While scale-invariant modeling has substantially boosted the performance of\nvisual recognition tasks, it remains largely under-explored in deep networks\nbased image restoration. Naively applying those scale-invariant techniques\n(e.g. multi-scale testing, random-scale data augmentation) to image restoration\ntasks usually leads to inferior performance. In this paper, we show that\nproperly modeling scale-invariance into neural networks can bring significant\nbenefits to image restoration performance. Inspired from spatial-wise\nconvolution for shift-invariance, \"scale-wise convolution\" is proposed to\nconvolve across multiple scales for scale-invariance. In our scale-wise\nconvolutional network (SCN), we first map the input image to the feature space\nand then build a feature pyramid representation via bi-linear down-scaling\nprogressively. The feature pyramid is then passed to a residual network with\nscale-wise convolutions. The proposed scale-wise convolution learns to\ndynamically activate and aggregate features from different input scales in each\nresidual building block, in order to exploit contextual information on multiple\nscales. In experiments, we compare the restoration accuracy and parameter\nefficiency among our model and many different variants of multi-scale neural\nnetworks. The proposed network with scale-wise convolution achieves superior\nperformance in multiple image restoration tasks including image\nsuper-resolution, image denoising and image compression artifacts removal. Code\nand models are available at: https://github.com/ychfan/scn_sr\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:50:23 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Fan", "Yuchen", ""], ["Yu", "Jiahui", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1912.09033", "submitter": "Zhongjie Yu", "authors": "Zhongjie Yu, Lin Chen, Zhongwei Cheng, Jiebo Luo", "title": "TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot\n  Learning", "comments": "Accepted at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successful application of deep learning to many visual recognition tasks\nrelies heavily on the availability of a large amount of labeled data which is\nusually expensive to obtain. The few-shot learning problem has attracted\nincreasing attention from researchers for building a robust model upon only a\nfew labeled samples. Most existing works tackle this problem under the\nmeta-learning framework by mimicking the few-shot learning task with an\nepisodic training strategy. In this paper, we propose a new transfer-learning\nframework for semi-supervised few-shot learning to fully utilize the auxiliary\ninformation from labeled base-class data and unlabeled novel-class data. The\nframework consists of three components: 1) pre-training a feature extractor on\nbase-class data; 2) using the feature extractor to initialize the classifier\nweights for the novel classes; and 3) further updating the model with a\nsemi-supervised learning method. Under the proposed framework, we develop a\nnovel method for semi-supervised few-shot learning called TransMatch by\ninstantiating the three components with Imprinting and MixMatch. Extensive\nexperiments on two popular benchmark datasets for few-shot learning,\nCUB-200-2011 and miniImageNet, demonstrate that our proposed method can\neffectively utilize the auxiliary information from labeled base-class data and\nunlabeled novel-class data to significantly improve the accuracy of few-shot\nlearning task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 06:50:45 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 19:25:18 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Yu", "Zhongjie", ""], ["Chen", "Lin", ""], ["Cheng", "Zhongwei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.09057", "submitter": "Frederik Hagelskjaer", "authors": "Frederik Hagelskj{\\ae}r, Anders Glent Buch", "title": "PointVoteNet: Accurate Object Detection and 6 DoF Pose Estimation in\n  Point Clouds", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for 6 DoF pose estimation of rigid objects\nin point cloud data. Many recent learning-based approaches use primarily RGB\ninformation for detecting objects, in some cases with an added refinement step\nusing depth data. Our method consumes unordered point sets with/without RGB\ninformation, from initial detection to the final transformation estimation\nstage. This allows us to achieve accurate pose estimates, in some cases\nsurpassing state of the art methods trained on the same data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 08:19:46 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 12:00:38 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hagelskj\u00e6r", "Frederik", ""], ["Buch", "Anders Glent", ""]]}, {"id": "1912.09059", "submitter": "Mahmood Sharif", "authors": "Mahmood Sharif, Lujo Bauer, Michael K. Reiter", "title": "$n$-ML: Mitigating Adversarial Examples via Ensembles of Topologically\n  Manipulated Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a new defense called $n$-ML against adversarial examples,\ni.e., inputs crafted by perturbing benign inputs by small amounts to induce\nmisclassifications by classifiers. Inspired by $n$-version programming, $n$-ML\ntrains an ensemble of $n$ classifiers, and inputs are classified by a vote of\nthe classifiers in the ensemble. Unlike prior such approaches, however, the\nclassifiers in the ensemble are trained specifically to classify adversarial\nexamples differently, rendering it very difficult for an adversarial example to\nobtain enough votes to be misclassified. We show that $n$-ML roughly retains\nthe benign classification accuracies of state-of-the-art models on the MNIST,\nCIFAR10, and GTSRB datasets, while simultaneously defending against adversarial\nexamples with better resilience than the best defenses known to date and, in\nmost cases, with lower classification-time overhead.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 08:24:07 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Sharif", "Mahmood", ""], ["Bauer", "Lujo", ""], ["Reiter", "Michael K.", ""]]}, {"id": "1912.09064", "submitter": "Mahmood Sharif", "authors": "Mahmood Sharif, Keane Lucas, Lujo Bauer, Michael K. Reiter, Saurabh\n  Shintre", "title": "Optimization-Guided Binary Diversification to Mislead Neural Networks\n  for Malware Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by the transformative impact of deep neural networks (DNNs) on\ndifferent areas (e.g., image and speech recognition), researchers and\nanti-virus vendors are proposing end-to-end DNNs for malware detection from raw\nbytes that do not require manual feature engineering. Given the security\nsensitivity of the task that these DNNs aim to solve, it is important to assess\ntheir susceptibility to evasion.\n  In this work, we propose an attack that guides binary-diversification tools\nvia optimization to mislead DNNs for malware detection while preserving the\nfunctionality of binaries. Unlike previous attacks on such DNNs, ours\nmanipulates instructions that are a functional part of the binary, which makes\nit particularly challenging to defend against. We evaluated our attack against\nthree DNNs in white-box and black-box settings, and found that it can often\nachieve success rates near 100%. Moreover, we found that our attack can fool\nsome commercial anti-viruses, in certain cases with a success rate of 85%. We\nexplored several defenses, both new and old, and identified some that can\nsuccessfully prevent over 80% of our evasion attempts. However, these defenses\nmay still be susceptible to evasion by adaptive attackers, and so we advocate\nfor augmenting malware-detection systems with methods that do not rely on\nmachine learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 08:41:16 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Sharif", "Mahmood", ""], ["Lucas", "Keane", ""], ["Bauer", "Lujo", ""], ["Reiter", "Michael K.", ""], ["Shintre", "Saurabh", ""]]}, {"id": "1912.09121", "submitter": "Haifeng Li", "authors": "Haifeng Li, Kaijian Qiu, Li Chen, Xiaoming Mei, Liang Hong, Chao Tao", "title": "SCAttNet: Semantic Segmentation Network with Spatial and Channel\n  Attention Mechanism for High-Resolution Remote Sensing Images", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": "IEEE Geoscience and Remote Sensing Letters 2020", "doi": "10.1109/LGRS.2020.2988294", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution remote sensing images (HRRSIs) contain substantial ground\nobject information, such as texture, shape, and spatial location. Semantic\nsegmentation, which is an important task for element extraction, has been\nwidely used in processing mass HRRSIs. However, HRRSIs often exhibit large\nintraclass variance and small interclass variance due to the diversity and\ncomplexity of ground objects, thereby bringing great challenges to a semantic\nsegmentation task. In this paper, we propose a new end-to-end semantic\nsegmentation network, which integrates lightweight spatial and channel\nattention modules that can refine features adaptively. We compare our method\nwith several classic methods on the ISPRS Vaihingen and Potsdam datasets.\nExperimental results show that our method can achieve better semantic\nsegmentation results. The source codes are available at\nhttps://github.com/lehaifeng/SCAttNet.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 10:58:01 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:03:57 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Li", "Haifeng", ""], ["Qiu", "Kaijian", ""], ["Chen", "Li", ""], ["Mei", "Xiaoming", ""], ["Hong", "Liang", ""], ["Tao", "Chao", ""]]}, {"id": "1912.09216", "submitter": "Charalambos Poullis", "authors": "Bodhiswatta Chatterjee and Charalambos Poullis", "title": "Semantic Segmentation from Remote Sensor Data and the Exploitation of\n  Latent Learning for Classification of Auxiliary Tasks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address three different aspects of semantic segmentation\nfrom remote sensor data using deep neural networks. Firstly, we focus on the\nsemantic segmentation of buildings from remote sensor data and propose ICT-Net.\nThe proposed network has been tested on the INRIA and AIRS benchmark datasets\nand is shown to outperform all other state of the art by more than 1.5% and\n1.8% on the Jaccard index, respectively.\n  Secondly, as the building classification is typically the first step of the\nreconstruction process, we investigate the relationship of the classification\naccuracy to the reconstruction accuracy.\n  Finally, we present the simple yet compelling concept of latent learning and\nthe implications it carries within the context of deep learning. We posit that\na network trained on a primary task (i.e. building classification) is\nunintentionally learning about auxiliary tasks (e.g. the classification of\nroad, tree, etc) which are complementary to the primary task. We extensively\ntested the proposed technique on the ISPRS benchmark dataset which contains\nmulti-label ground truth, and report an average classification accuracy (F1\nscore) of 54.29% (SD=17.03) for roads, 10.15% (SD=2.54) for cars, 24.11%\n(SD=5.25) for trees, 42.74% (SD=6.62) for low vegetation, and 18.30% (SD=16.08)\nfor clutter.\n  The source code and supplemental material is publicly available at\nhttp://www.theICTlab.org/lp/2019ICT-Net/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:25:26 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Chatterjee", "Bodhiswatta", ""], ["Poullis", "Charalambos", ""]]}, {"id": "1912.09231", "submitter": "Xu Tang", "authors": "Yang Liu, Xu Tang, Xiang Wu, Junyu Han, Jingtuo Liu, Errui Ding", "title": "HAMBox: Delving into Online High-quality Anchors Mining for Detecting\n  Outer Faces", "comments": "9 pages, 6 figures. arXiv admin note: text overlap with 1802.09058 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current face detectors utilize anchors to frame a multi-task learning problem\nwhich combines classification and bounding box regression. Effective anchor\ndesign and anchor matching strategy enable face detectors to localize faces\nunder large pose and scale variations. However, we observe that more than 80%\ncorrectly predicted bounding boxes are regressed from the unmatched anchors\n(the IoUs between anchors and target faces are lower than a threshold) in the\ninference phase. It indicates that these unmatched anchors perform excellent\nregression ability, but the existing methods neglect to learn from them. In\nthis paper, we propose an Online High-quality Anchor Mining Strategy (HAMBox),\nwhich explicitly helps outer faces compensate with high-quality anchors. Our\nproposed HAMBox method could be a general strategy for anchor-based\nsingle-stage face detection. Experiments on various datasets, including WIDER\nFACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed\nmethod. Furthermore, our team win the championship on the Face Detection test\ntrack of WIDER Face and Pedestrian Challenge 2019. We will release the codes\nwith PaddlePaddle.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:44:26 GMT"}], "update_date": "2019-12-31", "authors_parsed": [["Liu", "Yang", ""], ["Tang", "Xu", ""], ["Wu", "Xiang", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""]]}, {"id": "1912.09239", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai and Sion Hannuna and Nishan Canagarajah", "title": "Towards automated mobile-phone-based plant pathology management", "comments": "13 pages, India-UK Advanced Technology Centre of Excellence in Next\n  Generation Networks, Systems and Services (IU-ATC), 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework which uses computer vision algorithms to\nstandardise images and analyse them for identifying crop diseases\nautomatically. The tools are created to bridge the information gap between\nfarmers, advisory call centres and agricultural experts using the images of\ndiseased/infected crop captured by mobile-phones. These images are generally\nsensitive to a number of factors including camera type and lighting. We\ntherefore propose a technique for standardising the colour of plant images\nwithin the context of the advisory system. Subsequently, to aid the advisory\nprocess, the disease recognition process is automated using image processing in\nconjunction with machine learning techniques. We describe our proposed leaf\nextraction, affected area segmentation and disease classification techniques.\nThe proposed disease recognition system is tested using six mango diseases and\nthe results show over 80% accuracy. The final output of our system is a list of\npossible diseases with relevant management advice.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:55:05 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 01:39:17 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Hannuna", "Sion", ""], ["Canagarajah", "Nishan", ""]]}, {"id": "1912.09278", "submitter": "Kerstin Hammernik", "authors": "Kerstin Hammernik, Jo Schlemper, Chen Qin, Jinming Duan, Ronald M.\n  Summers, Daniel Rueckert", "title": "$\\Sigma$-net: Systematic Evaluation of Iterative Deep Neural Networks\n  for Fast Parallel MR Image Reconstruction", "comments": "Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To systematically investigate the influence of various data\nconsistency layers, (semi-)supervised learning and ensembling strategies,\ndefined in a $\\Sigma$-net, for accelerated parallel MR image reconstruction\nusing deep learning.\n  Theory and Methods: MR image reconstruction is formulated as learned unrolled\noptimization scheme with a Down-Up network as regularization and varying data\nconsistency layers. The different architectures are split into sensitivity\nnetworks, which rely on explicit coil sensitivity maps, and parallel coil\nnetworks, which learn the combination of coils implicitly. Different content\nand adversarial losses, a semi-supervised fine-tuning scheme and model\nensembling are investigated.\n  Results: Evaluated on the fastMRI multicoil validation set, architectures\ninvolving raw k-space data outperform image enhancement methods significantly.\nSemi-supervised fine-tuning adapts to new k-space data and provides, together\nwith reconstructions based on adversarial training, the visually most appealing\nresults although quantitative quality metrics are reduced. The $\\Sigma$-net\nensembles the benefits from different models and achieves similar scores\ncompared to the single state-of-the-art approaches.\n  Conclusion: This work provides an open-source framework to perform a\nsystematic wide-range comparison of state-of-the-art reconstruction approaches\nfor parallel MR image reconstruction on the fastMRI knee dataset and explores\nthe importance of data consistency. A suitable trade-off between perceptual\nimage quality and quantitative scores are achieved with the ensembled\n$\\Sigma$-net.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:52:39 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hammernik", "Kerstin", ""], ["Schlemper", "Jo", ""], ["Qin", "Chen", ""], ["Duan", "Jinming", ""], ["Summers", "Ronald M.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1912.09287", "submitter": "Minh Vu", "authors": "Minh H. Vu and Guus Grimbergen and Tufve Nyholm and Tommy L\\\"ofstedt", "title": "Evaluation of Multi-Slice Inputs to Convolutional Neural Networks for\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14391", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When using Convolutional Neural Networks (CNNs) for segmentation of organs\nand lesions in medical images, the conventional approach is to work with inputs\nand outputs either as single slice (2D) or whole volumes (3D). One common\nalternative, in this study denoted as pseudo-3D, is to use a stack of adjacent\nslices as input and produce a prediction for at least the central slice. This\napproach gives the network the possibility to capture 3D spatial information,\nwith only a minor additional computational cost. In this study, we\nsystematically evaluate the segmentation performance and computational costs of\nthis pseudo-3D approach as a function of the number of input slices, and\ncompare the results to conventional end-to-end 2D and 3D CNNs. The standard\npseudo-3D method regards the neighboring slices as multiple input image\nchannels. We additionally evaluate a simple approach where the input stack is a\nvolumetric input that is repeatably convolved in 3D to obtain a 2D feature map.\nThis 2D map is in turn fed into a standard 2D network. We conducted experiments\nusing two different CNN backbone architectures and on five diverse data sets\ncovering different anatomical regions, imaging modalities, and segmentation\ntasks. We found that while both pseudo-3D methods can process a large number of\nslices at once and still be computationally much more efficient than fully 3D\nCNNs, a significant improvement over a regular 2D CNN was only observed for one\nof the five data sets. An analysis of the structural properties of the\nsegmentation masks revealed no relations to the segmentation performance with\nrespect to the number of input slices. The conclusion is therefore that in the\ngeneral case, multi-slice inputs appear to not significantly improve\nsegmentation results over using 2D or 3D CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:26:16 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 19:31:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Vu", "Minh H.", ""], ["Grimbergen", "Guus", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "1912.09299", "submitter": "David Honz\\'atko", "authors": "Siavash Bigdeli, David Honz\\'atko, Sabine S\\\"usstrunk and L. Andrea\n  Dunbar", "title": "Image Restoration using Plug-and-Play CNN MAP Denoisers", "comments": "Code and models available at\n  https://github.com/DawyD/cnn-map-denoiser . Accepted for publication in\n  VISAPP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play denoisers can be used to perform generic image restoration\ntasks independent of the degradation type. These methods build on the fact that\nthe Maximum a Posteriori (MAP) optimization can be solved using smaller\nsub-problems, including a MAP denoising optimization. We present the first\nend-to-end approach to MAP estimation for image denoising using deep neural\nnetworks. We show that our method is guaranteed to minimize the MAP denoising\nobjective, which is then used in an optimization algorithm for generic image\nrestoration. We provide theoretical analysis of our approach and show the\nquantitative performance of our method in several experiments. Our experimental\nresults show that the proposed method can achieve 70x faster performance\ncompared to the state-of-the-art, while maintaining the theoretical perspective\nof MAP.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:57:27 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 08:52:18 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Bigdeli", "Siavash", ""], ["Honz\u00e1tko", "David", ""], ["S\u00fcsstrunk", "Sabine", ""], ["Dunbar", "L. Andrea", ""]]}, {"id": "1912.09316", "submitter": "Peiyu Yu", "authors": "Peiyu Yu, Yongming Rao, Jiwen Lu, Jie Zhou", "title": "P$^2$GNet: Pose-Guided Point Cloud Generating Networks for 6-DoF Object\n  Pose Estimation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to perform fast and accurate object pose estimation even\nunder severe occlusion by exploiting learned object model priors from everyday\nlife. However, most recently proposed pose estimation algorithms neglect to\nutilize the information of object models, often end up with limited accuracy,\nand tend to fall short in cluttered scenes. In this paper, we present a novel\nlearning-based model, \\underline{P}ose-Guided \\underline{P}oint Cloud\n\\underline{G}enerating Networks for 6D Object Pose Estimation (P$^2$GNet),\ndesigned to effectively exploit object model priors to facilitate 6D object\npose estimation. We achieve this with an end-to-end estimation-by-generation\nworkflow that combines the appearance information from the RGB-D image and the\nstructure knowledge from object point cloud to enable accurate and robust pose\nestimation. Experiments on two commonly used benchmarks for 6D pose estimation,\nYCB-Video dataset and LineMOD dataset, demonstrate that P$^2$GNet outperforms\nthe state-of-the-art method by a large margin and shows marked robustness\ntowards heavy occlusion, while achieving real-time inference.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:57:05 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 02:55:54 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Yu", "Peiyu", ""], ["Rao", "Yongming", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "1912.09336", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Danna Gurari", "title": "VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visualization tool to exhaustively search and browse through a\nset of large-scale machine learning datasets. Built on the top of the VizWiz\ndataset, our dataset browser tool has the potential to support and enable a\nvariety of qualitative and quantitative research, and open new directions for\nvisualizing and researching with multimodal information. The tool is publicly\navailable at https://vizwiz.org/browse.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:18:34 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Gurari", "Danna", ""]]}, {"id": "1912.09344", "submitter": "Nan Xue", "authors": "Nan Xue and Song Bai and Fu-Dong Wang and Gui-Song Xia and Tianfu Wu\n  and Liangpei Zhang and Philip H.S. Torr", "title": "Learning Regional Attraction for Line Segment Detection", "comments": "Accepted to IEEE TPAMI. arXiv admin note: text overlap with\n  arXiv:1812.02122", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2958642", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents regional attraction of line segment maps, and hereby\nposes the problem of line segment detection (LSD) as a problem of region\ncoloring. Given a line segment map, the proposed regional attraction first\nestablishes the relationship between line segments and regions in the image\nlattice. Based on this, the line segment map is equivalently transformed to an\nattraction field map (AFM), which can be remapped to a set of line segments\nwithout loss of information. Accordingly, we develop an end-to-end framework to\nlearn attraction field maps for raw input images, followed by a squeeze module\nto detect line segments. Apart from existing works, the proposed detector\nproperly handles the local ambiguity and does not rely on the accurate\nidentification of edge pixels. Comprehensive experiments on the Wireframe\ndataset and the YorkUrban dataset demonstrate the superiority of our method. In\nparticular, we achieve an F-measure of 0.831 on the Wireframe dataset,\nadvancing the state-of-the-art performance by 10.3 percent.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:05:23 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Xue", "Nan", ""], ["Bai", "Song", ""], ["Wang", "Fu-Dong", ""], ["Xia", "Gui-Song", ""], ["Wu", "Tianfu", ""], ["Zhang", "Liangpei", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1912.09351", "submitter": "Seokju Lee", "authors": "Seokju Lee, Sunghoon Im, Stephen Lin, In So Kweon", "title": "Instance-wise Depth and Motion Learning from Monocular Videos", "comments": "Project page at https://sites.google.com/site/seokjucv/home/instadm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end joint training framework that explicitly models\n6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular\ncamera setup without supervision. Our technical contributions are three-fold.\nFirst, we propose a differentiable forward rigid projection module that plays a\nkey role in our instance-wise depth and motion learning. Second, we design an\ninstance-wise photometric and geometric consistency loss that effectively\ndecomposes background and moving object regions. Lastly, we introduce a new\nauto-annotation scheme to produce video instance segmentation maps that will be\nutilized as input to our training pipeline. These proposed elements are\nvalidated in a detailed ablation study. Through extensive experiments conducted\non the KITTI dataset, our framework is shown to outperform the state-of-the-art\ndepth and motion estimation methods. Our code and dataset will be available at\nhttps://github.com/SeokjuLee/Insta-DM.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:35:30 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 11:53:52 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Lee", "Seokju", ""], ["Im", "Sunghoon", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "1912.09380", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Gabriel Gagnon-Turcotte, Angkoon Phinyomark,\n  Kyrre Glette, Erik Scheme, Fran\\c{c}ois Laviolette, and Benoit Gosselin", "title": "A Transferable Adaptive Domain Adversarial Neural Network for Virtual\n  Reality Augmented EMG-Based Gesture Recognition", "comments": "10 Pages. The last three authors shared senior authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of electromyography-based (EMG) gesture recognition,\ndisparities exist between the offline accuracy reported in the literature and\nthe real-time usability of a classifier. This gap mainly stems from two\nfactors: 1) The absence of a controller, making the data collected dissimilar\nto actual control. 2) The difficulty of including the four main dynamic factors\n(gesture intensity, limb position, electrode shift, and transient changes in\nthe signal), as including their permutations drastically increases the amount\nof data to be recorded. Contrarily, online datasets are limited to the exact\nEMG-based controller used to record them, necessitating the recording of a new\ndataset for each control method or variant to be tested. Consequently, this\npaper proposes a new type of dataset to serve as an intermediate between\noffline and online datasets, by recording the data using a real-time\nexperimental protocol. The protocol, performed in virtual reality, includes the\nfour main dynamic factors and uses an EMG-independent controller to guide\nmovements. This EMG-independent feedback ensures that the user is in-the-loop\nduring recording, while enabling the resulting dynamic dataset to be used as an\nEMG-based benchmark. The dataset is comprised of 20 able-bodied participants\ncompleting three to four sessions over a period of 14 to 21 days. The ability\nof the dynamic dataset to serve as a benchmark is leveraged to evaluate the\nimpact of different recalibration techniques for long-term (across-day) gesture\nrecognition, including a novel algorithm, named TADANN. TADANN consistently and\nsignificantly (p<0.05) outperforms using fine-tuning as the recalibration\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:41:56 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 14:55:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Gagnon-Turcotte", "Gabriel", ""], ["Phinyomark", "Angkoon", ""], ["Glette", "Kyrre", ""], ["Scheme", "Erik", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Gosselin", "Benoit", ""]]}, {"id": "1912.09390", "submitter": "Marc Eder", "authors": "Marc Eder, Mykhailo Shvets, John Lim, Jan-Michael Frahm", "title": "Tangent Images for Mitigating Spherical Distortion", "comments": "Updated version of CVPR 2020 publication (9 pages, 13 pages\n  supplementary). Code: https://github.com/meder411/Tangent-Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose \"tangent images,\" a spherical image representation\nthat facilitates transferable and scalable $360^\\circ$ computer vision.\nInspired by techniques in cartography and computer graphics, we render a\nspherical image to a set of distortion-mitigated, locally-planar image grids\ntangent to a subdivided icosahedron. By varying the resolution of these grids\nindependently of the subdivision level, we can effectively represent high\nresolution spherical images while still benefiting from the low-distortion\nicosahedral spherical approximation. We show that training standard\nconvolutional neural networks on tangent images compares favorably to the many\nspecialized spherical convolutional kernels that have been developed, while\nalso scaling efficiently to handle significantly higher spherical resolutions.\nFurthermore, because our approach does not require specialized kernels, we show\nthat we can transfer networks trained on perspective images to spherical data\nwithout fine-tuning and with limited performance drop-off. Finally, we\ndemonstrate that tangent images can be used to improve the quality of sparse\nfeature detection on spherical images, illustrating its usefulness for\ntraditional computer vision tasks like structure-from-motion and SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:06:20 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 17:06:28 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 14:43:39 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Eder", "Marc", ""], ["Shvets", "Mykhailo", ""], ["Lim", "John", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1912.09393", "submitter": "Luca Bertinetto", "authors": "Luca Bertinetto, Romain Mueller, Konstantinos Tertikas, Sina\n  Samangooei, Nicholas A. Lord", "title": "Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks", "comments": "To appear at CVPR 2020. Code available at\n  https://github.com/fiveai/making-better-mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have improved image classification dramatically over the\npast decade, but have done so by focusing on performance measures that treat\nall classes other than the ground truth as equally wrong. This has led to a\nsituation in which mistakes are less likely to be made than before, but are\nequally likely to be absurd or catastrophic when they do occur. Past works have\nrecognised and tried to address this issue of mistake severity, often by using\ngraph distances in class hierarchies, but this has largely been neglected since\nthe advent of the current deep learning era in computer vision. In this paper,\nwe aim to renew interest in this problem by reviewing past approaches and\nproposing two simple modifications of the cross-entropy loss which outperform\nthe prior art under several metrics on two large datasets with complex class\nhierarchies: tieredImageNet and iNaturalist'19.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:08:51 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 16:19:22 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Bertinetto", "Luca", ""], ["Mueller", "Romain", ""], ["Tertikas", "Konstantinos", ""], ["Samangooei", "Sina", ""], ["Lord", "Nicholas A.", ""]]}, {"id": "1912.09395", "submitter": "Andreas Kofler", "authors": "Andreas Kofler, Markus Haltmeier, Tobias Schaeffter, Marc\n  Kachelrie{\\ss}, Marc Dewey, Christian Wald and Christoph Kolbitsch", "title": "Neural Networks-based Regularization for Large-Scale Medical Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/ab990e", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a generalized Deep Learning-based approach for\nsolving ill-posed large-scale inverse problems occuring in medical image\nreconstruction. Recently, Deep Learning methods using iterative neural networks\nand cascaded neural networks have been reported to achieve state-of-the-art\nresults with respect to various quantitative quality measures as PSNR, NRMSE\nand SSIM across different imaging modalities. However, the fact that these\napproaches employ the forward and adjoint operators repeatedly in the network\narchitecture requires the network to process the whole images or volumes at\nonce, which for some applications is computationally infeasible. In this work,\nwe follow a different reconstruction strategy by decoupling the regularization\nof the solution from ensuring consistency with the measured data. The\nregularization is given in the form of an image prior obtained by the output of\na previously trained neural network which is used in a Tikhonov regularization\nframework. By doing so, more complex and sophisticated network architectures\ncan be used for the removal of the artefacts or noise than it is usually the\ncase in iterative networks. Due to the large scale of the considered problems\nand the resulting computational complexity of the employed networks, the priors\nare obtained by processing the images or volumes as patches or slices. We\nevaluated the method for the cases of 3D cone-beam low dose CT and undersampled\n2D radial cine MRI and compared it to a total variation-minimization-based\nreconstruction algorithm as well as to a method with regularization based on\nlearned overcomplete dictionaries. The proposed method outperformed all the\nreported methods with respect to all chosen quantitative measures and further\naccelerates the regularization step in the reconstruction by several orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:15:27 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:52:58 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kofler", "Andreas", ""], ["Haltmeier", "Markus", ""], ["Schaeffter", "Tobias", ""], ["Kachelrie\u00df", "Marc", ""], ["Dewey", "Marc", ""], ["Wald", "Christian", ""], ["Kolbitsch", "Christoph", ""]]}, {"id": "1912.09405", "submitter": "Stephen Law Dr", "authors": "Andrew Elliott and Stephen Law and Chris Russell", "title": "Explaining Classifiers using Adversarial Perturbations on the Perceptual\n  Ball", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple regularization of adversarial perturbations based upon\nthe perceptual loss. While the resulting perturbations remain imperceptible to\nthe human eye, they differ from existing adversarial perturbations in that they\nare semi-sparse alterations that highlight objects and regions of interest\nwhile leaving the background unaltered. As a semantically meaningful adverse\nperturbations, it forms a bridge between counterfactual explanations and\nadversarial perturbations in the space of images. We evaluate our approach on\nseveral standard explainability benchmarks, namely, weak localization,\ninsertion deletion, and the pointing game demonstrating that perceptually\nregularized counterfactuals are an effective explanation for image-based\nclassifiers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:25:07 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 14:55:40 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 18:50:43 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 21:51:19 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Elliott", "Andrew", ""], ["Law", "Stephen", ""], ["Russell", "Chris", ""]]}, {"id": "1912.09421", "submitter": "Hsin-Ying Lee", "authors": "Hsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong,\n  Ming-Hsuan Yang, Weilong Yang", "title": "Neural Design Network: Graphic Layout Generation with Constraints", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphic design is essential for visual communication with layouts being\nfundamental to composing attractive designs. Layout generation differs from\npixel-level image synthesis and is unique in terms of the requirement of mutual\nrelations among the desired components. We propose a method for design layout\ngeneration that can satisfy user-specified constraints. The proposed neural\ndesign network (NDN) consists of three modules. The first module predicts a\ngraph with complete relations from a graph with user-specified relations. The\nsecond module generates a layout from the predicted graph. Finally, the third\nmodule fine-tunes the predicted layout. Quantitative and qualitative\nexperiments demonstrate that the generated layouts are visually similar to real\ndesign layouts. We also construct real designs based on predicted layouts for a\nbetter understanding of the visual quality. Finally, we demonstrate a practical\napplication on layout recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:47:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 23:32:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lee", "Hsin-Ying", ""], ["Jiang", "Lu", ""], ["Essa", "Irfan", ""], ["Le", "Phuong B", ""], ["Gong", "Haifeng", ""], ["Yang", "Ming-Hsuan", ""], ["Yang", "Weilong", ""]]}, {"id": "1912.09476", "submitter": "Srikrishna Varadarajan", "authors": "Srikrishna Varadarajan, Sonaal Kant and Muktabh Mayank Srivastava", "title": "Benchmark for Generic Product Detection: A Low Data Baseline for Dense\n  Object Detection", "comments": "corrected a mistake in evaluation; added more comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in densely packed scenes is a new area where standard object\ndetectors fail to train well. Dense object detectors like RetinaNet trained on\nlarge and dense datasets show great performance. We train a standard object\ndetector on a small, normally packed dataset with data augmentation techniques.\nThis dataset is 265 times smaller than the standard dataset, in terms of number\nof annotations. This low data baseline achieves satisfactory results (mAP=0.56)\nat standard IoU of 0.5. We also create a varied benchmark for generic SKU\nproduct detection by providing full annotations for multiple public datasets.\nIt can be accessed at\nhttps://github.com/ParallelDots/generic-sku-detection-benchmark. We hope that\nthis benchmark helps in building robust detectors that perform reliably across\ndifferent settings in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 18:57:45 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 18:50:23 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Varadarajan", "Srikrishna", ""], ["Kant", "Sonaal", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1912.09497", "submitter": "Rewa Sood", "authors": "Rewa Sood and Mirabela Rusu", "title": "Anisotropic Super Resolution in Prostate MRI using Super Resolution\n  Generative Adversarial Networks", "comments": "International Symposium on Biomedical Imaging, 4 pages, 4 figures, 1\n  table", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019), Venice, Italy, 2019, pp. 1688-1691", "doi": "10.1109/ISBI.2019.8759237", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the\npatient to remain still for long periods of time, which causes patient\ndiscomfort and increases the probability of motion induced image artifacts. A\npossible solution is to acquire low resolution (LR) images and to process them\nwith the Super Resolution Generative Adversarial Network (SRGAN) to create a\nsuper-resolved version. This work applies SRGAN to MR images of the prostate\nand performs three experiments. The first experiment explores improving the\nin-plane MR image resolution by factors of 4 and 8, and shows that, while the\nPSNR and SSIM (Structural SIMilarity) metrics are lower than the isotropic\nbicubic interpolation baseline, the SRGAN is able to create images that have\nhigh edge fidelity. The second experiment explores anisotropic super-resolution\nvia synthetic images, in that the input images to the network are\nanisotropically downsampled versions of HR images. This experiment demonstrates\nthe ability of the modified SRGAN to perform anisotropic super-resolution, with\nquantitative image metrics that are comparable to those of the anisotropic\nbicubic interpolation baseline. Finally, the third experiment applies a\nmodified version of the SRGAN to super-resolve anisotropic images obtained from\nthe through-plane slices of the volumetric MR data. The output super-resolved\nimages contain a significant amount of high frequency information that make\nthem visually close to their HR counterparts. Overall, the promising results\nfrom each experiment show that super-resolution for MR images is a successful\ntechnique and that producing isotropic MR image volumes from anisotropic slices\nis an achievable goal.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:05:18 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Sood", "Rewa", ""], ["Rusu", "Mirabela", ""]]}, {"id": "1912.09507", "submitter": "Rewa Sood", "authors": "Rewa Sood, Binit Topiwala, Karthik Choutagunta, Rohit Sood, Mirabela\n  Rusu", "title": "An Application of Generative Adversarial Networks for Super Resolution\n  Medical Imaging", "comments": "International Conference on Machine Learning Applications, 6 pages, 5\n  figures, 2 tables", "journal-ref": "17th IEEE International Conference on Machine Learning and\n  Applications,2018, pp. 326-331", "doi": "10.1109/ICMLA.2018.00055", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the\npatient to remain still for long periods of time, which causes patient\ndiscomfort and increases the probability of motion induced image artifacts. A\npossible solution is to acquire low resolution (LR) images and to process them\nwith the Super Resolution Generative Adversarial Network (SRGAN) to create an\nHR version. Acquiring LR images requires a lower scan time than acquiring HR\nimages, which allows for higher patient comfort and scanner throughput. This\nwork applies SRGAN to MR images of the prostate to improve the in-plane\nresolution by factors of 4 and 8. The term 'super resolution' in the context of\nthis paper defines the post processing enhancement of medical images as opposed\nto 'high resolution' which defines native image resolution acquired during the\nMR acquisition phase. We also compare the SRGAN to three other models: SRCNN,\nSRResNet, and Sparse Representation. While the SRGAN results do not have the\nbest Peak Signal to Noise Ratio (PSNR) or Structural Similarity (SSIM) metrics,\nthey are the visually most similar to the original HR images, as portrayed by\nthe Mean Opinion Score (MOS) results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:20:04 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Sood", "Rewa", ""], ["Topiwala", "Binit", ""], ["Choutagunta", "Karthik", ""], ["Sood", "Rohit", ""], ["Rusu", "Mirabela", ""]]}, {"id": "1912.09513", "submitter": "Yiyang Zhou", "authors": "Weisong Wen, Yiyang Zhou, Guohao Zhang, Saman Fahandezh-Saadi, Xiwei\n  Bai, Wei Zhan, Masayoshi Tomizuka, Li-Ta Hsu", "title": "UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in\n  Urban Scenes", "comments": "7 pages, ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping and localization is a critical module of autonomous driving, and\nsignificant achievements have been reached in this field. Beyond Global\nNavigation Satellite System (GNSS), research in point cloud registration,\nvisual feature matching, and inertia navigation has greatly enhanced the\naccuracy and robustness of mapping and localization in different scenarios.\nHowever, highly urbanized scenes are still challenging: LIDAR- and camera-based\nmethods perform poorly with numerous dynamic objects; the GNSS-based solutions\nexperience signal loss and multipath problems; the inertia measurement units\n(IMU) suffer from drifting. Unfortunately, current public datasets either do\nnot adequately address this urban challenge or do not provide enough sensor\ninformation related to mapping and localization. Here we present UrbanLoco: a\nmapping/localization dataset collected in highly-urbanized environments with a\nfull sensor-suite. The dataset includes 13 trajectories collected in San\nFrancisco and Hong Kong, covering a total length of over 40 kilometers. Our\ndataset includes a wide variety of urban terrains: urban canyons, bridges,\ntunnels, sharp turns, etc. More importantly, our dataset includes information\nfrom LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly\navailable through the link in the footnote. Dataset Link:\nhttps://advdataset2019.wixsite.com/urbanloco.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:31:07 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 02:42:27 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wen", "Weisong", ""], ["Zhou", "Yiyang", ""], ["Zhang", "Guohao", ""], ["Fahandezh-Saadi", "Saman", ""], ["Bai", "Xiwei", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""], ["Hsu", "Li-Ta", ""]]}, {"id": "1912.09532", "submitter": "Van Nhan Nguyen", "authors": "Van Nhan Nguyen, Robert Jenssen, and Davide Roverso", "title": "LS-Net: Fast Single-Shot Line-Segment Detector", "comments": "Highlighted the paper's contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low-altitude Unmanned Aerial Vehicle (UAV) flights, power lines are\nconsidered as one of the most threatening hazards and one of the most difficult\nobstacles to avoid. In recent years, many vision-based techniques have been\nproposed to detect power lines to facilitate self-driving UAVs and automatic\nobstacle avoidance. However, most of the proposed methods are typically based\non a common three-step approach: (i) edge detection, (ii) the Hough transform,\nand (iii) spurious line elimination based on power line constrains. These\napproaches not only are slow and inaccurate but also require a huge amount of\neffort in post-processing to distinguish between power lines and spurious\nlines. In this paper, we introduce LS-Net, a fast single-shot line-segment\ndetector, and apply it to power line detection. The LS-Net is by design fully\nconvolutional and consists of three modules: (i) a fully convolutional feature\nextractor, (ii) a classifier, and (iii) a line segment regressor. Due to the\nunavailability of large datasets with annotations of power lines, we render\nsynthetic images of power lines using the Physically Based Rendering (PBR)\napproach and propose a series of effective data augmentation techniques to\ngenerate more training data. With a customized version of the VGG-16 network as\nthe backbone, the proposed approach outperforms existing state-of-the-art\napproaches. In addition, the LS-Net can detect power lines in near real-time\n(20.4 FPS). This suggests that our proposed approach has a promising role in\nautomatic obstacle avoidance and as a valuable component of self-driving UAVs,\nespecially for automatic autonomous power line inspection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:19:51 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 11:39:00 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Nguyen", "Van Nhan", ""], ["Jenssen", "Robert", ""], ["Roverso", "Davide", ""]]}, {"id": "1912.09539", "submitter": "Hamidreza Kasaei", "authors": "S. Hamidreza Kasaei", "title": "Interactive Open-Ended Learning for 3D Object Recognition", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thesis contributes in several important ways to the research area of 3D\nobject category learning and recognition. To cope with the mentioned\nlimitations, we look at human cognition, in particular at the fact that human\nbeings learn to recognize object categories ceaselessly over time. This ability\nto refine knowledge from the set of accumulated experiences facilitates the\nadaptation to new environments. Inspired by this capability, we seek to create\na cognitive object perception and perceptual learning architecture that can\nlearn 3D object categories in an open-ended fashion. In this context,\n``open-ended'' implies that the set of categories to be learned is not known in\nadvance, and the training instances are extracted from actual experiences of a\nrobot, and thus become gradually available, rather than being available since\nthe beginning of the learning process. In particular, this architecture\nprovides perception capabilities that will allow robots to incrementally learn\nobject categories from the set of accumulated experiences and reason about how\nto perform complex tasks. This framework integrates detection, tracking,\nteaching, learning, and recognition of objects. An extensive set of systematic\nexperiments, in multiple experimental settings, was carried out to thoroughly\nevaluate the described learning approaches. Experimental results show that the\nproposed system is able to interact with human users, learn new object\ncategories over time, as well as perform complex tasks. The contributions\npresented in this thesis have been fully implemented and evaluated on different\nstandard object and scene datasets and empirically evaluated on different\nrobotic platforms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:46:51 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kasaei", "S. Hamidreza", ""]]}, {"id": "1912.09551", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro and Vinay P. Namboodiri", "title": "Deep Exemplar Networks for VQA and VQG", "comments": "This work is an extension of CVPR-2018 accepted paper\n  arXiv:1804.00298 and EMNLP-2018 accepted paper arXiv:1808.03986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the problem of solving semantic tasks such as\n`Visual Question Answering' (VQA), where one aims to answers related to an\nimage and `Visual Question Generation' (VQG), where one aims to generate a\nnatural question pertaining to an image. Solutions for VQA and VQG tasks have\nbeen proposed using variants of encoder-decoder deep learning based frameworks\nthat have shown impressive performance. Humans however often show\ngeneralization by relying on exemplar based approaches. For instance, the work\nby Tversky and Kahneman suggests that humans use exemplars when making\ncategorizations and decisions. In this work, we propose the incorporation of\nexemplar based approaches towards solving these problems. Specifically, we\nincorporate exemplar based approaches and show that an exemplar based module\ncan be incorporated in almost any of the deep learning architectures proposed\nin the literature and the addition of such a block results in improved\nperformance for solving these tasks. Thus, just as the incorporation of\nattention is now considered de facto useful for solving these tasks, similarly,\nincorporating exemplars also can be considered to improve any proposed\narchitecture for solving this task. We provide extensive empirical analysis for\nthe same through various architectures, ablations, and state of the art\ncomparisons.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:29:22 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Patro", "Badri N.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1912.09579", "submitter": "Junfeng Guan", "authors": "Junfeng Guan, Sohrab Madani, Suraj Jog, Haitham Hassanieh", "title": "High Resolution Millimeter Wave Imaging For Self-Driving Cars", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed much interest in expanding the use of networking\nsignals beyond communication to sensing, localization, robotics, and autonomous\nsystems. This paper explores how we can leverage recent advances in 5G\nmillimeter wave (mmWave) technology for imaging in self-driving cars.\nSpecifically, the use of mmWave in 5G has led to the creation of compact phased\narrays with hundreds of antenna elements that can be electronically steered.\nSuch phased arrays can expand the use of mmWave beyond vehicular communications\nand simple ranging sensors to a full-fledged imaging system that enables\nself-driving cars to see through fog, smog, snow, etc. Unfortunately, using\nmmWave signals for imaging in self-driving cars is challenging due to the very\nlow resolution, the presence of fake artifacts resulting from multipath\nreflections and the absence of portions of the car due to specularity. This\npaper presents HawkEye, a system that can enable high resolution mmWave imaging\nin self driving cars. HawkEye addresses the above challenges by leveraging\nrecent advances in deep learning known as Generative Adversarial Networks\n(GANs). HawkEye introduces a GAN architecture that is customized to mmWave\nimaging and builds a system that can significantly enhance the quality of\nmmWave images for self-driving cars.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 22:27:56 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 22:35:12 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Guan", "Junfeng", ""], ["Madani", "Sohrab", ""], ["Jog", "Suraj", ""], ["Hassanieh", "Haitham", ""]]}, {"id": "1912.09581", "submitter": "Kai-Fu Yang", "authors": "Kai-Fu Yang, Wen-Wen Jiang, Teng-Fei Zhan, and Yong-Jie Li", "title": "Line Drawings of Natural Scenes Guide Visual Attention", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual search is an important strategy of the human visual system for fast\nscene perception. The guided search theory suggests that the global layout or\nother top-down sources of scenes play a crucial role in guiding object\nsearching. In order to verify the specific roles of scene layout and regional\ncues in guiding visual attention, we executed a psychophysical experiment to\nrecord the human fixations on line drawings of natural scenes with an\neye-tracking system in this work. We collected the human fixations of ten\nsubjects from 498 natural images and of another ten subjects from the\ncorresponding 996 human-marked line drawings of boundaries (two boundary maps\nper image) under free-viewing condition. The experimental results show that\nwith the absence of some basic features like color and luminance, the\ndistribution of the fixations on the line drawings has a high correlation with\nthat on the natural images. Moreover, compared to the basic cues of regions,\nsubjects pay more attention to the closed regions of line drawings which are\nusually related to the dominant objects of the scenes. Finally, we built a\ncomputational model to demonstrate that the fixation information on the line\ndrawings can be used to significantly improve the performances of classical\nbottom-up models for fixation prediction in natural scenes. These results\nsupport that Gestalt features and scene layout are important cues for guiding\nfast visual object searching.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 22:41:43 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Yang", "Kai-Fu", ""], ["Jiang", "Wen-Wen", ""], ["Zhan", "Teng-Fei", ""], ["Li", "Yong-Jie", ""]]}, {"id": "1912.09597", "submitter": "Eric Geiger", "authors": "Eric Geiger, Irina A. Kogan", "title": "Non-congruent non-degenerate curves with identical signatures", "comments": "33 pages, 22 figures. Page 20: In the proof of Corollary 31 the\n  notation for the length, $L_W$, of a reconstructed curve $\\Gamma_W$ is\n  introduced and defined. Page 23: The upper bound on the integral in equation\n  (35) is updated to use $L_W$ instead of $L$ and the definition of $L_W$ is\n  referred to. Page 23: The assumption \"$m$ and $\\xi$ are relatively prime\" is\n  added to Proposition 36", "journal-ref": "Journal of Mathematical Imaging and Vision, 63, 601-625 (2021)", "doi": "10.1007/s10851-020-01015-x", "report-no": null, "categories": "math.DG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the equality of differential signatures (Calabi et al, Int. J. Comput.\nVis. 26: 107-135, 1998) is known to be a necessary condition for congruence, it\nis not sufficient (Musso and Nicolodi, J. Math Imaging Vis. 35: 68-85, 2009).\nHickman (J. Math Imaging Vis. 43: 206-213, 2012, Theorem 2) claimed that for\nnon-degenerate planar curves, equality of Euclidean signatures implies\ncongruence. We prove that while Hickman's claim holds for simple, closed curves\nwith simple signatures, it fails for curves with non-simple signatures. In the\nlater case, we associate a directed graph with the signature and show how\nvarious paths along the graph give rise to a family of non-congruent,\nnon-degenerate curves with identical signatures. Using this additional\nstructure, we formulate congruence criteria for non-degenerate, closed, simple\ncurves and show how the paths reflect the global and local symmetries of the\ncorresponding curve.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:37:11 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:24:01 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 01:42:59 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 04:01:20 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Geiger", "Eric", ""], ["Kogan", "Irina A.", ""]]}, {"id": "1912.09601", "submitter": "Walter Mayor", "authors": "Walter M. Mayor Toro, Juan C. Perafan Villota, Oscar H. Mondragon,\n  Johan S. Obando Ceron", "title": "Divide and Conquer: an Accurate Machine Learning Algorithm to Process\n  Split Videos on a Parallel Processing Infrastructure", "comments": "3 pages, 2 figures, LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day the number of traffic cameras in cities rapidly increase and huge\namount of video data are generated. Parallel processing infrastruture, such as\nHadoop, and programming models, such as MapReduce, are being used to promptly\nprocess that amount of data. The common approach for video processing by using\nHadoop MapReduce is to process an entire video on only one node, however, in\norder to avoid parallelization problems, such as load imbalance, we propose to\nprocess videos by splitting it into equal parts and processing each resulting\nchunk on a different node. We used some machine learning techniques to detect\nand track the vehicles. However, video division may produce inaccurate results.\nTo solve this problem we proposed a heuristic algorithm to avoid process a\nvehicle in more than one chunk.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:51:32 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Toro", "Walter M. Mayor", ""], ["Villota", "Juan C. Perafan", ""], ["Mondragon", "Oscar H.", ""], ["Ceron", "Johan S. Obando", ""]]}, {"id": "1912.09621", "submitter": "Barath Narayanan Narayanan", "authors": "Barath Narayanan Narayanan, Manawaduge Supun De Silva, Russell C.\n  Hardie, Nathan K. Kueterman, Redha Ali", "title": "Understanding Deep Neural Network Predictions for Medical Imaging\n  Applications", "comments": "20 pages, 28 Figures and 9 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided detection has been a research area attracting great interest\nin the past decade. Machine learning algorithms have been utilized extensively\nfor this application as they provide a valuable second opinion to the doctors.\nDespite several machine learning models being available for medical imaging\napplications, not many have been implemented in the real-world due to the\nuninterpretable nature of the decisions made by the network. In this paper, we\ninvestigate the results provided by deep neural networks for the detection of\nmalaria, diabetic retinopathy, brain tumor, and tuberculosis in different\nimaging modalities. We visualize the class activation mappings for all the\napplications in order to enhance the understanding of these networks. This type\nof visualization, along with the corresponding network performance metrics,\nwould aid the data science experts in better understanding of their models as\nwell as assisting doctors in their decision-making process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 02:57:05 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Narayanan", "Barath Narayanan", ""], ["De Silva", "Manawaduge Supun", ""], ["Hardie", "Russell C.", ""], ["Kueterman", "Nathan K.", ""], ["Ali", "Redha", ""]]}, {"id": "1912.09622", "submitter": "Dawei Du", "authors": "Ruyi Ji and Dawei Du and Libo Zhang and Longyin Wen and Yanjun Wu and\n  Chen Zhao and Feiyue Huang and Siwei Lyu", "title": "Learning Semantic Neural Tree for Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing human parsing methods formulate the task as semantic\nsegmentation, which regard each semantic category equally and fail to exploit\nthe intrinsic physiological structure of human body, resulting in inaccurate\nresults. In this paper, we design a novel semantic neural tree for human\nparsing, which uses a tree architecture to encode physiological structure of\nhuman body, and designs a coarse to fine process in a cascade manner to\ngenerate accurate results. Specifically, the semantic neural tree is designed\nto segment human regions into multiple semantic subregions (e.g., face, arms,\nand legs) in a hierarchical way using a new designed attention routing module.\nMeanwhile, we introduce the semantic aggregation module to combine multiple\nhierarchical features to exploit more context information for better\nperformance. Our semantic neural tree can be trained in an end-to-end fashion\nby standard stochastic gradient descent (SGD) with back-propagation. Several\nexperiments conducted on four challenging datasets for both single and multiple\nhuman parsing, i.e., LIP, PASCAL-Person-Part, CIHP and MHP-v2, demonstrate the\neffectiveness of the proposed method. Code can be found at\nhttps://isrc.iscas.ac.cn/gitlab/research/sematree.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:05:48 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ji", "Ruyi", ""], ["Du", "Dawei", ""], ["Zhang", "Libo", ""], ["Wen", "Longyin", ""], ["Wu", "Yanjun", ""], ["Zhao", "Chen", ""], ["Huang", "Feiyue", ""], ["Lyu", "Siwei", ""]]}, {"id": "1912.09628", "submitter": "Qihang Yu", "authors": "Qihang Yu, Dong Yang, Holger Roth, Yutong Bai, Yixiao Zhang, Alan L.\n  Yuille, Daguang Xu", "title": "C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image\n  Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D convolution neural networks (CNN) have been proved very successful in\nparsing organs or tumours in 3D medical images, but it remains sophisticated\nand time-consuming to choose or design proper 3D networks given different task\ncontexts. Recently, Neural Architecture Search (NAS) is proposed to solve this\nproblem by searching for the best network architecture automatically. However,\nthe inconsistency between search stage and deployment stage often exists in NAS\nalgorithms due to memory constraints and large search space, which could become\nmore serious when applying NAS to some memory and time consuming tasks, such as\n3D medical image segmentation. In this paper, we propose coarse-to-fine neural\narchitecture search (C2FNAS) to automatically search a 3D segmentation network\nfrom scratch without inconsistency on network size or input size. Specifically,\nwe divide the search procedure into two stages: 1) the coarse stage, where we\nsearch the macro-level topology of the network, i.e. how each convolution\nmodule is connected to other modules; 2) the fine stage, where we search at\nmicro-level for operations in each cell based on previous searched macro-level\ntopology. The coarse-to-fine manner divides the search procedure into two\nconsecutive stages and meanwhile resolves the inconsistency. We evaluate our\nmethod on 10 public datasets from Medical Segmentation Decalthon (MSD)\nchallenge, and achieve state-of-the-art performance with the network searched\nusing one dataset, which demonstrates the effectiveness and generalization of\nour searched models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:39:50 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 03:34:33 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yu", "Qihang", ""], ["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Bai", "Yutong", ""], ["Zhang", "Yixiao", ""], ["Yuille", "Alan L.", ""], ["Xu", "Daguang", ""]]}, {"id": "1912.09629", "submitter": "Yuliang Liu", "authors": "Yuliang Liu, Tong He, Hao Chen, Xinyu Wang, Canjie Luo, Shuaitao\n  Zhang, Chunhua Shen, Lianwen Jin", "title": "Exploring the Capacity of an Orderless Box Discretization Network for\n  Multi-orientation Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-orientation scene text detection has recently gained significant\nresearch attention. Previous methods directly predict words or text lines,\ntypically by using quadrilateral shapes. However, many of these methods neglect\nthe significance of consistent labeling, which is important for maintaining a\nstable training process, especially when it comprises a large amount of data.\nHere we solve this problem by proposing a new method, Orderless Box\nDiscretization (OBD), which first discretizes the quadrilateral box into\nseveral key edges containing all potential horizontal and vertical positions.\nTo decode accurate vertex positions, a simple yet effective matching procedure\nis proposed for reconstructing the quadrilateral bounding boxes. Our method\nsolves the ambiguity issue, which has a significant impact on the learning\nprocess. Extensive ablation studies are conducted to validate the effectiveness\nof our proposed method quantitatively. More importantly, based on OBD, we\nprovide a detailed analysis of the impact of a collection of refinements, which\nmay inspire others to build state-of-the-art text detectors. Combining both OBD\nand these useful refinements, we achieve state-of-the-art performance on\nvarious benchmarks, including ICDAR 2015 and MLT. Our method also won the first\nplace in the text detection task at the recent ICDAR2019 Robust Reading\nChallenge for Reading Chinese Text on Signboards, further demonstrating its\nsuperior performance. The code is available at https://git.io/TextDet.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:43:48 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 08:43:49 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 03:47:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Yuliang", ""], ["He", "Tong", ""], ["Chen", "Hao", ""], ["Wang", "Xinyu", ""], ["Luo", "Canjie", ""], ["Zhang", "Shuaitao", ""], ["Shen", "Chunhua", ""], ["Jin", "Lianwen", ""]]}, {"id": "1912.09632", "submitter": "Chenfeng Xu", "authors": "Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang\n  Bai, Masayoshi Tomizuka", "title": "AutoScale: Learning to Scale for Crowd Counting and Localization", "comments": "the code is available at https://github.com/dk-liang/AutoScale.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on crowd counting mainly leverage Convolutional Neural Networks\n(CNNs) to count by regressing density maps, and have achieved great progress.\nIn the density map, each person is represented by a Gaussian blob, and the\nfinal count is obtained from the integration of the whole map. However, it is\ndifficult to accurately predict the density map on dense regions. A major issue\nis that the density map on dense regions usually accumulates density values\nfrom a number of nearby Gaussian blobs, yielding different large density values\non a small set of pixels. This makes the density map present a long-tailed\ndistribution of pixel-wise density values. In this paper, we aim to address\nthis long-tailed distribution issue in the density map. Specifically, we\npropose a simple yet effective Learning to Scale (L2S) module, which\nautomatically scales dense regions into reasonable density levels. It\ndynamically separates the overlapped blobs, decomposes the accumulated values\nin the ground-truth density map, and thus alleviates the long-tailed\ndistribution of density values, which helps the model to better learn the\ndensity map. We also explore the effectiveness of L2S in localizing people by\nfinding the local minima of the quantized distance (w.r.t. person location\nmap), which has a similar issue as density map regression. To the best of our\nknowledge, such localization method is also novel in localization-based crowd\ncounting. We further introduce a customized dynamic cross-entropy loss,\nsignificantly improving the localization-based model optimization. Extensive\nexperiments demonstrate that the proposed framework termed AutoScale improves\nupon some state-of-the-art methods in both regression and localization\nbenchmarks on three crowded datasets and achieves very competitive performance\non two sparse datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:54:17 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 07:43:06 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 05:17:20 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Xu", "Chenfeng", ""], ["Liang", "Dingkang", ""], ["Xu", "Yongchao", ""], ["Bai", "Song", ""], ["Zhan", "Wei", ""], ["Bai", "Xiang", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1912.09640", "submitter": "Jieru Mei", "authors": "Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan\n  Yuille, Jianchao Yang", "title": "AtomNAS: Fine-Grained End-to-End Neural Architecture Search", "comments": "ICLR 2020 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search space design is very critical to neural architecture search (NAS)\nalgorithms. We propose a fine-grained search space comprised of atomic blocks,\na minimal search unit that is much smaller than the ones used in recent NAS\nalgorithms. This search space allows a mix of operations by composing different\ntypes of atomic blocks, while the search space in previous methods only allows\nhomogeneous operations. Based on this search space, we propose a resource-aware\narchitecture search framework which automatically assigns the computational\nresources (e.g., output channel numbers) for each operation by jointly\nconsidering the performance and the computational cost. In addition, to\naccelerate the search process, we propose a dynamic network shrinkage technique\nwhich prunes the atomic blocks with negligible influence on outputs on the fly.\nInstead of a search-and-retrain two-stage paradigm, our method simultaneously\nsearches and trains the target architecture. Our method achieves\nstate-of-the-art performance under several FLOPs configurations on ImageNet\nwith a small searching cost. We open our entire codebase at:\nhttps://github.com/meijieru/AtomNAS.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 04:42:43 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 13:08:53 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Mei", "Jieru", ""], ["Li", "Yingwei", ""], ["Lian", "Xiaochen", ""], ["Jin", "Xiaojie", ""], ["Yang", "Linjie", ""], ["Yuille", "Alan", ""], ["Yang", "Jianchao", ""]]}, {"id": "1912.09641", "submitter": "Xi Liu", "authors": "Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai\n  Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, Xiang Bai, Baoguang\n  Shi, Dimosthenis Karatzas, Shijian Lu, C. V. Jawahar", "title": "ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard", "comments": "International Conference on Document Analysis and Recognition, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese scene text reading is one of the most challenging problems in\ncomputer vision and has attracted great interest. Different from English text,\nChinese has more than 6000 commonly used characters and Chinesecharacters can\nbe arranged in various layouts with numerous fonts. The Chinese signboards in\nstreet view are a good choice for Chinese scene text images since they have\ndifferent backgrounds, fonts and layouts. We organized a competition called\nICDAR2019-ReCTS, which mainly focuses on reading Chinese text on signboard.\nThis report presents the final results of the competition. A large-scale\ndataset of 25,000 annotated signboard images, in which all the text lines and\ncharacters are annotated with locations and transcriptions, were released. Four\ntasks, namely character recognition, text line recognition, text line detection\nand end-to-end recognition were set up. Besides, considering the Chinese text\nambiguity issue, we proposed a multi ground truth (multi-GT) evaluation method\nto make evaluation fairer. The competition started on March 1, 2019 and ended\non April 30, 2019. 262 submissions from 46 teams are received. Most of the\nparticipants come from universities, research institutes, and tech companies in\nChina. There are also some participants from the United States, Australia,\nSingapore, and Korea. 21 teams submit results for Task 1, 23 teams submit\nresults for Task 2, 24 teams submit results for Task 3, and 13 teams submit\nresults for Task 4. The official website for the competition is\nhttp://rrc.cvc.uab.es/?ch=12.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 04:50:17 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Liu", "Xi", ""], ["Zhang", "Rui", ""], ["Zhou", "Yongsheng", ""], ["Jiang", "Qianyi", ""], ["Song", "Qi", ""], ["Li", "Nan", ""], ["Zhou", "Kai", ""], ["Wang", "Lei", ""], ["Wang", "Dong", ""], ["Liao", "Minghui", ""], ["Yang", "Mingkun", ""], ["Bai", "Xiang", ""], ["Shi", "Baoguang", ""], ["Karatzas", "Dimosthenis", ""], ["Lu", "Shijian", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1912.09654", "submitter": "Lin Zhao", "authors": "Lin Zhao, Wenbing Tao", "title": "JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel joint instance and semantic segmentation\napproach, which is called JSNet, in order to address the instance and semantic\nsegmentation of 3D point clouds simultaneously. Firstly, we build an effective\nbackbone network to extract robust features from the raw point clouds.\nSecondly, to obtain more discriminative features, a point cloud feature fusion\nmodule is proposed to fuse the different layer features of the backbone\nnetwork. Furthermore, a joint instance semantic segmentation module is\ndeveloped to transform semantic features into instance embedding space, and\nthen the transformed features are further fused with instance features to\nfacilitate instance segmentation. Meanwhile, this module also aggregates\ninstance features into semantic feature space to promote semantic segmentation.\nFinally, the instance predictions are generated by applying a simple mean-shift\nclustering on instance embeddings. As a result, we evaluate the proposed JSNet\non a large-scale 3D indoor point cloud dataset S3DIS and a part dataset\nShapeNet, and compare it with existing approaches. Experimental results\ndemonstrate our approach outperforms the state-of-the-art method in 3D instance\nsegmentation with a significant improvement in 3D semantic prediction and our\nmethod is also beneficial for part segmentation. The source code for this work\nis available at https://github.com/dlinzhao/JSNet.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 06:30:08 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zhao", "Lin", ""], ["Tao", "Wenbing", ""]]}, {"id": "1912.09666", "submitter": "Qing Jin", "authors": "Qing Jin, Linjie Yang, Zhenyu Liao", "title": "AdaBits: Neural Network Quantization with Adaptive Bit-Widths", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with adaptive configurations have gained increasing\nattention due to the instant and flexible deployment of these models on\nplatforms with different resource budgets. In this paper, we investigate a\nnovel option to achieve this goal by enabling adaptive bit-widths of weights\nand activations in the model. We first examine the benefits and challenges of\ntraining quantized model with adaptive bit-widths, and then experiment with\nseveral approaches including direct adaptation, progressive training and joint\ntraining. We discover that joint training is able to produce comparable\nperformance on the adaptive model as individual models. We further propose a\nnew technique named Switchable Clipping Level (S-CL) to further improve\nquantized models at the lowest bit-width. With our proposed techniques applied\non a bunch of models including MobileNet-V1/V2 and ResNet-50, we demonstrate\nthat bit-width of weights and activations is a new option for adaptively\nexecutable deep neural networks, offering a distinct opportunity for improved\naccuracy-efficiency trade-off as well as instant adaptation according to the\nplatform constraints in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:10:23 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 19:42:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Jin", "Qing", ""], ["Yang", "Linjie", ""], ["Liao", "Zhenyu", ""]]}, {"id": "1912.09670", "submitter": "Faqiang Liu", "authors": "Faqiang Liu, Mingkun Xu, Guoqi Li, Jing Pei, Luping Shi, Rong Zhao", "title": "Adversarial symmetric GANs: bridging adversarial samples and adversarial\n  networks", "comments": null, "journal-ref": "Neural networks,2020", "doi": "10.1016/j.neunet.2020.10.016", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks have achieved remarkable performance on\nvarious tasks but suffer from training instability. Despite many training\nstrategies proposed to improve training stability, this issue remains as a\nchallenge. In this paper, we investigate the training instability from the\nperspective of adversarial samples and reveal that adversarial training on fake\nsamples is implemented in vanilla GANs, but adversarial training on real\nsamples has long been overlooked. Consequently, the discriminator is extremely\nvulnerable to adversarial perturbation and the gradient given by the\ndiscriminator contains non-informative adversarial noises, which hinders the\ngenerator from catching the pattern of real samples. Here, we develop\nadversarial symmetric GANs (AS-GANs) that incorporate adversarial training of\nthe discriminator on real samples into vanilla GANs, making adversarial\ntraining symmetrical. The discriminator is therefore more robust and provides\nmore informative gradient with less adversarial noise, thereby stabilizing\ntraining and accelerating convergence. The effectiveness of the AS-GANs is\nverified on image generation on CIFAR-10 , CelebA, and LSUN with varied network\narchitectures. Not only the training is more stabilized, but the FID scores of\ngenerated samples are consistently improved by a large margin compared to the\nbaseline. The bridging of adversarial samples and adversarial networks provides\na new approach to further develop adversarial networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:20:13 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 01:29:37 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 06:20:27 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 06:48:08 GMT"}, {"version": "v5", "created": "Sat, 14 Nov 2020 01:05:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Liu", "Faqiang", ""], ["Xu", "Mingkun", ""], ["Li", "Guoqi", ""], ["Pei", "Jing", ""], ["Shi", "Luping", ""], ["Zhao", "Rong", ""]]}, {"id": "1912.09674", "submitter": "Hui Yuan", "authors": "Hao Liu, Hui Yuan, Qi Liu, Junhui Hou, Ju Liu", "title": "A Comprehensive Study and Comparison of Core Technologies for MPEG 3D\n  Point Cloud Compression", "comments": "17pages, has been accepted by IEEE Transactions on Boradcasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud based 3D visual representation is becoming popular due to its\nability to exhibit the real world in a more comprehensive and immersive way.\nHowever, under a limited network bandwidth, it is very challenging to\ncommunicate this kind of media due to its huge data volume. Therefore, the MPEG\nhave launched the standardization for point cloud compression (PCC), and\nproposed three model categories, i.e., TMC1, TMC2, and TMC3. Because the 3D\ngeometry compression methods of TMC1 and TMC3 are similar, TMC1 and TMC3 are\nfurther merged into a new platform namely TMC13. In this paper, we first\nintroduce some basic technologies that are usually used in 3D point cloud\ncompression, then review the encoder architectures of these test models in\ndetail, and finally analyze their rate distortion performance as well as\ncomplexity quantitatively for different cases (i.e., lossless geometry and\nlossless color, lossless geometry and lossy color, lossy geometry and lossy\ncolor) by using 16 benchmark 3D point clouds that are recommended by MPEG.\nExperimental results demonstrate that the coding efficiency of TMC2 is the best\non average (especially for lossy geometry and lossy color compression) for\ndense point clouds while TMC13 achieves the optimal coding performance for\nsparse and noisy point clouds with lower time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:23:20 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Liu", "Hao", ""], ["Yuan", "Hui", ""], ["Liu", "Qi", ""], ["Hou", "Junhui", ""], ["Liu", "Ju", ""]]}, {"id": "1912.09675", "submitter": "Hui Yuan", "authors": "Hui Yuan, Shiyun Zhao, Junhui Hou, Xuekai Wei, and Sam Kwong", "title": "Spatial and Temporal Consistency-Aware Dynamic Adaptive Streaming for\n  360-Degree Videos", "comments": "16 pages, This paper has been accepted by the IEEE Journal of\n  Selected Topics in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 360-degree video allows users to enjoy the whole scene by interactively\nswitching viewports. However, the huge data volume of the 360-degree video\nlimits its remote applications via network. To provide high quality of\nexperience (QoE) for remote web users, this paper presents a tile-based\nadaptive streaming method for 360-degree videos. First, we propose a simple yet\neffective rate adaptation algorithm to determine the requested bitrate for\ndownloading the current video segment by considering the balance between the\nbuffer length and video quality. Then, we propose to use a Gaussian model to\npredict the field of view at the beginning of each requested video segment. To\ndeal with the circumstance that the view angle is switched during the display\nof a video segment, we propose to download all the tiles in the 360-degree\nvideo with different priorities based on a Zipf model. Finally, in order to\nallocate bitrates for all the tiles, a two-stage optimization algorithm is\nproposed to preserve the quality of tiles in FoV and guarantee the spatial and\ntemporal smoothness. Experimental results demonstrate the effectiveness and\nadvantage of the proposed method compared with the state-of-the-art methods.\nThat is, our method preserves both the quality and the smoothness of tiles in\nFoV, thus providing the best QoE for users.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:41:02 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yuan", "Hui", ""], ["Zhao", "Shiyun", ""], ["Hou", "Junhui", ""], ["Wei", "Xuekai", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.09678", "submitter": "Qiang Wang", "authors": "Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao,\n  Xiaowen Chu", "title": "IRS: A Large Naturalistic Indoor Robotics Stereo Dataset to Train Deep\n  Models for Disparity and Surface Normal Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor robotics localization, navigation, and interaction heavily rely on\nscene understanding and reconstruction. Compared to the monocular vision which\nusually does not explicitly introduce any geometrical constraint, stereo\nvision-based schemes are more promising and robust to produce accurate\ngeometrical information, such as surface normal and depth/disparity. Besides,\ndeep learning models trained with large-scale datasets have shown their\nsuperior performance in many stereo vision tasks. However, existing stereo\ndatasets rarely contain the high-quality surface normal and disparity ground\ntruth, which hardly satisfies the demand of training a prospective deep model\nfor indoor scenes. To this end, we introduce a large-scale synthetic but\nnaturalistic indoor robotics stereo (IRS) dataset with over 100K stereo RGB\nimages and high-quality surface normal and disparity maps. Leveraging the\nadvanced rendering techniques of our customized rendering engine, the dataset\nis considerably close to the real-world captured images and covers several\nvisual effects, such as brightness changes, light reflection/transmission, lens\nflare, vivid shadow, etc. We compare the data distribution of IRS with existing\nstereo datasets to illustrate the typical visual attributes of indoor scenes.\nBesides, we present DTN-Net, a two-stage deep model for surface normal\nestimation. Extensive experiments show the advantages and effectiveness of IRS\nin training deep models for disparity estimation, and DTN-Net provides\nstate-of-the-art results for normal estimation compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:55:09 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 13:58:20 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Qiang", ""], ["Zheng", "Shizhen", ""], ["Yan", "Qingsong", ""], ["Deng", "Fei", ""], ["Zhao", "Kaiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1912.09685", "submitter": "Yang He", "authors": "Yang He, Shadi Rahimian, Bernt Schiele and Mario Fritz", "title": "Segmentations-Leak: Membership Inference Attacks and Defenses in\n  Semantic Image Segmentation", "comments": "Accepted to ECCV 2020. Code at:\n  https://github.com/SSAW14/segmentation_membership_inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's success of state of the art methods for semantic segmentation is\ndriven by large datasets. Data is considered an important asset that needs to\nbe protected, as the collection and annotation of such datasets comes at\nsignificant efforts and associated costs. In addition, visual data might\ncontain private or sensitive information, that makes it equally unsuited for\npublic release. Unfortunately, recent work on membership inference in the\nbroader area of adversarial machine learning and inference attacks on machine\nlearning models has shown that even black box classifiers leak information on\nthe dataset that they were trained on. We show that such membership inference\nattacks can be successfully carried out on complex, state of the art models for\nsemantic segmentation. In order to mitigate the associated risks, we also study\na series of defenses against such membership inference attacks and find\neffective counter measures against the existing risks with little effect on the\nutility of the segmentation method. Finally, we extensively evaluate our\nattacks and defenses on a range of relevant real-world datasets: Cityscapes,\nBDD100K, and Mapillary Vistas.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 08:11:47 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 22:06:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["He", "Yang", ""], ["Rahimian", "Shadi", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1912.09694", "submitter": "Haien Zeng", "authors": "Haien Zeng, Hanjiang Lai, Jian Yin", "title": "Controllable Face Aging", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the following two observations: 1) people are aging differently\nunder different conditions for changeable facial attributes, e.g., skin color\nmay become darker when working outside, and 2) it needs to keep some unchanged\nfacial attributes during the aging process, e.g., race and gender, we propose a\ncontrollable face aging method via attribute disentanglement generative\nadversarial network. To offer fine control over the synthesized face images,\nfirst, an individual embedding of the face is directly learned from an image\nthat contains the desired facial attribute. Second, since the image may contain\nother unwanted attributes, an attribute disentanglement network is used to\nseparate the individual embedding and learn the common embedding that contains\ninformation about the face attribute (e.g., race). With the common embedding,\nwe can manipulate the generated face image with the desired attribute in an\nexplicit manner. Experimental results on two common benchmarks demonstrate that\nour proposed generator achieves comparable performance on the aging effect with\nstate-of-the-art baselines while gaining more flexibility for attribute\ncontrol. Code is available at supplementary material.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 08:38:31 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zeng", "Haien", ""], ["Lai", "Hanjiang", ""], ["Yin", "Jian", ""]]}, {"id": "1912.09697", "submitter": "Xingkui Wei", "authors": "Xingkui Wei, Yinda Zhang, Zhuwen Li, Yanwei Fu and Xiangyang Xue", "title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment", "comments": "Accepted by ECCV2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from motion (SfM) is an essential computer vision problem which has\nnot been well handled by deep learning. One of the promising trends is to apply\nexplicit structural constraint, e.g. 3D cost volume, into the network. However,\nexisting methods usually assume accurate camera poses either from GT or other\nmethods, which is unrealistic in practice. In this work, we design a physical\ndriven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment\n(BA), which consists of two cost volume based architectures for depth and pose\nestimation respectively, iteratively running to improve both. The explicit\nconstraints on both depth (structure) and pose (motion), when combined with the\nlearning components, bring the merit from both traditional BA and emerging deep\nlearning technology. Extensive experiments on various datasets show that our\nmodel achieves the state-of-the-art performance on both depth and pose\nestimation with superior robustness against less number of inputs and the noise\nin initialization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 08:47:41 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 07:31:28 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wei", "Xingkui", ""], ["Zhang", "Yinda", ""], ["Li", "Zhuwen", ""], ["Fu", "Yanwei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1912.09720", "submitter": "Ali Mottaghi", "authors": "Ali Mottaghi and Serena Yeung", "title": "Adversarial Representation Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to develop label-efficient algorithms by querying the\nmost informative samples to be labeled by an oracle. The design of efficient\ntraining methods that require fewer labels is an important research direction\nthat allows more effective use of computational and human resources for\nlabeling and training deep neural networks. In this work, we demonstrate how we\ncan use recent advances in deep generative models, to outperform the\nstate-of-the-art in achieving the highest classification accuracy using as few\nlabels as possible. Unlike previous approaches, our approach uses not only\nlabeled images to train the classifier but also unlabeled images and generated\nimages for co-training the whole model. Our experiments show that the proposed\nmethod significantly outperforms existing approaches in active learning on a\nwide range of datasets (MNIST, CIFAR-10, SVHN, CelebA, and ImageNet).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:42:06 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Mottaghi", "Ali", ""], ["Yeung", "Serena", ""]]}, {"id": "1912.09745", "submitter": "Konstantinos Papadopoulos", "authors": "Konstantinos Papadopoulos, Enjie Ghorbel, Djamila Aouada, Bj\\\"orn\n  Ottersten", "title": "Vertex Feature Encoding and Hierarchical Temporal Modeling in a\n  Spatial-Temporal Graph Convolutional Network for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the Spatial-Temporal Graph Convolutional Network (ST-GCN)\nfor skeleton-based action recognition by introducing two novel modules, namely,\nthe Graph Vertex Feature Encoder (GVFE) and the Dilated Hierarchical Temporal\nConvolutional Network (DH-TCN). On the one hand, the GVFE module learns\nappropriate vertex features for action recognition by encoding raw skeleton\ndata into a new feature space. On the other hand, the DH-TCN module is capable\nof capturing both short-term and long-term temporal dependencies using a\nhierarchical dilated convolutional network. Experiments have been conducted on\nthe challenging NTU RGB-D-60 and NTU RGB-D 120 datasets. The obtained results\nshow that our method competes with state-of-the-art approaches while using a\nsmaller number of layers and parameters; thus reducing the required training\ntime and memory.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:39:07 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Papadopoulos", "Konstantinos", ""], ["Ghorbel", "Enjie", ""], ["Aouada", "Djamila", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1912.09748", "submitter": "Ting-Ting Liang", "authors": "Tingting Liang and Yongtao Wang and Qijie Zhao and huan zhang and Zhi\n  Tang and Haibin Ling", "title": "MFPN: A Novel Mixture Feature Pyramid Network of Multiple Architectures\n  for Object Detection", "comments": "7 pages, 3figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramids are widely exploited in many detectors to solve the scale\nvariation problem for object detection. In this paper, we first investigate the\nFeature Pyramid Network (FPN) architectures and briefly categorize them into\nthree typical fashions: top-down, bottom-up and fusing-splitting, which have\ntheir own merits for detecting small objects, large objects, and medium-sized\nobjects, respectively. Further, we design three FPNs of different architectures\nand propose a novel Mixture Feature Pyramid Network (MFPN) which inherits the\nmerits of all these three kinds of FPNs, by assembling the three kinds of FPNs\nin a parallel multi-branch architecture and mixing the features. MFPN can\nsignificantly enhance both one-stage and two-stage FPN-based detectors with\nabout 2 percent Average Precision(AP) increment on the MS-COCO benchmark, at\nlittle sacrifice in running time latency. By simply assembling MFPN with the\none-stage and two-stage baseline detectors, we achieve competitive single-model\ndetection results on the COCO detection benchmark without bells and whistles.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:44:27 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Liang", "Tingting", ""], ["Wang", "Yongtao", ""], ["Zhao", "Qijie", ""], ["zhang", "huan", ""], ["Tang", "Zhi", ""], ["Ling", "Haibin", ""]]}, {"id": "1912.09784", "submitter": "Chongxuan Li", "authors": "Chongxuan Li, Kun Xu, Jiashuo Liu, Jun Zhu, Bo Zhang", "title": "Triple Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified game-theoretical framework to perform classification and\nconditional image generation given limited supervision. It is formulated as a\nthree-player minimax game consisting of a generator, a classifier and a\ndiscriminator, and therefore is referred to as Triple Generative Adversarial\nNetwork (Triple-GAN). The generator and the classifier characterize the\nconditional distributions between images and labels to perform conditional\ngeneration and classification, respectively. The discriminator solely focuses\non identifying fake image-label pairs. Under a nonparametric assumption, we\nprove the unique equilibrium of the game is that the distributions\ncharacterized by the generator and the classifier converge to the data\ndistribution. As a byproduct of the three-player mechanism, Triple-GAN is\nflexible to incorporate different semi-supervised classifiers and GAN\narchitectures. We evaluate Triple-GAN in two challenging settings, namely,\nsemi-supervised learning and the extreme low data regime. In both settings,\nTriple-GAN can achieve excellent classification results and generate meaningful\nsamples in a specific class simultaneously. In particular, using a commonly\nadopted 13-layer CNN classifier, Triple-GAN outperforms extensive\nsemi-supervised learning methods substantially on more than 10 benchmarks no\nmatter data augmentation is applied or not.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:17:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 10:09:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Li", "Chongxuan", ""], ["Xu", "Kun", ""], ["Liu", "Jiashuo", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1912.09802", "submitter": "Markus Nagel", "authors": "Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen\n  Blankevoort, Max Welling", "title": "Taxonomy and Evaluation of Structured Compression of Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks in many real-world applications is\nleading to new challenges in building more efficient architectures. One\neffective way of making networks more efficient is neural network compression.\nWe provide an overview of existing neural network compression methods that can\nbe used to make neural networks more efficient by changing the architecture of\nthe network. First, we introduce a new way to categorize all published\ncompression methods, based on the amount of data and compute needed to make the\nmethods work in practice. These are three 'levels of compression solutions'.\nSecond, we provide a taxonomy of tensor factorization based and probabilistic\ncompression methods. Finally, we perform an extensive evaluation of different\ncompression techniques from the literature for models trained on ImageNet. We\nshow that SVD and probabilistic compression or pruning methods are\ncomplementary and give the best results of all the considered methods. We also\nprovide practical ways to combine them.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:11:44 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kuzmin", "Andrey", ""], ["Nagel", "Markus", ""], ["Pitre", "Saurabh", ""], ["Pendyam", "Sandeep", ""], ["Blankevoort", "Tijmen", ""], ["Welling", "Max", ""]]}, {"id": "1912.09847", "submitter": "Xiangxiang Qin", "authors": "Xiangxiang Qin", "title": "Transfer Learning with Edge Attention for Prostate MRI Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is one of the common diseases in men, and it is the most\ncommon malignant tumor in developed countries. Studies have shown that the male\nprostate incidence rate is as high as 2.5% to 16%, Currently, the inci-dence of\nprostate cancer in Asia is lower than that in the West, but it is increas-ing\nrapidly. If prostate cancer can be found as early as possible and treated in\ntime, it will have a high survival rate. Therefore, it is of great significance\nfor the diagnosis and treatment of prostate cancer. In this paper, we propose a\ntrans-fer learning method based on deep neural network for prostate MRI\nsegmenta-tion. In addition, we design a multi-level edge attention module using\nwavelet decomposition to overcome the difficulty of ambiguous boundary in\nprostate MRI segmentation tasks. The prostate images were provided by MICCAI\nGrand Challenge-Prostate MR Image Segmentation 2012 (PROMISE 12) challenge\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:32:16 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Qin", "Xiangxiang", ""]]}, {"id": "1912.09857", "submitter": "Bennet Breier", "authors": "Bennet Breier, Arno Onken", "title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of\n  Zebrafish Swim Bout Classification", "comments": "18 pages incl. references and appendix, 16 figures, ICLR 2020\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semmelhack et al. (2014) have achieved high classification accuracy in\ndistinguishing swim bouts of zebrafish using a Support Vector Machine (SVM).\nConvolutional Neural Networks (CNNs) have reached superior performance in\nvarious image recognition tasks over SVMs, but these powerful networks remain a\nblack box. Reaching better transparency helps to build trust in their\nclassifications and makes learned features interpretable to experts. Using a\nrecently developed technique called Deep Taylor Decomposition, we generated\nheatmaps to highlight input regions of high relevance for predictions. We find\nthat our CNN makes predictions by analyzing the steadiness of the tail's trunk,\nwhich markedly differs from the manually extracted features used by Semmelhack\net al. (2014). We further uncovered that the network paid attention to\nexperimental artifacts. Removing these artifacts ensured the validity of\npredictions. After correction, our best CNN beats the SVM by 6.12%, achieving a\nclassification accuracy of 96.32%. Our work thus demonstrates the utility of AI\nexplainability for CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:51:35 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Breier", "Bennet", ""], ["Onken", "Arno", ""]]}, {"id": "1912.09903", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi, Daniel Y.F. Chung, Constantin C. Coussios, J. Alison\n  Noble", "title": "Heterogeneous tissue characterization using ultrasound: a comparison of\n  fractal analysis backscatter models on liver tumors", "comments": "31 pages, 7 figures, 3 tables, journal article", "journal-ref": "Ultrasound in Medicine & Biology, vol. 42(7), pp. 1612-1626, 2016", "doi": "10.1016/j.ultrasmedbio.2016.02.007", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing tumor tissue heterogeneity via ultrasound has recently been\nsuggested for predicting early response to treatment. The ultrasound\nbackscattering characteristics can assist in better understanding the tumor\ntexture by highlighting local concentration and spatial arrangement of tissue\nscatterers. However, it is challenging to quantify the various tissue\nheterogeneities ranging from fine-to-coarse of the echo envelope peaks in tumor\ntexture. Local parametric fractal features extracted via maximum likelihood\nestimation from five well-known statistical model families are evaluated for\nthe purpose of ultrasound tissue characterization. The fractal dimension\n(self-similarity measure) was used to characterize the spatial distribution of\nscatterers, while the Lacunarity (sparsity measure) was applied to determine\nscatterer number density. Performance was assessed based on 608 cross-sectional\nclinical ultrasound RF images of liver tumors (230 and 378 demonstrating\nrespondent and non-respondent cases, respectively). Crossvalidation via\nleave-one-tumor-out and with different k-folds methodologies using a Bayesian\nclassifier were employed for validation. The fractal properties of the\nbackscattered echoes based on the Nakagami model (Nkg) and its extend\nfour-parameter Nakagami-generalized inverse Gaussian (NIG) distribution\nachieved best results - with nearly similar performance - for characterizing\nliver tumor tissue. Accuracy, sensitivity and specificity for the Nkg/NIG were:\n85.6%/86.3%, 94.0%/96.0%, and 73.0%/71.0%, respectively. Other statistical\nmodels, such as the Rician, Rayleigh, and K-distribution were found to not be\nas effective in characterizing the subtle changes in tissue texture as an\nindication of response to treatment. Employing the most relevant and practical\nstatistical model could have potential consequences for the design of an early\nand effective clinical therapy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:03:11 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Al-Kadi", "Omar S.", ""], ["Chung", "Daniel Y. F.", ""], ["Coussios", "Constantin C.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1912.09923", "submitter": "Julian Iseringhausen", "authors": "Jonathan Klein and Martin Laurenzis and Matthias B. Hullin and Julian\n  Iseringhausen", "title": "A Calibration Scheme for Non-Line-of-Sight Imaging Setups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have given rise to a large number of techniques for \"looking\naround corners\", i.e., for reconstructing occluded objects from time-resolved\nmeasurements of indirect light reflections off a wall. While the direct view of\ncameras is routinely calibrated in computer vision applications, the\ncalibration of non-line-of-sight setups has so far relied on manual measurement\nof the most important dimensions (device positions, wall position and\norientation, etc.). In this paper, we propose a semi-automatic method for\ncalibrating such systems that relies on mirrors as known targets. A roughly\ndetermined initialization is refined in order to optimize a spatio-temporal\nconsistency. Our system is general enough to be applicable to a variety of\nsensing scenarios ranging from single sources/detectors via scanning\narrangements to large-scale arrays. It is robust towards bad initialization and\nthe achieved accuracy is proportional to the depth resolution of the camera\nsystem. We demonstrate this capability with a real-world setup and despite a\nlarge number of dead pixels and very low temporal resolution achieve a result\nthat outperforms a manual calibration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:34:56 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Klein", "Jonathan", ""], ["Laurenzis", "Martin", ""], ["Hullin", "Matthias B.", ""], ["Iseringhausen", "Julian", ""]]}, {"id": "1912.09930", "submitter": "Joanna Materzynska", "authors": "Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang,\n  Trevor Darrell", "title": "Something-Else: Compositional Action Recognition with Spatial-Temporal\n  Interaction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action is naturally compositional: humans can easily recognize and\nperform actions with objects that are different from those used in training\ndemonstrations. In this paper, we study the compositionality of action by\nlooking into the dynamics of subject-object interactions. We propose a novel\nmodel which can explicitly reason about the geometric relations between\nconstituent objects and an agent performing an action. To train our model, we\ncollect dense object box annotations on the Something-Something dataset. We\npropose a novel compositional action recognition task where the training\ncombinations of verbs and nouns do not overlap with the test set. The novel\naspects of our model are applicable to activities with prominent object\ninteraction dynamics and to objects which can be tracked using state-of-the-art\napproaches; for activities without clearly defined spatial object-agent\ninteractions, we rely on baseline scene-level spatio-temporal representations.\nWe show the effectiveness of our approach not only on the proposed\ncompositional action recognition task, but also in a few-shot compositional\nsetting which requires the model to generalize across both object appearance\nand action category.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:44:14 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 09:55:33 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 08:24:58 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Materzynska", "Joanna", ""], ["Xiao", "Tete", ""], ["Herzig", "Roei", ""], ["Xu", "Huijuan", ""], ["Wang", "Xiaolong", ""], ["Darrell", "Trevor", ""]]}, {"id": "1912.09953", "submitter": "James Townsend", "authors": "James Townsend, Thomas Bird, Julius Kunze, David Barber", "title": "HiLLoC: Lossless Image Compression with Hierarchical Latent Variable\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make the following striking observation: fully convolutional VAE models\ntrained on 32x32 ImageNet can generalize well, not just to 64x64 but also to\nfar larger photographs, with no changes to the model. We use this property,\napplying fully convolutional models to lossless compression, demonstrating a\nmethod to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless\ncompression to large color photographs, and achieving state of the art for\ncompression of full size ImageNet images. We release Craystack, an open source\nlibrary for convenient prototyping of lossless compression using probabilistic\nmodels, along with full implementations of all of our compression results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 17:20:38 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Townsend", "James", ""], ["Bird", "Thomas", ""], ["Kunze", "Julius", ""], ["Barber", "David", ""]]}, {"id": "1912.09970", "submitter": "Zhigang Jia", "authors": "Meixiang Zhao, Zhigang Jia, Yunfeng Cai, Xiao Chen and Dunwei Gong", "title": "Advanced Variations of Two-Dimensional Principal Component Analysis for\n  Face Recognition", "comments": "arXiv admin note: text overlap with arXiv:1905.06458", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional principal component analysis (2DPCA) has become one of\nthe most powerful tools of artificial intelligent algorithms. In this paper, we\nreview 2DPCA and its variations, and propose a general ridge regression model\nto extract features from both row and column directions. To enhance the\ngeneralization ability of extracted features, a novel relaxed 2DPCA (R2DPCA) is\nproposed with a new ridge regression model. R2DPCA generates a weighting vector\nwith utilizing the label information, and maximizes a relaxed criterion with\napplying an optimal algorithm to get the essential features. The R2DPCA-based\napproaches for face recognition and image reconstruction are also proposed and\nthe selected principle components are weighted to enhance the role of main\ncomponents. Numerical experiments on well-known standard databases indicate\nthat R2DPCA has high generalization ability and can achieve a higher\nrecognition rate than the state-of-the-art methods, including in the deep\nlearning methods such as CNNs, DBNs, and DNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 08:38:24 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zhao", "Meixiang", ""], ["Jia", "Zhigang", ""], ["Cai", "Yunfeng", ""], ["Chen", "Xiao", ""], ["Gong", "Dunwei", ""]]}, {"id": "1912.09972", "submitter": "Mario Manzo", "authors": "Mario Manzo", "title": "Attributed Relational SIFT-based Regions Graph (ARSRG): concepts and\n  applications", "comments": "28 pages, 7 figures, submitted to Journal of Artificial Intelligence\n  Research (https://www.jair.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphs are widely adopted tools for encoding information. Generally, they are\napplied to disparate research fields where data needs to be represented in\nterms of local and spatial connections. In this context, a structure for\nditigal image representation, called Attributed Relational SIFT-based Regions\nGraph (ARSRG), previously introduced, is presented. ARSRG has not been explored\nin detail in previous works and for this reason the goal is to investigate\nunknown aspects. The study is divided into two parts. A first, theoretical,\nintroducing formal definitions, not yet specified previously, with purpose to\nclarify its structural configuration. A second, experimental, which provides\nfundamental elements about its adaptability and flexibility regarding different\napplications. The theoretical vision combined with the experimental one shows\nhow the structure is adaptable to image representation including contents of\ndifferent nature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 17:52:29 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Manzo", "Mario", ""]]}, {"id": "1912.09978", "submitter": "Ylenia Giarratano", "authors": "Ylenia Giarratano, Eleonora Bianchi, Calum Gray, Andrew Morris, Tom\n  MacGillivray, Baljean Dhillon, Miguel O. Bernabeu", "title": "Automated Segmentation of Optical Coherence Tomography Angiography\n  Images: Benchmark Data and Clinically Relevant Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging modality for the visualisation of microvasculature in vivo that has\nencountered broad adoption in retinal research. OCTA potential in the\nassessment of pathological conditions and the reproducibility of studies relies\non the quality of the image analysis. However, automated segmentation of\nparafoveal OCTA images is still an open problem. In this study, we generate the\nfirst open dataset of retinal parafoveal OCTA images with associated ground\ntruth manual segmentations. Furthermore, we establish a standard for OCTA image\nsegmentation by surveying a broad range of state-of-the-art vessel enhancement\nand binarisation procedures. We provide the most comprehensive comparison of\nthese methods under a unified framework to date. Our results show that, for the\nset of images considered, deep learning architectures (U-Net and CS-Net)\nachieve the best performance. For applications where manually segmented data is\nnot available to retrain these approaches, our findings suggest that optimal\noriented flux is the best handcrafted filter from those considered.\nFurthermore, we report on the importance of preserving network structure in the\nsegmentation to enable deep vascular phenotyping. We introduce new metrics for\nnetwork structure evaluation in segmented angiograms. Our results demonstrate\nthat segmentation methods with equal Dice score perform very differently in\nterms of network structure preservation. Moreover, we compare the error in the\ncomputation of clinically relevant vascular network metrics (e.g. foveal\navascular zone area and vessel density) across segmentation methods. Our\nresults show up to 25% differences in vessel density accuracy depending on the\nsegmentation method employed. These findings should be taken into account when\ncomparing the results of clinical studies and performing meta-analyses.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:03:09 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 18:16:20 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Giarratano", "Ylenia", ""], ["Bianchi", "Eleonora", ""], ["Gray", "Calum", ""], ["Morris", "Andrew", ""], ["MacGillivray", "Tom", ""], ["Dhillon", "Baljean", ""], ["Bernabeu", "Miguel O.", ""]]}, {"id": "1912.10013", "submitter": "Battista Biggio", "authors": "Marco Melis and Ambra Demontis and Maura Pintor and Angelo Sotgiu and\n  Battista Biggio", "title": "secml: A Python Library for Secure and Explainable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present secml, an open-source Python library for secure and explainable\nmachine learning. It implements the most popular attacks against machine\nlearning, including not only test-time evasion attacks to generate adversarial\nexamples against deep neural networks, but also training-time poisoning attacks\nagainst support vector machines and many other algorithms. These attacks enable\nevaluating the security of learning algorithms and of the corresponding\ndefenses under both white-box and black-box threat models. To this end, secml\nprovides built-in functions to compute security evaluation curves, showing how\nquickly classification performance decreases against increasing adversarial\nperturbations of the input data. secml also includes explainability methods to\nhelp understand why adversarial attacks succeed against a given model, by\nvisualizing the most influential features and training prototypes contributing\nto each decision. It is distributed under the Apache License 2.0, and hosted at\nhttps://gitlab.com/secml/secml.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:41:37 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Melis", "Marco", ""], ["Demontis", "Ambra", ""], ["Pintor", "Maura", ""], ["Sotgiu", "Angelo", ""], ["Biggio", "Battista", ""]]}, {"id": "1912.10016", "submitter": "Manuel Carbonell", "authors": "Manuel Carbonell, Alicia Forn\\'es, Mauricio Villegas, Josep Llad\\'os", "title": "A Neural Model for Text Localization, Transcription and Named Entity\n  Recognition in Full Pages", "comments": "To be published in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last years, the consolidation of deep neural network architectures for\ninformation extraction in document images has brought big improvements in the\nperformance of each of the tasks involved in this process, consisting of text\nlocalization, transcription, and named entity recognition. However, this\nprocess is traditionally performed with separate methods for each task. In this\nwork we propose an end-to-end model that combines a one stage object detection\nnetwork with branches for the recognition of text and named entities\nrespectively in a way that shared features can be learned simultaneously from\nthe training error of each of the tasks. By doing so the model jointly performs\nhandwritten text detection, transcription, and named entity recognition at page\nlevel with a single feed forward step. We exhaustively evaluate our approach on\ndifferent datasets, discussing its advantages and limitations compared to\nsequential approaches. The results show that the model is capable of benefiting\nfrom shared features for simultaneously solving interdependent tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:45:19 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 17:34:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Carbonell", "Manuel", ""], ["Forn\u00e9s", "Alicia", ""], ["Villegas", "Mauricio", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "1912.10021", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero, Nisha Srinivas, Esteban Villalobos, Jorge\n  Perez-Facuse, Roberto Rosenthal, Domingo Mery, Karl Ricanek, Kevin W. Bowyer", "title": "Identity Document to Selfie Face Matching Across Adolescence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching live images (``selfies'') to images from ID documents is a problem\nthat can arise in various applications. A challenging instance of the problem\narises when the face image on the ID document is from early adolescence and the\nlive image is from later adolescence. We explore this problem using a private\ndataset called Chilean Young Adult (CHIYA) dataset, where we match live face\nimages taken at age 18-19 to face images on ID documents created at ages 9 to\n18. State-of-the-art deep learning face matchers (e.g., ArcFace) have\nrelatively poor accuracy for document-to-selfie face matching. To achieve\nhigher accuracy, we fine-tune the best available open-source model with triplet\nloss for a few-shot learning. Experiments show that our approach achieves\nhigher accuracy than the DocFace+ model recently developed for this problem.\nOur fine-tuned model was able to improve the true acceptance rate for the most\ndifficult (largest age span) subset from 62.92% to 96.67% at a false acceptance\nrate of 0.01%. Our fine-tuned model is available for use by other researchers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:58:33 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["Srinivas", "Nisha", ""], ["Villalobos", "Esteban", ""], ["Perez-Facuse", "Jorge", ""], ["Rosenthal", "Roberto", ""], ["Mery", "Domingo", ""], ["Ricanek", "Karl", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "1912.10059", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Nader Karimi, Shadrokh Samavi", "title": "Saliency Based Fire Detection Using Texture and Color Features", "comments": "5 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to industry deployment and extension of urban areas, early warning\nsystems have an essential role in giving emergency. Fire is an event that can\nrapidly spread and cause injury, death, and damage. Early detection of fire\ncould significantly reduce these injuries. Video-based fire detection is a low\ncost and fast method in comparison with conventional fire detectors. Most\navailable fire detection methods have a high false-positive rate and low\naccuracy. In this paper, we increase accuracy by using spatial and temporal\nfeatures. Captured video sequences are divided into Spatio-temporal blocks.\nThen a saliency map and combination of color and texture features are used for\ndetecting fire regions. We use the HSV color model as a spatial feature and\nLBP-TOP for temporal processing of fire texture. Fire detection tests on\npublicly available datasets have shown the accuracy and robustness of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:05:19 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Jamali", "Maedeh", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.10087", "submitter": "Matteo Grimaldi", "authors": "Matteo Grimaldi, Valentino Peluso, Andrea Calimera", "title": "EAST: Encoding-Aware Sparse Training for Deep Memory Compression of\n  ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of Deep Convolutional Neural Networks (ConvNets) on tiny\nend-nodes with limited non-volatile memory space calls for smart compression\nstrategies capable of shrinking the footprint yet preserving predictive\naccuracy. There exist a number of strategies for this purpose, from those that\nplay with the topology of the model or the arithmetic precision, e.g. pruning\nand quantization, to those that operate a model agnostic compression, e.g.\nweight encoding. The tighter the memory constraint, the higher the probability\nthat these techniques alone cannot meet the requirement, hence more awareness\nand cooperation across different optimizations become mandatory. This work\naddresses the issue by introducing EAST, Encoding-Aware Sparse Training, a\nnovel memory-constrained training procedure that leads quantized ConvNets\ntowards deep memory compression. EAST implements an adaptive group pruning\ndesigned to maximize the compression rate of the weight encoding scheme (the\nLZ4 algorithm in this work). If compared to existing methods, EAST meets the\nmemory constraint with lower sparsity, hence ensuring higher accuracy. Results\nconducted on a state-of-the-art ConvNet (ResNet-9) deployed on a low-power\nmicrocontroller (ARM Cortex-M4) validate the proposal.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:20:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Grimaldi", "Matteo", ""], ["Peluso", "Valentino", ""], ["Calimera", "Andrea", ""]]}, {"id": "1912.10088", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti\n  Ghadiyaram, Alan Bovik", "title": "From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of\n  Picture Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blind or no-reference (NR) perceptual picture quality prediction is a\ndifficult, unsolved problem of great consequence to the social and streaming\nmedia industries that impacts billions of viewers daily. Unfortunately, popular\nNR prediction models perform poorly on real-world distorted pictures. To\nadvance progress on this problem, we introduce the largest (by far) subjective\npicture quality database, containing about 40000 real-world distorted pictures\nand 120000 patches, on which we collected about 4M human judgments of picture\nquality. Using these picture and patch quality labels, we built deep\nregion-based architectures that learn to produce state-of-the-art global\npicture quality predictions as well as useful local picture quality maps. Our\ninnovations include picture quality prediction architectures that produce\nglobal-to-local inferences as well as local-to-global inferences (via\nfeedback).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:22:55 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Niu", "Haoran", ""], ["Gupta", "Praful", ""], ["Mahajan", "Dhruv", ""], ["Ghadiyaram", "Deepti", ""], ["Bovik", "Alan", ""]]}, {"id": "1912.10103", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Andrea Calimera", "title": "TentacleNet: A Pseudo-Ensemble Template for Accurate Binary\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization is an attractive strategy for implementing lightweight Deep\nConvolutional Neural Networks (CNNs). Despite the unquestionable savings\noffered, memory footprint above all, it may induce an excessive accuracy loss\nthat prevents a widespread use. This work elaborates on this aspect introducing\nTentacleNet, a new template designed to improve the predictive performance of\nbinarized CNNs via parallelization. Inspired by the ensemble learning theory,\nit consists of a compact topology that is end-to-end trainable and organized to\nminimize memory utilization. Experimental results collected over three\nrealistic benchmarks show TentacleNet fills the gap left by classical binary\nmodels, ensuring substantial memory savings w.r.t. state-of-the-art binary\nensemble methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 21:18:16 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 12:37:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mocerino", "Luca", ""], ["Calimera", "Andrea", ""]]}, {"id": "1912.10107", "submitter": "Marc Bosch", "authors": "Joseph Nassar and Viveca Pavon-Harr and Marc Bosch and Ian McCulloh", "title": "Assessing Data Quality of Annotations with Krippendorff Alpha For\n  Applications in Computer Vision", "comments": "Accepted to AAAI Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current supervised deep learning frameworks rely on annotated data for\nmodeling the underlying data distribution of a given task. In particular for\ncomputer vision algorithms powered by deep learning, the quality of annotated\ndata is the most critical factor in achieving the desired algorithm\nperformance. Data annotation is, typically, a manual process where the\nannotator follows guidelines and operates in a best-guess manner. Labeling\ncriteria among annotators can show discrepancies in labeling results. This may\nimpact the algorithm inference performance. Given the popularity and widespread\nuse of deep learning among computer vision, more and more custom datasets are\nneeded to train neural networks to tackle different kinds of tasks.\nUnfortunately, there is no full understanding of the factors that affect\nannotated data quality, and how it translates into algorithm performance. In\nthis paper we studied this problem for object detection and recognition.We\nconducted several data annotation experiments to measure inter annotator\nagreement and consistency, as well as how the selection of ground truth impacts\nthe perceived algorithm performance.We propose a methodology to monitor the\nquality of annotations during the labeling of images and how it can be used to\nmeasure performance. We also show that neglecting to monitor the annotation\nprocess can result in significant loss in algorithm precision. Through these\nexperiments, we observe that knowledge of the labeling process, training data,\nand ground truth data used for algorithm evaluation are fundamental components\nto accurately assess trustworthiness of an AI system.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 21:23:42 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Nassar", "Joseph", ""], ["Pavon-Harr", "Viveca", ""], ["Bosch", "Marc", ""], ["McCulloh", "Ian", ""]]}, {"id": "1912.10120", "submitter": "Anjian Li", "authors": "Anjian Li, Somil Bansal, Georgios Giovanis, Varun Tolani, Claire\n  Tomlin, Mo Chen", "title": "Generating Robust Supervision for Learning-Based Visual Navigation Using\n  Hamilton-Jacobi Reachability", "comments": "Learning for Dynamics and Control (L4DC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bansal et al. (2019), a novel visual navigation framework that combines\nlearning-based and model-based approaches has been proposed. Specifically, a\nConvolutional Neural Network (CNN) predicts a waypoint that is used by the\ndynamics model for planning and tracking a trajectory to the waypoint. However,\nthe CNN inevitably makes prediction errors which often lead to collisions in\ncluttered and tight spaces. In this paper, we present a novel Hamilton-Jacobi\n(HJ) reachability-based method to generate supervision for the CNN for waypoint\nprediction in an unseen environment. By modeling CNN prediction error as\n\"disturbances\" in robot's dynamics, our generated waypoints are robust to these\ndisturbances, and consequently to the prediction errors. Moreover, using\nglobally optimal HJ reachability analysis leads to predicting waypoints that\nare time-efficient and avoid greedy behavior. Through simulations and hardware\nexperiments, we demonstrate the advantages of the proposed approach on\nnavigating through cluttered, narrow indoor environments.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:11:11 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 18:47:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Anjian", ""], ["Bansal", "Somil", ""], ["Giovanis", "Georgios", ""], ["Tolani", "Varun", ""], ["Tomlin", "Claire", ""], ["Chen", "Mo", ""]]}, {"id": "1912.10122", "submitter": "Da Chen", "authors": "Da Chen, Jean-Marie Mirebeau, Huazhong Shu and Laurent D. Cohen", "title": "Eikonal Region-based Active Contours for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimal path model based on the Eikonal partial differential equation\n(PDE) has served as a fundamental tool for the applications of image\nsegmentation and boundary detection in the passed three decades. However, the\nexisting minimal paths-based image segmentation approaches commonly rely on the\nimage boundary features, potentially limiting their performance in some\nsituations. In this paper, we introduce a new variational image segmentation\nmodel based on the minimal path framework and the Eikonal PDE, where the\nregion-based functional that defines the homogeneity criteria can be taken into\naccount for estimating the associated geodesic paths. This is done by\nestablishing a geodesic curve interpretation to the region-based active contour\nevolution problem. The image segmentation processing is carried out in an\niterative manner in our approach. A crucial ingredient in each iteration is to\nconstruct an asymmetric Randers geodesic metric using a sufficiently small\nvector field, such that a set of geodesic paths can be tracked from the\ngeodesic distance map which is the solution to an Eikonal PDE. The object\nboundary can be delineated by the concatenation of the final geodesic paths. We\ninvoke the Finsler variant of the fast marching method to estimate the geodesic\ndistance map, yielding an efficient implementation of the proposed Eikonal\nregion-based active contour model. Experimental results on both of the\nsynthetic and real images exhibit that our model indeed achieves encouraging\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:17:50 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "Da", ""], ["Mirebeau", "Jean-Marie", ""], ["Shu", "Huazhong", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "1912.10150", "submitter": "Zhenyi Wang", "authors": "Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong\n  Yuan, Changyou Chen", "title": "Learning Diverse Stochastic Human-Action Generators by Learning Smooth\n  Latent Transitions", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-motion generation is a long-standing challenging task due to the\nrequirement of accurately modeling complex and diverse dynamic patterns. Most\nexisting methods adopt sequence models such as RNN to directly model\ntransitions in the original action space. Due to high dimensionality and\npotential noise, such modeling of action transitions is particularly\nchallenging. In this paper, we focus on skeleton-based action generation and\npropose to model smooth and diverse transitions on a latent space of action\nsequences with much lower dimensionality. Conditioned on a latent sequence,\nactions are generated by a frame-wise decoder shared by all latent\naction-poses. Specifically, an implicit RNN is defined to model smooth latent\nsequences, whose randomness (diversity) is controlled by noise from the input.\nDifferent from standard action-prediction methods, our model can generate\naction sequences from pure noise without any conditional action poses.\nRemarkably, it can also generate unseen actions from mixed classes during\ntraining. Our model is learned with a bi-directional generative-adversarial-net\nframework, which not only can generate diverse action sequences of a particular\nclass or mix classes, but also learns to classify action sequences within the\nsame model. Experimental results show the superiority of our method in both\ndiverse action-sequence generation and classification, relative to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 00:02:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Zhenyi", ""], ["Yu", "Ping", ""], ["Zhao", "Yang", ""], ["Zhang", "Ruiyi", ""], ["Zhou", "Yufan", ""], ["Yuan", "Junsong", ""], ["Chen", "Changyou", ""]]}, {"id": "1912.10154", "submitter": "Yin Cui", "authors": "Yin Cui, Zeqi Gu, Dhruv Mahajan, Laurens van der Maaten, Serge\n  Belongie, Ser-Nam Lim", "title": "Measuring Dataset Granularity", "comments": "Code is available at:\n  https://github.com/richardaecn/dataset-granularity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing visibility of fine-grained recognition in our field,\n\"fine-grained'' has thus far lacked a precise definition. In this work,\nbuilding upon clustering theory, we pursue a framework for measuring dataset\ngranularity. We argue that dataset granularity should depend not only on the\ndata samples and their labels, but also on the distance function we choose. We\npropose an axiomatic framework to capture desired properties for a dataset\ngranularity measure and provide examples of measures that satisfy these\nproperties. We assess each measure via experiments on datasets with\nhierarchical labels of varying granularity. When measuring granularity in\ncommonly used datasets with our measure, we find that certain datasets that are\nwidely considered fine-grained in fact contain subsets of considerable size\nthat are substantially more coarse-grained than datasets generally regarded as\ncoarse-grained. We also investigate the interplay between dataset granularity\nwith a variety of factors and find that fine-grained datasets are more\ndifficult to learn from, more difficult to transfer to, more difficult to\nperform few-shot learning with, and more vulnerable to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 00:44:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cui", "Yin", ""], ["Gu", "Zeqi", ""], ["Mahajan", "Dhruv", ""], ["van der Maaten", "Laurens", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1912.10178", "submitter": "Wenxiao Wang", "authors": "Wenxiao Wang, Shuai Zhao, Minghao Chen, Jinming Hu, Deng Cai, Haifeng\n  Liu", "title": "DBP: Discrimination Based Block-Level Pruning for Deep Model\n  Acceleration", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is one of the most popular methods of accelerating the\ninference of deep convolutional neural networks (CNNs). The dominant pruning\nmethods, filter-level pruning methods, evaluate their performance through the\nreduction ratio of computations and deem that a higher reduction ratio of\ncomputations is equivalent to a higher acceleration ratio in terms of inference\ntime. However, we argue that they are not equivalent if parallel computing is\nconsidered. Given that filter-level pruning only prunes filters in layers and\ncomputations in a layer usually run in parallel, most computations reduced by\nfilter-level pruning usually run in parallel with the un-reduced ones. Thus,\nthe acceleration ratio of filter-level pruning is limited. To get a higher\nacceleration ratio, it is better to prune redundant layers because computations\nof different layers cannot run in parallel. In this paper, we propose our\nDiscrimination based Block-level Pruning method (DBP). Specifically, DBP takes\na sequence of consecutive layers (e.g., Conv-BN-ReLu) as a block and removes\nredundant blocks according to the discrimination of their output features. As a\nresult, DBP achieves a considerable acceleration ratio by reducing the depth of\nCNNs. Extensive experiments show that DBP has surpassed state-of-the-art\nfilter-level pruning methods in both accuracy and acceleration ratio. Our code\nwill be made available soon.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 02:11:47 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Wenxiao", ""], ["Zhao", "Shuai", ""], ["Chen", "Minghao", ""], ["Hu", "Jinming", ""], ["Cai", "Deng", ""], ["Liu", "Haifeng", ""]]}, {"id": "1912.10185", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu", "title": "Jacobian Adversarially Regularized Networks for Robustness", "comments": "ICLR 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples are crafted with imperceptible perturbations with the\nintent to fool neural networks. Against such attacks, adversarial training and\nits variants stand as the strongest defense to date. Previous studies have\npointed out that robust models that have undergone adversarial training tend to\nproduce more salient and interpretable Jacobian matrices than their non-robust\ncounterparts. A natural question is whether a model trained with an objective\nto produce salient Jacobian can result in better robustness. This paper answers\nthis question with affirmative empirical results. We propose Jacobian\nAdversarially Regularized Networks (JARN) as a method to optimize the saliency\nof a classifier's Jacobian by adversarially regularizing the model's Jacobian\nto resemble natural training images. Image classifiers trained with JARN show\nimproved robust accuracy compared to standard models on the MNIST, SVHN and\nCIFAR-10 datasets, uncovering a new angle to boost robustness without using\nadversarial training examples.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 02:46:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 08:06:12 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew Soon", ""], ["Fu", "Jie", ""]]}, {"id": "1912.10193", "submitter": "Huibing Wang", "authors": "Jinjia Peng, Guangqi Jiang, Dongyan Chen, Tongtong Zhao, Huibing Wang,\n  Xianping Fu", "title": "Eliminating cross-camera bias for vehicle re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (reID) often requires recognize a target vehicle in\nlarge datasets captured from multi-cameras. It plays an important role in the\nautomatic analysis of the increasing urban surveillance videos, which has\nbecome a hot topic in recent years. However, the appearance of vehicle images\nis easily affected by the environment that various illuminations, different\nbackgrounds and viewpoints, which leads to the large bias between different\ncameras. To address this problem, this paper proposes a cross-camera adaptation\nframework (CCA), which smooths the bias by exploiting the common space between\ncameras for all samples. CCA first transfers images from multi-cameras into one\ncamera to reduce the impact of the illumination and resolution, which generates\nthe samples with the similar distribution. Then, to eliminate the influence of\nbackground and focus on the valuable parts, we propose an attention alignment\nnetwork (AANet) to learn powerful features for vehicle reID. Specially, in\nAANet, the spatial transfer network with attention module is introduced to\nlocate a series of the most discriminative regions with high-attention weights\nand suppress the background. Moreover, comprehensive experimental results have\ndemonstrated that our proposed CCA can achieve excellent performances on\nbenchmark datasets VehicleID and VeRi-776.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 04:14:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Peng", "Jinjia", ""], ["Jiang", "Guangqi", ""], ["Chen", "Dongyan", ""], ["Zhao", "Tongtong", ""], ["Wang", "Huibing", ""], ["Fu", "Xianping", ""]]}, {"id": "1912.10201", "submitter": "Mehmet Turkan", "authors": "Yigit Oktar, Diclehan Karakaya, Oguzhan Ulucan, Mehmet Turkan", "title": "Convolutional Neural Networks: A Binocular Vision Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is arguable that whether the single camera captured (monocular) image\ndatasets are sufficient enough to train and test convolutional neural networks\n(CNNs) for imitating the biological neural network structures of the human\nbrain. As human visual system works in binocular, the collaboration of the eyes\nwith the two brain lobes needs more investigation for improvements in such\nCNN-based visual imagery analysis applications. It is indeed questionable that\nif respective visual fields of each eye and the associated brain lobes are\nresponsible for different learning abilities of the same scene. There are such\nopen questions in this field of research which need rigorous investigation in\norder to further understand the nature of the human visual system, hence\nimprove the currently available deep learning applications. This position paper\nanalyses a binocular CNNs architecture that is more analogous to the biological\nstructure of the human visual system than the conventional deep learning\ntechniques. While taking a structure called optic chiasma into account, this\narchitecture consists of basically two parallel CNN structures associated with\neach visual field and the brain lobe, fully connected later possibly as in the\nprimary visual cortex (V1). Experimental results demonstrate that binocular\nlearning of two different visual fields leads to better classification rates on\naverage, when compared to classical CNN architectures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:17:12 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Oktar", "Yigit", ""], ["Karakaya", "Diclehan", ""], ["Ulucan", "Oguzhan", ""], ["Turkan", "Mehmet", ""]]}, {"id": "1912.10205", "submitter": "Tianwei Wang", "authors": "Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xiaoxue Chen,\n  Yaqiang Wu, Qianying Wang and Mingxiang Cai", "title": "Decoupled Attention Network for Text Recognition", "comments": "9 pages, 8 figures, 6 tables, accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text recognition has attracted considerable research interests because of its\nvarious applications. The cutting-edge text recognition methods are based on\nattention mechanisms. However, most of attention methods usually suffer from\nserious alignment problem due to its recurrency alignment operation, where the\nalignment relies on historical decoding results. To remedy this issue, we\npropose a decoupled attention network (DAN), which decouples the alignment\noperation from using historical decoding results. DAN is an effective, flexible\nand robust end-to-end text recognizer, which consists of three components: 1) a\nfeature encoder that extracts visual features from the input image; 2) a\nconvolutional alignment module that performs the alignment operation based on\nvisual features from the encoder; and 3) a decoupled text decoder that makes\nfinal prediction by jointly using the feature map and attention maps.\nExperimental results show that DAN achieves state-of-the-art performance on\nmultiple text recognition tasks, including offline handwritten text recognition\nand regular/irregular scene text recognition.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:51:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Tianwei", ""], ["Zhu", "Yuanzhi", ""], ["Jin", "Lianwen", ""], ["Luo", "Canjie", ""], ["Chen", "Xiaoxue", ""], ["Wu", "Yaqiang", ""], ["Wang", "Qianying", ""], ["Cai", "Mingxiang", ""]]}, {"id": "1912.10207", "submitter": "Qing Jin", "authors": "Qing Jin, Linjie Yang, Zhenyu Liao", "title": "Towards Efficient Training for Neural Network Quantization", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization reduces computation costs of neural networks but suffers from\nperformance degeneration. Is this accuracy drop due to the reduced capacity, or\ninefficient training during the quantization procedure? After looking into the\ngradient propagation process of neural networks by viewing the weights and\nintermediate activations as random variables, we discover two critical rules\nfor efficient training. Recent quantization approaches violates the two rules\nand results in degenerated convergence. To deal with this problem, we propose a\nsimple yet effective technique, named scale-adjusted training (SAT), to comply\nwith the discovered rules and facilitates efficient training. We also analyze\nthe quantization error introduced in calculating the gradient in the popular\nparameterized clipping activation (PACT) technique. Through SAT together with\ngradient-calibrated PACT, quantized models obtain comparable or even better\nperformance than their full-precision counterparts, achieving state-of-the-art\naccuracy with consistent improvement over previous quantization methods on a\nwide spectrum of models including MobileNet-V1/V2 and PreResNet-50.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:57:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Jin", "Qing", ""], ["Yang", "Linjie", ""], ["Liao", "Zhenyu", ""]]}, {"id": "1912.10220", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "Spatio-Temporal Segmentation in 3D Echocardiographic Sequences using\n  Fractional Brownian Motion", "comments": "11 pages, 10 figures, 2 tables, journal article", "journal-ref": "IEEE Transactions on Biomedical Engineering, 2019", "doi": "10.1109/TBME.2019.2958701", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect for an improved cardiac functional analysis is the\naccurate segmentation of the left ventricle (LV). A novel approach for\nfully-automated segmentation of the LV endocardium and epicardium contours is\npresented. This is mainly based on the natural physical characteristics of the\nLV shape structure. Both sides of the LV boundaries exhibit natural elliptical\ncurvatures by having details on various scales, i.e. exhibiting fractal-like\ncharacteristics. The fractional Brownian motion (fBm), which is a\nnon-stationary stochastic process, integrates well with the stochastic nature\nof ultrasound echoes. It has the advantage of representing a wide range of\nnon-stationary signals and can quantify statistical local self-similarity\nthroughout the time-sequence ultrasound images. The locally characterized\nboundaries of the fBm segmented LV were further iteratively refined using\nglobal information by means of second-order moments. The method is benchmarked\nusing synthetic 3D+time echocardiographic sequences for normal and different\nischemic cardiomyopathy, and results compared with state-of-the-art LV\nsegmentation. Furthermore, the framework was validated against real data from\ncanine cases with expert-defined segmentations and demonstrated improved\naccuracy. The fBm-based segmentation algorithm is fully automatic and has the\npotential to be used clinically together with 3D echocardiography for improved\ncardiovascular disease diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 08:22:18 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1912.10227", "submitter": "Xin Ma", "authors": "Xin Ma, Yi Li, Huaibo Huang, Mandi Luo, Ran He", "title": "Exploiting Style and Attention in Real-World Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world image super-resolution (SR) is a challenging image translation\nproblem. Low-resolution (LR) images are often generated by various unknown\ntransformations rather than by applying simple bilinear down-sampling on\nhigh-resolution (HR) images. To address this issue, this paper proposes a novel\npipeline which exploits style and attention mechanism in real-world SR. Our\npipeline consists of a style Variational Autoencoder (styleVAE) and a SR\nnetwork incorporated with attention mechanism. To get real-world-like\nlow-quality images paired with the HR images, we design the styleVAE to\ntransfer the complex nuisance factors in real-world LR images to the generated\nLR images. We also use mutual information estimation (MI) to get better style\ninformation. For our SR network, we firstly propose a global attention residual\nblock to learn long-range dependencies in images. Then another local attention\nresidual block is proposed to enforce the attention of SR network moving to\nlocal areas of images in which texture detail will be filled. It is worth\nnoticing that styleVAE can be presented in a plug-and-play manner and thus can\nhelp to improve the generalization and robustness of our SR method as well as\nother SR methods. Extensive experiments demonstrate that our method surpasses\nthe state-of-the-art work, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:10:53 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 11:58:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ma", "Xin", ""], ["Li", "Yi", ""], ["Huang", "Huaibo", ""], ["Luo", "Mandi", ""], ["He", "Ran", ""]]}, {"id": "1912.10230", "submitter": "Erdem Akag\\\"und\\\"uz", "authors": "Irem Ulku and Erdem Akagunduz", "title": "A Survey on Deep Learning-based Architectures for Semantic Segmentation\n  on 2D images", "comments": "Updated with new studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is the pixel-wise labelling of an image. Since the\nproblem is defined at the pixel level, determining image class labels only is\nnot acceptable, but localising them at the original image pixel resolution is\nnecessary. Boosted by the extraordinary ability of convolutional neural\nnetworks (CNN) in creating semantic, high level and hierarchical image\nfeatures; several deep learning-based 2D semantic segmentation approaches have\nbeen proposed within the last decade. In this survey, we mainly focus on the\nrecent scientific developments in semantic segmentation, specifically on deep\nlearning-based methods using 2D images. We started with an analysis of the\npublic image sets and leaderboards for 2D semantic segmentation, with an\noverview of the techniques employed in performance evaluation. In examining the\nevolution of the field, we chronologically categorised the approaches into\nthree main periods, namely pre-and early deep learning era, the fully\nconvolutional era, and the post-FCN era. We technically analysed the solutions\nput forward in terms of solving the fundamental problems of the field, such as\nfine-grained localisation and scale invariance. Before drawing our conclusions,\nwe present a table of methods from all mentioned eras, with a summary of each\napproach that explains their contribution to the field. We conclude the survey\nby discussing the current challenges of the field and to what extent they have\nbeen solved.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:31:09 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 15:05:12 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 19:40:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ulku", "Irem", ""], ["Akagunduz", "Erdem", ""]]}, {"id": "1912.10233", "submitter": "Deli Zhao", "authors": "Deli Zhao and Jiapeng Zhu and Bo Zhang", "title": "Latent Variables on Spheres for Autoencoders in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Auto-Encoder (VAE) has been widely applied as a fundamental\ngenerative model in machine learning. For complex samples like imagery objects\nor scenes, however, VAE suffers from the dimensional dilemma between\nreconstruction precision that needs high-dimensional latent codes and\nprobabilistic inference that favors a low-dimensional latent space. By virtue\nof high-dimensional geometry, we propose a very simple algorithm, called\nSpherical Auto-Encoder (SAE), completely different from existing VAEs to\naddress the issue. SAE is in essence the vanilla autoencoder with spherical\nnormalization on the latent space. We analyze the unique characteristics of\nrandom variables on spheres in high dimensions and argue that random variables\non spheres are agnostic to various prior distributions and data modes when the\ndimension is sufficiently high. Therefore, SAE can harness a high-dimensional\nlatent space to improve the inference precision of latent codes while maintain\nthe property of stochastic sampling from priors. The experiments on sampling\nand inference validate our theoretical analysis and the superiority of SAE.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:53:53 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 02:20:03 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Deli", ""], ["Zhu", "Jiapeng", ""], ["Zhang", "Bo", ""]]}, {"id": "1912.10241", "submitter": "Sudip Das", "authors": "Sudip Das, Partha Sarathi Mukherjee, Ujjwal Bhattacharya", "title": "Seek and You Will Find: A New Optimized Framework for Efficient\n  Detection of Pedestrian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of object detection and localization, particularly pedestrian\ndetection have received considerable attention in recent times due to its\nseveral prospective applications such as surveillance, driving assistance,\nautonomous cars, etc. Also, a significant trend of latest research studies in\nrelated problem areas is the use of sophisticated Deep Learning based\napproaches to improve the benchmark performance on various standard datasets. A\ntrade-off between the speed (number of video frames processed per second) and\ndetection accuracy has often been reported in the existing literature. In this\narticle, we present a new but simple deep learning based strategy for\npedestrian detection that improves this trade-off. Since training of similar\nmodels using publicly available sample datasets failed to improve the detection\nperformance to some significant extent, particularly for the instances of\npedestrians of smaller sizes, we have developed a new sample dataset consisting\nof more than 80K annotated pedestrian figures in videos recorded under varying\ntraffic conditions. Performance of the proposed model on the test samples of\nthe new dataset and two other existing datasets, namely Caltech Pedestrian\nDataset (CPD) and CityPerson Dataset (CD) have been obtained. Our proposed\nsystem shows nearly 16\\% improvement over the existing state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 10:33:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Das", "Sudip", ""], ["Mukherjee", "Partha Sarathi", ""], ["Bhattacharya", "Ujjwal", ""]]}, {"id": "1912.10256", "submitter": "Wenbo Hu", "authors": "Wen-Jin Fu, Xiao-Jun Wu, He-Feng Yin, Wen-Bo Hu", "title": "Research on Clustering Performance of Sparse Subspace Clustering", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sparse subspace clustering has been a valid tool to deal with\nhigh-dimensional data. There are two essential steps in the framework of sparse\nsubspace clustering. One is solving the coefficient matrix of data, and the\nother is constructing the affinity matrix from the coefficient matrix, which is\napplied to the spectral clustering. This paper investigates the factors which\naffect clustering performance from both clustering accuracy and stability of\nthe approaches based on existing algorithms. We select four methods to solve\nthe coefficient matrix and use four different ways to construct a similarity\nmatrix for each coefficient matrix. Then we compare the clustering performance\nof different combinations on three datasets. The experimental results indicate\nthat both the coefficient matrix and affinity matrix have a huge influence on\nclustering performance and how to develop a stable and valid algorithm still\nneeds to be studied.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 12:41:08 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fu", "Wen-Jin", ""], ["Wu", "Xiao-Jun", ""], ["Yin", "He-Feng", ""], ["Hu", "Wen-Bo", ""]]}, {"id": "1912.10268", "submitter": "Snehal Bhayani", "authors": "Snehal Bhayani, Zuzana Kukelova and Janne Heikkil\\\"a", "title": "A sparse resultant based method for efficient minimal solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications require robust and efficient estimation of\ncamera geometry. The robust estimation is usually based on solving camera\ngeometry problems from a minimal number of input data measurements, i.e.\nsolving minimal problems in a RANSAC framework. Minimal problems often result\nin complex systems of polynomial equations. Many state-of-the-art efficient\npolynomial solvers to these problems are based on Gr\\\"obner bases and the\naction-matrix method that has been automatized and highly optimized in recent\nyears. In this paper we study an alternative algebraic method for solving\nsystems of polynomial equations, i.e., the sparse resultant-based method and\npropose a novel approach to convert the resultant constraint to an eigenvalue\nproblem. This technique can significantly improve the efficiency and stability\nof existing resultant-based solvers. We applied our new resultant-based method\nto a large variety of computer vision problems and show that for most of the\nconsidered problems, the new method leads to solvers that are the same size as\nthe the best available Gr\\\"obner basis solvers and of similar accuracy. For\nsome problems the new sparse-resultant based method leads to even smaller and\nmore stable solvers than the state-of-the-art Gr\\\"obner basis solvers. Our new\nmethod can be fully automatized and incorporated into existing tools for\nautomatic generation of efficient polynomial solvers and as such it represents\na competitive alternative to popular Gr\\\"obner basis methods for minimal\nproblems in computer vision.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 14:29:44 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bhayani", "Snehal", ""], ["Kukelova", "Zuzana", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1912.10269", "submitter": "Wang Nan", "authors": "Nan Wang, Yabin Zhou, Fenglei Han, Haitao Zhu, Jingzheng Yao", "title": "UWGAN: Underwater GAN for Real-world Underwater Color Restoration and\n  Dehazing", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world underwater environment, exploration of seabed resources,\nunderwater archaeology, and underwater fishing rely on a variety of sensors,\nvision sensor is the most important one due to its high information content,\nnon-intrusive, and passive nature. However, wavelength-dependent light\nattenuation and back-scattering result in color distortion and haze effect,\nwhich degrade the visibility of images. To address this problem, firstly, we\nproposed an unsupervised generative adversarial network (GAN) for generating\nrealistic underwater images (color distortion and haze effect) from in-air\nimage and depth map pairs based on improved underwater imaging model. Secondly,\nU-Net, which is trained efficiently using synthetic underwater dataset, is\nadopted for color restoration and dehazing. Our model directly reconstructs\nunderwater clear images using end-to-end autoencoder networks, while\nmaintaining scene content structural similarity. The results obtained by our\nmethod were compared with existing methods qualitatively and quantitatively.\nExperimental results obtained by the proposed model demonstrate well\nperformance on open real-world underwater datasets, and the processing speed\ncan reach up to 125FPS running on one NVIDIA 1060 GPU. Source code, sample\ndatasets are made publicly available at\nhttps://github.com/infrontofme/UWGAN_UIE.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 14:31:35 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 08:27:11 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Nan", ""], ["Zhou", "Yabin", ""], ["Han", "Fenglei", ""], ["Zhu", "Haitao", ""], ["Yao", "Jingzheng", ""]]}, {"id": "1912.10280", "submitter": "Xiao Wang", "authors": "Bo Jiang, Zitai Zhou, Xiao Wang, Jin Tang, Bin Luo", "title": "\\emph{cm}SalGAN: RGB-D Salient Object Detection with Cross-View\n  Generative Adversarial Networks", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image salient object detection (SOD) is an active research topic in computer\nvision and multimedia area. Fusing complementary information of RGB and depth\nhas been demonstrated to be effective for image salient object detection which\nis known as RGB-D salient object detection problem. The main challenge for\nRGB-D salient object detection is how to exploit the salient cues of both\nintra-modality (RGB, depth) and cross-modality simultaneously which is known as\ncross-modality detection problem. In this paper, we tackle this challenge by\ndesigning a novel cross-modality Saliency Generative Adversarial Network\n(\\emph{cm}SalGAN). \\emph{cm}SalGAN aims to learn an optimal view-invariant and\nconsistent pixel-level representation for RGB and depth images via a novel\nadversarial learning framework, which thus incorporates both information of\nintra-view and correlation information of cross-view images simultaneously for\nRGB-D saliency detection problem. To further improve the detection results, the\nattention mechanism and edge detection module are also incorporated into\n\\emph{cm}SalGAN. The entire \\emph{cm}SalGAN can be trained in an end-to-end\nmanner by using the standard deep neural network framework. Experimental\nresults show that \\emph{cm}SalGAN achieves the new state-of-the-art RGB-D\nsaliency detection performance on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 15:44:51 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 05:36:44 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Jiang", "Bo", ""], ["Zhou", "Zitai", ""], ["Wang", "Xiao", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1912.10293", "submitter": "Raghav Sardana", "authors": "Raghav Sardana (1), Rahul Kottath (1 and 2), Vinod Karar (1 and 2),\n  Shashi Poddar (1) ((1) CSIR-Central Scientific Instruments Organisation, (2)\n  Academy of Scientific & Innovative Research)", "title": "Joint Forward-Backward Visual Odometry for Stereo Cameras", "comments": "Accepted to Advances in Robotics 2019", "journal-ref": null, "doi": "10.1145/3352593.3352651", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual odometry is a widely used technique in the field of robotics and\nautomation to keep a track on the location of a robot using visual cues alone.\nIn this paper, we propose a joint forward backward visual odometry framework by\ncombining both, the forward motion and backward motion estimated from stereo\ncameras. The basic framework of LIBVIOS2 is used here for pose estimation as it\ncan run in real-time on standard CPUs. The complementary nature of errors in\nthe forward and backward mode of visual odometry helps in providing a refined\nmotion estimation upon combining these individual estimates. In addition, two\nreliability measures, that is, forward-backward relative pose error and\nforward-backward absolute pose error have been proposed for evaluating visual\nodometry frameworks on its own without the requirement of any ground truth\ndata. The proposed scheme is evaluated on the KITTI visual odometry dataset.\nThe experimental results demonstrate improved accuracy of the proposed scheme\nover the traditional odometry pipeline without much increase in the system\noverload.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 16:40:01 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Sardana", "Raghav", "", "1 and 2"], ["Kottath", "Rahul", "", "1 and 2"], ["Karar", "Vinod", "", "1 and 2"], ["Poddar", "Shashi", ""]]}, {"id": "1912.10308", "submitter": "Lei Kang", "authors": "Lei Kang, Pau Riba, Mauricio Villegas, Alicia Forn\\'es, Mar\\c{c}al\n  Rusi\\~nol", "title": "Candidate Fusion: Integrating Language Modelling into a\n  Sequence-to-Sequence Handwritten Word Recognition Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence models have recently become very popular for tackling\nhandwritten word recognition problems. However, how to effectively integrate an\nexternal language model into such recognizer is still a challenging problem.\nThe main challenge faced when training a language model is to deal with the\nlanguage model corpus which is usually different to the one used for training\nthe handwritten word recognition system. Thus, the bias between both word\ncorpora leads to incorrectness on the transcriptions, providing similar or even\nworse performances on the recognition task. In this work, we introduce\nCandidate Fusion, a novel way to integrate an external language model to a\nsequence-to-sequence architecture. Moreover, it provides suggestions from an\nexternal language knowledge, as a new input to the sequence-to-sequence\nrecognizer. Hence, Candidate Fusion provides two improvements. On the one hand,\nthe sequence-to-sequence recognizer has the flexibility not only to combine the\ninformation from itself and the language model, but also to choose the\nimportance of the information provided by the language model. On the other\nhand, the external language model has the ability to adapt itself to the\ntraining corpus and even learn the most commonly errors produced from the\nrecognizer. Finally, by conducting comprehensive experiments, the Candidate\nFusion proves to outperform the state-of-the-art language models for\nhandwritten word recognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:14:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kang", "Lei", ""], ["Riba", "Pau", ""], ["Villegas", "Mauricio", ""], ["Forn\u00e9s", "Alicia", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""]]}, {"id": "1912.10311", "submitter": "Daniel McDuff", "authors": "Daniel McDuff and Jonah Berger", "title": "Do Facial Expressions Predict Ad Sharing? A Large-Scale Observational\n  Study", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often share news and information with their social connections, but\nwhy do some advertisements get shared more than others? A large-scale test\nexamines whether facial responses predict sharing. Facial expressions play a\nkey role in emotional expression. Using scalable automated facial coding\nalgorithms, we quantify the facial expressions of thousands of individuals in\nresponse to hundreds of advertisements. Results suggest that not all emotions\nexpressed during viewing increase sharing, and that the relationship between\nemotion and transmission is more complex than mere valence alone. Facial\nactions linked to positive emotions (i.e., smiles) were associated with\nincreased sharing. But while some actions associated with negative emotion\n(e.g., lip depressor, associated with sadness) were linked to decreased\nsharing, others (i.e., nose wrinkles, associated with disgust) were linked to\nincreased sharing. The ability to quickly collect facial responses at scale in\npeoples' natural environment has important implications for marketers and opens\nup a range of avenues for further research.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:25:57 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["McDuff", "Daniel", ""], ["Berger", "Jonah", ""]]}, {"id": "1912.10314", "submitter": "Icaro Dourado", "authors": "Icaro Cavalcante Dourado, Salvatore Tabbone, Ricardo da Silva Torres", "title": "Multimodal Prediction based on Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning model, based on rank-fusion graphs, for\ngeneral applicability in multimodal prediction tasks, such as multimodal\nregression and image classification. Rank-fusion graphs encode information from\nmultiple descriptors and retrieval models, thus being able to capture\nunderlying relationships between modalities, samples, and the collection\nitself. The solution is based on the encoding of multiple ranks for a query (or\ntest sample), defined according to different criteria, into a graph. Later, we\nproject the generated graph into an induced vector space, creating fusion\nvectors, targeting broader generality and efficiency. A fusion vector estimator\nis then built to infer whether a multimodal input object refers to a class or\nnot. Our method is capable of promoting a fusion model better than early-fusion\nand late-fusion alternatives. Performed experiments in the context of multiple\nmultimodal and visual datasets, as well as several descriptors and retrieval\nmodels, demonstrate that our learning model is highly effective for different\nprediction scenarios involving visual, textual, and multimodal features,\nyielding better effectiveness than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:47:35 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 00:26:14 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 15:25:17 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 14:14:04 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dourado", "Icaro Cavalcante", ""], ["Tabbone", "Salvatore", ""], ["Torres", "Ricardo da Silva", ""]]}, {"id": "1912.10321", "submitter": "Ari Heljakka", "authors": "Ari Heljakka, Yuxin Hou, Juho Kannala, Arno Solin", "title": "Deep Automodulators", "comments": "To appear in Advances in Neural Information Processing Systems\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new category of generative autoencoders called automodulators.\nThese networks can faithfully reproduce individual real-world input images like\nregular autoencoders, but also generate a fused sample from an arbitrary\ncombination of several such images, allowing instantaneous 'style-mixing' and\nother new applications. An automodulator decouples the data flow of decoder\noperations from statistical properties thereof and uses the latent vector to\nmodulate the former by the latter, with a principled approach for mutual\ndisentanglement of decoder layers. Prior work has explored similar decoder\narchitecture with GANs, but their focus has been on random sampling. A\ncorresponding autoencoder could operate on real input images. For the first\ntime, we show how to train such a general-purpose model with sharp outputs in\nhigh resolution, using novel training techniques, demonstrated on four image\ndata sets. Besides style-mixing, we show state-of-the-art results in\nautoencoder comparison, and visual image quality nearly indistinguishable from\nstate-of-the-art GANs. We expect the automodulator variants to become a useful\nbuilding block for image applications and other data domains.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 19:16:33 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 18:01:55 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 16:54:08 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 12:58:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Heljakka", "Ari", ""], ["Hou", "Yuxin", ""], ["Kannala", "Juho", ""], ["Solin", "Arno", ""]]}, {"id": "1912.10336", "submitter": "Chao Qu `", "authors": "Chao Qu, Ty Nguyen, Camillo J. Taylor", "title": "Depth Completion via Deep Basis Fitting", "comments": "WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the task of image-guided depth completion where our\nsystem must infer the depth at every pixel of an input image based on the image\ncontent and a sparse set of depth measurements. We propose a novel approach\nthat builds upon the strengths of modern deep learning techniques and classical\noptimization algorithms and significantly improves performance. The proposed\nmethod replaces the final $1\\times 1$ convolutional layer employed in most\ndepth completion networks with a least squares fitting module which computes\nweights by fitting the implicit depth bases to the given sparse depth\nmeasurements. In addition, we show how our proposed method can be naturally\nextended to a multi-scale formulation for improved self-supervised training. We\ndemonstrate through extensive experiments on various datasets that our approach\nachieves consistent improvements over state-of-the-art baseline methods with\nsmall computational overhead.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:07:12 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Qu", "Chao", ""], ["Nguyen", "Ty", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1912.10338", "submitter": "El Wardani Dadi Dr.", "authors": "El Wardani Dadi", "title": "Tifinagh-IRCAM Handwritten character recognition using Deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit the benefits of the deep learning approach to\ndesign an efficient system of Amazigh handwritten recognition. Indeed, this\napproach has proved a greater efficiency in the various domains, especially\nrecognition tasks. However, to take full advantage of this approach it's\nnecessary to construct an adequate dataset of training and testing that\nrepresent faithfully the concerned problem. To this end, we have prepared our\ndataset of 102 writers each one contains 33 characters of IRCAM-Tifinagh.\nInspired by the MNIST database, the set of characters is size-normalized and\ncentered in a fixed-size image. The resulting is a grey level image of size\n28x28, where the black color is the non-color of the character. The number of\nimages produced after this preprocessing step is 3,366.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:12:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dadi", "El Wardani", ""]]}, {"id": "1912.10364", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Chuan-Sheng Foo, Hakan Bilen", "title": "Learning to Impute: A General Framework for Semi-supervised Learning", "comments": "Semi-supervised Learning, Meta-Learning, Learning to impute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent semi-supervised learning methods have shown to achieve comparable\nresults to their supervised counterparts while using only a small portion of\nlabels in image classification tasks thanks to their regularization strategies.\nIn this paper, we take a more direct approach for semi-supervised learning and\npropose learning to impute the labels of unlabeled samples such that a network\nachieves better generalization when it is trained on these labels. We pose the\nproblem in a learning-to-learn formulation which can easily be incorporated to\nthe state-of-the-art semi-supervised techniques and boost their performance\nespecially when the labels are limited. We demonstrate that our method is\napplicable to both classification and regression problems including image\nclassification and facial landmark detection tasks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 00:27:21 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 09:10:00 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 13:53:04 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Li", "Wei-Hong", ""], ["Foo", "Chuan-Sheng", ""], ["Bilen", "Hakan", ""]]}, {"id": "1912.10373", "submitter": "Ferdous Sohel", "authors": "F. Sohel, A. El-Sallam, and M. Bennamoun", "title": "Robust Pose Invariant Shape and Texture based Hand Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel personal identification and verification system\nusing information extracted from the hand shape and texture. The system has two\nmajor constituent modules: a fully automatic and robust peg free segmentation\nand pose normalisation module, and a recognition module. In the first module,\nthe hand is segmented from its background using a thresholding technique based\non Otsu`s method combined with a skin colour detector. A set of fully automatic\nalgorithms are then proposed to segment the palm and fingers. In these\nalgorithms, the skeleton and the contour of the hand and fingers are estimated\nand used to determine the global pose of the hand and the pose of each\nindividual finger. Finally the palm and fingers are cropped, pose corrected and\nnormalised. In the recognition module, various shape and texture based features\nare extracted and used for matching purposes. The modified Hausdorff distance,\nthe Iterative Closest Point (ICP) and Independent Component Analysis (ICA)\nalgorithms are used for shape and texture features of the fingers. For the\npalmprints, we use the Discrete Cosine Transform (DCT), directional line\nfeatures and ICA. Recognition (identification and verification) tests were\nperformed using fusion strategies based on the similarity scores of the fingers\nand the palm. Experimental results show that the proposed system exhibits a\nsuperior performance over existing systems with an accuracy of over 98\\% for\nhand identification and verification (at equal error rate) in a database of 560\ndifferent subjects.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 02:19:02 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sohel", "F.", ""], ["El-Sallam", "A.", ""], ["Bennamoun", "M.", ""]]}, {"id": "1912.10377", "submitter": "Muhammad Haroon Shakeel", "authors": "Waseem Abbas, Muhammad Haroon Shakeel, Numan Khurshid, Murtaza Taj", "title": "Patch-based Generative Adversarial Network Towards Retinal Vessel\n  Segmentation", "comments": null, "journal-ref": "Neural Information Processing. ICONIP 2019", "doi": "10.1007/978-3-030-36808-1_6", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal blood vessels are considered to be the reliable diagnostic biomarkers\nof ophthalmologic and diabetic retinopathy. Monitoring and diagnosis totally\ndepends on expert analysis of both thin and thick retinal vessels which has\nrecently been carried out by various artificial intelligent techniques.\nExisting deep learning methods attempt to segment retinal vessels using a\nunified loss function optimized for both thin and thick vessels with equal\nimportance. Due to variable thickness, biased distribution, and difference in\nspatial features of thin and thick vessels, unified loss function are more\ninfluential towards identification of thick vessels resulting in weak\nsegmentation. To address this problem, a conditional patch-based generative\nadversarial network is proposed which utilizes a generator network and a\npatch-based discriminator network conditioned on the sample data with an\nadditional loss function to learn both thin and thick vessels. Experiments are\nconducted on publicly available STARE and DRIVE datasets which show that the\nproposed model outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 03:33:31 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Abbas", "Waseem", ""], ["Shakeel", "Muhammad Haroon", ""], ["Khurshid", "Numan", ""], ["Taj", "Murtaza", ""]]}, {"id": "1912.10405", "submitter": "Boxiao Pan", "authors": "Boxiao Pan, Zhangjie Cao, Ehsan Adeli, Juan Carlos Niebles", "title": "Adversarial Cross-Domain Action Recognition with Co-Attention", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has been a widely studied topic with a heavy focus on\nsupervised learning involving sufficient labeled videos. However, the problem\nof cross-domain action recognition, where training and testing videos are drawn\nfrom different underlying distributions, remains largely under-explored.\nPrevious methods directly employ techniques for cross-domain image recognition,\nwhich tend to suffer from the severe temporal misalignment problem. This paper\nproposes a Temporal Co-attention Network (TCoN), which matches the\ndistributions of temporally aligned action features between source and target\ndomains using a novel cross-domain co-attention mechanism. Experimental results\non three cross-domain action recognition datasets demonstrate that TCoN\nimproves both previous single-domain and cross-domain methods significantly\nunder the cross-domain setting.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 09:39:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Pan", "Boxiao", ""], ["Cao", "Zhangjie", ""], ["Adeli", "Ehsan", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1912.10406", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Zhengxing Wu, Junzhi Yu, Li Wen", "title": "Rethinking Temporal Object Detection from Robotic Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object detection (VID) has been vigorously studied for years but almost\nall literature adopts a static accuracy-based evaluation, i.e., average\nprecision (AP). From a robotic perspective, the importance of recall continuity\nand localization stability is equal to that of accuracy, but the AP is\ninsufficient to reflect detectors' performance across time. In this paper,\nnon-reference assessments are proposed for continuity and stability based on\nobject tracklets. These temporal evaluations can serve as supplements to static\nAP. Further, we develop an online tracklet refinement for improving detectors'\ntemporal performance through short tracklet suppression, fragment filling, and\ntemporal location fusion.\n  In addition, we propose a small-overlap suppression to extend VID methods to\nsingle object tracking (SOT) task so that a flexible SOT-by-detection framework\nis then formed.\n  Extensive experiments are conducted on ImageNet VID dataset and real-world\nrobotic tasks, where the superiority of our proposed approaches are validated\nand verified. Codes will be publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 09:50:06 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 06:35:15 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Xingyu", ""], ["Wu", "Zhengxing", ""], ["Yu", "Junzhi", ""], ["Wen", "Li", ""]]}, {"id": "1912.10427", "submitter": "Jung Un Yun", "authors": "Jung Un Yun, In Kyu Park", "title": "Joint Face Super-Resolution and Deblurring Using a Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image super-resolution (SR) is an important preprocessing for facial\nimage analysis, face recognition, and image-based 3D face reconstruction.\nRecent convolutional neural network (CNN) based method has shown excellent\nperformance by learning mapping relation using pairs of low-resolution (LR) and\nhigh-resolution (HR) facial images. However, since the HR facial image\nreconstruction using CNN is conventionally aimed to increase the PSNR and SSIM\nmetrics, the reconstructed HR image might not be realistic even with high\nscores. An adversarial framework is proposed in this study to reconstruct the\nHR facial image by simultaneously generating an HR image with and without blur.\nFirst, the spatial resolution of the LR facial image is increased by eight\ntimes using a five-layer CNN. Then, the encoder extracts the features of the\nup-scaled image. These features are finally sent to two branches (decoders) to\ngenerate an HR facial image with and without blur. In addition, local and\nglobal discriminators are combined to focus on the reconstruction of HR facial\nstructures. Experiment results show that the proposed algorithm generates a\nrealistic HR facial image. Furthermore, the proposed method can generate a\nvariety of different facial images.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 12:03:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yun", "Jung Un", ""], ["Park", "In Kyu", ""]]}, {"id": "1912.10428", "submitter": "Christoph Mayer", "authors": "Christoph Mayer, Matthieu Paul, Radu Timofte", "title": "Adversarial Feature Distribution Alignment for Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with only a few labeled samples can lead to\noverfitting. This is problematic in semi-supervised learning where only a few\nlabeled samples are available. In this paper, we show that a consequence of\noverfitting in SSL is feature distribution misalignment between labeled and\nunlabeled samples. Hence, we propose a new feature distribution alignment\nmethod. Our method is particularly effective when using only a small amount of\nlabeled samples. We test our method on CIFAR10 and SVHN. On SVHN we achieve a\ntest error of 3.88% (250 labeled samples) and 3.39% (1000 labeled samples)\nwhich is close to the fully supervised model 2.89% (73k labeled samples). In\ncomparison, the current SOTA achieves only 4.29% and 3.74%. Finally, we provide\na theoretical insight why feature distribution alignment occurs and show that\nour method reduces it.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 12:07:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mayer", "Christoph", ""], ["Paul", "Matthieu", ""], ["Timofte", "Radu", ""]]}, {"id": "1912.10454", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, Marc Modat, M.\n  Jorge Cardoso, Sebastien Ourselin, Lauge Sorensen", "title": "On the Initialization of Long Short-Term Memory Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-36708-4_23", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight initialization is important for faster convergence and stability of\ndeep neural networks training. In this paper, a robust initialization method is\ndeveloped to address the training instability in long short-term memory (LSTM)\nnetworks. It is based on a normalized random initialization of the network\nweights that aims at preserving the variance of the network input and output in\nthe same range. The method is applied to standard LSTMs for univariate time\nseries regression and to LSTMs robust to missing values for multivariate\ndisease progression modeling. The results show that in all cases, the proposed\ninitialization method outperforms the state-of-the-art initialization\ntechniques in terms of training convergence and generalization performance of\nthe obtained solution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 14:19:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Modat", "Marc", ""], ["Cardoso", "M. Jorge", ""], ["Ourselin", "Sebastien", ""], ["Sorensen", "Lauge", ""]]}, {"id": "1912.10479", "submitter": "Xing Di", "authors": "Xing Di and Vishal M. Patel", "title": "Facial Synthesis from Visual Attributes via Sketch using Multi-Scale\n  Generators", "comments": "This work is accepted in IEEE Transactions on Biometrics, Behavior,\n  and Identity Science (T-BIOM). arXiv admin note: substantial text overlap\n  with arXiv:1801.00077", "journal-ref": null, "doi": "10.1109/TBIOM.2019.2961926", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic synthesis of faces from visual attributes is an important problem\nin computer vision and has wide applications in law enforcement and\nentertainment. With the advent of deep generative convolutional neural networks\n(CNNs), attempts have been made to synthesize face images from attributes and\ntext descriptions. In this paper, we take a different approach, where we\nformulate the original problem as a stage-wise learning problem. We first\nsynthesize the facial sketch corresponding to the visual attributes and then we\ngenerate the face image based on the synthesized sketch. The proposed\nframework, is based on a combination of two different Generative Adversarial\nNetworks (GANs) - (1) a sketch generator network which synthesizes realistic\nsketch from the input attributes, and (2) a face generator network which\nsynthesizes facial images from the synthesized sketch images with the help of\nfacial attributes. Extensive experiments and comparison with recent methods are\nperformed to verify the effectiveness of the proposed attribute-based two-stage\nface synthesis method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 22:53:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Di", "Xing", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1912.10493", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Zixuan Peng, Philipp Fuernstahl, Christine Tanner,\n  Orcun Goksel", "title": "Active Learning for Segmentation Based on Bayesian Sample Queries", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of anatomical structures is a fundamental image analysis task\nfor many applications in the medical field. Deep learning methods have been\nshown to perform well, but for this purpose large numbers of manual annotations\nare needed in the first place, which necessitate prohibitive levels of\nresources that are often unavailable. In an active learning framework of\nselecting informed samples for manual labeling, expert clinician time for\nmanual annotation can be optimally utilized, enabling the establishment of\nlarge labeled datasets for machine learning. In this paper, we propose a novel\nmethod that combines representativeness with uncertainty in order to estimate\nideal samples to be annotated, iteratively from a given dataset. Our novel\nrepresentativeness metric is based on Bayesian sampling, by using\ninformation-maximizing autoencoders. We conduct experiments on a shoulder\nmagnetic resonance imaging (MRI) dataset for the segmentation of four\nmusculoskeletal tissue classes. Quantitative results show that the annotation\nof representative samples selected by our proposed querying method yields an\nimproved segmentation performance at each active learning iteration, compared\nto a baseline method that also employs uncertainty and representativeness\nmetrics. For instance, with only 10% of the dataset annotated, our method\nreaches within 5% of Dice score expected from the upper bound scenario of all\nthe dataset given as annotated (an impractical scenario due to resource\nconstraints), and this gap drops down to a mere 2% when less than a fifth of\nthe dataset samples are annotated. Such active learning approach to selecting\nsamples to annotate enables an optimal use of the expert clinician time, being\noften the bottleneck in realizing machine learning solutions in medicine.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:49:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ozdemir", "Firat", ""], ["Peng", "Zixuan", ""], ["Fuernstahl", "Philipp", ""], ["Tanner", "Christine", ""], ["Goksel", "Orcun", ""]]}, {"id": "1912.10525", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou, Anton Aubanell, Ilaria Bonavita, Mario Ceresa,\n  Gemma Piella, Vicent Ribas, Miguel \\'Angel Gonz\\'alez Ballester", "title": "Re-Identification and Growth Detection of Pulmonary Nodules without\n  Image Registration Using 3D Siamese Neural Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer follow-up is a complex, error prone, and time consuming task for\nclinical radiologists. Several lung CT scan images taken at different time\npoints of a given patient need to be individually inspected, looking for\npossible cancerogenous nodules. Radiologists mainly focus their attention in\nnodule size, density, and growth to assess the existence of malignancy. In this\nstudy, we present a novel method based on a 3D siamese neural network, for the\nre-identification of nodules in a pair of CT scans of the same patient without\nthe need for image registration. The network was integrated into a two-stage\nautomatic pipeline to detect, match, and predict nodule growth given pairs of\nCT scans. Results on an independent test set reported a nodule detection\nsensitivity of 94.7%, an accuracy for temporal nodule matching of 88.8%, and a\nsensitivity of 92.0% with a precision of 88.4% for nodule growth detection.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 20:09:44 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Rafael-Palou", "Xavier", ""], ["Aubanell", "Anton", ""], ["Bonavita", "Ilaria", ""], ["Ceresa", "Mario", ""], ["Piella", "Gemma", ""], ["Ribas", "Vicent", ""], ["Ballester", "Miguel \u00c1ngel Gonz\u00e1lez", ""]]}, {"id": "1912.10545", "submitter": "Tao Hu", "authors": "Tao Hu, Geng Lin, Zhizhong Han, Matthias Zwicker", "title": "Learning to Generate Dense Point Clouds with Textures on Multiple\n  Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from images is a core problem in computer vision. With\nrecent advances in deep learning, it has become possible to recover plausible\n3D shapes even from single RGB images for the first time. However, obtaining\ndetailed geometry and texture for objects with arbitrary topology remains\nchallenging. In this paper, we propose a novel approach for reconstructing\npoint clouds from RGB images. Unlike other methods, we can recover dense point\nclouds with hundreds of thousands of points, and we also include RGB textures.\nIn addition, we train our model on multiple categories which leads to superior\ngeneralization to unseen categories compared to previous techniques. We achieve\nthis using a two-stage approach, where we first infer an object coordinate map\nfrom the input RGB image, and then obtain the final point cloud using a\nreprojection and completion step. We show results on standard benchmarks that\ndemonstrate the advantages of our technique. Code is available at\nhttps://github.com/TaoHuUMD/3D-Reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 21:40:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hu", "Tao", ""], ["Lin", "Geng", ""], ["Han", "Zhizhong", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1912.10557", "submitter": "Yuelong Li", "authors": "Vishal Monga, Yuelong Li and Yonina C. Eldar", "title": "Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal\n  and Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks provide unprecedented performance gains in many real\nworld problems in signal and image processing. Despite these gains, future\ndevelopment and practical deployment of deep networks is hindered by their\nblackbox nature, i.e., lack of interpretability, and by the need for very large\ntraining sets. An emerging technique called algorithm unrolling or unfolding\noffers promise in eliminating these issues by providing a concrete and\nsystematic connection between iterative algorithms that are used widely in\nsignal processing and deep neural networks. Unrolling methods were first\nproposed to develop fast neural network approximations for sparse coding. More\nrecently, this direction has attracted enormous attention and is rapidly\ngrowing both in theoretic investigations and practical applications. The\ngrowing popularity of unrolled deep networks is due in part to their potential\nin developing efficient, high-performance and yet interpretable network\narchitectures from reasonable size training sets. In this article, we review\nalgorithm unrolling for signal and image processing. We extensively cover\npopular techniques for algorithm unrolling in various domains of signal and\nimage processing including imaging, vision and recognition, and speech\nprocessing. By reviewing previous works, we reveal the connections between\niterative algorithms and neural networks and present recent theoretical\nresults. Finally, we provide a discussion on current limitations of unrolling\nand suggest possible future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 23:02:18 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 16:26:41 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 05:37:40 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Monga", "Vishal", ""], ["Li", "Yuelong", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1912.10589", "submitter": "Yuan Yao", "authors": "Yuan Yao, Nico Schertler, Enrique Rosales, Helge Rhodin, Leonid Sigal,\n  Alla Sheffer", "title": "Front2Back: Single View 3D Shape Reconstruction via Front to Back\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of a 3D shape from a single 2D image is a classical computer\nvision problem, whose difficulty stems from the inherent ambiguity of\nrecovering occluded or only partially observed surfaces. Recent methods address\nthis challenge through the use of largely unstructured neural networks that\neffectively distill conditional mapping and priors over 3D shape. In this work,\nwe induce structure and geometric constraints by leveraging three core\nobservations: (1) the surface of most everyday objects is often almost entirely\nexposed from pairs of typical opposite views; (2) everyday objects often\nexhibit global reflective symmetries which can be accurately predicted from\nsingle views; (3) opposite orthographic views of a 3D shape share consistent\nsilhouettes. Following these observations, we first predict orthographic 2.5D\nvisible surface maps (depth, normal and silhouette) from perspective 2D images,\nand detect global reflective symmetries in this data; second, we predict the\nback facing depth and normal maps using as input the front maps and, when\navailable, the symmetric reflections of these maps; and finally, we reconstruct\na 3D mesh from the union of these maps using a surface reconstruction method\nbest suited for this data. Our experiments demonstrate that our framework\noutperforms state-of-the art approaches for 3D shape reconstructions from 2D\nand 2.5D data in terms of input fidelity and details preservation.\nSpecifically, we achieve 12% better performance on average in ShapeNet\nbenchmark dataset, and up to 19% for certain classes of objects (e.g., chairs\nand vessels).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 02:27:05 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 23:51:41 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yao", "Yuan", ""], ["Schertler", "Nico", ""], ["Rosales", "Enrique", ""], ["Rhodin", "Helge", ""], ["Sigal", "Leonid", ""], ["Sheffer", "Alla", ""]]}, {"id": "1912.10609", "submitter": "Chong Huang", "authors": "Chong Huang, Yuanjie Dang, Peng Chen, Xin Yang, Kwang-Ting (Tim) Cheng", "title": "One-Shot Imitation Filming of Human Motion Videos", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning has been applied to mimic the operation of a human\ncameraman in several autonomous cinematography systems. To imitate different\nfilming styles, existing methods train multiple models, where each model\nhandles a particular style and requires a significant number of training\nsamples. As a result, existing methods can hardly generalize to unseen styles.\nIn this paper, we propose a framework, which can imitate a filming style by\n\"seeing\" only a single demonstration video of the same style, i.e., one-shot\nimitation filming. This is done by two key enabling techniques: 1) feature\nextraction of the filming style from the demo video, and 2) filming style\ntransfer from the demo video to the new situation. We implement the approach\nwith deep neural network and deploy it to a 6 degrees of freedom (DOF) real\ndrone cinematography system by first predicting the future camera motions, and\nthen converting them to the drone's control commands via an odometer. Our\nexperimental results on extensive datasets and showcases exhibit significant\nimprovements in our approach over conventional baselines and our approach can\nsuccessfully mimic the footage with an unseen style.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:50:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Huang", "Chong", "", "Tim"], ["Dang", "Yuanjie", "", "Tim"], ["Chen", "Peng", "", "Tim"], ["Yang", "Xin", "", "Tim"], ["Kwang-Ting", "", "", "Tim"], ["Cheng", "", ""]]}, {"id": "1912.10615", "submitter": "Jiexiong Tang", "authors": "Jiexiong Tang, Hanme Kim, Vitor Guizilini, Sudeep Pillai, Rares Ambrus", "title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying salient points in images is a crucial component for visual\nodometry, Structure-from-Motion or SLAM algorithms. Recently, several learned\nkeypoint methods have demonstrated compelling performance on challenging\nbenchmarks. However, generating consistent and accurate training data for\ninterest-point detection in natural images still remains challenging,\nespecially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a\nnovel proxy task for the self-supervision of keypoint detection, description\nand matching. By making the sampling of inlier-outlier sets from point-pair\ncorrespondences fully differentiable within the keypoint learning framework, we\nshow that are able to simultaneously self-supervise keypoint description and\nimprove keypoint matching. Second, we introduce KeyPointNet, a keypoint-network\narchitecture that is especially amenable to robust keypoint detection and\ndescription. We design the network to allow local keypoint aggregation to avoid\nartifacts due to spatial discretizations commonly used for this task, and we\nimprove fine-grained keypoint descriptor performance by taking advantage of\nefficient sub-pixel convolutions to upsample the descriptor feature-maps to a\nhigher operating resolution. Through extensive experiments and ablative\nanalysis, we show that the proposed self-supervised keypoint learning method\ngreatly improves the quality of feature matching and homography estimation on\nchallenging benchmarks over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 04:37:50 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tang", "Jiexiong", ""], ["Kim", "Hanme", ""], ["Guizilini", "Vitor", ""], ["Pillai", "Sudeep", ""], ["Ambrus", "Rares", ""]]}, {"id": "1912.10644", "submitter": "Mingye Xu", "authors": "Mingye Xu, Zhipeng Zhou, Yu Qiao", "title": "Geometry Sharing Network for 3D Point Cloud Classification and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 06:46:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Xu", "Mingye", ""], ["Zhou", "Zhipeng", ""], ["Qiao", "Yu", ""]]}, {"id": "1912.10647", "submitter": "Mostafa Sadeghi", "authors": "Mostafa Sadeghi, Xavier Alameda-Pineda", "title": "Mixture of Inference Networks for VAE-based Audio-visual Speech\n  Enhancement", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in unsupervised (unknown noise) audio-visual\nspeech enhancement based on variational autoencoders (VAEs), where the\nprobability distribution of clean speech spectra is simulated using an\nencoder-decoder architecture. The trained generative model (decoder) is then\ncombined with a noise model at test time to estimate the clean speech. In the\nspeech enhancement phase (test time), the initialization of the latent\nvariables, which describe the generative process of clean speech via decoder,\nis crucial, as the overall inference problem is non-convex. This is usually\ndone by using the output of the trained encoder where the noisy audio and clean\nvisual data are given as input. Current audio-visual VAE models do not provide\nan effective initialization because the two modalities are tightly coupled\n(concatenated) in the associated architectures. To overcome this issue,\ninspired by mixture models, we introduce the mixture of inference networks\nvariational autoencoder (MIN-VAE). Two encoder networks input, respectively,\naudio and visual data, and the posterior of the latent variables is modeled as\na mixture of two Gaussian distributions output from each encoder network. The\nmixture variable is also latent, and therefore the inference of learning the\noptimal balance between the audio and visual inference networks is unsupervised\nas well. By training a shared decoder, the overall network learns to adaptively\nfuse the two modalities. Moreover, at test time, the visual encoder, which\ntakes (clean) visual data, is used for initialization. A variational inference\napproach is derived to train the proposed generative model. Thanks to the novel\ninference procedure and the robust initialization, the proposed MIN-VAE\nexhibits superior performance on speech enhancement than using the standard\naudio-only as well as audio-visual counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 06:55:14 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 14:24:15 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 17:19:42 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 20:22:45 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Sadeghi", "Mostafa", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "1912.10657", "submitter": "He-Feng Yin", "authors": "Xing Liu, Xiao-Jun Wu, Zhen Liu, He-Feng Yin", "title": "A Compared Study Between Some Subspace Based Algorithms", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology of face recognition has made some progress in recent years.\nAfter studying the PCA, 2DPCA, R1-PCA, L1-PCA, KPCA and KECA algorithms, in\nthis paper ECA (2DECA) is proposed by extracting features in PCA (2DPCA) based\non Renyi entropy contribution. And then we conduct a study on the 2DL1-PCA and\n2DR1-PCA algorithms. On the basis of the experiments, this paper compares the\ndifference of the recognition accuracy and operational efficiency between the\nabove algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:40:51 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Xing", ""], ["Wu", "Xiao-Jun", ""], ["Liu", "Zhen", ""], ["Yin", "He-Feng", ""]]}, {"id": "1912.10659", "submitter": "Yu Chen", "authors": "Yu Chen, Shuhan Shen, Yisong Chen, Guoping Wang", "title": "Graph-Based Parallel Large Scale Structure from Motion", "comments": "In submission to Pattern Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Structure from Motion (SfM) achieves great success in 3D\nreconstruction, it still meets challenges on large scale scenes. In this work,\nlarge scale SfM is deemed as a graph problem, and we tackle it in a\ndivide-and-conquer manner. Firstly, the images clustering algorithm divides\nimages into clusters with strong connectivity, leading to robust local\nreconstructions. Then followed with an image expansion step, the connection and\ncompleteness of scenes are enhanced by expanding along with a maximum spanning\ntree. After local reconstructions, we construct a minimum spanning tree (MinST)\nto find accurate similarity transformations. Then the MinST is transformed into\na Minimum Height Tree (MHT) to find a proper anchor node and is further\nutilized to prevent error accumulation. When evaluated on different kinds of\ndatasets, our approach shows superiority over the state-of-the-art in accuracy\nand efficiency. Our algorithm is open-sourced at\nhttps://github.com/AIBluefisher/GraphSfM.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:44:52 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 05:23:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Yu", ""], ["Shen", "Shuhan", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""]]}, {"id": "1912.10664", "submitter": "Xuehui Yu", "authors": "Xuehui Yu, Yuqi Gong, Nan Jiang, Qixiang Ye, Zhenjun Han", "title": "Scale Match for Tiny Person Detection", "comments": "accepted by WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object detection has achieved unprecedented ad-vance with the rise of\ndeep convolutional neural networks.However, detecting tiny objects (for example\ntiny per-sons less than 20 pixels) in large-scale images remainsnot well\ninvestigated. The extremely small objects raisea grand challenge about feature\nrepresentation while themassive and complex backgrounds aggregate the risk\noffalse alarms. In this paper, we introduce a new benchmark,referred to as\nTinyPerson, opening up a promising directionfor tiny object detection in a long\ndistance and with mas-sive backgrounds. We experimentally find that the scale\nmis-match between the dataset for network pre-training and thedataset for\ndetector learning could deteriorate the featurerepresentation and the\ndetectors. Accordingly, we proposea simple yet effective Scale Match approach\nto align theobject scales between the two datasets for favorable tiny-object\nrepresentation. Experiments show the significantperformance gain of our\nproposed approach over state-of-the-art detectors, and the challenging aspects\nof TinyPersonrelated to real-world scenarios. The TinyPerson benchmarkand the\ncode for our approach will be publicly\navailable(https://github.com/ucas-vg/TinyBenchmark).(Attention: evaluation\nrules of AP have updated in benchmark after this paper accepted, So this paper\nuse old rules. we will keep old rules of AP in benchmark, but we recommand the\nnew and we will use the new in latter research.)\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:55:50 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yu", "Xuehui", ""], ["Gong", "Yuqi", ""], ["Jiang", "Nan", ""], ["Ye", "Qixiang", ""], ["Han", "Zhenjun", ""]]}, {"id": "1912.10667", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu, Yuxin Tian, Shawn Newsam", "title": "Generalizing Deep Models for Overhead Image Segmentation Through\n  Getis-Ord Gi* Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  That most deep learning models are purely data driven is both a strength and\na weakness. Given sufficient training data, the optimal model for a particular\nproblem can be learned. However, this is usually not the case and so instead\nthe model is either learned from scratch from a limited amount of training data\nor pre-trained on a different problem and then fine-tuned. Both of these\nsituations are potentially suboptimal and limit the generalizability of the\nmodel. Inspired by this, we investigate methods to inform or guide deep\nlearning models for geospatial image analysis to increase their performance\nwhen a limited amount of training data is available or when they are applied to\nscenarios other than which they were trained on. In particular, we exploit the\nfact that there are certain fundamental rules as to how things are distributed\non the surface of the Earth and these rules do not vary substantially between\nlocations. Based on this, we develop a novel feature pooling method for\nconvolutional neural networks using Getis-Ord Gi* analysis from geostatistics.\nExperimental results show our proposed pooling function has significantly\nbetter generalization performance compared to a standard data-driven approach\nwhen applied to overhead image segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:58:26 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Tian", "Yuxin", ""], ["Newsam", "Shawn", ""]]}, {"id": "1912.10675", "submitter": "Aly Magassouba", "authors": "Aly Magassouba, Komei Sugiura and Hisashi Kawai", "title": "A Multimodal Target-Source Classifier with Attention Branches to\n  Understand Ambiguous Instructions for Fetching Daily Objects", "comments": "9 pages, 5 figures, accepted for IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we focus on multimodal language understanding for fetching\ninstructions in the domestic service robots context. This task consists of\npredicting a target object, as instructed by the user, given an image and an\nunstructured sentence, such as \"Bring me the yellow box (from the wooden\ncabinet).\" This is challenging because of the ambiguity of natural language,\ni.e., the relevant information may be missing or there might be several\ncandidates. To solve such a task, we propose the multimodal target-source\nclassifier model with attention branches (MTCM-AB), which is an extension of\nthe MTCM. Our methodology uses the attention branch network (ABN) to develop a\nmultimodal attention mechanism based on linguistic and visual inputs.\nExperimental validation using a standard dataset showed that the MTCM-AB\noutperformed both state-of-the-art methods and the MTCM. In particular the\nMTCM-AB accuracy on average was 90.1% while human performance was 90.3% on the\nPFN-PIC dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 08:25:11 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 01:08:48 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1912.10687", "submitter": "Kyuho Bae", "authors": "Kyuho Bae (1), Andre Ivan (1), Hajime Nagahara (2), In Kyu Park (1)\n  ((1) Inha University, (2) Osaka University)", "title": "5D Light Field Synthesis from a Monocular Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercially available light field cameras have difficulty in capturing 5D\n(4D + time) light field videos. They can only capture still light filed images\nor are excessively expensive for normal users to capture the light field video.\nTo tackle this problem, we propose a deep learning-based method for\nsynthesizing a light field video from a monocular video. We propose a new\nsynthetic light field video dataset that renders photorealistic scenes using\nUnrealCV rendering engine because no light field dataset is available. The\nproposed deep learning framework synthesizes the light field video with a full\nset (9$\\times$9) of sub-aperture images from a normal monocular video. The\nproposed network consists of three sub-networks, namely, feature extraction, 5D\nlight field video synthesis, and temporal consistency refinement. Experimental\nresults show that our model can successfully synthesize the light field video\nfor synthetic and actual scenes and outperforms the previous frame-by-frame\nmethods quantitatively and qualitatively. The synthesized light field can be\nused for conventional light field applications, namely, depth estimation,\nviewpoint change, and refocusing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 08:52:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bae", "Kyuho", "", "Inha University"], ["Ivan", "Andre", "", "Inha University"], ["Nagahara", "Hajime", "", "Osaka University"], ["Park", "In Kyu", "", "Inha University"]]}, {"id": "1912.10694", "submitter": "Haoran Wei", "authors": "Haoran Wei, Yue Zhang, Zhonghan Chang, Hao Li, Hongqi Wang, Xian Sun", "title": "Oriented Objects as pairs of Middle Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of oriented objects is frequently appeared in the field of\nnatural scene text detection as well as object detection in aerial images.\nTraditional detectors for oriented objects are common to rotate anchors on the\nbasis of the RCNN frameworks, which will multiple the number of anchors with a\nvariety of angles, coupled with rotating NMS algorithm, the computational\ncomplexities of these models are greatly increased. In this paper, we propose a\nnovel model named Oriented Objects Detection Network O^2-DNet to detect\noriented objects by predicting a pair of middle lines inside each target.\nO^2-DNet is an one-stage, anchor-free and NMS-free model. The target line\nsegments of our model are defined as two corresponding middle lines of original\nrotating bounding box annotations which can be transformed directly instead of\nadditional manual tagging. Experiments show that our O^2-DNet achieves\nexcellent performance on ICDAR 2015 and DOTA datasets. It is noteworthy that\nthe objects in COCO can be regard as a special form of oriented objects with an\nangle of 90 degrees. O^2-DNet can still achieve competitive results in these\ngeneral natural object detection datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:08:35 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 05:09:16 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 01:25:20 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wei", "Haoran", ""], ["Zhang", "Yue", ""], ["Chang", "Zhonghan", ""], ["Li", "Hao", ""], ["Wang", "Hongqi", ""], ["Sun", "Xian", ""]]}, {"id": "1912.10702", "submitter": "Bin Dai", "authors": "Bin Dai, Ziyu Wang, David Wipf", "title": "The Usual Suspects? Reassessing Blame for VAE Posterior Collapse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In narrow asymptotic settings Gaussian VAE models of continuous data have\nbeen shown to possess global optima aligned with ground-truth distributions.\nEven so, it is well known that poor solutions whereby the latent posterior\ncollapses to an uninformative prior are sometimes obtained in practice.\nHowever, contrary to conventional wisdom that largely assigns blame for this\nphenomena on the undue influence of KL-divergence regularization, we will argue\nthat posterior collapse is, at least in part, a direct consequence of bad local\nminima inherent to the loss surface of deep autoencoder networks. In\nparticular, we prove that even small nonlinear perturbations of affine VAE\ndecoder models can produce such minima, and in deeper models, analogous minima\ncan force the VAE to behave like an aggressive truncation operator, provably\ndiscarding information along all latent dimensions in certain circumstances.\nRegardless, the underlying message here is not meant to undercut valuable\nexisting explanations of posterior collapse, but rather, to refine the\ndiscussion and elucidate alternative risk factors that may have been previously\nunderappreciated.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:40:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dai", "Bin", ""], ["Wang", "Ziyu", ""], ["Wipf", "David", ""]]}, {"id": "1912.10718", "submitter": "Fang Aiqing", "authors": "Aiqing Fang, Xinbo Zhao, Yanning Zhang", "title": "Cross-Modal Image Fusion Theory Guided by Subjective Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual perception system has very strong robustness and contextual\nawareness in a variety of image processing tasks. This robustness and the\nperception ability of contextual awareness is closely related to the\ncharacteristics of multi-task auxiliary learning and subjective attention of\nthe human visual perception system. In order to improve the robustness and\ncontextual awareness of image fusion tasks, we proposed a multi-task auxiliary\nlearning image fusion theory guided by subjective attention. The image fusion\ntheory effectively unifies the subjective task intention and prior knowledge of\nhuman brain. In order to achieve our proposed image fusion theory, we first\nanalyze the mechanism of multi-task auxiliary learning, build a multi-task\nauxiliary learning network. Secondly, based on the human visual attention\nperception mechanism, we introduce the human visual attention network guided by\nsubjective tasks on the basis of the multi-task auxiliary learning network. The\nsubjective intention is introduced by the subjective attention task model, so\nthat the network can fuse images according to the subjective intention.\nFinally, in order to verify the superiority of our image fusion theory, we\ncarried out experiments on the combined vision system image data set, and the\ninfrared and visible image data set for experimental verification. The\nexperimental results demonstrate the superiority of our fusion theory over\nstate-of-arts in contextual awareness and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:29:34 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Zhang", "Yanning", ""]]}, {"id": "1912.10726", "submitter": "Yudie Wang", "authors": "Y. D. Wang (1), Z. W. Li (1), C. Zeng (1), G. S. Xia (1), H. F. Shen\n  (1) ((1) Wuhan University, Wuhan, China)", "title": "Extracting urban water by combining deep learning and Google Earth\n  Engine", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban water is important for the urban ecosystem. Accurate and efficient\ndetection of urban water with remote sensing data is of great significance for\nurban management and planning. In this paper, we proposed a new method to\ncombine Google Earth Engine (GEE) with multiscale convolutional neural network\n(MSCNN) to extract urban water from Landsat images, which is summarized as\noffline training and online prediction (OTOP). That is, the training of MSCNN\nwas completed offline, and the process of urban water extraction was\nimplemented on GEE with the trained parameters of MSCNN. The OTOP can give full\nplay to the respective advantages of GEE and CNN, and make the use of deep\nlearning method on GEE more flexible. It can process available satellite images\nwith high performance without data download and storage, and the overall\nperformance of urban water extraction is also higher than that of the modified\nnormalized difference water index (MNDWI) and random forest. The mean kappa,\nF1-score and intersection over union (IoU) of urban water extraction with the\nOTOP in Changchun, Wuhan, Kunming and Guangzhou reached 0.924, 0.930 and 0.869,\nrespectively. The results of the extended validation in the other major cities\nof China also show that the OTOP is robust and can be used to extract different\ntypes of urban water, which benefits from the structural design and training of\nthe MSCNN. Therefore, the OTOP is especially suitable for the study of\nlarge-scale and long-term urban water change detection in the background of\nurbanization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:50:03 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Y. D.", "", "Wuhan University, Wuhan, China"], ["Li", "Z. W.", "", "Wuhan University, Wuhan, China"], ["Zeng", "C.", "", "Wuhan University, Wuhan, China"], ["Xia", "G. S.", "", "Wuhan University, Wuhan, China"], ["Shen", "H. F.", "", "Wuhan University, Wuhan, China"]]}, {"id": "1912.10738", "submitter": "Fang Aiqing", "authors": "Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Yanning Zhang", "title": "Non-linear and Selective Fusion of Cross-Modal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual perception system has strong robustness in image fusion.\nThis robustness is based on human visual perception system's characteristics of\nfeature selection and non-linear fusion of different features. In order to\nsimulate the human visual perception mechanism in image fusion tasks, we\npropose a multi-source image fusion framework that combines illuminance factors\nand attention mechanisms. The framework effectively combines traditional image\nfeatures and modern deep learning features. First, we perform multi-scale\ndecomposition of multi-source images. Then, the visual saliency map and the\ndeep feature map are combined with the illuminance fusion factor to perform\nhigh-low frequency nonlinear fusion. Secondly, the characteristics of high and\nlow frequency fusion are selected through the channel attention network to\nobtain the final fusion map. By simulating the nonlinear characteristics and\nselection characteristics of the human visual perception system in image\nfusion, the fused image is more in line with the human visual perception\nmechanism. Finally, we validate our fusion framework on public datasets of\ninfrared and visible images, medical images and multi-focus images. The\nexperimental results demonstrate the superiority of our fusion framework over\nstate-of-arts in visual quality, objective fusion metrics and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:19:19 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 10:46:35 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Yang", "Jiaqi", ""], ["Zhang", "Yanning", ""]]}, {"id": "1912.10739", "submitter": "Markus Hofinger", "authors": "Markus Hofinger, Samuel Rota Bul\\`o, Lorenzo Porzi, Arno Knapitsch,\n  Thomas Pock, Peter Kontschieder", "title": "Improving Optical Flow on a Pyramid Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we review the coarse-to-fine spatial feature pyramid concept,\nwhich is used in state-of-the-art optical flow estimation networks to make\nexploration of the pixel flow search space computationally tractable and\nefficient. Within an individual pyramid level, we improve the cost volume\nconstruction process by departing from a warping- to a sampling-based strategy,\nwhich avoids ghosting and hence enables us to better preserve fine flow\ndetails. We further amplify the positive effects through a level-specific, loss\nmax-pooling strategy that adaptively shifts the focus of the learning process\non under-performing predictions. Our second contribution revises the gradient\nflow across pyramid levels. The typical operations performed at each pyramid\nlevel can lead to noisy, or even contradicting gradients across levels. We show\nand discuss how properly blocking some of these gradient components leads to\nimproved convergence and ultimately better performance. Finally, we introduce a\ndistillation concept to counteract the issue of catastrophic forgetting and\nthus preserving knowledge over models sequentially trained on multiple\ndatasets. Our findings are conceptually simple and easy to implement, yet\nresult in compelling improvements on relevant error measures that we\ndemonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying\nThings, Sintel and KITTI. We establish new state-of-the-art results on the\nchallenging Sintel and KITTI 2012 test datasets, and even show the portability\nof our findings to different optical flow and depth from stereo approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:24:00 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 12:31:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hofinger", "Markus", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Porzi", "Lorenzo", ""], ["Knapitsch", "Arno", ""], ["Pock", "Thomas", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1912.10752", "submitter": "Balaji Selvaraj", "authors": "S. Balaji, T. Kavya, Natasha Sebastian", "title": "Learn-able parameter guided Activation Functions", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore the concept of adding learn-able slope and mean\nshift parameters to an activation function to improve the total response\nregion. The characteristics of an activation function depend highly on the\nvalue of parameters. Making the parameters learn-able, makes the activation\nfunction more dynamic and capable to adapt as per the requirements of its\nneighboring layers. The introduced slope parameter is independent of other\nparameters in the activation function. The concept was applied to ReLU to\ndevelop Dual Line and DualParametric ReLU activation function. Evaluation on\nMNIST and CIFAR10 show that the proposed activation function Dual Line achieves\ntop-5 position for mean accuracy among 43 activation functions tested with\nLENET4, LENET5, and WideResNet architectures. This is the first time more than\n40 activation functions were analyzed on MNIST andCIFAR10 dataset at the same\ntime. The study on the distribution of positive slope parameter beta indicates\nthat the activation function adapts as per the requirements of the neighboring\nlayers. The study shows that model performance increases with the proposed\nactivation functions\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:54:05 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Balaji", "S.", ""], ["Kavya", "T.", ""], ["Sebastian", "Natasha", ""]]}, {"id": "1912.10768", "submitter": "Zi-Qi Li", "authors": "Xing Liu, Xiao-Jun Wu, Zi-Qi Li", "title": "2DR1-PCA and 2DL1-PCA: two variant 2DPCA algorithms based on none L2\n  norm", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two novel methods: 2DR1-PCA and 2DL1-PCA are proposed for face\nrecognition. Compared to the traditional 2DPCA algorithm, 2DR1-PCA and 2DL1-PCA\nare based on the R1 norm and L1 norm, respectively. The advantage of these\nproposed methods is they are less sensitive to outliers. These proposed methods\nare tested on the ORL, YALE and XM2VTS databases and the performance of the\nrelated methods is compared experimentally.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:43:51 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Xing", ""], ["Wu", "Xiao-Jun", ""], ["Li", "Zi-Qi", ""]]}, {"id": "1912.10773", "submitter": "Sampo Kuutti", "authors": "Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, Saber Fallah", "title": "A Survey of Deep Learning Applications to Autonomous Vehicle Control", "comments": "23 pages, 3 figures, Accepted in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a controller for autonomous vehicles capable of providing adequate\nperformance in all driving scenarios is challenging due to the highly complex\nenvironment and inability to test the system in the wide variety of scenarios\nwhich it may encounter after deployment. However, deep learning methods have\nshown great promise in not only providing excellent performance for complex and\nnon-linear control problems, but also in generalising previously learned rules\nto new scenarios. For these reasons, the use of deep learning for vehicle\ncontrol is becoming increasingly popular. Although important advancements have\nbeen achieved in this field, these works have not been fully summarised. This\npaper surveys a wide range of research works reported in the literature which\naim to control a vehicle through deep learning methods. Although there exists\noverlap between control and perception, the focus of this paper is on vehicle\ncontrol, rather than the wider perception problem which includes tasks such as\nsemantic segmentation and object detection. The paper identifies the strengths\nand limitations of available deep learning methods through comparative analysis\nand discusses the research challenges in terms of computation, architecture\nselection, goal specification, generalisation, verification and validation, as\nwell as safety. Overall, this survey brings timely and topical information to a\nrapidly evolving field relevant to intelligent transportation systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:50:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kuutti", "Sampo", ""], ["Bowden", "Richard", ""], ["Jin", "Yaochu", ""], ["Barber", "Phil", ""], ["Fallah", "Saber", ""]]}, {"id": "1912.10775", "submitter": "Wenkai Han", "authors": "Wenkai Han, Chenglu Wen, Cheng Wang, Xin Li, Qing Li", "title": "Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature\n  Modeling", "comments": "AAAI2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully exploring correlation among points in point clouds is essential for\ntheir feature modeling. This paper presents a novel end-to-end graph model,\nnamed Point2Node, to represent a given point cloud. Point2Node can dynamically\nexplore correlation among all graph nodes from different levels, and adaptively\naggregate the learned features. Specifically, first, to fully explore the\nspatial correlation among points for enhanced feature description, in a\nhigh-dimensional node graph, we dynamically integrate the node's correlation\nwith self, local, and non-local nodes. Second, to more effectively integrate\nlearned features, we design a data-aware gate mechanism to self-adaptively\naggregate features at the channel level. Extensive experiments on various point\ncloud benchmarks demonstrate that our method outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:54:18 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Han", "Wenkai", ""], ["Wen", "Chenglu", ""], ["Wang", "Cheng", ""], ["Li", "Xin", ""], ["Li", "Qing", ""]]}, {"id": "1912.10776", "submitter": "Ji Zhao", "authors": "Banglei Guan, Ji Zhao, Zhang Li, Fang Sun, Friedrich Fraundorfer", "title": "Minimal Solutions for Relative Pose with a Single Affine Correspondence", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present four cases of minimal solutions for two-view\nrelative pose estimation by exploiting the affine transformation between\nfeature points and we demonstrate efficient solvers for these cases. It is\nshown, that under the planar motion assumption or with knowledge of a vertical\ndirection, a single affine correspondence is sufficient to recover the relative\ncamera pose. The four cases considered are two-view planar relative motion for\ncalibrated cameras as a closed-form and a least-squares solution, a closed-form\nsolution for unknown focal length and the case of a known vertical direction.\nThese algorithms can be used efficiently for outlier detection within a RANSAC\nloop and for initial motion estimation. All the methods are evaluated on both\nsynthetic data and real-world datasets from the KITTI benchmark. The\nexperimental results demonstrate that our methods outperform comparable\nstate-of-the-art methods in accuracy with the benefit of a reduced number of\nneeded RANSAC iterations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:55:44 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 02:36:39 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Guan", "Banglei", ""], ["Zhao", "Ji", ""], ["Li", "Zhang", ""], ["Sun", "Fang", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "1912.10799", "submitter": "Andr\\'e Neves", "authors": "Andr\\'e Neves and Carlos Dam\\'asio and Jo\\~ao Pires and Fernando Birra", "title": "Dete\\c{c}\\~ao de estruturas permanentes a partir de dados de s\\'eries\n  temporais Sentinel 1 e 2", "comments": "12 pages, in Portuguese, 7 figures, conference: INForum 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mapping structures such as settlements, roads, individual houses and any\nother types of artificial structures is of great importance for the analysis of\nurban growth, masking, image alignment and, especially in the studied use case,\nthe definition of Fuel Management Networks (FGC), which protect buildings from\nforest fires. Current cartography has a low generation frequency and their\nresolution may not be suitable for extracting small structures such as small\nsettlements or roads, which may lack forest fire protection. In this paper, we\nuse time series data, extracted from Sentinel-1 and 2 constellations, over\nSantar\\'em, Ma\\c{c}\\~ao, to explore the detection of permanent structures at a\nresolution of 10 by 10 meters. For this purpose, a XGBoost classification model\nis trained with 133 attributes extracted from the time series from all the\nbands, including normalized radiometric indices. The results show that the use\nof time series data increases the accuracy of the extraction of permanent\nstructures when compared using only static data, using multitemporal data also\nincreases the number of detected roads. In general, the final result has a\npermanent structure mapping with a higher resolution than state of the art\nsettlement maps, small structures and roads are also more accurately\nrepresented. Regarding the use case, by using our final map for the creation of\nFGC it is possible to simplify and accelerate the process of delimitation of\nthe official FGC.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:08:07 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Neves", "Andr\u00e9", ""], ["Dam\u00e1sio", "Carlos", ""], ["Pires", "Jo\u00e3o", ""], ["Birra", "Fernando", ""]]}, {"id": "1912.10803", "submitter": "Angshul Majumdar Dr.", "authors": "Vanika Singhal, Hemant K. Aggarwal, Snigdha Tariyal and Angshul\n  Majumdar", "title": "Discriminative Robust Deep Dictionary Learning for Hyperspectral Image\n  Classification", "comments": "Final version accepted at IEEE Transactions on Geosciences and Remote\n  Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new framework for deep learning that has been\nparticularly tailored for hyperspectral image classification. We learn multiple\nlevels of dictionaries in a robust fashion. The last layer is discriminative\nthat learns a linear classifier. The training proceeds greedily, at a time a\nsingle level of dictionary is learnt and the coefficients used to train the\nnext level. The coefficients from the final level are used for classification.\nRobustness is incorporated by minimizing the absolute deviations instead of the\nmore popular Euclidean norm. The inbuilt robustness helps combat mixed noise\n(Gaussian and sparse) present in hyperspectral images. Results show that our\nproposed techniques outperforms all other deep learning methods Deep Belief\nNetwork (DBN), Stacked Autoencoder (SAE) and Convolutional Neural Network\n(CNN). The experiments have been carried out on benchmark hyperspectral imaging\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 10:59:25 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Singhal", "Vanika", ""], ["Aggarwal", "Hemant K.", ""], ["Tariyal", "Snigdha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.10822", "submitter": "Jithin James", "authors": "Jithin James", "title": "DeepHashing using TripletLoss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is one of the most efficient techniques for approximate nearest\nneighbour search for large scale image retrieval. Most of the techniques are\nbased on hand-engineered features and do not give optimal results all the time.\nDeep Convolutional Neural Networks have proven to generate very effective\nrepresentation of images that are used for various computer vision tasks and\ninspired by this there have been several Deep Hashing models like Wang et al.\n(2016) have been proposed. These models train on the triplet loss function\nwhich can be used to train models with superior representation capabilities.\nTaking the latest advancements in training using the triplet loss I propose new\ntechniques that help the Deep Hash-ing models train more faster and\nefficiently. Experiment result1show that using the more efficient techniques\nfor training on the triplet loss, we have obtained a 5%percent improvement in\nour model compared to the original work of Wang et al.(2016). Using a larger\nmodel and more training data we can drastically improve the performance using\nthe techniques we propose\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:17:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["James", "Jithin", ""]]}, {"id": "1912.10836", "submitter": "Aykut \\c{C}ay{\\i}r", "authors": "Aykut \\c{C}ay{\\i}r, U\\u{g}ur \\\"Unal and Hasan Da\\u{g}", "title": "Random CapsNet Forest Model for Imbalanced Malware Type Classification\n  Task", "comments": "30 pages, 10 figures, typos are corrected, references are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Behavior of a malware varies with respect to malware types. Therefore,knowing\ntype of a malware affects strategies of system protection softwares. Many\nmalware type classification models empowered by machine and deep learning\nachieve superior accuracies to predict malware types.Machine learning based\nmodels need to do heavy feature engineering and feature engineering is\ndominantly effecting performance of models.On the other hand, deep learning\nbased models require less feature engineering than machine learning based\nmodels. However, traditional deep learning architectures and components cause\nvery complex and data sensitive models. Capsule network architecture minimizes\nthis complexity and data sensitivity unlike classical convolutional neural\nnetwork architectures. This paper proposes an ensemble capsule network model\nbased on bootstrap aggregating technique. The proposed method are tested on two\nmalware datasets, whose the-state-of-the-art results are well-known.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 06:40:40 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:51:31 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 19:56:53 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 20:21:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["\u00c7ay\u0131r", "Aykut", ""], ["\u00dcnal", "U\u011fur", ""], ["Da\u011f", "Hasan", ""]]}, {"id": "1912.10837", "submitter": "Siming Bayer", "authors": "Siming Bayer, Xia Zhong, Weilin Fu, Nishant Ravikumar, Andreas Maier", "title": "Analyzing an Imitation Learning Network for Fundus Image Registration\n  Using a Divide-and-Conquer Approach", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of microvascular circulation on fundoscopic images is a\nnon-invasive clinical indication for the diagnosis and monitoring of diseases,\nsuch as diabetes and hypertensions. The differences between intra-patient\nimages can be assessed quantitatively by registering serial acquisitions. Due\nto the variability of the images (i.e. contrast, luminosity) and the anatomical\nchanges of the retina, the registration of fundus images remains a challenging\ntask. Recently, several deep learning approaches have been proposed to register\nfundus images in an end-to-end fashion, achieving remarkable results. However,\nthe results are difficult to interpret and analyze. In this work, we propose an\nimitation learning framework for the registration of 2D color funduscopic\nimages for a wide range of applications such as disease monitoring, image\nstitching and super-resolution. We follow a divide-and-conquer approach to\nimprove the interpretability of the proposed network, and analyze both the\ninfluence of the input image and the hyperparameters on the registration\nresult. The results show that the proposed registration network reduces the\ninitial target registration error up to 95\\%.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:01:49 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bayer", "Siming", ""], ["Zhong", "Xia", ""], ["Fu", "Weilin", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.10867", "submitter": "Andrea Bandini", "authors": "Andrea Bandini and Jos\\'e Zariffa", "title": "Analysis of the hands in egocentric vision: A survey", "comments": null, "journal-ref": "Published in IEEE Transaction on Pattern Analysis and Machine\n  Interaction, 2020", "doi": "10.1109/TPAMI.2020.2986648", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision (a.k.a. first-person vision - FPV) applications have\nthrived over the past few years, thanks to the availability of affordable\nwearable cameras and large annotated datasets. The position of the wearable\ncamera (usually mounted on the head) allows recording exactly what the camera\nwearers have in front of them, in particular hands and manipulated objects.\nThis intrinsic advantage enables the study of the hands from multiple\nperspectives: localizing hands and their parts within the images; understanding\nwhat actions and activities the hands are involved in; and developing\nhuman-computer interfaces that rely on hand gestures. In this survey, we review\nthe literature that focuses on the hands using egocentric vision, categorizing\nthe existing approaches into: localization (where are the hands or parts of\nthem?); interpretation (what are the hands doing?); and application (e.g.,\nsystems that used egocentric hand cues for solving a specific problem).\nMoreover, a list of the most prominent datasets with hand-based annotations is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:30:02 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 15:11:31 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Bandini", "Andrea", ""], ["Zariffa", "Jos\u00e9", ""]]}, {"id": "1912.10917", "submitter": "Wuyang Chen", "authors": "Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang\n  Wang", "title": "FasterSeg: Searching for Faster Real-time Semantic Segmentation", "comments": "ICLR 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FasterSeg, an automatically designed semantic segmentation network\nwith not only state-of-the-art performance but also faster speed than current\nmethods. Utilizing neural architecture search (NAS), FasterSeg is discovered\nfrom a novel and broader search space integrating multi-resolution branches,\nthat has been recently found to be vital in manually designed segmentation\nmodels. To better calibrate the balance between the goals of high accuracy and\nlow latency, we propose a decoupled and fine-grained latency regularization,\nthat effectively overcomes our observed phenomenons that the searched networks\nare prone to \"collapsing\" to low-latency yet poor-accuracy models. Moreover, we\nseamlessly extend FasterSeg to a new collaborative search (co-searching)\nframework, simultaneously searching for a teacher and a student network in the\nsame single run. The teacher-student distillation further boosts the student\nmodel's accuracy. Experiments on popular segmentation benchmarks demonstrate\nthe competency of FasterSeg. For example, FasterSeg can run over 30% faster\nthan the closest manually designed competitor on Cityscapes, while maintaining\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:26:39 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 21:20:48 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Chen", "Wuyang", ""], ["Gong", "Xinyu", ""], ["Liu", "Xianming", ""], ["Zhang", "Qian", ""], ["Li", "Yuan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1912.10920", "submitter": "Andrey Voynov", "authors": "Andrey Voynov, Artem Babenko", "title": "RPGAN: GANs Interpretability via Random Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Random Path Generative Adversarial Network\n(RPGAN) -- an alternative design of GANs that can serve as a tool for\ngenerative model analysis. While the latent space of a typical GAN consists of\ninput vectors, randomly sampled from the standard Gaussian distribution, the\nlatent space of RPGAN consists of random paths in a generator network. As we\nshow, this design allows to understand factors of variation, captured by\ndifferent generator layers, providing their natural interpretability. With\nexperiments on standard benchmarks, we demonstrate that RPGAN reveals several\ninteresting insights about the roles that different layers play in the image\ngeneration process. Aside from interpretability, the RPGAN model also provides\ncompetitive generation quality and allows efficient incremental learning on new\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:29:54 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 22:08:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Voynov", "Andrey", ""], ["Babenko", "Artem", ""]]}, {"id": "1912.10946", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey", "title": "PSNet: Parametric Sigmoid Norm Based CNN for Face Recognition", "comments": "Accepted in IEEE CICT 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Networks (CNN) have become very popular recently due\nto its outstanding performance in various computer vision applications. It is\nalso used over widely studied face recognition problem. However, the existing\nlayers of CNN are unable to cope with the problem of hard examples which\ngenerally produce lower class scores. Thus, the existing methods become biased\ntowards the easy examples. In this paper, we resolve this problem by\nincorporating a Parametric Sigmoid Norm (PSN) layer just before the final\nfully-connected layer. We propose a PSNet CNN model by using the PSN layer. The\nPSN layer facilitates high gradient flow for harder examples as compared to\neasy examples. Thus, it forces the network to learn the visual characteristics\nof hard examples. We conduct the face recognition experiments to test the\nperformance of PSN layer. The suitability of the PSN layer with different loss\nfunctions is also experimented. The widely used Labeled Faces in the Wild (LFW)\nand YouTube Faces (YTF) datasets are used in the experiments. The experimental\nresults confirm the relevance of the proposed PSN layer.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:10:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1912.10952", "submitter": "Xin Chen", "authors": "Xin Chen, Lingxi Xie, Jun Wu and Qi Tian", "title": "Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild", "comments": "An extension of P-DARTS. Previous version: arXiv:1904.12760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of neural architecture search (NAS), researchers\nfound powerful network architectures for a wide range of vision tasks. However,\nit remains unclear if the searched architecture can transfer across different\ntypes of tasks as manually designed ones did. This paper puts forward this\nproblem, referred to as NAS in the wild, which explores the possibility of\nfinding the optimal architecture in a proxy dataset and then deploying it to\nmostly unseen scenarios.\n  We instantiate this setting using a currently popular algorithm named\ndifferentiable architecture search (DARTS), which often suffers unsatisfying\nperformance while being transferred across different tasks. We argue that the\naccuracy drop originates from the formulation that uses a super-network for\nsearch but a sub-network for re-training. The different properties of these\nstages have resulted in a significant optimization gap, and consequently, the\narchitectural parameters \"over-fit\" the super-network. To alleviate the gap, we\npresent a progressive method that gradually increases the network depth during\nthe search stage, which leads to the Progressive DARTS (P-DARTS) algorithm.\nWith a reduced search cost (7 hours on a single GPU), P-DARTS achieves improved\nperformance on both the proxy dataset (CIFAR10) and a few target problems\n(ImageNet classification, COCO detection and three ReID benchmarks). Our code\nis available at \\url{https://github.com/chenxin061/pdarts}.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 16:23:28 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 16:03:32 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chen", "Xin", ""], ["Xie", "Lingxi", ""], ["Wu", "Jun", ""], ["Tian", "Qi", ""]]}, {"id": "1912.10960", "submitter": "Basile Van Hoorick", "authors": "Basile Van Hoorick", "title": "Image Outpainting and Harmonization using Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the inherently ambiguous task of predicting what resides beyond all\nfour edges of an image has rarely been explored before, we demonstrate that\nGANs hold powerful potential in producing reasonable extrapolations. Two\noutpainting methods are proposed that aim to instigate this line of research:\nthe first approach uses a context encoder inspired by common inpainting\narchitectures and paradigms, while the second approach adds an extra\npost-processing step using a single-image generative model. This way, the\nhallucinated details are integrated with the style of the original image, in an\nattempt to further boost the quality of the result and possibly allow for\narbitrary output resolutions to be supported.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 16:38:30 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 23:37:30 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Van Hoorick", "Basile", ""]]}, {"id": "1912.10982", "submitter": "Nuno C. Garcia", "authors": "Nuno C. Garcia, Sarah Adel Bargal, Vitaly Ablavsky, Pietro Morerio,\n  Vittorio Murino, Stan Sclaroff", "title": "DMCL: Distillation Multiple Choice Learning for Multimodal Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of learning an ensemble of specialist\nnetworks using multimodal data, while considering the realistic and challenging\nscenario of possible missing modalities at test time. Our goal is to leverage\nthe complementary information of multiple modalities to the benefit of the\nensemble and each individual network. We introduce a novel Distillation\nMultiple Choice Learning framework for multimodal data, where different\nmodality networks learn in a cooperative setting from scratch, strengthening\none another. The modality networks learned using our method achieve\nsignificantly higher accuracy than if trained separately, due to the guidance\nof other modalities. We evaluate this approach on three video action\nrecognition benchmark datasets. We obtain state-of-the-art results in\ncomparison to other approaches that work with missing modalities at test time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:16:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Garcia", "Nuno C.", ""], ["Bargal", "Sarah Adel", ""], ["Ablavsky", "Vitaly", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1912.11000", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Dan Ruan, Jiayu Xiao, Lixia Wang, Bin Sun, Rola Saouaf,\n  Wensha Yang, Debiao Li, Zhaoyang Fan", "title": "Fully Automated Multi-Organ Segmentation in Abdominal Magnetic Resonance\n  Imaging with Deep Neural Networks", "comments": "21 pages, 4 figures, submitted to the journal Medical Physics", "journal-ref": null, "doi": "10.1002/mp.14429", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of multiple organs-at-risk (OARs) is essential for radiation\ntherapy treatment planning and other clinical applications. We developed an\nAutomated deep Learning-based Abdominal Multi-Organ segmentation (ALAMO)\nframework based on 2D U-net and a densely connected network structure with\ntailored design in data augmentation and training procedures such as deep\nconnection, auxiliary supervision, and multi-view. The model takes in\nmulti-slice MR images and generates the output of segmentation results.\nThree-Tesla T1 VIBE (Volumetric Interpolated Breath-hold Examination) images of\n102 subjects were collected and used in our study. Ten OARs were studied,\nincluding the liver, spleen, pancreas, left/right kidneys, stomach, duodenum,\nsmall intestine, spinal cord, and vertebral bodies. Two radiologists manually\nlabeled and obtained the consensus contours as the ground-truth. In the\ncomplete cohort of 102, 20 samples were held out for independent testing, and\nthe rest were used for training and validation. The performance was measured\nusing volume overlapping and surface distance. The ALAMO framework generated\nsegmentation labels in good agreement with the manual results. Specifically,\namong the 10 OARs, 9 achieved high Dice Similarity Coefficients (DSCs) in the\nrange of 0.87-0.96, except for the duodenum with a DSC of 0.80. The inference\ncompletes within one minute for a 3D volume of 320x288x180. Overall, the ALAMO\nmodel matches the state-of-the-art performance. The proposed ALAMO framework\nallows for fully automated abdominal MR segmentation with high accuracy and low\nmemory and computation time demands.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:01:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Yuhua", ""], ["Ruan", "Dan", ""], ["Xiao", "Jiayu", ""], ["Wang", "Lixia", ""], ["Sun", "Bin", ""], ["Saouaf", "Rola", ""], ["Yang", "Wensha", ""], ["Li", "Debiao", ""], ["Fan", "Zhaoyang", ""]]}, {"id": "1912.11006", "submitter": "Gongfan Fang", "authors": "Gongfan Fang, Jie Song, Chengchao Shen, Xinchao Wang, Da Chen, Mingli\n  Song", "title": "Data-Free Adversarial Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) has made remarkable progress in the last few\nyears and become a popular paradigm for model compression and knowledge\ntransfer. However, almost all existing KD algorithms are data-driven, i.e.,\nrelying on a large amount of original training data or alternative data, which\nis usually unavailable in real-world scenarios. In this paper, we devote\nourselves to this challenging problem and propose a novel adversarial\ndistillation mechanism to craft a compact student model without any real-world\ndata. We introduce a model discrepancy to quantificationally measure the\ndifference between student and teacher models and construct an optimizable\nupper bound. In our work, the student and the teacher jointly act the role of\nthe discriminator to reduce this discrepancy, when a generator adversarially\nproduces some \"hard samples\" to enlarge it. Extensive experiments demonstrate\nthat the proposed data-free method yields comparable performance to existing\ndata-driven methods. More strikingly, our approach can be directly extended to\nsemantic segmentation, which is more complicated than classification, and our\napproach achieves state-of-the-art results. Code and pretrained models are\navailable at https://github.com/VainF/Data-Free-Adversarial-Distillation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:08:33 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 07:01:32 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 12:12:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fang", "Gongfan", ""], ["Song", "Jie", ""], ["Shen", "Chengchao", ""], ["Wang", "Xinchao", ""], ["Chen", "Da", ""], ["Song", "Mingli", ""]]}, {"id": "1912.11022", "submitter": "Preeti Gopal Ms.", "authors": "Preeti Gopal, Sharat Chandran, Imants Svalbe, Ajit Rajwade", "title": "Low radiation tomographic reconstruction with and without template\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dose tomography is highly preferred in medical procedures for its reduced\nradiation risk when compared to standard-dose Computed Tomography (CT).\nHowever, the lower the intensity of X-rays, the higher the acquisition noise\nand hence the reconstructions suffer from artefacts. A large body of work has\nfocussed on improving the algorithms to minimize these artefacts. In this work,\nwe propose two new techniques, rescaled non-linear least squares and\nPoisson-Gaussian convolution, that reconstruct the underlying image making use\nof an accurate or near-accurate statistical model of the noise in the\nprojections. We also propose a reconstruction method when prior knowledge of\nthe underlying object is available in the form of templates. This is applicable\nto longitudinal studies wherein the same object is scanned multiple times to\nobserve the changes that evolve in it over time. Our results on 3D data show\nthat prior information can be used to compensate for the low-dose artefacts,\nand we demonstrate that it is possible to simultaneously prevent the prior from\nadversely biasing the reconstructions of new changes in the test object, via a\nmethod called ``re-irradiation''. Additionally, we also present two techniques\nfor automated tuning of the regularization parameters for tomographic\ninversion.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:39:41 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Gopal", "Preeti", ""], ["Chandran", "Sharat", ""], ["Svalbe", "Imants", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1912.11027", "submitter": "William Lotter", "authors": "William Lotter, Abdul Rahman Diab, Bryan Haslam, Jiye G. Kim, Giorgia\n  Grisot, Eric Wu, Kevin Wu, Jorge Onieva Onieva, Jerrold L. Boxerman, Meiyun\n  Wang, Mack Bandler, Gopal Vijayaraghavan, A. Gregory Sorensen", "title": "Robust breast cancer detection in mammography and digital breast\n  tomosynthesis using annotation-efficient deep learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer remains a global challenge, causing over 1 million deaths\nglobally in 2018. To achieve earlier breast cancer detection, screening x-ray\nmammography is recommended by health organizations worldwide and has been\nestimated to decrease breast cancer mortality by 20-40%. Nevertheless,\nsignificant false positive and false negative rates, as well as high\ninterpretation costs, leave opportunities for improving quality and access. To\naddress these limitations, there has been much recent interest in applying deep\nlearning to mammography; however, obtaining large amounts of annotated data\nposes a challenge for training deep learning models for this purpose, as does\nensuring generalization beyond the populations represented in the training\ndataset. Here, we present an annotation-efficient deep learning approach that\n1) achieves state-of-the-art performance in mammogram classification, 2)\nsuccessfully extends to digital breast tomosynthesis (DBT; \"3D mammography\"),\n3) detects cancers in clinically-negative prior mammograms of cancer patients,\n4) generalizes well to a population with low screening rates, and 5)\noutperforms five-out-of-five full-time breast imaging specialists by improving\nabsolute sensitivity by an average of 14%. Our results demonstrate promise\ntowards software that can improve the accuracy of and access to screening\nmammography worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:45:04 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 18:26:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lotter", "William", ""], ["Diab", "Abdul Rahman", ""], ["Haslam", "Bryan", ""], ["Kim", "Jiye G.", ""], ["Grisot", "Giorgia", ""], ["Wu", "Eric", ""], ["Wu", "Kevin", ""], ["Onieva", "Jorge Onieva", ""], ["Boxerman", "Jerrold L.", ""], ["Wang", "Meiyun", ""], ["Bandler", "Mack", ""], ["Vijayaraghavan", "Gopal", ""], ["Sorensen", "A. Gregory", ""]]}, {"id": "1912.11035", "submitter": "Sheng-Yu Wang", "authors": "Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, Alexei A.\n  Efros", "title": "CNN-generated images are surprisingly easy to spot... for now", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we ask whether it is possible to create a \"universal\" detector\nfor telling apart real images from these generated by a CNN, regardless of\narchitecture or dataset used. To test this, we collect a dataset consisting of\nfake images generated by 11 different CNN-based image generator models, chosen\nto span the space of commonly used architectures today (ProGAN, StyleGAN,\nBigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks,\nimplicit maximum likelihood estimation, second-order attention\nsuper-resolution, seeing-in-the-dark). We demonstrate that, with careful pre-\nand post-processing and data augmentation, a standard image classifier trained\non only one specific CNN generator (ProGAN) is able to generalize surprisingly\nwell to unseen architectures, datasets, and training methods (including the\njust released StyleGAN2). Our findings suggest the intriguing possibility that\ntoday's CNN-generated images share some common systematic flaws, preventing\nthem from achieving realistic image synthesis. Code and pre-trained networks\nare available at https://peterwang512.github.io/CNNDetection/ .\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:58:58 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 12:58:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Sheng-Yu", ""], ["Wang", "Oliver", ""], ["Zhang", "Richard", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1912.11066", "submitter": "Senthil Yogamani", "authors": "Pullarao Maddu, Wayne Doherty, Ganesh Sistu, Isabelle Leang, Michal\n  Uricar, Sumanth Chennupati, Hazem Rashed, Jonathan Horgan, Ciaran Hughes and\n  Senthil Yogamani", "title": "FisheyeMultiNet: Real-time Multi-task Learning Architecture for\n  Surround-view Automated Parking System", "comments": "Accepted for publication at Irish Machine Vision and Image Processing\n  (IMVIP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Parking is a low speed manoeuvring scenario which is quite\nunstructured and complex, requiring full 360{\\deg} near-field sensing around\nthe vehicle. In this paper, we discuss the design and implementation of an\nautomated parking system from the perspective of camera based deep learning\nalgorithms. We provide a holistic overview of an industrial system covering the\nembedded system, use cases and the deep learning architecture. We demonstrate a\nreal-time multi-task deep learning network called FisheyeMultiNet, which\ndetects all the necessary objects for parking on a low-power embedded system.\nFisheyeMultiNet runs at 15 fps for 4 cameras and it has three tasks namely\nobject detection, semantic segmentation and soiling detection. To encourage\nfurther research, we release a partial dataset of 5,000 images containing\nsemantic segmentation and bounding box detection ground truth via WoodScape\nproject \\cite{yogamani2019woodscape}.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:11:50 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Maddu", "Pullarao", ""], ["Doherty", "Wayne", ""], ["Sistu", "Ganesh", ""], ["Leang", "Isabelle", ""], ["Uricar", "Michal", ""], ["Chennupati", "Sumanth", ""], ["Rashed", "Hazem", ""], ["Horgan", "Jonathan", ""], ["Hughes", "Ciaran", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1912.11082", "submitter": "Xinsheng Xuan", "authors": "Xinsheng Xuan, Bo Peng, Wei Wang and Jing Dong", "title": "Scalable Fine-grained Generated Image Classification Based on Deep\n  Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generated images could reach very high quality, even human eyes\ncould not tell them apart from real images. Although there are already some\nmethods for detecting generated images in current forensic community, most of\nthese methods are used to detect a single type of generated images. The new\ntypes of generated images are emerging one after another, and the existing\ndetection methods cannot cope well. These problems prompted us to propose a\nscalable framework for multi-class classification based on deep metric\nlearning, which aims to classify the generated images finer. In addition, we\nhave increased the scalability of our framework to cope with the constant\nemergence of new types of generated images, and through fine-tuning to make the\nmodel obtain better detection performance on the new type of generated data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:19:37 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Xuan", "Xinsheng", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""]]}, {"id": "1912.11121", "submitter": "Alexander Sax", "authors": "Alexander Sax, Jeffrey O. Zhang, Bradley Emi, Amir Zamir, Silvio\n  Savarese, Leonidas Guibas, Jitendra Malik", "title": "Learning to Navigate Using Mid-Level Visual Priors", "comments": "In Conference on Robot Learning, 2019. See project website and demos\n  at http://perceptual.actor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much does having visual priors about the world (e.g. the fact that the\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\nnavigating a complex environment)? What are the consequences of not utilizing\nsuch visual priors in learning? We study these questions by integrating a\ngeneric perceptual skill set (a distance estimator, an edge detector, etc.)\nwithin a reinforcement learning framework (see Fig. 1). This skill set\n(\"mid-level vision\") provides the policy with a more processed state of the\nworld compared to raw images.\n  Our large-scale study demonstrates that using mid-level vision results in\npolicies that learn faster, generalize better, and achieve higher final\nperformance, when compared to learning from scratch and/or using\nstate-of-the-art visual and non-visual representation learning methods. We show\nthat conventional computer vision objectives are particularly effective in this\nregard and can be conveniently integrated into reinforcement learning\nframeworks. Finally, we found that no single visual representation was\nuniversally useful for all downstream tasks, hence we computationally derive a\ntask-agnostic set of representations optimized to support arbitrary downstream\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:45:50 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sax", "Alexander", ""], ["Zhang", "Jeffrey O.", ""], ["Emi", "Bradley", ""], ["Zamir", "Amir", ""], ["Savarese", "Silvio", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""]]}, {"id": "1912.11164", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng and Yi Yang", "title": "Unsupervised Scene Adaptation with Memory Regularization in vivo", "comments": "7 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unsupervised scene adaptation problem of learning from both\nlabeled source data and unlabeled target data. Existing methods focus on\nminoring the inter-domain gap between the source and target domains. However,\nthe intra-domain knowledge and inherent uncertainty learned by the network are\nunder-explored. In this paper, we propose an orthogonal method, called memory\nregularization in vivo to exploit the intra-domain knowledge and regularize the\nmodel training. Specifically, we refer to the segmentation model itself as the\nmemory module, and minor the discrepancy of the two classifiers, i.e., the\nprimary classifier and the auxiliary classifier, to reduce the prediction\ninconsistency. Without extra parameters, the proposed method is complementary\nto the most existing domain adaptation methods and could generally improve the\nperformance of existing methods. Albeit simple, we verify the effectiveness of\nmemory regularization on two synthetic-to-real benchmarks: GTA5 -> Cityscapes\nand SYNTHIA -> Cityscapes, yielding +11.1% and +11.3% mIoU improvement over the\nbaseline model, respectively. Besides, a similar +12.0% mIoU improvement is\nobserved on the cross-city benchmark: Cityscapes -> Oxford RobotCar.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 01:12:36 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 08:31:32 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zheng", "Zhedong", ""], ["Yang", "Yi", ""]]}, {"id": "1912.11171", "submitter": "Yuxin Wen", "authors": "Yuxin Wen, Jiehong Lin, Ke Chen, C. L. Philip Chen, Kui Jia", "title": "Geometry-Aware Generation of Adversarial Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been shown to be vulnerable to adversarial\nexamples. While most of the existing methods for adversarial attack and defense\nwork on the 2D image domain, a few recent attempts have been made to extend\nthem to 3D point cloud data. However, adversarial results obtained by these\nmethods typically contain point outliers, which are both noticeable and easy to\ndefend against using the simple techniques of outlier removal. Motivated by the\ndifferent mechanisms by which humans perceive 2D images and 3D shapes, in this\npaper we propose the new design of \\emph{geometry-aware objectives}, whose\nsolutions favor (the discrete versions of) the desired surface properties of\nsmoothness and fairness. To generate adversarial point clouds, we use a\ntargeted attack misclassification loss that supports continuous pursuit of\nincreasingly malicious signals. Regularizing the targeted attack loss with our\nproposed geometry-aware objectives results in our proposed method,\nGeometry-Aware Adversarial Attack ($GeoA^3$). The results of $GeoA^3$ tend to\nbe more harmful, arguably harder to defend against, and of the key adversarial\ncharacterization of being imperceptible to humans. While the main focus of this\npaper is to learn to generate adversarial point clouds, we also present a\nsimple but effective algorithm termed $Geo_{+}A^3$-IterNormPro, with Iterative\nNormal Projection (IterNorPro) that solves a new objective function\n$Geo_{+}A^3$, towards surface-level adversarial attacks via generation of\nadversarial point clouds. We quantitatively evaluate our methods on both\nsynthetic and physical objects in terms of attack success rate and geometric\nregularity. For a qualitative evaluation, we conduct subjective studies by\ncollecting human preferences from Amazon Mechanical Turk. Comparative results\nin comprehensive experiments confirm the advantages of our proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 01:52:46 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 09:07:45 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 06:23:06 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wen", "Yuxin", ""], ["Lin", "Jiehong", ""], ["Chen", "Ke", ""], ["Chen", "C. L. Philip", ""], ["Jia", "Kui", ""]]}, {"id": "1912.11180", "submitter": "Ke Chen", "authors": "Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang and Kui\n  Jia", "title": "Cascading Convolutional Color Constancy", "comments": "9 pages, 5 figures, 4 tables, accepted by AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regressing the illumination of a scene from the representations of object\nappearances is popularly adopted in computational color constancy. However,\nit's still challenging due to intrinsic appearance and label ambiguities caused\nby unknown illuminants, diverse reflection property of materials and extrinsic\nimaging factors (such as different camera sensors). In this paper, we introduce\na novel algorithm by Cascading Convolutional Color Constancy (in short, C4) to\nimprove robustness of regression learning and achieve stable generalization\ncapability across datasets (different cameras and scenes) in a unique\nframework. The proposed C4 method ensembles a series of dependent illumination\nhypotheses from each cascade stage via introducing a weighted\nmultiply-accumulate loss function, which can inherently capture different modes\nof illuminations and explicitly enforce coarse-to-fine network optimization.\nExperimental results on the public Color Checker and NUS 8-Camera benchmarks\ndemonstrate superior performance of the proposed algorithm in comparison with\nthe state-of-the-art methods, especially for more difficult scenes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:46:36 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Yu", "Huanglin", ""], ["Chen", "Ke", ""], ["Wang", "Kaiqi", ""], ["Qian", "Yanlin", ""], ["Zhang", "Zhaoxiang", ""], ["Jia", "Kui", ""]]}, {"id": "1912.11186", "submitter": "Lyndon Chan", "authors": "Lyndon Chan, Mahdi S. Hosseini, Konstantinos N. Plataniotis", "title": "A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in\n  Different Image Domains", "comments": "23 pages; accepted by International Journal of Computer Vision\n  (IJCV). Associated code available at\n  https://github.com/lyndonchan/wsss-analysis. To view Supplementary Materials,\n  please download pdf file listed under \"Ancillary files\". Int J Comput Vis\n  (2020)", "journal-ref": null, "doi": "10.1007/s11263-020-01373-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed methods for weakly-supervised semantic segmentation have\nachieved impressive performance in predicting pixel classes despite being\ntrained with only image labels which lack positional information. Because image\nannotations are cheaper and quicker to generate, weak supervision is more\npractical than full supervision for training segmentation algorithms. These\nmethods have been predominantly developed to solve the background separation\nand partial segmentation problems presented by natural scene images and it is\nunclear whether they can be simply transferred to other domains with different\ncharacteristics, such as histopathology and satellite images, and still perform\nwell. This paper evaluates state-of-the-art weakly-supervised semantic\nsegmentation methods on natural scene, histopathology, and satellite image\ndatasets and analyzes how to determine which method is most suitable for a\ngiven dataset. Our experiments indicate that histopathology and satellite\nimages present a different set of problems for weakly-supervised semantic\nsegmentation than natural scene images, such as ambiguous boundaries and class\nco-occurrence. Methods perform well for datasets they were developed on, but\ntend to perform poorly on other datasets. We present some practical techniques\nfor these methods on unseen datasets and argue that more work is needed for a\ngeneralizable approach to weakly-supervised semantic segmentation. Our full\ncode implementation is available on GitHub:\nhttps://github.com/lyndonchan/wsss-analysis.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:00:34 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 04:42:47 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 20:19:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chan", "Lyndon", ""], ["Hosseini", "Mahdi S.", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1912.11188", "submitter": "Zhao Zhong", "authors": "Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong", "title": "Adversarial AutoAugment", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) has been widely utilized to improve generalization in\ntraining deep neural networks. Recently, human-designed data augmentation has\nbeen gradually replaced by automatically learned augmentation policy. Through\nfinding the best policy in well-designed search space of data augmentation,\nAutoAugment can significantly improve validation accuracy on image\nclassification tasks. However, this approach is not computationally practical\nfor large-scale problems. In this paper, we develop an adversarial method to\narrive at a computationally-affordable solution called Adversarial AutoAugment,\nwhich can simultaneously optimize target related object and augmentation policy\nsearch loss. The augmentation policy network attempts to increase the training\nloss of a target network through generating adversarial augmentation policies,\nwhile the target network can learn more robust features from harder examples to\nimprove the generalization. In contrast to prior work, we reuse the computation\nin target network training for policy evaluation, and dispense with the\nretraining of the target network. Compared to AutoAugment, this leads to about\n12x reduction in computing cost and 11x shortening in time overhead on\nImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100,\nImageNet, and demonstrate significant performance improvements over\nstate-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is\nthe currently best performing single model. On ImageNet, we achieve a leading\nperformance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D\nwithout extra data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:17:17 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhang", "Xinyu", ""], ["Wang", "Qiang", ""], ["Zhang", "Jian", ""], ["Zhong", "Zhao", ""]]}, {"id": "1912.11191", "submitter": "Zhao Zhong", "authors": "Muyuan Fang, Qiang Wang, Zhao Zhong", "title": "BETANAS: BalancEd TrAining and selective drop for Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic neural architecture search techniques are becoming increasingly\nimportant in machine learning area. Especially, weight sharing methods have\nshown remarkable potentials on searching good network architectures with few\ncomputational resources. However, existing weight sharing methods mainly suffer\nlimitations on searching strategies: these methods either uniformly train all\nnetwork paths to convergence which introduces conflicts between branches and\nwastes a large amount of computation on unpromising candidates, or selectively\ntrain branches with different frequency which leads to unfair evaluation and\ncomparison among paths. To address these issues, we propose a novel neural\narchitecture search method with balanced training strategy to ensure fair\ncomparisons and a selective drop mechanism to reduce conflicts among candidate\npaths. The experimental results show that our proposed method can achieve a\nleading performance of 79.0% on ImageNet under mobile settings, which\noutperforms other state-of-the-art methods in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:24:06 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Fang", "Muyuan", ""], ["Wang", "Qiang", ""], ["Zhong", "Zhao", ""]]}, {"id": "1912.11194", "submitter": "Qi Qi", "authors": "Qi Qi, Yan Yan, Xiaoyu Wang, Tianbao Yang", "title": "A Simple and Effective Framework for Pairwise Deep Metric Learning", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning (DML) has received much attention in deep learning due\nto its wide applications in computer vision. Previous studies have focused on\ndesigning complicated losses and hard example mining methods, which are mostly\nheuristic and lack of theoretical understanding. In this paper, we cast DML as\na simple pairwise binary classification problem that classifies a pair of\nexamples as similar or dissimilar. It identifies the most critical issue in\nthis problem--imbalanced data pairs. To tackle this issue, we propose a simple\nand effective framework to sample pairs in a batch of data for updating the\nmodel. The key to this framework is to define a robust loss for all pairs over\na mini-batch of data, which is formulated by distributionally robust\noptimization. The flexibility in constructing the uncertainty decision set of\nthe dual variable allows us to recover state-of-the-art complicated losses and\nalso to induce novel variants. Empirical studies on several benchmark data sets\ndemonstrate that our simple and effective method outperforms the\nstate-of-the-art results. Codes are available at:\nhttps://github.com/qiqi-helloworld/A-Simple-and-Effective-Framework-for-Pairewise-Distance-Metric-Learning\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:47:25 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 15:58:11 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 15:44:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Qi", "Qi", ""], ["Yan", "Yan", ""], ["Wang", "Xiaoyu", ""], ["Yang", "Tianbao", ""]]}, {"id": "1912.11234", "submitter": "Feng Liang", "authors": "Feng Liang, Chen Lin, Ronghao Guo, Ming Sun, Wei Wu, Junjie Yan and\n  Wanli Ouyang", "title": "Computation Reallocation for Object Detection", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The allocation of computation resources in the backbone is a crucial issue in\nobject detection. However, classification allocation pattern is usually adopted\ndirectly to object detector, which is proved to be sub-optimal. In order to\nreallocate the engaged computation resources in a more efficient way, we\npresent CR-NAS (Computation Reallocation Neural Architecture Search) that can\nlearn computation reallocation strategies across different feature resolution\nand spatial position diectly on the target detection dataset. A two-level\nreallocation space is proposed for both stage and spatial reallocation. A novel\nhierarchical search procedure is adopted to cope with the complex search space.\nWe apply CR-NAS to multiple backbones and achieve consistent improvements. Our\nCR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO\nAP respectively without any additional computation budget. The models\ndiscovered by CR-NAS can be equiped to other powerful detection neck/head and\nbe easily transferred to other dataset, e.g. PASCAL VOC, and other vision\ntasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to\nimprove the performance of various networks, which is demanding.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 07:19:04 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Liang", "Feng", ""], ["Lin", "Chen", ""], ["Guo", "Ronghao", ""], ["Sun", "Ming", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1912.11236", "submitter": "Le Zhang Dr", "authors": "Le Zhang, Zenglin Shi, Joey Tianyi Zhou, Ming-Ming Cheng, Yun Liu,\n  Jia-Wang Bian, Zeng Zeng and Chunhua Shen", "title": "Ordered or Orderless: A Revisit for Video based Person Re-Identification", "comments": "Under Minor Revision in IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is recurrent network really necessary for learning a good visual\nrepresentation for video based person re-identification (VPRe-id)? In this\npaper, we first show that the common practice of employing recurrent neural\nnetworks (RNNs) to aggregate temporal spatial features may not be optimal.\nSpecifically, with a diagnostic analysis, we show that the recurrent structure\nmay not be effective to learn temporal dependencies than what we expected and\nimplicitly yields an orderless representation. Based on this observation, we\nthen present a simple yet surprisingly powerful approach for VPRe-id, where we\ntreat VPRe-id as an efficient orderless ensemble of image based person\nre-identification problem. More specifically, we divide videos into individual\nimages and re-identify person with ensemble of image based rankers. Under the\ni.i.d. assumption, we provide an error bound that sheds light upon how could we\nimprove VPRe-id. Our work also presents a promising way to bridge the gap\nbetween video and image based person re-identification. Comprehensive\nexperimental evaluations demonstrate that the proposed solution achieves\nstate-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID\n2011, and MARS).\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 07:29:14 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhang", "Le", ""], ["Shi", "Zenglin", ""], ["Zhou", "Joey Tianyi", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Yun", ""], ["Bian", "Jia-Wang", ""], ["Zeng", "Zeng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1912.11258", "submitter": "Peng Xu", "authors": "Peng Xu, Chaitanya K. Joshi, Xavier Bresson", "title": "Multi-Graph Transformer for Free-Hand Sketch Recognition", "comments": "This paper has been accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations of free-hand sketches remains a\nchallenging task given the signal sparsity and the high-level abstraction of\nsketches. Existing techniques have focused on exploiting either the static\nnature of sketches with Convolutional Neural Networks (CNNs) or the temporal\nsequential property with Recurrent Neural Networks (RNNs). In this work, we\npropose a new representation of sketches as multiple sparsely connected graphs.\nWe design a novel Graph Neural Network (GNN), the Multi-Graph Transformer\n(MGT), for learning representations of sketches from multiple graphs which\nsimultaneously capture global and local geometric stroke structures, as well as\ntemporal information. We report extensive numerical experiments on a sketch\nrecognition task to demonstrate the performance of the proposed approach.\nParticularly, MGT applied on 414k sketches from Google QuickDraw: (i) achieves\nsmall recognition gap to the CNN-based performance upper bound (72.80% vs.\n74.22%), and (ii) outperforms all RNN-based models by a significant margin. To\nthe best of our knowledge, this is the first work proposing to represent\nsketches as graphs and apply GNNs for sketch recognition. Code and trained\nmodels are available at\nhttps://github.com/PengBoXiangShang/multigraph_transformer.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 09:28:10 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 17:16:50 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 04:34:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Xu", "Peng", ""], ["Joshi", "Chaitanya K.", ""], ["Bresson", "Xavier", ""]]}, {"id": "1912.11264", "submitter": "Zhiqiang Gong", "authors": "Zhiqiang Gong, Weidong Hu, Xiaoyong Du, Ping Zhong, Panhe Hu", "title": "Deep Manifold Embedding for Hyperspectral Image Classification", "comments": "Accepted by IEEE TCYB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have played a more and more important role in\nhyperspectral image classification. However, the general deep learning methods\nmainly take advantage of the information of sample itself or the pairwise\ninformation between samples while ignore the intrinsic data structure within\nthe whole data. To tackle this problem, this work develops a novel deep\nmanifold embedding method(DMEM) for hyperspectral image classification. First,\neach class in the image is modelled as a specific nonlinear manifold and the\ngeodesic distance is used to measure the correlation between the samples. Then,\nbased on the hierarchical clustering, the manifold structure of the data can be\ncaptured and each nonlinear data manifold can be divided into several\nsub-classes. Finally, considering the distribution of each sub-class and the\ncorrelation between different subclasses, the DMEM is constructed to preserve\nthe estimated geodesic distances on the data manifold between the learned low\ndimensional features of different samples. Experiments over three real-world\nhyperspectral image datasets have demonstrated the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 09:41:56 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 12:40:19 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 16:15:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gong", "Zhiqiang", ""], ["Hu", "Weidong", ""], ["Du", "Xiaoyong", ""], ["Zhong", "Ping", ""], ["Hu", "Panhe", ""]]}, {"id": "1912.11312", "submitter": "Sabine M\\\"uller", "authors": "Sabine M\\\"uller, Joachim Weickert, Norbert Graf", "title": "Robustness of Brain Tumor Segmentation", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The segmentation of brain tumors is one of the most active areas of\nmedical image analysis. While current methods perform superhuman on benchmark\ndata sets, their applicability in daily clinical practice has not been\nevaluated. In our work we investigate the generalization behavior of deep\nneural networks in this scenario.\n  Approach: We evaluate the performance of three state-of-the-art methods, a\nbasic U-net architecture and a cascadic Mumford-Shah approach. We also propose\ntwo simple modifications (which do not change the topology) to improve\ngeneralization performance.\n  Results: In our experiments we show that a well-trained U-network shows the\nbest generalization behavior and is sufficient to solve this segmentation\nproblem. We illustrate why extensions of this model in a realistic scenario can\nbe not only pointless but even harmful.\n  Conclusions: We conclude from our experiments that the generalization\nperformance of deep neural networks is severely limited in medical image\nanalysis especially in the area of brain tumor segmentation. In our opinion,\ncurrent topologies are optimized for the actual benchmark data set, but are not\ndirectly applicable in daily clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:18:37 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 20:07:46 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 21:56:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["M\u00fcller", "Sabine", ""], ["Weickert", "Joachim", ""], ["Graf", "Norbert", ""]]}, {"id": "1912.11316", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson,\n  Isabelle Bloch", "title": "TRADI: Tracking deep neural network weight distributions for uncertainty\n  estimation", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During training, the weights of a Deep Neural Network (DNN) are optimized\nfrom a random initialization towards a nearly optimum value minimizing a loss\nfunction. Only this final state of the weights is typically kept for testing,\nwhile the wealth of information on the geometry of the weight space,\naccumulated over the descent towards the minimum is discarded. In this work we\npropose to make use of this knowledge and leverage it for computing the\ndistributions of the weights of the DNN. This can be further used for\nestimating the epistemic uncertainty of the DNN by sampling an ensemble of\nnetworks from these distributions. To this end we introduce a method for\ntracking the trajectory of the weights during optimization, that does not\nrequire any changes in the architecture nor on the training procedure. We\nevaluate our method on standard classification and regression benchmarks, and\non out-of-distribution detection for classification and semantic segmentation.\nWe achieve competitive results, while preserving computational efficiency in\ncomparison to other popular approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:22:45 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 20:21:09 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 13:05:19 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 19:21:28 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 12:27:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Franchi", "Gianni", ""], ["Bursuc", "Andrei", ""], ["Aldea", "Emanuel", ""], ["Dubuisson", "Severine", ""], ["Bloch", "Isabelle", ""]]}, {"id": "1912.11325", "submitter": "Xue-Feng Zhu", "authors": "Fei Feng, Xiao-Jun Wu, Tianyang Xu, Josef Kittler, Xue-Feng Zhu", "title": "Adaptive Distraction Context Aware Tracking Based on Correlation Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Discriminative Correlation Filter (CF) uses a circulant convolution\noperation to provide several training samples for the design of a classifier\nthat can distinguish the target from the background. The filter design may be\ninterfered by objects close to the target during the tracking process,\nresulting in tracking failure. This paper proposes an adaptive distraction\ncontext aware tracking algorithm to solve this problem. In the response map\nobtained for the previous frame by the CF algorithm, we adaptively find the\nimage blocks that are similar to the target and use them as negative samples.\nThis diminishes the influence of similar image blocks on the classifier in the\ntracking process and its accuracy is improved. The tracking results on video\nsequences show that the algorithm can cope with rapid changes such as occlusion\nand rotation, and can adaptively use the distractive objects around the target\nas negative samples to improve the accuracy of target tracking.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:52:53 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Feng", "Fei", ""], ["Wu", "Xiao-Jun", ""], ["Xu", "Tianyang", ""], ["Kittler", "Josef", ""], ["Zhu", "Xue-Feng", ""]]}, {"id": "1912.11343", "submitter": "Xue-Feng Zhu", "authors": "Yi-Xuan Wang, Xiao-Jun Wu, Xue-Feng Zhu", "title": "Robust Visual Tracking via Implicit Low-Rank Constraints and Structural\n  Color Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the guaranteed discrimination and efficiency of spatial appearance\nmodel, Discriminative Correlation Filters (DCF-) based tracking methods have\nachieved outstanding performance recently. However, the construction of\neffective temporal appearance model is still challenging on account of filter\ndegeneration becomes a significant factor that causes tracking failures in the\nDCF framework. To encourage temporal continuity and to explore the smooth\nvariation of target appearance, we propose to enhance low-rank structure of the\nlearned filters, which can be realized by constraining the successive filters\nwithin a $\\ell_2$-norm ball. Moreover, we design a global descriptor,\nstructural color histograms, to provide complementary support to the final\nresponse map, improving the stability and robustness to the DCF framework. The\nexperimental results on standard benchmarks demonstrate that our Implicit\nLow-Rank Constraints and Structural Color Histograms (ILRCSCH) tracker\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 13:27:48 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Wang", "Yi-Xuan", ""], ["Wu", "Xiao-Jun", ""], ["Zhu", "Xue-Feng", ""]]}, {"id": "1912.11350", "submitter": "Jing Gao", "authors": "Jing Gao, N. Anantrasirichai and David Bull", "title": "Atmospheric turbulence removal using convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel deep learning-based method for mitigating the\neffects of atmospheric distortion. We have built an end-to-end supervised\nconvolutional neural network (CNN) to reconstruct turbulence-corrupted video\nsequence. Our framework has been developed on the residual learning concept,\nwhere the spatio-temporal distortions are learnt and predicted. Our experiments\ndemonstrate that the proposed method can deblur, remove ripple effect and\nenhance contrast of the video sequences simultaneously. Our model was trained\nand tested with both simulated and real distortions. Experimental results of\nthe real distortions show that our method outperforms the existing ones by up\nto 3.8% in term of the quality of restored images, and it achieves faster speed\nthan the state-of-the-art methods by up to 23 times with GPU implementation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:22:55 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Gao", "Jing", ""], ["Anantrasirichai", "N.", ""], ["Bull", "David", ""]]}, {"id": "1912.11370", "submitter": "Xiaohua Zhai", "authors": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver,\n  Jessica Yung, Sylvain Gelly, Neil Houlsby", "title": "Big Transfer (BiT): General Visual Representation Learning", "comments": "The first three authors contributed equally. Results on ObjectNet are\n  reported in v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer of pre-trained representations improves sample efficiency and\nsimplifies hyperparameter tuning when training deep neural networks for vision.\nWe revisit the paradigm of pre-training on large supervised datasets and\nfine-tuning the model on a target task. We scale up pre-training, and propose a\nsimple recipe that we call Big Transfer (BiT). By combining a few carefully\nselected components, and transferring using a simple heuristic, we achieve\nstrong performance on over 20 datasets. BiT performs well across a surprisingly\nwide range of data regimes -- from 1 example per class to 1M total examples.\nBiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3%\non the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT\nattains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10\nwith 10 examples per class. We conduct detailed analysis of the main components\nthat lead to high transfer performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 14:04:11 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:47:49 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 20:48:23 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""], ["Zhai", "Xiaohua", ""], ["Puigcerver", "Joan", ""], ["Yung", "Jessica", ""], ["Gelly", "Sylvain", ""], ["Houlsby", "Neil", ""]]}, {"id": "1912.11393", "submitter": "Gopal Sharma", "authors": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis,\n  Subhransu Maji", "title": "Neural Shape Parsers for Constructive Solid Geometry", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.08290", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructive Solid Geometry (CSG) is a geometric modeling technique that\ndefines complex shapes by recursively applying boolean operations on primitives\nsuch as spheres and cylinders. We present CSGNe, a deep network architecture\nthat takes as input a 2D or 3D shape and outputs a CSG program that models it.\nParsing shapes into CSG programs is desirable as it yields a compact and\ninterpretable generative model. However, the task is challenging since the\nspace of primitives and their combinations can be prohibitively large. CSGNe\nuses a convolutional encoder and recurrent decoder based on deep networks to\nmap shapes to modeling instructions in a feed-forward manner and is\nsignificantly faster than bottom-up approaches. We investigate two\narchitectures for this task --- a vanilla encoder (CNN) - decoder (RNN) and\nanother architecture that augments the encoder with an explicit memory module\nbased on the program execution stack. The stack augmentation improves the\nreconstruction quality of the generated shape and learning efficiency. Our\napproach is also more effective as a shape primitive detector compared to a\nstate-of-the-art object detector. Finally, we demonstrate CSGNet can be trained\non novel datasets without program annotations through policy gradient\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 00:08:13 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sharma", "Gopal", ""], ["Goyal", "Rishabh", ""], ["Liu", "Difan", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""]]}, {"id": "1912.11425", "submitter": "Christopher J. Anders", "authors": "Christopher J. Anders, Leander Weber, David Neumann, Wojciech Samek,\n  Klaus-Robert M\\\"uller, Sebastian Lapuschkin", "title": "Finding and Removing Clever Hans: Using Explanation Methods to Debug and\n  Improve Deep Models", "comments": "47 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Contemporary learning models for computer vision are typically trained on\nvery large (benchmark) datasets with millions of samples. These may, however,\ncontain biases, artifacts, or errors that have gone unnoticed and are\nexploitable by the model. In the worst case, the trained model does not learn a\nvalid and generalizable strategy to solve the problem it was trained for, and\nbecomes a 'Clever-Hans' (CH) predictor that bases its decisions on spurious\ncorrelations in the training data, potentially yielding an unrepresentative or\nunfair, and possibly even hazardous predictor. In this paper, we contribute by\nproviding a comprehensive analysis framework based on a scalable statistical\nanalysis of attributions from explanation methods for large data corpora. Based\non a recent technique - Spectral Relevance Analysis - we propose the following\ntechnical contributions and resulting findings: (a) a scalable quantification\nof artifactual and poisoned classes where the machine learning models under\nstudy exhibit CH behavior, (b) several approaches denoted as Class Artifact\nCompensation (ClArC), which are able to effectively and significantly reduce a\nmodel's CH behavior. I.e., we are able to un-Hans models trained on (poisoned)\ndatasets, such as the popular ImageNet data corpus. We demonstrate that ClArC,\ndefined in a simple theoretical framework, may be implemented as part of a\nNeural Network's training or fine-tuning process, or in a post-hoc manner by\ninjecting additional layers, preventing any further propagation of undesired CH\nfeatures, into the network architecture. Using our proposed methods, we provide\nqualitative and quantitative analyses of the biases and artifacts in various\ndatasets. We demonstrate that these insights can give rise to improved, more\nrepresentative and fairer models operating on implicitly cleaned data corpora.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:40:27 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 20:13:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Anders", "Christopher J.", ""], ["Weber", "Leander", ""], ["Neumann", "David", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Lapuschkin", "Sebastian", ""]]}, {"id": "1912.11460", "submitter": "Hamid Karimi", "authors": "Hamid Karimi, Tyler Derr, Jiliang Tang", "title": "Characterizing the Decision Boundary of Deep Neural Networks", "comments": "Please contact the first author for any issue or the question\n  regarding this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and in particular, deep neural classifiers have become\nan integral part of many modern applications. Despite their practical success,\nwe still have limited knowledge of how they work and the demand for such an\nunderstanding is evergrowing. In this regard, one crucial aspect of deep neural\nnetwork classifiers that can help us deepen our knowledge about their\ndecision-making behavior is to investigate their decision boundaries.\nNevertheless, this is contingent upon having access to samples populating the\nareas near the decision boundary. To achieve this, we propose a novel approach\nwe call Deep Decision boundary Instance Generation (DeepDIG). DeepDIG utilizes\na method based on adversarial example generation as an effective way of\ngenerating samples near the decision boundary of any deep neural network model.\nThen, we introduce a set of important principled characteristics that take\nadvantage of the generated instances near the decision boundary to provide\nmultifaceted understandings of deep neural networks. We have performed\nextensive experiments on multiple representative datasets across various deep\nneural network models and characterized their decision boundaries. The code is\npublicly available at https://github.com/hamidkarimi/DeepDIG/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:30:11 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 20:55:18 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 16:18:25 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Karimi", "Hamid", ""], ["Derr", "Tyler", ""], ["Tang", "Jiliang", ""]]}, {"id": "1912.11463", "submitter": "Mukul Khanna", "authors": "Zeeshan Khan, Mukul Khanna and Shanmuganathan Raman", "title": "FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback\n  Network", "comments": "2019 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) image generation from a single exposure low dynamic\nrange (LDR) image has been made possible due to the recent advances in Deep\nLearning. Various feed-forward Convolutional Neural Networks (CNNs) have been\nproposed for learning LDR to HDR representations. To better utilize the power\nof CNNs, we exploit the idea of feedback, where the initial low level features\nare guided by the high level features using a hidden state of a Recurrent\nNeural Network. Unlike a single forward pass in a conventional feed-forward\nnetwork, the reconstruction from LDR to HDR in a feedback network is learned\nover multiple iterations. This enables us to create a coarse-to-fine\nrepresentation, leading to an improved reconstruction at every iteration.\nVarious advantages over standard feed-forward networks include early\nreconstruction ability and better reconstruction quality with fewer network\nparameters. We design a dense feedback block and propose an end-to-end feedback\nnetwork- FHDR for HDR image generation from a single exposure LDR image.\nQualitative and quantitative evaluations show the superiority of our approach\nover the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:38:40 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Khan", "Zeeshan", ""], ["Khanna", "Mukul", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1912.11473", "submitter": "Han Hu", "authors": "Ze Yang and Yinghao Xu and Han Xue and Zheng Zhang and Raquel Urtasun\n  and Liwei Wang and Stephen Lin and Han Hu", "title": "Dense RepPoints: Representing Visual Objects with Dense Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new object representation, called Dense RepPoints, that utilizes\na large set of points to describe an object at multiple levels, including both\nbox level and pixel level. Techniques are proposed to efficiently process these\ndense points, maintaining near-constant complexity with increasing point\nnumbers. Dense RepPoints is shown to represent and learn object segments well,\nwith the use of a novel distance transform sampling method combined with\nset-to-set supervision. The distance transform sampling combines the strengths\nof contour and grid representations, leading to performance that surpasses\ncounterparts based on contours or grids. Code is available at\n\\url{https://github.com/justimyhxu/Dense-RepPoints}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:59:10 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 18:15:03 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 17:23:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yang", "Ze", ""], ["Xu", "Yinghao", ""], ["Xue", "Han", ""], ["Zhang", "Zheng", ""], ["Urtasun", "Raquel", ""], ["Wang", "Liwei", ""], ["Lin", "Stephen", ""], ["Hu", "Han", ""]]}, {"id": "1912.11474", "submitter": "Changan Chen", "authors": "Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual\n  Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, Kristen Grauman", "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments", "comments": "Accepted to ECCV 2020 (Spotlight). Project page:\n  http://vision.cs.utexas.edu/projects/audio_visual_navigation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving around in the world is naturally a multisensory experience, but\ntoday's embodied agents are deaf---restricted to solely their visual perception\nof the environment. We introduce audio-visual navigation for complex,\nacoustically and visually realistic 3D environments. By both seeing and\nhearing, the agent must learn to navigate to a sounding object. We propose a\nmulti-modal deep reinforcement learning approach to train navigation policies\nend-to-end from a stream of egocentric audio-visual observations, allowing the\nagent to (1) discover elements of the geometry of the physical space indicated\nby the reverberating audio and (2) detect and follow sound-emitting targets. We\nfurther introduce SoundSpaces: a first-of-its-kind dataset of audio renderings\nbased on geometrical acoustic simulations for two sets of publicly available 3D\nenvironments (Matterport3D and Replica), and we instrument Habitat to support\nthe new sensor, making it possible to insert arbitrary sound sources in an\narray of real-world scanned environments. Our results show that audio greatly\nbenefits embodied visual navigation in 3D spaces, and our work lays groundwork\nfor new research in embodied AI with audio-visual perception.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:59:50 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 17:59:47 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 18:00:31 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Changan", ""], ["Jain", "Unnat", ""], ["Schissler", "Carl", ""], ["Gari", "Sebastia Vicenc Amengual", ""], ["Al-Halah", "Ziad", ""], ["Ithapu", "Vamsi Krishna", ""], ["Robinson", "Philip", ""], ["Grauman", "Kristen", ""]]}, {"id": "1912.11477", "submitter": "Shizhan Lu", "authors": "Shizhan Lu", "title": "Self-adaption grey DBSCAN clustering", "comments": "8 pages, 4 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1906.11416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis, a classical issue in data mining, is widely used in\nvarious research areas. This article aims at proposing a self-adaption grey\nDBSCAN clustering (SAG-DBSCAN) algorithm. First, the grey relational matrix is\nused to obtain the grey local density indicator, and then this indicator is\napplied to make self-adapting noise identification for obtaining a dense subset\nof clustering dataset, finally, the DBSCAN which automatically selects\nparameters is utilized to cluster the dense subset. Several frequently-used\ndatasets were used to demonstrate the performance and effectiveness of the\nproposed clustering algorithm and to compare the results with those of other\nstate-of-the-art algorithms. The comprehensive comparisons indicate that our\nmethod has advantages over other compared methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:46:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lu", "Shizhan", ""]]}, {"id": "1912.11494", "submitter": "Narciso L\\'opez", "authors": "Andrea V\\'azquez, Narciso L\\'opez-L\\'opez, Nicole Labra, Miguel\n  Figueroa, Cyril Poupon, Jean-Fran\\c{c}ois Mangin, Cecilia Hern\\'andez, Pamela\n  Guevara", "title": "Parallel optimization of fiber bundle segmentation for massive\n  tractography datasets", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO\n  NACIONAL/2016-21160342, CONICYT FONDECYT 1161427, CONICYT PIA/Anillo de\n  Investigaci\\'on en Ciencia y Tecnolog\\'ia ACT172121, CONICYT BASAL FB0008 and\n  from CONICYT Basal FB0001", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759208", "report-no": null, "categories": "cs.DS cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimized algorithm that performs automatic classification of\nwhite matter fibers based on a multi-subject bundle atlas. We implemented a\nparallel algorithm that improves upon its previous version in both execution\ntime and memory usage. Our new version uses the local memory of each processor,\nwhich leads to a reduction in execution time. Hence, it allows the analysis of\nbigger subject and/or atlas datasets. As a result, the segmentation of a\nsubject of 4,145,000 fibers is reduced from about 14 minutes in the previous\nversion to about 6 minutes, yielding an acceleration of 2.34. In addition, the\nnew algorithm reduces the memory consumption of the previous version by a\nfactor of 0.79.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 19:08:51 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["V\u00e1zquez", "Andrea", ""], ["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["Labra", "Nicole", ""], ["Figueroa", "Miguel", ""], ["Poupon", "Cyril", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Hern\u00e1ndez", "Cecilia", ""], ["Guevara", "Pamela", ""]]}, {"id": "1912.11521", "submitter": "Tong He", "authors": "Jialin Gao, Tong He, Xi Zhou, Shiming Ge", "title": "Focusing and Diffusion: Bidirectional Attentive Graph Convolutional\n  Networks for Skeleton-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collection of approaches based on graph convolutional networks have proven\nsuccess in skeleton-based action recognition by exploring neighborhood\ninformation and dense dependencies between intra-frame joints. However, these\napproaches usually ignore the spatial-temporal global context as well as the\nlocal relation between inter-frame and intra-frame. In this paper, we propose a\nfocusing and diffusion mechanism to enhance graph convolutional networks by\npaying attention to the kinematic dependence of articulated human pose in a\nframe and their implicit dependencies over frames. In the focusing process, we\nintroduce an attention module to learn a latent node over the intra-frame\njoints to convey spatial contextual information. In this way, the sparse\nconnections between joints in a frame can be well captured, while the global\ncontext over the entire sequence is further captured by these hidden nodes with\na bidirectional LSTM. In the diffusing process, the learned spatial-temporal\ncontextual information is passed back to the spatial joints, leading to a\nbidirectional attentive graph convolutional network (BAGCN) that can facilitate\nskeleton-based action recognition. Extensive experiments on the challenging NTU\nRGB+D and Skeleton-Kinetics benchmarks demonstrate the efficacy of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:35:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gao", "Jialin", ""], ["He", "Tong", ""], ["Zhou", "Xi", ""], ["Ge", "Shiming", ""]]}, {"id": "1912.11540", "submitter": "Abdolreza Rashno Dr.", "authors": "Elyas Rashno, Abdolreza Rashno and Sadegh Fadaei", "title": "Fluid segmentation in Neutrosophic domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) as retina imaging technology is currently\nused by ophthalmologist as a non-invasive and non-contact method for diagnosis\nof agerelated degeneration (AMD) and diabetic macular edema (DME) diseases.\nFluid regions in OCT images reveal the main signs of AMD and DME. In this\npaper, an efficient and fast clustering in neutrosophic (NS) domain referred as\nneutrosophic C-means is adapted for fluid segmentation. For this task, a NCM\ncost function in NS domain is adapted for fluid segmentation and then optimized\nby gradient descend methods which leads to binary segmentation of OCT Bscans to\nfluid and tissue regions. The proposed method is evaluated in OCT datasets of\nsubjects with DME abnormalities. Results showed that the proposed method\noutperforms existing fluid segmentation methods by 6% in dice coefficient and\nsensitivity criteria.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:52:00 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Rashno", "Elyas", ""], ["Rashno", "Abdolreza", ""], ["Fadaei", "Sadegh", ""]]}, {"id": "1912.11545", "submitter": "Dror Simon", "authors": "Dror Simon and Aviad Aberdam", "title": "Barycenters of Natural Images -- Constrained Wasserstein Barycenters for\n  Image Morphing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image interpolation, or image morphing, refers to a visual transition between\ntwo (or more) input images. For such a transition to look visually appealing,\nits desirable properties are (i) to be smooth; (ii) to apply the minimal\nrequired change in the image; and (iii) to seem \"real\", avoiding unnatural\nartifacts in each image in the transition. To obtain a smooth and\nstraightforward transition, one may adopt the well-known Wasserstein Barycenter\nProblem (WBP). While this approach guarantees minimal changes under the\nWasserstein metric, the resulting images might seem unnatural. In this work, we\npropose a novel approach for image morphing that possesses all three desired\nproperties. To this end, we define a constrained variant of the WBP that\nenforces the intermediate images to satisfy an image prior. We describe an\nalgorithm that solves this problem and demonstrate it using the sparse prior\nand generative adversarial networks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 21:26:40 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Simon", "Dror", ""], ["Aberdam", "Aviad", ""]]}, {"id": "1912.11566", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek\n  Hoiem", "title": "Boundary Cues for 3D Object Shape Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early work in computer vision considered a host of geometric cues for both\nshape reconstruction and recognition. However, since then, the vision community\nhas focused heavily on shading cues for reconstruction, and moved towards\ndata-driven approaches for recognition. In this paper, we reconsider these\nperhaps overlooked \"boundary\" cues (such as self occlusions and folds in a\nsurface), as well as many other established constraints for shape\nreconstruction. In a variety of user studies and quantitative tasks, we\nevaluate how well these cues inform shape reconstruction (relative to each\nother) in terms of both shape quality and shape recognition. Our findings\nsuggest many new directions for future research in shape reconstruction, such\nas automatic boundary cue detection and relaxing assumptions in shape from\nshading (e.g. orthographic projection, Lambertian surfaces).\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:40:30 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Karsch", "Kevin", ""], ["Liao", "Zicheng", ""], ["Rock", "Jason", ""], ["Barron", "Jonathan T.", ""], ["Hoiem", "Derek", ""]]}, {"id": "1912.11570", "submitter": "Alex Lamb", "authors": "Alex Lamb, Sherjil Ozair, Vikas Verma, David Ha", "title": "SketchTransfer: A Challenging New Task for Exploring Detail-Invariance\n  and the Abstractions Learned by Deep Networks", "comments": "Accepted WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have achieved excellent results in perceptual tasks, yet their\nability to generalize to variations not seen during training has come under\nincreasing scrutiny. In this work we focus on their ability to have invariance\ntowards the presence or absence of details. For example, humans are able to\nwatch cartoons, which are missing many visual details, without being explicitly\ntrained to do so. As another example, 3D rendering software is a relatively\nrecent development, yet people are able to understand such rendered scenes even\nthough they are missing details (consider a film like Toy Story). The failure\nof machine learning algorithms to do this indicates a significant gap in\ngeneralization between human abilities and the abilities of deep networks. We\npropose a dataset that will make it easier to study the detail-invariance\nproblem concretely. We produce a concrete task for this: SketchTransfer, and we\nshow that state-of-the-art domain transfer algorithms still struggle with this\ntask. The state-of-the-art technique which achieves over 95\\% on MNIST\n$\\xrightarrow{}$ SVHN transfer only achieves 59\\% accuracy on the\nSketchTransfer task, which is much better than random (11\\% accuracy) but falls\nshort of the 87\\% accuracy of a classifier trained directly on labeled\nsketches. This indicates that this task is approachable with today's best\nmethods but has substantial room for improvement.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 00:38:47 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lamb", "Alex", ""], ["Ozair", "Sherjil", ""], ["Verma", "Vikas", ""], ["Ha", "David", ""]]}, {"id": "1912.11597", "submitter": "Shin'ya Yamaguchi", "authors": "Shin'ya Yamaguchi, Sekitoshi Kanai, Takeharu Eda", "title": "Effective Data Augmentation with Multi-Domain Learning GANs", "comments": "AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deep learning applications, the massive data development (e.g.,\ncollecting, labeling), which is an essential process in building practical\napplications, still incurs seriously high costs. In this work, we propose an\neffective data augmentation method based on generative adversarial networks\n(GANs), called Domain Fusion. Our key idea is to import the knowledge contained\nin an outer dataset to a target model by using a multi-domain learning GAN. The\nmulti-domain learning GAN simultaneously learns the outer and target dataset\nand generates new samples for the target tasks. The simultaneous learning\nprocess makes GANs generate the target samples with high fidelity and variety.\nAs a result, we can obtain accurate models for the target tasks by using these\ngenerated samples even if we only have an extremely low volume target dataset.\nWe experimentally evaluate the advantages of Domain Fusion in image\nclassification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor\nScene Recognition. When trained on each target dataset reduced the samples to\n5,000 images, Domain Fusion achieves better classification accuracy than the\ndata augmentation using fine-tuned GANs. Furthermore, we show that Domain\nFusion improves the quality of generated samples, and the improvements can\ncontribute to higher accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 05:39:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yamaguchi", "Shin'ya", ""], ["Kanai", "Sekitoshi", ""], ["Eda", "Takeharu", ""]]}, {"id": "1912.11603", "submitter": "Shin'ya Yamaguchi", "authors": "Shin'ya Yamaguchi, Sekitoshi Kanai, Tetsuya Shioda, Shoichiro Takeda", "title": "Image Enhanced Rotation Prediction for Self-Supervised Learning", "comments": "Accepted to IEEE ICIP 2021. The title has been changed from \"Multiple\n  Pretext-Task for Self-Supervised Learning via Mixing Multiple Image\n  Transformations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rotation prediction (Rotation) is a simple pretext-task for\nself-supervised learning (SSL), where models learn useful representations for\ntarget vision tasks by solving pretext-tasks. Although Rotation captures\ninformation of object shapes, it hardly captures information of textures. To\ntackle this problem, we introduce a novel pretext-task called image enhanced\nrotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and\nanother pretext-task based on image enhancement (e.g., sharpening and\nsolarizing) while maintaining simplicity. Through the simultaneous prediction\nof rotation and image enhancement, models learn representations to capture the\ninformation of not only object shapes but also textures. Our experimental\nresults show that IE-Rot models outperform Rotation on various standard\nbenchmarks including ImageNet classification, PASCAL-VOC detection, and COCO\ndetection/segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:11:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 08:12:54 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yamaguchi", "Shin'ya", ""], ["Kanai", "Sekitoshi", ""], ["Shioda", "Tetsuya", ""], ["Takeda", "Shoichiro", ""]]}, {"id": "1912.11606", "submitter": "Shen Cai", "authors": "Hui Cao, Haikuan Du, Siyu Zhang, and Shen Cai", "title": "InSphereNet: a Concise Representation and Classification Method for 3D\n  Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an InSphereNet method for the problem of 3D object\nclassification. Unlike previous methods that use points, voxels, or multi-view\nimages as inputs of deep neural network (DNN), the proposed method constructs a\nclass of more representative features named infilling spheres from signed\ndistance field (SDF). Because of the admirable spatial representation of\ninfilling spheres, we can not only utilize very fewer number of spheres to\naccomplish classification task, but also design a lightweight InSphereNet with\nless layers and parameters than previous methods. Experiments on ModelNet40\nshow that the proposed method leads to superior performance than PointNet and\nPointNet++ in accuracy. In particular, if there are only a few dozen sphere\ninputs or about 100000 DNN parameters, the accuracy of our method remains at a\nvery high level (over 88%). This further validates the conciseness and\neffectiveness of the proposed InSphere 3D representation. Keywords: 3D object\nclassification , signed distance field , deep learning , infilling sphere\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:26:20 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 01:48:16 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Cao", "Hui", ""], ["Du", "Haikuan", ""], ["Zhang", "Siyu", ""], ["Cai", "Shen", ""]]}, {"id": "1912.11616", "submitter": "Bin Liu", "authors": "Bin Liu, Xiuping Liu, Zhixin Yang, Charlie C.L. Wang", "title": "Concise and Effective Network for 3D Human Modeling from Orthogonal\n  Silhouettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the problem of 3D human modeling from two\northogonal silhouettes of individuals (i.e., front and side views). Different\nfrom our prior work {\\cite{wang2003virtual}}, a supervised learning approach\nbased on \\textit{convolutional neural network} (CNN) is investigated to solve\nthe problem by establishing a mapping function that can effectively extract\nfeatures from two silhouettes and fuse them into coefficients in the shape\nspace of human bodies. A new CNN structure is proposed in our work to exact not\nonly the discriminative features of front and side views and also their mixed\nfeatures for the mapping function. 3D human models with high accuracy are\nsynthesized from coefficients generated by the mapping function. Existing CNN\napproaches for 3D human modeling usually learn a large number of parameters\n(from {8.5M} to {355.4M}) from two binary images. Differently, we investigate a\nnew network architecture and conduct the samples on silhouettes as input. As a\nconsequence, more accurate models can be generated by our network with only\n{2.4M} coefficients. The training of our network is conducted on samples\nobtained by augmenting a publicly accessible dataset. Learning transfer by\nusing datasets with a smaller number of scanned models is applied to our\nnetwork to enable the function of generating results with gender-oriented (or\ngeographical) patterns.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 08:05:37 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 02:52:13 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liu", "Bin", ""], ["Liu", "Xiuping", ""], ["Yang", "Zhixin", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "1912.11619", "submitter": "Qijie Wei", "authors": "Qijie Wei, Xirong Li, Weihong Yu, Xiao Zhang, Yongpeng Zhang, Bojie\n  Hu, Bin Mo, Di Gong, Ning Chen, Dayong Ding, Youxin Chen", "title": "Learn to Segment Retinal Lesions and Beyond", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards automated retinal screening, this paper makes an endeavor to\nsimultaneously achieve pixel-level retinal lesion segmentation and image-level\ndisease classification. Such a multi-task approach is crucial for accurate and\nclinically interpretable disease diagnosis. Prior art is insufficient due to\nthree challenges, i.e., lesions lacking objective boundaries, clinical\nimportance of lesions irrelevant to their size, and the lack of one-to-one\ncorrespondence between lesion and disease classes. This paper attacks the three\nchallenges in the context of diabetic retinopathy (DR) grading. We propose\nLesion-Net, a new variant of fully convolutional networks, with its expansive\npath re-designed to tackle the first challenge. A dual Dice loss that leverages\nboth semantic segmentation and image classification losses is introduced to\nresolve the second challenge. Lastly, we build a multi-task network that\nemploys Lesion-Net as a side-attention branch for both DR grading and result\ninterpretation. A set of 12K fundus images is manually segmented by 45\nophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in\ntotal. Extensive experiments on this large-scale dataset show that our proposed\napproach surpasses the prior art for multiple tasks including lesion\nsegmentation, lesion classification and DR grading\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 08:14:04 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 01:44:53 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 03:43:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wei", "Qijie", ""], ["Li", "Xirong", ""], ["Yu", "Weihong", ""], ["Zhang", "Xiao", ""], ["Zhang", "Yongpeng", ""], ["Hu", "Bojie", ""], ["Mo", "Bin", ""], ["Gong", "Di", ""], ["Chen", "Ning", ""], ["Ding", "Dayong", ""], ["Chen", "Youxin", ""]]}, {"id": "1912.11630", "submitter": "Zhiguang Zhang", "authors": "Zhiguang Zhang", "title": "Ranking and Classification driven Feature Learning for Person\n  Re_identification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 10:03:13 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhang", "Zhiguang", ""]]}, {"id": "1912.11642", "submitter": "Ke Zhang", "authors": "Ke Zhang, Xinsheng Wang, Yurong Guo, Dongliang Chang, Zhenbing Zhao,\n  Zhanyu Ma and Tony X.Han", "title": "Competing Ratio Loss for Discriminative Multi-class Image Classification", "comments": "Submitted to TIP. arXiv admin note: substantial text overlap with\n  arXiv:1907.13349", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. Many image\nclassification studies use deep convolutional neural network and focus on\nmodifying the network structure to improve image classification performance.\nConversely, our study focuses on loss function design. Cross-entropy Loss (CEL)\nhas been widely used for training deep convolutional neural network for the\ntask of multi-class classification. Although CEL has been successfully\nimplemented in several image classification tasks, it only focuses on the\nposterior probability of the correct class. For this reason, a negative log\nlikelihood ratio loss (NLLR) was proposed to better differentiate between the\ncorrect class and the competing incorrect ones. However, during the training of\nthe deep convolutional neural network, the value of NLLR is not always positive\nor negative, which severely affects the convergence of NLLR. Our proposed\ncompeting ratio loss (CRL) calculates the posterior probability ratio between\nthe correct class and the competing incorrect classes to further enlarge the\nprobability difference between the correct and incorrect classes. We added\nhyperparameters to CRL, thereby ensuring its value to be positive and that the\nupdate size of backpropagation is suitable for the CRL's fast convergence. To\ndemonstrate the performance of CRL, we conducted experiments on general image\nclassification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image\nclassification tasks (CUB200-2011 and Stanford Car), and the challenging face\nage estimation task (using Adience). Experimental results show the\neffectiveness and robustness of the proposed loss function on different deep\nconvolutional neural network architectures and different image classification\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 11:38:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhang", "Ke", ""], ["Wang", "Xinsheng", ""], ["Guo", "Yurong", ""], ["Chang", "Dongliang", ""], ["Zhao", "Zhenbing", ""], ["Ma", "Zhanyu", ""], ["Han", "Tony X.", ""]]}, {"id": "1912.11651", "submitter": "Kanchana Ranasinghe Mr", "authors": "Kanchana Ranasinghe, Sahan Liyanaarachchi, Harsha Ranasinghe and\n  Mayuka Jayawardhana", "title": "Extending Multi-Object Tracking systems to better exploit appearance and\n  3D information", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking multiple objects in real time is essential for a variety of\nreal-world applications, with self-driving industry being at the foremost. This\nwork involves exploiting temporally varying appearance and motion information\nfor tracking. Siamese networks have recently become highly successful at\nappearance based single object tracking and Recurrent Neural Networks have\nstarted dominating both motion and appearance based tracking. Our work focuses\non combining Siamese networks and RNNs to exploit appearance and motion\ninformation respectively to build a joint system capable of real time\nmulti-object tracking. We further explore heuristics based constraints for\ntracking in the Birds Eye View Space for efficiently exploiting 3D information\nas a constrained optimization problem for track prediction.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:16:38 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ranasinghe", "Kanchana", ""], ["Liyanaarachchi", "Sahan", ""], ["Ranasinghe", "Harsha", ""], ["Jayawardhana", "Mayuka", ""]]}, {"id": "1912.11658", "submitter": "Ilya Vasilev", "authors": "Ilia Zharikov, Filipp Nikitin, Ilia Vasiliev and Vladimir Dokholyan\n  (Moscow Institute of Physics and Technology)", "title": "DDI-100: Dataset for Text Detection and Recognition", "comments": "Accepted by CCVPR 2019. Dataset is available here:\n  https://github.com/machine-intelligence-laboratory/DDI-100", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays document analysis and recognition remain challenging tasks. However,\nonly a few datasets designed for text detection (TD) and optical character\nrecognition (OCR) problems exist. In this paper we present Distorted Document\nImages dataset (DDI-100) and demonstrate its usefulness in a wide range of\ndocument analysis problems. DDI-100 dataset is a synthetic dataset based on\n7000 real unique document pages and consists of more than 100000 augmented\nimages. Ground truth comprises text and stamp masks, text and characters\nbounding boxes with relevant annotations. Validation of DDI-100 dataset was\nconducted using several TD and OCR models that show high-quality performance on\nreal data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:47:35 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zharikov", "Ilia", "", "Moscow Institute of Physics and Technology"], ["Nikitin", "Filipp", "", "Moscow Institute of Physics and Technology"], ["Vasiliev", "Ilia", "", "Moscow Institute of Physics and Technology"], ["Dokholyan", "Vladimir", "", "Moscow Institute of Physics and Technology"]]}, {"id": "1912.11659", "submitter": "Rohan Mahadev", "authors": "Rohan Mahadev and Hongyu Lu", "title": "Improving Visual Recognition using Ambient Sound for Supervision", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our brains combine vision and hearing to create a more elaborate\ninterpretation of the world. When the visual input is insufficient, a rich\npanoply of sounds can be used to describe our surroundings. Since more than\n1,000 hours of videos are uploaded to the internet everyday, it is arduous, if\nnot impossible, to manually annotate these videos. Therefore, incorporating\naudio along with visual data without annotations is crucial for leveraging this\nexplosion of data for recognizing and understanding objects and scenes.\nOwens,et.al suggest that a rich representation of the physical world can be\nlearned by using a convolutional neural network to predict sound textures\nassociated with a given video frame. We attempt to reproduce the claims from\ntheir experiments, of which the code is not publicly available. In addition, we\npropose improvements in the pretext task that result in better performance in\nother downstream computer vision tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:48:53 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mahadev", "Rohan", ""], ["Lu", "Hongyu", ""]]}, {"id": "1912.11660", "submitter": "Yu Li", "authors": "Yu Li, Sheng Tang, Rui Zhang, Yongdong Zhang, Jintao Li, Shuicheng Yan", "title": "Asymmetric GAN for Unpaired Image-to-image Translation", "comments": "Accepted by IEEE Transactions on Image Processing (TIP) 2019", "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2922854", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image translation problem aims to model the mapping from\none domain to another with unpaired training data. Current works like the\nwell-acknowledged Cycle GAN provide a general solution for any two domains\nthrough modeling injective mappings with a symmetric structure. While in\nsituations where two domains are asymmetric in complexity, i.e., the amount of\ninformation between two domains is different, these approaches pose problems of\npoor generation quality, mapping ambiguity, and model sensitivity. To address\nthese issues, we propose Asymmetric GAN (AsymGAN) to adapt the asymmetric\ndomains by introducing an auxiliary variable (aux) to learn the extra\ninformation for transferring from the information-poor domain to the\ninformation-rich domain, which improves the performance of state-of-the-art\napproaches in the following ways. First, aux better balances the information\nbetween two domains which benefits the quality of generation. Second, the\nimbalance of information commonly leads to mapping ambiguity, where we are able\nto model one-to-many mappings by tuning aux, and furthermore, our aux is\ncontrollable. Third, the training of Cycle GAN can easily make the generator\npair sensitive to small disturbances and variations while our model decouples\nthe ill-conditioned relevance of generators by injecting aux during training.\nWe verify the effectiveness of our proposed method both qualitatively and\nquantitatively on asymmetric situation, label-photo task, on Cityscapes and\nHelen datasets, and show many applications of asymmetric image translations. In\nconclusion, our AsymGAN provides a better solution for unpaired image-to-image\ntranslation in asymmetric domains.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:49:41 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Li", "Yu", ""], ["Tang", "Sheng", ""], ["Zhang", "Rui", ""], ["Zhang", "Yongdong", ""], ["Li", "Jintao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1912.11676", "submitter": "Sajjad Mozaffari", "authors": "Sajjad Mozaffari, Omar Y. Al-Jarrah, Mehrdad Dianati, Paul Jennings,\n  and Alexandros Mouzakitis", "title": "Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving\n  Applications: A Review", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2020.3012034", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviour prediction function of an autonomous vehicle predicts the future\nstates of the nearby vehicles based on the current and past observations of the\nsurrounding environment. This helps enhance their awareness of the imminent\nhazards. However, conventional behaviour prediction solutions are applicable in\nsimple driving scenarios that require short prediction horizons. Most recently,\ndeep learning-based approaches have become popular due to their superior\nperformance in more complex environments compared to the conventional\napproaches. Motivated by this increased popularity, we provide a comprehensive\nreview of the state-of-the-art of deep learning-based approaches for vehicle\nbehaviour prediction in this paper. We firstly give an overview of the generic\nproblem of vehicle behaviour prediction and discuss its challenges, followed by\nclassification and review of the most recent deep learning-based solutions\nbased on three criteria: input representation, output type, and prediction\nmethod. The paper also discusses the performance of several well-known\nsolutions, identifies the research gaps in the literature and outlines\npotential new research directions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 14:22:41 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:52:45 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Mozaffari", "Sajjad", ""], ["Al-Jarrah", "Omar Y.", ""], ["Dianati", "Mehrdad", ""], ["Jennings", "Paul", ""], ["Mouzakitis", "Alexandros", ""]]}, {"id": "1912.11683", "submitter": "Rafael Valle", "authors": "Rafael Valle, Fitsum Reda, Mohammad Shoeybi, Patrick Legresley, Andrew\n  Tao, Bryan Catanzaro", "title": "Neural ODEs for Image Segmentation with Level Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for image segmentation that combines Neural\nOrdinary Differential Equations (NODEs) and the Level Set method. Our approach\nparametrizes the evolution of an initial contour with a NODE that implicitly\nlearns from data a speed function describing the evolution. In addition, for\ncases where an initial contour is not available and to alleviate the need for\ncareful choice or design of contour embedding functions, we propose a\nNODE-based method that evolves an image embedding into a dense per-pixel\nsemantic label space. We evaluate our methods on kidney segmentation (KiTS19)\nand on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to\nimproving initial contours provided by deep learning models while using a\nfraction of their number of parameters, our approach achieves F scores that are\nhigher than several state-of-the-art deep learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 15:02:24 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Valle", "Rafael", ""], ["Reda", "Fitsum", ""], ["Shoeybi", "Mohammad", ""], ["Legresley", "Patrick", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1912.11684", "submitter": "Chuang Gan", "authors": "Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, Joshua B. Tenenbaum", "title": "Look, Listen, and Act: Towards Audio-Visual Embodied Navigation", "comments": "Accepted by ICRA 2020. Project page: http://avn.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial ability of mobile intelligent agents is to integrate the evidence\nfrom multiple sensory inputs in an environment and to make a sequence of\nactions to reach their goals. In this paper, we attempt to approach the problem\nof Audio-Visual Embodied Navigation, the task of planning the shortest path\nfrom a random starting location in a scene to the sound source in an indoor\nenvironment, given only raw egocentric visual and audio sensory data. To\naccomplish this task, the agent is required to learn from various modalities,\ni.e. relating the audio signal to the visual environment. Here we describe an\napproach to audio-visual embodied navigation that takes advantage of both\nvisual and audio pieces of evidence. Our solution is based on three key ideas:\na visual perception mapper module that constructs its spatial memory of the\nenvironment, a sound perception module that infers the relative location of the\nsound source from the agent, and a dynamic path planner that plans a sequence\nof actions based on the audio-visual observations and the spatial memory of the\nenvironment to navigate toward the goal. Experimental results on a newly\ncollected Visual-Audio-Room dataset using the simulated multi-modal environment\ndemonstrate the effectiveness of our approach over several competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 15:07:26 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 00:18:49 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Gan", "Chuang", ""], ["Zhang", "Yiwei", ""], ["Wu", "Jiajun", ""], ["Gong", "Boqing", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1912.11691", "submitter": "Fahimeh Fooladgar", "authors": "Fahimeh Fooladgar and Shohreh Kasaei", "title": "Multi-Modal Attention-based Fusion Model for Semantic Segmentation of\n  RGB-Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D scene understanding is mainly considered as a crucial requirement in\ncomputer vision and robotics applications. One of the high-level tasks in 3D\nscene understanding is semantic segmentation of RGB-Depth images. With the\navailability of RGB-D cameras, it is desired to improve the accuracy of the\nscene understanding process by exploiting the depth features along with the\nappearance features. As depth images are independent of illumination, they can\nimprove the quality of semantic labeling alongside RGB images. Consideration of\nboth common and specific features of these two modalities improves the\nperformance of semantic segmentation. One of the main problems in RGB-Depth\nsemantic segmentation is how to fuse or combine these two modalities to achieve\nmore advantages of each modality while being computationally efficient.\nRecently, the methods that encounter deep convolutional neural networks have\nreached the state-of-the-art results by early, late, and middle fusion\nstrategies. In this paper, an efficient encoder-decoder model with the\nattention-based fusion block is proposed to integrate mutual influences between\nfeature maps of these two modalities. This block explicitly extracts the\ninterdependences among concatenated feature maps of these modalities to exploit\nmore powerful feature maps from RGB-Depth images. The extensive experimental\nresults on three main challenging datasets of NYU-V2, SUN RGB-D, and Stanford\n2D-3D-Semantic show that the proposed network outperforms the state-of-the-art\nmodels with respect to computational cost as well as model size. Experimental\nresults also illustrate the effectiveness of the proposed lightweight\nattention-based fusion model in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 16:53:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fooladgar", "Fahimeh", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1912.11695", "submitter": "Siming Yan", "authors": "Zhenpei Yang, Siming Yan, Qixing Huang", "title": "Extreme Relative Pose Network under Hybrid Representations", "comments": "Accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel RGB-D based relative pose estimation\napproach that is suitable for small-overlapping or non-overlapping scans and\ncan output multiple relative poses. Our method performs scene completion and\nmatches the completed scans. However, instead of using a fixed representation\nfor completion, the key idea is to utilize hybrid representations that combine\n360-image, 2D image-based layout, and planar patches. This approach offers\nadaptively feature representations for relative pose estimation. Besides, we\nintroduce a global-2-local matching procedure, which utilizes initial relative\nposes obtained during the global phase to detect and then integrate geometric\nrelations for pose refinement. Experimental results justify the potential of\nthis approach across a wide range of benchmark datasets. For example, on\nScanNet, the rotation translation errors of the top-1/top-5 predictions of our\napproach are 28.6/0.90m and 16.8/0.76m, respectively. Our approach also\nconsiderably boosts the performance of multi-scan reconstruction in few-view\nreconstruction settings.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 17:12:52 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 18:14:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yang", "Zhenpei", ""], ["Yan", "Siming", ""], ["Huang", "Qixing", ""]]}, {"id": "1912.11711", "submitter": "Yijun Li", "authors": "Yijun Li, Lu Jiang, Ming-Hsuan Yang", "title": "Controllable and Progressive Image Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image extrapolation aims at expanding the narrow field of view of a given\nimage patch. Existing models mainly deal with natural scene images of\nhomogeneous regions and have no control of the content generation process. In\nthis work, we study conditional image extrapolation to synthesize new images\nguided by the input structured text. The text is represented as a graph to\nspecify the objects and their spatial relation to the unknown regions of the\nimage. Inspired by drawing techniques, we propose a progressive generative\nmodel of three stages, i.e., generating a coarse bounding-boxes layout,\nrefining it to a finer segmentation layout, and mapping the layout to a\nrealistic output. Such a multi-stage design is shown to facilitate the training\nprocess and generate more controllable results. We validate the effectiveness\nof the proposed method on the face and human clothing dataset in terms of\nvisual results, quantitative evaluations and flexible controls.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 19:22:02 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Li", "Yijun", ""], ["Jiang", "Lu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1912.11721", "submitter": "Tempestt Neal", "authors": "Md A. Noor, G. Kaptan, V. Cherukupally, P. Gera, T. Neal", "title": "A Closer Look at Mobile App Usage as a Persistent Biometric: A Small\n  Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore mobile app use as a behavioral biometric\nidentifier. While several efforts have also taken on this challenge, many have\nalluded to the inconsistency in human behavior, resulting in updating the\nbiometric template frequently and periodically. Here, we represent app usage as\nsimple images wherein each pixel value provides some information about the\nuser's app usage. Then, we feed use these images to train a deep learning\nnetwork (convolutional neural net) to classify the user's identity. Our\ncontribution lies in the random order in which the images are fed to the\nclassifier, thereby presenting novel evidence that there are some aspects of\napp usage that are indeed persistent. Our results yield a 96.8% $F$-score\nwithout any updates to the template data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 22:11:24 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Noor", "Md A.", ""], ["Kaptan", "G.", ""], ["Cherukupally", "V.", ""], ["Gera", "P.", ""], ["Neal", "T.", ""]]}, {"id": "1912.11744", "submitter": "Qingshan Xu", "authors": "Qingshan Xu and Wenbing Tao", "title": "Planar Prior Assisted PatchMatch Multi-View Stereo", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The completeness of 3D models is still a challenging problem in multi-view\nstereo (MVS) due to the unreliable photometric consistency in low-textured\nareas. Since low-textured areas usually exhibit strong planarity, planar models\nare advantageous to the depth estimation of low-textured areas. On the other\nhand, PatchMatch multi-view stereo is very efficient for its sampling and\npropagation scheme. By taking advantage of planar models and PatchMatch\nmulti-view stereo, we propose a planar prior assisted PatchMatch multi-view\nstereo framework in this paper. In detail, we utilize a probabilistic graphical\nmodel to embed planar models into PatchMatch multi-view stereo and contribute a\nnovel multi-view aggregated matching cost. This novel cost takes both\nphotometric consistency and planar compatibility into consideration, making it\nsuited for the depth estimation of both non-planar and planar regions.\nExperimental results demonstrate that our method can efficiently recover the\ndepth information of extremely low-textured areas, thus obtaining high complete\n3D models and achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 01:34:05 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Xu", "Qingshan", ""], ["Tao", "Wenbing", ""]]}, {"id": "1912.11746", "submitter": "Qingshan Xu", "authors": "Qingshan Xu and Wenbing Tao", "title": "Learning Inverse Depth Regression for Multi-View Stereo with Correlation\n  Cost Volume", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown to be effective for depth inference in multi-view\nstereo (MVS). However, the scalability and accuracy still remain an open\nproblem in this domain. This can be attributed to the memory-consuming cost\nvolume representation and inappropriate depth inference. Inspired by the\ngroup-wise correlation in stereo matching, we propose an average group-wise\ncorrelation similarity measure to construct a lightweight cost volume. This can\nnot only reduce the memory consumption but also reduce the computational burden\nin the cost volume filtering. Based on our effective cost volume\nrepresentation, we propose a cascade 3D U-Net module to regularize the cost\nvolume to further boost the performance. Unlike the previous methods that treat\nmulti-view depth inference as a depth regression problem or an inverse depth\nclassification problem, we recast multi-view depth inference as an inverse\ndepth regression task. This allows our network to achieve sub-pixel estimation\nand be applicable to large-scale scenes. Through extensive experiments on DTU\ndataset and Tanks and Temples dataset, we show that our proposed network with\nCorrelation cost volume and Inverse DEpth Regression (CIDER), achieves\nstate-of-the-art results, demonstrating its superior performance on scalability\nand accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 01:40:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Xu", "Qingshan", ""], ["Tao", "Wenbing", ""]]}, {"id": "1912.11774", "submitter": "Delong Zhu Mr.", "authors": "Delong Zhu, Jianbang Liu, Nachuan Ma, Zhe Min, and Max Q.-H. Meng", "title": "Autonomous Removal of Perspective Distortion for Robotic Elevator Button\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elevator button recognition is considered an indispensable function for\nenabling the autonomous elevator operation of mobile robots. However, due to\nunfavorable image conditions and various image distortions, the recognition\naccuracy remains to be improved. In this paper, we present a novel algorithm\nthat can autonomously correct perspective distortions of elevator panel images.\nThe algorithm first leverages the Gaussian Mixture Model (GMM) to conduct a\ngrid fitting process based on button recognition results, then utilizes the\nestimated grid centers as reference features to estimate camera motions for\ncorrecting perspective distortions. The algorithm performs on a single image\nautonomously and does not need explicit feature detection or feature matching\nprocedure, which is much more robust to noises and outliers than traditional\nfeature-based geometric approaches. To verify the effectiveness of the\nalgorithm, we collect an elevator panel dataset of 50 images captured from\ndifferent angles of view. Experimental results show that the proposed algorithm\ncan accurately estimate camera motions and effectively remove perspective\ndistortions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 04:23:51 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhu", "Delong", ""], ["Liu", "Jianbang", ""], ["Ma", "Nachuan", ""], ["Min", "Zhe", ""], ["Meng", "Max Q. -H.", ""]]}, {"id": "1912.11803", "submitter": "Na Zhao", "authors": "Na Zhao, Tat-Seng Chua, Gim Hee Lee", "title": "SESS: Self-Ensembling Semi-Supervised 3D Object Detection", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of existing point cloud-based 3D object detection methods\nheavily relies on large-scale high-quality 3D annotations. However, such\nannotations are often tedious and expensive to collect. Semi-supervised\nlearning is a good alternative to mitigate the data annotation issue, but has\nremained largely unexplored in 3D object detection. Inspired by the recent\nsuccess of self-ensembling technique in semi-supervised image classification\ntask, we propose SESS, a self-ensembling semi-supervised 3D object detection\nframework. Specifically, we design a thorough perturbation scheme to enhance\ngeneralization of the network on unlabeled and new unseen data. Furthermore, we\npropose three consistency losses to enforce the consistency between two sets of\npredicted 3D object proposals, to facilitate the learning of structure and\nsemantic invariances of objects. Extensive experiments conducted on SUN RGB-D\nand ScanNet datasets demonstrate the effectiveness of SESS in both inductive\nand transductive semi-supervised 3D object detection. Our SESS achieves\ncompetitive performance compared to the state-of-the-art fully-supervised\nmethod by using only 50% labeled data. Our code is available at\nhttps://github.com/Na-Z/sess.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:48:04 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 07:01:15 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 14:32:55 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhao", "Na", ""], ["Chua", "Tat-Seng", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1912.11822", "submitter": "Hui Yuan", "authors": "Hui Yuan, Xiaoqian Hu, Junhui Hou, Xuekai Wei, and Sam Kwong", "title": "An Ensemble Rate Adaptation Framework for Dynamic Adaptive Streaming\n  Over HTTP", "comments": "This article has been accepted by IEEE Transactions on Broadcasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate adaptation is one of the most important issues in dynamic adaptive\nstreaming over HTTP (DASH). Due to the frequent fluctuations of the network\nbandwidth and complex variations of video content, it is difficult to deal with\nthe varying network conditions and video content perfectly by using a single\nrate adaptation method. In this paper, we propose an ensemble rate adaptation\nframework for DASH, which aims to leverage the advantages of multiple methods\ninvolved in the framework to improve the quality of experience (QoE) of users.\nThe proposed framework is simple yet very effective. Specifically, the proposed\nframework is composed of two modules, i.e., the method pool and method\ncontroller. In the method pool, several rate adap tation methods are\nintegrated. At each decision time, only the method that can achieve the best\nQoE is chosen to determine the bitrate of the requested video segment. Besides,\nwe also propose two strategies for switching methods, i.e., InstAnt Method\nSwitching, and InterMittent Method Switching, for the method controller to\ndetermine which method can provide the best QoEs. Simulation results\ndemonstrate that, the proposed framework always achieves the highest QoE for\nthe change of channel environment and video complexity, compared with\nstate-of-the-art rate adaptation methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 09:54:18 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yuan", "Hui", ""], ["Hu", "Xiaoqian", ""], ["Hou", "Junhui", ""], ["Wei", "Xuekai", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.11843", "submitter": "Coloma Ballester", "authors": "Pierrick Chatillon and Coloma Ballester", "title": "History-based Anomaly Detector: an Adversarial Approach to Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a difficult problem in many areas and has recently been\nsubject to a lot of attention. Classifying unseen data as anomalous is a\nchallenging matter. Latest proposed methods rely on Generative Adversarial\nNetworks (GANs) to estimate the normal data distribution, and produce an\nanomaly score prediction for any given data. In this article, we propose a\nsimple yet new adversarial method to tackle this problem, denoted as\nHistory-based anomaly detector (HistoryAD). It consists of a self-supervised\nmodel, trained to recognize 'normal' samples by comparing them to samples based\non the training history of a previously trained GAN. Quantitative and\nqualitative results are presented evaluating its performance. We also present a\ncomparison to several state-of-the-art methods for anomaly detection showing\nthat our proposal achieves top-tier results on several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 11:41:17 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 15:41:03 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chatillon", "Pierrick", ""], ["Ballester", "Coloma", ""]]}, {"id": "1912.11844", "submitter": "Matthieu Paul", "authors": "Matthieu Paul, Christoph Mayer, Luc Van Gool, Radu Timofte", "title": "Efficient Video Semantic Segmentation with Labels Propagation and\n  Refinement", "comments": "Accepted at WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of real-time semantic segmentation of high\ndefinition videos using a hybrid GPU / CPU approach. We propose an Efficient\nVideo Segmentation(EVS) pipeline that combines:\n  (i) On the CPU, a very fast optical flow method, that is used to exploit the\ntemporal aspect of the video and propagate semantic information from one frame\nto the next. It runs in parallel with the GPU.\n  (ii) On the GPU, two Convolutional Neural Networks: A main segmentation\nnetwork that is used to predict dense semantic labels from scratch, and a\nRefiner that is designed to improve predictions from previous frames with the\nhelp of a fast Inconsistencies Attention Module (IAM). The latter can identify\nregions that cannot be propagated accurately.\n  We suggest several operating points depending on the desired frame rate and\naccuracy. Our pipeline achieves accuracy levels competitive to the existing\nreal-time methods for semantic image segmentation(mIoU above 60%), while\nachieving much higher frame rates. On the popular Cityscapes dataset with high\nresolution frames (2048 x 1024), the proposed operating points range from 80 to\n1000 Hz on a single GPU and CPU.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 11:45:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Paul", "Matthieu", ""], ["Mayer", "Christoph", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "1912.11850", "submitter": "Amir Markovitz", "authors": "Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-Manor, Shai\n  Avidan", "title": "Graph Embedded Pose Clustering for Anomaly Detection", "comments": "Code is available at https://github.com/amirmk89/gepc. CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for anomaly detection of human actions. Our method\nworks directly on human pose graphs that can be computed from an input video\nsequence. This makes the analysis independent of nuisance parameters such as\nviewpoint or illumination. We map these graphs to a latent space and cluster\nthem. Each action is then represented by its soft-assignment to each of the\nclusters. This gives a kind of \"bag of words\" representation to the data, where\nevery action is represented by its similarity to a group of base action-words.\nThen, we use a Dirichlet process based mixture, that is useful for handling\nproportional data such as our soft-assignment vectors, to determine if an\naction is normal or not.\n  We evaluate our method on two types of data sets. The first is a fine-grained\nanomaly detection data set (e.g. ShanghaiTech) where we wish to detect unusual\nvariations of some action. The second is a coarse-grained anomaly detection\ndata set (e.g., a Kinetics-based data set) where few actions are considered\nnormal, and every other action should be considered abnormal.\n  Extensive experiments on the benchmarks show that our method performs\nconsiderably better than other state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:11:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 13:57:19 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Markovitz", "Amir", ""], ["Sharir", "Gilad", ""], ["Friedman", "Itamar", ""], ["Zelnik-Manor", "Lihi", ""], ["Avidan", "Shai", ""]]}, {"id": "1912.11852", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao,\n  Jun Zhu", "title": "Benchmarking Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which becomes\none of the most important research problems in the development of deep\nlearning. While a lot of efforts have been made in recent years, it is of great\nsignificance to perform correct and complete evaluations of the adversarial\nattack and defense algorithms. In this paper, we establish a comprehensive,\nrigorous, and coherent benchmark to evaluate adversarial robustness on image\nclassification tasks. After briefly reviewing plenty of representative attack\nand defense methods, we perform large-scale experiments with two robustness\ncurves as the fair-minded evaluation criteria to fully understand the\nperformance of these methods. Based on the evaluation results, we draw several\nimportant findings and provide insights for future research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:37:01 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Dong", "Yinpeng", ""], ["Fu", "Qi-An", ""], ["Yang", "Xiao", ""], ["Pang", "Tianyu", ""], ["Su", "Hang", ""], ["Xiao", "Zihao", ""], ["Zhu", "Jun", ""]]}, {"id": "1912.11853", "submitter": "Yosuke Shinya", "authors": "Laurent Dillard, Yosuke Shinya, Taiji Suzuki", "title": "Domain Adaptation Regularization for Spectral Pruning", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of computer vision related tasks. However, their\ncomputational cost limits their ability to be implemented in embedded systems\nwith restricted resources or strict latency constraints. Model compression has\ntherefore been an active field of research to overcome this issue.\nAdditionally, DNNs typically require massive amounts of labeled data to be\ntrained. This represents a second limitation to their deployment. Domain\nAdaptation (DA) addresses this issue by allowing knowledge learned on one\nlabeled source distribution to be transferred to a target distribution,\npossibly unlabeled. In this paper, we investigate on possible improvements of\ncompression methods in DA setting. We focus on a compression method that was\npreviously developed in the context of a single data distribution and show\nthat, with a careful choice of data to use during compression and additional\nregularization terms directly related to DA objectives, it is possible to\nimprove compression results. We also show that our method outperforms an\nexisting compression method studied in the DA setting by a large margin for\nhigh compression rates. Although our work is based on one specific compression\nmethod, we also outline some general guidelines for improving compression in DA\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:38:13 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 12:27:50 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 09:08:08 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dillard", "Laurent", ""], ["Shinya", "Yosuke", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1912.11856", "submitter": "Issam Hammad", "authors": "Issam Hammad, Kamal El-Sankary, and Jason Gu", "title": "A Comparative Study on Machine Learning Algorithms for the Control of a\n  Wall Following Robot", "comments": "Accepted and presented at IEEE International Conference on Robotics\n  and Biomimetics (ROBIO) -2019", "journal-ref": "IEEE International Conference on Robotics and Biomimetics (ROBIO)\n  2019", "doi": "10.1109/ROBIO49542.2019.8961836", "report-no": null, "categories": "cs.LG cs.CV cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comparison of the performance of various machine learning models to predict\nthe direction of a wall following robot is presented in this paper. The models\nwere trained using an open-source dataset that contains 24 ultrasound sensors\nreadings and the corresponding direction for each sample. This dataset was\ncaptured using SCITOS G5 mobile robot by placing the sensors on the robot\nwaist. In addition to the full format with 24 sensors per record, the dataset\nhas two simplified formats with 4 and 2 input sensor readings per record.\nSeveral control models were proposed previously for this dataset using all\nthree dataset formats. In this paper, two primary research contributions are\npresented. First, presenting machine learning models with accuracies higher\nthan all previously proposed models for this dataset using all three formats. A\nperfect solution for the 4 and 2 inputs sensors formats is presented using\nDecision Tree Classifier by achieving a mean accuracy of 100%. On the other\nhand, a mean accuracy of 99.82% was achieves using the 24 sensor inputs by\nemploying the Gradient Boost Classifier. Second, presenting a comparative study\non the performance of different machine learning and deep learning algorithms\non this dataset. Therefore, providing an overall insight on the performance of\nthese algorithms for similar sensor fusion problems. All the models in this\npaper were evaluated using Monte-Carlo cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:05:05 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 13:27:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hammad", "Issam", ""], ["El-Sankary", "Kamal", ""], ["Gu", "Jason", ""]]}, {"id": "1912.11861", "submitter": "Filippos Gouidis Mr.", "authors": "Filippos Gouidis, Alexandros Vassiliades, Theodore Patkos, Antonis\n  Argyros, Nick Bassiliades and Dimitris Plexousakis", "title": "A Review on Intelligent Object Perception Methods Combining\n  Knowledge-based Reasoning and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object perception is a fundamental sub-field of Computer Vision, covering a\nmultitude of individual areas and having contributed high-impact results. While\nMachine Learning has been traditionally applied to address related problems,\nrecent works also seek ways to integrate knowledge engineering in order to\nexpand the level of intelligence of the visual interpretation of objects, their\nproperties and their relations with their environment. In this paper, we\nattempt a systematic investigation of how knowledge-based methods contribute to\ndiverse object perception tasks. We review the latest achievements and identify\nprominent research directions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:26:49 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:50:43 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Gouidis", "Filippos", ""], ["Vassiliades", "Alexandros", ""], ["Patkos", "Theodore", ""], ["Argyros", "Antonis", ""], ["Bassiliades", "Nick", ""], ["Plexousakis", "Dimitris", ""]]}, {"id": "1912.11868", "submitter": "Nicolas Dobigeon", "authors": "Claire Guilloteau, Thomas Oberlin, Olivier Bern\\'e and Nicolas\n  Dobigeon", "title": "Hyperspectral and multispectral image fusion under spectrally varying\n  spatial blurs -- Application to high dimensional infrared astronomical\n  imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging has become a significant source of valuable data for\nastronomers over the past decades. Current instrumental and observing time\nconstraints allow direct acquisition of multispectral images, with high spatial\nbut low spectral resolution, and hyperspectral images, with low spatial but\nhigh spectral resolution. To enhance scientific interpretation of the data, we\npropose a data fusion method which combines the benefits of each image to\nrecover a high spatio-spectral resolution datacube. The proposed inverse\nproblem accounts for the specificities of astronomical instruments, such as\nspectrally variant blurs. We provide a fast implementation by solving the\nproblem in the frequency domain and in a low-dimensional subspace to\nefficiently handle the convolution operators as well as the high dimensionality\nof the data. We conduct experiments on a realistic synthetic dataset of\nsimulated observation of the upcoming James Webb Space Telescope, and we show\nthat our fusion algorithm outperforms state-of-the-art methods commonly used in\nremote sensing for Earth observation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:58:40 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Guilloteau", "Claire", ""], ["Oberlin", "Thomas", ""], ["Bern\u00e9", "Olivier", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1912.11872", "submitter": "Ting Yao", "authors": "Tao Mei, Wei Zhang, Ting Yao", "title": "Vision and Language: from Visual Perception to Content Creation", "comments": null, "journal-ref": "APSIPA Transactions on Signal and Information Processing 9 (2020)\n  e11", "doi": "10.1017/ATSIP.2020.10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and language are two fundamental capabilities of human intelligence.\nHumans routinely perform tasks through the interactions between vision and\nlanguage, supporting the uniquely human capacity to talk about what they see or\nhallucinate a picture on a natural-language description. The valid question of\nhow language interacts with vision motivates us researchers to expand the\nhorizons of computer vision area. In particular, \"vision to language\" is\nprobably one of the most popular topics in the past five years, with a\nsignificant growth in both volume of publications and extensive applications,\ne.g., captioning, visual question answering, visual dialog, language\nnavigation, etc. Such tasks boost visual perception with more comprehensive\nunderstanding and diverse linguistic representations. Going beyond the\nprogresses made in \"vision to language,\" language can also contribute to vision\nunderstanding and offer new possibilities of visual content creation, i.e.,\n\"language to vision.\" The process performs as a prism through which to create\nvisual content conditioning on the language inputs. This paper reviews the\nrecent advances along these two dimensions: \"vision to language\" and \"language\nto vision.\" More concretely, the former mainly focuses on the development of\nimage/video captioning, as well as typical encoder-decoder structures and\nbenchmarks, while the latter summarizes the technologies of visual content\ncreation. The real-world deployment or services of vision and language are\nelaborated as well.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 14:07:20 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Mei", "Tao", ""], ["Zhang", "Wei", ""], ["Yao", "Ting", ""]]}, {"id": "1912.11888", "submitter": "Ke Chen", "authors": "Zelin Xu, Ke Chen and Kui Jia", "title": "W-PoseNet: Dense Correspondence Regularized Pixel Pair Pose Regression", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving 6D pose estimation is non-trivial to cope with intrinsic appearance\nand shape variation and severe inter-object occlusion, and is made more\nchallenging in light of extrinsic large illumination changes and low quality of\nthe acquired data under an uncontrolled environment. This paper introduces a\nnovel pose estimation algorithm W-PoseNet, which densely regresses from input\ndata to 6D pose and also 3D coordinates in model space. In other words, local\nfeatures learned for pose regression in our deep network are regularized by\nexplicitly learning pixel-wise correspondence mapping onto 3D pose-sensitive\ncoordinates as an auxiliary task. Moreover, a sparse pair combination of\npixel-wise features and soft voting on pixel-pair pose predictions are designed\nto improve robustness to inconsistent and sparse local features. Experiment\nresults on the popular YCB-Video and LineMOD benchmarks show that the proposed\nW-PoseNet consistently achieves superior performance to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 15:51:29 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:22:26 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Xu", "Zelin", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""]]}, {"id": "1912.11891", "submitter": "Murari Mandal", "authors": "Murari Mandal, Vansh Dhar, Abhishek Mishra, Santosh Kumar Vipparthi", "title": "3DFR: A Swift 3D Feature Reductionist Framework for Scene Independent\n  Change Detection", "comments": "IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Process. Letters, vol. 26, no. 12, pp. 1882-1886, 2019", "doi": "10.1109/LSP.2019.2952253", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an end-to-end swift 3D feature reductionist\nframework (3DFR) for scene independent change detection. The 3DFR framework\nconsists of three feature streams: a swift 3D feature reductionist stream\n(AvFeat), a contemporary feature stream (ConFeat) and a temporal median feature\nmap. These multilateral foreground/background features are further refined\nthrough an encoder-decoder network. As a result, the proposed framework not\nonly detects temporal changes but also learns high-level appearance features.\nThus, it incorporates the object semantics for effective change detection.\nFurthermore, the proposed framework is validated through a scene independent\nevaluation scheme in order to demonstrate the robustness and generalization\ncapability of the network. The performance of the proposed method is evaluated\non the benchmark CDnet 2014 dataset. The experimental results show that the\nproposed 3DFR network outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 16:05:43 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mandal", "Murari", ""], ["Dhar", "Vansh", ""], ["Mishra", "Abhishek", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "1912.11903", "submitter": "Ajinkya Tejankar", "authors": "Ajinkya Tejankar and Hamed Pirsiavash", "title": "A simple baseline for domain adaptation using rotation prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, domain adaptation has become a hot research area with lots of\napplications. The goal is to adapt a model trained in one domain to another\ndomain with scarce annotated data. We propose a simple yet effective method\nbased on self-supervised learning that outperforms or is on par with most\nstate-of-the-art algorithms, e.g. adversarial domain adaptation. Our method\ninvolves two phases: predicting random rotations (self-supervised) on the\ntarget domain along with correct labels for the source domain (supervised), and\nthen using self-distillation on the target domain. Our simple method achieves\nstate-of-the-art results on semi-supervised domain adaptation on DomainNet\ndataset.\n  Further, we observe that the unlabeled target datasets of popular domain\nadaptation benchmarks do not contain any categories apart from testing\ncategories. We believe this introduces a bias that does not exist in many real\napplications. We show that removing this bias from the unlabeled data results\nin a large drop in performance of state-of-the-art methods, while our simple\nmethod is relatively robust.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 17:32:04 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Tejankar", "Ajinkya", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "1912.11913", "submitter": "Xiaolong Li", "authors": "Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran\n  Song", "title": "Category-Level Articulated Object Pose Estimation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project addresses the task of category-level pose estimation for\narticulated objects from a single depth image. We present a novel\ncategory-level approach that correctly accommodates object instances previously\nunseen during training. We introduce Articulation-aware Normalized Coordinate\nSpace Hierarchy (ANCSH) - a canonical representation for different articulated\nobjects in a given category. As the key to achieve intra-category\ngeneralization, the representation constructs a canonical object space as well\nas a set of canonical part spaces. The canonical object space normalizes the\nobject orientation,scales and articulations (e.g. joint parameters and states)\nwhile each canonical part space further normalizes its part pose and scale. We\ndevelop a deep network based on PointNet++ that predicts ANCSH from a single\ndepth point cloud, including part segmentation, normalized coordinates, and\njoint parameters in the canonical object space. By leveraging the canonicalized\njoints, we demonstrate: 1) improved performance in part pose and scale\nestimations using the induced kinematic constraints from joints; 2) high\naccuracy for joint parameter estimation in camera space.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 18:34:37 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 19:46:04 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Li", "Xiaolong", ""], ["Wang", "He", ""], ["Yi", "Li", ""], ["Guibas", "Leonidas", ""], ["Abbott", "A. Lynn", ""], ["Song", "Shuran", ""]]}, {"id": "1912.11932", "submitter": "Vijai Thottathil Jayadevan", "authors": "Vijai Jayadevan, Edward Delp, and Zygmunt Pizlo", "title": "Skeleton Extraction from 3D Point Clouds by Decomposing the Object into\n  Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing a point cloud into its components and extracting curve skeletons\nfrom point clouds are two related problems. Decomposition of a shape into its\ncomponents is often obtained as a byproduct of skeleton extraction. In this\nwork, we propose to extract curve skeletons, from unorganized point clouds, by\ndecomposing the object into its parts, identifying part skeletons and then\nlinking these part skeletons together to obtain the complete skeleton. We\nbelieve it is the most natural way to extract skeletons in the sense that this\nwould be the way a human would approach the problem. Our parts are generalized\ncylinders (GCs). Since, the axis of a GC is an integral part of its definition,\nthe parts have natural skeletal representations. We use translational symmetry,\nthe fundamental property of GCs, to extract parts from point clouds. We\ndemonstrate how this method can handle a large variety of shapes. We compare\nour method with state of the art methods and show how a part based approach can\ndeal with some of the limitations of other methods. We present an improved\nversion of an existing point set registration algorithm and demonstrate its\nutility in extracting parts from point clouds. We also show how this method can\nbe used to extract skeletons from and identify parts of noisy point clouds. A\npart based approach also provides a natural and intuitive interface for user\ninteraction. We demonstrate the ease with which mistakes, if any, can be fixed\nwith minimal user interaction with the help of a graphical user interface.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:52:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Jayadevan", "Vijai", ""], ["Delp", "Edward", ""], ["Pizlo", "Zygmunt", ""]]}, {"id": "1912.11947", "submitter": "Xinzi Sun", "authors": "Xinzi Sun, Pengfei Zhang, Dechun Wang, Yu Cao, Benyuan Liu", "title": "Colorectal Polyp Segmentation by U-Net with Dilation Convolution", "comments": "8 pages. ICMLA (International Conference on Machine Learning and\n  Applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) is one of the most commonly diagnosed cancers and a\nleading cause of cancer deaths in the United States. Colorectal polyps that\ngrow on the intima of the colon or rectum is an important precursor for CRC.\nCurrently, the most common way for colorectal polyp detection and precancerous\npathology is the colonoscopy. Therefore, accurate colorectal polyp segmentation\nduring the colonoscopy procedure has great clinical significance in CRC early\ndetection and prevention. In this paper, we propose a novel end-to-end deep\nlearning framework for the colorectal polyp segmentation. The model we design\nconsists of an encoder to extract multi-scale semantic features and a decoder\nto expand the feature maps to a polyp segmentation map. We improve the feature\nrepresentation ability of the encoder by introducing the dilated convolution to\nlearn high-level semantic features without resolution reduction. We further\ndesign a simplified decoder which combines multi-scale semantic features with\nfewer parameters than the traditional architecture. Furthermore, we apply three\npost processing techniques on the output segmentation map to improve colorectal\npolyp detection performance. Our method achieves state-of-the-art results on\nCVC-ClinicDB and ETIS-Larib Polyp DB.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 23:27:18 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sun", "Xinzi", ""], ["Zhang", "Pengfei", ""], ["Wang", "Dechun", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1912.11953", "submitter": "Seyed Vahid Mirnezami", "authors": "Seyed Vahid Mirnezami, Ali HamidiSepehr and Mahdi Ghaebi", "title": "Apricot variety classification using image processing and machine\n  learning approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apricot which is a cultivated type of Zerdali (wild apricot) has an important\nplace in human nutrition and its medical properties are essential for human\nhealth. The objective of this research was to obtain a model for apricot mass\nand separate apricot variety with image processing technology using external\nfeatures of apricot fruit. In this study, five verities of apricot were used.\nIn order to determine the size of the fruits, three mutually perpendicular axes\nwere defined, length, width, and thickness. Measurements show that the effect\nof variety on all properties was statistically significant at the 1%\nprobability level. Furthermore, there is no significant difference between the\nestimated dimensions by image processing approach and the actual dimensions.\nThe developed system consists of a digital camera, a light diffusion chamber, a\ndistance adjustment pedestal, and a personal computer. Images taken by the\ndigital camera were stored as (RGB) for further analysis. The images were taken\nfor a number of 49 samples of each cultivar in three directions. A linear\nequation is recommended to calculate the apricot mass based on the length and\nthe width with R 2 = 0.97. In addition, ANFIS model with C-means was the best\nmodel for classifying the apricot varieties based on the physical features\nincluding length, width, thickness, mass, and projected area of three\nperpendicular surfaces. The accuracy of the model was 87.7.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 00:51:53 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mirnezami", "Seyed Vahid", ""], ["HamidiSepehr", "Ali", ""], ["Ghaebi", "Mahdi", ""]]}, {"id": "1912.11954", "submitter": "Hui Yuan", "authors": "Hui Yuan, Huayong Fu, Ju Liu, Junhui Hou, and Sam Kwong", "title": "Non-Cooperative Game Theory Based Rate Adaptation for Dynamic Video\n  Streaming over HTTP", "comments": "This paper has been published on IEEE Transactions on Mobile\n  Computing. H. Yuan, H. Fu, J. Liu, J. Hou, and S. Kwong, \"Non-Cooperative\n  Game Theory Based Rate Adaptation for Dynamic Video Streaming over HTTP,\"\n  IEEE Transactions on Mobile Computing, vol.17, no.10, pp. 2334-2348, Oct.\n  2018", "journal-ref": "IEEE Transactions on Mobile Computing, vol.17, no.10, pp.\n  2334-2348, Oct. 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Adaptive Streaming over HTTP (DASH) has demonstrated to be an\nemerging and promising multimedia streaming technique, owing to its capability\nof dealing with the variability of networks. Rate adaptation mechanism, a\nchallenging and open issue, plays an important role in DASH based systems since\nit affects Quality of Experience (QoE) of users, network utilization, etc. In\nthis paper, based on non-cooperative game theory, we propose a novel algorithm\nto optimally allocate the limited export bandwidth of the server to multi-users\nto maximize their QoE with fairness guaranteed. The proposed algorithm is\nproxy-free. Specifically, a novel user QoE model is derived by taking a variety\nof factors into account, like the received video quality, the reference buffer\nlength, and user accumulated buffer lengths, etc. Then, the bandwidth competing\nproblem is formulated as a non-cooperation game with the existence of Nash\nEquilibrium that is theoretically proven. Finally, a distributed iterative\nalgorithm with stability analysis is proposed to find the Nash Equilibrium.\nCompared with state-of-the-art methods, extensive experimental results in terms\nof both simulated and realistic networking scenarios demonstrate that the\nproposed algorithm can produce higher QoE, and the actual buffer lengths of all\nusers keep nearly optimal states, i.e., moving around the reference buffer all\nthe time. Besides, the proposed algorithm produces no playback interruption.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 01:19:14 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yuan", "Hui", ""], ["Fu", "Huayong", ""], ["Liu", "Ju", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.11960", "submitter": "Sravanti Addepalli", "authors": "Sravanti Addepalli, Gaurav Kumar Nayak, Anirban Chakraborty, R.\n  Venkatesh Babu", "title": "DeGAN : Data-Enriching GAN for Retrieving Representative Samples from a\n  Trained Classifier", "comments": "Accepted at AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of digital information explosion, an abundance of data from\nnumerous modalities is being generated as well as archived everyday. However,\nmost problems associated with training Deep Neural Networks still revolve\naround lack of data that is rich enough for a given task. Data is required not\nonly for training an initial model, but also for future learning tasks such as\nModel Compression and Incremental Learning. A diverse dataset may be used for\ntraining an initial model, but it may not be feasible to store it throughout\nthe product life cycle due to data privacy issues or memory constraints. We\npropose to bridge the gap between the abundance of available data and lack of\nrelevant data, for the future learning tasks of a given trained network. We use\nthe available data, that may be an imbalanced subset of the original training\ndataset, or a related domain dataset, to retrieve representative samples from a\ntrained classifier, using a novel Data-enriching GAN (DeGAN) framework. We\ndemonstrate that data from a related domain can be leveraged to achieve\nstate-of-the-art performance for the tasks of Data-free Knowledge Distillation\nand Incremental Learning on benchmark datasets. We further demonstrate that our\nproposed framework can enrich any data, even from unrelated domains, to make it\nmore useful for the future learning tasks of a given network.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:05:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Addepalli", "Sravanti", ""], ["Nayak", "Gaurav Kumar", ""], ["Chakraborty", "Anirban", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1912.11966", "submitter": "Endre Grovik", "authors": "Endre Gr{\\o}vik, Darvin Yi, Michael Iv, Elizabeth Tong, Line Brennhaug\n  Nilsen, Anna Latysheva, Cathrine Saxhaug, Kari Dolven Jacobsen, {\\AA}slaug\n  Helland, Kyrre Eeg Emblem, Daniel Rubin, Greg Zaharchuk", "title": "Handling Missing MRI Input Data in Deep Learning Segmentation of Brain\n  Metastases: A Multi-Center Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose was to assess the clinical value of a novel DropOut model for\ndetecting and segmenting brain metastases, in which a neural network is trained\non four distinct MRI sequences using an input dropout layer, thus simulating\nthe scenario of missing MRI data by training on the full set and all possible\nsubsets of the input data. This retrospective, multi-center study, evaluated\n165 patients with brain metastases. A deep learning based segmentation model\nfor automatic segmentation of brain metastases, named DropOut, was trained on\nmulti-sequence MRI from 100 patients, and validated/tested on 10/55 patients.\nThe segmentation results were compared with the performance of a\nstate-of-the-art DeepLabV3 model. The MR sequences in the training set included\npre- and post-gadolinium (Gd) T1-weighted 3D fast spin echo, post-Gd\nT1-weighted inversion recovery (IR) prepped fast spoiled gradient echo, and 3D\nfluid attenuated inversion recovery (FLAIR), whereas the test set did not\ninclude the IR prepped image-series. The ground truth were established by\nexperienced neuroradiologists. The results were evaluated using precision,\nrecall, Dice score, and receiver operating characteristics (ROC) curve\nstatistics, while the Wilcoxon rank sum test was used to compare the\nperformance of the two neural networks. The area under the ROC curve (AUC),\naveraged across all test cases, was 0.989+-0.029 for the DropOut model and\n0.989+-0.023 for the DeepLabV3 model (p=0.62). The DropOut model showed a\nsignificantly higher Dice score compared to the DeepLabV3 model (0.795+-0.105\nvs. 0.774+-0.104, p=0.017), and a significantly lower average false positive\nrate of 3.6/patient vs. 7.0/patient (p<0.001) using a 10mm3 lesion-size limit.\nThe DropOut model may facilitate accurate detection and segmentation of brain\nmetastases on a multi-center basis, even when the test cohort is missing MRI\ninput data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:49:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gr\u00f8vik", "Endre", ""], ["Yi", "Darvin", ""], ["Iv", "Michael", ""], ["Tong", "Elizabeth", ""], ["Nilsen", "Line Brennhaug", ""], ["Latysheva", "Anna", ""], ["Saxhaug", "Cathrine", ""], ["Jacobsen", "Kari Dolven", ""], ["Helland", "\u00c5slaug", ""], ["Emblem", "Kyrre Eeg", ""], ["Rubin", "Daniel", ""], ["Zaharchuk", "Greg", ""]]}, {"id": "1912.11967", "submitter": "Zhaofu Diao", "authors": "Zhaofu Diao", "title": "A single target tracking algorithm based on Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the single target tracking field, occlusion leads to the loss of tracking\ntargets is a ubiquitous and arduous problem. To solve this problem, we propose\na single target tracking algorithm with anti-occlusion capability. The main\ncontent of our algorithm is to use the Region Proposal Network to obtain the\ntracked target and potential interferences, and use the occlusion awareness\nmodule to judge whether the interfering object occludes the target. If no\nocclusion occurs, continue tracking. If occlusion occurs, the prediction module\nis started, and the motion trajectory of the target in subsequent frames is\npredicted according to the motion trajectory before occlusion. The result\nobtained by the prediction module is used to replace the target position\nfeature obtained by the original tracking algorithm. So we solve the problem\nthat the occlusion causes the tracking algorithm to lose the target. In actual\nperformance, our algorithm can successfully track the target in the occluded\ndataset. On the VOT2018 dataset, our algorithm has an EAO of 0.421, an Accuracy\nof 0.67, and a Robustness of 0.186. Compared with SiamRPN ++, they increased by\n1.69%, 11.67% and 9.3%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:55:48 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Diao", "Zhaofu", ""]]}, {"id": "1912.11969", "submitter": "Haizhong Zheng", "authors": "Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, Atul Prakash", "title": "Efficient Adversarial Training with Transferable Adversarial Examples", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition 2020 (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is an effective defense method to protect classification\nmodels against adversarial attacks. However, one limitation of this approach is\nthat it can require orders of magnitude additional training time due to high\ncost of generating strong adversarial examples during training. In this paper,\nwe first show that there is high transferability between models from\nneighboring epochs in the same training process, i.e., adversarial examples\nfrom one epoch continue to be adversarial in subsequent epochs. Leveraging this\nproperty, we propose a novel method, Adversarial Training with Transferable\nAdversarial Examples (ATTA), that can enhance the robustness of trained models\nand greatly improve the training efficiency by accumulating adversarial\nperturbations through epochs. Compared to state-of-the-art adversarial training\nmethods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and\nrequires 12~14x less training time on MNIST and CIFAR10 datasets with\ncomparable model robustness.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:05:05 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:48:22 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zheng", "Haizhong", ""], ["Zhang", "Ziqi", ""], ["Gu", "Juncheng", ""], ["Lee", "Honglak", ""], ["Prakash", "Atul", ""]]}, {"id": "1912.11976", "submitter": "Chen Chao", "authors": "Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng, Xinyu\n  Jin, Xian-Sheng Hua", "title": "HoMM: Higher-order Moment Matching for Unsupervised Domain Adaptation", "comments": "Accept by AAAI-2020, codes are available at\n  https://github.com/chenchao666/HoMM-Master", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the discrepancy of feature distributions between different domains\nis one of the most promising directions in unsupervised domain adaptation. From\nthe perspective of distribution matching, most existing discrepancy-based\nmethods are designed to match the second-order or lower statistics, which\nhowever, have limited expression of statistical characteristic for non-Gaussian\ndistributions. In this work, we explore the benefits of using higher-order\nstatistics (mainly refer to third-order and fourth-order statistics) for domain\nmatching. We propose a Higher-order Moment Matching (HoMM) method, and further\nextend the HoMM into reproducing kernel Hilbert spaces (RKHS). In particular,\nour proposed HoMM can perform arbitrary-order moment tensor matching, we show\nthat the first-order HoMM is equivalent to Maximum Mean Discrepancy (MMD) and\nthe second-order HoMM is equivalent to Correlation Alignment (CORAL). Moreover,\nthe third-order and the fourth-order moment tensor matching are expected to\nperform comprehensive domain alignment as higher-order statistics can\napproximate more complex, non-Gaussian distributions. Besides, we also exploit\nthe pseudo-labeled target samples to learn discriminative representations in\nthe target domain, which further improves the transfer performance. Extensive\nexperiments are conducted, showing that our proposed HoMM consistently\noutperforms the existing moment matching methods by a large margin. Codes are\navailable at \\url{https://github.com/chenchao666/HoMM-Master}\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:53:03 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Chen", "Chao", ""], ["Fu", "Zhihang", ""], ["Chen", "Zhihong", ""], ["Jin", "Sheng", ""], ["Cheng", "Zhaowei", ""], ["Jin", "Xinyu", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1912.11985", "submitter": "Su-Jing Wang", "authors": "Ying He, Su-Jing Wang, Jingting Li, Moi Hoon Yap", "title": "Spotting Macro- and Micro-expression Intervals in Long Video Sequences", "comments": "7 pages, 4 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents baseline results for the Third Facial Micro-Expression\nGrand Challenge (MEGC 2020). Both macro- and micro-expression intervals in\nCAS(ME)$^2$ and SAMM Long Videos are spotted by employing the method of Main\nDirectional Maximal Difference Analysis (MDMD). The MDMD method uses the\nmagnitude maximal difference in the main direction of optical flow features to\nspot facial movements. The single-frame prediction results of the original MDMD\nmethod are post-processed into reasonable video intervals. The metric F1-scores\nof baseline results are evaluated: for CAS(ME)$^2$, the F1-scores are 0.1196\nand 0.0082 for macro- and micro-expressions respectively, and the overall\nF1-score is 0.0376; for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364\nfor macro- and micro-expressions respectively, and the overall F1-score is\n0.0445. The baseline project codes are publicly available at\nhttps://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 06:04:22 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 13:53:28 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 09:41:20 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["He", "Ying", ""], ["Wang", "Su-Jing", ""], ["Li", "Jingting", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1912.11986", "submitter": "Yibin Wu", "authors": "Yibin Wu", "title": "Formula Derivation and Analysis of the VINS-Mono", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VINS-Mono is a monocular visual-inertial 6 DOF state estimator proposed\nby Aerial Robotics Group of HKUST in 2017. It can be performed on MAVs,\nsmartphones and many other intelligent platforms. Because of the excellent\nrobustness, accuracy and scalability, it has gained extensive attention\nworldwide. In this manuscript, the main equations including IMU\npre-integration, visual-inertial co-initialization and tightly-coupled\nnonlinear optimization are derived and analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:34:02 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 05:16:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wu", "Yibin", ""]]}, {"id": "1912.11987", "submitter": "Evgeniy Martyushev", "authors": "E.V. Martyushev", "title": "Necessary and Sufficient Polynomial Constraints on Compatible Triplets\n  of Essential Matrices", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essential matrix incorporates relative rotation and translation\nparameters of two calibrated cameras. The well-known algebraic characterization\nof essential matrices, i.e. necessary and sufficient conditions under which an\narbitrary matrix (of rank two) becomes essential, consists of a unique matrix\nequation of degree three. Based on this equation, a number of efficient\nalgorithmic solutions to different relative pose estimation problems have been\nproposed. In three views, a possible way to describe the geometry of three\ncalibrated cameras comes from considering compatible triplets of essential\nmatrices. The compatibility is meant the correspondence of a triplet to a\ncertain configuration of calibrated cameras. The main goal of this paper is to\ngive an algebraic characterization of compatible triplets of essential\nmatrices. Specifically, we propose necessary and sufficient polynomial\nconstraints on a triplet of real rank-two essential matrices that ensure its\ncompatibility. The constraints are given in the form of six cubic matrix\nequations, one quartic and one sextic scalar equations. An important advantage\nof the proposed constraints is their sufficiency even in the case of cameras\nwith collinear centers. The applications of the constraints may include\nrelative camera pose estimation in three and more views, averaging of essential\nmatrices for incremental structure from motion, multiview camera\nauto-calibration, etc.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 18:03:00 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Martyushev", "E. V.", ""]]}, {"id": "1912.11995", "submitter": "Shadrokh Samavi", "authors": "Reihaneh Teymoori, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi", "title": "An Abstraction Model for Semantic Segmentation Algorithms", "comments": "6 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a process of classifying each pixel in the image.\nDue to its advantages, sematic segmentation is used in many tasks such as\ncancer detection, robot-assisted surgery, satellite image analysis,\nself-driving car control, etc. In this process, accuracy and efficiency are the\ntwo crucial goals for this purpose, and there are several state of the art\nneural networks. In each method, by employing different techniques, new\nsolutions have been presented for increasing efficiency, accuracy, and reducing\nthe costs. The diversity of the implemented approaches for semantic\nsegmentation makes it difficult for researches to achieve a comprehensive view\nof the field. To offer a comprehensive view, in this paper, an abstraction\nmodel for the task of semantic segmentation is offered. The proposed framework\nconsists of four general blocks that cover the majority of majority of methods\nthat have been proposed for semantic segmentation. We also compare different\napproaches and consider the importance of each part in the overall performance\nof a method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 05:39:24 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Teymoori", "Reihaneh", ""], ["Nabizadeh", "Zahra", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.12014", "submitter": "Pengcheng Yang", "authors": "Pengcheng Yang, Boxing Chen, Pei Zhang, Xu Sun", "title": "Visual Agreement Regularized Training for Multi-Modal Machine\n  Translation", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal machine translation aims at translating the source sentence into\na different language in the presence of the paired image. Previous work\nsuggests that additional visual information only provides dispensable help to\ntranslation, which is needed in several very special cases such as translating\nambiguous words. To make better use of visual information, this work presents\nvisual agreement regularized training. The proposed approach jointly trains the\nsource-to-target and target-to-source translation models and encourages them to\nshare the same focus on the visual information when generating semantically\nequivalent visual words (e.g. \"ball\" in English and \"ballon\" in French).\nBesides, a simple yet effective multi-head co-attention model is also\nintroduced to capture interactions between visual and textual features. The\nresults show that our approaches can outperform competitive baselines by a\nlarge margin on the Multi30k dataset. Further analysis demonstrates that the\nproposed regularized training can effectively improve the agreement of\nattention on the image, leading to better use of visual information.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 07:46:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Pengcheng", ""], ["Chen", "Boxing", ""], ["Zhang", "Pei", ""], ["Sun", "Xu", ""]]}, {"id": "1912.12027", "submitter": "Shadrokh Samavi", "authors": "Fateme Mostafaie, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi", "title": "A General Framework for Saliency Detection Methods", "comments": "5 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection is one of the most challenging problems in the fields of\nimage analysis and computer vision. Many approaches propose different\narchitectures based on the psychological and biological properties of the human\nvisual attention system. However, there is not still an abstract framework,\nwhich summarized the existed methods. In this paper, we offered a general\nframework for saliency models, which consists of five main steps:\npre-processing, feature extraction, saliency map generation, saliency map\ncombination, and post-processing. Also, we study different saliency models\ncontaining each level and compare their performance together. This framework\nhelps researchers to have a comprehensive view of studying new methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 08:49:53 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mostafaie", "Fateme", ""], ["Nabizadeh", "Zahra", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.12033", "submitter": "Qingyong Hu", "authors": "Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, Mohammed\n  Bennamoun", "title": "Deep Learning for 3D Point Clouds: A Survey", "comments": "Accepted by IEEE TPAMI. Project page:\n  https://github.com/QingyongHu/SoTA-Point-Cloud", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud learning has lately attracted increasing attention due to its\nwide applications in many areas, such as computer vision, autonomous driving,\nand robotics. As a dominating technique in AI, deep learning has been\nsuccessfully used to solve various 2D vision problems. However, deep learning\non point clouds is still in its infancy due to the unique challenges faced by\nthe processing of point clouds with deep neural networks. Recently, deep\nlearning on point clouds has become even thriving, with numerous methods being\nproposed to address different problems in this area. To stimulate future\nresearch, this paper presents a comprehensive review of recent progress in deep\nlearning methods for point clouds. It covers three major tasks, including 3D\nshape classification, 3D object detection and tracking, and 3D point cloud\nsegmentation. It also presents comparative results on several publicly\navailable datasets, together with insightful observations and inspiring future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 09:15:54 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 10:54:36 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Guo", "Yulan", ""], ["Wang", "Hanyun", ""], ["Hu", "Qingyong", ""], ["Liu", "Hao", ""], ["Liu", "Li", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1912.12044", "submitter": "He-Feng Yin", "authors": "Xiao-Yun Cai and He-Feng Yin", "title": "A sparsity augmented probabilistic collaborative representation based\n  classification method", "comments": "This manuscript has been accepted for publication by Journal of\n  Algorithms and Computational Technology (JACT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to enhance the performance of image recognition, a sparsity\naugmented probabilistic collaborative representation based classification\n(SA-ProCRC) method is presented. The proposed method obtains the dense\ncoefficient through ProCRC, then augments the dense coefficient with a sparse\none, and the sparse coefficient is attained by the orthogonal matching pursuit\n(OMP) algorithm. In contrast to conventional methods which require explicit\ncomputation of the reconstruction residuals for each class, the proposed method\nemploys the augmented coefficient and the label matrix of the training samples\nto classify the test sample. Experimental results indicate that the proposed\nmethod can achieve promising results for face and scene images. The source code\nof our proposed SA-ProCRC is accessible at\nhttps://github.com/yinhefeng/SAProCRC.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:06:20 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Cai", "Xiao-Yun", ""], ["Yin", "He-Feng", ""]]}, {"id": "1912.12082", "submitter": "Mobina Mahdavi", "authors": "Mobina Mahdavi, Fahimeh Fooladgar, Shohreh Kasaei", "title": "Pointwise Attention-Based Atrous Convolutional Neural Networks", "comments": "7 pages, 6 figures. Author one and author two contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid progress of deep convolutional neural networks, in almost all\nrobotic applications, the availability of 3D point clouds improves the accuracy\nof 3D semantic segmentation methods. Rendering of these irregular,\nunstructured, and unordered 3D points to 2D images from multiple viewpoints\nimposes some issues such as loss of information due to 3D to 2D projection,\ndiscretizing artifacts, and high computational costs. To efficiently deal with\na large number of points and incorporate more context of each point, a\npointwise attention-based atrous convolutional neural network architecture is\nproposed. It focuses on salient 3D feature points among all feature maps while\nconsidering outstanding contextual information via spatial channel-wise\nattention modules. The proposed model has been evaluated on the two most\nimportant 3D point cloud datasets for the 3D semantic segmentation task. It\nachieves a reasonable performance compared to state-of-the-art models in terms\nof accuracy, with a much smaller number of parameters.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:12:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mahdavi", "Mobina", ""], ["Fooladgar", "Fahimeh", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1912.12095", "submitter": "Hongsen Liu", "authors": "Hongsen Liu, Yang Cong, Yandong Tang", "title": "One Point, One Object: Simultaneous 3D Object Segmentation and 6-DOF\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a single-shot method for simultaneous 3D object segmentation and\n6-DOF pose estimation in pure 3D point clouds scenes based on a consensus that\n\\emph{one point only belongs to one object}, i.e., each point has the potential\npower to predict the 6-DOF pose of its corresponding object. Unlike the\nrecently proposed methods of the similar task, which rely on 2D detectors to\npredict the projection of 3D corners of the 3D bounding boxes and the 6-DOF\npose must be estimated by a PnP like spatial transformation method, ours is\nconcise enough not to require additional spatial transformation between\ndifferent dimensions. Due to the lack of training data for many objects, the\nrecently proposed 2D detection methods try to generate training data by using\nrendering engine and achieve good results. However, rendering in 3D space along\nwith 6-DOF is relatively difficult. Therefore, we propose an augmented reality\ntechnology to generate the training data in semi-virtual reality 3D space. The\nkey component of our method is a multi-task CNN architecture that can\nsimultaneously predicts the 3D object segmentation and 6-DOF pose estimation in\npure 3D point clouds.\n  For experimental evaluation, we generate expanded training data for two\nstate-of-the-arts 3D object datasets \\cite{PLCHF}\\cite{TLINEMOD} by using\nAugmented Reality technology (AR). We evaluate our proposed method on the two\ndatasets. The results show that our method can be well generalized into\nmultiple scenarios and provide performance comparable to or better than the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:48:03 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Liu", "Hongsen", ""], ["Cong", "Yang", ""], ["Tang", "Yandong", ""]]}, {"id": "1912.12098", "submitter": "Tolga Birdal", "authors": "Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti,\n  Leonidas Guibas, Federico Tombari", "title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds", "comments": "Oral Presentation at ECCV 2020. Find our video under:\n  https://youtu.be/LHh56snwhTA. We release our sources at:\n  http://tolgabirdal.github.io/qecnetworks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D capsule module for processing point clouds that is\nequivariant to 3D rotations and translations, as well as invariant to\npermutations of the input points. The operator receives a sparse set of local\nreference frames, computed from an input point cloud and establishes end-to-end\ntransformation equivariance through a novel dynamic routing procedure on\nquaternions. Further, we theoretically connect dynamic routing between capsules\nto the well-known Weiszfeld algorithm, a scheme for solving \\emph{iterative\nre-weighted least squares} (IRLS) problems with provable convergence\nproperties. It is shown that such group dynamic routing can be interpreted as\nrobust IRLS rotation averaging on capsule votes, where information is routed\nbased on the final inlier scores. Based on our operator, we build a capsule\nnetwork that disentangles geometry from pose, paving the way for more\ninformative descriptors and a structured latent space. Our architecture allows\njoint object classification and orientation estimation without explicit\nsupervision of rotations. We validate our algorithm empirically on common\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:51:17 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 19:09:44 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 13:12:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhao", "Yongheng", ""], ["Birdal", "Tolga", ""], ["Lenssen", "Jan Eric", ""], ["Menegatti", "Emanuele", ""], ["Guibas", "Leonidas", ""], ["Tombari", "Federico", ""]]}, {"id": "1912.12101", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Vlad Catalin Frasineanu, Jens Lambrecht", "title": "A 3D-Deep-Learning-based Augmented Reality Calibration Method for\n  Robotic Environments using Depth Sensor Data", "comments": "7 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality and mobile robots are gaining much attention within\nindustries due to the high potential to make processes cost and time efficient.\nTo facilitate augmented reality, a calibration between the Augmented Reality\ndevice and the environment is necessary. This is a challenge when dealing with\nmobile robots due to the mobility of all entities making the environment\ndynamic. On this account, we propose a novel approach to calibrate the\nAugmented Reality device using 3D depth sensor data. We use the depth camera of\na cutting edge Augmented Reality Device - the Microsoft Hololens for deep\nlearning based calibration. Therefore, we modified a neural network based on\nthe recently published VoteNet architecture which works directly on the point\ncloud input observed by the Hololens. We achieve satisfying results and\neliminate external tools like markers, thus enabling a more intuitive and\nflexible work flow for Augmented Reality integration. The results are adaptable\nto work with all depth cameras and are promising for further research.\nFurthermore, we introduce an open source 3D point cloud labeling tool, which is\nto our knowledge the first open source tool for labeling raw point cloud data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:56:13 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Frasineanu", "Vlad Catalin", ""], ["Lambrecht", "Jens", ""]]}, {"id": "1912.12106", "submitter": "Ali Borji", "authors": "Ali Borji, Sikun Lin", "title": "White Noise Analysis of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A white noise analysis of modern deep neural networks is presented to unveil\ntheir biases at the whole network level or the single neuron level. Our\nanalysis is based on two popular and related methods in psychophysics and\nneurophysiology namely classification images and spike triggered analysis.\nThese methods have been widely used to understand the underlying mechanisms of\nsensory systems in humans and monkeys. We leverage them to investigate the\ninherent biases of deep neural networks and to obtain a first-order\napproximation of their functionality. We emphasize on CNNs since they are\ncurrently the state of the art methods in computer vision and are a decent\nmodel of human visual processing. In addition, we study multi-layer\nperceptrons, logistic regression, and recurrent neural networks. Experiments\nover four classic datasets, MNIST, Fashion-MNIST, CIFAR-10, and ImageNet, show\nthat the computed bias maps resemble the target classes and when used for\nclassification lead to an over twofold performance than the chance level.\nFurther, we show that classification images can be used to attack a black-box\nclassifier and to detect adversarial patch attacks. Finally, we utilize spike\ntriggered averaging to derive the filters of CNNs and explore how the behavior\nof a network changes when neurons in different layers are modulated. Our effort\nillustrates a successful example of borrowing from neurosciences to study ANNs\nand highlights the importance of cross-fertilization and synergy across machine\nlearning, deep learning, and computational neuroscience.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:14:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Borji", "Ali", ""], ["Lin", "Sikun", ""]]}, {"id": "1912.12120", "submitter": "Daniel San Martin", "authors": "Daniel San Martin, Daniel Manzano", "title": "A Deep Learning Model for Chilean Bills Classification", "comments": "3 pages, 3 figures, Posters Content EVIC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic bill classification is an attractive task with many potential\napplications such as automated detection and counting in images or videos. To\naddress this purpose we present a Deep Learning Model to classify Chilean\nBanknotes, because of its successful results in image processing applications.\nFor optimal performance of the proposed model, data augmentation techniques are\nintroduced due to the limited number of image samples. Positive results were\nachieved in this work, verifying that it could be a stating point to be\nextended to more complex applications.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 20:29:20 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Martin", "Daniel San", ""], ["Manzano", "Daniel", ""]]}, {"id": "1912.12121", "submitter": "Y. Alex Kolchinski", "authors": "Y. Alex Kolchinski, Sharon Zhou, Shengjia Zhao, Mitchell Gordon,\n  Stefano Ermon", "title": "Approximating Human Judgment of Generated Image Quality", "comments": "To appear in the Shared Visual Representations in Human and Machine\n  Intelligence workshop at NeurIPS 2019. The first two authors contributed\n  equally to the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have made immense progress in recent years, particularly in\ntheir ability to generate high quality images. However, that quality has been\ndifficult to evaluate rigorously, with evaluation dominated by heuristic\napproaches that do not correlate well with human judgment, such as the\nInception Score and Fr\\'echet Inception Distance. Real human labels have also\nbeen used in evaluation, but are inefficient and expensive to collect for each\nimage. Here, we present a novel method to automatically evaluate images based\non their quality as perceived by humans. By not only generating image\nembeddings from Inception network activations and comparing them to the\nactivations for real images, of which other methods perform a variant, but also\nregressing the activation statistics to match gold standard human labels, we\ndemonstrate 66% accuracy in predicting human scores of image realism, matching\nthe human inter-rater agreement rate. Our approach also generalizes across\ngenerative models, suggesting the potential for capturing a model-agnostic\nmeasure of image quality. We open source our dataset of human labels for the\nadvancement of research and techniques in this area.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 19:51:02 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kolchinski", "Y. Alex", ""], ["Zhou", "Sharon", ""], ["Zhao", "Shengjia", ""], ["Gordon", "Mitchell", ""], ["Ermon", "Stefano", ""]]}, {"id": "1912.12123", "submitter": "Mkhuseli Ngxande", "authors": "Mkhuseli Ngxande, Jules-Raymond Tapamo, Michael Burke", "title": "Bias Remediation in Driver Drowsiness Detection systems using Generative\n  Adversarial Networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets are crucial when training a deep neural network. When datasets are\nunrepresentative, trained models are prone to bias because they are unable to\ngeneralise to real world settings. This is particularly problematic for models\ntrained in specific cultural contexts, which may not represent a wide range of\nraces, and thus fail to generalise. This is a particular challenge for Driver\ndrowsiness detection, where many publicly available datasets are\nunrepresentative as they cover only certain ethnicity groups. Traditional\naugmentation methods are unable to improve a model's performance when tested on\nother groups with different facial attributes, and it is often challenging to\nbuild new, more representative datasets. In this paper, we introduce a novel\nframework that boosts the performance of detection of drowsiness for different\nethnicity groups. Our framework improves Convolutional Neural Network (CNN)\ntrained for prediction by using Generative Adversarial networks (GAN) for\ntargeted data augmentation based on a population bias visualisation strategy\nthat groups faces with similar facial attributes and highlights where the model\nis failing. A sampling method selects faces where the model is not performing\nwell, which are used to fine-tune the CNN. Experiments show the efficacy of our\napproach in improving driver drowsiness detection for under represented\nethnicity groups. Here, models trained on publicly available datasets are\ncompared with a model trained using the proposed data augmentation strategy.\nAlthough developed in the context of driver drowsiness detection, the proposed\nframework is not limited to the driver drowsiness detection task, but can be\napplied to other applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 13:04:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ngxande", "Mkhuseli", ""], ["Tapamo", "Jules-Raymond", ""], ["Burke", "Michael", ""]]}, {"id": "1912.12125", "submitter": "Kilian Kleeberger", "authors": "Kilian Kleeberger and Christian Landgraf and Marco F. Huber", "title": "Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking", "comments": "Accepted at 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new public dataset for 6D object pose\nestimation and instance segmentation for industrial bin-picking. The dataset\ncomprises both synthetic and real-world scenes. For both, point clouds, depth\nimages, and annotations comprising the 6D pose (position and orientation), a\nvisibility score, and a segmentation mask for each object are provided. Along\nwith the raw data, a method for precisely annotating real-world scenes is\nproposed. To the best of our knowledge, this is the first public dataset for 6D\nobject pose estimation and instance segmentation for bin-picking containing\nsufficiently annotated data for learning-based approaches. Furthermore, it is\none of the largest public datasets for object pose estimation in general. The\ndataset is publicly available at http://www.bin-picking.ai/en/dataset.html.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:32:04 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kleeberger", "Kilian", ""], ["Landgraf", "Christian", ""], ["Huber", "Marco F.", ""]]}, {"id": "1912.12129", "submitter": "Angshul Majumdar Dr.", "authors": "Jyoti Maggu and Angshul Majumdar", "title": "Kernel Transform Learning", "comments": null, "journal-ref": "Pattern Recognition Letters, 98, pp.117-122 (2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes kernel transform learning. The idea of dictionary learning\nis well known; it is a synthesis formulation where a basis is learnt along with\nthe coefficients so as to generate or synthesize the data. Transform learning\nis its analysis equivalent; the transforms operates or analyses on the data to\ngenerate the coefficients. The concept of kernel dictionary learning has been\nintroduced in the recent past, where the dictionary is represented as a linear\ncombination of non-linear version of the data. Its success has been showcased\nin feature extraction. In this work we propose to kernelize transform learning\non line similar to kernel dictionary learning. An efficient solution for kernel\ntransform learning has been proposed especially for problems where the number\nof samples is much larger than the dimensionality of the input samples making\nthe kernel matrix very high dimensional. Kernel transform learning has been\ncompared with other representation learning tools like autoencoder, restricted\nBoltzmann machine as well as with dictionary learning (and its kernelized\nversion). Our proposed kernel transform learning yields better results than all\nthe aforesaid techniques; experiments have been carried out on benchmark\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 10:55:38 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Maggu", "Jyoti", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.12131", "submitter": "Angshul Majumdar Dr.", "authors": "Anupriya Gogna and Angshul Majumdar", "title": "Discriminative Autoencoder for Feature Extraction: Application to\n  Character Recognition", "comments": "The final version has been accepted at Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, autoencoders are unsupervised representation learning tools.\nIn this work, we propose a novel discriminative autoencoder. Use of supervised\ndiscriminative learning ensures that the learned representation is robust to\nvariations commonly encountered in image datasets. Using the basic\ndiscriminating autoencoder as a unit, we build a stacked architecture aimed at\nextracting relevant representation from the training data. The efficiency of\nour feature extraction algorithm ensures a high classification accuracy with\neven simple classification schemes like KNN (K-nearest neighbor). We\ndemonstrate the superiority of our model for representation learning by\nconducting experiments on standard datasets for character/image recognition and\nsubsequent comparison with existing supervised deep architectures like class\nsparse stacked autoencoder and discriminative deep belief network.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 10:12:22 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gogna", "Anupriya", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.12132", "submitter": "Cenk Gazen", "authors": "Shreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk\n  Gazen, Jason Hickey", "title": "Machine Learning for Precipitation Nowcasting from Radar Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution nowcasting is an essential tool needed for effective\nadaptation to climate change, particularly for extreme weather. As Deep\nLearning (DL) techniques have shown dramatic promise in many domains, including\nthe geosciences, we present an application of DL to the problem of\nprecipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1\nhour) predictions of precipitation. We treat forecasting as an image-to-image\ntranslation problem and leverage the power of the ubiquitous UNET convolutional\nneural network. We find this performs favorably when compared to three commonly\nused models: optical flow, persistence and NOAA's numerical one-hour HRRR\nnowcasting prediction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:46:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Agrawal", "Shreya", ""], ["Barrington", "Luke", ""], ["Bromberg", "Carla", ""], ["Burge", "John", ""], ["Gazen", "Cenk", ""], ["Hickey", "Jason", ""]]}, {"id": "1912.12134", "submitter": "Jiajie Ye", "authors": "Jiajie Ye, Yisheng Guan, Junfa Liu, Xinghong Huang and Hong Zhang", "title": "Large-scale Multi-modal Person Identification in Real Unconstrained\n  Environments", "comments": "6 pages, IEEE International Conference on Robotics and Biomimetics\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person identification (P-ID) under real unconstrained noisy environments is a\nhuge challenge. In multiple-feature learning with Deep Convolutional Neural\nNetworks (DCNNs) or Machine Learning method for large-scale person\nidentification in the wild, the key is to design an appropriate strategy for\ndecision layer fusion or feature layer fusion which can enhance discriminative\npower. It is necessary to extract different types of valid features and\nestablish a reasonable framework to fuse different types of information. In\ntraditional methods, different persons are identified based on single modal\nfeatures to identify, such as face feature, audio feature, and head feature.\nThese traditional methods cannot realize a highly accurate level of person\nidentification in real unconstrained environments. The study aims to propose a\nfusion module to fuse multi-modal features for person identification in real\nunconstrained environments.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 03:16:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ye", "Jiajie", ""], ["Guan", "Yisheng", ""], ["Liu", "Junfa", ""], ["Huang", "Xinghong", ""], ["Zhang", "Hong", ""]]}, {"id": "1912.12135", "submitter": "Duhwan Mun", "authors": "Hyungki Kim and Duhwan Mun", "title": "Deep-learning-based classification and retrieval of components of a\n  process plant from segmented point clouds", "comments": "31 pages, 7305 words, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology to recognize the type of component represented by a point cloud is\nrequired in the reconstruction process of an as-built model of a process plant\nbased on laser scanning. The reconstruction process of a process plant through\nlaser scanning is divided into point cloud registration, point cloud\nsegmentation, and component type recognition and placement. Loss of shape data\nor imbalance of point cloud density problems generally occur in the point cloud\ndata collected from large-scale facilities. In this study, we experimented with\nthe possibility of applying object recognition technology based on 3D deep\nlearning networks, which have been showing high performance recently, and\nanalyzed the results. For training data, we used a segmented point cloud\nrepository about components that we constructed by scanning a process plant.\nFor networks, we selected the multi-view convolutional neural network (MVCNN),\nwhich is a view-based method, and PointNet, which is designed to allow the\ndirect input of point cloud data. In the case of the MVCNN, we also performed\nan experiment on the generation method for two types of multi-view images that\ncan complement the shape occlusion of the segmented point cloud. In this\nexperiment, the MVCNN showed the highest retrieval accuracy of approximately\n87%, whereas PointNet showed the highest retrieval mean average precision of\napproximately 84%. Furthermore, both networks showed high recognition\nperformance for the segmented point cloud of plant components when there was\nsufficient training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:34:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kim", "Hyungki", ""], ["Mun", "Duhwan", ""]]}, {"id": "1912.12137", "submitter": "Bas Peters", "authors": "Bas Peters, Eldad Haber, Keegan Lensink", "title": "Symmetric block-low-rank layers for fully reversible multilevel neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factors that limit the size of the input and output of a neural network\ninclude memory requirements for the network states/activations to compute\ngradients, as well as memory for the convolutional kernels or other weights.\nThe memory restriction is especially limiting for applications where we want to\nlearn how to map volumetric data to the desired output, such as video-to-video.\nRecently developed fully reversible neural networks enable gradient\ncomputations using storage of the network states for a couple of layers only.\nWhile this saves a tremendous amount of memory, it is the convolutional kernels\nthat take up most memory if fully reversible networks contain multiple\ninvertible pooling/coarsening layers. Invertible coarsening operators such as\nthe orthogonal wavelet transform cause the number of channels to grow\nexplosively. We address this issue by combining fully reversible networks with\nlayers that contain the convolutional kernels in a compressed form directly.\nSpecifically, we introduce a layer that has a symmetric block-low-rank\nstructure. In spirit, this layer is similar to bottleneck and\nsqueeze-and-expand structures. We contribute symmetry by construction, and a\ncombination of notation and flattening of tensors allows us to interpret these\nnetwork structures in linear algebraic fashion as a block-low-rank matrix in\nfactorized form and observe various properties. A video segmentation example\nshows that we can train a network to segment the entire video in one go, which\nwould not be possible, in terms of memory requirements, using non-reversible\nnetworks and previously proposed reversible networks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 06:29:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Peters", "Bas", ""], ["Haber", "Eldad", ""], ["Lensink", "Keegan", ""]]}, {"id": "1912.12138", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yulin Sun, Yang Wang, Zhengjun Zha, Shuicheng Yan, Meng\n  Wang", "title": "Convolutional Dictionary Pair Learning Network for Image Representation\n  Learning", "comments": "Accepted by the 24th European Conference on Artificial Intelligence\n  (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are\npowerful image representation learning systems based on different mechanisms\nand principles, however whether we can seamlessly integrate them to improve the\nper-formance is noteworthy exploring. To address this issue, we propose a novel\ngeneralized end-to-end representation learning architecture, dubbed\nConvolutional Dictionary Pair Learning Network (CDPL-Net) in this paper, which\nintegrates the learning schemes of the CNN and dictionary pair learning into a\nunified framework. Generally, the architecture of CDPL-Net includes two\nconvolutional/pooling layers and two dictionary pair learn-ing (DPL) layers in\nthe representation learning module. Besides, it uses two fully-connected layers\nas the multi-layer perception layer in the nonlinear classification module. In\nparticular, the DPL layer can jointly formulate the discriminative synthesis\nand analysis representations driven by minimizing the batch based\nreconstruction error over the flatted feature maps from the convolution/pooling\nlayer. Moreover, DPL layer uses l1-norm on the analysis dictionary so that\nsparse representation can be delivered, and the embedding process will also be\nrobust to noise. To speed up the training process of DPL layer, the efficient\nstochastic gradient descent is used. Extensive simulations on real databases\nshow that our CDPL-Net can deliver enhanced performance over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:34:28 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 09:49:42 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 12:12:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zhang", "Zhao", ""], ["Sun", "Yulin", ""], ["Wang", "Yang", ""], ["Zha", "Zhengjun", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.12139", "submitter": "Manh Duong Phung", "authors": "Qiuchen Zhu, Manh Duong Phung, Quang Ha", "title": "Crack Detection Using Enhanced Hierarchical Convolutional Neural\n  Networks", "comments": "In Proceedings of Australasian Conference on Robotics and Automation\n  2019 (ACRA), Adelaide, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAV) are expected to replace human in hazardous\ntasks of surface inspection due to their flexibility in operating space and\ncapability of collecting high quality visual data. In this study, we propose\nenhanced hierarchical convolutional neural networks (HCNN) to detect cracks\nfrom image data collected by UAVs. Unlike traditional HCNN, here a set of\nbranch networks is utilised to reduce the obscuration in the down-sampling\nprocess. Moreover, the feature preserving blocks combine the current and\nprevious terms from the convolutional blocks to provide input to the loss\nfunctions. As a result, the weights of resized images can be reduced to\nminimise the information loss. Experiments on images of different crack\ndatasets have been carried out to demonstrate the effectiveness of proposed\nHCNN.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:35:00 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhu", "Qiuchen", ""], ["Phung", "Manh Duong", ""], ["Ha", "Quang", ""]]}, {"id": "1912.12142", "submitter": "Andrew Borkowski M.D.", "authors": "Andrew A. Borkowski, Marilyn M. Bui, L. Brannon Thomas, Catherine P.\n  Wilson, Lauren A. DeLand, Stephen M. Mastorides", "title": "Lung and Colon Cancer Histopathological Image Dataset (LC25000)", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Machine Learning, a subset of Artificial Intelligence, has led\nto remarkable advancements in many areas, including medicine. Machine Learning\nalgorithms require large datasets to train computer models successfully.\nAlthough there are medical image datasets available, more image datasets are\nneeded from a variety of medical entities, especially cancer pathology. Even\nmore scarce are ML-ready image datasets. To address this need, we created an\nimage dataset (LC25000) with 25,000 color images in 5 classes. Each class\ncontains 5,000 images of the following histologic entities: colon\nadenocarcinoma, benign colonic tissue, lung adenocarcinoma, lung squamous cell\ncarcinoma, and benign lung tissue. All images are de-identified, HIPAA\ncompliant, validated, and freely available for download to AI researchers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:28:00 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Borkowski", "Andrew A.", ""], ["Bui", "Marilyn M.", ""], ["Thomas", "L. Brannon", ""], ["Wilson", "Catherine P.", ""], ["DeLand", "Lauren A.", ""], ["Mastorides", "Stephen M.", ""]]}, {"id": "1912.12144", "submitter": "Joy Bose", "authors": "Kushal Singla, Niloy Mukherjee, Hari Manassery Koduvely, Joy Bose", "title": "Evaluating Usage of Images for App Classification", "comments": "5 pages, 3 figures, 3 tables, INDICON conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App classification is useful in a number of applications such as adding apps\nto an app store or building a user model based on the installed apps. Presently\nthere are a number of existing methods to classify apps based on a given\ntaxonomy on the basis of their text metadata. However, text based methods for\napp classification may not work in all cases, such as when the text\ndescriptions are in a different language, or missing, or inadequate to classify\nthe app. One solution in such cases is to utilize the app images to supplement\nthe text description. In this paper, we evaluate a number of approaches in\nwhich app images can be used to classify the apps. In one approach, we use\nOptical character recognition (OCR) to extract text from images, which is then\nused to supplement the text description of the app. In another, we use pic2vec\nto convert the app images into vectors, then train an SVM to classify the\nvectors to the correct app label. In another, we use the captionbot.ai tool to\ngenerate natural language descriptions from the app images. Finally, we use a\nmethod to detect and label objects in the app images and use a voting technique\nto determine the category of the app based on all the images. We compare the\nperformance of our image-based techniques to classify a number of apps in our\ndataset. We use a text based SVM app classifier as our base and obtained an\nimproved classification accuracy of 96% for some classes when app images are\nadded.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:27:02 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singla", "Kushal", ""], ["Mukherjee", "Niloy", ""], ["Koduvely", "Hari Manassery", ""], ["Bose", "Joy", ""]]}, {"id": "1912.12147", "submitter": "Eduardo Arnold", "authors": "Eduardo Arnold, Mehrdad Dianati, Robert de Temple, Saber Fallah", "title": "Cooperative Perception for 3D Object Detection in Driving Scenarios\n  using Infrastructure Sensors", "comments": "13 pages, 4 tables, 7 figures. Published in IEEE Transactions on\n  Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3028424", "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is a common function within the perception system of an\nautonomous vehicle and outputs a list of 3D bounding boxes around objects of\ninterest. Various 3D object detection methods have relied on fusion of\ndifferent sensor modalities to overcome limitations of individual sensors.\nHowever, occlusion, limited field-of-view and low-point density of the sensor\ndata cannot be reliably and cost-effectively addressed by multi-modal sensing\nfrom a single point of view. Alternatively, cooperative perception incorporates\ninformation from spatially diverse sensors distributed around the environment\nas a way to mitigate these limitations. This article proposes two schemes for\ncooperative 3D object detection using single modality sensors. The early fusion\nscheme combines point clouds from multiple spatially diverse sensing points of\nview before detection. In contrast, the late fusion scheme fuses the\nindependently detected bounding boxes from multiple spatially diverse sensors.\nWe evaluate the performance of both schemes, and their hybrid combination,\nusing a synthetic cooperative dataset created in two complex driving scenarios,\na T-junction and a roundabout. The evaluation shows that the early fusion\napproach outperforms late fusion by a significant margin at the cost of higher\ncommunication bandwidth. The results demonstrate that cooperative perception\ncan recall more than 95% of the objects as opposed to 30% for single-point\nsensing in the most challenging scenario. To provide practical insights into\nthe deployment of such system, we report how the number of sensors and their\nconfiguration impact the detection performance of the system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:19:27 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:59:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Arnold", "Eduardo", ""], ["Dianati", "Mehrdad", ""], ["de Temple", "Robert", ""], ["Fallah", "Saber", ""]]}, {"id": "1912.12148", "submitter": "Jianwu Fang", "authors": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, and Jianru Xue", "title": "DADA: A Large-scale Benchmark and Model for Driver Attention Prediction\n  in Accidental Scenarios", "comments": "12 pages, 13 figures, submitted to IEEE-TITS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver attention prediction has recently absorbed increasing attention in\ntraffic scene understanding and is prone to be an essential problem in\nvision-centered and human-like driving systems. This work, different from other\nattempts, makes an attempt to predict the driver attention in accidental\nscenarios containing normal, critical and accidental situations simultaneously.\nHowever, challenges tread on the heels of that because of the dynamic traffic\nscene, intricate and imbalanced accident categories. With the hypothesis that\ndriver attention can provide a selective role of crash-object for assisting\ndriving accident detection or prediction, this paper designs a multi-path\nsemantic-guided attentive fusion network (MSAFNet) that learns the\nspatio-temporal semantic and scene variation in prediction. For fulfilling\nthis, a large-scale benchmark with 2000 video sequences (named as DADA-2000) is\ncontributed with laborious annotation for driver attention (fixation, saccade,\nfocusing time), accident objects/intervals, as well as the accident categories,\nand superior performance to state-of-the-arts are provided by thorough\nevaluations. As far as we know, this is the first comprehensive and\nquantitative study for the human-eye sensing exploration in accidental\nscenarios. DADA-2000 is available at https://github.com/JWFangit/LOTVS-DADA.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:41:06 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fang", "Jianwu", ""], ["Yan", "Dingxin", ""], ["Qiao", "Jiahuan", ""], ["Xue", "Jianru", ""]]}, {"id": "1912.12162", "submitter": "Shuai Wang", "authors": "Shuai Wang and Zhendong Su", "title": "Metamorphic Testing for Object Detection Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep neural networks (DNNs) have led to object detectors\nthat can rapidly process pictures or videos, and recognize the objects that\nthey contain. Despite the promising progress by industrial manufacturers such\nas Amazon and Google in commercializing deep learning-based object detection as\na standard computer vision service, object detection systems - similar to\ntraditional software - may still produce incorrect results. These errors, in\nturn, can lead to severe negative outcomes for the users of these object\ndetection systems. For instance, an autonomous driving system that fails to\ndetect pedestrians can cause accidents or even fatalities. However, principled,\nsystematic methods for testing object detection systems do not yet exist,\ndespite their importance.\n  To fill this critical gap, we introduce the design and realization of MetaOD,\nthe first metamorphic testing system for object detectors to effectively reveal\nerroneous detection results by commercial object detectors. To this end, we (1)\nsynthesize natural-looking images by inserting extra object instances into\nbackground images, and (2) design metamorphic conditions asserting the\nequivalence of object detection results between the original and synthetic\nimages after excluding the prediction results on the inserted objects. MetaOD\nis designed as a streamlined workflow that performs object extraction,\nselection, and insertion. Evaluated on four commercial object detection\nservices and four pretrained models provided by the TensorFlow API, MetaOD\nfound tens of thousands of detection defects in these object detectors. To\nfurther demonstrate the practical usage of MetaOD, we use the synthetic images\nthat cause erroneous detection results to retrain the model. Our results show\nthat the model performance is increased significantly, from an mAP score of 9.3\nto an mAP score of 10.5.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 02:03:46 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Shuai", ""], ["Su", "Zhendong", ""]]}, {"id": "1912.12164", "submitter": "Arthur Pajot", "authors": "Arthur Pajot, Emmanuel de Bezenac, Patrick Gallinari", "title": "Unsupervised Adversarial Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inpainting in an unsupervised setting where there is neither\naccess to paired nor unpaired training data. The only available information is\nprovided by the uncomplete observations and the inpainting process statistics.\nIn this context, an observation should give rise to several plausible\nreconstructions which amounts at learning a distribution over the space of\nreconstructed images. We model the reconstruction process by using a\nconditional GAN with constraints on the stochastic component that introduce an\nexplicit dependency between this component and the generated output. This\nallows us sampling from the latent component in order to generate a\ndistribution of images associated to an observation. We demonstrate the\ncapacity of our model on several image datasets: faces (CelebA), food images\n(Recipe-1M) and bedrooms (LSUN Bedrooms) with different types of imputation\nmasks. The approach yields comparable performance to model variants trained\nwith additional supervision.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:06:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Pajot", "Arthur", ""], ["de Bezenac", "Emmanuel", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1912.12165", "submitter": "Wenfeng Feng", "authors": "Wenfeng Feng, Xin Zhang, Guangpeng Zhao", "title": "ResNetX: a more disordered and deeper network architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing efficient network structures has always been the core content of\nneural network research.\n  ResNet and its variants have proved to be efficient in architecture.\n  However, how to theoretically character the influence of network structure on\nperformance is still vague.\n  With the help of techniques in complex networks, We here provide a natural\nyet efficient extension to ResNet by folding its backbone chain.\n  Our architecture has two structural features when being mapped to directed\nacyclic graphs:\n  First is a higher degree of the disorder compared with ResNet, which let\nResNetX explore a larger number of feature maps with different sizes of\nreceptive fields.\n  Second is a larger proportion of shorter paths compared to ResNet, which\nimproves the direct flow of information through the entire network.\n  Our architecture exposes a new dimension, namely \"fold depth\", in addition to\nexisting dimensions of depth, width, and cardinality.\n  Our architecture is a natural extension to ResNet, and can be integrated with\nexisting state-of-the-art methods with little effort. Image classification\nresults on CIFAR-10 and CIFAR-100 benchmarks suggested that our new network\narchitecture performs better than ResNet.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:05:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Feng", "Wenfeng", ""], ["Zhang", "Xin", ""], ["Zhao", "Guangpeng", ""]]}, {"id": "1912.12167", "submitter": "Tien-Ju Yang", "authors": "Tien-Ju Yang, Vivienne Sze", "title": "Design Considerations for Efficient Deep Neural Networks on\n  Processing-in-Memory Accelerators", "comments": "Accepted by IEDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes various design considerations for deep neural networks\nthat enable them to operate efficiently and accurately on processing-in-memory\naccelerators. We highlight important properties of these accelerators and the\nresulting design considerations using experiments conducted on various\nstate-of-the-art deep neural networks with the large-scale ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:23:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Tien-Ju", ""], ["Sze", "Vivienne", ""]]}, {"id": "1912.12168", "submitter": "Chandranath Adak", "authors": "Chandranath Adak, Bidyut B. Chaudhuri, Chin-Teng Lin, Michael\n  Blumenstein", "title": "Intra-Variable Handwriting Inspection Reinforced with Idiosyncrasy\n  Analysis", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security, 2020", "doi": "10.1109/TIFS.2020.2991833", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we work on intra-variable handwriting, where the writing\nsamples of an individual can vary significantly. Such within-writer variation\nthrows a challenge for automatic writer inspection, where the state-of-the-art\nmethods do not perform well. To deal with intra-variability, we analyze the\nidiosyncrasy in individual handwriting. We identify/verify the writer from\nhighly idiosyncratic text-patches. Such patches are detected using a deep\nrecurrent reinforcement learning-based architecture. An idiosyncratic score is\nassigned to every patch, which is predicted by employing deep regression\nanalysis. For writer identification, we propose a deep neural architecture,\nwhich makes the final decision by the idiosyncratic score-induced weighted\naverage of patch-based decisions. For writer verification, we propose two\nalgorithms for patch-fed deep feature aggregation, which assist in\nauthentication using a triplet network. The experiments were performed on two\ndatabases, where we obtained encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 10:56:52 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 15:51:51 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Adak", "Chandranath", ""], ["Chaudhuri", "Bidyut B.", ""], ["Lin", "Chin-Teng", ""], ["Blumenstein", "Michael", ""]]}, {"id": "1912.12169", "submitter": "Haozhen Zhao", "authors": "Nathaniel Huber-Fliflet, Fusheng Wei, Haozhen Zhao, Han Qin, Shi Ye,\n  Amy Tsang", "title": "Image Analytics for Legal Document Review: A Transfer Learning Approach", "comments": "2019 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though technology assisted review in electronic discovery has been focusing\non text data, the need of advanced analytics to facilitate reviewing multimedia\ncontent is on the rise. In this paper, we present several applications of deep\nlearning in computer vision to Technology Assisted Review of image data in\nlegal industry. These applications include image classification, image\nclustering, and object detection. We use transfer learning techniques to\nleverage established pretrained models for feature extraction and fine tuning.\nThese applications are first of their kind in the legal industry for image\ndocument review. We demonstrate effectiveness of these applications with\nsolving real world business challenges.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:11:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Huber-Fliflet", "Nathaniel", ""], ["Wei", "Fusheng", ""], ["Zhao", "Haozhen", ""], ["Qin", "Han", ""], ["Ye", "Shi", ""], ["Tsang", "Amy", ""]]}, {"id": "1912.12170", "submitter": "Woohyung Chun", "authors": "Woohyung Chun, Sung-Min Hong, Junho Huh, Inyup Kang", "title": "Mitigating large adversarial perturbations on X-MAS (X minus Moving\n  Averaged Samples)", "comments": "X-MAS is the essential condition for the proposed mitigation as well\n  as human beings. The codes and data for evaluation are available in\n  https://github.com/stonylinux/mitigating_large_adversarial_perturbations_on_X-MAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the scheme that mitigates the adversarial perturbation $\\epsilon$\non the adversarial example $X_{adv}$ ($=$ $X$ $\\pm$ $\\epsilon$, $X$ is a benign\nsample) by subtracting the estimated perturbation $\\hat{\\epsilon}$ from $X$ $+$\n$\\epsilon$ and adding $\\hat{\\epsilon}$ to $X$ $-$ $\\epsilon$. The estimated\nperturbation $\\hat{\\epsilon}$ comes from the difference between $X_{adv}$ and\nits moving-averaged outcome $W_{avg}*X_{adv}$ where $W_{avg}$ is $N \\times N$\nmoving average kernel that all the coefficients are one. Usually, the adjacent\nsamples of an image are close to each other such that we can let $X$ $\\approx$\n$W_{avg}*X$ (naming this relation after X-MAS[X minus Moving Averaged\nSamples]). By doing that, we can make the estimated perturbation\n$\\hat{\\epsilon}$ falls within the range of $\\epsilon$. The scheme is also\nextended to do the multi-level mitigation by configuring the mitigated\nadversarial example $X_{adv}$ $\\pm$ $\\hat{\\epsilon}$ as a new adversarial\nexample to be mitigated. The multi-level mitigation gets $X_{adv}$ closer to\n$X$ with a smaller (i.e. mitigated) perturbation than original unmitigated\nperturbation by setting the moving averaged adversarial sample $W_{avg} *\nX_{adv}$ (which has the smaller perturbation than $X_{adv}$ if $X$ $\\approx$\n$W_{avg}*X$) as the boundary condition that the multi-level mitigation cannot\ncross over (i.e. decreasing $\\epsilon$ cannot go below and increasing\n$\\epsilon$ cannot go beyond). With the multi-level mitigation, we can get high\nprediction accuracies even in the adversarial example having a large\nperturbation (i.e. $\\epsilon$ $>$ $16$). The proposed scheme is evaluated with\nadversarial examples crafted by the FGSM (Fast Gradient Sign Method) based\nattacks on ResNet-50 trained with ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 22:37:12 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 13:52:44 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 11:23:15 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2020 14:16:36 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Chun", "Woohyung", ""], ["Hong", "Sung-Min", ""], ["Huh", "Junho", ""], ["Kang", "Inyup", ""]]}, {"id": "1912.12171", "submitter": "Xiaoxiang Zhu", "authors": "Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang,\n  Lichao Mou, Hossein Bagheri, Matthias H\\\"aberle, Yuansheng Hua, Rong Huang,\n  Lloyd Hughes, Hao Li, Yao Sun, Guichen Zhang, Shiyao Han, Michael Schmitt and\n  Yuanyuan Wang", "title": "So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones\n  Classification", "comments": "Article submitted to IEEE Geoscience and Remote Sensing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to labeled reference data is one of the grand challenges in supervised\nmachine learning endeavors. This is especially true for an automated analysis\nof remote sensing images on a global scale, which enables us to address global\nchallenges such as urbanization and climate change using state-of-the-art\nmachine learning techniques. To meet these pressing needs, especially in urban\nresearch, we provide open access to a valuable benchmark dataset named \"So2Sat\nLCZ42,\" which consists of local climate zone (LCZ) labels of about half a\nmillion Sentinel-1 and Sentinel-2 image patches in 42 urban agglomerations\n(plus 10 additional smaller areas) across the globe. This dataset was labeled\nby 15 domain experts following a carefully designed labeling work flow and\nevaluation process over a period of six months. As rarely done in other labeled\nremote sensing dataset, we conducted rigorous quality assessment by domain\nexperts. The dataset achieved an overall confidence of 85%. We believe this LCZ\ndataset is a first step towards an unbiased globallydistributed dataset for\nurban growth monitoring using machine learning methods, because LCZ provide a\nrather objective measure other than many other semantic land use and land cover\nclassifications. It provides measures of the morphology, compactness, and\nheight of urban areas, which are less dependent on human and culture. This\ndataset can be accessed from http://doi.org/10.14459/2018mp1483140.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:42:01 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhu", "Xiao Xiang", ""], ["Hu", "Jingliang", ""], ["Qiu", "Chunping", ""], ["Shi", "Yilei", ""], ["Kang", "Jian", ""], ["Mou", "Lichao", ""], ["Bagheri", "Hossein", ""], ["H\u00e4berle", "Matthias", ""], ["Hua", "Yuansheng", ""], ["Huang", "Rong", ""], ["Hughes", "Lloyd", ""], ["Li", "Hao", ""], ["Sun", "Yao", ""], ["Zhang", "Guichen", ""], ["Han", "Shiyao", ""], ["Schmitt", "Michael", ""], ["Wang", "Yuanyuan", ""]]}, {"id": "1912.12177", "submitter": "Ziwen Ke", "authors": "Ziwen Ke, Jing Cheng, Leslie Ying, Hairong Zheng, Yanjie Zhu, and Dong\n  Liang", "title": "An Unsupervised Deep Learning Method for Multi-coil Cine MRI", "comments": "12 pages, 10 figures, submitted to PMB", "journal-ref": null, "doi": "10.1088/1361-6560/abaffa", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved good success in cardiac magnetic resonance imaging\n(MRI) reconstruction, in which convolutional neural networks (CNNs) learn a\nmapping from the undersampled k-space to the fully sampled images. Although\nthese deep learning methods can improve the reconstruction quality compared\nwith iterative methods without requiring complex parameter selection or lengthy\nreconstruction time, the following issues still need to be addressed: 1) all\nthese methods are based on big data and require a large amount of fully sampled\nMRI data, which is always difficult to obtain for cardiac MRI; 2) the effect of\ncoil correlation on reconstruction in deep learning methods for dynamic MR\nimaging has never been studied. In this paper, we propose an unsupervised deep\nlearning method for multi-coil cine MRI via a time-interleaved sampling\nstrategy. Specifically, a time-interleaved acquisition scheme is utilized to\nbuild a set of fully encoded reference data by directly merging the k-space\ndata of adjacent time frames. Then these fully encoded data can be used to\ntrain a parallel network for reconstructing images of each coil separately.\nFinally, the images from each coil are combined via a CNN to implicitly explore\nthe correlations between coils. The comparisons with classic k-t FOCUSS, k-t\nSLR, L+S and KLR methods on in vivo datasets show that our method can achieve\nimproved reconstruction results in an extremely short amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 08:19:41 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 10:09:15 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 03:42:59 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Ke", "Ziwen", ""], ["Cheng", "Jing", ""], ["Ying", "Leslie", ""], ["Zheng", "Hairong", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "1912.12178", "submitter": "Zilong Ji", "authors": "Zilong Ji, Xiaolong Zou, Tiejun Huang, Si Wu", "title": "Unsupervised Few-shot Learning via Self-supervised Training", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from limited exemplars (few-shot learning) is a fundamental,\nunsolved problem that has been laboriously explored in the machine learning\ncommunity. However, current few-shot learners are mostly supervised and rely\nheavily on a large amount of labeled examples. Unsupervised learning is a more\nnatural procedure for cognitive mammals and has produced promising results in\nmany machine learning tasks. In the current study, we develop a method to learn\nan unsupervised few-shot learner via self-supervised training (UFLST), which\ncan effectively generalize to novel but related classes. The proposed model\nconsists of two alternate processes, progressive clustering and episodic\ntraining. The former generates pseudo-labeled training examples for\nconstructing episodic tasks; and the later trains the few-shot learner using\nthe generated episodic tasks which further optimizes the feature\nrepresentations of data. The two processes facilitate with each other, and\neventually produce a high quality few-shot learner. Using the benchmark dataset\nOmniglot and Mini-ImageNet, we show that our model outperforms other\nunsupervised few-shot learning methods. Using the benchmark dataset Market1501,\nwe further demonstrate the feasibility of our model to a real-world application\non person re-identification.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:09:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ji", "Zilong", ""], ["Zou", "Xiaolong", ""], ["Huang", "Tiejun", ""], ["Wu", "Si", ""]]}, {"id": "1912.12179", "submitter": "Tristan Sylvain", "authors": "Tristan Sylvain, Linda Petrini, Devon Hjelm", "title": "Locality and compositionality in zero-shot learning", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study locality and compositionality in the context of\nlearning representations for Zero Shot Learning (ZSL). In order to well-isolate\nthe importance of these properties in learned representations, we impose the\nadditional constraint that, differently from most recent work in ZSL, no\npre-training on different datasets (e.g. ImageNet) is performed. The results of\nour experiments show how locality, in terms of small parts of the input, and\ncompositionality, i.e. how well can the learned representations be expressed as\na function of a smaller vocabulary, are both deeply related to generalization\nand motivate the focus on more local-aware models in future research directions\nfor representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:50:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sylvain", "Tristan", ""], ["Petrini", "Linda", ""], ["Hjelm", "Devon", ""]]}, {"id": "1912.12180", "submitter": "Nal Kalchbrenner", "authors": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans", "title": "Axial Attention in Multidimensional Transformers", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Axial Transformers, a self-attention-based autoregressive model\nfor images and other data organized as high dimensional tensors. Existing\nautoregressive models either suffer from excessively large computational\nresource requirements for high dimensional data, or make compromises in terms\nof distribution expressiveness or ease of implementation in order to decrease\nresource requirements. Our architecture, by contrast, maintains both full\nexpressiveness over joint distributions over data and ease of implementation\nwith standard deep learning frameworks, while requiring reasonable memory and\ncomputation and achieving state-of-the-art results on standard generative\nmodeling benchmarks. Our models are based on axial attention, a simple\ngeneralization of self-attention that naturally aligns with the multiple\ndimensions of the tensors in both the encoding and the decoding settings.\nNotably the proposed structure of the layers allows for the vast majority of\nthe context to be computed in parallel during decoding without introducing any\nindependence assumptions. This semi-parallel structure goes a long way to\nmaking decoding from even a very large Axial Transformer broadly applicable. We\ndemonstrate state-of-the-art results for the Axial Transformer on the\nImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic\nPushing video benchmark. We open source the implementation of Axial\nTransformers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:27:27 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ho", "Jonathan", ""], ["Kalchbrenner", "Nal", ""], ["Weissenborn", "Dirk", ""], ["Salimans", "Tim", ""]]}, {"id": "1912.12184", "submitter": "Yenwu Ti", "authors": "Chia-Mu Yu, Ching-Tang Chang, Yen-Wu Ti", "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural\n  Network and Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in AI technology have made the forgery of digital images and\nvideos easier, and it has become significantly more difficult to identify such\nforgeries. These forgeries, if disseminated with malicious intent, can\nnegatively impact social and political stability, and pose significant ethical\nand legal challenges as well. Deepfake is a variant of auto-encoders that use\ndeep learning techniques to identify and exchange images of a person's face in\na picture or film. Deepfake can result in an erosion of public trust in digital\nimages and videos, which has far-reaching effects on political and social\nstability. This study therefore proposes a solution for facial forgery\ndetection to determine if a picture or film has ever been processed by\nDeepfake. The proposed solution reaches detection efficiency by using the\nrecently proposed separable convolutional neural network (CNN) and image\nsegmentation. In addition, this study also examined how different image\nsegmentation methods affect detection results. Finally, the ensemble model is\nused to improve detection capabilities. Experiment results demonstrated the\nexcellent performance of the proposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 08:32:27 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yu", "Chia-Mu", ""], ["Chang", "Ching-Tang", ""], ["Ti", "Yen-Wu", ""]]}, {"id": "1912.12186", "submitter": "Hu Wang", "authors": "Hu Wang, Guansong Pang, Chunhua Shen, Congbo Ma", "title": "Unsupervised Representation Learning by Predicting Random Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have gained tremendous success in a broad range of\nmachine learning tasks due to its remarkable capability to learn semantic-rich\nfeatures from high-dimensional data. However, they often require large-scale\nlabelled data to successfully learn such features, which significantly hinders\ntheir adaption into unsupervised learning tasks, such as anomaly detection and\nclustering, and limits their applications into critical domains where obtaining\nmassive labelled data is prohibitively expensive. To enable unsupervised\nlearning on those domains, in this work we propose to learn features without\nusing any labelled data by training neural networks to predict data distances\nin a randomly projected space. Random mapping is a theoretically proven\napproach to obtain approximately preserved distances. To well predict these\nrandom distances, the representation learner is optimised to learn genuine\nclass structures that are implicitly embedded in the randomly projected space.\nEmpirical results on 19 real-world datasets show that our learned\nrepresentations substantially outperform a few state-of-the-art competing\nmethods in both anomaly detection and clustering tasks. Code is available at\nhttps://git.io/RDP\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 05:09:11 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 11:57:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Hu", ""], ["Pang", "Guansong", ""], ["Shen", "Chunhua", ""], ["Ma", "Congbo", ""]]}, {"id": "1912.12191", "submitter": "Sukriti Verma", "authors": "Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad\n  Deshmukh, Balaji Krishnamurthy, Sameer Singh", "title": "Explain Your Move: Understanding Agent Actions Using Specific and\n  Relevant Feature Attribution", "comments": "Accepted at the International Conference on Learning Representations\n  (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep reinforcement learning (RL) is applied to more tasks, there is a need\nto visualize and understand the behavior of learned agents. Saliency maps\nexplain agent behavior by highlighting the features of the input state that are\nmost relevant for the agent in taking an action. Existing perturbation-based\napproaches to compute saliency often highlight regions of the input that are\nnot relevant to the action taken by the agent. Our proposed approach, SARFA\n(Specific and Relevant Feature Attribution), generates more focused saliency\nmaps by balancing two aspects (specificity and relevance) that capture\ndifferent desiderata of saliency. The first captures the impact of perturbation\non the relative expected reward of the action to be explained. The second\ndownweighs irrelevant features that alter the relative expected rewards of\nactions other than the action to be explained. We compare SARFA with existing\napproaches on agents trained to play board games (Chess and Go) and Atari games\n(Breakout, Pong and Space Invaders). We show through illustrative examples\n(Chess, Atari, Go), human studies (Chess), and automated evaluation methods\n(Chess) that SARFA generates saliency maps that are more interpretable for\nhumans than existing approaches. For the code release and demo videos, see\nhttps://nikaashpuri.github.io/sarfa-saliency/.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:52:15 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 06:59:01 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 12:51:36 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 20:27:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Puri", "Nikaash", ""], ["Verma", "Sukriti", ""], ["Gupta", "Piyush", ""], ["Kayastha", "Dhruv", ""], ["Deshmukh", "Shripad", ""], ["Krishnamurthy", "Balaji", ""], ["Singh", "Sameer", ""]]}, {"id": "1912.12215", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Yan Yan, Philip H. S. Torr, Nicu Sebe", "title": "Local Class-Specific and Global Image-Level Generative Adversarial\n  Networks for Semantic-Guided Scene Generation", "comments": "Accepted to CVPR 2020, camera ready (10 pages) + supplementary (18\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of semantic-guided scene generation. One\nopen challenge in scene generation is the difficulty of the generation of small\nobjects and detailed local texture, which has been widely observed in global\nimage-level generation methods. To tackle this issue, in this work we consider\nlearning the scene generation in a local context, and correspondingly design a\nlocal class-specific generative network with semantic maps as a guidance, which\nseparately constructs and learns sub-generators concentrating on the generation\nof different classes, and is able to provide more scene details. To learn more\ndiscriminative class-specific feature representations for the local generation,\na novel classification module is also proposed. To combine the advantage of\nboth the global image-level and the local class-specific generation, a joint\ngeneration network is designed with an attention fusion module and a\ndual-discriminator structure embedded. Extensive experiments on two scene image\ngeneration tasks show superior generation performance of the proposed model.\nThe state-of-the-art results are established by large margins on both tasks and\non challenging public benchmarks. The source code and trained models are\navailable at https://github.com/Ha0Tang/LGGAN.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:14:53 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 21:45:48 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 01:31:12 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Yan", "Yan", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "1912.12270", "submitter": "Siddharth Ancha", "authors": "Siddharth Ancha, Junyu Nan, David Held", "title": "Combining Deep Learning and Verification for Precise Object Instance\n  Detection", "comments": "9 pages main paper, 2 pages references, 10 pages supplementary\n  material", "journal-ref": "Conference on Robot Learning (CoRL), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning object detectors often return false positives with very high\nconfidence. Although they optimize generic detection performance, such as mean\naverage precision (mAP), they are not designed for reliability. For a reliable\ndetection system, if a high confidence detection is made, we would want high\ncertainty that the object has indeed been detected. To achieve this, we have\ndeveloped a set of verification tests which a proposed detection must pass to\nbe accepted. We develop a theoretical framework which proves that, under\ncertain assumptions, our verification tests will not accept any false\npositives. Based on an approximation to this framework, we present a practical\ndetection system that can verify, with high precision, whether each detection\nof a machine-learning based object detector is correct. We show that these\ntests can improve the overall accuracy of a base detector and that accepted\nexamples are highly likely to be correct. This allows the detector to operate\nin a high precision regime and can thus be used for robotic perception systems\nas a reliable instance detection method. Code is available at\nhttps://github.com/siddancha/FlowVerify.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:11:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 20:51:23 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 21:40:48 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 15:18:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ancha", "Siddharth", ""], ["Nan", "Junyu", ""], ["Held", "David", ""]]}, {"id": "1912.12290", "submitter": "Louren\\c{c}o Vaz Pato", "authors": "Louren\\c{c}o V. Pato, Renato Negrinho, Pedro M. Q. Aguiar", "title": "Seeing without Looking: Contextual Rescoring of Object Detections for AP\n  Maximization", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of current object detectors lack context: class predictions are\nmade independently from other detections. We propose to incorporate context in\nobject detection by post-processing the output of an arbitrary detector to\nrescore the confidences of its detections. Rescoring is done by conditioning on\ncontextual information from the entire set of detections: their confidences,\npredicted classes, and positions. We show that AP can be improved by simply\nreassigning the detection confidence values such that true positives that\nsurvive longer (i.e., those with the correct class and large IoU) are scored\nhigher than false positives or detections with small IoU. In this setting, we\nuse a bidirectional RNN with attention for contextual rescoring and introduce a\ntraining target that uses the IoU with ground truth to maximize AP for the\ngiven set of detections. The fact that our approach does not require access to\nvisual features makes it computationally inexpensive and agnostic to the\ndetection architecture. In spite of this simplicity, our model consistently\nimproves AP over strong pre-trained baselines (Cascade R-CNN and Faster R-CNN\nwith several backbones), particularly by reducing the confidence of duplicate\ndetections (a learned form of non-maximum suppression) and removing\nout-of-context objects by conditioning on the confidences, classes, positions,\nand sizes of the co-occurrent detections. Code is available at\nhttps://github.com/LourencoVazPato/seeing-without-looking/\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:56:29 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 16:09:20 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Pato", "Louren\u00e7o V.", ""], ["Negrinho", "Renato", ""], ["Aguiar", "Pedro M. Q.", ""]]}, {"id": "1912.12294", "submitter": "Dian Chen", "authors": "Dian Chen and Brady Zhou and Vladlen Koltun and Philipp Kr\\\"ahenb\\\"uhl", "title": "Learning by Cheating", "comments": "Paper published in CoRL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based urban driving is hard. The autonomous system needs to learn to\nperceive the world and act in it. We show that this challenging learning\nproblem can be simplified by decomposing it into two stages. We first train an\nagent that has access to privileged information. This privileged agent cheats\nby observing the ground-truth layout of the environment and the positions of\nall traffic participants. In the second stage, the privileged agent acts as a\nteacher that trains a purely vision-based sensorimotor agent. The resulting\nsensorimotor agent does not have access to any privileged information and does\nnot cheat. This two-stage training procedure is counter-intuitive at first, but\nhas a number of important advantages that we analyze and empirically\ndemonstrate. We use the presented approach to train a vision-based autonomous\ndriving system that substantially outperforms the state of the art on the CARLA\nbenchmark and the recent NoCrash benchmark. Our approach achieves, for the\nfirst time, 100% success rate on all tasks in the original CARLA benchmark,\nsets a new record on the NoCrash benchmark, and reduces the frequency of\ninfractions by an order of magnitude compared to the prior state of the art.\nFor the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:59:04 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Chen", "Dian", ""], ["Zhou", "Brady", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1912.12296", "submitter": "Vladislav Golyanik", "authors": "Vladislav Golyanik and Christian Theobalt", "title": "A Quantum Computational Approach to Correspondence Problems on Point\n  Sets", "comments": "11 pages, 5 figures, 2 tables, CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern adiabatic quantum computers (AQC) are already used to solve difficult\ncombinatorial optimisation problems in various domains of science. Currently,\nonly a few applications of AQC in computer vision have been demonstrated. We\nreview AQC and derive a new algorithm for correspondence problems on point sets\nsuitable for execution on AQC. Our algorithm has a subquadratic computational\ncomplexity of the state preparation. Examples of successful transformation\nestimation and point set alignment by simulated sampling are shown in the\nsystematic experimental evaluation. Finally, we analyse the differences in the\nsolutions and the corresponding energy values.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 05:48:33 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 18:01:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "1912.12318", "submitter": "Xiaofeng Yang", "authors": "Yabo Fu, Yang Lei, Tonghe Wang, Walter J. Curran, Tian Liu, Xiaofeng\n  Yang", "title": "Deep Learning in Medical Image Registration: A Review", "comments": "32 pages, 4 figures, 9 tables", "journal-ref": null, "doi": "10.1088/1361-6560/ab843e", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a review of deep learning (DL) based medical image\nregistration methods. We summarized the latest developments and applications of\nDL-based registration methods in the medical field. These methods were\nclassified into seven categories according to their methods, functions and\npopularity. A detailed review of each category was presented, highlighting\nimportant contributions and identifying specific challenges. A short assessment\nwas presented following the detailed review of each category to summarize its\nachievements and future potentials. We provided a comprehensive comparison\namong DL-based methods for lung and brain deformable registration using\nbenchmark datasets. Lastly, we analyzed the statistics of all the cited works\nfrom various aspects, revealing the popularity and future trend of development\nin medical image registration using deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 19:32:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Fu", "Yabo", ""], ["Lei", "Yang", ""], ["Wang", "Tonghe", ""], ["Curran", "Walter J.", ""], ["Liu", "Tian", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "1912.12325", "submitter": "Ali Pour Yazdanpanah", "authors": "Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield", "title": "ODE-based Deep Network for MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand\nand scan time directly depends on the number of acquired k-space samples. The\ndata-driven methods based on deep neural networks have resulted in promising\nimprovements, compared to the conventional methods, in image reconstruction\nalgorithms. The connection between deep neural network and Ordinary\nDifferential Equation (ODE) has been observed and studied recently. The studies\nshow that different residual networks can be interpreted as Euler\ndiscretization of an ODE. In this paper, we propose an ODE-based deep network\nfor MRI reconstruction to enable the rapid acquisition of MR images with\nimproved image quality. Our results with undersampled data demonstrate that our\nmethod can deliver higher quality images in comparison to the reconstruction\nmethods based on the standard UNet network and Residual network.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 20:13:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Yazdanpanah", "Ali Pour", ""], ["Afacan", "Onur", ""], ["Warfield", "Simon K.", ""]]}, {"id": "1912.12378", "submitter": "Chetan Srinidhi L", "authors": "Chetan L. Srinidhi, Ozan Ciga, Anne L. Martel", "title": "Deep neural network models for computational histopathology: A survey", "comments": "Published in Medical Image Analysis, Vol. 67, Jan 2021.\n  (10.1016/j.media.2020.101813)", "journal-ref": "Medical Image Analysis, Vol. 67, Jan 2021", "doi": "10.1016/j.media.2020.101813", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological images contain rich phenotypic information that can be used\nto monitor underlying mechanisms contributing to diseases progression and\npatient survival outcomes. Recently, deep learning has become the mainstream\nmethodological choice for analyzing and interpreting cancer histology images.\nIn this paper, we present a comprehensive review of state-of-the-art deep\nlearning approaches that have been used in the context of histopathological\nimage analysis. From the survey of over 130 papers, we review the fields\nprogress based on the methodological aspect of different machine learning\nstrategies such as supervised, weakly supervised, unsupervised, transfer\nlearning and various other sub-variants of these methods. We also provide an\noverview of deep learning based survival models that are applicable for\ndisease-specific prognosis tasks. Finally, we summarize several existing open\ndatasets and highlight critical challenges and limitations with current deep\nlearning approaches, along with possible avenues for future research.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 01:04:25 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:49:53 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Srinidhi", "Chetan L.", ""], ["Ciga", "Ozan", ""], ["Martel", "Anne L.", ""]]}, {"id": "1912.12385", "submitter": "Zhiqiang Gong", "authors": "Zhiqiang Gong, Ping Zhong, Weidong Hu", "title": "Statistical Loss and Analysis for Deep Learning in Hyperspectral Image\n  Classification", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep learning methods, especially the convolutional neural networks\n(CNNs), have shown impressive performance on extracting abstract and high-level\nfeatures from the hyperspectral image. However, general training process of\nCNNs mainly considers the pixel-wise information or the samples' correlation to\nformulate the penalization while ignores the statistical properties especially\nthe spectral variability of each class in the hyperspectral image. These\nsamples-based penalizations would lead to the uncertainty of the training\nprocess due to the imbalanced and limited number of training samples. To\novercome this problem, this work characterizes each class from the\nhyperspectral image as a statistical distribution and further develops a novel\nstatistical loss with the distributions, not directly with samples for deep\nlearning. Based on the Fisher discrimination criterion, the loss penalizes the\nsample variance of each class distribution to decrease the intra-class variance\nof the training samples. Moreover, an additional diversity-promoting condition\nis added to enlarge the inter-class variance between different class\ndistributions and this could better discriminate samples from different classes\nin hyperspectral image. Finally, the statistical estimation form of the\nstatistical loss is developed with the training samples through multi-variant\nstatistical analysis. Experiments over the real-world hyperspectral images show\nthe effectiveness of the developed statistical loss for deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 02:41:49 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 07:01:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gong", "Zhiqiang", ""], ["Zhong", "Ping", ""], ["Hu", "Weidong", ""]]}, {"id": "1912.12394", "submitter": "Da Ju", "authors": "Da Ju, Kurt Shuster, Y-Lan Boureau, Jason Weston", "title": "All-in-One Image-Grounded Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As single-task accuracy on individual language and image tasks has improved\nsubstantially in the last few years, the long-term goal of a generally skilled\nagent that can both see and talk becomes more feasible to explore. In this\nwork, we focus on leveraging individual language and image tasks, along with\nresources that incorporate both vision and language towards that objective. We\ndesign an architecture that combines state-of-the-art Transformer and ResNeXt\nmodules fed into a novel attentive multimodal module to produce a combined\nmodel trained on many tasks. We provide a thorough analysis of the components\nof the model, and transfer performance when training on one, some, or all of\nthe tasks. Our final models provide a single system that obtains good results\non all vision and language tasks considered, and improves the state-of-the-art\nin image-grounded conversational applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 03:51:52 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 23:10:55 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Ju", "Da", ""], ["Shuster", "Kurt", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""]]}, {"id": "1912.12396", "submitter": "Jingtao Guo", "authors": "Jingtao Guo, Zhenzhen Qian, Zuowei Zhou and Yi Liu", "title": "MulGAN: Facial Attribute Editing by Exemplar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on face attribute editing by exemplars have achieved promising\nresults due to the increasing power of deep convolutional networks and\ngenerative adversarial networks. These methods encode attribute-related\ninformation in images into the predefined region of the latent feature space by\nemploying a pair of images with opposite attributes as input to train model,\nthe face attribute transfer between the input image and the exemplar can be\nachieved by exchanging their attribute-related latent feature region. However,\nthey suffer from three limitations: (1) the model must be trained using a pair\nof images with opposite attributes as input; (2) weak capability of editing\nmultiple attributes by exemplars; (3) poor quality of generating image. Instead\nof imposing opposite-attribute constraints on the input image in order to make\nthe attribute information of images be encoded in the predefined region of the\nlatent feature space, in this work we directly apply the attribute labels\nconstraint to the predefined region of the latent feature space. Meanwhile, an\nattribute classification loss is employed to make the model learn to extract\nthe attribute-related information of images into the predefined latent feature\nregion of the corresponding attribute, which enables our method to transfer\nmultiple attributes of the exemplar simultaneously. Besides, a novel model\nstructure is designed to enhance attribute transfer capabilities by exemplars\nwhile improve the quality of the generated image. Experiments demonstrate the\neffectiveness of our model on overcoming the above three limitations by\ncomparing with other methods on the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 04:02:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Guo", "Jingtao", ""], ["Qian", "Zhenzhen", ""], ["Zhou", "Zuowei", ""], ["Liu", "Yi", ""]]}, {"id": "1912.12405", "submitter": "Animesh Singh", "authors": "Animesh Singh, Sandip Saha, Ritesh Sarkhel, Mahantapas Kundu, Mita\n  Nasipuri, Nibaran Das", "title": "A Genetic Algorithm based Kernel-size Selection Approach for a\n  Multi-column Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network-based architectures give promising results in various\ndomains including pattern recognition. Finding the optimal combination of the\nhyper-parameters of such a large-sized architecture is tedious and requires a\nlarge number of laboratory experiments. But, identifying the optimal\ncombination of a hyper-parameter or appropriate kernel size for a given\narchitecture of deep learning is always a challenging and tedious task. Here,\nwe introduced a genetic algorithm-based technique to reduce the efforts of\nfinding the optimal combination of a hyper-parameter (kernel size) of a\nconvolutional neural network-based architecture. The method is evaluated on\nthree popular datasets of different handwritten Bangla characters and digits.\nThe implementation of the proposed methodology can be found in the following\nlink: https://github.com/DeepQn/GA-Based-Kernel-Size.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 05:37:28 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 17:06:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Singh", "Animesh", ""], ["Saha", "Sandip", ""], ["Sarkhel", "Ritesh", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Das", "Nibaran", ""]]}, {"id": "1912.12408", "submitter": "Songtao He", "authors": "Songtao He, Favyen Bastani, Satvat Jagwani, Edward Park, Sofiane\n  Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Samuel Madden,\n  Mohammad Amin Sadeghi", "title": "RoadTagger: Robust Road Attribute Inference with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring road attributes such as lane count and road type from satellite\nimagery is challenging. Often, due to the occlusion in satellite imagery and\nthe spatial correlation of road attributes, a road attribute at one position on\na road may only be apparent when considering far-away segments of the road.\nThus, to robustly infer road attributes, the model must integrate scattered\ninformation and capture the spatial correlation of features along roads.\nExisting solutions that rely on image classifiers fail to capture this\ncorrelation, resulting in poor accuracy. We find this failure is caused by a\nfundamental limitation -- the limited effective receptive field of image\nclassifiers. To overcome this limitation, we propose RoadTagger, an end-to-end\narchitecture which combines both Convolutional Neural Networks (CNNs) and Graph\nNeural Networks (GNNs) to infer road attributes. The usage of graph neural\nnetworks allows information propagation on the road network graph and\neliminates the receptive field limitation of image classifiers. We evaluate\nRoadTagger on both a large real-world dataset covering 688 km^2 area in 20 U.S.\ncities and a synthesized micro-dataset. In the evaluation, RoadTagger improves\ninference accuracy over the CNN image classifier based approaches. RoadTagger\nalso demonstrates strong robustness against different disruptions in the\nsatellite imagery and the ability to learn complicated inductive rules for\naggregating scattered information along the road network.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 06:09:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["He", "Songtao", ""], ["Bastani", "Favyen", ""], ["Jagwani", "Satvat", ""], ["Park", "Edward", ""], ["Abbar", "Sofiane", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Chawla", "Sanjay", ""], ["Madden", "Samuel", ""], ["Sadeghi", "Mohammad Amin", ""]]}, {"id": "1912.12410", "submitter": "Masakiyo Kitazawa", "authors": "Masakiyo Kitazawa, Takuya Matsumoto, Yasuhiro Kohno", "title": "Classifying topological sector via machine learning", "comments": "7 pages, 4 figures, 4 tables, talk presented at the 37th\n  International Symposium on Lattice Field Theory - Lattice 2019, 16-22 June\n  2019, Wuhan, China", "journal-ref": null, "doi": null, "report-no": "J-PARC-TH-0211", "categories": "hep-lat cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ a machine learning technique for an estimate of the topological\ncharge $Q$ of gauge configurations in SU(3) Yang-Mills theory in vacuum. As a\nfirst trial, we feed the four-dimensional topological charge density with and\nwithout smoothing into the convolutional neural network and train it to\nestimate the value of $Q$. We find that the trained neural network can estimate\nthe value of $Q$ from the topological charge density at small flow time with\nhigh accuracy. Next, we perform the dimensional reduction of the input data as\na preprocessing and analyze lower dimensional data by the neural network. We\nfind that the accuracy of the neural network does not have\nstatistically-significant dependence on the dimension of the input data. From\nthis result we argue that the neural network does not find characteristic\nfeatures responsible for the determination of $Q$ in the higher dimensional\nspace.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 06:22:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kitazawa", "Masakiyo", ""], ["Matsumoto", "Takuya", ""], ["Kohno", "Yasuhiro", ""]]}, {"id": "1912.12422", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai and Cong Yao", "title": "TextScanner: Reading Characters in Order for Robust Scene Text\n  Recognition", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by deep learning and the large volume of data, scene text recognition\nhas evolved rapidly in recent years. Formerly, RNN-attention based methods have\ndominated this field, but suffer from the problem of \\textit{attention drift}\nin certain situations. Lately, semantic segmentation based algorithms have\nproven effective at recognizing text of different forms (horizontal, oriented\nand curved). However, these methods may produce spurious characters or miss\ngenuine characters, as they rely heavily on a thresholding procedure operated\non segmentation maps. To tackle these challenges, we propose in this paper an\nalternative approach, called TextScanner, for scene text recognition.\nTextScanner bears three characteristics: (1) Basically, it belongs to the\nsemantic segmentation family, as it generates pixel-wise, multi-channel\nsegmentation maps for character class, position and order; (2) Meanwhile, akin\nto RNN-attention based methods, it also adopts RNN for context modeling; (3)\nMoreover, it performs paralleled prediction for character position and class,\nand ensures that characters are transcripted in correct order. The experiments\non standard benchmark datasets demonstrate that TextScanner outperforms the\nstate-of-the-art methods. Moreover, TextScanner shows its superiority in\nrecognizing more difficult text such Chinese transcripts and aligning with\ntarget characters.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 07:52:00 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 10:18:26 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Wan", "Zhaoyi", ""], ["He", "Minghang", ""], ["Chen", "Haoran", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""]]}, {"id": "1912.12431", "submitter": "Hong Wu", "authors": "Fiseha B. Tesema, Hong Wu, Mingjian Chen, Junpeng Lin, William Zhu,\n  Kaizhu Huang", "title": "Hybrid Channel Based Pedestrian Detection", "comments": "9 pages, 4 figures, Submitted to Neurocomputing, The 5th line of\n  table 3 was accidentally mistaken. The data have been corrected and the\n  related descriptions in section 4.4 have also be revised accordingly. Typos\n  corrected, references corrected", "journal-ref": "Neurocomputing, 389(5), 2020, 1-8", "doi": "10.1016/j.neucom.2019.12.110", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection has achieved great improvements with the help of\nConvolutional Neural Networks (CNNs). CNN can learn high-level features from\ninput images, but the insufficient spatial resolution of CNN feature channels\n(feature maps) may cause a loss of information, which is harmful especially to\nsmall instances. In this paper, we propose a new pedestrian detection\nframework, which extends the successful RPN+BF framework to combine handcrafted\nfeatures and CNN features. RoI-pooling is used to extract features from both\nhandcrafted channels (e.g. HOG+LUV, CheckerBoards or RotatedFilters) and CNN\nchannels. Since handcrafted channels always have higher spatial resolution than\nCNN channels, we apply RoI-pooling with larger output resolution to handcrafted\nchannels to keep more detailed information. Our ablation experiments show that\nthe developed handcrafted features can reach better detection accuracy than the\nCNN features extracted from the VGG-16 net, and a performance gain can be\nachieved by combining them. Experimental results on Caltech pedestrian dataset\nwith the original annotations and the improved annotations demonstrate the\neffectiveness of the proposed approach. When using a more advanced RPN in our\nframework, our approach can be further improved and get competitive results on\nboth benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 09:55:35 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 04:20:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Tesema", "Fiseha B.", ""], ["Wu", "Hong", ""], ["Chen", "Mingjian", ""], ["Lin", "Junpeng", ""], ["Zhu", "William", ""], ["Huang", "Kaizhu", ""]]}, {"id": "1912.12436", "submitter": "Hwann-Tzong Chen", "authors": "Kuo-Wei Lee, Shih-Hung Liu, Hwann-Tzong Chen, Koichi Ito", "title": "Silhouette-Net: 3D Hand Pose Estimation from Silhouettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation has received a lot of attention for its wide range of\napplications and has made great progress owing to the development of deep\nlearning. Existing approaches mainly consider different input modalities and\nsettings, such as monocular RGB, multi-view RGB, depth, or point cloud, to\nprovide sufficient cues for resolving variations caused by self occlusion and\nviewpoint change. In contrast, this work aims to address the less-explored idea\nof using minimal information to estimate 3D hand poses. We present a new\narchitecture that automatically learns a guidance from implicit depth\nperception and solves the ambiguity of hand pose through end-to-end training.\nThe experimental results show that 3D hand poses can be accurately estimated\nfrom solely {\\em hand silhouettes} without using depth maps. Extensive\nevaluations on the {\\em 2017 Hands In the Million Challenge} (HIM2017)\nbenchmark dataset further demonstrate that our method achieves comparable or\neven better performance than recent depth-based approaches and serves as the\nstate-of-the-art of its own kind on estimating 3D hand poses from silhouettes.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 10:29:42 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lee", "Kuo-Wei", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Hwann-Tzong", ""], ["Ito", "Koichi", ""]]}, {"id": "1912.12452", "submitter": "Jonas Wacker", "authors": "Jonas Wacker, Marcelo Ladeira, Jos\\'e Eduardo Vaz Nascimento", "title": "Transfer Learning for Brain Tumor Segmentation", "comments": "11 pages, 4 figures, MICCAI BraTS 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are the most common malignant brain tumors that are treated with\nchemoradiotherapy and surgery. Magnetic Resonance Imaging (MRI) is used by\nradiotherapists to manually segment brain lesions and to observe their\ndevelopment throughout the therapy. The manual image segmentation process is\ntime-consuming and results tend to vary among different human raters.\nTherefore, there is a substantial demand for automatic image segmentation\nalgorithms that produce a reliable and accurate segmentation of various brain\ntissue types. Recent advances in deep learning have led to convolutional neural\nnetwork architectures that excel at various visual recognition tasks. They have\nbeen successfully applied to the medical context including medical image\nsegmentation. In particular, fully convolutional networks (FCNs) such as the\nU-Net produce state-of-the-art results in the automatic segmentation of brain\ntumors. MRI brain scans are volumetric and exist in various co-registered\nmodalities that serve as input channels for these FCN architectures. Training\nalgorithms for brain tumor segmentation on this complex input requires large\namounts of computational resources and is prone to overfitting. In this work,\nwe construct FCNs with pretrained convolutional encoders. We show that we can\nstabilize the training process this way and achieve an improvement with respect\nto dice scores and Hausdorff distances. We also test our method on a privately\nobtained clinical dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 12:45:34 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 22:12:40 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wacker", "Jonas", ""], ["Ladeira", "Marcelo", ""], ["Nascimento", "Jos\u00e9 Eduardo Vaz", ""]]}, {"id": "1912.12476", "submitter": "Yang Shen", "authors": "Yue Cao and Yang Shen", "title": "Energy-based Graph Convolutional Networks for Scoring Protein Docking\n  Models", "comments": null, "journal-ref": "Proteins: Structure, Function, and Bioinformatics 88, no. 8\n  (2020): 1091-1099", "doi": "10.1002/prot.25888", "report-no": null, "categories": "q-bio.BM cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structural information about protein-protein interactions, often missing at\nthe interactome scale, is important for mechanistic understanding of cells and\nrational discovery of therapeutics. Protein docking provides a computational\nalternative to predict such information. However, ranking near-native docked\nmodels high among a large number of candidates, often known as the scoring\nproblem, remains a critical challenge. Moreover, estimating model quality, also\nknown as the quality assessment problem, is rarely addressed in protein\ndocking.\n  In this study the two challenging problems in protein docking are regarded as\nrelative and absolute scoring, respectively, and addressed in one\nphysics-inspired deep learning framework. We represent proteins' and encounter\ncomplexes' 3D structures as intra- and inter-molecular residue contact graphs\nwith atom-resolution node and edge features. And we propose a novel graph\nconvolutional kernel that pool interacting nodes' features through edge\nfeatures so that generalized interaction energies can be learned directly from\ngraph data. The resulting energy-based graph convolutional networks (EGCN) with\nmulti-head attention are trained to predict intra- and inter-molecular\nenergies, binding affinities, and quality measures (interface RMSD) for\nencounter complexes. Compared to a state-of-the-art scoring function for model\nranking, EGCN has significantly improved ranking for a CAPRI test set involving\nhomology docking; and is comparable for Score_set, a CAPRI benchmark set\ngenerated by diverse community-wide docking protocols not known to training\ndata. For Score_set quality assessment, EGCN shows about 27% improvement to our\nprevious efforts. Directly learning from 3D structure data in graph\nrepresentation, EGCN represents the first successful development of graph\nconvolutional networks for protein docking.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 15:57:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Cao", "Yue", ""], ["Shen", "Yang", ""]]}, {"id": "1912.12485", "submitter": "Song Tao", "authors": "Song Tao, Jia Wang", "title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to alleviate the notorious mode collapse phenomenon in generative\nadversarial networks (GANs), we propose a novel training method of GANs in\nwhich certain fake samples are considered as real ones during the training\nprocess. This strategy can reduce the gradient value that generator receives in\nthe region where gradient exploding happens. We show the process of an\nunbalanced generation and a vicious circle issue resulted from gradient\nexploding in practical training, which explains the instability of GANs. We\nalso theoretically prove that gradient exploding can be alleviated by\npenalizing the difference between discriminator outputs and fake-as-real\nconsideration for very close real and fake samples. Accordingly, Fake-As-Real\nGAN (FARGAN) is proposed with a more stable training process and a more\nfaithful generated distribution. Experiments on different datasets verify our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 16:49:13 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 10:34:08 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Tao", "Song", ""], ["Wang", "Jia", ""]]}, {"id": "1912.12510", "submitter": "Chandramouli Shama Sastry", "authors": "Chandramouli Shama Sastry, Sageev Oore", "title": "Detecting Out-of-Distribution Examples with In-distribution Examples and\n  Gram Matrices", "comments": "NeurIPS 2019 Workshop on Safety and Robustness in Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When presented with Out-of-Distribution (OOD) examples, deep neural networks\nyield confident, incorrect predictions. Detecting OOD examples is challenging,\nand the potential risks are high. In this paper, we propose to detect OOD\nexamples by identifying inconsistencies between activity patterns and class\npredicted. We find that characterizing activity patterns by Gram matrices and\nidentifying anomalies in gram matrix values can yield high OOD detection rates.\nWe identify anomalies in the gram matrices by simply comparing each value with\nits respective range observed over the training data. Unlike many approaches,\nthis can be used with any pre-trained softmax classifier and does not require\naccess to OOD data for fine-tuning hyperparameters, nor does it require OOD\naccess for inferring parameters. The method is applicable across a variety of\narchitectures and vision datasets and, for the important and surprisingly hard\ntask of detecting far-from-distribution out-of-distribution examples, it\ngenerally performs better than or equal to state-of-the-art OOD detection\nmethods (including those that do assume access to OOD examples).\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 19:44:03 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:17:55 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Sastry", "Chandramouli Shama", ""], ["Oore", "Sageev", ""]]}, {"id": "1912.12522", "submitter": "Antoine Yang", "authors": "Antoine Yang, Pedro M. Esperan\\c{c}a, Fabio M. Carlucci", "title": "NAS evaluation is frustratingly hard", "comments": "Published as a conference paper at ICLR2020; 13 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an exciting new field which promises to\nbe as much as a game-changer as Convolutional Neural Networks were in 2012.\nDespite many great works leading to substantial improvements on a variety of\ntasks, comparison between different methods is still very much an open issue.\nWhile most algorithms are tested on the same datasets, there is no shared\nexperimental protocol followed by all. As such, and due to the under-use of\nablation studies, there is a lack of clarity regarding why certain methods are\nmore effective than others. Our first contribution is a benchmark of $8$ NAS\nmethods on $5$ datasets. To overcome the hurdle of comparing methods with\ndifferent search spaces, we propose using a method's relative improvement over\nthe randomly sampled average architecture, which effectively removes advantages\narising from expertly engineered search spaces or training protocols.\nSurprisingly, we find that many NAS techniques struggle to significantly beat\nthe average architecture baseline. We perform further experiments with the\ncommonly used DARTS search space in order to understand the contribution of\neach component in the NAS pipeline. These experiments highlight that: (i) the\nuse of tricks in the evaluation protocol has a predominant impact on the\nreported performance of architectures; (ii) the cell-based search space has a\nvery narrow accuracy range, such that the seed has a considerable impact on\narchitecture rankings; (iii) the hand-designed macro-structure (cells) is more\nimportant than the searched micro-structure (operations); and (iv) the\ndepth-gap is a real phenomenon, evidenced by the change in rankings between $8$\nand $20$ cell architectures. To conclude, we suggest best practices, that we\nhope will prove useful for the community and help mitigate current NAS\npitfalls. The code used is available at\nhttps://github.com/antoyang/NAS-Benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:24:12 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 11:42:17 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 22:10:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yang", "Antoine", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Carlucci", "Fabio M.", ""]]}, {"id": "1912.12533", "submitter": "Ozan Ciga", "authors": "Ozan Ciga, Anne L. Martel", "title": "Learning to segment images with classification labels", "comments": "Published on Elsevier, Medical Image Analysis", "journal-ref": "Medical Image Analysis, Volume 68, 2021, 101912, ISSN 1361-8415", "doi": "10.1016/j.media.2020.101912", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two of the most common tasks in medical imaging are classification and\nsegmentation. Either task requires labeled data annotated by experts, which is\nscarce and expensive to collect. Annotating data for segmentation is generally\nconsidered to be more laborious as the annotator has to draw around the\nboundaries of regions of interest, as opposed to assigning image patches a\nclass label. Furthermore, in tasks such as breast cancer histopathology, any\nrealistic clinical application often includes working with whole slide images,\nwhereas most publicly available training data are in the form of image patches,\nwhich are given a class label. We propose an architecture that can alleviate\nthe requirements for segmentation-level ground truth by making use of\nimage-level labels to reduce the amount of time spent on data curation. In\naddition, this architecture can help unlock the potential of previously\nacquired image-level datasets on segmentation tasks by annotating a small\nnumber of regions of interest. In our experiments, we show using only one\nsegmentation-level annotation per class, we can achieve performance comparable\nto a fully annotated dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 22:01:39 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 19:56:32 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ciga", "Ozan", ""], ["Martel", "Anne L.", ""]]}, {"id": "1912.12555", "submitter": "Hanwen Kang", "authors": "Hanwen Kang and Chao Chen", "title": "Visual Perception and Modelling in Unstructured Orchard for Apple\n  Harvesting Robots", "comments": "21 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision perception and modelling are the essential tasks of robotic harvesting\nin the unstructured orchard. This paper develops a framework of visual\nperception and modelling for robotic harvesting of fruits in the orchard\nenvironments. The developed framework includes visual perception, scenarios\nmapping, and fruit modelling. The Visual perception module utilises a\ndeep-learning model to perform multi-purpose visual perception task within the\nworking scenarios; The scenarios mapping module applies OctoMap to represent\nthe multiple classes of objects or elements within the environment; The fruit\nmodelling module estimates the geometry property of objects and estimates the\nproper access pose of each fruit. The developed framework is implemented and\nevaluated in the apple orchards. The experiment results show that visual\nperception and modelling algorithm can accurately detect and localise the\nfruits, and modelling working scenarios in real orchard environments. The\n$F_{1}$ score and mean intersection of union of visual perception module on\nfruit detection and segmentation are 0.833 and 0.852, respectively. The\naccuracy of the fruit modelling in terms of centre localisation and pose\nestimation are 0.955 and 0.923, respectively. Overall, an accurate visual\nperception and modelling algorithm are presented in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:30:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kang", "Hanwen", ""], ["Chen", "Chao", ""]]}, {"id": "1912.12557", "submitter": "Sima Behpour", "authors": "Sima Behpour", "title": "Active Learning in Video Tracking", "comments": null, "journal-ref": "In International Conference on Machine Learning, pp. 563-572. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning methods, like uncertainty sampling, combined with\nprobabilistic prediction techniques have achieved success in various problems\nlike image classification and text classification. For more complex\nmultivariate prediction tasks, the relationships between labels play an\nimportant role in designing structured classifiers with better performance.\nHowever, computational time complexity limits prevalent probabilistic methods\nfrom effectively supporting active learning. Specifically, while\nnon-probabilistic methods based on structured support vector machines can be\ntractably applied to predicting bipartite matchings, conditional random fields\nare intractable for these structures. We propose an adversarial approach for\nactive learning with structured prediction domains that is tractable for\nmatching. We evaluate this approach algorithmically in an important structured\nprediction problems: object tracking in videos. We demonstrate better accuracy\nand computational efficiency for our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:42:06 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 05:53:18 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 00:15:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Behpour", "Sima", ""]]}, {"id": "1912.12570", "submitter": "Zhihao Lei", "authors": "Zhihao Lei, Lin Qi, Ying Wei, Yunlong Zhou", "title": "Infant brain MRI segmentation with dilated convolution pyramid\n  downsampling and self-attention", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a dual aggregation network to adaptively aggregate\ndifferent information in infant brain MRI segmentation. More precisely, we\nadded two modules based on 3D-UNet to better model information at different\nlevels and locations. The dilated convolution pyramid downsampling module is\nmainly to solve the problem of loss of spatial information on the downsampling\nprocess, and it can effectively save details while reducing the resolution. The\nself-attention module can integrate the remote dependence on the feature maps\nin two dimensions of spatial and channel, effectively improving the\nrepresentation ability and discriminating ability of the model. Our results are\ncompared to the winners of iseg2017's first evaluation, the DICE ratio of WM\nand GM increased by 0.7%, and CSF is comparable.In the latest evaluation of the\niseg-2019 cross-dataset challenge,we achieve the first place in the DICE of WM\nand GM, and the DICE of CSF is second.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 02:33:57 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 03:01:46 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Lei", "Zhihao", ""], ["Qi", "Lin", ""], ["Wei", "Ying", ""], ["Zhou", "Yunlong", ""]]}, {"id": "1912.12577", "submitter": "Yujing Lou", "authors": "Yujing Lou, Yang You, Chengkun Li, Zhoujun Cheng, Liangwei Li,\n  Lizhuang Ma, Weiming Wang, Cewu Lu", "title": "Human Correspondence Consensus for 3D Object Semantic Understanding", "comments": "18 pages; ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic understanding of 3D objects is crucial in many applications such as\nobject manipulation. However, it is hard to give a universal definition of\npoint-level semantics that everyone would agree on. We observe that people have\na consensus on semantic correspondences between two areas from different\nobjects, but are less certain about the exact semantic meaning of each area.\nTherefore, we argue that by providing human labeled correspondences between\ndifferent objects from the same category instead of explicit semantic labels,\none can recover rich semantic information of an object. In this paper, we\nintroduce a new dataset named CorresPondenceNet. Based on this dataset, we are\nable to learn dense semantic embeddings with a novel geodesic consistency loss.\nAccordingly, several state-of-the-art networks are evaluated on this\ncorrespondence benchmark. We further show that CorresPondenceNet could not only\nboost fine-grained understanding of heterogeneous objects but also cross-object\nregistration and partial object matching.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 04:24:22 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 05:24:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lou", "Yujing", ""], ["You", "Yang", ""], ["Li", "Chengkun", ""], ["Cheng", "Zhoujun", ""], ["Li", "Liangwei", ""], ["Ma", "Lizhuang", ""], ["Wang", "Weiming", ""], ["Lu", "Cewu", ""]]}, {"id": "1912.12607", "submitter": "Ruihao Gong", "authors": "Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong\n  Li, Xiuqi Yang, Junjie Yan", "title": "Towards Unified INT8 Training for Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently low-bit (e.g., 8-bit) network quantization has been extensively\nstudied to accelerate the inference. Besides inference, low-bit training with\nquantized gradients can further bring more considerable acceleration, since the\nbackward process is often computation-intensive. Unfortunately, the\ninappropriate quantization of backward propagation usually makes the training\nunstable and even crash. There lacks a successful unified low-bit training\nframework that can support diverse networks on various tasks. In this paper, we\ngive an attempt to build a unified 8-bit (INT8) training framework for common\nconvolutional neural networks from the aspects of both accuracy and speed.\nFirst, we empirically find the four distinctive characteristics of gradients,\nwhich provide us insightful clues for gradient quantization. Then, we\ntheoretically give an in-depth analysis of the convergence bound and derive two\nprinciples for stable INT8 training. Finally, we propose two universal\ntechniques, including Direction Sensitive Gradient Clipping that reduces the\ndirection deviation of gradients and Deviation Counteractive Learning Rate\nScaling that avoids illegal gradient update along the wrong direction. The\nexperiments show that our unified solution promises accurate and efficient INT8\ntraining for a variety of networks and tasks, including MobileNetV2,\nInceptionV3 and object detection that prior studies have never succeeded.\nMoreover, it enjoys a strong flexibility to run on off-the-shelf hardware, and\nreduces the training time by 22% on Pascal GPU without too much optimization\neffort. We believe that this pioneering study will help lead the community\ntowards a fully unified INT8 training for convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 08:37:53 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhu", "Feng", ""], ["Gong", "Ruihao", ""], ["Yu", "Fengwei", ""], ["Liu", "Xianglong", ""], ["Wang", "Yanfei", ""], ["Li", "Zhelong", ""], ["Yang", "Xiuqi", ""], ["Yan", "Junjie", ""]]}, {"id": "1912.12616", "submitter": "Sherif Tarabishy", "authors": "Sherif Tarabishy, Stamatios Psarras, Marcin Kosicki, Martha Tsigkari", "title": "Deep learning surrogate models for spatial and visual connectivity", "comments": "Accepted manuscript in the International Journal of Architectural\n  Computing (2019)", "journal-ref": null, "doi": "10.1177/1478077119894483", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and visual connectivity are important metrics when developing\nworkplace layouts. Calculating those metrics in real-time can be difficult,\ndepending on the size of the floor plan being analysed and the resolution of\nthe analyses. This paper investigates the possibility of considerably speeding\nup the outcomes of such computationally intensive simulations by using machine\nlearning to create models capable of identifying the spatial and visual\nconnectivity potential of a space. To that end we present the entire process of\ninvestigating different machine learning models and a pipeline for training\nthem on such task, from the incorporation of a bespoke spatial and visual\nconnectivity analysis engine through a distributed computation pipeline, to the\nprocess of synthesizing training data and evaluating the performance of\ndifferent neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 09:17:19 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tarabishy", "Sherif", ""], ["Psarras", "Stamatios", ""], ["Kosicki", "Marcin", ""], ["Tsigkari", "Martha", ""]]}, {"id": "1912.12640", "submitter": "Benedetta Tondi", "authors": "Mauro Barni, Quoc-Tin Phan, Benedetta Tondi", "title": "Copy Move Source-Target Disambiguation through Multi-Branch CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to identify the source and target regions of a copy-move\nforgery so allow a correct localisation of the tampered area. First, we cast\nthe problem into a hypothesis testing framework whose goal is to decide which\nregion between the two nearly-duplicate regions detected by a generic copy-move\ndetector is the original one. Then we design a multi-branch CNN architecture\nthat solves the hypothesis testing problem by learning a set of features\ncapable to reveal the presence of interpolation artefacts and boundary\ninconsistencies in the copy-moved area. The proposed architecture, trained on a\nsynthetic dataset explicitly built for this purpose, achieves good results on\ncopy-move forgeries from both synthetic and realistic datasets. Based on our\ntests, the proposed disambiguation method can reliably reveal the target region\neven in realistic cases where an approximate version of the copy-move\nlocalization mask is provided by a state-of-the-art copy-move detection\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:56:33 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 10:24:36 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Barni", "Mauro", ""], ["Phan", "Quoc-Tin", ""], ["Tondi", "Benedetta", ""]]}, {"id": "1912.12655", "submitter": "Washington Luis De Souza Ramos", "authors": "Washington L. S. Ramos, Michel M. Silva, Edson R. Araujo, Alan C.\n  Neves, Erickson R. Nascimento", "title": "Personalizing Fast-Forward Videos Based on Visual and Textual Features\n  from Social Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of Social Networks has fueled the habit of people logging their\nday-to-day activities, and long First-Person Videos (FPVs) are one of the main\ntools in this new habit. Semantic-aware fast-forward methods are able to\ndecrease the watch time and select meaningful moments, which is key to increase\nthe chances of these videos being watched. However, these methods can not\nhandle semantics in terms of personalization. In this work, we present a new\napproach to automatically creating personalized fast-forward videos for FPVs.\nOur approach explores the availability of text-centric data from the user's\nsocial networks such as status updates to infer her/his topics of interest and\nassigns scores to the input frames according to her/his preferences. Extensive\nexperiments are conducted on three different datasets with simulated and\nreal-world users as input, achieving an average F1 score of up to 12.8\npercentage points higher than the best competitors. We also present a user\nstudy to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:09:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ramos", "Washington L. S.", ""], ["Silva", "Michel M.", ""], ["Araujo", "Edson R.", ""], ["Neves", "Alan C.", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "1912.12656", "submitter": "Tianshu Chu", "authors": "Tianshu Chu, Qin Luo, Jie Yang, Xiaolin Huang", "title": "Mixed-Precision Quantized Neural Network with Progressively Decreasing\n  Bitwidth For Image Classification and Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient model inference is an important and practical issue in the\ndeployment of deep neural network on resource constraint platforms. Network\nquantization addresses this problem effectively by leveraging low-bit\nrepresentation and arithmetic that could be conducted on dedicated embedded\nsystems. In the previous works, the parameter bitwidth is set homogeneously and\nthere is a trade-off between superior performance and aggressive compression.\nActually the stacked network layers, which are generally regarded as\nhierarchical feature extractors, contribute diversely to the overall\nperformance. For a well-trained neural network, the feature distributions of\ndifferent categories differentiate gradually as the network propagates forward.\nHence the capability requirement on the subsequent feature extractors is\nreduced. It indicates that the neurons in posterior layers could be assigned\nwith lower bitwidth for quantized neural networks. Based on this observation, a\nsimple but effective mixed-precision quantized neural network with\nprogressively ecreasing bitwidth is proposed to improve the trade-off between\naccuracy and compression. Extensive experiments on typical network\narchitectures and benchmark datasets demonstrate that the proposed method could\nachieve better or comparable results while reducing the memory space for\nquantized parameters by more than 30\\% in comparison with the homogeneous\ncounterparts. In addition, the results also demonstrate that the\nhigher-precision bottom layers could boost the 1-bit network performance\nappreciably due to a better preservation of the original image information\nwhile the lower-precision posterior layers contribute to the regularization of\n$k-$bit networks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:11:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chu", "Tianshu", ""], ["Luo", "Qin", ""], ["Yang", "Jie", ""], ["Huang", "Xiaolin", ""]]}, {"id": "1912.12674", "submitter": "Haohang Xu", "authors": "Haohang Xu, Hongkai Xiong, Guojun Qi", "title": "FLAT: Few-Shot Learning via Autoencoding Transformation Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most significant challenges facing a few-shot learning task is the\ngeneralizability of the (meta-)model from the base to the novel categories.\nMost of existing few-shot learning models attempt to address this challenge by\neither learning the meta-knowledge from multiple simulated tasks on the base\ncategories, or resorting to data augmentation by applying various\ntransformations to training examples. However, the supervised nature of model\ntraining in these approaches limits their ability of exploring variations\nacross different categories, thus restricting their cross-category\ngeneralizability in modeling novel concepts. To this end, we present a novel\nregularization mechanism by learning the change of feature representations\ninduced by a distribution of transformations without using the labels of data\nexamples. We expect this regularizer could expand the semantic space of base\ncategories to cover that of novel categories through the transformation of\nfeature representations. It could minimize the risk of overfitting into base\ncategories by inspecting the transformation-augmented variations at the encoded\nfeature level. This results in the proposed FLAT (Few-shot Learning via\nAutoencoding Transformations) approach by autoencoding the applied\ntransformations. The experiment results show the superior performances to the\ncurrent state-of-the-art methods in literature.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 15:26:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Xu", "Haohang", ""], ["Xiong", "Hongkai", ""], ["Qi", "Guojun", ""]]}, {"id": "1912.12688", "submitter": "Zongxin Yang", "authors": "Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, Shuicheng Yan", "title": "Very Long Natural Scenery Image Prediction by Outpainting", "comments": "ICCV-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing to image inpainting, image outpainting receives less attention due\nto two challenges in it. The first challenge is how to keep the spatial and\ncontent consistency between generated images and original input. The second\nchallenge is how to maintain high quality in generated results, especially for\nmulti-step generations in which generated regions are spatially far away from\nthe initial input. To solve the two problems, we devise some innovative\nmodules, named Skip Horizontal Connection and Recurrent Content Transfer, and\nintegrate them into our designed encoder-decoder structure. By this design, our\nnetwork can generate highly realistic outpainting prediction effectively and\nefficiently. Other than that, our method can generate new images with very long\nsizes while keeping the same style and semantic content as the given input. To\ntest the effectiveness of the proposed architecture, we collect a new scenery\ndataset with diverse, complicated natural scenes. The experimental results on\nthis dataset have demonstrated the efficacy of our proposed network. The code\nand dataset are available from https://github.com/z-x-yang/NS-Outpainting.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 16:29:01 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Yang", "Zongxin", ""], ["Dong", "Jian", ""], ["Liu", "Ping", ""], ["Yang", "Yi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1912.12717", "submitter": "Steffen Wolf", "authors": "Steffen Wolf, Yuyan Li, Constantin Pape, Alberto Bailoni, Anna\n  Kreshuk, Fred A. Hamprecht", "title": "The Semantic Mutex Watershed for Efficient Bottom-Up Semantic Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic instance segmentation is the task of simultaneously partitioning an\nimage into distinct segments while associating each pixel with a class label.\nIn commonly used pipelines, segmentation and label assignment are solved\nseparately since joint optimization is computationally expensive. We propose a\ngreedy algorithm for joint graph partitioning and labeling derived from the\nefficient Mutex Watershed partitioning algorithm. It optimizes an objective\nfunction closely related to the Symmetric Multiway Cut objective and\nempirically shows efficient scaling behavior. Due to the algorithm's efficiency\nit can operate directly on pixels without prior over-segmentation of the image\ninto superpixels. We evaluate the performance on the Cityscapes dataset (2D\nurban scenes) and on a 3D microscopy volume. In urban scenes, the proposed\nalgorithm combined with current deep neural networks outperforms the strong\nbaseline of `Panoptic Feature Pyramid Networks' by Kirillov et al. (2019). In\nthe 3D electron microscopy images, we show explicitly that our joint\nformulation outperforms a separate optimization of the partitioning and\nlabeling problems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 19:48:39 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wolf", "Steffen", ""], ["Li", "Yuyan", ""], ["Pape", "Constantin", ""], ["Bailoni", "Alberto", ""], ["Kreshuk", "Anna", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1912.12726", "submitter": "Guilherme Nardari", "authors": "Steven W. Chen, Guilherme V. Nardari, Elijah S. Lee, Chao Qu, Xu Liu,\n  Roseli A. F. Romero, Vijay Kumar", "title": "SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory", "comments": "8 pages, 5 figures, IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an end-to-end pipeline for tree diameter estimation\nbased on semantic segmentation and lidar odometry and mapping. Accurate mapping\nof this type of environment is challenging since the ground and the trees are\nsurrounded by leaves, thorns and vines, and the sensor typically experiences\nextreme motion. We propose a semantic feature based pose optimization that\nsimultaneously refines the tree models while estimating the robot pose. The\npipeline utilizes a custom virtual reality tool for labeling 3D scans that is\nused to train a semantic segmentation network. The masked point cloud is used\nto compute a trellis graph that identifies individual instances and extracts\nrelevant features that are used by the SLAM module. We show that traditional\nlidar and image based methods fail in the forest environment on both Unmanned\nAerial Vehicle (UAV) and hand-carry systems, while our method is more robust,\nscalable, and automatically generates tree diameter estimations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 20:38:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Steven W.", ""], ["Nardari", "Guilherme V.", ""], ["Lee", "Elijah S.", ""], ["Qu", "Chao", ""], ["Liu", "Xu", ""], ["Romero", "Roseli A. F.", ""], ["Kumar", "Vijay", ""]]}, {"id": "1912.12735", "submitter": "Hichem Sahbi", "authors": "Mingyuan Jiu and Hichem Sahbi", "title": "Deep Context-Aware Kernel Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context plays a crucial role in visual recognition as it provides\ncomplementary clues for different learning tasks including image classification\nand annotation. As the performances of these tasks are currently reaching a\nplateau, any extra knowledge, including context, should be leveraged in order\nto seek significant leaps in these performances. In the particular scenario of\nkernel machines, context-aware kernel design aims at learning positive\nsemi-definite similarity functions which return high values not only when data\nshare similar contents, but also similar structures (a.k.a contexts). However,\nthe use of context in kernel design has not been fully explored; indeed,\ncontext in these solutions is handcrafted instead of being learned. In this\npaper, we introduce a novel deep network architecture that learns context in\nkernel design. This architecture is fully determined by the solution of an\nobjective function mixing a content term that captures the intrinsic similarity\nbetween data, a context criterion which models their structure and a\nregularization term that helps designing smooth kernel network representations.\nThe solution of this objective function defines a particular deep network\narchitecture whose parameters correspond to different variants of learned\ncontexts including layerwise, stationary and classwise; larger values of these\nparameters correspond to the most influencing contextual relationships between\ndata. Extensive experiments conducted on the challenging ImageCLEF Photo\nAnnotation and Corel5k benchmarks show that our deep context networks are\nhighly effective for image classification and the learned contexts further\nenhance the performance of image annotation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:18:09 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1912.12756", "submitter": "Yue Pan", "authors": "Yue Pan", "title": "Target-less registration of point clouds: A review", "comments": "9 pages, 14 figures, written as the final report of the geomatics\n  seminar at ETH Zurich", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration has been one of the basic steps of point cloud\nprocessing, which has a lot of applications in remote sensing and robotics. In\nthis report, we summarized the basic workflow of target-less point cloud\nregistration,namely correspondence determination and transformation estimation.\nThen we reviewed three commonly used groups of registration approaches, namely\nthe feature matching based methods, the iterative closest points algorithm and\nthe randomly hypothesis and verify based methods. Besides, we analyzed the\nadvantage and disadvantage of these methods are introduced their common\napplication scenarios. At last, we discussed the challenges of current point\ncloud registration methods and proposed several open questions for the future\ndevelopment of automatic registration approaches.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 23:12:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Pan", "Yue", ""]]}, {"id": "1912.12791", "submitter": "Lin Sun", "authors": "Qi Chen, Lin Sun, Zhixin Wang, Kui Jia, Alan Yuille", "title": "Object as Hotspots: An Anchor-Free 3D Object Detection Approach via\n  Firing of Hotspots", "comments": "small affiliation fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D object detection in LiDAR based point clouds suffers from the\nchallenges of data sparsity and irregularities. Existing methods strive to\norganize the points regularly, e.g. voxelize, pass them through a designed\n2D/3D neural network, and then define object-level anchors that predict offsets\nof 3D bounding boxes using collective evidences from all the points on the\nobjects of interest. Contrary to the state-of-the-art anchor-based methods,\nbased on the very nature of data sparsity, we observe that even points on an\nindividual object part are informative about semantic information of the\nobject. We thus argue in this paper for an approach opposite to existing\nmethods using object-level anchors. Inspired by compositional models, which\nrepresent an object as parts and their spatial relations, we propose to\nrepresent an object as composition of its interior non-empty voxels, termed\nhotspots, and the spatial relations of hotspots. This gives rise to the\nrepresentation of Object as Hotspots (OHS). Based on OHS, we further propose an\nanchor-free detection head with a novel ground truth assignment strategy that\ndeals with inter-object point-sparsity imbalance to prevent the network from\nbiasing towards objects with more points. Experimental results show that our\nproposed method works remarkably well on objects with a small number of points.\nNotably, our approach ranked 1st on KITTI 3D Detection Benchmark for cyclist\nand pedestrian detection, and achieved state-of-the-art performance on NuScenes\n3D Detection Benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:02:22 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 01:11:06 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 05:04:42 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chen", "Qi", ""], ["Sun", "Lin", ""], ["Wang", "Zhixin", ""], ["Jia", "Kui", ""], ["Yuille", "Alan", ""]]}, {"id": "1912.12799", "submitter": "Ying Peng", "authors": "Jie Wu, Ying Peng, Chenghao Zheng, Zongbo Hao, Jian Zhang", "title": "PMC-GANs: Generating Multi-Scale High-Quality Pedestrian with Multimodal\n  Cascaded GANs", "comments": "Accepted by The British Machine Vision Conference (BMVC2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative adversarial networks (GANs) have shown great advantages\nin synthesizing images, leading to a boost of explorations of using faked\nimages to augment data. This paper proposes a multimodal cascaded generative\nadversarial networks (PMC-GANs) to generate realistic and diversified\npedestrian images and augment pedestrian detection data. The generator of our\nmodel applies a residual U-net structure, with multi-scale residual blocks to\nencode features, and attention residual blocks to help decode and rebuild\npedestrian images. The model constructs in a coarse-to-fine fashion and adopts\ncascade structure, which is beneficial to produce high-resolution pedestrians.\nPMC-GANs outperforms baselines, and when used for data augmentation, it\nimproves pedestrian detection results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:30:51 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wu", "Jie", ""], ["Peng", "Ying", ""], ["Zheng", "Chenghao", ""], ["Hao", "Zongbo", ""], ["Zhang", "Jian", ""]]}, {"id": "1912.12811", "submitter": "Xinyu Wang", "authors": "Fang Liang, Wenjun Peng, Qinghao Liu, Haijin Wang", "title": "Rethinking Convolutional Features in Correlation Filter Based Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Both accuracy and efficiency are of significant importance to the task of\nvisual object tracking. In recent years, as the surge of deep learning, Deep\nConvolutional NeuralNetwork (DCNN) becomes a very popular choice among the\ntracking community. However, due to the high computational complexity,\nend-to-end visual object trackers can hardly achieve an acceptable inference\ntime and therefore can difficult to be utilized in many real-world\napplications. In this paper, we revisit a hierarchical deep feature-based\nvisual tracker and found that both the performance and efficiency of the deep\ntracker are limited by the poor feature quality. Therefore, we propose a\nfeature selection module to select more discriminative features for the\ntrackers. After removing redundant features, our proposed tracker achieves\nsignificant improvements in both performance and efficiency. Finally,\ncomparisons with state-of-the-art trackers are provided.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 04:39:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Liang", "Fang", ""], ["Peng", "Wenjun", ""], ["Liu", "Qinghao", ""], ["Wang", "Haijin", ""]]}, {"id": "1912.12814", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang\n  Dai, Shuicheng Yan, Jiashi Feng", "title": "RC-DARTS: Resource Constrained Differentiable Architecture Search", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances show that Neural Architectural Search (NAS) method is able to\nfind state-of-the-art image classification deep architectures. In this paper,\nwe consider the one-shot NAS problem for resource constrained applications.\nThis problem is of great interest because it is critical to choose different\narchitectures according to task complexity when the resource is constrained.\nPrevious techniques are either too slow for one-shot learning or does not take\nthe resource constraint into consideration. In this paper, we propose the\nresource constrained differentiable architecture search (RC-DARTS) method to\nlearn architectures that are significantly smaller and faster while achieving\ncomparable accuracy. Specifically, we propose to formulate the RC-DARTS task as\na constrained optimization problem by adding the resource constraint. An\niterative projection method is proposed to solve the given constrained\noptimization problem. We also propose a multi-level search strategy to enable\nlayers at different depths to adaptively learn different types of neural\narchitectures. Through extensive experiments on the Cifar10 and ImageNet\ndatasets, we show that the RC-DARTS method learns lightweight neural\narchitectures which have smaller model size and lower computational complexity\nwhile achieving comparable or better performances than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 05:02:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jin", "Xiaojie", ""], ["Wang", "Jiang", ""], ["Slocum", "Joshua", ""], ["Yang", "Ming-Hsuan", ""], ["Dai", "Shengyang", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1912.12817", "submitter": "Jooyoung Lee", "authors": "Jooyoung Lee, Seunghyun Cho, Munchurl Kim", "title": "An End-to-End Joint Learning Scheme of Image Compression and Quality\n  Enhancement with Improved Entropy Minimization", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learned image compression methods have been actively studied. Among\nthem, entropy-minimization based approaches have achieved superior results\ncompared to conventional image codecs such as BPG and JPEG2000. However, the\nquality enhancement and rate-minimization are conflictively coupled in the\nprocess of image compression. That is, maintaining high image quality entails\nless compression and vice versa. However, by jointly training separate quality\nenhancement in conjunction with image compression, the coding efficiency can be\nimproved. In this paper, we propose a novel joint learning scheme of image\ncompression and quality enhancement, called JointIQ-Net, as well as entropy\nmodel improvement, thus achieving significantly improved coding efficiency\nagainst the previous methods. Our proposed JointIQ-Net combines an image\ncompression sub-network and a quality enhancement sub-network in a cascade,\nboth of which are end-to-end trained in a combined manner within the\nJointIQ-Net. Also the JointIQ-Net benefits from improved entropy-minimization\nthat newly adopts a Gussian Mixture Model (GMM) and further exploits global\ncontext to estimate the probabilities of latent representations. In order to\nshow the effectiveness of our proposed JointIQ-Net, extensive experiments have\nbeen performed, and showed that the JointIQ-Net achieves a remarkable\nperformance improvement in coding efficiency in terms of both PSNR and MS-SSIM,\ncompared to the previous learned image compression methods and the conventional\ncodecs such as VVC Intra (VTM 7.1), BPG, and JPEG2000. To the best of our\nknowledge, this is the first end-to-end optimized image compression method that\noutperforms VTM 7.1 (Intra), the latest reference software of the VVC standard,\nin terms of the PSNR and MS-SSIM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 05:10:05 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 08:45:53 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Lee", "Jooyoung", ""], ["Cho", "Seunghyun", ""], ["Kim", "Munchurl", ""]]}, {"id": "1912.12829", "submitter": "Asma Khatun Dr.", "authors": "Asma Khatun and Sk. Golam Sarowar Hossain", "title": "Early Detection of Diabetic Retinopathy and Severity Scale Measurement:\n  A Progressive Review & Scopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of diabetic retinopathy prevents visual loss and blindness of\na human eye. Based on the types of feature extraction method used, DR detection\nmethod can be broadly classified as Deep Convolutional Neural Network (CNN)\nbased and traditional feature extraction (machine learning) based. This paper\npresents a comprehensive survey of existing feature extraction methods based on\nDeep CNN and conventional feature extraction for DR detection. In addition to\nthat, this paper focuses on the severity scale measurement of the DR detection\nand to the best of our knowledge this is the first survey paper which covers\nseverity grading scale. It is also necessary to mention that this is the first\nstudy which reviews the proposed Deep CNN based method in the state of the art\nfor DR detection methods. This study discovers that recently proposed deep\nlearning based DR detection methods provides higher accuracy than existing\ntraditional feature extraction methods in the literature and also useful in\nlarge scale datasets. However, deep learning based methods require GPU\nimplementation to get the desirable output. The one of the other major finding\nof this paper is that there are no obvious standard severity scale detection\ncriteria to measure the grading. Some used binary class while many other used\nmulti stage class.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 07:06:45 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Khatun", "Asma", ""], ["Hossain", "Sk. Golam Sarowar", ""]]}, {"id": "1912.12838", "submitter": "Tong Zheng", "authors": "Tong Zheng, Hirohisa Oda, Takayasu Moriya, Shota Nakamura, Masahiro\n  Oda, Masaki Mori, Horitsugu Takabatake, Hiroshi Natori and Kensaku Mori", "title": "Multi-modality super-resolution loss for GAN-based super-resolution of\n  clinical CT images using micro CT image database", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper newly introduces multi-modality loss function for GAN-based\nsuper-resolution that can maintain image structure and intensity on unpaired\ntraining dataset of clinical CT and micro CT volumes. Precise non-invasive\ndiagnosis of lung cancer mainly utilizes 3D multidetector computed-tomography\n(CT) data. On the other hand, we can take micro CT images of resected lung\nspecimen in 50 micro meter or higher resolution. However, micro CT scanning\ncannot be applied to living human imaging. For obtaining highly detailed\ninformation such as cancer invasion area from pre-operative clinical CT volumes\nof lung cancer patients, super-resolution (SR) of clinical CT volumes to\n$\\mu$CT level might be one of substitutive solutions. While most SR methods\nrequire paired low- and high-resolution images for training, it is infeasible\nto obtain precisely paired clinical CT and micro CT volumes. We aim to propose\nunpaired SR approaches for clincial CT using micro CT images based on unpaired\nimage translation methods such as CycleGAN or UNIT. Since clinical CT and micro\nCT are very different in structure and intensity, direct application of\nGAN-based unpaired image translation methods in super-resolution tends to\ngenerate arbitrary images. Aiming to solve this problem, we propose new loss\nfunction called multi-modality loss function to maintain the similarity of\ninput images and corresponding output images in super-resolution task.\nExperimental results demonstrated that the newly proposed loss function made\nCycleGAN and UNIT to successfully perform SR of clinical CT images of lung\ncancer patients into micro CT level resolution, while original CycleGAN and\nUNIT failed in super-resolution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 07:49:58 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:06:05 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zheng", "Tong", ""], ["Oda", "Hirohisa", ""], ["Moriya", "Takayasu", ""], ["Nakamura", "Shota", ""], ["Oda", "Masahiro", ""], ["Mori", "Masaki", ""], ["Takabatake", "Horitsugu", ""], ["Natori", "Hiroshi", ""], ["Mori", "Kensaku", ""]]}, {"id": "1912.12847", "submitter": "Yaojun Wu", "authors": "Yaojun Wu, Tianyu He, Zhibo Chen", "title": "Generative Memorize-Then-Recall framework for low bit-rate Surveillance\n  Video Compression", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of surveillance video have developed rapidly in recent years to\nprotect public safety and daily life, which often detect and recognize objects\nin video sequences. Traditional coding frameworks remove temporal redundancy in\nsurveillance video by block-wise motion compensation, lacking the extraction\nand utilization of inherent structure information. In this paper, we figure out\nthis issue by disentangling surveillance video into the structure of a global\nspatio-temporal feature (memory) for Group of Picture (GoP) and skeleton for\neach frame (clue). The memory is obtained by sequentially feeding frame inside\nGoP into a recurrent neural network, describing appearance for objects that\nappeared inside GoP. While the skeleton is calculated by a pose estimator, it\nis regarded as a clue to recall memory. Furthermore, an attention mechanism is\nintroduced to obtain the relation between appearance and skeletons. Finally, we\nemploy generative adversarial network to reconstruct each frame. Experimental\nresults indicate that our method effectively generates realistic reconstruction\nbased on appearance and skeleton, which show much higher compression\nperformance on surveillance video compared with the latest video compression\nstandard H.265.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 08:34:32 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 15:28:53 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 14:28:58 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Wu", "Yaojun", ""], ["He", "Tianyu", ""], ["Chen", "Zhibo", ""]]}, {"id": "1912.12859", "submitter": "Hao Ge", "authors": "Hao Ge, Xiaoguang Tu, Mei Xie, Zheng Ma", "title": "Defending from adversarial examples with a two-stream architecture", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has shown impressive performance on many\ntasks. However, recent researches showed that deep learning systems are\nvulnerable to small, specially crafted perturbations that are imperceptible to\nhumans. Images with such perturbations are the so called adversarial examples,\nwhich have proven to be an indisputable threat to the DNN based applications.\nThe lack of better understanding of the DNNs has prevented the development of\nefficient defenses against adversarial examples. In this paper, we propose a\ntwo-stream architecture to protect CNN from attacking by adversarial examples.\nOur model draws on the idea of \"two-stream\" which commonly used in the security\nfield, and successfully defends different kinds of attack methods by the\ndifferences of \"high-resolution\" and \"low-resolution\" networks in feature\nextraction. We provide a reasonable interpretation on why our two-stream\narchitecture is difficult to defeat, and show experimentally that our method is\nhard to defeat with state-of-the-art attacks. We demonstrate that our\ntwo-stream architecture is robust to adversarial examples built by currently\nknown attacking algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 09:15:40 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ge", "Hao", ""], ["Tu", "Xiaoguang", ""], ["Xie", "Mei", ""], ["Ma", "Zheng", ""]]}, {"id": "1912.12874", "submitter": "Jiaxin Xie", "authors": "Jiaxin Xie, Chenyang Lei, Zhuwen Li, Li Erran Li and Qifeng Chen", "title": "Video Depth Estimation by Fusing Flow-to-Depth Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth from a monocular video can enable billions of devices and robots with a\nsingle camera to see the world in 3D. In this paper, we present an approach\nwith a differentiable flow-to-depth layer for video depth estimation. The model\nconsists of a flow-to-depth layer, a camera pose refinement module, and a depth\nfusion network. Given optical flow and camera pose, our flow-to-depth layer\ngenerates depth proposals and the corresponding confidence maps by explicitly\nsolving an epipolar geometry optimization problem. Our flow-to-depth layer is\ndifferentiable, and thus we can refine camera poses by maximizing the\naggregated confidence in the camera pose refinement module. Our depth fusion\nnetwork can utilize depth proposals and their confidence maps inferred from\ndifferent adjacent frames to produce the final depth map. Furthermore, the\ndepth fusion network can additionally take the depth proposals generated by\nother methods to improve the results further. The experiments on three public\ndatasets show that our approach outperforms state-of-the-art depth estimation\nmethods, and has reasonable cross dataset generalization capability: our model\ntrained on KITTI still performs well on the unseen Waymo dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 10:45:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 06:34:04 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xie", "Jiaxin", ""], ["Lei", "Chenyang", ""], ["Li", "Zhuwen", ""], ["Li", "Li Erran", ""], ["Chen", "Qifeng", ""]]}, {"id": "1912.12883", "submitter": "Filiz Gurkan Golcuk", "authors": "Filiz Gurkan and Bilge Gunsel", "title": "Integration of Regularized l1 Tracking and Instance Segmentation for\n  Video Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.09.072", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a tracking-by-detection method that integrates a deep object\ndetector with a particle filter tracker under the regularization framework\nwhere the tracked object is represented by a sparse dictionary. A novel\nobservation model which establishes consensus between the detector and tracker\nis formulated that enables us to update the dictionary with the guidance of the\ndeep detector. This yields an efficient representation of the object appearance\nthrough the video sequence hence improves robustness to occlusion and pose\nchanges. Moreover we propose a new state vector consisting of translation,\nrotation, scaling and shearing parameters that allows tracking the deformed\nobject bounding boxes hence significantly increases robustness to scale\nchanges. Numerical results reported on challenging VOT2016 and VOT2018\nbenchmarking data sets demonstrate that the introduced tracker, L1DPF-M,\nachieves comparable robustness on both data sets while it outperforms\nstate-of-the-art trackers on both data sets where the improvement achieved in\nsuccess rate at IoU-th=0.5 is 11% and 9%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:14:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gurkan", "Filiz", ""], ["Gunsel", "Bilge", ""]]}, {"id": "1912.12888", "submitter": "Ling Luo", "authors": "Ling Luo, Dingyu Xue, Xinglong Feng, Yichun Yu, Peng Wang", "title": "Real-time Segmentation and Facial Skin Tones Grading", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern approaches for semantic segmention usually pay too much attention to\nthe accuracy of the model, and therefore it is strongly recommended to\nintroduce cumbersome backbones, which brings heavy computation burden and\nmemory footprint. To alleviate this problem, we propose an efficient\nsegmentation method based on deep convolutional neural networks (DCNNs) for the\ntask of hair and facial skin segmentation, which achieving remarkable trade-off\nbetween speed and performance on three benchmark datasets. As far as we know,\nthe accuracy of skin tones classification is usually unsatisfactory due to the\ninfluence of external environmental factors such as illumination and background\nnoise. Therefore, we use the segmentated face to obtain a specific face area,\nand further exploit the color moment algorithm to extract its color features.\nSpecifically, for a 224 x 224 standard input, using our high-resolution spatial\ndetail information and low-resolution contextual information fusion network\n(HLNet), we achieve 90.73% Pixel Accuracy on Figaro1k dataset at over 16 FPS in\nthe case of CPU environment. Additional experiments on CamVid dataset further\nconfirm the universality of the proposed model. We further use masked color\nmoment for skin tones grade evaluation and approximate 80% classification\naccuracy demonstrate the feasibility of the proposed scheme.Code is available\nat\nhttps://github.com/JACKYLUO1991/Face-skin-hair-segmentaiton-and-skin-color-evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:35:36 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 03:52:04 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Luo", "Ling", ""], ["Xue", "Dingyu", ""], ["Feng", "Xinglong", ""], ["Yu", "Yichun", ""], ["Wang", "Peng", ""]]}, {"id": "1912.12898", "submitter": "Yue Liao", "authors": "Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, Jiashi Feng", "title": "PPDM: Parallel Point Detection and Matching for Real-time Human-Object\n  Interaction Detection", "comments": "Accepted to CVPR 2020. Code is available at\n  https://github.com/YueLiao/PPDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a single-stage Human-Object Interaction (HOI) detection method\nthat has outperformed all existing methods on HICO-DET dataset at 37 fps on a\nsingle Titan XP GPU. It is the first real-time HOI detection method.\nConventional HOI detection methods are composed of two stages, i.e.,\nhuman-object proposals generation, and proposals classification. Their\neffectiveness and efficiency are limited by the sequential and separate\narchitecture. In this paper, we propose a Parallel Point Detection and Matching\n(PPDM) HOI detection framework. In PPDM, an HOI is defined as a point triplet <\nhuman point, interaction point, object point>. Human and object points are the\ncenter of the detection boxes, and the interaction point is the midpoint of the\nhuman and object points. PPDM contains two parallel branches, namely point\ndetection branch and point matching branch. The point detection branch predicts\nthree points. Simultaneously, the point matching branch predicts two\ndisplacements from the interaction point to its corresponding human and object\npoints. The human point and the object point originated from the same\ninteraction point are considered as matched pairs. In our novel parallel\narchitecture, the interaction points implicitly provide context and\nregularization for human and object detection. The isolated detection boxes are\nunlikely to form meaning HOI triplets are suppressed, which increases the\nprecision of HOI detection. Moreover, the matching between human and object\ndetection boxes is only applied around limited numbers of filtered candidate\ninteraction points, which saves much computational cost. Additionally, we build\na new application-oriented database named HOI-A, which severs as a good\nsupplement to the existing datasets. The source code and the dataset will be\nmade publicly available to facilitate the development of HOI detection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 12:00:55 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:38:30 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 12:29:07 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liao", "Yue", ""], ["Liu", "Si", ""], ["Wang", "Fei", ""], ["Chen", "Yanjie", ""], ["Qian", "Chen", ""], ["Feng", "Jiashi", ""]]}, {"id": "1912.12932", "submitter": "Regis Pierrard", "authors": "R\\'egis Pierrard (LIST, MICS), Jean-Philippe Poli (LIST), C\\'eline\n  Hudelot (MICS)", "title": "A New Approach for Explainable Multiple Organ Annotation with Few Data", "comments": null, "journal-ref": "IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAI),\n  Aug 2019, Macao, Macau SAR China", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of deep learning, such models are still far from\nsome human abilities like learning from few examples, reasoning and explaining\ndecisions. In this paper, we focus on organ annotation in medical images and we\nintroduce a reasoning framework that is based on learning fuzzy relations on a\nsmall dataset for generating explanations. Given a catalogue of relations, it\nefficiently induces the most relevant relations and combines them for building\nconstraints in order to both solve the organ annotation task and generate\nexplanations. We test our approach on a publicly available dataset of medical\nimages where several organs are already segmented. A demonstration of our model\nis proposed with an example of explained annotations. It was trained on a small\ntraining set containing as few as a couple of examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:06:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Pierrard", "R\u00e9gis", "", "LIST, MICS"], ["Poli", "Jean-Philippe", "", "LIST"], ["Hudelot", "C\u00e9line", "", "MICS"]]}, {"id": "1912.12936", "submitter": "Olga Zatsarynna", "authors": "Olga Zatsarynna, Johann Sawatzky, Juergen Gall", "title": "Discovering Latent Classes for Semi-Supervised Semantic Segmentation", "comments": "In DAGM German Conference on Pattern Recognition (GCPR'20)", "journal-ref": null, "doi": "10.1007/978-3-030-71278-5_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High annotation costs are a major bottleneck for the training of semantic\nsegmentation systems. Therefore, methods working with less annotation effort\nare of special interest. This paper studies the problem of semi-supervised\nsemantic segmentation. This means that only a small subset of the training\nimages is annotated while the other training images do not contain any\nannotation. In order to leverage the information present in the unlabeled\nimages, we propose to learn a second task that is related to semantic\nsegmentation but easier. On labeled images, we learn latent classes consistent\nwith semantic classes so that the variety of semantic classes assigned to a\nlatent class is as low as possible. On unlabeled images, we predict a\nprobability map for latent classes and use it as a supervision signal to learn\nsemantic segmentation. The latent classes, as well as the semantic classes, are\nsimultaneously predicted by a two-branch network. In our experiments on Pascal\nVOC and Cityscapes, we show that the latent classes learned this way have an\nintuitive meaning and that the proposed method achieves state of the art\nresults for semi-supervised semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:16:24 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 17:30:52 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 16:50:47 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 21:16:53 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zatsarynna", "Olga", ""], ["Sawatzky", "Johann", ""], ["Gall", "Juergen", ""]]}, {"id": "1912.12978", "submitter": "Shervan Fekri-Ershad", "authors": "Nazgol Hor, Shervan Fekri-Ershad", "title": "Image retrieval approach based on local texture information derived from\n  predefined patterns and spatial domain information", "comments": "Faculty of computer engineering, Najafabad Branch, Islamic azad\n  university, Najafabad, Iran", "journal-ref": "International Journal of Computer Science Engineering (IJCSE),\n  Vol. 8 No.06, pp. 246-254, Nov-Dec 2019,", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the development of Information technology and communication, a large\npart of the databases is dedicated to images and videos. Thus retrieving images\nrelated to a query image from a large database has become an important area of\nresearch in computer vision. Until now, there are various methods of image\nretrieval that try to define image contents by texture, color or shape\nproperties. In this paper, a method is presented for image retrieval based on a\ncombination of local texture information derived from two different texture\ndescriptors. First, the color channels of the input image are separated. The\ntexture information is extracted using two descriptors such as evaluated local\nbinary patterns and predefined pattern units. After extracting the features,\nthe similarity matching is done based on distance criteria. The performance of\nthe proposed method is evaluated in terms of precision and recall on the\nSimplicity database. The comparative results showed that the proposed approach\noffers higher precision rate than many known methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:11:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hor", "Nazgol", ""], ["Fekri-Ershad", "Shervan", ""]]}, {"id": "1912.12987", "submitter": "Zhiyi Cheng", "authors": "Zhiyi Cheng, Xiatian Zhu, Shaogang Gong", "title": "Characteristic Regularisation for Super-Resolving Face Images", "comments": "Accepted by WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing facial image super-resolution (SR) methods focus mostly on improving\nartificially down-sampled low-resolution (LR) imagery. Such SR models, although\nstrong at handling artificial LR images, often suffer from significant\nperformance drop on genuine LR test data. Previous unsupervised domain\nadaptation (UDA) methods address this issue by training a model using unpaired\ngenuine LR and HR data as well as cycle consistency loss formulation. However,\nthis renders the model overstretched with two tasks: consistifying the visual\ncharacteristics and enhancing the image resolution. Importantly, this makes the\nend-to-end model training ineffective due to the difficulty of back-propagating\ngradients through two concatenated CNNs. To solve this problem, we formulate a\nmethod that joins the advantages of conventional SR and UDA models.\nSpecifically, we separate and control the optimisations for characteristics\nconsistifying and image super-resolving by introducing Characteristic\nRegularisation (CR) between them. This task split makes the model training more\neffective and computationally tractable. Extensive evaluations demonstrate the\nperformance superiority of our method over state-of-the-art SR and UDA models\non both genuine and artificial LR facial imagery data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:27:24 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Cheng", "Zhiyi", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1912.13000", "submitter": "Zhe Wu", "authors": "Zhe Wu, Zuxuan Wu, Bharat Singh, Larry S. Davis", "title": "Recognizing Instagram Filtered Images with Feature De-stylization", "comments": "Accepted in AAAI 2020 as an oral presentation paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to suffer from poor generalization when\nsmall perturbations are added (like Gaussian noise), yet little work has been\ndone to evaluate their robustness to more natural image transformations like\nphoto filters. This paper presents a study on how popular pretrained models are\naffected by commonly used Instagram filters. To this end, we introduce\nImageNet-Instagram, a filtered version of ImageNet, where 20 popular Instagram\nfilters are applied to each image in ImageNet. Our analysis suggests that\nsimple structure preserving filters which only alter the global appearance of\nan image can lead to large differences in the convolutional feature space. To\nimprove generalization, we introduce a lightweight de-stylization module that\npredicts parameters used for scaling and shifting feature maps to \"undo\" the\nchanges incurred by filters, inverting the process of style transfer tasks. We\nfurther demonstrate the module can be readily plugged into modern CNN\narchitectures together with skip connections. We conduct extensive studies on\nImageNet-Instagram, and show quantitatively and qualitatively, that the\nproposed module, among other things, can effectively improve generalization by\nsimply learning normalization parameters without retraining the entire network,\nthus recovering the alterations in the feature space caused by the filters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:48:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wu", "Zhe", ""], ["Wu", "Zuxuan", ""], ["Singh", "Bharat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1912.13077", "submitter": "Changhao Chen", "authors": "Changhao Chen, Stefano Rosa, Chris Xiaoxuan Lu, Niki Trigoni, Andrew\n  Markham", "title": "SelectFusion: A Generic Framework to Selectively Learn Multisensory\n  Fusion", "comments": "An extended journal version of arXiv:1903.01534 (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous vehicles and mobile robotic systems are typically equipped with\nmultiple sensors to provide redundancy. By integrating the observations from\ndifferent sensors, these mobile agents are able to perceive the environment and\nestimate system states, e.g. locations and orientations. Although deep learning\napproaches for multimodal odometry estimation and localization have gained\ntraction, they rarely focus on the issue of robust sensor fusion - a necessary\nconsideration to deal with noisy or incomplete sensor observations in the real\nworld. Moreover, current deep odometry models also suffer from a lack of\ninterpretability. To this extent, we propose SelectFusion, an end-to-end\nselective sensor fusion module which can be applied to useful pairs of sensor\nmodalities such as monocular images and inertial measurements, depth images and\nLIDAR point clouds. During prediction, the network is able to assess the\nreliability of the latent features from different sensor modalities and\nestimate both trajectory at scale and global pose. In particular, we propose\ntwo fusion modules based on different attention strategies: deterministic soft\nfusion and stochastic hard fusion, and we offer a comprehensive study of the\nnew strategies compared to trivial direct fusion. We evaluate all fusion\nstrategies in both ideal conditions and on progressively degraded datasets that\npresent occlusions, noisy and missing data and time misalignment between\nsensors, and we investigate the effectiveness of the different fusion\nstrategies in attending the most reliable features, which in itself, provides\ninsights into the operation of the various models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:25:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Changhao", ""], ["Rosa", "Stefano", ""], ["Lu", "Chris Xiaoxuan", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1912.13091", "submitter": "Chong You", "authors": "Daniel P. Robinson and Rene Vidal and Chong You", "title": "Basis Pursuit and Orthogonal Matching Pursuit for Subspace-preserving\n  Recovery: Theoretical Analysis", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an overcomplete dictionary $A$ and a signal $b = Ac^*$ for some sparse\nvector $c^*$ whose nonzero entries correspond to linearly independent columns\nof $A$, classical sparse signal recovery theory considers the problem of\nwhether $c^*$ can be recovered as the unique sparsest solution to $b = A c$. It\nis now well-understood that such recovery is possible by practical algorithms\nwhen the dictionary $A$ is incoherent or restricted isometric. In this paper,\nwe consider the more general case where $b$ lies in a subspace $\\mathcal{S}_0$\nspanned by a subset of linearly dependent columns of $A$, and the remaining\ncolumns are outside of the subspace. In this case, the sparsest representation\nmay not be unique, and the dictionary may not be incoherent or restricted\nisometric. The goal is to have the representation $c$ correctly identify the\nsubspace, i.e. the nonzero entries of $c$ should correspond to columns of $A$\nthat are in the subspace $\\mathcal{S}_0$. Such a representation $c$ is called\nsubspace-preserving, a key concept that has found important applications for\nlearning low-dimensional structures in high-dimensional data. We present\nvarious geometric conditions that guarantee subspace-preserving recovery. Among\nthem, the major results are characterized by the covering radius and the\nangular distance, which capture the distribution of points in the subspace and\nthe similarity between points in the subspace and points outside the subspace,\nrespectively. Importantly, these conditions do not require the dictionary to be\nincoherent or restricted isometric. By establishing that the\nsubspace-preserving recovery problem and the classical sparse signal recovery\nproblem are equivalent under common assumptions on the latter, we show that\nseveral of our proposed conditions are generalizations of some well-known\nconditions in the sparse signal recovery literature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:31:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""], ["You", "Chong", ""]]}, {"id": "1912.13171", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo,\n  Chia-Wen Lin", "title": "Deep Learning on Image Denoising: An overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have received much attention in the area of image\ndenoising. However, there are substantial differences in the various types of\ndeep learning methods dealing with image denoising. Specifically,\ndiscriminative learning based on deep learning can ably address the issue of\nGaussian noise. Optimization models based on deep learning are effective in\nestimating the real noise. However, there has thus far been little related\nresearch to summarize the different deep learning techniques for image\ndenoising. In this paper, we offer a comparative study of deep techniques in\nimage denoising. We first classify the deep convolutional neural networks\n(CNNs) for additive white noisy images; the deep CNNs for real noisy images;\nthe deep CNNs for blind denoising and the deep CNNs for hybrid noisy images,\nwhich represents the combination of noisy, blurred and low-resolution images.\nThen, we analyze the motivations and principles of the different types of deep\nlearning methods. Next, we compare the state-of-the-art methods on public\ndenoising datasets in terms of quantitative and qualitative analysis. Finally,\nwe point out some potential challenges and directions of future research.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 05:03:57 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 10:54:04 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 07:26:22 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 06:55:36 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tian", "Chunwei", ""], ["Fei", "Lunke", ""], ["Zheng", "Wenxian", ""], ["Xu", "Yong", ""], ["Zuo", "Wangmeng", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "1912.13179", "submitter": "Shadrokh Samavi", "authors": "Sajjad Abbasi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi", "title": "Modeling Teacher-Student Techniques in Deep Neural Networks for\n  Knowledge Distillation", "comments": "six pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is a new method for transferring knowledge of a\nstructure under training to another one. The typical application of KD is in\nthe form of learning a small model (named as a student) by soft labels produced\nby a complex model (named as a teacher). Due to the novel idea introduced in\nKD, recently, its notion is used in different methods such as compression and\nprocesses that are going to enhance the model accuracy. Although different\ntechniques are proposed in the area of KD, there is a lack of a model to\ngeneralize KD techniques. In this paper, various studies in the scope of KD are\ninvestigated and analyzed to build a general model for KD. All the methods and\ntechniques in KD can be summarized through the proposed model. By utilizing the\nproposed model, different methods in KD are better investigated and explored.\nThe advantages and disadvantages of different approaches in KD can be better\nunderstood and develop a new strategy for KD can be possible. Using the\nproposed model, different KD methods are represented in an abstract view.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 05:32:02 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Abbasi", "Sajjad", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.13182", "submitter": "Mengting Chen", "authors": "Mengting Chen, Yuxin Fang, Xinggang Wang, Heng Luo, Yifeng Geng, Xinyu\n  Zhang, Chang Huang, Wenyu Liu, Bo Wang", "title": "Diversity Transfer Network for Few-Shot Learning", "comments": "9 pages, 3 figures, AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a challenging task that aims at training a classifier\nfor unseen classes with only a few training examples. The main difficulty of\nfew-shot learning lies in the lack of intra-class diversity within insufficient\ntraining samples. To alleviate this problem, we propose a novel generative\nframework, Diversity Transfer Network (DTN), that learns to transfer latent\ndiversities from known categories and composite them with support features to\ngenerate diverse samples for novel categories in feature space. The learning\nproblem of the sample generation (i.e., diversity transfer) is solved via\nminimizing an effective meta-classification loss in a single-stage network,\ninstead of the generative loss in previous works.\n  Besides, an organized auxiliary task co-training over known categories is\nproposed to stabilize the meta-training process of DTN. We perform extensive\nexperiments and ablation studies on three datasets, i.e., \\emph{mini}ImageNet,\nCIFAR100 and CUB. The results show that DTN, with single-stage training and\nfaster convergence speed, obtains the state-of-the-art results among the\nfeature generation based few-shot learning methods. Code and supplementary\nmaterial are available at: \\texttt{https://github.com/Yuxin-CV/DTN}\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 05:44:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Mengting", ""], ["Fang", "Yuxin", ""], ["Wang", "Xinggang", ""], ["Luo", "Heng", ""], ["Geng", "Yifeng", ""], ["Zhang", "Xinyu", ""], ["Huang", "Chang", ""], ["Liu", "Wenyu", ""], ["Wang", "Bo", ""]]}, {"id": "1912.13183", "submitter": "Shadrokh Samavi", "authors": "Emad Malekhosseini, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi", "title": "Modeling Neural Architecture Search Methods for Deep Networks", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many research works on the designing of architectures for the deep\nneural networks (DNN), which are named neural architecture search (NAS)\nmethods. Although there are many automatic and manual techniques for NAS\nproblems, there is no unifying model in which these NAS methods can be explored\nand compared. In this paper, we propose a general abstraction model for NAS\nmethods. By using the proposed framework, it is possible to compare different\ndesign approaches for categorizing and identifying critical areas of interest\nin designing DNN architectures. Also, under this framework, different methods\nin the NAS area are summarized; hence a better view of their advantages and\ndisadvantages is possible.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 05:51:03 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Malekhosseini", "Emad", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.13192", "submitter": "Shaoshuai Shi", "authors": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang\n  Wang, Hongsheng Li", "title": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "comments": "Accepted by CVPR 2020. arXiv admin note: substantial text overlap\n  with arXiv:2102.00463", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel and high-performance 3D object detection framework, named\nPointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds.\nOur proposed method deeply integrates both 3D voxel Convolutional Neural\nNetwork (CNN) and PointNet-based set abstraction to learn more discriminative\npoint cloud features. It takes advantages of efficient learning and\nhigh-quality proposals of the 3D voxel CNN and the flexible receptive fields of\nthe PointNet-based networks. Specifically, the proposed framework summarizes\nthe 3D scene with a 3D voxel CNN into a small set of keypoints via a novel\nvoxel set abstraction module to save follow-up computations and also to encode\nrepresentative scene features. Given the high-quality 3D proposals generated by\nthe voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific\nfeatures from the keypoints to the RoI-grid points via keypoint set abstraction\nwith multiple receptive fields. Compared with conventional pooling operations,\nthe RoI-grid feature points encode much richer context information for\naccurately estimating object confidences and locations. Extensive experiments\non both the KITTI dataset and the Waymo Open dataset show that our proposed\nPV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins\nby using only point clouds. Code is available at\nhttps://github.com/open-mmlab/OpenPCDet.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:34:10 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 06:37:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Shi", "Shaoshuai", ""], ["Guo", "Chaoxu", ""], ["Jiang", "Li", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1912.13199", "submitter": "Seyed Vahid Mirnezami", "authors": "Ali HamidiSepehr, Seyed Vahid Mirnezami, Jason K. Ward", "title": "Comparison of object detection methods for crop damage assessment using\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe weather events can cause large financial losses to farmers. Detailed\ninformation on the location and severity of damage will assist farmers,\ninsurance companies, and disaster response agencies in making wise post-damage\ndecisions. The goal of this study was a proof-of-concept to detect damaged crop\nareas from aerial imagery using computer vision and deep learning techniques. A\nspecific objective was to compare existing object detection algorithms to\ndetermine which was best suited for crop damage detection. Two modes of crop\ndamage common in maize (corn) production were simulated: stalk lodging at the\nlowest ear and stalk lodging at ground level. Simulated damage was used to\ncreate a training and analysis data set. An unmanned aerial system (UAS)\nequipped with a RGB camera was used for image acquisition. Three popular object\ndetectors (Faster R-CNN, YOLOv2, and RetinaNet) were assessed for their ability\nto detect damaged regions in a field. Average precision was used to compare\nobject detectors. YOLOv2 and RetinaNet were able to detect crop damage across\nmultiple late-season growth stages. Faster R-CNN was not successful as the\nother two advanced detectors. Detecting crop damage at later growth stages was\nmore difficult for all tested object detectors. Weed pressure in simulated\ndamage plots and increased target density added additional complexity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:54:48 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 03:50:21 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 00:32:32 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["HamidiSepehr", "Ali", ""], ["Mirnezami", "Seyed Vahid", ""], ["Ward", "Jason K.", ""]]}, {"id": "1912.13200", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian,\n  Chang Xu", "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?", "comments": "New version in arXiv:2105.14202", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer. The\ncodes are publicly available at: https://github.com/huaweinoah/AdderNet.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:56:47 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 06:26:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 02:31:03 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 03:25:46 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 09:53:28 GMT"}, {"version": "v6", "created": "Thu, 1 Jul 2021 04:46:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Xu", "Chunjing", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Tian", "Qi", ""], ["Xu", "Chang", ""]]}, {"id": "1912.13214", "submitter": "Shadrokh Samavi", "authors": "Mahdi Ahmadi, Nader Karimi, Shadrokh Samavi", "title": "Image Seam-Carving by Controlling Positional Distribution of Seams", "comments": "Five pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retargeting is a new image processing task that renders the change of\naspect ratio in images. One of the most famous image-retargeting algorithms is\nseam-carving. Although seam-carving is fast and straightforward, it usually\ndistorts the images. In this paper, we introduce a new seam-carving algorithm\nthat not only has the simplicity of the original seam-carving but also lacks\nthe usual unwanted distortion existed in the original method. The positional\ndistribution of seams is introduced. We show that the proposed method\noutperforms the original seam-carving in terms of retargeted image quality\nassessment and seam coagulation measures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 08:17:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ahmadi", "Mahdi", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1912.13243", "submitter": "Pavol Bielik", "authors": "Philippe Schlattner, Pavol Bielik, Martin Vechev", "title": "Learning to Infer User Interface Attributes from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new domain of learning to infer user interface attributes that\nhelps developers automate the process of user interface implementation.\nConcretely, given an input image created by a designer, we learn to infer its\nimplementation which when rendered, looks visually the same as the input image.\nTo achieve this, we take a black box rendering engine and a set of attributes\nit supports (e.g., colors, border radius, shadow or text properties), use it to\ngenerate a suitable synthetic training dataset, and then train specialized\nneural models to predict each of the attribute values. To improve pixel-level\naccuracy, we additionally use imitation learning to train a neural policy that\nrefines the predicted attribute values by learning to compute the similarity of\nthe original and rendered images in their attribute space, rather than based on\nthe difference of pixel values. We instantiate our approach to the task of\ninferring Android Button attribute values and achieve 92.5% accuracy on a\ndataset consisting of real-world Google Play Store applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 09:45:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Schlattner", "Philippe", ""], ["Bielik", "Pavol", ""], ["Vechev", "Martin", ""]]}, {"id": "1912.13256", "submitter": "Lanfei Wang", "authors": "Lanfei Wang and Lingxi Xie and Tianyi Zhang and Jun Guo and Qi Tian", "title": "Scalable NAS with Factorizable Architectural Parameters", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an emerging topic in machine learning and\ncomputer vision. The fundamental ideology of NAS is using an automatic\nmechanism to replace manual designs for exploring powerful network\narchitectures. One of the key factors of NAS is to scale-up the search space,\ne.g., increasing the number of operators, so that more possibilities are\ncovered, but existing search algorithms often get lost in a large number of\noperators. For avoiding huge computing and competition among similar operators\nin the same pool, this paper presents a scalable algorithm by factorizing a\nlarge set of candidate operators into smaller subspaces. As a practical\nexample, this allows us to search for effective activation functions along with\nthe regular operators including convolution, pooling, skip-connect, etc. With a\nsmall increase in search costs and no extra costs in re-training, we find\ninteresting architectures that were not explored before, and achieve\nstate-of-the-art performance on CIFAR10 and ImageNet, two standard image\nclassification benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 10:26:56 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 18:47:42 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wang", "Lanfei", ""], ["Xie", "Lingxi", ""], ["Zhang", "Tianyi", ""], ["Guo", "Jun", ""], ["Tian", "Qi", ""]]}, {"id": "1912.13276", "submitter": "Shervin Rahimzadeh Arashloo", "authors": "Shervin Rahimzadeh Arashloo", "title": "Unseen Face Presentation Attack Detection Using Class-Specific Sparse\n  One-Class Multiple Kernel Fusion Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses face presentation attack detection in the challenging\nconditions of an unseen attack scenario where the system is exposed to novel\npresentation attacks that were not present in the training step. For this\npurpose, a pure one-class face presentation attack detection approach based on\nkernel regression is developed which only utilises bona fide (genuine) samples\nfor training. In the context of the proposed approach, a number of innovations,\nincluding multiple kernel fusion, client-specific modelling, sparse\nregularisation and probabilistic modelling of score distributions are\nintroduced to improve the efficacy of the method. The results of experimental\nevaluations conducted on the OULU-NPU, Replay-Mobile, Replay-Attack and\nMSU-MFSD datasets illustrate that the proposed method compares very favourably\nwith other methods operating in an unseen attack detection scenario while\nachieving very competitive performance to multi-class methods (benefiting from\npresentation attack data for training) despite using only bona fide samples for\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 11:53:20 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Arashloo", "Shervin Rahimzadeh", ""]]}, {"id": "1912.13290", "submitter": "Vladimir Novik", "authors": "N. S. Kulberg (1 and 3), A. B. Elizarov (1), V. P. Novik (1), V. A.\n  Gombolevsky (1), A. P. Gonchar (1), A. L. Alliua (2), V. Yu. Bosin (1), A. V.\n  Vladzymyrsky (1), S. P. Morozov (1) ((1) State Budget-Funded Health Care\n  Institution of the City of Moscow Research and Practical Clinical Center for\n  Diagnostics and Telemedicine Technologies of the Moscow Health Care\n  Department, (2) Federal State Budgetary Scientific Institution Russian\n  Scientific Center of Surgery named after Academician B.V. Petrovsky, (3)\n  Federal Research Center Computer Science and Control of Russian Academy of\n  Sciences)", "title": "Automatic segmentation and determining radiodensity of the liver in a\n  large-scale CT database", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes an automatic technique for liver segmentation in computed\ntomography (CT) images. Localization of the liver volume is based on the\ncorrelation with an optimized set of liver templates developed by the authors\nthat allows clear geometric interpretation. Radiodensity values are calculated\nbased on the boundaries of the segmented liver, which allows identifying liver\nabnormalities. The performance of the technique was evaluated on 700 CT images\nfrom dataset of the Unified Radiological Information System (URIS) of Moscow.\nDespite the decrease in accuracy, the technique is applicable to CT volumes\nwith a partially visible region of the liver. The technique can be used to\nprocess CT images obtained in various patient positions in a wide range of\nexposition parameters. It is capable in dealing with low dose CT scans in real\nlarge-scale medical database with over 1 million of studies.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:41:05 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kulberg", "N. S.", "", "1 and 3"], ["Elizarov", "A. B.", ""], ["Novik", "V. P.", ""], ["Gombolevsky", "V. A.", ""], ["Gonchar", "A. P.", ""], ["Alliua", "A. L.", ""], ["Bosin", "V. Yu.", ""], ["Vladzymyrsky", "A. V.", ""], ["Morozov", "S. P.", ""]]}, {"id": "1912.13298", "submitter": "Maximilian Schambach", "authors": "Maximilian Schambach and Fernando Puente Le\\'on", "title": "Microlens array grid estimation, light field decoding, and calibration", "comments": "\\copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "IEEE Transactions on Computational Imaging, vol. 6, pp. 591-603,\n  2020", "doi": "10.1109/tci.2020.2964257", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantitatively investigate multiple algorithms for microlens array grid\nestimation for microlens array-based light field cameras. Explicitly taking\ninto account natural and mechanical vignetting effects, we propose a new method\nfor microlens array grid estimation that outperforms the ones previously\ndiscussed in the literature. To quantify the performance of the algorithms, we\npropose an evaluation pipeline utilizing application-specific ray-traced white\nimages with known microlens positions. Using a large dataset of synthesized\nwhite images, we thoroughly compare the performance of the different estimation\nalgorithms. As an example, we apply our results to the decoding and calibration\nof light fields taken with a Lytro Illum camera. We observe that decoding as\nwell as calibration benefit from a more accurate, vignetting-aware grid\nestimation, especially in peripheral subapertures of the light field.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 13:27:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Schambach", "Maximilian", ""], ["Le\u00f3n", "Fernando Puente", ""]]}, {"id": "1912.13335", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Byoung-Dai Lee, Shi Sub Byon, Sung Hyun Kim, and\n  Byung-ilLee", "title": "Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View\n  Residual Learning", "comments": "The manuscript is currently under review and copyright shall be\n  transferred to the publisher upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate quantification of pulmonary nodules can greatly assist the early\ndiagnosis of lung cancer, which can enhance patient survival possibilities. A\nnumber of nodule segmentation techniques have been proposed, however, all of\nthe existing techniques rely on radiologist 3-D volume of interest (VOI) input\nor use the constant region of interest (ROI) and only investigate the presence\nof nodule voxels within the given VOI. Such approaches restrain the solutions\nto investigate the nodule presence outside the given VOI and also include the\nredundant structures into VOI, which may lead to inaccurate nodule\nsegmentation. In this work, a novel semi-automated approach for 3-D\nsegmentation of nodule in volumetric computerized tomography (CT) lung scans\nhas been proposed. The proposed technique can be segregated into two stages, at\nthe first stage, it takes a 2-D ROI containing the nodule as input and it\nperforms patch-wise investigation along the axial axis with a novel adaptive\nROI strategy. The adaptive ROI algorithm enables the solution to dynamically\nselect the ROI for the surrounding slices to investigate the presence of nodule\nusing deep residual U-Net architecture. The first stage provides the initial\nestimation of nodule which is further utilized to extract the VOI. At the\nsecond stage, the extracted VOI is further investigated along the coronal and\nsagittal axis with two different networks and finally, all the estimated masks\nare fed into the consensus module to produce the final volumetric segmentation\nof nodule. The proposed approach has been rigorously evaluated on the LIDC\ndataset, which is the largest publicly available dataset. The result suggests\nthat the approach is significantly robust and accurate as compared to the\nprevious state of the art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:03:18 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 10:57:24 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Usman", "Muhammad", ""], ["Lee", "Byoung-Dai", ""], ["Byon", "Shi Sub", ""], ["Kim", "Sung Hyun", ""], ["Byung-ilLee", "", ""]]}, {"id": "1912.13344", "submitter": "Hongwen Zhang", "authors": "Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, Zhenan Sun", "title": "Learning 3D Human Shape and Pose from Dense Body Parts", "comments": "Journal article accepted by IEEE TPAMI. Project page:\n  https://hongwenzhang.github.io/dense2mesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D human shape and pose from monocular images is challenging\ndespite the promising results achieved by the most recent learning-based\nmethods. The commonly occurred misalignment comes from the facts that the\nmapping from images to the model space is highly non-linear and the\nrotation-based pose representation of body models is prone to result in the\ndrift of joint positions. In this work, we investigate learning 3D human shape\nand pose from dense correspondences of body parts and propose a\nDecompose-and-aggregate Network (DaNet) to address these issues. DaNet adopts\nthe dense correspondence maps, which densely build a bridge between 2D pixels\nand 3D vertices, as intermediate representations to facilitate the learning of\n2D-to-3D mapping. The prediction modules of DaNet are decomposed into one\nglobal stream and multiple local streams to enable global and fine-grained\nperceptions for the shape and pose predictions, respectively. Messages from\nlocal streams are further aggregated to enhance the robust prediction of the\nrotation-based poses, where a position-aided rotation feature refinement\nstrategy is proposed to exploit spatial relationships between body joints.\nMoreover, a Part-based Dropout (PartDrop) strategy is introduced to drop out\ndense information from intermediate representations during training,\nencouraging the network to focus on more complementary body parts as well as\nneighboring position features. The efficacy of the proposed method is validated\non both indoor and real-world datasets including Human3.6M, UP3D, COCO, and\n3DPW, showing that our method could significantly improve the reconstruction\nperformance in comparison with previous state-of-the-art methods. Our code is\npublicly available at https://hongwenzhang.github.io/dense2mesh .\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:09:51 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 10:46:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Hongwen", ""], ["Cao", "Jie", ""], ["Lu", "Guo", ""], ["Ouyang", "Wanli", ""], ["Sun", "Zhenan", ""]]}, {"id": "1912.13360", "submitter": "Dinesh Jayaraman", "authors": "Brian Yang, Dinesh Jayaraman, Glen Berseth, Alexei Efros, and Sergey\n  Levine", "title": "Morphology-Agnostic Visual Robotic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for visuomotor robotic control typically require\ncharacterizing the robot in advance by calibrating the camera or performing\nsystem identification. We propose MAVRIC, an approach that works with minimal\nprior knowledge of the robot's morphology, and requires only a camera view\ncontaining the robot and its environment and an unknown control interface.\nMAVRIC revolves around a mutual information-based method for self-recognition,\nwhich discovers visual \"control points\" on the robot body within a few seconds\nof exploratory interaction, and these control points in turn are then used for\nvisual servoing. MAVRIC can control robots with imprecise actuation, no\nproprioceptive feedback, unknown morphologies including novel tools, unknown\ncamera poses, and even unsteady handheld cameras. We demonstrate our method on\nvisually-guided 3D point reaching, trajectory following, and robot-to-robot\nimitation.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:45:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Yang", "Brian", ""], ["Jayaraman", "Dinesh", ""], ["Berseth", "Glen", ""], ["Efros", "Alexei", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.13361", "submitter": "Ali Lotfi Rezaabad", "authors": "Ali Lotfi Rezaabad and Sriram Vishwanath", "title": "Learning Representations by Maximizing Mutual Information in Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) have ushered in a new era of unsupervised\nlearning methods for complex distributions. Although these techniques are\nelegant in their approach, they are typically not useful for representation\nlearning. In this work, we propose a simple yet powerful class of VAEs that\nsimultaneously result in meaningful learned representations. Our solution is to\ncombine traditional VAEs with mutual information maximization, with the goal to\nenhance amortized inference in VAEs using Information Theoretic techniques. We\ncall this approach InfoMax-VAE, and such an approach can significantly boost\nthe quality of learned high-level representations. We realize this through the\nexplicit maximization of information measures associated with the\nrepresentation. Using extensive experiments on varied datasets and setups, we\nshow that InfoMax-VAE outperforms contemporary popular approaches, including\nInfo-VAE and $\\beta$-VAE.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:44:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 05:42:39 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Rezaabad", "Ali Lotfi", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1912.13423", "submitter": "Ugur Akpinar", "authors": "Ugur Akpinar, Erdem Sahin, Monjurul Meem, Rajesh Menon, Atanas Gotchev", "title": "Learning Wavefront Coding for Extended Depth of Field Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth of field is an important factor of imaging systems that highly affects\nthe quality of the acquired spatial information. Extended depth of field (EDoF)\nimaging is a challenging ill-posed problem and has been extensively addressed\nin the literature. We propose a computational imaging approach for EDoF, where\nwe employ wavefront coding via a diffractive optical element (DOE) and we\nachieve deblurring through a convolutional neural network. Thanks to the\nend-to-end differentiable modeling of optical image formation and computational\npost-processing, we jointly optimize the optical design, i.e., DOE, and the\ndeblurring through standard gradient descent methods. Based on the properties\nof the underlying refractive lens and the desired EDoF range, we provide an\nanalytical expression for the search space of the DOE, which is instrumental in\nthe convergence of the end-to-end network. We achieve superior EDoF imaging\nperformance compared to the state of the art, where we demonstrate results with\nminimal artifacts in various scenarios, including deep 3D scenes and broadband\nimaging.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:00:09 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 18:59:13 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Akpinar", "Ugur", ""], ["Sahin", "Erdem", ""], ["Meem", "Monjurul", ""], ["Menon", "Rajesh", ""], ["Gotchev", "Atanas", ""]]}, {"id": "1912.13452", "submitter": "Jiri Borovec", "authors": "Jiri Borovec", "title": "BIRL: Benchmark on Image Registration methods with Landmark validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This report presents a generic image registration benchmark with automatic\nevaluation using landmark annotations. The key features of the BIRL framework\nare: easily extendable, performance evaluation, parallel experimentation,\nsimple visualisations, experiment's time-out limit, resuming unfinished\nexperiments. From the research practice, we identified and focused on these two\nmain use-cases: (a) comparison of user's (newly developed) method with some\nState-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA\nmethods on user's custom dataset (which should contain landmark annotation).\nMoreover, we present an integration of several standard image registration\nmethods aiming at biomedical imaging into the BIRL framework. This report also\ncontains experimental results of these SOTA methods on the CIMA dataset, which\nis a dataset of Whole Slice Imaging (WSI) from histology/pathology containing\nseveral multi-stain tissue samples from three tissue kinds. Source and results:\nhttps://borda.github.io/BIRL\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:50:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 09:56:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Borovec", "Jiri", ""]]}, {"id": "1912.13457", "submitter": "Lingzhi Li", "authors": "Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen", "title": "FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping", "comments": "Accepted to CVPR 2020 (Oral), generated dataset and project webpage:\n  lingzhili.com/FaceShifterPage/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel two-stage framework, called FaceShifter, for\nhigh fidelity and occlusion aware face swapping. Unlike many existing face\nswapping works that leverage only limited information from the target image\nwhen synthesizing the swapped face, our framework, in its first stage,\ngenerates the swapped face in high-fidelity by exploiting and integrating the\ntarget attributes thoroughly and adaptively. We propose a novel attributes\nencoder for extracting multi-level target face attributes, and a new generator\nwith carefully designed Adaptive Attentional Denormalization (AAD) layers to\nadaptively integrate the identity and the attributes for face synthesis. To\naddress the challenging facial occlusions, we append a second stage consisting\nof a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is\ntrained to recover anomaly regions in a self-supervised way without any manual\nannotations. Extensive experiments on wild faces demonstrate that our face\nswapping results are not only considerably more perceptually appealing, but\nalso better identity preserving in comparison to other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:57:46 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:50:01 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 07:43:58 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Lingzhi", ""], ["Bao", "Jianmin", ""], ["Yang", "Hao", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""]]}, {"id": "1912.13458", "submitter": "Lingzhi Li", "authors": "Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen,\n  Baining Guo", "title": "Face X-ray for More General Face Forgery Detection", "comments": "Accepted to CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel image representation called face X-ray for\ndetecting forgery in face images. The face X-ray of an input face image is a\ngreyscale image that reveals whether the input image can be decomposed into the\nblending of two images from different sources. It does so by showing the\nblending boundary for a forged image and the absence of blending for a real\nimage. We observe that most existing face manipulation methods share a common\nstep: blending the altered face into an existing background image. For this\nreason, face X-ray provides an effective way for detecting forgery generated by\nmost existing face manipulation algorithms. Face X-ray is general in the sense\nthat it only assumes the existence of a blending step and does not rely on any\nknowledge of the artifacts associated with a specific face manipulation\ntechnique. Indeed, the algorithm for computing face X-ray can be trained\nwithout fake images generated by any of the state-of-the-art face manipulation\nmethods. Extensive experiments show that face X-ray remains effective when\napplied to forgery generated by unseen face manipulation techniques, while most\nexisting face forgery detection or deepfake detection algorithms experience a\nsignificant performance drop.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:57:56 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 02:22:40 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Lingzhi", ""], ["Bao", "Jianmin", ""], ["Zhang", "Ting", ""], ["Yang", "Hao", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Guo", "Baining", ""]]}, {"id": "1912.13461", "submitter": "Anindya Harchowdhury", "authors": "Anindya Harchowdhury, Lindsay Kleeman, Leena Vachhani", "title": "3D Sensing of a Moving Object with a Nodding 2D LIDAR and Reconfigurable\n  Mirrors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception in 3D has become standard practice for a large part of robotics\napplications. High quality 3D perception is costly. Our previous work on a\nnodding 2D Lidar provides high quality 3D depth information with low cost, but\nthe sparse data generated by this sensor poses challenges in understanding the\ncharacteristics of moving objects within an uncertain environment. This paper\nproposes a novel design of the nodding Lidar but provides dynamic\nreconfigurability in terms of limiting the field of view of the sensor using a\nset of optical mirrors. It not only provides denser scans, but it also achieves\na three times higher scan update rate. Additionally, we propose a novel\ncalibration mechanism for this sensor and prove its effectiveness for dynamic\nobject detection and tracking.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:22:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Harchowdhury", "Anindya", ""], ["Kleeman", "Lindsay", ""], ["Vachhani", "Leena", ""]]}, {"id": "1912.13470", "submitter": "Haoshu Fang", "authors": "Hao-Shu Fang, Chenxi Wang, Minghao Gou, Cewu Lu", "title": "GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for\n  Object Grasping", "comments": "Report for our recent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object grasping is critical for many applications, which is also a\nchallenging computer vision problem. However, for the clustered scene, current\nresearches suffer from the problems of insufficient training data and the\nlacking of evaluation benchmarks. In this work, we contribute a large-scale\ngrasp pose detection dataset with a unified evaluation system. Our dataset\ncontains 87,040 RGBD images with over 370 million grasp poses. Meanwhile, our\nevaluation system directly reports whether a grasping is successful or not by\nanalytic computation, which is able to evaluate any kind of grasp poses without\nexhausted labeling pose ground-truth. We conduct extensive experiments to show\nthat our dataset and evaluation system can align well with real-world\nexperiments. Our dataset, source code and models will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:15:11 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 03:49:58 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Wang", "Chenxi", ""], ["Gou", "Minghao", ""], ["Lu", "Cewu", ""]]}, {"id": "1912.13471", "submitter": "Yaniv Benny", "authors": "Yaniv Benny and Lior Wolf", "title": "OneGAN: Simultaneous Unsupervised Learning of Conditional Image\n  Generation, Foreground Segmentation, and Fine-Grained Clustering", "comments": "To be published in the European Conference on Computer Vision (ECCV)\n  2020", "journal-ref": null, "doi": "10.1007/978-3-030-58574-7_31", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for simultaneously learning, in an unsupervised manner,\n(i) a conditional image generator, (ii) foreground extraction and segmentation,\n(iii) clustering into a two-level class hierarchy, and (iv) object removal and\nbackground completion, all done without any use of annotation. The method\ncombines a Generative Adversarial Network and a Variational Auto-Encoder, with\nmultiple encoders, generators and discriminators, and benefits from solving all\ntasks at once. The input to the training scheme is a varied collection of\nunlabeled images from the same domain, as well as a set of background images\nwithout a foreground object. In addition, the image generator can mix the\nbackground from one image, with a foreground that is conditioned either on that\nof a second image or on the index of a desired cluster. The method obtains\nstate of the art results in comparison to the literature methods, when compared\nto the current state of the art in each of the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:15:58 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:29:00 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Benny", "Yaniv", ""], ["Wolf", "Lior", ""]]}, {"id": "1912.13503", "submitter": "Alexander Sax", "authors": "Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra\n  Malik", "title": "Side-Tuning: A Baseline for Network Adaptation via Additive Side\n  Networks", "comments": "In ECCV 2020 (Spotlight). For more, see project website and code at\n  http://sidetuning.berkeley.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training a neural network for a desired task, one may prefer to adapt a\npre-trained network rather than starting from randomly initialized weights.\nAdaptation can be useful in cases when training data is scarce, when a single\nlearner needs to perform multiple tasks, or when one wishes to encode priors in\nthe network. The most commonly employed approaches for network adaptation are\nfine-tuning and using the pre-trained network as a fixed feature extractor,\namong others.\n  In this paper, we propose a straightforward alternative: side-tuning.\nSide-tuning adapts a pre-trained network by training a lightweight \"side\"\nnetwork that is fused with the (unchanged) pre-trained network via summation.\nThis simple method works as well as or better than existing solutions and it\nresolves some of the basic issues with fine-tuning, fixed features, and other\ncommon approaches. In particular, side-tuning is less prone to overfitting, is\nasymptotically consistent, and does not suffer from catastrophic forgetting in\nincremental learning. We demonstrate the performance of side-tuning under a\ndiverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy),\nreinforcement learning, imitation learning (visual navigation in Habitat), NLP\nquestion-answering (SQuAD v2), and single-task transfer learning (Taskonomy),\nwith consistently promising results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:52:32 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 00:02:34 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 22:36:58 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 00:44:06 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Zhang", "Jeffrey O", ""], ["Sax", "Alexander", ""], ["Zamir", "Amir", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""]]}]