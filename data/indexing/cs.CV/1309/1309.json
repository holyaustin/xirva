[{"id": "1309.0123", "submitter": "Ryan Wen Liu", "authors": "Ryan Wen Liu and Tian Xu", "title": "A Robust Alternating Direction Method for Constrained Hybrid Variational\n  Deblurring Model", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new constrained hybrid variational deblurring model is\ndeveloped by combining the non-convex first- and second-order total variation\nregularizers. Moreover, a box constraint is imposed on the proposed model to\nguarantee high deblurring performance. The developed constrained hybrid\nvariational model could achieve a good balance between preserving image details\nand alleviating ringing artifacts. In what follows, we present the\ncorresponding numerical solution by employing an iteratively reweighted\nalgorithm based on alternating direction method of multipliers. The\nexperimental results demonstrate the superior performance of the proposed\nmethod in terms of quantitative and qualitative image quality assessments.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 14:29:52 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2013 01:53:49 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Liu", "Ryan Wen", ""], ["Xu", "Tian", ""]]}, {"id": "1309.0213", "submitter": "Fei Gao", "authors": "Fei Gao, Dacheng Tao, Xinbo Gao, Xuelong Li", "title": "Learning to Rank for Blind Image Quality Assessment", "comments": null, "journal-ref": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL.\n  26, NO. 10, OCTOBER 2015, PP. 2275-2290", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image quality assessment (BIQA) aims to predict perceptual image\nquality scores without access to reference images. State-of-the-art BIQA\nmethods typically require subjects to score a large number of images to train a\nrobust model. However, subjective quality scores are imprecise, biased, and\ninconsistent, and it is challenging to obtain a large scale database, or to\nextend existing databases, because of the inconvenience of collecting images,\ntraining the subjects, conducting subjective experiments, and realigning human\nquality evaluations. To combat these limitations, this paper explores and\nexploits preference image pairs (PIPs) such as \"the quality of image $I_a$ is\nbetter than that of image $I_b$\" for training a robust BIQA model. The\npreference label, representing the relative quality of two images, is generally\nprecise and consistent, and is not sensitive to image content, distortion type,\nor subject identity; such PIPs can be generated at very low cost. The proposed\nBIQA method is one of learning to rank. We first formulate the problem of\nlearning the mapping from the image features to the preference label as one of\nclassification. In particular, we investigate the utilization of a multiple\nkernel learning algorithm based on group lasso (MKLGL) to provide a solution. A\nsimple but effective strategy to estimate perceptual image quality scores is\nthen presented. Experiments show that the proposed BIQA method is highly\neffective and achieves comparable performance to state-of-the-art BIQA\nalgorithms. Moreover, the proposed method can be easily extended to new\ndistortion categories.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 12:26:53 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2013 01:52:03 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 00:26:17 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Gao", "Fei", ""], ["Tao", "Dacheng", ""], ["Gao", "Xinbo", ""], ["Li", "Xuelong", ""]]}, {"id": "1309.0261", "submitter": "Dan Ciresan", "authors": "Dan Cire\\c{s}an and J\\\"urgen Schmidhuber", "title": "Multi-Column Deep Neural Networks for Offline Handwritten Chinese\n  Character Classification", "comments": "5 pages, 1 figure, IDSIA tech report", "journal-ref": null, "doi": null, "report-no": "IDSIA-05-13", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our Multi-Column Deep Neural Networks achieve best known recognition rates on\nChinese characters from the ICDAR 2011 and 2013 offline handwriting\ncompetitions, approaching human performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 20:35:17 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Cire\u015fan", "Dan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1309.0270", "submitter": "Mahdi S. Hosseini", "authors": "Mahdi S. Hosseini and Konstantinos N. Plataniotis", "title": "High-Accuracy Total Variation for Compressed Video Sensing", "comments": "Submitted to IEEE Transaction on Image Processing, Revised", "journal-ref": null, "doi": "10.1109/TIP.2014.2332755", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous total variation (TV) regularizers, engaged in image restoration\nproblem, encode the gradients by means of simple $[-1,1]$ FIR filter. Despite\nits low computational processing, this filter severely deviates signal's high\nfrequency components pertinent to edge/discontinuous information and cause\nseveral deficiency issues known as texture and geometric loss. This paper\naddresses this problem by proposing an alternative model to the TV\nregularization problem via high order accuracy differential FIR filters to\npreserve rapid transitions in signal recovery. A numerical encoding scheme is\ndesigned to extend the TV model into multidimensional representation (tensorial\ndecomposition). We adopt this design to regulate the spatial and temporal\nredundancy in compressed video sensing problem to jointly recover frames from\nunder-sampled measurements. We then seek the solution via alternating direction\nmethods of multipliers and find a unique solution to quadratic minimization\nstep with capability of handling different boundary conditions. The resulting\nalgorithm uses much lower sampling rate and highly outperforms alternative\nstate-of-the-art methods. This is evaluated both in terms of restoration\naccuracy and visual quality of the recovered frames.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 22:00:48 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 00:05:34 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Hosseini", "Mahdi S.", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1309.0309", "submitter": "Xiaojiang Peng", "authors": "Xiaojiang Peng, Qiang Peng, Yu Qiao, Junzhou Chen, Mehtab Afzal", "title": "A Study on Unsupervised Dictionary Learning and Feature Encoding for\n  Action Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts have been devoted to develop alternative methods to traditional\nvector quantization in image domain such as sparse coding and soft-assignment.\nThese approaches can be split into a dictionary learning phase and a feature\nencoding phase which are often closely connected. In this paper, we investigate\nthe effects of these phases by separating them for video-based action\nclassification. We compare several dictionary learning methods and feature\nencoding schemes through extensive experiments on KTH and HMDB51 datasets.\nExperimental results indicate that sparse coding performs consistently better\nthan the other encoding methods in large complex dataset (i.e., HMDB51), and it\nis robust to different dictionaries. For small simple dataset (i.e., KTH) with\nless variation, however, all the encoding strategies perform competitively. In\naddition, we note that the strength of sophisticated encoding approaches comes\nnot from their corresponding dictionaries but the encoding mechanisms, and we\ncan just use randomly selected exemplars as dictionaries for video-based action\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 07:06:05 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Peng", "Xiaojiang", ""], ["Peng", "Qiang", ""], ["Qiao", "Yu", ""], ["Chen", "Junzhou", ""], ["Afzal", "Mehtab", ""]]}, {"id": "1309.0985", "submitter": "Stephane Roux", "authors": "Stephane Roux (LMT), Hugo Leclerc (LMT), Fran\\c{c}ois Hild (LMT)", "title": "Efficient binary tomographic reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.class-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic reconstruction of a binary image from few projections is\nconsidered. A novel {\\em heuristic} algorithm is proposed, the central element\nof which is a nonlinear transformation $\\psi(p)=\\log(p/(1-p))$ of the\nprobability $p$ that a pixel of the sought image be 1-valued. It consists of\nbackprojections based on $\\psi(p)$ and iterative corrections. Application of\nthis algorithm to a series of artificial test cases leads to exact binary\nreconstructions, (i.e recovery of the binary image for each single pixel) from\nthe knowledge of projection data over a few directions. Images up to $10^6$\npixels are reconstructed in a few seconds. A series of test cases is performed\nfor comparison with previous methods, showing a better efficiency and reduced\ncomputation times.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 11:40:19 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Roux", "Stephane", "", "LMT"], ["Leclerc", "Hugo", "", "LMT"], ["Hild", "Fran\u00e7ois", "", "LMT"]]}, {"id": "1309.0999", "submitter": "Suranjan Ganguly", "authors": "Ayan Seal, Mita Nasipuri, Debotosh Bhattacharjee, Dipak Kumar Basu", "title": "Minutiae Based Thermal Face Recognition using Blood Perfusion Data", "comments": "4 pages, Image Information Processing (ICIIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper describes an efficient approach for human face recognition based\non blood perfusion data from infra-red face images. Blood perfusion data are\ncharacterized by the regional blood flow in human tissue and therefore do not\ndepend entirely on surrounding temperature. These data bear a great potential\nfor deriving discriminating facial thermogram for better classification and\nrecognition of face images in comparison to optical image data. Blood perfusion\ndata are related to distribution of blood vessels under the face skin. A\ndistribution of blood vessels are unique for each person and as a set of\nextracted minutiae points from a blood perfusion data of a human face should be\nunique for that face. There may be several such minutiae point sets for a\nsingle face but all of these correspond to that particular face only. Entire\nface image is partitioned into equal blocks and the total number of minutiae\npoints from each block is computed to construct final vector. Therefore, the\nsize of the feature vectors is found to be same as total number of blocks\nconsidered. For classification, a five layer feed-forward backpropagation\nneural network has been used. A number of experiments were conducted to\nevaluate the performance of the proposed face recognition system with varying\nblock sizes. Experiments have been performed on the database created at our own\nlaboratory. The maximum success of 91.47% recognition has been achieved with\nblock size 8X8.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 12:18:55 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Seal", "Ayan", ""], ["Nasipuri", "Mita", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.1000", "submitter": "Suranjan Ganguly", "authors": "Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri and\n  Dipak Kr.Basu", "title": "Automated Thermal Face recognition based on Minutiae Extraction", "comments": "29 pages, Int. J. Computational Intelligence Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper an efficient approach for human face recognition based on the\nuse of minutiae points in thermal face image is proposed. The thermogram of\nhuman face is captured by thermal infra-red camera. Image processing methods\nare used to pre-process the captured thermogram, from which different\nphysiological features based on blood perfusion data are extracted. Blood\nperfusion data are related to distribution of blood vessels under the face\nskin. In the present work, three different methods have been used to get the\nblood perfusion image, namely bit-plane slicing and medial axis transform,\nmorphological erosion and medial axis transform, sobel edge operators.\nDistribution of blood vessels is unique for each person and a set of extracted\nminutiae points from a blood perfusion data of a human face should be unique\nfor that face. Two different methods are discussed for extracting minutiae\npoints from blood perfusion data. For extraction of features entire face image\nis partitioned into equal size blocks and the total number of minutiae points\nfrom each block is computed to construct final feature vector. Therefore, the\nsize of the feature vectors is found to be same as total number of blocks\nconsidered. A five layer feed-forward back propagation neural network is used\nas the classification tool. A number of experiments were conducted to evaluate\nthe performance of the proposed face recognition methodologies with varying\nblock size on the database created at our own laboratory. It has been found\nthat the first method supercedes the other two producing an accuracy of 97.62%\nwith block size 16X16 for bit-plane 4.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 12:26:50 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Seal", "Ayan", ""], ["Ganguly", "Suranjan", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kr.", ""]]}, {"id": "1309.1009", "submitter": "Suranjan Ganguly", "authors": "Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri,\n  Dipak Kumar Basu", "title": "A Comparative Study of Human thermal face recognition based on Haar\n  wavelet transform (HWT) and Local Binary Pattern (LBP)", "comments": "17 pages Computational Intelligence and Neuroscience 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Thermal infra-red (IR) images focus on changes of temperature distribution on\nfacial muscles and blood vessels. These temperature changes can be regarded as\ntexture features of images. A comparative study of face recognition methods\nworking in thermal spectrum is carried out in this paper. In these study two\nlocal-matching methods based on Haar wavelet transform and Local Binary Pattern\n(LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale,\nmulti-direction changes of texture. Local binary patterns (LBP) are a type of\nfeature used for classification in computer vision. Firstly, human thermal IR\nface image is preprocessed and cropped the face region only from the entire\nimage. Secondly, two different approaches are used to extract the features from\nthe cropped face region. In the first approach, the training images and the\ntest images are processed with Haar wavelet transform and the LL band and the\naverage of LH/HL/HH bands sub-images are created for each face image. Then a\ntotal confidence matrix is formed for each face image by taking a weighted sum\nof the corresponding pixel values of the LL band and average band. For LBP\nfeature extraction, each of the face images in training and test datasets is\ndivided into 161 numbers of sub images, each of size 8X8 pixels. For each such\nsub images, LBP features are extracted which are concatenated in row wise\nmanner. PCA is performed separately on the individual feature set for\ndimensionality reeducation. Finally two different classifiers are used to\nclassify face images. One such classifier multi-layer feed forward neural\nnetwork and another classifier is minimum distance classifier. The Experiments\nhave been performed on the database created at our own laboratory and Terravic\nFacial IR Database.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 12:41:48 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Seal", "Ayan", ""], ["Ganguly", "Suranjan", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.1080", "submitter": "Edward Rosten", "authors": "Damian Eads, David Helmbold, Ed Rosten", "title": "Boosting in Location Space", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of object detection is to find objects in an image. An object\ndetector accepts an image and produces a list of locations as $(x,y)$ pairs.\nHere we introduce a new concept: {\\bf location-based boosting}. Location-based\nboosting differs from previous boosting algorithms because it optimizes a new\nspatial loss function to combine object detectors, each of which may have\nmarginal performance, into a single, more accurate object detector. A\nstructured representation of object locations as a list of $(x,y)$ pairs is a\nmore natural domain for object detection than the spatially unstructured\nrepresentation produced by classifiers. Furthermore, this formulation allows us\nto take advantage of the intuition that large areas of the background are\nuninteresting and it is not worth expending computational effort on them. This\nresults in a more scalable algorithm because it does not need to take measures\nto prevent the background data from swamping the foreground data such as\nsubsampling or applying an ad-hoc weighting to the pixels. We first present the\ntheory of location-based boosting, and then motivate it with empirical results\non a challenging data set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 15:49:09 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Eads", "Damian", ""], ["Helmbold", "David", ""], ["Rosten", "Ed", ""]]}, {"id": "1309.1155", "submitter": "Suranjan Ganguly", "authors": "Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri,\n  Dipak Kumar Basu", "title": "Minutiae Based Thermal Human Face Recognition using Label Connected\n  Component Algorithm", "comments": "7 pages, Conference. arXiv admin note: substantial text overlap with\n  arXiv:1309.1000, arXiv:1309.0999, arXiv:1309.1009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, a thermal infra red face recognition system for human\nidentification and verification using blood perfusion data and back propagation\nfeed forward neural network is proposed. The system consists of three steps. At\nthe very first step face region is cropped from the colour 24-bit input images.\nSecondly face features are extracted from the croped region, which will be\ntaken as the input of the back propagation feed forward neural network in the\nthird step and classification and recognition is carried out. The proposed\napproaches are tested on a number of human thermal infra red face images\ncreated at our own laboratory. Experimental results reveal the higher degree\nperformance\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 13:32:28 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Seal", "Ayan", ""], ["Ganguly", "Suranjan", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.1156", "submitter": "Suranjan Ganguly", "authors": "Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri,\n  Dipak kr. Basu", "title": "Thermal Human face recognition based on Haar wavelet transform and\n  series matching technique", "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:1309.1009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Thermal infrared (IR) images represent the heat patterns emitted from hot\nobject and they do not consider the energies reflected from an object. Objects\nliving or non-living emit different amounts of IR energy according to their\nbody temperature and characteristics. Humans are homoeothermic and hence\ncapable of maintaining constant temperature under different surrounding\ntemperature. Face recognition from thermal (IR) images should focus on changes\nof temperature on facial blood vessels. These temperature changes can be\nregarded as texture features of images and wavelet transform is a very good\ntool to analyze multi-scale and multi-directional texture. Wavelet transform is\nalso used for image dimensionality reduction, by removing redundancies and\npreserving original features of the image. The sizes of the facial images are\nnormally large. So, the wavelet transform is used before image similarity is\nmeasured. Therefore this paper describes an efficient approach of human face\nrecognition based on wavelet transform from thermal IR images. The system\nconsists of three steps. At the very first step, human thermal IR face image is\npreprocessed and the face region is only cropped from the entire image.\nSecondly, Haar wavelet is used to extract low frequency band from the cropped\nface region. Lastly, the image classification between the training images and\nthe test images is done, which is based on low-frequency components. The\nproposed approach is tested on a number of human thermal infrared face images\ncreated at our own laboratory and Terravic Facial IR Database. Experimental\nresults indicated that the thermal infra red face images can be recognized by\nthe proposed system effectively. The maximum success of 95% recognition has\nbeen achieved.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 13:45:25 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Seal", "Ayan", ""], ["Ganguly", "Suranjan", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak kr.", ""]]}, {"id": "1309.1539", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Choon Meng Lee, Loong-Fah Cheong and Kim-Chuan Toh", "title": "Practical Matrix Completion and Corruption Recovery using Proximal\n  Alternating Robust Subspace Minimization", "comments": "Published at IJCV", "journal-ref": null, "doi": "10.1007/s11263-014-0746-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion is a problem of immense practical importance.\nRecent works on the subject often use nuclear norm as a convex surrogate of the\nrank function. Despite its solid theoretical foundation, the convex version of\nthe problem often fails to work satisfactorily in real-life applications. Real\ndata often suffer from very few observations, with support not meeting the\nrandom requirements, ubiquitous presence of noise and potentially gross\ncorruptions, sometimes with these simultaneously occurring.\n  This paper proposes a Proximal Alternating Robust Subspace Minimization\n(PARSuMi) method to tackle the three problems. The proximal alternating scheme\nexplicitly exploits the rank constraint on the completed matrix and uses the\n$\\ell_0$ pseudo-norm directly in the corruption recovery step. We show that the\nproposed method for the non-convex and non-smooth model converges to a\nstationary point. Although it is not guaranteed to find the global optimal\nsolution, in practice we find that our algorithm can typically arrive at a good\nlocal minimizer when it is supplied with a reasonably good starting point based\non convex optimization. Extensive experiments with challenging synthetic and\nreal data demonstrate that our algorithm succeeds in a much larger range of\npractical problems where convex optimization fails, and it also outperforms\nvarious state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 05:38:32 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 00:17:16 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lee", "Choon Meng", ""], ["Cheong", "Loong-Fah", ""], ["Toh", "Kim-Chuan", ""]]}, {"id": "1309.1628", "submitter": "Pawel Dlotko PhD", "authors": "Pawe{\\l} D{\\l}otko, Ruben Specogna", "title": "Topology preserving thinning for cell complexes", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2348799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A topology preserving skeleton is a synthetic representation of an object\nthat retains its topology and many of its significant morphological properties.\nThe process of obtaining the skeleton, referred to as skeletonization or\nthinning, is a very active research area. It plays a central role in reducing\nthe amount of information to be processed during image analysis and\nvisualization, computer-aided diagnosis or by pattern recognition algorithms.\n  This paper introduces a novel topology preserving thinning algorithm which\nremoves \\textit{simple cells}---a generalization of simple points---of a given\ncell complex. The test for simple cells is based on \\textit{acyclicity tables}\nautomatically produced in advance with homology computations. Using acyclicity\ntables render the implementation of thinning algorithms straightforward.\nMoreover, the fact that tables are automatically filled for all possible\nconfigurations allows to rigorously prove the generality of the algorithm and\nto obtain fool-proof implementations. The novel approach enables, for the first\ntime, according to our knowledge, to thin a general unstructured simplicial\ncomplex. Acyclicity tables for cubical and simplicial complexes and an open\nsource implementation of the thinning algorithm are provided as additional\nmaterial to allow their immediate use in the vast number of practical\napplications arising in medical imaging and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 13:11:48 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 13:39:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["D\u0142otko", "Pawe\u0142", ""], ["Specogna", "Ruben", ""]]}, {"id": "1309.1830", "submitter": "Surya Prasath", "authors": "V. B. S. Prasath, O. Haddad", "title": "Radar shadow detection in SAR images using DEM and projections", "comments": "10 pages, 6 figures", "journal-ref": "J. Appl. Remote Sens. 8(1), 083628 (May 19, 2014)", "doi": "10.1117/1.JRS.8.083628", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) images are widely used in target recognition\ntasks nowadays. In this letter, we propose an automatic approach for radar\nshadow detection and extraction from SAR images utilizing geometric projections\nalong with the digital elevation model (DEM) which corresponds to the given\ngeo-referenced SAR image. First, the DEM is rotated into the radar geometry so\nthat each row would match that of a radar line of sight. Next, we extract the\nshadow regions by processing row by row until the image is covered fully. We\ntest the proposed shadow detection approach on different DEMs and a simulated\n1D signals and 2D hills and volleys modeled by various variance based Gaussian\nfunctions. Experimental results indicate the proposed algorithm produces good\nresults in detecting shadows in SAR images with high resolution.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 07:14:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Prasath", "V. B. S.", ""], ["Haddad", "O.", ""]]}, {"id": "1309.1853", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, David Suter, Anton van den Hengel", "title": "A General Two-Step Approach to Learning-Based Hashing", "comments": "13 pages. Appearing in Int. Conf. Computer Vision (ICCV) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to hashing apply a single form of hash function, and\nan optimization process which is typically deeply coupled to this specific\nform. This tight coupling restricts the flexibility of the method to respond to\nthe data, and can result in complex optimization problems that are difficult to\nsolve. Here we propose a flexible yet simple framework that is able to\naccommodate different types of loss functions and hash functions. This\nframework allows a number of existing approaches to hashing to be placed in\ncontext, and simplifies the development of new problem-specific hashing\nmethods. Our framework decomposes hashing learning problem into two steps: hash\nbit learning and hash function learning based on the learned bits. The first\nstep can typically be formulated as binary quadratic problems, and the second\nstep can be accomplished by training standard binary classifiers. Both problems\nhave been extensively studied in the literature. Our extensive experiments\ndemonstrate that the proposed framework is effective, flexible and outperforms\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 11:33:36 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Suter", "David", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1309.2057", "submitter": "Sapan Naik j", "authors": "Sapan Naik and Nikunj Patel", "title": "Single image super resolution in spatial and wavelet domain", "comments": "10 pages, 5 figures, 1 table, sample code given", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.5, No.4, August 2013", "doi": "10.5121/ijma.2013.5402", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently single image super resolution is very important research area to\ngenerate high resolution image from given low resolution image. Algorithms of\nsingle image resolution are mainly based on wavelet domain and spatial domain.\nFilters support to model the regularity of natural images is exploited in\nwavelet domain while edges of images get sharp during up sampling in spatial\ndomain. Here single image super resolution algorithm is presented which based\non both spatial and wavelet domain and take the advantage of both. Algorithm is\niterative and use back projection to minimize reconstruction error. Wavelet\nbased denoising method is also introduced to remove noise.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 07:33:50 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Naik", "Sapan", ""], ["Patel", "Nikunj", ""]]}, {"id": "1309.2074", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Clustering and Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1308.0273,\n  arXiv:1308.0275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:16:02 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 18:50:35 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1309.2084", "submitter": "Pedro Neto", "authors": "Pedro Neto, D\\'ario Pereira, Norberto Pires, Paulo Moreira", "title": "Real-Time and Continuous Hand Gesture Spotting: an Approach Based on\n  Artificial Neural Networks", "comments": "2013 IEEE International Conference on Robotics and Automation (ICRA)\n  pp. 178-183, Karlsruhe, Germany, 2013", "journal-ref": null, "doi": "10.1109/ICRA.2013.6630573", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New and more natural human-robot interfaces are of crucial interest to the\nevolution of robotics. This paper addresses continuous and real-time hand\ngesture spotting, i.e., gesture segmentation plus gesture recognition. Gesture\npatterns are recognized by using artificial neural networks (ANNs) specifically\nadapted to the process of controlling an industrial robot. Since in continuous\ngesture recognition the communicative gestures appear intermittently with the\nnoncommunicative, we are proposing a new architecture with two ANNs in series\nto recognize both kinds of gesture. A data glove is used as interface\ntechnology. Experimental results demonstrated that the proposed solution\npresents high recognition rates (over 99% for a library of ten gestures and\nover 96% for a library of thirty gestures), low training and learning time and\na good capacity to generalize from particular situations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:41:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Neto", "Pedro", ""], ["Pereira", "D\u00e1rio", ""], ["Pires", "Norberto", ""], ["Moreira", "Paulo", ""]]}, {"id": "1309.2094", "submitter": "Dirk Lorenz", "authors": "Dirk A. Lorenz, Frank Sch\\\"opfer, Stephan Wenger", "title": "The Linearized Bregman Method via Split Feasibility Problems: Analysis\n  and Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linearized Bregman method is a method to calculate sparse solutions to\nsystems of linear equations. We formulate this problem as a split feasibility\nproblem, propose an algorithmic framework based on Bregman projections and\nprove a general convergence result for this framework. Convergence of the\nlinearized Bregman method will be obtained as a special case. Our approach also\nallows for several generalizations such as other objective functions,\nincremental iterations, incorporation of non-gaussian noise models or box\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 10:04:34 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 06:31:53 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Lorenz", "Dirk A.", ""], ["Sch\u00f6pfer", "Frank", ""], ["Wenger", "Stephan", ""]]}, {"id": "1309.2240", "submitter": "Bernhard Schmitzer", "authors": "Bernhard Schmitzer and Christoph Schn\\\"orr", "title": "Contour Manifolds and Optimal Transport", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing shapes by suitable measures in object segmentation, as proposed in\n[24], allows to combine the advantages of the representations as parametrized\ncontours and indicator functions. The pseudo-Riemannian structure of optimal\ntransport can be used to model shapes in ways similar as with contours, while\nthe Kantorovich functional enables the application of convex optimization\nmethods for global optimality of the segmentation functional.\n  In this paper we provide a mathematical study of the shape measure\nrepresentation and its relation to the contour description. In particular we\nshow that the pseudo-Riemannian structure of optimal transport, when restricted\nto the set of shape measures, yields a manifold which is diffeomorphic to the\nmanifold of closed contours. A discussion of the metric induced by optimal\ntransport and the corresponding geodesic equation is given.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 17:51:50 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Schmitzer", "Bernhard", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1309.2506", "submitter": "Ahlam Maqqor", "authors": "Ahlam Maqqor, Akram Halli, and Khaled Satori", "title": "A multi-stream hmm approach to offline handwritten arabic word\n  recognition", "comments": "12 pages,13 figure,International Journal on Natural Language\n  Computing(IJNLC),ISSN:2278-1307[Online];2319-4111[Print],August 2013, Volume\n  2, Number 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In This paper we presented new approach for cursive Arabic text recognition\nsystem. The objective is to propose methodology analytical offline recognition\nof handwritten Arabic for rapid implementation. The first part in the writing\nrecognition system is the preprocessing phase is the preprocessing phase to\nprepare the data was introduces and extracts a set of simple statistical\nfeatures by two methods : from a window which is sliding long that text line\nthe right to left and the approach VH2D (consists in projecting every character\non the abscissa, on the ordinate and the diagonals 45{\\deg} and 135{\\deg}) . It\nthen injects the resulting feature vectors to Hidden Markov Model (HMM) and\ncombined the two HMM by multi-stream approach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 13:40:30 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Maqqor", "Ahlam", ""], ["Halli", "Akram", ""], ["Satori", "Khaled", ""]]}, {"id": "1309.2752", "submitter": "Surya Prasath", "authors": "Juan C. Moreno, V. B. S. Prasath, Gil Santos, Hugo Proenca", "title": "Robust Periocular Recognition By Fusing Sparse Representations of Color\n  and Geometry Information", "comments": "23 pages, 5 figures, 3 tables", "journal-ref": null, "doi": "10.1007/s11265-015-1023-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a re-weighted elastic net (REN) model for biometric\nrecognition. The new model is applied to data separated into geometric and\ncolor spatial components. The geometric information is extracted using a fast\ncartoon - texture decomposition model based on a dual formulation of the total\nvariation norm allowing us to carry information about the overall geometry of\nimages. Color components are defined using linear and nonlinear color spaces,\nnamely the red-green-blue (RGB), chromaticity-brightness (CB) and\nhue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparse\nrepresentations for classification purposes are obtained. The scheme is\nnumerically solved using a gradient projection (GP) algorithm. In the empirical\nvalidation of the proposed model, we have chosen the periocular region, which\nis an emerging trait known for its robustness against low quality data. Our\nresults were obtained in the publicly available UBIRIS.v2 data set and show\nconsistent improvements in recognition effectiveness when compared to related\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 08:11:14 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Moreno", "Juan C.", ""], ["Prasath", "V. B. S.", ""], ["Santos", "Gil", ""], ["Proenca", "Hugo", ""]]}, {"id": "1309.3006", "submitter": "AL-Wassai F.A.", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar", "title": "The Classification Accuracy of Multiple-Metric Learning Algorithm on\n  Multi-Sensor Fusion", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in title the paper", "journal-ref": "International Journal of Soft Computing and Engineering (IJSCE)\n  ISSN: 2231-2307, Volume-3, Issue-4, September 2013,pp.124-131", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on two main issues; first one is the impact of Similarity\nSearch to learning the training sample in metric space, and searching based on\nsupervised learning classi-fication. In particular, four metrics space\nsearching are based on spatial information that are introduced as the\nfollowing; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); Manhattan\nDistance (MD) and Euclidean Distance(ED) classifiers. The second issue\ninvestigates the performance of combination of mul-ti-sensor images on the\nsupervised learning classification accura-cy. QuickBird multispectral data (MS)\nand panchromatic data (PAN) have been used in this study to demonstrate the\nenhance-ment and accuracy assessment of fused image over the original images.\nThe supervised classification results of fusion image generated better than the\nMS did. QuickBird and the best results with ED classifier than the other did.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 23:58:23 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 20:47:37 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""]]}, {"id": "1309.3256", "submitter": "Rachel Ward", "authors": "Abhinav Nellore and Rachel Ward", "title": "Recovery guarantees for exemplar-based clustering", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a certain class of distributions, we prove that the linear programming\nrelaxation of $k$-medoids clustering---a variant of $k$-means clustering where\nmeans are replaced by exemplars from within the dataset---distinguishes points\ndrawn from nonoverlapping balls with high probability once the number of points\ndrawn and the separation distance between any two balls are sufficiently large.\nOur results hold in the nontrivial regime where the separation distance is\nsmall enough that points drawn from different balls may be closer to each other\nthan points drawn from the same ball; in this case, clustering by thresholding\npairwise distances between points can fail. We also exhibit numerical evidence\nof high-probability recovery in a substantially more permissive regime.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:38:18 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 03:56:31 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Nellore", "Abhinav", ""], ["Ward", "Rachel", ""]]}, {"id": "1309.3418", "submitter": "Suranjan Ganguly", "authors": "Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu", "title": "A Novel Approach in detecting pose orientation of a 3D face required for\n  face", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we present a novel approach that takes as input a 3D image and\ngives as output its pose i.e. it tells whether the face is oriented with\nrespect the X, Y or Z axes with angles of rotation up to 40 degree. All the\nexperiments have been performed on the FRAV3D Database. After applying the\nproposed algorithm to the 3D facial surface we have obtained i.e. on 848 3D\nface images our method detected the pose correctly for 566 face images,thus\ngiving an approximately 67 % of correct pose detection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 10:12:22 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.3425", "submitter": "Suranjan Ganguly", "authors": "Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu", "title": "A method for nose-tip based 3D face registration using maximum intensity\n  algorithm", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we present a novel technique of registering 3D images across\npose. In this context, we have taken into account the images which are aligned\nacross X, Y and Z axes. We have first determined the angle across which the\nimage is rotated with respect to X, Y and Z axes and then translation is\nperformed on the images. After testing the proposed method on 472 images from\nthe FRAV3D database, the method correctly registers 358 images thus giving a\nperformance rate of 75.84%.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 11:28:29 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak kr.", ""]]}, {"id": "1309.3809", "submitter": "Ishani Chakraborty", "authors": "Ishani Chakraborty and Ahmed Elgammal", "title": "Visual-Semantic Scene Understanding by Sharing Labels in a Context\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of naming objects in complex, natural scenes\ncontaining widely varying object appearance and subtly different names.\nInformed by cognitive research, we propose an approach based on sharing context\nbased object hypotheses between visual and lexical spaces. To this end, we\npresent the Visual Semantic Integration Model (VSIM) that represents object\nlabels as entities shared between semantic and visual contexts and infers a new\nimage by updating labels through context switching. At the core of VSIM is a\nsemantic Pachinko Allocation Model and a visual nearest neighbor Latent\nDirichlet Allocation Model. For inference, we derive an iterative Data\nAugmentation algorithm that pools the label probabilities and maximizes the\njoint label posterior of an image. Our model surpasses the performance of\nstate-of-art methods in several visual tasks on the challenging SUN09 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 00:22:01 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Chakraborty", "Ishani", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1309.3842", "submitter": "Anne Marie Svane", "authors": "Anne Marie Svane", "title": "Estimation of intrinsic volumes from digital grey-scale images", "comments": "33 pages", "journal-ref": null, "doi": "10.1007/s10851-013-0469-9", "report-no": null, "categories": "math.ST cs.CV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local algorithms are common tools for estimating intrinsic volumes from\nblack-and-white digital images. However, these algorithms are typically biased\nin the design based setting, even when the resolution tends to infinity.\nMoreover, images recorded in practice are most often blurred grey-scale images\nrather than black-and-white. In this paper, an extended definition of local\nalgorithms, applying directly to grey-scale images without thresholding, is\nsuggested. We investigate the asymptotics of these new algorithms when the\nresolution tends to infinity and apply this to construct estimators for surface\narea and integrated mean curvature that are asymptotically unbiased in certain\nnatural settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 07:56:12 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Svane", "Anne Marie", ""]]}, {"id": "1309.3848", "submitter": "Michael Van den Bergh", "authors": "Michael Van den Bergh, Xavier Boix, Gemma Roig, Luc Van Gool", "title": "SEEDS: Superpixels Extracted via Energy-Driven Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel algorithms aim to over-segment the image by grouping pixels that\nbelong to the same object. Many state-of-the-art superpixel algorithms rely on\nminimizing objective functions to enforce color ho- mogeneity. The optimization\nis accomplished by sophis- ticated methods that progressively build the\nsuperpix- els, typically by adding cuts or growing superpixels. As a result,\nthey are computationally too expensive for real-time applications. We introduce\na new approach based on a simple hill-climbing optimization. Starting from an\ninitial superpixel partitioning, it continuously refines the superpixels by\nmodifying the boundaries. We define a robust and fast to evaluate energy\nfunction, based on enforcing color similarity between the bound- aries and the\nsuperpixel color histogram. In a series of experiments, we show that we achieve\nan excellent com- promise between accuracy and efficiency. We are able to\nachieve a performance comparable to the state-of- the-art, but in real-time on\na single Intel i7 CPU at 2.8GHz.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 08:23:10 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Bergh", "Michael Van den", ""], ["Boix", "Xavier", ""], ["Roig", "Gemma", ""], ["Van Gool", "Luc", ""]]}, {"id": "1309.4024", "submitter": "Patrick C. McGuire", "authors": "P.C. McGuire, A. Bonnici, K.R. Bruner, C. Gross, J. Orm\\\"o, R.A.\n  Smosna, S. Walter, L. Wendt", "title": "The Cyborg Astrobiologist: Matching of Prior Textures by Image\n  Compression for Geological Mapping and Novelty Detection", "comments": "27 pages, 3 figures, 2 tables, accepted for publication in the\n  International Journal of Astrobiology", "journal-ref": "International Journal of Astrobiology, 13(03), pp. 191-202 (2014)", "doi": "10.1017/S1473550413000372", "report-no": null, "categories": "cs.CV astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (abridged) We describe an image-comparison technique of Heidemann and Ritter\nthat uses image compression, and is capable of: (i) detecting novel textures in\na series of images, as well as of: (ii) alerting the user to the similarity of\na new image to a previously-observed texture. This image-comparison technique\nhas been implemented and tested using our Astrobiology Phone-cam system, which\nemploys Bluetooth communication to send images to a local laptop server in the\nfield for the image-compression analysis. We tested the system in a field site\ndisplaying a heterogeneous suite of sandstones, limestones, mudstones and\ncoalbeds. Some of the rocks are partly covered with lichen. The image-matching\nprocedure of this system performed very well with data obtained through our\nfield test, grouping all images of yellow lichens together and grouping all\nimages of a coal bed together, and giving a 91% accuracy for similarity\ndetection. Such similarity detection could be employed to make maps of\ndifferent geological units. The novelty-detection performance of our system was\nalso rather good (a 64% accuracy). Such novelty detection may become valuable\nin searching for new geological units, which could be of astrobiological\ninterest. The image-comparison technique is an unsupervised technique that is\nnot capable of directly classifying an image as containing a particular\ngeological feature; labeling of such geological features is done post facto by\nhuman geologists associated with this study, for the purpose of analyzing the\nsystem's performance. By providing more advanced capabilities for similarity\ndetection and novelty detection, this image-compression technique could be\nuseful in giving more scientific autonomy to robotic planetary rovers, and in\nassisting human astronauts in their geological exploration and assessment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 16:32:35 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["McGuire", "P. C.", ""], ["Bonnici", "A.", ""], ["Bruner", "K. R.", ""], ["Gross", "C.", ""], ["Orm\u00f6", "J.", ""], ["Smosna", "R. A.", ""], ["Walter", "S.", ""], ["Wendt", "L.", ""]]}, {"id": "1309.4061", "submitter": "Andreas Christian M\\\"uller", "authors": "Andreas Christian Mueller, Sven Behnke", "title": "Learning a Loopy Model For Semantic Segmentation Exactly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structured models using maximum margin techniques has become an\nindispensable tool for com- puter vision researchers, as many computer vision\napplications can be cast naturally as an image labeling problem. Pixel-based or\nsuperpixel-based conditional random fields are particularly popular examples.\nTyp- ically, neighborhood graphs, which contain a large number of cycles, are\nused. As exact inference in loopy graphs is NP-hard in general, learning these\nmodels without approximations is usually deemed infeasible. In this work we\nshow that, despite the theoretical hardness, it is possible to learn loopy\nmodels exactly in practical applications. To this end, we analyze the use of\nmultiple approximate inference techniques together with cutting plane training\nof structural SVMs. We show that our proposed method yields exact solutions\nwith an optimality guarantees in a computer vision application, for little\nadditional computational cost. We also propose a dynamic caching scheme to\naccelerate training further, yielding runtimes that are comparable with\napproximate methods. We hope that this insight can lead to a reconsideration of\nthe tractability of loopy models in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 18:30:41 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Mueller", "Andreas Christian", ""], ["Behnke", "Sven", ""]]}, {"id": "1309.4151", "submitter": "Qiyu Jin", "authors": "Qiyu Jin, Ion Grama and Quansheng Liu", "title": "A Non-Local Means Filter for Removing the Poisson Noise", "comments": "24pages,6figures. arXiv admin note: text overlap with\n  arXiv:1211.6143, arXiv:1201.5968", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new image denoising algorithm to deal with the Poisson noise model is\ngiven, which is based on the idea of Non-Local Mean. By using the \"Oracle\"\nconcept, we establish a theorem to show that the Non-Local Means Filter can\neffectively deal with Poisson noise with some modification. Under the\ntheoretical result, we construct our new algorithm called Non-Local Means\nPoisson Filter and demonstrate in theory that the filter converges at the usual\noptimal rate. The filter is as simple as the classic Non-Local Means and the\nsimulation results show that our filter is very competitive.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 02:06:19 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Jin", "Qiyu", ""], ["Grama", "Ion", ""], ["Liu", "Quansheng", ""]]}, {"id": "1309.4306", "submitter": "Raja Giryes", "authors": "Raja Giryes and Michael Elad", "title": "Sparsity Based Poisson Denoising with Dictionary Learning", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2014.2362057", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Poisson denoising appears in various imaging applications,\nsuch as low-light photography, medical imaging and microscopy. In cases of high\nSNR, several transformations exist so as to convert the Poisson noise into an\nadditive i.i.d. Gaussian noise, for which many effective algorithms are\navailable. However, in a low SNR regime, these transformations are\nsignificantly less accurate, and a strategy that relies directly on the true\nnoise statistics is required. A recent work by Salmon et al. took this route,\nproposing a patch-based exponential image representation model based on GMM\n(Gaussian mixture model), leading to state-of-the-art results. In this paper,\nwe propose to harness sparse-representation modeling to the image patches,\nadopting the same exponential idea. Our scheme uses a greedy pursuit with\nboot-strapping based stopping condition and dictionary learning within the\ndenoising process. The reconstruction performance of the proposed scheme is\ncompetitive with leading methods in high SNR, and achieving state-of-the-art\nresults in cases of low SNR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 13:46:26 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 16:16:34 GMT"}, {"version": "v3", "created": "Tue, 14 Oct 2014 13:24:34 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Giryes", "Raja", ""], ["Elad", "Michael", ""]]}, {"id": "1309.4385", "submitter": "Gregory Howland", "authors": "Gregory A. Howland, Daniel J. Lum, Matthew R. Ware and John C. Howell", "title": "Photon counting compressive depth mapping", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1364/OE.21.023822", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a compressed sensing, photon counting lidar system based on\nthe single-pixel camera. Our technique recovers both depth and intensity maps\nfrom a single under-sampled set of incoherent, linear projections of a scene of\ninterest at ultra-low light levels around 0.5 picowatts. Only two-dimensional\nreconstructions are required to image a three-dimensional scene. We demonstrate\nintensity imaging and depth mapping at 256 x 256 pixel transverse resolution\nwith acquisition times as short as 3 seconds. We also show novelty filtering,\nreconstructing only the difference between two instances of a scene. Finally,\nwe acquire 32 x 32 pixel real-time video for three-dimensional object tracking\nat 14 frames-per-second.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 16:51:32 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Howland", "Gregory A.", ""], ["Lum", "Daniel J.", ""], ["Ware", "Matthew R.", ""], ["Howell", "John C.", ""]]}, {"id": "1309.4426", "submitter": "Christian Widmer", "authors": "Christian Widmer, Philipp Drewe, Xinghua Lou, Shefali Umrania,\n  Stephanie Heinrich, Gunnar R\\\"atsch", "title": "GRED: Graph-Regularized 3D Shape Reconstruction from Highly Anisotropic\n  and Noisy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of microscopy images can provide insight into many biological\nprocesses. One particularly challenging problem is cell nuclear segmentation in\nhighly anisotropic and noisy 3D image data. Manually localizing and segmenting\neach and every cell nuclei is very time consuming, which remains a bottleneck\nin large scale biological experiments. In this work we present a tool for\nautomated segmentation of cell nuclei from 3D fluorescent microscopic data. Our\ntool is based on state-of-the-art image processing and machine learning\ntechniques and supports a friendly graphical user interface (GUI). We show that\nour tool is as accurate as manual annotation but greatly reduces the time for\nthe registration.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 18:55:37 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Widmer", "Christian", ""], ["Drewe", "Philipp", ""], ["Lou", "Xinghua", ""], ["Umrania", "Shefali", ""], ["Heinrich", "Stephanie", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1309.4573", "submitter": "Suranjan Ganguly", "authors": "Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu", "title": "A novel approach for nose tip detection using smoothing by weighted\n  median filtering applied to 3D face images in variant poses", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 08:40:21 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.4577", "submitter": "Suranjan Ganguly", "authors": "Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu", "title": "Detection of pose orientation across single and multiple axes in case of\n  3D face images", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we propose a new approach that takes as input a 3D face image\nacross X, Y and Z axes as well as both Y and X axes and gives output as its\npose i.e. it tells whether the face is oriented with respect the X, Y or Z axes\nor is it oriented across multiple axes with angles of rotation up to 42 degree.\nAll the experiments have been performed on the FRAV3D, GAVADB and Bosphorus\ndatabase which has two figures of each individual across multiple axes. After\napplying the proposed algorithm to the 3D facial surface from FRAV3D on 848 3D\nfaces, 566 3D faces were correctly recognized for pose thus giving 67% of\ncorrect identification rate. We had experimented on 420 images from the GAVADB\ndatabase, and only 336 images were detected for correct pose identification\nrate i.e. 80% and from Bosphorus database on 560 images only 448 images were\ndetected for correct pose identification i.e. 80%.abstract goes here.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 08:47:34 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.4582", "submitter": "Suranjan Ganguly", "authors": "Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu", "title": "A novel approach to nose-tip and eye corners detection using H-K\n  Curvature Analysis in case of 3D images", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we present a novel method that combines a HK curvature-based\napproach for three-dimensional (3D) face detection in different poses (X-axis,\nY-axis and Z-axis). Salient face features, such as the eyes and nose, are\ndetected through an analysis of the curvature of the entire facial surface. All\nthe experiments have been performed on the FRAV3D Database. After applying the\nproposed algorithm to the 3D facial surface we have obtained considerably good\nresults i.e. on 752 3D face images our method detected the eye corners for 543\nface images, thus giving a 72.20% of eye corners detection and 743 face images\nfor nose-tip detection thus giving a 98.80% of good nose tip localization\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 09:02:56 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1309.5004", "submitter": "Deborah Pereg Bsc.", "authors": "Deborah Pereg, Doron Benzvi", "title": "Blind Deconvolution via Maximum Kurtosis Adaptive Filtering", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for identifying a parametrically\ndescribed destructive unknown system based on a non-gaussianity measure. It is\nknown that under certain conditions the output of a linear system is more\ngaussian than the input. Hence, an inverse filter is searched, such that its\noutput is minimally gaussian. We use the kurtosis as a measure of the\nnon-gaussianity of the signal. A maximum of the kurtosis as a function of the\ndeconvolving filter coefficients is searched. The search is done iteratively\nusing the gradient ascent algorithm, and the coefficients at the maximum point\ncorrespond to the inverse filter coefficients. This filter may be applied to\nthe distorted signal to obtain the original undistorted signal. While a similar\napproach has been used before, it was always directed at a particular kind of a\nsignal, commonly of impulsive characteristics. In this paper a successful\nattempt has been made to apply the algorithm to a wider range of signals, such\nas to process distorted audio signals and destructed images. This innovative\nimplementation required the revelation of a way to preprocess the distorted\nsignal at hand. The experimental results show very good performance in terms of\nrecovering audio signals and blurred images, both for an FIR and IIR distorting\nfilters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 14:45:54 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Pereg", "Deborah", ""], ["Benzvi", "Doron", ""]]}, {"id": "1309.5174", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, N. Siddharth, Jeffrey Mark Siskind", "title": "Saying What You're Looking For: Linguistics Meets Video Search", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to searching large video corpora for video clips which\ndepict a natural-language query in the form of a sentence. This approach uses\ncompositional semantics to encode subtle meaning that is lost in other systems,\nsuch as the difference between two sentences which have identical words but\nentirely different meaning: \"The person rode the horse} vs. \\emph{The horse\nrode the person\". Given a video-sentence pair and a natural-language parser,\nalong with a grammar that describes the space of sentential queries, we produce\na score which indicates how well the video depicts the sentence. We produce\nsuch a score for each video clip in a corpus and return a ranked list of clips.\nFurthermore, this approach addresses two fundamental problems simultaneously:\ndetecting and tracking objects, and recognizing whether those tracks depict the\nquery. Because both tracking and object detection are unreliable, this uses\nknowledge about the intended sentential query to focus the tracker on the\nrelevant participants and ensures that the resulting tracks are described by\nthe sentential query. While earlier work was limited to single-word queries\nwhich correspond to either verbs or nouns, we show how one can search for\ncomplex queries which contain multiple phrases, such as prepositional phrases,\nand modifiers, such as adverbs. We demonstrate this approach by searching for\n141 queries involving people and horses interacting with each other in 10\nfull-length Hollywood movies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 05:07:29 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Barbu", "Andrei", ""], ["Siddharth", "N.", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1309.5357", "submitter": "Vikas Dongre", "authors": "Vikas J. Dongre and Vijay H.Mankar", "title": "Development of Comprehensive Devnagari Numeral and Character Database\n  for Offline Handwritten Character Recognition", "comments": "5 pages, 8 figures, journal paper", "journal-ref": "Vikas J. Dongre,Vijay H.Mankar, \"Development of Comprehensive\n  Devnagari Numeral and Character Database for Offline Handwritten Character\n  Recognition\", Applied Computational Intelligence and Soft Computing,Volume\n  2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In handwritten character recognition, benchmark database plays an important\nrole in evaluating the performance of various algorithms and the results\nobtained by various researchers. In Devnagari script, there is lack of such\nofficial benchmark. This paper focuses on the generation of offline benchmark\ndatabase for Devnagari handwritten numerals and characters. The present work\ngenerated 5137 and 20305 isolated samples for numeral and character database,\nrespectively, from 750 writers of all ages, sex, education, and profession. The\noffline sample images are stored in TIFF image format as it occupies less\nmemory. Also, the data is presented in binary level so that memory requirement\nis further reduced. It will facilitate research on handwriting recognition of\nDevnagari script through free access to the researchers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2013 03:09:50 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Dongre", "Vikas J.", ""], ["Mankar", "Vijay H.", ""]]}, {"id": "1309.5401", "submitter": "Nikolay Atanasov", "authors": "Nikolay Atanasov and Bharath Sankaran and Jerome Le Ny and George J.\n  Pappas and Kostas Daniilidis", "title": "Nonmyopic View Planning for Active Object Detection", "comments": "12 pages (two-column); 7 figures; 2 tables; Manuscript submitted to\n  the IEEE Transactions on Robotics (TRO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central problems in computer vision is the detection of\nsemantically important objects and the estimation of their pose. Most of the\nwork in object detection has been based on single image processing and its\nperformance is limited by occlusions and ambiguity in appearance and geometry.\nThis paper proposes an active approach to object detection by controlling the\npoint of view of a mobile depth camera. When an initial static detection phase\nidentifies an object of interest, several hypotheses are made about its class\nand orientation. The sensor then plans a sequence of views, which balances the\namount of energy used to move with the chance of identifying the correct\nhypothesis. We formulate an active hypothesis testing problem, which includes\nsensor mobility, and solve it using a point-based approximate POMDP algorithm.\nThe validity of our approach is verified through simulation and real-world\nexperiments with the PR2 robot. The results suggest that our approach\noutperforms the widely-used greedy view point selection and provides a\nsignificant improvement over static object detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 21:35:21 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Atanasov", "Nikolay", ""], ["Sankaran", "Bharath", ""], ["Ny", "Jerome Le", ""], ["Pappas", "George J.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1309.5427", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "Latent Fisher Discriminant Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Linear Discriminant Analysis (LDA) is a well-known method for dimensionality\nreduction and classification. Previous studies have also extended the\nbinary-class case into multi-classes. However, many applications, such as\nobject detection and keyframe extraction cannot provide consistent\ninstance-label pairs, while LDA requires labels on instance level for training.\nThus it cannot be directly applied for semi-supervised classification problem.\nIn this paper, we overcome this limitation and propose a latent variable Fisher\ndiscriminant analysis model. We relax the instance-level labeling into\nbag-level, is a kind of semi-supervised (video-level labels of event type are\nrequired for semantic frame extraction) and incorporates a data-driven prior\nover the latent variables. Hence, our method combines the latent variable\ninference and dimension reduction in an unified bayesian framework. We test our\nmethod on MUSK and Corel data sets and yield competitive results compared to\nthe baseline approach. We also demonstrate its capacity on the challenging\nTRECVID MED11 dataset for semantic keyframe extraction and conduct a\nhuman-factors ranking-based experimental evaluation, which clearly demonstrates\nour proposed method consistently extracts more semantically meaningful\nkeyframes than challenging baselines.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 03:42:04 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1309.5594", "submitter": "Chunhua Shen", "authors": "Fumin Shen and Chunhua Shen", "title": "Generic Image Classification Approaches Excel on Face Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main finding of this work is that the standard image classification\npipeline, which consists of dictionary learning, feature encoding, spatial\npyramid pooling and linear classification, outperforms all state-of-the-art\nface recognition methods on the tested benchmark datasets (we have tested on\nAR, Extended Yale B, the challenging FERET, and LFW-a datasets). This\nsurprising and prominent result suggests that those advances in generic image\nclassification can be directly applied to improve face recognition systems. In\nother words, face recognition may not need to be viewed as a separate object\nclassification problem.\n  While recently a large body of residual based face recognition methods focus\non developing complex dictionary learning algorithms, in this work we show that\na dictionary of randomly extracted patches (even from non-face images) can\nachieve very promising results using the image classification pipeline. That\nmeans, the choice of dictionary learning methods may not be important. Instead,\nwe find that learning multiple dictionaries using different low-level image\nfeatures often improve the final classification accuracy. Our proposed face\nrecognition approach offers the best reported results on the widely-used face\nrecognition benchmark datasets. In particular, on the challenging FERET and\nLFW-a datasets, we improve the best reported accuracies in the literature by\nabout 20% and 30% respectively.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 11:52:03 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 03:23:36 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""]]}, {"id": "1309.5655", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "A new look at reweighted message passing", "comments": "TPAMI accepted version", "journal-ref": "TPAMI, 37(5):919-930 (May, 2015)", "doi": "10.1109/TPAMI.2014.2363465", "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of message passing techniques for MAP estimation in\ngraphical models which we call {\\em Sequential Reweighted Message Passing}\n(SRMP). Special cases include well-known techniques such as {\\em Min-Sum\nDiffusion} (MSD) and a faster {\\em Sequential Tree-Reweighted Message Passing}\n(TRW-S). Importantly, our derivation is simpler than the original derivation of\nTRW-S, and does not involve a decomposition into trees. This allows easy\ngeneralizations. We present such a generalization for the case of higher-order\ngraphical models, and test it on several real-world problems with promising\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 21:19:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 11:57:31 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 17:45:24 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1309.6195", "submitter": "Benyuan Liu", "authors": "Benyuan Liu and Hongqi Fan and Zaiqi Lu and Qiang Fu", "title": "Scan-based Compressed Terahertz Imaging and Real-Time Reconstruction via\n  the Complex-valued Fast Block Sparse Bayesian Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing based Terahertz imaging (CS-THz) is a computational\nimaging technique. It uses only one THz receiver to accumulate the random\nmodulated image measurements where the original THz image is reconstruct from\nthese measurements using compressed sensing solvers. The advantage of the\nCS-THz is its reduced acquisition time compared with the raster scan mode.\nHowever, when it applied to large-scale two-dimensional (2D) imaging, the\nincreased dimension resulted in both high computational complexity and\nexcessive memory usage. In this paper, we introduced a novel CS-based THz\nimaging system that progressively compressed the THz image column by column.\nTherefore, the CS-THz system could be simplified with a much smaller sized\nmodulator and reduced dimension. In order to utilize the block structure and\nthe correlation of adjacent columns of the THz image, a complex-valued block\nsparse Bayesian learning algorithm was proposed. We conducted systematic\nevaluation of state-of-the-art CS algorithms under the scan based CS-THz\narchitecture. The compression ratios and the choices of the sensing matrices\nwere analyzed in detail using both synthetic and real-life THz images.\nSimulation results showed that both the scan based architecture and the\nproposed recovery algorithm were superior and efficient for large scale CS-THz\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 23:08:27 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Liu", "Benyuan", ""], ["Fan", "Hongqi", ""], ["Lu", "Zaiqi", ""], ["Fu", "Qiang", ""]]}, {"id": "1309.6301", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Solving OSCAR regularization problems by proximal splitting algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OSCAR (octagonal selection and clustering algorithm for regression)\nregularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for\nits grouping behavior) and was proposed to encourage group sparsity in\nscenarios where the groups are a priori unknown. The OSCAR regularizer has a\nnon-trivial proximity operator, which limits its applicability. We reformulate\nthis regularizer as a weighted sorted L_1 norm, and propose its grouping\nproximity operator (GPO) and approximate proximity operator (APO), thus making\nstate-of-the-art proximal splitting algorithms (PSAs) available to solve\ninverse problems with OSCAR regularization. The GPO is in fact the APO followed\nby additional grouping and averaging operations, which are costly in time and\nstorage, explaining the reason why algorithms with APO are much faster than\nthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an\nexact proximity operator. Although convergence of PSAs with APO is may not be\nguaranteed, we have experimentally found that APO behaves similarly to GPO when\nthe regularization parameter of the pair-wise L_inf norm is set to an\nappropriately small value. Experiments on recovery of group-sparse signals\n(with unknown groups) show that PSAs with APO are very fast and accurate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 19:48:56 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 19:36:41 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1309.6379", "submitter": "ANqi Qiu DR", "authors": "Jia Du, A. Pasha Hosseinbor, Moo K. Chung, Barbara B. Bendlin, Gaurav\n  Suryawanshi, Andrew L. Alexander, Anqi Qiu", "title": "Diffeomorphic Metric Mapping and Probabilistic Atlas Generation of\n  Hybrid Diffusion Imaging based on BFOR Signal Basis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 01:57:50 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Du", "Jia", ""], ["Hosseinbor", "A. Pasha", ""], ["Chung", "Moo K.", ""], ["Bendlin", "Barbara B.", ""], ["Suryawanshi", "Gaurav", ""], ["Alexander", "Andrew L.", ""], ["Qiu", "Anqi", ""]]}, {"id": "1309.6390", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovi\\'c", "title": "Contextually learnt detection of unusual motion-based behaviour in\n  crowded public spaces", "comments": null, "journal-ref": "Computer and Information Sciences II Computer and Information\n  Sciences II, pp 403-410, 2012", "doi": "10.1007/978-1-4471-2155-8_51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in analyzing behaviour in crowded public\nplaces at the level of holistic motion. Our aim is to learn, without user\ninput, strong scene priors or labelled data, the scope of \"normal behaviour\"\nfor a particular scene and thus alert to novelty in unseen footage. The first\ncontribution is a low-level motion model based on what we term tracklet\nprimitives, which are scene-specific elementary motions. We propose a\nclustering-based algorithm for tracklet estimation from local approximations to\ntracks of appearance features. This is followed by two methods for motion\nnovelty inference from tracklet primitives: (a) we describe an approach based\non a non-hierarchial ensemble of Markov chains as a means of capturing\nbehavioural characteristics at different scales, and (b) a more flexible\nalternative which exhibits a higher generalizing power by accounting for\nconstraints introduced by intentionality and goal-oriented planning of human\nmotion in a particular scene. Evaluated on a 2h long video of a busy city\nmarketplace, both algorithms are shown to be successful at inferring unusual\nbehaviour, the latter model achieving better performance for novelties at a\nlarger spatial scale.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 03:22:59 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Arandjelovi\u0107", "Ognjen", ""]]}, {"id": "1309.6391", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Rhys Martin, Ognjen Arandjelovi\\'c", "title": "Multiple-object tracking in cluttered and crowded public spaces", "comments": null, "journal-ref": "Lecture Notes in Computer Science, volume 6455, pp 89-98, 2010", "doi": "10.1007/978-3-642-17277-9_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of tracking moving objects of variable\nappearance in challenging scenes rich with features and texture. Reliable\ntracking is of pivotal importance in surveillance applications. It is made\nparticularly difficult by the nature of objects encountered in such scenes:\nthese too change in appearance and scale, and are often articulated (e.g.\nhumans). We propose a method which uses fast motion detection and segmentation\nas a constraint for both building appearance models and their robust\npropagation (matching) in time. The appearance model is based on sets of local\nappearances automatically clustered using spatio-kinetic similarity, and is\nupdated with each new appearance seen. This integration of all seen appearances\nof a tracked object makes it extremely resilient to errors caused by occlusion\nand the lack of permanence of due to low data quality, appearance change or\nbackground clutter. These theoretical strengths of our algorithm are\nempirically demonstrated on two hour long video footage of a busy city\nmarketplace.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 03:34:01 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Martin", "Rhys", ""], ["Arandjelovi\u0107", "Ognjen", ""]]}, {"id": "1309.6487", "submitter": "Xi Peng", "authors": "Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, and Shijie Xiao", "title": "A Unified Framework for Representation-based Subspace Clustering of\n  Out-of-sample and Large-scale Data", "comments": "in IEEE Trans. on Neural Networks and Learning Systems, 2015", "journal-ref": "IEEE Trans. on Neural Networks and Learning Systems, vol. 27, no.\n  12, pp. 2499-2512, Dec. 2016", "doi": "10.1109/TNNLS.2015.2490080", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the framework of spectral clustering, the key of subspace clustering is\nbuilding a similarity graph which describes the neighborhood relations among\ndata points. Some recent works build the graph using sparse, low-rank, and\n$\\ell_2$-norm-based representation, and have achieved state-of-the-art\nperformance. However, these methods have suffered from the following two\nlimitations. First, the time complexities of these methods are at least\nproportional to the cube of the data size, which make those methods inefficient\nfor solving large-scale problems. Second, they cannot cope with out-of-sample\ndata that are not used to construct the similarity graph. To cluster each\nout-of-sample datum, the methods have to recalculate the similarity graph and\nthe cluster membership of the whole data set. In this paper, we propose a\nunified framework which makes representation-based subspace clustering\nalgorithms feasible to cluster both out-of-sample and large-scale data. Under\nour framework, the large-scale problem is tackled by converting it as\nout-of-sample problem in the manner of \"sampling, clustering, coding, and\nclassifying\". Furthermore, we give an estimation for the error bounds by\ntreating each subspace as a point in a hyperspace. Extensive experimental\nresults on various benchmark data sets show that our methods outperform several\nrecently-proposed scalable methods in clustering large-scale data set.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 12:53:13 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 14:43:50 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Tang", "Huajin", ""], ["Zhang", "Lei", ""], ["Yi", "Zhang", ""], ["Xiao", "Shijie", ""]]}, {"id": "1309.6691", "submitter": "Chunhua Shen", "authors": "Yao Li, Wenjing Jia, Chunhua Shen, Anton van den Hengel", "title": "Characterness: An Indicator of Text in the Wild", "comments": "11 pages; Appearing in IEEE Trans. on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text in an image provides vital information for interpreting its contents,\nand text in a scene can aide with a variety of tasks from navigation, to\nobstacle avoidance, and odometry. Despite its value, however, identifying\ngeneral text in images remains a challenging research problem. Motivated by the\nneed to consider the widely varying forms of natural text, we propose a\nbottom-up approach to the problem which reflects the `characterness' of an\nimage region. In this sense our approach mirrors the move from saliency\ndetection methods to measures of `objectness'. In order to measure the\ncharacterness we develop three novel cues that are tailored for character\ndetection, and a Bayesian method for their integration. Because text is made up\nof sets of characters, we then design a Markov random field (MRF) model so as\nto exploit the inherent dependencies between characters.\n  We experimentally demonstrate the effectiveness of our characterness cues as\nwell as the advantage of Bayesian multi-cue integration. The proposed text\ndetector outperforms state-of-the-art methods on a few benchmark scene text\ndetection datasets. We also show that our measurement of `characterness' is\nsuperior than state-of-the-art saliency detection models when applied to the\nsame task.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 23:30:18 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Li", "Yao", ""], ["Jia", "Wenjing", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1309.6964", "submitter": "Ryan Kennedy", "authors": "Ryan Kennedy, Laura Balzano, Stephen J. Wright, Camillo J. Taylor", "title": "Online Algorithms for Factorization-Based Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of online algorithms for real-time factorization-based\nstructure from motion, leveraging a relationship between incremental singular\nvalue decomposition and recently proposed methods for online matrix completion.\nOur methods are orders of magnitude faster than previous state of the art, can\nhandle missing data and a variable number of feature points, and are robust to\nnoise and sparse outliers. We demonstrate our methods on both real and\nsynthetic sequences and show that they perform well in both online and batch\nsettings. We also provide an implementation which is able to produce 3D models\nin real time using a laptop with a webcam.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 16:46:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2014 19:33:45 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 17:43:36 GMT"}, {"version": "v4", "created": "Sat, 16 Jul 2016 16:45:54 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kennedy", "Ryan", ""], ["Balzano", "Laura", ""], ["Wright", "Stephen J.", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1309.7170", "submitter": "Kiana Hajebi", "authors": "Kiana Hajebi and Hong Zhang", "title": "An Efficient Index for Visual Search in Appearance-based SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-quantization can be a computationally expensive step in visual\nbag-of-words (BoW) search when the vocabulary is large. A BoW-based appearance\nSLAM needs to tackle this problem for an efficient real-time operation. We\npropose an effective method to speed up the vector-quantization process in\nBoW-based visual SLAM. We employ a graph-based nearest neighbor search (GNNS)\nalgorithm to this aim, and experimentally show that it can outperform the\nstate-of-the-art. The graph-based search structure used in GNNS can efficiently\nbe integrated into the BoW model and the SLAM framework. The graph-based index,\nwhich is a k-NN graph, is built over the vocabulary words and can be extracted\nfrom the BoW's vocabulary construction procedure, by adding one iteration to\nthe k-means clustering, which adds small extra cost. Moreover, exploiting the\nfact that images acquired for appearance-based SLAM are sequential, GNNS search\ncan be initiated judiciously which helps increase the speedup of the\nquantization process considerably.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 09:25:47 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Hajebi", "Kiana", ""], ["Zhang", "Hong", ""]]}, {"id": "1309.7276", "submitter": "Bijeesh Tv", "authors": "Bijeesh T. V, Nimmi I. P", "title": "Adopting level set theory based algorithms to segment human ear", "comments": "15 pages", "journal-ref": null, "doi": "10.5121/ijci.2013.2407", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human identification has always been a topic that interested researchers\naround the world. Biometric methods are found to be more effective and much\neasier for the users than the traditional identification methods like keys,\nsmart cards and passwords. Unlike with the traditional methods, with biometric\nmethods the data acquisition is most of the times passive, which means the\nusers do not take active part in data acquisition. Data acquisition can be\nperformed using cameras, scanners or sensors. Human physiological biometrics\nsuch as face, eye and ear are good candidates for uniquely identifying an\nindividual. However, human ear scores over face and eye because of certain\nadvantages it has over face. The most challenging phase in human identification\nbased on ear biometric is the segmentation of the ear image from the captured\nimage which may contain many unwanted details. In this work, PDE based image\nprocessing techniques are used to segment out the ear image. Level Set Theory\nbased image processing is employed to obtain the contour of the ear image. A\nfew Level set algorithms are compared for their efficiency in segmenting test\near images.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 05:48:18 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["T.", "Bijeesh", "V"], ["P", "Nimmi I.", ""]]}, {"id": "1309.7434", "submitter": "Omar Oreifej", "authors": "Dong Zhang, Omar Oreifej, Mubarak Shah", "title": "Face Verification Using Boosted Cross-Image Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for face verification, where a pair of\nimages needs to be classified as belonging to the same person or not. This\nproblem is relatively new and not well-explored in the literature. Current\nmethods mostly adopt techniques borrowed from face recognition, and process\neach of the images in the pair independently, which is counter intuitive. In\ncontrast, we propose to extract cross-image features, i.e. features across the\npair of images, which, as we demonstrate, is more discriminative to the\nsimilarity and the dissimilarity of faces. Our features are derived from the\npopular Haar-like features, however, extended to handle the face verification\nproblem instead of face detection. We collect a large bank of cross-image\nfeatures using filters of different sizes, locations, and orientations.\nConsequently, we use AdaBoost to select and weight the most discriminative\nfeatures. We carried out extensive experiments on the proposed ideas using\nthree standard face verification datasets, and obtained promising results\noutperforming state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 06:21:18 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Zhang", "Dong", ""], ["Oreifej", "Omar", ""], ["Shah", "Mubarak", ""]]}, {"id": "1309.7484", "submitter": "Junzhou Chen", "authors": "Chen Junzhou, Li Qing, Peng Qiang and Kin Hong Wong", "title": "CSIFT Based Locality-constrained Linear Coding for Image Classification", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the past decade, SIFT descriptor has been witnessed as one of the most\nrobust local invariant feature descriptors and widely used in various vision\ntasks. Most traditional image classification systems depend on the\nluminance-based SIFT descriptors, which only analyze the gray level variations\nof the images. Misclassification may happen since their color contents are\nignored. In this article, we concentrate on improving the performance of\nexisting image classification algorithms by adding color information. To\nachieve this purpose, different kinds of colored SIFT descriptors are\nintroduced and implemented. Locality-constrained Linear Coding (LLC), a\nstate-of-the-art sparse coding technology, is employed to construct the image\nclassification system for the evaluation. The real experiments are carried out\non several benchmarks. With the enhancements of color SIFT, the proposed image\nclassification system obtains approximate 3% improvement of classification\naccuracy on the Caltech-101 dataset and approximate 4% improvement of\nclassification accuracy on the Caltech-256 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 18:05:12 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Junzhou", "Chen", ""], ["Qing", "Li", ""], ["Qiang", "Peng", ""], ["Wong", "Kin Hong", ""]]}, {"id": "1309.7512", "submitter": "Alexander Fix", "authors": "Alexander Fix and Thorsten Joachims and Sam Park and Ramin Zabih", "title": "Structured learning of sum-of-submodular higher order energy functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions can be exactly minimized in polynomial time, and the\nspecial case that graph cuts solve with max flow \\cite{KZ:PAMI04} has had\nsignificant impact in computer vision\n\\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address\nthe important class of sum-of-submodular (SoS) functions\n\\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a\nvariant of max flow called submodular flow \\cite{Edmonds:ADM77}. SoS functions\ncan naturally express higher order priors involving, e.g., local image patches;\nhowever, it is difficult to fully exploit their expressive power because they\nhave so many parameters. Rather than trying to formulate existing higher order\npriors as an SoS function, we take a discriminative learning approach,\neffectively searching the space of SoS functions for a higher order prior that\nperforms well on our training set. We adopt a structural SVM approach\n\\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training\nproblem in terms of quadratic programming; as a result we can efficiently\nsearch the space of SoS priors via an extended cutting-plane algorithm. We also\nshow how the state-of-the-art max flow method for vision problems\n\\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow\nproblem. Experimental comparisons are made against the OpenCV implementation of\nthe GrabCut interactive segmentation technique \\cite{Rother:GrabCut04}, which\nuses hand-tuned parameters instead of machine learning. On a standard dataset\n\\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of\nparameter values, and produces significantly better segmentations. While our\nfocus is on binary labeling problems, we show that our techniques can be\nnaturally generalized to handle more than two labels.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 23:55:01 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 02:45:20 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Fix", "Alexander", ""], ["Joachims", "Thorsten", ""], ["Park", "Sam", ""], ["Zabih", "Ramin", ""]]}, {"id": "1309.7522", "submitter": "Dian Pratiwi", "authors": "Dian Pratiwi, Diaz D. Santika, Bens Pardamean", "title": "An Application of Backpropagation Artificial Neural Network Method for\n  Measuring The Severity of Osteoarthritis", "comments": "4 pages, 4 figures, 3 tables", "journal-ref": "International Journal of Engineering & Technology IJET-IJENS Vol.\n  11 No.3, June 2011", "doi": null, "report-no": null, "categories": "cs.NE cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The examination of Osteoarthritis disease through X-ray by rheumatology can\nbe classified into four grade of severity. This paper discusses about the\napplication of artificial neural network backpropagation method for measuring\nthe severity of the disease, where the observed X-ray range from wrist to\nfingers. The main procedures of system in this paper is divided into three,\nwhich are image processing, feature extraction, and artificial neural network\nprocess. First, an X-ray image digital (200x150 pixels and greyscale) will be\nthresholded, then extracted features based on probabilistic values of the color\nintensity of seven bit quantization result, and statistical textures. That\nfeature values then will be normalizing to interval [0.1, 0.9], and then the\nresult would be processing on backpropagation artificial neural network system\nas input to determine the severity of disease from an X-ray had input before\nit. From testing with learning rate 0.3, momentum 0.4, hidden units five pieces\nand about 132 feature vectors, this system had had a level of accuracy of 100%\nfor learning data, 80% for learning and non-learning data, and 66.6% for\nnon-learning data\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 02:23:26 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Pratiwi", "Dian", ""], ["Santika", "Diaz D.", ""], ["Pardamean", "Bens", ""]]}, {"id": "1309.7609", "submitter": "Kevin  Rojas", "authors": "Kevin Rojas Laura, Christhian Cardenas Alvarez", "title": "Identificaci\\'on y Registro Catastral de Cuerpos de Agua mediante\n  T\\'ecnicas de Procesamiento Digital de Imagenes", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effects of global climate change on Peruvian glaciers have brought about\nseveral processes of deglaciation during the last few years. The immediate\neffect is the change of size of lakes and rivers. Public institutions that\nmonitor water resources currently have only recent studies which make up less\nthan 10% of the total. The effects of climate change and the lack of updated\ninformation intensify social-economic problems related to water resources in\nPeru. The objective of this research is to develop a software application to\nautomate the Cadastral Registry of Water Bodies in Peru, using techniques of\ndigital image processing, which would provide tools for detection, record,\ntemporal analysis and visualization of water bodies. The images used are from\nthe satellite Landsat5, which undergo a pre-processing of calibration and\ncorrection of the satellite. Detection results are archived into a file that\ncontains location vectors and images of the segmentated bodies of water.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 15:36:43 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Laura", "Kevin Rojas", ""], ["Alvarez", "Christhian Cardenas", ""]]}, {"id": "1309.7615", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "Correcting Multi-focus Images via Simple Standard Deviation for Image\n  Fusion", "comments": null, "journal-ref": "International Journal of Image, Graphics and Signal Processing\n  (IJIGSP), Vol. 5, No. 12, pp. 56-61, October 2013", "doi": "10.5815/ijigsp.2013.12.08", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Image fusion is one of the recent trends in image registration which is an\nessential field of image processing. The basic principle of this paper is to\nfuse multi-focus images using simple statistical standard deviation. Firstly,\nthe simple standard deviation for the k-by-k window inside each of the\nmulti-focus images was computed. The contribution in this paper came from the\nidea that the focused part inside an image had high details rather than the\nunfocused part. Hence, the dispersion between pixels inside the focused part is\nhigher than the dispersion inside the unfocused part. Secondly, a simple\ncomparison between the standard deviation for each k-by-k window in the\nmulti-focus images could be computed. The highest standard deviation between\nall the computed standard deviations for the multi-focus images could be\ntreated as the optimal that is to be placed in the fused image. The\nexperimental visual results show that the proposed method produces very\nsatisfactory results in spite of its simplicity.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 16:14:47 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1309.7643", "submitter": "Zhizhen Zhao", "authors": "Zhizhen Zhao and Amit Singer", "title": "Rotationally Invariant Image Representation for Viewing Direction\n  Classification in Cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new rotationally invariant viewing angle classification method\nfor identifying, among a large number of Cryo-EM projection images, similar\nviews without prior knowledge of the molecule. Our rotationally invariant\nfeatures are based on the bispectrum. Each image is denoised and compressed\nusing steerable principal component analysis (PCA) such that rotating an image\nis equivalent to phase shifting the expansion coefficients. Thus we are able to\nextend the theory of bispectrum of 1D periodic signals to 2D images. The\nrandomized PCA algorithm is then used to efficiently reduce the dimensionality\nof the bispectrum coefficients, enabling fast computation of the similarity\nbetween any pair of images. The nearest neighbors provide an initial\nclassification of similar viewing angles. In this way, rotational alignment is\nonly performed for images with their nearest neighbors. The initial nearest\nneighbor classification and alignment are further improved by a new\nclassification method called vector diffusion maps. Our pipeline for viewing\nangle classification and alignment is experimentally shown to be faster and\nmore accurate than reference-free alignment with rotationally invariant K-means\nclustering, MSA/MRA 2D classification, and their modern approximations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 19:24:51 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 21:06:18 GMT"}, {"version": "v3", "created": "Sat, 1 Mar 2014 00:56:22 GMT"}, {"version": "v4", "created": "Mon, 17 Mar 2014 23:34:52 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Zhao", "Zhizhen", ""], ["Singer", "Amit", ""]]}, {"id": "1309.7912", "submitter": "Ricardo Fabbri", "authors": "Mauro de Amorim, Ricardo Fabbri, Lucia Maria dos Santos Pinto and\n  Francisco Duarte Moura Neto", "title": "An Image-Based Fluid Surface Pattern Model", "comments": "a reduced version in Portuguese appears in proceedings of the XVI EMC\n  - Computational Modeling Meeting (Encontro de Modelagem Computacional), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This work aims at generating a model of the ocean surface and its dynamics\nfrom one or more video cameras. The idea is to model wave patterns from video\nas a first step towards a larger system of photogrammetric monitoring of marine\nconditions for use in offshore oil drilling platforms. The first part of the\nproposed approach consists in reducing the dimensionality of sensor data made\nup of the many pixels of each frame of the input video streams. This enables\nfinding a concise number of most relevant parameters to model the temporal\ndataset, yielding an efficient data-driven model of the evolution of the\nobserved surface. The second part proposes stochastic modeling to better\ncapture the patterns embedded in the data. One can then draw samples from the\nfinal model, which are expected to simulate the behavior of previously observed\nflow, in order to determine conditions that match new observations. In this\npaper we focus on proposing and discussing the overall approach and on\ncomparing two different techniques for dimensionality reduction in the first\nstage: principal component analysis and diffusion maps. Work is underway on the\nsecond stage of constructing better stochastic models of fluid surface dynamics\nas proposed here.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 16:39:21 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["de Amorim", "Mauro", ""], ["Fabbri", "Ricardo", ""], ["Pinto", "Lucia Maria dos Santos", ""], ["Neto", "Francisco Duarte Moura", ""]]}, {"id": "1309.7959", "submitter": "Laurens Bliek", "authors": "Laurens Bliek", "title": "Exploration and Exploitation in Visuomotor Prediction of Autonomous\n  Agents", "comments": "Award-winning paper of the internal conference 'Almende research\n  workshop 2013'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses various techniques to let an agent learn how to predict\nthe effects of its own actions on its sensor data autonomously, and their\nusefulness to apply them to visual sensors. An Extreme Learning Machine is used\nfor visuomotor prediction, while various autonomous control techniques that can\naid the prediction process by balancing exploration and exploitation are\ndiscussed and tested in a simple system: a camera moving over a 2D greyscale\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 07:10:53 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Bliek", "Laurens", ""]]}]