[{"id": "1301.0127", "submitter": "Madhur Srivastava", "authors": "Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi", "title": "A Semi-automated Statistical Algorithm for Object Separation", "comments": "21 pages, 9 figures, The final publication is available at\n  link.springer.com", "journal-ref": null, "doi": "10.1007/s00034-013-9613-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explicate a semi-automated statistical algorithm for object identification\nand segregation in both gray scale and color images. The algorithm makes\noptimal use of the observation that definite objects in an image are typically\nrepresented by pixel values having narrow Gaussian distributions about\ncharacteristic mean values. Furthermore, for visually distinct objects, the\ncorresponding Gaussian distributions have negligible overlap with each other\nand hence the Mahalanobis distance between these distributions are large. These\nstatistical facts enable one to sub-divide images into multiple thresholds of\nvariable sizes, each segregating similar objects. The procedure incorporates\nthe sensitivity of human eye to the gray pixel values into the variable\nthreshold size, while mapping the Gaussian distributions into localized\n\\delta-functions, for object separation. The effectiveness of this recursive\nstatistical algorithm is demonstrated using a wide variety of images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 19:51:28 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 07:35:11 GMT"}, {"version": "v3", "created": "Sat, 3 Aug 2013 00:00:41 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Srivastava", "Madhur", ""], ["Singh", "Satish K.", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1301.0167", "submitter": "Karthik  S", "authors": "H. R. Mamatha, S. Karthik, Murthy K. Srikanta", "title": "Classifier Fusion Method to Recognize Handwritten Kannada Numerals", "comments": "6 pages having 3 tables and 9 figures. Published in ICECT 2012\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) is one of the important fields in image\nprocessing and pattern recognition domain. Handwritten character recognition\nhas always been a challenging task. Only a little work can be traced towards\nthe recognition of handwritten characters for the south Indian languages.\nKannada is one such south Indian language which is also one of the official\nlanguage of India. Accurate recognition of Kannada characters is a challenging\ntask because of the high degree of similarity between the characters. Hence,\ngood quality features are to be extracted and better classifiers are needed to\nimprove the accuracy of the OCR for Kannada characters. This paper explores the\neffectiveness of feature extraction method like run length count (RLC) and\ndirectional chain code (DCC) for the recognition of handwritten Kannada\nnumerals. In this paper, a classifier fusion method is implemented to improve\nthe recognition rate. For the classifier fusion, we have considered K-nearest\nneighbour (KNN) and Linear classifier (LC). The novelty of this method is to\nachieve better accuracy with few features using classifier fusion approach.\nProposed method achieves an average recognition rate of 96%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2013 04:45:44 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Mamatha", "H. R.", ""], ["Karthik", "S.", ""], ["Srikanta", "Murthy K.", ""]]}, {"id": "1301.0432", "submitter": "Fahad Mahmood Mr", "authors": "F. Mahmood, F. Kunwar", "title": "A Self-Organizing Neural Scheme for Door Detection in Different\n  Environments", "comments": "Page No 13-18, 7 figures, Published with International Journal of\n  Computer Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 60(9):13-18, 2012", "doi": "10.5120/9719-3679", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doors are important landmarks for indoor mobile robot navigation and also\nassist blind people to independently access unfamiliar buildings. Most existing\nalgorithms of door detection are limited to work for familiar environments\nbecause of restricted assumptions about color, texture and shape. In this paper\nwe propose a novel approach which employs feature based classification and uses\nthe Kohonen Self-Organizing Map (SOM) for the purpose of door detection.\nGeneric and stable features are used for the training of SOM that increase the\nperformance significantly: concavity, bottom-edge intensity profile and door\nedges. To validate the robustness and generalizability of our method, we\ncollected a large dataset of real world door images from a variety of\nenvironments and different lighting conditions. The algorithm achieves more\nthan 95% detection which demonstrates that our door detection method is generic\nand robust with variations of color, texture, occlusions, lighting condition,\nscales, and viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 12:04:28 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Mahmood", "F.", ""], ["Kunwar", "F.", ""]]}, {"id": "1301.0435", "submitter": "Fahad Mahmood Mr", "authors": "F. Mahmood, Syed. M. B. Haider, F. Kunwar", "title": "Investigating the performance of Correspondence Algorithms in Vision\n  based Driver-assistance in Indoor Environment", "comments": "7 pages, 9 figures,Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "IJCA 60(9):6-12, 2012", "doi": "10.5120/9718-3663", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the experimental comparison of fourteen stereo matching\nalgorithms in variant illumination conditions. Different adaptations of global\nand local stereo matching techniques are chosen for evaluation The variant\nstrength and weakness of the chosen correspondence algorithms are explored by\nemploying the methodology of the prediction error strategy. The algorithms are\ngauged on the basis of their performance on real world data set taken in\nvarious indoor lighting conditions and at different times of the day\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 12:16:25 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Mahmood", "F.", ""], ["Haider", "Syed. M. B.", ""], ["Kunwar", "F.", ""]]}, {"id": "1301.0612", "submitter": "Yang Wang", "authors": "Yang Wang, Tele Tan", "title": "Adaptive Foreground and Shadow Detection inImage Sequences", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-552-559", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method of foreground segmentation that\ndistinguishes moving objects from their moving cast shadows in monocular image\nsequences. The models of background, edge information, and shadow are set up\nand adaptively updated. A Bayesian belief network is proposed to describe the\nrelationships among the segmentation label, background, intensity, and edge\ninformation. The notion of Markov random field is used to encourage the spatial\nconnectivity of the segmented regions. The solution is obtained by maximizing\nthe posterior possibility density of the segmentation field.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:59:10 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Wang", "Yang", ""], ["Tan", "Tele", ""]]}, {"id": "1301.0998", "submitter": "Sambit Bakshi", "authors": "Sambit Bakshi, Hunny Mehrotra, Banshidhar Majhi", "title": "Stratified SIFT Matching for Human Iris Recognition", "comments": "7 pages", "journal-ref": "Proceedings of International Conference on Advances in Computing\n  and Communications (ACC-2011)", "doi": "10.1007/978-3-642-22720-2_17", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient three fold stratified SIFT matching for iris\nrecognition. The objective is to filter wrongly paired conventional SIFT\nmatches. In Strata I, the keypoints from gallery and probe iris images are\npaired using traditional SIFT approach. Due to high image similarity at\ndifferent regions of iris there may be some impairments. These are detected and\nfiltered by finding gradient of paired keypoints in Strata II. Further, the\nscaling factor of paired keypoints is used to remove impairments in Strata III.\nThe pairs retained after Strata III are likely to be potential matches for iris\nrecognition. The proposed system performs with an accuracy of 96.08% and 97.15%\non publicly available CASIAV3 and BATH databases respectively. This marks\nsignificant improvement of accuracy and FAR over the existing SIFT matching for\niris.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 12:05:28 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Bakshi", "Sambit", ""], ["Mehrotra", "Hunny", ""], ["Majhi", "Banshidhar", ""]]}, {"id": "1301.1295", "submitter": "Roberto Herrera", "authors": "Roberto H. Herrera, Jean-Baptiste Tary and Mirko van der Baan", "title": "Time-Frequency Representation of Microseismic Signals using the\n  Synchrosqueezing Transform", "comments": "4 pages, 2 figures, GeoConvention 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resonance frequencies can provide useful information on the deformation\noccurring during fracturing experiments or $CO_2$ management, complementary to\nthe microseismic event distribution. An accurate time-frequency representation\nis of crucial importance prior to interpreting the cause of resonance\nfrequencies during microseismic experiments. The popular methods of Short-Time\nFourier Transform (STFT) and wavelet analysis have limitations in representing\nclose frequencies and dealing with fast varying instantaneous frequencies and\nthis is often the nature of microseismic signals. The synchrosqueezing\ntransform (SST) is a promising tool to track these resonant frequencies and\nprovide a detailed time-frequency representation. Here we apply the\nsynchrosqueezing transform to microseismic signals and also show its potential\nto general seismic signal processing applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 18:26:25 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Herrera", "Roberto H.", ""], ["Tary", "Jean-Baptiste", ""], ["van der Baan", "Mirko", ""]]}, {"id": "1301.1374", "submitter": "Rituparna Sarkar", "authors": "R. Sarkar, S. Das and N. Vaswani", "title": "PaFiMoCS: Particle Filtered Modified-CS and Applications in Visual\n  Tracking across Illumination Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of tracking (causally estimating) a time sequence of\nsparse spatial signals with changing sparsity patterns, as well as other\nunknown states, from a sequence of nonlinear observations corrupted by\n(possibly) non-Gaussian noise. In many applications, particularly those in\nvisual tracking, the unknown state can be split into a small dimensional part,\ne.g. global motion, and a spatial signal, e.g. illumination or shape\ndeformation. The spatial signal is often well modeled as being sparse in some\ndomain. For a long sequence, its sparsity pattern can change over time,\nalthough the changes are usually slow. To address the above problem, we propose\na novel solution approach called Particle Filtered Modified-CS (PaFiMoCS). The\nkey idea of PaFiMoCS is to importance sample for the small dimensional state\nvector, while replacing importance sampling by slow sparsity pattern change\nconstrained posterior mode tracking for recovering the sparse spatial signal.\nWe show that the problem of tracking moving objects across spatially varying\nillumination change is an example of the above problem and explain how to\ndesign PaFiMoCS for it. Experiments on both simulated data as well as on real\nvideos with significant illumination changes demonstrate the superiority of the\nproposed algorithm as compared with existing particle filter based tracking\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 01:18:21 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Sarkar", "R.", ""], ["Das", "S.", ""], ["Vaswani", "N.", ""]]}, {"id": "1301.1551", "submitter": "Philipp Ewerling", "authors": "Philipp Ewerling", "title": "A novel processing pipeline for optical multi-touch surfaces", "comments": "MSc Thesis, 80 pages, a condensed version of this thesis has been\n  published as \"Finger and hand detection for multi-touch interfaces based on\n  maximally stable extremal regions\" in Proceedings of the 2012 ACM\n  international conference on Interactive tabletops and surfaces.\n  (http://doi.acm.org/10.1145/2396636.2396663)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis a new approach for touch detection on optical multi-touch\ndevices is proposed that exploits the fact that the camera images reveal not\nonly the actual touch points but also objects above the screen such as the hand\nor arm of a user. The touch processing relies on the Maximally Stable Extremal\nRegions algorithm for finding the users' fingertips in the camera image. The\nhierarchical structure of the generated extremal regions serves as a starting\npoint for agglomerative clustering of the fingertips into hands. Furthermore, a\nheuristic is suggested that supports the identification of individual fingers\nas well as the distinction between left hands and right hands if all five\nfingers of a hand are in contact with the touch surface.\n  The evaluation confirmed that the system is robust against detection errors\nresulting from non-uniform illumination and reliably assigns touch points to\nindividual hands based on the implicitly tracked context information. The\nefficient multi-threaded implementation handles two-handed input from multiple\nusers in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 14:48:23 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Ewerling", "Philipp", ""]]}, {"id": "1301.1576", "submitter": "Lukas F. Lang", "authors": "Clemens Kirisits, Lukas F. Lang, Otmar Scherzer", "title": "Optical Flow on Evolving Surfaces with an Application to the Analysis of\n  4D Microscopy Data", "comments": "The final publication is available at link.springer.com", "journal-ref": null, "doi": "10.1007/978-3-642-38267-3_21", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the concept of optical flow to a dynamic non-Euclidean setting.\nOptical flow is traditionally computed from a sequence of flat images. It is\nthe purpose of this paper to introduce variational motion estimation for images\nthat are defined on an evolving surface. Volumetric microscopy images depicting\na live zebrafish embryo serve as both biological motivation and test data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 16:06:02 GMT"}, {"version": "v2", "created": "Tue, 21 May 2013 12:03:37 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Kirisits", "Clemens", ""], ["Lang", "Lukas F.", ""], ["Scherzer", "Otmar", ""]]}, {"id": "1301.1671", "submitter": "Camille Couprie", "authors": "Camille Couprie, Cl\\'ement Farabet, Yann LeCun", "title": "Causal graph-based video segmentation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous approaches in image processing and computer vision are making use of\nsuper-pixels as a pre-processing step. Among the different methods producing\nsuch over-segmentation of an image, the graph-based approach of Felzenszwalb\nand Huttenlocher is broadly employed. One of its interesting properties is that\nthe regions are computed in a greedy manner in quasi-linear time. The algorithm\nmay be trivially extended to video segmentation by considering a video as a 3D\nvolume, however, this can not be the case for causal segmentation, when\nsubsequent frames are unknown. We propose an efficient video segmentation\napproach that computes temporally consistent pixels in a causal manner, filling\nthe need for causal and real time applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 20:56:17 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Couprie", "Camille", ""], ["Farabet", "Cl\u00e9ment", ""], ["LeCun", "Yann", ""]]}, {"id": "1301.1897", "submitter": "Nargess Memarsadeghi", "authors": "Nargess Memarsadeghi, Jacqueline Le Moigne, Peter N. Blake, Peter A.\n  Morey, Wayne B. Landsman, Victor J. Chambers, Samuel H. Moseley", "title": "Image Registration for Stability Testing of MEMS", "comments": "Proceedings of 2011 IS&T/SPIE Electronic Imaging, Computational\n  Imaging IX Conference, San Francisco, CA, January 2011, Vol. 7873, 78730G-1:7", "journal-ref": null, "doi": "10.1117/12.872076", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Image registration, or alignment of two or more images covering the same\nscenes or objects, is of great interest in many disciplines such as remote\nsensing, medical imaging, astronomy, and computer vision. In this paper, we\nintroduce a new application of image registration algorithms. We demonstrate\nhow through a wavelet based image registration algorithm, engineers can\nevaluate stability of Micro-Electro-Mechanical Systems (MEMS). In particular,\nwe applied image registration algorithms to assess alignment stability of the\nMicroShutters Subsystem (MSS) of the Near Infrared Spectrograph (NIRSpec)\ninstrument of the James Webb Space Telescope (JWST). This work introduces a new\nmethodology for evaluating stability of MEMS devices to engineers as well as a\nnew application of image registration algorithms to computer scientists.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 15:49:26 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Memarsadeghi", "Nargess", ""], ["Moigne", "Jacqueline Le", ""], ["Blake", "Peter N.", ""], ["Morey", "Peter A.", ""], ["Landsman", "Wayne B.", ""], ["Chambers", "Victor J.", ""], ["Moseley", "Samuel H.", ""]]}, {"id": "1301.1907", "submitter": "Nargess Memarsadeghi", "authors": "Nargess Memarsadeghi, Lucy A. McFadden, David Skillman, Brian McLean,\n  Max Mutchler, Uri Carsenty, Eric E. Palmer, and the Dawn Mission's Satellite\n  Working Group", "title": "Moon Search Algorithms for NASA's Dawn Mission to Asteroid Vesta", "comments": "Proceedings of the 2012 IS&T/SPIE Electronic Imaging, Computational\n  Imaging X Conference, San Francisco, CA, January 2012, Vol. 8296, pages\n  82960H-1:12", "journal-ref": null, "doi": "10.1117/12.915564", "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A moon or natural satellite is a celestial body that orbits a planetary body\nsuch as a planet, dwarf planet, or an asteroid. Scientists seek understanding\nthe origin and evolution of our solar system by studying moons of these bodies.\nAdditionally, searches for satellites of planetary bodies can be important to\nprotect the safety of a spacecraft as it approaches or orbits a planetary body.\nIf a satellite of a celestial body is found, the mass of that body can also be\ncalculated once its orbit is determined. Ensuring the Dawn spacecraft's safety\non its mission to the asteroid (4) Vesta primarily motivated the work of Dawn's\nSatellite Working Group (SWG) in summer of 2011. Dawn mission scientists and\nengineers utilized various computational tools and techniques for Vesta's\nsatellite search. The objectives of this paper are to 1) introduce the natural\nsatellite search problem, 2) present the computational challenges, approaches,\nand tools used when addressing this problem, and 3) describe applications of\nvarious image processing and computational algorithms for performing satellite\nsearches to the electronic imaging and computer science community. Furthermore,\nwe hope that this communication would enable Dawn mission scientists to improve\ntheir satellite search algorithms and tools and be better prepared for\nperforming the same investigation in 2015, when the spacecraft is scheduled to\napproach and orbit the dwarf planet (1) Ceres.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 16:15:45 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Memarsadeghi", "Nargess", ""], ["McFadden", "Lucy A.", ""], ["Skillman", "David", ""], ["McLean", "Brian", ""], ["Mutchler", "Max", ""], ["Carsenty", "Uri", ""], ["Palmer", "Eric E.", ""], ["Group", "the Dawn Mission's Satellite Working", ""]]}, {"id": "1301.2032", "submitter": "Chunhua Shen", "authors": "Chunhua Shen and Peng Wang and Sakrapee Paisitkriangkrai and Anton van\n  den Hengel", "title": "Training Effective Node Classifiers for Cascade Classification", "comments": "Appearing in Int'l J. Computer Vision. This is a substantially\n  revised version of http://arxiv.org/abs/1008.3742", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascade classifiers are widely used in real-time object detection. Different\nfrom conventional classifiers that are designed for a low overall\nclassification error rate, a classifier in each node of the cascade is required\nto achieve an extremely high detection rate and moderate false positive rate.\nAlthough there are a few reported methods addressing this requirement in the\ncontext of object detection, there is no principled feature selection method\nthat explicitly takes into account this asymmetric node learning objective. We\nprovide such an algorithm here. We show that a special case of the biased\nminimax probability machine has the same formulation as the linear asymmetric\nclassifier (LAC) of Wu et al (2005). We then design a new boosting algorithm\nthat directly optimizes the cost function of LAC. The resulting\ntotally-corrective boosting algorithm is implemented by the column generation\ntechnique in convex optimization. Experimental results on object detection\nverify the effectiveness of the proposed boosting algorithm as a node\nclassifier in cascade object detection, and show performance better than that\nof the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 05:26:18 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Shen", "Chunhua", ""], ["Wang", "Peng", ""], ["Paisitkriangkrai", "Sakrapee", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1301.2252", "submitter": "Kannan Achan", "authors": "Kannan Achan, Brendan J. Frey, Ralf Koetter", "title": "A Factorized Variational Technique for Phase Unwrapping in Markov Random\n  Fields", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-1-6", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some types of medical and topographic imaging device produce images in which\nthe pixel values are \"phase-wrapped\", i.e. measured modulus a known scalar.\nPhase unwrapping can be viewed as the problem of inferring the number of shifts\nbetween each and every pair of neighboring pixels, subject to an a priori\npreference for smooth surfaces, and subject to a zero curl constraint, which\nrequires that the shifts must sum to 0 around every loop. We formulate phase\nunwrapping as a mean field inference problem in a Markov network, where the\nprior favors the zero curl constraint. We compare our mean field technique with\nthe least squares method on a synthetic 100x100 image, and give results on a\n512x512 synthetic aperture radar image from Sandia National Laboratories.<Long\nText>\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:19 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Achan", "Kannan", ""], ["Frey", "Brendan J.", ""], ["Koetter", "Ralf", ""]]}, {"id": "1301.2298", "submitter": "Dirk Ormoneit", "authors": "Dirk Ormoneit, Christiane Lemieux, David J. Fleet", "title": "Lattice Particle Filters", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-395-402", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach to approximate inference in state-space models isto apply\na particle filter, e.g., the Condensation Algorithm.However, the performance of\nparticle filters often varies significantlydue to their stochastic nature.We\npresent a class of algorithms, called lattice particle filters, thatcircumvent\nthis difficulty by placing the particles deterministicallyaccording to a\nQuasi-Monte Carlo integration rule.We describe a practical realization of this\nidea, discuss itstheoretical properties, and its efficiency.Experimental\nresults with a synthetic 2D tracking problem show that thelattice particle\nfilter is equivalent to a conventional particle filterthat has between 10 and\n60% more particles, depending ontheir \"sparsity\" in the state-space.We also\npresent results on inferring 3D human motion frommoving light displays.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:25:38 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Ormoneit", "Dirk", ""], ["Lemieux", "Christiane", ""], ["Fleet", "David J.", ""]]}, {"id": "1301.2351", "submitter": "Teruyoshi Washizawa", "authors": "Teruyoshi Washizawa", "title": "Application of Hopfield Network to Saccades", "comments": "6 pages, 6 figures", "journal-ref": "IEEE Transactions on NEURAL NETWORKS, vol.4, no.6, pp-995-997,\n  NOVEMBER 1993", "doi": "10.1109/72.286896", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human eye movement mechanisms (saccades) are very useful for scene analysis,\nincluding object representation and pattern recognition. In this letter, a\nHopfield neural network to emulate saccades is proposed. The network uses an\nenergy function that includes location and identification tasks. Computer\nsimulation shows that the network performs those tasks cooperatively. The\nresult suggests that the network is applicable to shift-invariant pattern\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 22:57:01 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Washizawa", "Teruyoshi", ""]]}, {"id": "1301.2542", "submitter": "Amera Eletrebe", "authors": "Mohamed Eisa, Amira Eletrebi, Ebrahim Elhenawy", "title": "Enhancing the retrieval performance by combing the texture and edge\n  features", "comments": "7 pages,8 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, anew algorithm which is based on geometrical moments and local\nbinary patterns (LBP) for content based image retrieval (CBIR) is proposed. In\ngeometrical moments, each vector is compared with the all other vectors for\nedge map generation. The same concept is utilized at LBP calculation which is\ngenerating nine LBP patterns from a given 3x3 pattern. Finally, nine LBP\nhistograms are calculated which are used as a feature vector for image\nretrieval. Moments are important features used in recognition of different\ntypes of images. Two experiments have been carried out for proving the worth of\nour algorithm. The results after being investigated shows a significant\nimprovement in terms of their evaluation measures as compared to LBP and other\nexisting transform domain techniques.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 15:13:47 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Eisa", "Mohamed", ""], ["Eletrebi", "Amira", ""], ["Elhenawy", "Ebrahim", ""]]}, {"id": "1301.2628", "submitter": "Xu-Cheng Yin", "authors": "Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, Hong-Wei Hao", "title": "Robust Text Detection in Natural Scene Images", "comments": "A Draft Version (Submitted to IEEE TPAMI)", "journal-ref": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36,\n  no. 5, pp. 970-983, 2014", "doi": "10.1109/TPAMI.2013.182", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection in natural scene images is an important prerequisite for many\ncontent-based image analysis tasks. In this paper, we propose an accurate and\nrobust method for detecting texts in natural scene images. A fast and effective\npruning algorithm is designed to extract Maximally Stable Extremal Regions\n(MSERs) as character candidates using the strategy of minimizing regularized\nvariations. Character candidates are grouped into text candidates by the\ningle-link clustering algorithm, where distance weights and threshold of the\nclustering algorithm are learned automatically by a novel self-training\ndistance metric learning algorithm. The posterior probabilities of text\ncandidates corresponding to non-text are estimated with an character\nclassifier; text candidates with high probabilities are then eliminated and\nfinally texts are identified with a text classifier. The proposed system is\nevaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measure\nis over 76% and is significantly better than the state-of-the-art performance\nof 71%. Experimental results on a publicly available multilingual dataset also\nshow that our proposed method can outperform the other competitive method with\nthe f measure increase of over 9 percent. Finally, we have setup an online demo\nof our proposed scene text detection system at\nhttp://kems.ustb.edu.cn/learning/yin/dtext.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 23:08:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2013 19:57:46 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2013 16:27:49 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Yin", "Xu-Cheng", ""], ["Yin", "Xuwang", ""], ["Huang", "Kaizhu", ""], ["Hao", "Hong-Wei", ""]]}, {"id": "1301.2715", "submitter": "Joseph Antonides", "authors": "Joseph Antonides and Toshiro Kubota", "title": "Binocular disparity as an explanation for the moon illusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.pop-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present another explanation for the moon illusion, the phenomenon in which\nthe moon looks larger near the horizon than near the zenith. In our model of\nthe moon illusion, the sky is considered a spatially-contiguous and\ngeometrically-smooth surface. When an object such as the moon breaks the\ncontiguity of the surface, instead of perceiving the object as appearing\nthrough a hole in the surface, humans perceive an occlusion of the surface.\nBinocular vision dictates that the moon is distant, but this perception model\ncontradicts our binocular vision, dictating that the moon is closer than the\nsky. To resolve the contradiction, the brain distorts the projections of the\nmoon to increase the binocular disparity, which results in an increase in the\nperceived size of the moon. The degree of distortion depends upon the apparent\ndistance to the sky, which is influenced by the surrounding objects and the\ncondition of the sky. As the apparent distance to the sky decreases, the\nillusion becomes stronger. At the horizon, apparent distance to the sky is\nminimal, whereas at the zenith, few distance cues are present, causing\ndifficulty with distance estimation and weakening the illusion.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 20:12:09 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 04:56:31 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Antonides", "Joseph", ""], ["Kubota", "Toshiro", ""]]}, {"id": "1301.2820", "submitter": "Eugenio Culurciello Eugenio Culurciello", "authors": "Eugenio Culurciello, Jordan Bates, Aysegul Dundar, Jose Carrasco,\n  Clement Farabet", "title": "Clustering Learning for Robotic Vision", "comments": "Code for this paper is available here:\n  https://github.com/culurciello/CL_paper1_code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the clustering learning technique applied to multi-layer\nfeedforward deep neural networks. We show that this unsupervised learning\ntechnique can compute network filters with only a few minutes and a much\nreduced set of parameters. The goal of this paper is to promote the technique\nfor general-purpose robotic vision systems. We report its use in static image\ndatasets and object tracking datasets. We show that networks trained with\nclustering learning can outperform large networks trained for many hours on\ncomplex datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2013 20:49:30 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2013 14:53:21 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 22:48:38 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Culurciello", "Eugenio", ""], ["Bates", "Jordan", ""], ["Dundar", "Aysegul", ""], ["Carrasco", "Jose", ""], ["Farabet", "Clement", ""]]}, {"id": "1301.2840", "submitter": "Christian Osendorfer", "authors": "Christian Osendorfer and Justin Bayer and Sebastian Urban and Patrick\n  van der Smagt", "title": "Unsupervised Feature Learning for low-level Local Image Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature learning has shown impressive results for a wide range\nof input modalities, in particular for object classification tasks in computer\nvision. Using a large amount of unlabeled data, unsupervised feature learning\nmethods are utilized to construct high-level representations that are\ndiscriminative enough for subsequently trained supervised classification\nalgorithms. However, it has never been \\emph{quantitatively} investigated yet\nhow well unsupervised learning methods can find \\emph{low-level\nrepresentations} for image patches without any additional supervision. In this\npaper we examine the performance of pure unsupervised methods on a low-level\ncorrespondence task, a problem that is central to many Computer Vision\napplications. We find that a special type of Restricted Boltzmann Machines\n(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple\nbinarization scheme produces compact representations that perform better than\nseveral state-of-the-art descriptors.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 01:34:17 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 13:42:10 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2013 18:54:11 GMT"}, {"version": "v4", "created": "Thu, 25 Apr 2013 14:26:04 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Osendorfer", "Christian", ""], ["Bayer", "Justin", ""], ["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1301.2884", "submitter": "Anh Cat Le Ngo", "authors": "Anh Cat Le Ngo, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng, Guoping\n  Qiu", "title": "Wavelet-based Scale Saliency", "comments": "Partly published in ACIIDS 2013 - Kuala Lumpur Malaysia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both pixel-based scale saliency (PSS) and basis project methods focus on\nmultiscale analysis of data content and structure. Their theoretical relations\nand practical combination are previously discussed. However, no models have\never been proposed for calculating scale saliency on basis-projected\ndescriptors since then. This paper extend those ideas into mathematical models\nand implement them in the wavelet-based scale saliency (WSS). While PSS uses\npixel-value descriptors, WSS treats wavelet sub-bands as basis descriptors. The\npaper discusses different wavelet descriptors: discrete wavelet transform\n(DWT), wavelet packet transform (DWPT), quaternion wavelet transform (QWT) and\nbest basis quaternion wavelet packet transform (QWPTBB). WSS saliency maps of\ndifferent descriptors are generated and compared against other saliency methods\nby both quantitative and quanlitative methods. Quantitative results, ROC\ncurves, AUC values and NSS values are collected from simulations on Bruce and\nKootstra image databases with human eye-tracking data as ground-truth.\nFurthermore, qualitative visual results of saliency maps are analyzed and\ncompared against each other as well as eye-tracking data inclusive in the\ndatabases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 08:36:00 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Ngo", "Anh Cat Le", ""], ["Ang", "Kenneth Li-Minn", ""], ["Seng", "Jasmine Kah-Phooi", ""], ["Qiu", "Guoping", ""]]}, {"id": "1301.3193", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Learning Graphical Model Parameters with Approximate Marginal Inference", "comments": "To Appear, IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2013.31", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood based-learning of graphical models faces challenges of\ncomputational-complexity and robustness to model mis-specification. This paper\nstudies methods that fit parameters directly to maximize a measure of the\naccuracy of predicted marginals, taking into account both model and inference\napproximations at training time. Experiments on imaging problems suggest\nmarginalization-based learning performs better than likelihood-based\napproximations on difficult problems where the model being fit is approximate\nin nature.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 01:07:14 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1301.3323", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Takaki Makino and Kazuyuki Aihara", "title": "Auto-pooling: Learning to Improve Invariance of Image Features from\n  Image Sequences", "comments": "9 pages, 10 figures. Submission for ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning invariant representations from images is one of the hardest\nchallenges facing computer vision. Spatial pooling is widely used to create\ninvariance to spatial shifting, but it is restricted to convolutional models.\nIn this paper, we propose a novel pooling method that can learn soft clustering\nof features from image sequences. It is trained to improve the temporal\ncoherence of features, while keeping the information loss at minimum. Our\nmethod does not use spatial information, so it can be used with\nnon-convolutional models too. Experiments on images extracted from natural\nvideos showed that our method can cluster similar features together. When\ntrained by convolutional features, auto-pooling outperformed traditional\nspatial pooling on an image classification task, even though it does not use\nthe spatial topology of features.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 12:47:39 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 06:05:10 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2013 16:39:30 GMT"}, {"version": "v4", "created": "Mon, 18 Mar 2013 07:19:31 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Makino", "Takaki", ""], ["Aihara", "Kazuyuki", ""]]}, {"id": "1301.3342", "submitter": "Laurens van der Maaten", "authors": "Laurens van der Maaten", "title": "Barnes-Hut-SNE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an O(N log N)-implementation of t-SNE -- an embedding\ntechnique that is commonly used for the visualization of high-dimensional data\nin scatter plots and that normally runs in O(N^2). The new implementation uses\nvantage-point trees to compute sparse pairwise similarities between the input\ndata objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm\nused by astronomers to perform N-body simulations - to approximate the forces\nbetween the corresponding points in the embedding. Our experiments show that\nthe new algorithm, called Barnes-Hut-SNE, leads to substantial computational\nadvantages over standard t-SNE, and that it makes it possible to learn\nembeddings of data sets with millions of objects.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 13:44:18 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2013 11:00:32 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["van der Maaten", "Laurens", ""]]}, {"id": "1301.3385", "submitter": "Steven Young", "authors": "Steven R. Young and Itamar Arel", "title": "Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in\n  DeSTIN", "comments": "3 pages, 2 figures, Submitted to ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a basic enhancement to the DeSTIN deep learning\narchitecture by replacing the explicitly calculated transition tables that are\nused to capture temporal features with a simpler, more scalable mechanism. This\nmechanism uses feedback of state information to cluster over a space comprised\nof both the spatial input and the current state. The resulting architecture\nachieves state-of-the-art results on the MNIST classification benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 15:34:07 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 14:56:44 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Young", "Steven R.", ""], ["Arel", "Itamar", ""]]}, {"id": "1301.3457", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet, Italo Lima, Davi Geiger, Kris Gunsalus", "title": "A Geometric Descriptor for Cell-Division Detection", "comments": "This paper has been withdrawn by the author since the review process\n  for the conference to which it was applied ended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for cell-division detection based on a geometric-driven\ndescriptor that can be represented as a 5-layers processing network, based\nmainly on wavelet filtering and a test for mirror symmetry between pairs of\npixels. After the centroids of the descriptors are computed for a sequence of\nframes, the two-steps piecewise constant function that best fits the sequence\nof centroids determines the frame where the division occurs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 19:18:52 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 18:32:07 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Cicconet", "Marcelo", ""], ["Lima", "Italo", ""], ["Geiger", "Davi", ""], ["Gunsalus", "Kris", ""]]}, {"id": "1301.3461", "submitter": "Cheng Zhang", "authors": "Cheng Zhang and Carl Henrik Ek and Andreas Damianou and Hedvig\n  Kjellstrom", "title": "Factorized Topic Models", "comments": "ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a modification to a latent topic model, which makes\nthe model exploit supervision to produce a factorized representation of the\nobserved data. The structured parameterization separately encodes variance that\nis shared between classes from variance that is private to each class by the\nintroduction of a new prior over the topic space. The approach allows for a\nmore eff{}icient inference and provides an intuitive interpretation of the data\nin terms of an informative signal together with structured noise. The\nfactorized representation is shown to enhance inference performance for image,\ntext, and video classification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 19:32:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 11:05:05 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2013 09:50:28 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2013 14:16:39 GMT"}, {"version": "v5", "created": "Fri, 15 Mar 2013 17:14:58 GMT"}, {"version": "v6", "created": "Wed, 10 Apr 2013 20:15:04 GMT"}, {"version": "v7", "created": "Tue, 23 Apr 2013 08:13:55 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Zhang", "Cheng", ""], ["Ek", "Carl Henrik", ""], ["Damianou", "Andreas", ""], ["Kjellstrom", "Hedvig", ""]]}, {"id": "1301.3468", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising based on a probabilistic model of local image patches has\nbeen employed by various researchers, and recently a deep (denoising)\nautoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as\na good model for this. In this paper, we propose that another popular family of\nmodels in the field of deep learning, called Boltzmann machines, can perform\nimage denoising as well as, or in certain cases of high level of noise, better\nthan denoising autoencoders. We empirically evaluate the two models on three\ndifferent sets of images with different types and levels of noise. Throughout\nthe experiments we also examine the effect of the depth of the models. The\nexperiments confirmed our claim and revealed that the performance can be\nimproved by adding more hidden layers, especially when the level of noise is\nhigh.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 19:45:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 15:37:08 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 13:48:22 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2013 16:35:56 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2013 11:16:34 GMT"}, {"version": "v6", "created": "Mon, 4 Mar 2013 10:41:34 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1301.3476", "submitter": "Tommi Vatanen", "authors": "Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun", "title": "Pushing Stochastic Gradient towards Second-Order Methods --\n  Backpropagation Learning with Transformations in Nonlinearities", "comments": "10 pages, 5 figures, ICLR2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we proposed to transform the outputs of each hidden neuron in a\nmulti-layer perceptron network to have zero output and zero slope on average,\nand use separate shortcut connections to model the linear dependencies instead.\nWe continue the work by firstly introducing a third transformation to normalize\nthe scale of the outputs of each hidden neuron, and secondly by analyzing the\nconnections to second order optimization methods. We show that the\ntransformations make a simple stochastic gradient behave closer to second-order\noptimization methods and thus speed up learning. This is shown both in theory\nand with experiments. The experiments on the third transformation show that\nwhile it further increases the speed of learning, it can also hurt performance\nby converging to a worse local optimum, where both the inputs and outputs of\nmany hidden neurons are close to zero.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 20:21:54 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 09:23:23 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2013 18:00:00 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Vatanen", "Tommi", ""], ["Raiko", "Tapani", ""], ["Valpola", "Harri", ""], ["LeCun", "Yann", ""]]}, {"id": "1301.3516", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Learnable Pooling Regions for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologically inspired, from the early HMAX model to Spatial Pyramid Matching,\npooling has played an important role in visual recognition pipelines. Spatial\npooling, by grouping of local codes, equips these methods with a certain degree\nof robustness to translation and deformation yet preserving important spatial\ninformation. Despite the predominance of this approach in current recognition\nsystems, we have seen little progress to fully adapt the pooling strategy to\nthe task at hand. This paper proposes a model for learning task dependent\npooling scheme -- including previously proposed hand-crafted pooling schemes as\na particular instantiation. In our work, we investigate the role of different\nregularization terms showing that the smooth regularization term is crucial to\nachieve strong performance using the presented architecture. Finally, we\npropose an efficient and parallel method to train the model. Our experiments\nshow improved performance over hand-crafted pooling schemes on the CIFAR-10 and\nCIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on\nthe latter.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 22:15:06 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2013 13:51:04 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 18:12:46 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1301.3530", "submitter": "Charles Cadieu", "authors": "Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj,\n  James J. DiCarlo", "title": "The Neural Representation Benchmark and its Evaluation on Brain and\n  Machine", "comments": "The v1 version contained incorrectly computed kernel analysis curves\n  and KA-AUC values for V4, IT, and the HT-L3 models. They have been corrected\n  in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the development of effective learning representations\nis their evaluation and comparison to representations we know to be effective.\nIn natural sensory domains, the community has viewed the brain as a source of\ninspiration and as an implicit benchmark for success. However, it has not been\npossible to directly test representational learning algorithms directly against\nthe representations contained in neural systems. Here, we propose a new\nbenchmark for visual representations on which we have directly tested the\nneural representation in multiple visual cortical areas in macaque (utilizing\ndata from [Majaj et al., 2012]), and on which any computer vision algorithm\nthat produces a feature space can be tested. The benchmark measures the\neffectiveness of the neural or machine representation by computing the\nclassification loss on the ordered eigendecomposition of a kernel matrix\n[Montavon et al., 2011]. In our analysis we find that the neural representation\nin visual area IT is superior to visual area V4. In our analysis of\nrepresentational learning algorithms, we find that three-layer models approach\nthe representational performance of V4 and the algorithm in [Le et al., 2012]\nsurpasses the performance of V4. Impressively, we find that a recent supervised\nalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that of\nIT for an intermediate level of image variation difficulty, and surpasses IT at\na higher difficulty level. We believe this result represents a major milestone:\nit is the first learning algorithm we have found that exceeds our current\nestimate of IT representation performance. We hope that this benchmark will\nassist the community in matching the representational performance of visual\ncortex and will serve as an initial rallying point for further correspondence\nbetween representations derived in brains and machines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 23:42:21 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2013 20:39:46 GMT"}], "update_date": "2013-01-28", "authors_parsed": [["Cadieu", "Charles F.", ""], ["Hong", "Ha", ""], ["Yamins", "Dan", ""], ["Pinto", "Nicolas", ""], ["Majaj", "Najib J.", ""], ["DiCarlo", "James J.", ""]]}, {"id": "1301.3541", "submitter": "Rakesh Chalasani", "authors": "Rakesh Chalasani and Jose C. Principe", "title": "Deep Predictive Coding Networks", "comments": "13 Pages, 7 figures, submission for ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of data representation in deep learning methods is directly\nrelated to the prior model imposed on the representations; however, generally\nused fixed priors are not capable of adjusting to the context in the data. To\naddress this issue, we propose deep predictive coding networks, a hierarchical\ngenerative model that empirically alters priors on the latent representations\nin a dynamic and context-sensitive manner. This model captures the temporal\ndependencies in time-varying signals and uses top-down information to modulate\nthe representation in lower layers. The centerpiece of our model is a novel\nprocedure to infer sparse states of a dynamic model which is used for feature\nextraction. We also extend this feature extraction block to introduce a pooling\nfunction that captures locally invariant representations. When applied on a\nnatural video data, we show that our method is able to learn high-level visual\nfeatures. We also demonstrate the role of the top-down connections by showing\nthe robustness of the proposed model to structured noise.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 01:27:15 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 01:29:17 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 19:25:30 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Chalasani", "Rakesh", ""], ["Principe", "Jose C.", ""]]}, {"id": "1301.3551", "submitter": "Luis  Sanchez Giraldo", "authors": "Luis G. Sanchez Giraldo and Jose C. Principe", "title": "Information Theoretic Learning with Infinitely Divisible Kernels", "comments": "Modified submission for International Conference on Learning\n  Representations 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a framework for information theoretic learning\nbased on infinitely divisible matrices. We formulate an entropy-like functional\non positive definite matrices based on Renyi's axiomatic definition of entropy\nand examine some key properties of this functional that lead to the concept of\ninfinite divisibility. The proposed formulation avoids the plug in estimation\nof density and brings along the representation power of reproducing kernel\nHilbert spaces. As an application example, we derive a supervised metric\nlearning algorithm using a matrix based analogue to conditional entropy\nachieving results comparable with the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 01:49:52 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 06:40:01 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2013 14:53:42 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2013 00:12:21 GMT"}, {"version": "v5", "created": "Wed, 1 May 2013 06:18:31 GMT"}, {"version": "v6", "created": "Tue, 4 Jun 2013 04:42:39 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Giraldo", "Luis G. Sanchez", ""], ["Principe", "Jose C.", ""]]}, {"id": "1301.3560", "submitter": "Roozbeh Mottaghi", "authors": "Alan L. Yuille and Roozbeh Mottaghi", "title": "Complexity of Representation and Inference in Compositional Models with\n  Part Sharing", "comments": "ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes serial and parallel compositional models of multiple\nobjects with part sharing. Objects are built by part-subpart compositions and\nexpressed in terms of a hierarchical dictionary of object parts. These parts\nare represented on lattices of decreasing sizes which yield an executive\nsummary description. We describe inference and learning algorithms for these\nmodels. We analyze the complexity of this model in terms of computation time\n(for serial computers) and numbers of nodes (e.g., \"neurons\") for parallel\ncomputers. In particular, we compute the complexity gains by part sharing and\nits dependence on how the dictionary scales with the level of the hierarchy. We\nexplore three regimes of scaling behavior where the dictionary size (i)\nincreases exponentially with the level, (ii) is determined by an unsupervised\ncompositional learning algorithm applied to real data, (iii) decreases\nexponentially with scale. This analysis shows that in some regimes the use of\nshared parts enables algorithms which can perform inference in time linear in\nthe number of levels for an exponential number of objects. In other regimes\npart sharing has little advantage for serial computers but can give linear\nprocessing on parallel computers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 02:29:15 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Yuille", "Alan L.", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "1301.3572", "submitter": "Camille Couprie", "authors": "Camille Couprie, Cl\\'ement Farabet, Laurent Najman and Yann LeCun", "title": "Indoor Semantic Segmentation using depth information", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses multi-class segmentation of indoor scenes with RGB-D\ninputs. While this area of research has gained much attention recently, most\nworks still rely on hand-crafted features. In contrast, we apply a multiscale\nconvolutional network to learn features directly from the images and the depth\ninformation. We obtain state-of-the-art on the NYU-v2 depth dataset with an\naccuracy of 64.5%. We illustrate the labeling of indoor scenes in videos\nsequences that could be processed in real-time using appropriate hardware such\nas an FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 03:31:30 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 18:18:17 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Couprie", "Camille", ""], ["Farabet", "Cl\u00e9ment", ""], ["Najman", "Laurent", ""], ["LeCun", "Yann", ""]]}, {"id": "1301.3575", "submitter": "Boyi Xie", "authors": "Boyi Xie, Shuheng Zheng", "title": "Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale agglomerative clustering is hindered by computational burdens. We\npropose a novel scheme where exact inter-instance distance calculation is\nreplaced by the Hamming distance between Kernelized Locality-Sensitive Hashing\n(KLSH) hashed values. This results in a method that drastically decreases\ncomputation time. Additionally, we take advantage of certain labeled data\npoints via distance metric learning to achieve a competitive precision and\nrecall comparing to K-Means but in much less computation time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 03:52:09 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Xie", "Boyi", ""], ["Zheng", "Shuheng", ""]]}, {"id": "1301.3583", "submitter": "Yann Dauphin", "authors": "Yann N. Dauphin, Yoshua Bengio", "title": "Big Neural Networks Waste Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article exposes the failure of some big neural networks to leverage\nadded capacity to reduce underfitting. Past research suggest diminishing\nreturns when increasing the size of neural networks. Our experiments on\nImageNet LSVRC-2010 show that this may be due to the fact there are highly\ndiminishing returns for capacity in terms of training error, leading to\nunderfitting. This suggests that the optimization method - first order gradient\ndescent - fails at this regime. Directly attacking this problem, either through\nthe optimization method or the choices of parametrization, may allow to improve\nthe generalization error on large datasets, for which a large capacity is\nrequired.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 04:45:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 18:11:34 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2013 23:07:05 GMT"}, {"version": "v4", "created": "Thu, 14 Mar 2013 20:49:20 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Dauphin", "Yann N.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1301.3590", "submitter": "Arthur Szlam", "authors": "Arthur Szlam", "title": "Tree structured sparse coding on cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brief description of tree structured sparse coding on the binary cube.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 05:20:01 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Szlam", "Arthur", ""]]}, {"id": "1301.3592", "submitter": "Ian Lenz", "authors": "Ian Lenz and Honglak Lee and Ashutosh Saxena", "title": "Deep Learning for Detecting Robotic Grasps", "comments": "Current version was accepted to IJRR Special Issue on Robot Vision\n  2014 Workshop version accepted to ICLR 2013. Conference version accepted to\n  RSS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting robotic grasps in an RGB-D view of a\nscene containing objects. In this work, we apply a deep learning approach to\nsolve this problem, which avoids time-consuming hand-design of features. This\npresents two main challenges. First, we need to evaluate a huge number of\ncandidate grasps. In order to make detection fast, as well as robust, we\npresent a two-step cascaded structure with two deep networks, where the top\ndetections from the first are re-evaluated by the second. The first network has\nfewer features, is faster to run, and can effectively prune out unlikely\ncandidate grasps. The second, with more features, is slower but has to run only\non the top few detections. Second, we need to handle multimodal inputs well,\nfor which we present a method to apply structured regularization on the weights\nbased on multimodal group regularization. We demonstrate that our method\noutperforms the previous state-of-the-art methods in robotic grasp detection,\nand can be used to successfully execute grasps on two different robotic\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 05:33:56 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 00:10:20 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2013 00:40:34 GMT"}, {"version": "v4", "created": "Fri, 3 May 2013 06:54:42 GMT"}, {"version": "v5", "created": "Fri, 2 Aug 2013 18:15:54 GMT"}, {"version": "v6", "created": "Thu, 21 Aug 2014 18:17:37 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Lenz", "Ian", ""], ["Lee", "Honglak", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1301.3644", "submitter": "Kye-Hyeon Kim", "authors": "Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi", "title": "Regularized Discriminant Embedding for Visual Descriptor Learning", "comments": "3 pages + 1 additional page containing only cited references; The\n  full version of this manuscript is currently under review in an international\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can vary according to changes in viewpoint, resolution, noise, and\nillumination. In this paper, we aim to learn representations for an image,\nwhich are robust to wide changes in such environmental conditions, using\ntraining pairs of matching and non-matching local image patches that are\ncollected under various environmental conditions. We present a regularized\ndiscriminant analysis that emphasizes two challenging categories among the\ngiven training pairs: (1) matching, but far apart pairs and (2) non-matching,\nbut close pairs in the original feature space (e.g., SIFT feature space).\nCompared to existing work on metric learning and discriminant analysis, our\nmethod can better distinguish relevant images from irrelevant, but look-alike\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 10:12:37 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Kim", "Kye-Hyeon", ""], ["Cai", "Rui", ""], ["Zhang", "Lei", ""], ["Choi", "Seungjin", ""]]}, {"id": "1301.3666", "submitter": "Richard Socher", "authors": "Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani,\n  Christopher D. Manning, Andrew Y. Ng", "title": "Zero-Shot Learning Through Cross-Modal Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a model that can recognize objects in images even if no\ntraining data is available for the objects. The only necessary knowledge about\nthe unseen categories comes from unsupervised large text corpora. In our\nzero-shot framework distributional information in language can be seen as\nspanning a semantic basis for understanding what objects look like. Most\nprevious zero-shot learning models can only differentiate between unseen\nclasses. In contrast, our model can both obtain state of the art performance on\nclasses that have thousands of training images and obtain reasonable\nperformance on unseen classes. This is achieved by first using outlier\ndetection in the semantic space and then two separate recognition models.\nFurthermore, our model does not require any manually defined semantic features\nfor either words or images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 12:01:34 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 00:44:08 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Socher", "Richard", ""], ["Ganjoo", "Milind", ""], ["Sridhar", "Hamsa", ""], ["Bastani", "Osbert", ""], ["Manning", "Christopher D.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1301.3683", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Christoph Schn\\\"orr", "title": "Convex Variational Image Restoration with Histogram Priors", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel variational approach to image restoration (e.g.,\ndenoising, inpainting, labeling) that enables to complement established\nvariational approaches with a histogram-based prior enforcing closeness of the\nsolution to some given empirical measure. By minimizing a single objective\nfunction, the approach utilizes simultaneously two quite different sources of\ninformation for restoration: spatial context in terms of some smoothness prior\nand non-spatial statistics in terms of the novel prior utilizing the\nWasserstein distance between probability measures. We study the combination of\nthe functional lifting technique with two different relaxations of the\nhistogram prior and derive a jointly convex variational approach. Mathematical\nequivalence of both relaxations is established and cases where optimality holds\nare discussed. Additionally, we present an efficient algorithmic scheme for the\nnumerical treatment of the presented model. Experiments using the basic\ntotal-variation based denoising approach as a case study demonstrate our novel\nregularization approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 13:06:35 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 12:35:09 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Swoboda", "Paul", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1301.3755", "submitter": "Derek Rose", "authors": "Derek Rose, Itamar Arel", "title": "Gradient Driven Learning for Pooling in Visual Pipeline Feature\n  Extraction Models", "comments": "3 pages, 2 figures, submitted to ICLR2013 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-parameter selection remains a daunting task when building a pattern\nrecognition architecture which performs well, particularly in recently\nconstructed visual pipeline models for feature extraction. We re-formulate\npooling in an existing pipeline as a function of adjustable pooling map weight\nparameters and propose the use of supervised error signals from gradient\ndescent to tune the established maps within the model. This technique allows us\nto learn what would otherwise be a design choice within the model and\nspecialize the maps to aggregate areas of invariance for the task presented.\nPreliminary results show moderate potential gains in classification accuracy\nand highlight areas of importance within the intermediate feature\nrepresentation space.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 17:05:57 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Rose", "Derek", ""], ["Arel", "Itamar", ""]]}, {"id": "1301.3775", "submitter": "Jason Rolfe", "authors": "Jason Tyler Rolfe and Yann LeCun", "title": "Discriminative Recurrent Sparse Auto-Encoders", "comments": "Added clarifications suggested by reviewers. 15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discriminative recurrent sparse auto-encoder model, comprising\na recurrent encoder of rectified linear units, unrolled for a fixed number of\niterations, and connected to two linear decoders that reconstruct the input and\npredict its supervised classification. Training via\nbackpropagation-through-time initially minimizes an unsupervised sparse\nreconstruction error; the loss function is then augmented with a discriminative\nterm on the supervised classification. The depth implicit in the\ntemporally-unrolled form allows the system to exhibit all the power of deep\nnetworks, while substantially reducing the number of trainable parameters.\n  From an initially unstructured network the hidden units differentiate into\ncategorical-units, each of which represents an input prototype with a\nwell-defined class; and part-units representing deformations of these\nprototypes. The learned organization of the recurrent encoder is hierarchical:\npart-units are driven directly by the input, whereas the activity of\ncategorical-units builds up over time through interactions with the part-units.\nEven using a small number of hidden units per layer, discriminative recurrent\nsparse auto-encoders achieve excellent performance on MNIST.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 18:07:01 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 18:51:59 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 21:17:19 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2013 18:43:29 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Rolfe", "Jason Tyler", ""], ["LeCun", "Yann", ""]]}, {"id": "1301.3854", "submitter": "Brendan J. Frey", "authors": "Brendan J. Frey, Nebojsa Jojic", "title": "Learning Graphical Models of Images, Videos and Their Spatial\n  Transformations", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-184-191", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markov\nmodels are staples of static and dynamic data modeling and image and video\nmodeling in particular. We show how topographic transformations in the input,\nsuch as translation and shearing in images, can be accounted for in these\nmodels by including a discrete transformation variable. The resulting models\nperform clustering, dimensionality reduction and time-series analysis in a way\nthat is invariant to transformations in the input. Using the EM algorithm,\nthese transformation-invariant models can be fit to static data and time\nseries. We give results on filtering microscopy images, face and facial pose\nclustering, handwritten digit modeling and recognition, video clustering,\nobject tracking, and removal of distractions from video sequences.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:50:06 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Frey", "Brendan J.", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1301.3964", "submitter": "Anh Cat Le Ngo", "authors": "Anh Cat Le Ngo, Kenneth Ang Li-Minn, Guoping Qiu, Jasmine Seng\n  Kah-Phooi", "title": "Multiscale Discriminant Saliency for Visual Attention", "comments": "16 pages, ICCSA 2013 - BIOCA session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bottom-up saliency, an early stage of humans' visual attention, can be\nconsidered as a binary classification problem between center and surround\nclasses. Discriminant power of features for the classification is measured as\nmutual information between features and two classes distribution. The estimated\ndiscrepancy of two feature classes very much depends on considered scale\nlevels; then, multi-scale structure and discriminant power are integrated by\nemploying discrete wavelet features and Hidden markov tree (HMT). With wavelet\ncoefficients and Hidden Markov Tree parameters, quad-tree like label structures\nare constructed and utilized in maximum a posterior probability (MAP) of hidden\nclass variables at corresponding dyadic sub-squares. Then, saliency value for\neach dyadic square at each scale level is computed with discriminant power\nprinciple and the MAP. Finally, across multiple scales is integrated the final\nsaliency map by an information maximization rule. Both standard quantitative\ntools such as NSS, LCC, AUC and qualitative assessments are used for evaluating\nthe proposed multiscale discriminant saliency method (MDIS) against the\nwell-know information-based saliency method AIM on its Bruce Database wity\neye-tracking data. Simulation results are presented and analyzed to verify the\nvalidity of MDIS as well as point out its disadvantages for further research\ndirection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 02:12:48 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Ngo", "Anh Cat Le", ""], ["Li-Minn", "Kenneth Ang", ""], ["Qiu", "Guoping", ""], ["Kah-Phooi", "Jasmine Seng", ""]]}, {"id": "1301.4083", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre and Yoshua Bengio", "title": "Knowledge Matters: Importance of Prior Information for Optimization", "comments": "37 Pages, 5 figures, 5 tables JMLR Special Topics on Representation\n  Learning Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We explore the effect of introducing prior information into the intermediate\nlevel of neural networks for a learning task on which all the state-of-the-art\nmachine learning algorithms tested failed to learn. We motivate our work from\nthe hypothesis that humans learn such intermediate concepts from other\nindividuals via a form of supervision or guidance using a curriculum. The\nexperiments we have conducted provide positive evidence in favor of this\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\ndataset with 64x64 binary inputs images, each image with three sprites. The\nfinal task is to decide whether all the sprites are the same or one of them is\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\nwith different locations using scaling and rotation transformations. The first\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\nthe presence of sprites at each location, while the second part takes the\noutput of the first part as input and predicts the final task's target binary\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\nwas able to learn the task perfectly, whereas all other algorithms (include\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\ntrees and boosting) all perform no better than chance. We hypothesize that the\noptimization difficulty involved when the intermediate pre-training is not\nperformed is due to the {\\em composition} of two highly non-linear tasks. Our\nfindings are also consistent with hypotheses on cultural learning inspired by\nthe observations of optimization problems with deep learning, presumably\nbecause of effective local minima.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 13:06:52 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 05:43:57 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2013 17:11:19 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2013 20:13:08 GMT"}, {"version": "v5", "created": "Fri, 15 Mar 2013 05:41:47 GMT"}, {"version": "v6", "created": "Sat, 13 Jul 2013 16:38:36 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["G\u00fcl\u00e7ehre", "\u00c7a\u011flar", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1301.4157", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet", "title": "On the Product Rule for Classification Problems", "comments": "3 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss theoretical aspects of the product rule for classification\nproblems in supervised machine learning for the case of combining classifiers.\nWe show that (1) the product rule arises from the MAP classifier supposing\nequivalent priors and conditional independence given a class; (2) under some\nconditions, the product rule is equivalent to minimizing the sum of the squared\ndistances to the respective centers of the classes related with different\nfeatures, such distances being weighted by the spread of the classes; (3)\nobserving some hypothesis, the product rule is equivalent to concatenating the\nvectors of features.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 16:48:46 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Cicconet", "Marcelo", ""]]}, {"id": "1301.4377", "submitter": "Mohamed Ali Mahjoub", "authors": "Mohamed Ali Mahjoub, Nabil Ghanmy, Khlifia jayech, Ikram Miled", "title": "Multiple models of Bayesian networks applied to offline recognition of\n  Arabic handwritten city names", "comments": "arXiv admin note: substantial text overlap with arXiv:1204.1679", "journal-ref": null, "doi": null, "report-no": "ISSN 0974-0627", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of offline Arabic handwriting word\nrecognition. Off-line recognition of handwritten words is a difficult task due\nto the high variability and uncertainty of human writing. The majority of the\nrecent systems are constrained by the size of the lexicon to deal with and the\nnumber of writers. In this paper, we propose an approach for multi-writers\nArabic handwritten words recognition using multiple Bayesian networks. First,\nwe cut the image in several blocks. For each block, we compute a vector of\ndescriptors. Then, we use K-means to cluster the low-level features including\nZernik and Hu moments. Finally, we apply four variants of Bayesian networks\nclassifiers (Na\\\"ive Bayes, Tree Augmented Na\\\"ive Bayes (TAN), Forest\nAugmented Na\\\"ive Bayes (FAN) and DBN (dynamic bayesian network) to classify\nthe whole image of tunisian city name. The results demonstrate FAN and DBN\noutperform good recognition rates\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 13:26:55 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Mahjoub", "Mohamed Ali", ""], ["Ghanmy", "Nabil", ""], ["jayech", "Khlifia", ""], ["Miled", "Ikram", ""]]}, {"id": "1301.4558", "submitter": "Walid Mahdi", "authors": "Salah Werda, Walid Mahdi and Abdelmajid Ben Hamadou", "title": "Lip Localization and Viseme Classification for Visual Speech Recognition", "comments": "14 pages", "journal-ref": "International Journal of Computing and Information Sciences ISSN:\n  1708-0460 (print) - 1708-0479 (online) Volume 5, Number 1, December 2007", "doi": null, "report-no": "Vol.5, No.1, pp. 62-75, 2006", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for an automatic lip-reading system is ever increasing. Infact,\ntoday, extraction and reliable analysis of facial movements make up an\nimportant part in many multimedia systems such as videoconference, low\ncommunication systems, lip-reading systems. In addition, visual information is\nimperative among people with special needs. We can imagine, for example, a\ndependent person ordering a machine with an easy lip movement or by a simple\nsyllable pronunciation. Moreover, people with hearing problems compensate for\ntheir special needs by lip-reading as well as listening to the person with\nwhome they are talking.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2013 11:36:53 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Werda", "Salah", ""], ["Mahdi", "Walid", ""], ["Hamadou", "Abdelmajid Ben", ""]]}, {"id": "1301.4862", "submitter": "Pierre-Yves Oudeyer", "authors": "Adrien Baranes and Pierre-Yves Oudeyer", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal\n  Exploration in Robots", "comments": null, "journal-ref": "Baranes, A., Oudeyer, P-Y. (2013) Active Learning of Inverse\n  Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and\n  Autonomous Systems, 61(1), pp. 49-73", "doi": "10.1016/j.robot.2012.05.008", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive\nCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal\nexploration mechanism which allows active learning of inverse models in\nhigh-dimensional redundant robots. This allows a robot to efficiently and\nactively learn distributions of parameterized motor skills/policies that solve\na corresponding distribution of parameterized tasks/goals. The architecture\nmakes the robot sample actively novel parameterized tasks in the task space,\nbased on a measure of competence progress, each of which triggers low-level\ngoal-directed learning of the motor policy pa- rameters that allow to solve it.\nFor both learning and generalization, the system leverages regression\ntechniques which allow to infer the motor policy parameters corresponding to a\ngiven novel parameterized task, and based on the previously learnt\ncorrespondences between policy and task parameters. We present experiments with\nhigh-dimensional continuous sensorimotor spaces in three different robotic\nsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,\n2) learning omnidirectional locomotion with motor primitives in a quadruped\nrobot, 3) an arm learning to control a fishing rod with a flexible wire. We\nshow that 1) exploration in the task space can be a lot faster than exploration\nin the actuator space for learning inverse models in redundant robots; 2)\nselecting goals maximizing competence progress creates developmental\ntrajectories driving the robot to progressively focus on tasks of increasing\ncomplexity and is statistically significantly more efficient than selecting\ntasks randomly, as well as more efficient than different standard active motor\nbabbling methods; 3) this architecture allows the robot to actively discover\nwhich parts of its task space it can learn to reach and which part it cannot.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 13:26:07 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Baranes", "Adrien", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1301.5063", "submitter": "Ognjen Rudovic", "authors": "Ognjen Rudovic, Maja Pantic, Vladimir Pavlovic", "title": "Heteroscedastic Conditional Ordinal Random Fields for Pain Intensity\n  Estimation from Facial Images", "comments": "This paper has been withdrawn by the authors due to a crucial sign\n  error in equation 2&3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a novel method for automatic pain intensity estimation from facial\nimages based on the framework of kernel Conditional Ordinal Random Fields\n(KCORF). We extend this framework to account for heteroscedasticity on the\noutput labels(i.e., pain intensity scores) and introduce a novel dynamic\nfeatures, dynamic ranks, that impose temporal ordinal constraints on the static\nranks (i.e., intensity scores). Our experimental results show that the proposed\napproach outperforms state-of-the art methods for sequence classification with\nordinal data and other ordinal regression models. The approach performs\nsignificantly better than other models in terms of Intra-Class Correlation\nmeasure, which is the most accepted evaluation measure in the tasks of facial\nbehaviour intensity estimation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 03:40:52 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 18:43:47 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Pantic", "Maja", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1301.5348", "submitter": "Yangqing Jia", "authors": "Oriol Vinyals, Yangqing Jia, Trevor Darrell", "title": "Why Size Matters: Feature Coding as Nystrom Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the computer vision and machine learning community has been in\nfavor of feature extraction pipelines that rely on a coding step followed by a\nlinear classifier, due to their overall simplicity, well understood properties\nof linear classifiers, and their computational efficiency. In this paper we\npropose a novel view of this pipeline based on kernel methods and Nystrom\nsampling. In particular, we focus on the coding of a data point with a local\nrepresentation based on a dictionary with fewer elements than the number of\ndata points, and view it as an approximation to the actual function that would\ncompute pair-wise similarity to all data points (often too many to compute in\npractice), followed by a Nystrom sampling step to select a subset of all data\npoints.\n  Furthermore, since bounds are known on the approximation power of Nystrom\nsampling as a function of how many samples (i.e. dictionary size) we consider,\nwe can derive bounds on the approximation of the exact (but expensive to\ncompute) kernel matrix, and use it as a proxy to predict accuracy as a function\nof the dictionary size, which has been observed to increase but also to\nsaturate as we increase its size. This model may help explaining the positive\neffect of the codebook size and justifying the need to stack more layers (often\nreferred to as deep learning), as flat models empirically saturate as we add\nmore complexity.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 21:36:06 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2013 00:17:54 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Vinyals", "Oriol", ""], ["Jia", "Yangqing", ""], ["Darrell", "Trevor", ""]]}, {"id": "1301.5356", "submitter": "Ozan Sener", "authors": "Ozan Sener, Kemal Ugur, A. Aydin Alatan", "title": "Efficient MRF Energy Propagation for Video Segmentation via Bilateral\n  Filters", "comments": "Multimedia, IEEE Transactions on (Volume:16, Issue: 5, Aug. 2014)", "journal-ref": null, "doi": "10.1109/TMM.2014.2314069", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of an object from a video is a challenging task in multimedia\napplications. Depending on the application, automatic or interactive methods\nare desired; however, regardless of the application type, efficient computation\nof video object segmentation is crucial for time-critical applications;\nspecifically, mobile and interactive applications require near real-time\nefficiencies. In this paper, we address the problem of video segmentation from\nthe perspective of efficiency. We initially redefine the problem of video\nobject segmentation as the propagation of MRF energies along the temporal\ndomain. For this purpose, a novel and efficient method is proposed to propagate\nMRF energies throughout the frames via bilateral filters without using any\nglobal texture, color or shape model. Recently presented bi-exponential filter\nis utilized for efficiency, whereas a novel technique is also developed to\ndynamically solve graph-cuts for varying, non-lattice graphs in general linear\nfiltering scenario. These improvements are experimented for both automatic and\ninteractive video segmentation scenarios. Moreover, in addition to the\nefficiency, segmentation quality is also tested both quantitatively and\nqualitatively. Indeed, for some challenging examples, significant time\nefficiency is observed without loss of segmentation quality.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 22:26:33 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2013 17:55:33 GMT"}, {"version": "v3", "created": "Mon, 27 Oct 2014 03:29:00 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Sener", "Ozan", ""], ["Ugur", "Kemal", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "1301.5451", "submitter": "Qu Xiaobo", "authors": "Xiaobo Qu, Ying Chen, Xiaoxing Zhuang, Zhiyu Yan, Di Guo, Zhong Chen", "title": "Spread spectrum compressed sensing MRI using chirp radio frequency\n  pulses", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing has shown great potential in reducing data acquisition\ntime in magnetic resonance imaging (MRI). Recently, a spread spectrum\ncompressed sensing MRI method modulates an image with a quadratic phase. It\nperforms better than the conventional compressed sensing MRI with variable\ndensity sampling, since the coherence between the sensing and sparsity bases\nare reduced. However, spread spectrum in that method is implemented via a shim\ncoil which limits its modulation intensity and is not convenient to operate. In\nthis letter, we propose to apply chirp (linear frequency-swept) radio frequency\npulses to easily control the spread spectrum. To accelerate the image\nreconstruction, an alternating direction algorithm is modified by exploiting\nthe complex orthogonality of the quadratic phase encoding. Reconstruction on\nthe acquired data demonstrates that more image features are preserved using the\nproposed approach than those of conventional CS-MRI.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 09:59:38 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Qu", "Xiaobo", ""], ["Chen", "Ying", ""], ["Zhuang", "Xiaoxing", ""], ["Yan", "Zhiyu", ""], ["Guo", "Di", ""], ["Chen", "Zhong", ""]]}, {"id": "1301.5491", "submitter": "Stuart Bennett", "authors": "Stuart Bennett and Joan Lasenby", "title": "ChESS - Quick and Robust Detection of Chess-board Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of chess-board vertices is a common task in computer vision,\nunderpinning many applications, but relatively little work focusses on\ndesigning a specific feature detector that is fast, accurate and robust. In\nthis paper the `Chess-board Extraction by Subtraction and Summation' (ChESS)\nfeature detector, designed to exclusively respond to chess-board vertices, is\npresented. The method proposed is robust against noise, poor lighting and poor\ncontrast, requires no prior knowledge of the extent of the chess-board pattern,\nis computationally very efficient, and provides a strength measure of detected\nfeatures. Such a detector has significant application both in the key field of\ncamera calibration, as well as in Structured Light 3D reconstruction. Evidence\nis presented showing its robustness, accuracy, and efficiency in comparison to\nother commonly used detectors both under simulation and in experimental 3D\nreconstruction of flat plate and cylindrical objects\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 13:10:21 GMT"}], "update_date": "2013-01-24", "authors_parsed": [["Bennett", "Stuart", ""], ["Lasenby", "Joan", ""]]}, {"id": "1301.5582", "submitter": "Cheng Zhang", "authors": "Cheng Zhang and Hedvig Kjellstrom", "title": "Multi-Class Detection and Segmentation of Objects in Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of life of many people could be improved by autonomous humanoid\nrobots in the home. To function in the human world, a humanoid household robot\nmust be able to locate itself and perceive the environment like a human; scene\nperception, object detection and segmentation, and object spatial localization\nin 3D are fundamental capabilities for such humanoid robots. This paper\npresents a 3D multi-class object detection and segmentation method. The\ncontributions are twofold. Firstly, we present a multi-class detection method,\nwhere a minimal joint codebook is learned in a principled manner. Secondly, we\nincorporate depth information using RGB-D imagery, which increases the\nrobustness of the method and gives the 3D location of objects -- necessary\nsince the robot reasons in 3D space. Experiments show that the multi-class\nextension improves the detection efficiency with respect to the number of\nclasses and the depth extension improves the detection robustness and give\nsufficient natural 3D location of the objects.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 17:44:56 GMT"}], "update_date": "2013-01-24", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""]]}, {"id": "1301.6324", "submitter": "Togerchety Hitendra sarma", "authors": "T. Hitendra Sarma, P. Viswanath, D. Sai Koti Reddy and S. Sri Raghava", "title": "An improvement to k-nearest neighbor classifier", "comments": "Appeared in Third International Conference on Data Management, IMT\n  Ghaziabad, March 11-12, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest neighbor classifier (k-NNC) is simple to use and has little design\ntime like finding k values in k-nearest neighbor classifier, hence these are\nsuitable to work with dynamically varying data-sets. There exists some\nfundamental improvements over the basic k-NNC, like weighted k-nearest\nneighbors classifier (where weights to nearest neighbors are given based on\nlinear interpolation), using artificially generated training set called\nbootstrapped training set, etc. These improvements are orthogonal to space\nreduction and classification time reduction techniques, hence can be coupled\nwith any of them. The paper proposes another improvement to the basic k-NNC\nwhere the weights to nearest neighbors are given based on Gaussian distribution\n(instead of linear interpolation as done in weighted k-NNC) which is also\nindependent of any space reduction and classification time reduction technique.\nWe formally show that our proposed method is closely related to non-parametric\ndensity estimation using a Gaussian kernel. We experimentally demonstrate using\nvarious standard data-sets that the proposed method is better than the existing\nones in most cases.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2013 06:55:55 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Sarma", "T. Hitendra", ""], ["Viswanath", "P.", ""], ["Reddy", "D. Sai Koti", ""], ["Raghava", "S. Sri", ""]]}, {"id": "1301.6646", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi and Pascal Frossard", "title": "Image registration with sparse approximations in parametric dictionaries", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 2013 6:4, 2370-2403", "doi": "10.1137/130907872", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine in this paper the problem of image registration from the new\nperspective where images are given by sparse approximations in parametric\ndictionaries of geometric functions. We propose a registration algorithm that\nlooks for an estimate of the global transformation between sparse images by\nexamining the set of relative geometrical transformations between the\nrespective features. We propose a theoretical analysis of our registration\nalgorithm and we derive performance guarantees based on two novel important\nproperties of redundant dictionaries, namely the robust linear independence and\nthe transformation inconsistency. We propose several illustrations and insights\nabout the importance of these dictionary properties and show that common\nproperties such as coherence or restricted isometry property fail to provide\nsufficient information in registration problems. We finally show with\nillustrative experiments on simple visual objects and handwritten digits images\nthat our algorithm outperforms baseline competitor methods in terms of\ntransformation-invariant distance computation and classification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 19:06:44 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2013 14:21:42 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Frossard", "Pascal", ""]]}, {"id": "1301.6701", "submitter": "Dominique Gruyer", "authors": "Dominique Gruyer, Veronique Berge-Cherfaoui", "title": "Multi-objects association in perception of dynamical situation", "comments": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1999-PG-255-262", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current perception systems applied to the rebuilding of the environment\nfor intelligent vehicles, the part reserved to object association for the\ntracking is increasingly significant. This allows firstly to follow the objects\ntemporal evolution and secondly to increase the reliability of environment\nperception. We propose in this communication the development of a multi-objects\nassociation algorithm with ambiguity removal entering into the design of such a\ndynamic perception system for intelligent vehicles. This algorithm uses the\nbelief theory and data modelling with fuzzy mathematics in order to be able to\nhandle inaccurate as well as uncertain information due to imperfect sensors.\nThese theories also allow the fusion of numerical as well as symbolic data. We\ndevelop in this article the problem of matching between known and perceived\nobjects. This makes it possible to update a dynamic environment map for a\nvehicle. The belief theory will enable us to quantify the belief in the\nassociation of each perceived object with each known object. Conflicts can\nappear in the case of object appearance or disappearance, or in the case of a\nconfused situation or bad perception. These conflicts are removed or solved\nusing an assignment algorithm, giving a solution called the \" best \" and so\nensuring the tracking of some objects present in our environment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 15:58:26 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Gruyer", "Dominique", ""], ["Berge-Cherfaoui", "Veronique", ""]]}, {"id": "1301.6791", "submitter": "Weiyu Xu", "authors": "Jian-Feng Cai and Weiyu Xu", "title": "Guarantees of Total Variation Minimization for Signal Recovery", "comments": "lower bounds added; version with Gaussian width, improved bounds;\n  stability results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider using total variation minimization to recover\nsignals whose gradients have a sparse support, from a small number of\nmeasurements. We establish the proof for the performance guarantee of total\nvariation (TV) minimization in recovering \\emph{one-dimensional} signal with\nsparse gradient support. This partially answers the open problem of proving the\nfidelity of total variation minimization in such a setting \\cite{TVMulti}. In\nparticular, we have shown that the recoverable gradient sparsity can grow\nlinearly with the signal dimension when TV minimization is used. Recoverable\nsparsity thresholds of TV minimization are explicitly computed for\n1-dimensional signal by using the Grassmann angle framework. We also extend our\nresults to TV minimization for multidimensional signals. Stability of\nrecovering signal itself using 1-D TV minimization has also been established\nthrough a property called \"almost Euclidean property for 1-dimensional TV\nnorm\". We further give a lower bound on the number of random Gaussian\nmeasurements for recovering 1-dimensional signal vectors with $N$ elements and\n$K$-sparse gradients. Interestingly, the number of needed measurements is lower\nbounded by $\\Omega((NK)^{\\frac{1}{2}})$, rather than the $O(K\\log(N/K))$ bound\nfrequently appearing in recovering $K$-sparse signal vectors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 22:01:22 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2013 19:33:11 GMT"}, {"version": "v3", "created": "Fri, 17 May 2013 05:00:55 GMT"}, {"version": "v4", "created": "Mon, 20 May 2013 17:34:30 GMT"}, {"version": "v5", "created": "Wed, 18 Sep 2013 19:59:04 GMT"}, {"version": "v6", "created": "Fri, 11 Oct 2013 16:47:24 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Xu", "Weiyu", ""]]}, {"id": "1301.6847", "submitter": "Zhilin Zhang", "authors": "Taiyong Li, Zhilin Zhang", "title": "Robust Face Recognition via Block Sparse Bayesian Learning", "comments": "Accepted by Mathematical Problems in Engineering in 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) is an important task in pattern recognition and\ncomputer vision. Sparse representation (SR) has been demonstrated to be a\npowerful framework for FR. In general, an SR algorithm treats each face in a\ntraining dataset as a basis function, and tries to find a sparse representation\nof a test face under these basis functions. The sparse representation\ncoefficients then provide a recognition hint. Early SR algorithms are based on\na basic sparse model. Recently, it has been found that algorithms based on a\nblock sparse model can achieve better recognition rates. Based on this model,\nin this study we use block sparse Bayesian learning (BSBL) to find a sparse\nrepresentation of a test face for recognition. BSBL is a recently proposed\nframework, which has many advantages over existing block-sparse-model based\nalgorithms. Experimental results on the Extended Yale B, the AR and the CMU PIE\nface databases show that using BSBL can achieve better recognition rates and\nhigher robustness than state-of-the-art algorithms in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 07:23:00 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 00:19:12 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Li", "Taiyong", ""], ["Zhang", "Zhilin", ""]]}, {"id": "1301.6952", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, LNAO), Alexis Barbot\n  (LNAO), Benjamin Thyreau (LNAO), Vincent Frouin (LNAO), Ga\\\"el Varoquaux\n  (INRIA Saclay - Ile de France, LNAO), Aditya Siram, Daniel Marcus,\n  Jean-Baptiste Poline (LNAO)", "title": "PyXNAT: XNAT in Python", "comments": null, "journal-ref": "Frontiers in Neuroinformatics 6, 12 (2012) 1-14", "doi": "10.3389/fninf.2012.00012", "report-no": null, "categories": "cs.DB cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neuroimaging databases grow in size and complexity, the time researchers\nspend investigating and managing the data increases to the expense of data\nanalysis. As a result, investigators rely more and more heavily on scripting\nusing high-level languages to automate data management and processing tasks.\nFor this, a structured and programmatic access to the data store is necessary.\nWeb services are a first step toward this goal. They however lack in\nfunctionality and ease of use because they provide only low level interfaces to\ndatabases. We introduce here PyXNAT, a Python module that interacts with The\nExtensible Neuroimaging Archive Toolkit (XNAT) through native Python calls\nacross multiple operating systems. The choice of Python enables PyXNAT to\nexpose the XNAT Web Services and unify their features with a higher level and\nmore expressive language. PyXNAT provides XNAT users direct access to all the\nscientific packages in Python. Finally PyXNAT aims to be efficient and easy to\nuse, both as a backend library to build XNAT clients and as an alternative\nfrontend from the command line.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 15:42:18 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, LNAO"], ["Barbot", "Alexis", "", "LNAO"], ["Thyreau", "Benjamin", "", "LNAO"], ["Frouin", "Vincent", "", "LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Siram", "Aditya", "", "LNAO"], ["Marcus", "Daniel", "", "LNAO"], ["Poline", "Jean-Baptiste", "", "LNAO"]]}, {"id": "1301.7641", "submitter": "Anh Cat Le Ngo", "authors": "Anh Cat Le Ngo, Kenneth Li-Minn Ang, Guoping Qiu, Jasmine Kah-Phooi\n  Seng", "title": "Multi-scale Discriminant Saliency with Wavelet-based Hidden Markov Tree\n  Modelling", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.3964", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bottom-up saliency, an early stage of humans' visual attention, can be\nconsidered as a binary classification problem between centre and surround\nclasses. Discriminant power of features for the classification is measured as\nmutual information between distributions of image features and corresponding\nclasses . As the estimated discrepancy very much depends on considered scale\nlevel, multi-scale structure and discriminant power are integrated by employing\ndiscrete wavelet features and Hidden Markov Tree (HMT). With wavelet\ncoefficients and Hidden Markov Tree parameters, quad-tree like label structures\nare constructed and utilized in maximum a posterior probability (MAP) of hidden\nclass variables at corresponding dyadic sub-squares. Then, a saliency value for\neach square block at each scale level is computed with discriminant power\nprinciple. Finally, across multiple scales is integrated the final saliency map\nby an information maximization rule. Both standard quantitative tools such as\nNSS, LCC, AUC and qualitative assessments are used for evaluating the proposed\nmulti-scale discriminant saliency (MDIS) method against the well-know\ninformation based approach AIM on its released image collection with\neye-tracking data. Simulation results are presented and analysed to verify the\nvalidity of MDIS as well as point out its limitation for further research\ndirection.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 15:20:17 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 05:33:16 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Ngo", "Anh Cat Le", ""], ["Ang", "Kenneth Li-Minn", ""], ["Qiu", "Guoping", ""], ["Seng", "Jasmine Kah-Phooi", ""]]}, {"id": "1301.7661", "submitter": "Anh Cat Le Ngo", "authors": "Anh Cat Le Ngo, Guoping Qiu, Geoff Underwood, Kenneth Li-Minn Ang,\n  Jasmine Kah-Phooi Seng", "title": "Fast non parametric entropy estimation for spatial-temporal saliency\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates bottom-up visual saliency as center surround\nconditional entropy and presents a fast and efficient technique for the\ncomputation of such a saliency map. It is shown that the new saliency\nformulation is consistent with self-information based saliency,\ndecision-theoretic saliency and Bayesian definition of surprises but also faces\nthe same significant computational challenge of estimating probability density\nin very high dimensional spaces with limited samples. We have developed a fast\nand efficient nonparametric method to make the practical implementation of\nthese types of saliency maps possible. By aligning pixels from the center and\nsurround regions and treating their location coordinates as random variables,\nwe use a k-d partitioning method to efficiently estimating the center surround\nconditional entropy. We present experimental results on two publicly available\neye tracking still image databases and show that the new technique is\ncompetitive with state of the art bottom-up saliency computational methods. We\nhave also extended the technique to compute spatiotemporal visual saliency of\nvideo and evaluate the bottom-up spatiotemporal saliency against eye tracking\ndata on a video taken onboard a moving vehicle with the driver's eye being\ntracked by a head mounted eye-tracker.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 16:05:26 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Ngo", "Anh Cat Le", ""], ["Qiu", "Guoping", ""], ["Underwood", "Geoff", ""], ["Ang", "Kenneth Li-Minn", ""], ["Seng", "Jasmine Kah-Phooi", ""]]}]